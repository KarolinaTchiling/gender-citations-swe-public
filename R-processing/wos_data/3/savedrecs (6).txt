FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Xiong, C
   Gao, GY
   Zha, ZJ
   Yan, SC
   Ma, HD
   Kim, TK
AF Xiong, Chao
   Gao, Guangyu
   Zha, Zhengjun
   Yan, Shuicheng
   Ma, Huadong
   Kim, Tae-Kyun
TI Adaptive Learning for Celebrity Identification With Video Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive learning; celebrity identification; related samples;
   semi-supervised learning; video context
ID FRAMEWORK; SCALE
AB In this paper, we propose a novel semi-supervised learning strategy to address the problem of celebrity identification. The video context information is explored to facilitate the learning process based on the assumption that faces in the same video track share the same identity. Once a frame within a track is recognized confidently, the label can be propagated through the whole track, referred to as the confident track. More specifically, given a few static images and vast face videos, an initial weak classifier is trained and gradually evolves by iteratively promoting the confident tracks into the "labeled" set. The iterative selection process enriches the diversity of the "labeled" set such that the performance of the classifier is gradually improved. This learning theme may suffer from semantic drifting caused by errors in selecting the confident tracks. To address this issue, we propose to treat the selected frames as related samples-an intermediate state between labeled and unlabeled instead of labeled as in the traditional approach. To evaluate the performance, we construct a new dataset, which includes 3000 static images and 2700 face tracks of 30 celebrities. Comprehensive evaluations on this dataset and a public video dataset indicate significant improvement of our approach over established baseline methods.
C1 [Xiong, Chao; Kim, Tae-Kyun] Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, London SW7 2AZ, England.
   [Gao, Guangyu] Beijing Inst Technol, Sch Software, Beijing 100089, Peoples R China.
   [Zha, Zhengjun] Chinese Acad Sci, Inst Intelligent Machines, Hefei 230031, Anhui, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
   [Ma, Huadong] Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China.
C3 Imperial College London; Beijing Institute of Technology; Chinese
   Academy of Sciences; Hefei Institutes of Physical Science, CAS; National
   University of Singapore; Beijing University of Posts &
   Telecommunications
RP Gao, GY (corresponding author), Beijing Inst Technol, Sch Software, Beijing 100089, Peoples R China.
EM chao.xiong10@imperial.ac.uk; guangyu.ryan@gmail.com; zhazj@iim.ac.cn;
   eleyans@nus.edu.sg; mhd@bupt.edu.cn; tk.kim@imperial.ac.uk
RI Zha, Zheng-Jun/AAE-8408-2020; Zha, Zheng-Jun/AAF-8667-2020; Kim,
   Tae-Kyun/HTL-2208-2023; Yan, Shuicheng/HCI-1431-2022
OI Zha, Zheng-Jun/0000-0003-2510-8993; Kim, Tae-Kyun/0000-0002-7587-6053; 
CR [Anonymous], ADV NEURAL INF PROCE
   [Anonymous], P 12 EUR C COMP VIS
   [Anonymous], P 1999 INT C COMP VI
   [Anonymous], P 2000 INT C MACH LE
   [Anonymous], P 2012 IEEE COMP SOC
   [Anonymous], P 2010 EUR C COMP VI
   [Anonymous], 2006, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   [Anonymous], P 2011 IEEE INT C MU
   [Anonymous], J OPT SOC AM A
   [Anonymous], P 2012 EUR C COMP VI
   [Anonymous], P 2004 IEEE COMP SOC
   [Anonymous], P 2006 C N AM CHAPT
   [Anonymous], P 2003 INT C MACH LE
   Arandjelovic O, 2005, PROC CVPR IEEE, P860
   Bäuml M, 2013, PROC CVPR IEEE, P3602, DOI 10.1109/CVPR.2013.462
   Ballan L, 2010, MULTIMED TOOLS APPL, V48, P313, DOI 10.1007/s11042-009-0342-4
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bertini M., 2006, PROC 14 ANN ACM INT, P663
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Chen CY, 2013, PROC CVPR IEEE, P572, DOI 10.1109/CVPR.2013.80
   Choi J, 2013, PROC CVPR IEEE, P875, DOI 10.1109/CVPR.2013.118
   Collobert R, 2006, J MACH LEARN RES, V7, P1687
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Everingham M., 2006, BMVC, V2, P6
   Fathi A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.78
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Grossberg S, 2013, NEURAL NETWORKS, V37, P1, DOI [10.1016/j.neunet.2012.09.017, 10.1016/j.neunet.2011.10.011]
   Kim M., 2008, PROC 2008 IEEE C COM, P1
   Li X, 2013, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2013.116
   Liu Z, 2007, IEEE T MULTIMEDIA, V9, P89, DOI 10.1109/TMM.2006.886360
   Melacci S, 2011, J MACH LEARN RES, V12, P1149
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065
   Quang M. H., 2013, Proc. International Conference on Machine Learning, P100
   Saffari A, 2010, LECT NOTES COMPUT SC, V6313, P776
   Satoh S, 1999, IEEE MULTIMEDIA, V6, P22, DOI 10.1109/93.752960
   Schölkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27
   Sun SL, 2011, LECT NOTES ARTIF INT, V7121, P209
   Wang S, 2011, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2011.6126385
   Yan R, 2006, IEEE T PATTERN ANAL, V28, P578, DOI 10.1109/TPAMI.2006.65
   Yang Y, 2013, PROC CVPR IEEE, P1650, DOI 10.1109/CVPR.2013.216
   Yarowsky D., 1995, 33 ANN M ASS COMPUTA, P189, DOI DOI 10.3115/981658.981684
   Zhang X, 2012, IEEE T MULTIMEDIA, V14, P995, DOI 10.1109/TMM.2012.2186121
   Zhao M., 2008, PROC 8 IEEE INT C AU, P1
NR 44
TC 6
Z9 7
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1473
EP 1485
DI 10.1109/TMM.2014.2316475
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600026
DA 2024-07-18
ER

PT J
AU Wang, C
   Guo, YW
   Zhu, J
   Wang, LB
   Wang, WP
AF Wang, Chuan
   Guo, Yanwen
   Zhu, Jie
   Wang, Linbo
   Wang, Wenping
TI Video Object Co-Segmentation via Subspace Clustering and Quadratic
   Pseudo-Boolean Optimization in an MRF Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Co-segmentation; subspace clustering; video
ID SHAPES
AB Multiple videos may share a common foreground object, for instance a family member in home videos, or a leading role in various clips of a movie or TV series. In this paper, we present a novel method for co-segmenting the common foreground object from a group of video sequences. The issue was seldom touched on in the literature. Starting from over-segmentation of each video into Temporal Superpixels (TSPs), we first propose a new subspace clustering algorithm which segments the videos into consistent spatio-temporal regions with multiple classes, such that the common foreground has consistent labels across different videos. The subspace clustering algorithm exploits the fact that across different videos the common foreground shares similar appearance features, while motions can be used to better differentiate regions within each video, making accurate extraction of object boundaries easier. We further formulate video object co-segmentation as a Markov Random Field (MRF) model which imposes the constraint of foreground model automatically computed or specified with little user effort. The Quadratic Pseudo-Boolean Optimization (QPBO) is used to generate the results. Experiments show that this video co-segmentation framework can achieve good quality foreground extraction results without user interaction for those videos with unrelated background, and with only moderate user interaction for those videos with similar background. Comparisons with previous work also show the superiority of our approach.
C1 [Wang, Chuan; Wang, Wenping] Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Guo, Yanwen; Zhu, Jie; Wang, Linbo] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210008, Jiangsu, Peoples R China.
C3 University of Hong Kong; Nanjing University
RP Wang, C (corresponding author), Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM cwang@cs.hku.hk; ywguo@nju.edu.cn; magickuang@126.com;
   wanglb.2005@gmail.com; wenping@cs.hku.hk
RI zhang, jinlu/KEE-9374-2024; Wang, Chuan/K-8251-2019; wang,
   lin/GSE-3040-2022; Liu, Kai/IST-6808-2023; Huang, YQ/JOK-7580-2023;
   ZHANG, YINGFANG/JQW-2816-2023
OI Wang, Chuan/0000-0002-8559-4519; Wang, Linbo/0000-0001-7276-7065
FU National Natural Science Foundation of China [61373059, 61321491];
   National Basic Research Program of China [2010CB327903]; Jiangsu Green
   Blue Project
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61373059 and 61321491, the National
   Basic Research Program of China (2010CB327903), and the Jiangsu Green
   Blue Project. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Sheng-Wei (Kuan-Ta)
   Chen.
CR Agarwala A., ACM T GRAPH, V23, P584
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2007, CVPR, DOI DOI 10.1109/CVPR.2007.383203
   Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chang J., 2013, P IEEE CVPR
   Chen D.-J., 2012, P 20 ACM INT C MULTI, P805, DOI [10.1145/2393347.2396317, DOI 10.1145/2393347.2396317]
   Chiu W.-C., 2013, P IEEE CVPR
   Chuang YY, 2002, ACM T GRAPHIC, V21, P243, DOI 10.1145/566570.566572
   Fu Yanan., 2011, International Midwest Symposium on Circuits and Systems, P1
   Golovinskiy A, 2009, COMPUT GRAPH-UK, V33, P262, DOI 10.1016/j.cag.2009.03.010
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   Hu RZ, 2012, COMPUT GRAPH FORUM, V31, P1703, DOI 10.1111/j.1467-8659.2012.03175.x
   Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Kolmogorov V., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234
   Lin, 2010, ARXIV10095055
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Mukherjee L, 2012, LECT NOTES COMPUT SC, V7575, P128, DOI 10.1007/978-3-642-33765-9_10
   Mukherjee Lopamudra, 2011, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, P1881, DOI 10.1109/CVPR.2011.5995420
   Ochs P., 2011, P IEEE ICCV
   Ochs P, 2012, PROC CVPR IEEE, P614, DOI 10.1109/CVPR.2012.6247728
   Rubio J.C., 2012, ASIAN C COMPUTER VIS, P13, DOI DOI 10.1007/978-3-642-37444-9
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sidi O, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024160
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tong RF, 2011, COMPUT GRAPH FORUM, V30, P2049, DOI 10.1111/j.1467-8659.2011.02038.x
   Vidal R, 2004, LECT NOTES COMPUT SC, V3021, P1
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   Wang J., ACM T GRAPH, V24, P585
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45
   Zhong F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366194
NR 37
TC 31
Z9 34
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 903
EP 916
DI 10.1109/TMM.2014.2306393
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800002
DA 2024-07-18
ER

PT J
AU Qu, Z
   Wang, JQ
   Xu, M
   Lu, HQ
AF Qu, Zhan
   Wang, Jinqiao
   Xu, Min
   Lu, Hanqing
TI Context-Aware Video Retargeting via Graph Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Context-aware; grid graph model; spatial-temporal correlation; video
   retargeting
ID ENERGY MINIMIZATION
AB Video retargeting is a crowded but challenging research area. In order to maximally comfort the viewers' watching experience, the most challenging issue is how to retain the spatial shape of important objects while ensure temporal smoothness and coherence. Existing retargeting techniques deal with these spatial-temporal requirements individually, which preserve the spatial geometry and temporal coherence for each region. However, the spatial-temporal property of the video content should be context-relevant, i.e., the regions belonging to the same object are supposed to undergo uniform spatial-temporal transformation. Regardless of the contextual information, the divide-and-rule strategy of existing techniques usually incurs various spatial-temporal artifacts. In order to achieve satisfactory spatial-temporal coherent video retargeting, in this paper, a novel context-aware solution is proposed via graph model. First, we employ a grid-based warping framework to preserve the spatial structure and temporal motion trend at the unit of grid cell. Second, we propose a graph-based motion layer partition algorithm to estimate motions of different regions, which simultaneously provides the evaluation of contextual relationship between grid cells while estimating the motions of regions. Third, complementing the salience-based spatial-temporal information preservation, two novel context constraints are encoded for encouraging the grid cells of the same object to undergo uniform spatial and temporal transformation, respectively. Finally, we formulate the objective function as a quadratic programming problem. Our method achieves a satisfactory spatial-temporal coherence while maximally avoiding the influence of artifacts. In addition, the grid-cell-wise motion estimation could be calculated every few frames, which obviously improves the speed. Experimental results and comparisons with state-of-the-art methods demonstrate the effectiveness and efficiency of our approach.
C1 [Qu, Zhan; Wang, Jinqiao; Lu, Hanqing] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Xu, Min] Univ Technol Sydney, Sch Comp & Commun, INEXT, Sydney, NSW 2007, Australia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; University of
   Technology Sydney
RP Qu, Z (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM zqu@nlpr.ia.ac.cn; jqwang@nlpr.ia.ac.cn; min.xu25@gmail.com;
   luhq@nlpr.ia.ac.cn
RI qu, zhan/ISS-2619-2023
OI Xu, Min/0000-0001-9581-8849
FU 973 Program [2010CB327905]; National Natural Science Foundation of China
   [61070104, 61003161, 60905008, 61273034]
FX This work was supported in part by 973 Program under Grant 2010CB327905
   and the National Natural Science Foundation of China under Grants
   61070104, 61003161, 60905008, and 61273034. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Monica Aguilar.
CR [Anonymous], 2011, ICCV
   [Anonymous], 2012, PROC EUROGRAPHICS C
   [Anonymous], P CVPR
   [Anonymous], 2006, P 14 ACM INT C MULT
   [Anonymous], P CVPR
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Butman Moshe, 2008, P CVPR, P1
   Chen L.-Q., 2003, ACM MULTIMEDIA SYST, V9
   Deselaers T., 2008, P CVPR
   Ding Y., 2011, P CVPR
   Dong W.-M., 2009, SIGGRAPH
   Gal R., 2006, P 17 EUROGRAPHICS C, P297, DOI DOI 10.2312/EGWR/EGSR06/297-303
   Ghareeb M., 2011, 2011 International Conference on Information Networking (ICOIN), P206, DOI 10.1109/ICOIN.2011.5723179
   Gleicher ML, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1404880.1404882
   Grundmann M., 2010, P CVPR
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   [李保松 Li Baosong], 2011, [高分子通报, Polymer Bulletin], P1
   Liu Hao., 2003, P ACM INT C MULTIMED, P148
   Liu L.-G., 2010, COMPUT GRAPHIC FORUM, V29
   Mansfield A., 2010, P ECCV
   Panozzo D, 2012, COMPUT GRAPH FORUM, V31, P229, DOI 10.1111/j.1467-8659.2012.03001.x
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rasheed Z, 2003, PROC CVPR IEEE, P343
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Suoheng Li, 2012, 2012 International Conference on Computing, Networking and Communications (ICNC), P944, DOI 10.1109/ICCNC.2012.6167564
   Tao C., 2007, P ICCV
   Viola P.J. M., 2004, Int. J. Comput. Vis
   Wang J., 2006, P ACCV
   Wang Y.-S., 2010, ACM T GRAPHICS
   Wang YS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964983
   Wang YS, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618473
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wolf L., 2007, P ICCV
   Zhang Y.-F., 2008, SHRINKABILITY MAPS C
NR 41
TC 12
Z9 12
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1677
EP 1687
DI 10.1109/TMM.2013.2267727
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800018
DA 2024-07-18
ER

PT J
AU Tahir, MA
   Yan, F
   Koniusz, P
   Awais, M
   Barnard, M
   Mikolajczyk, K
   Bouridane, A
   Kittler, J
AF Tahir, Muhammad Atif
   Yan, Fei
   Koniusz, Peter
   Awais, Muhammad
   Barnard, Mark
   Mikolajczyk, Krystian
   Bouridane, Ahmed
   Kittler, Josef
TI A Robust and Scalable Visual Category and Action Recognition System
   Using Kernel Discriminant Analysis With Spectral Regression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition from still images; kernel discriminant analysis;
   SIFT; visual category recognition
ID CLASSIFICATION
AB Visual concept detection and action recognition are one of the most important tasks in content-based multimedia information retrieval (CBMIR) technology. It aims at annotating images using a vocabulary defined by a set of concepts of interest including scenes types (mountains, snow, etc.) or human actions (phoning, playing instrument). This paper describes our system in the ImageCLEF@ICPR10, Pascal VOC 08 Visual Concept Detection and Pascal VOC 10 Action Recognition Challenges. The proposed system ranked first in these large-scale tasks when evaluated independently by the organizers. The proposed system involves state-of-the-art local descriptor computation, vector quantization via clustering, structured scene or object representation via localized histograms of vector codes, similarity measure for kernel construction and classifier learning. The main novelty is the classifier-level and kernel-level fusion using Kernel Discriminant Analysis and Spectral Regression (SR-KDA) with RBF Chi-Squared kernels obtained from various image descriptors. The distinctiveness of the proposed method is also assessed experimentally using a video benchmark: the Mediamill Challenge along with benchmarks from ImageCLEF@ICPR10, Pascal VOC 10 and Pascal VOC 08. From the experimental results, it can be derived that the presented system consistently yields significant performance gains when compared with the state-of-the art methods. The other strong point is the introduction of SR-KDA in the classification stage where the time complexity scales linearly with respect to the number of concepts and the main computational complexity is independent of the number of categories.
C1 [Tahir, Muhammad Atif] Al Imam Mohammad Ibn Saud Islamic Univ, Coll Comp & Informat Sci, Riyadh 11432, Saudi Arabia.
   [Yan, Fei; Koniusz, Peter; Awais, Muhammad; Barnard, Mark; Mikolajczyk, Krystian; Kittler, Josef] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
   [Bouridane, Ahmed] Northumbria Univ, Comp Elect & Secur Res Grp, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
C3 Imam Mohammad Ibn Saud Islamic University (IMSIU); University of Surrey;
   Northumbria University
RP Tahir, MA (corresponding author), Al Imam Mohammad Ibn Saud Islamic Univ, Coll Comp & Informat Sci, Riyadh 11432, Saudi Arabia.
EM mtahir@ccis.imamu.edu.sa; f.yan@surrey.ac.uk; p.koniusz@surrey.ac.uk;
   m.awais@surrey.ac.uk; mark.barnard@surrey.ac.uk;
   k.mikolajczyk@surrey.ac.uk; ahmed.bouridane@unn.ac.uk;
   j.kittler@surrey.ac.uk
RI Awais, Muhammad/HCI-3725-2022; Asif, Muhammad/IQS-5311-2023
FU EPSRC project ACASVA [EP/F069421/1]; EPSRC [EP/F069421/1, EP/K01904X/1]
   Funding Source: UKRI
FX This work was supported in part by EPSRC project ACASVA EP/F069421/1.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Xian-Sheng Hua.
CR AN S, 2007, P CVPR
   [Anonymous], CLEF WORKING NOTES
   [Anonymous], 2004, IJCV
   [Anonymous], 1999, NEURAL NETWORKS SIGN
   [Anonymous], 2006, IEEE COMP SOC C COMP
   [Anonymous], 2003, P ICCV
   [Anonymous], P ACM INT WORKSH MUL
   [Anonymous], 2010, PASCAL VISUAL OBJECT
   Bar-Hillel A., 2003, ICML, P11
   Baudat G, 2000, NEURAL COMPUT, V12, P2385, DOI 10.1162/089976600300014980
   Cai D., 2007, P ICDM
   Cai D, 2011, VLDB J, V20, P21, DOI 10.1007/s00778-010-0189-3
   Cai Deng., 2007, P ICCV
   Chen Y., 2007, P NIPS
   Csurka Gabriella, 2004, ECCV WORKSH
   Dielmann A, 2007, IEEE T MULTIMEDIA, V9, P25, DOI 10.1109/TMM.2006.886337
   Everingham M., 2008, P PASC VOC WORKSH
   Fu ZY, 2011, LECT NOTES COMPUT SC, V7063, P490, DOI 10.1007/978-3-642-24958-7_57
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Gaidon A., 2008, TECHNICAL REPORT
   Golub G.H., 1989, MATRIX COMPUTATIONS
   Grauman K, 2007, J MACH LEARN RES, V8, P725
   Herbich R, 2001, J MACH LEARN RES, V1, P245, DOI 10.1162/153244301753683717
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Kittler J, 1998, PATTERN ANAL APPL, V1, P18, DOI 10.1007/BF01238023
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Kludas J., 2007, P INT WORKSH AD MULT
   Koniusz P., 2009, P BMVC
   Koniusz P., 2010, P ICPR
   Koniusz P., 2011, P ICIP
   Kuncheva L. I., 2004, COMBINING PATTERN CL, V390, P413
   Li F., 2003, P ICML
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Liu XZ, 2005, PATTERN RECOGN, V38, P887, DOI 10.1016/j.patcog.2004.11.008
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Marsza M. e., P PASC WORKSH
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mylonas P, 2009, IEEE T MULTIMEDIA, V11, P229, DOI 10.1109/TMM.2008.2009681
   Opelt A., 2006, PAMI, V28, P1531
   Poh N, 2005, IEEE T SIGNAL PROCES, V53, P4384, DOI 10.1109/TSP.2005.857006
   SNOEK CGM, 2006, P ACM INT C MULT
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Stefanie N., 2010, LNCS, V6388
   Stewart G. W., 1998, MATRIX ALGORITHMS, V1
   Tahir M. A., 2010, P ICPR2010 IST TURK
   Tahir M. A., 2009, P 2 INT WORKSH SUBSP
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   van Gemert JC, 2009, IEEE T MULTIMEDIA, V11, P780, DOI 10.1109/TMM.2009.2017619
   VANDESANDE KEA, 2008, P ACM INT C IM VID R
   Wu L, 2010, IEEE T IMAGE PROCESS, V19, P1908, DOI 10.1109/TIP.2010.2045169
   Wu L, 2009, IEEE T MULTIMEDIA, V11, P286, DOI 10.1109/TMM.2008.2009692
   Wu YC, 2007, J AM STAT ASSOC, V102, P974, DOI 10.1198/016214507000000617
   Yan F, 2012, J MACH LEARN RES, V13, P607
   Yao B., 2010, P CVPR
   Yuan J, 2007, PROC CVPR IEEE, P1930, DOI 10.1109/CVPR.2007.383222
   Zhang D, 2006, IEEE T MULTIMEDIA, V8, P509, DOI 10.1109/TMM.2006.870735
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhu GY, 2007, IEEE T MULTIMEDIA, V9, P1167, DOI 10.1109/TMM.2007.902847
NR 59
TC 12
Z9 13
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1653
EP 1664
DI 10.1109/TMM.2013.2264927
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800016
DA 2024-07-18
ER

PT J
AU Zhu, ZQ
   Li, SH
   Chen, XL
AF Zhu, Zuqing
   Li, Suoheng
   Chen, Xiaoliang
TI Design QoS-Aware Multi-Path Provisioning Strategies for Efficient
   Cloud-Assisted SVC Video Streaming to Heterogeneous Clients
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud-assisted video streaming; label switching; multi-path
   provisioning; scalable video coding
AB We layout a network infrastructure that leverages the storage and computing power of a cloud residing in the core for collecting network status and computing multi-path scalable video coding (SVC) streaming provisioning strategies. Therefore, in addition to its conventional tasks in the application layer, the cloud also gets involved in the network layer for the optimization of routing and forwarding. We call this scheme as cloud-assisted SVC streaming, and use it to further improve the performance of SVC streaming by using close cooperation between cloud and network. Compared to source-routing based provisioning, the cloud-assisted scheme can provide more cost-effective provisioning strategies by utilizing better knowledge of network environment together with more powerful computation power. We then propose several multi-path provisioning algorithms for cloud-assisted SVC streaming in heterogeneous networks. To the best of our knowledge, these are the first proposals to work on the problem of adaptive multi-path SVC streaming under the bandwidth, delay and differential delay constraints. Our design of the provisioning algorithms starts from an approach that is based on Max Flow and an Auxiliary Graph. Several extensions are then made based on this approach to address the situations such as provisioning from multiple sources and provisioning in dynamic network environments with rapid background traffic fluctuations. Simulations in both static and dynamic network environments show that the proposed algorithms can achieve effective performance improvements in terms of request blocking probability, bandwidth utilization, packet delay, packet loss rate, and video playback quality.
C1 [Zhu, Zuqing; Li, Suoheng; Chen, Xiaoliang] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhu, ZQ (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
EM zqzhu@ieee.org; lzzitc@mail.ustc.edu.cn; arabus@mail.ustc.edu.cn
RI Chen, Xiaoliang/U-2358-2019; Zhu, Zuqing/J-8431-2017
OI Zhu, Zuqing/0000-0002-4251-788X; Chen, Xiaoliang/0000-0002-7805-6237
FU Program for New Century Excellent Talents in University (NCET)
   [NCET-11-0884]
FX This work was supported by the Program for New Century Excellent Talents
   in University (NCET) under Project NCET-11-0884. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Joel Rodrigues.
CR [Anonymous], IEEE J SEL AREAS COM
   [Anonymous], 2008, IEEE 27 C COMP COMM
   Apostolopoulos J, 2002, IEEE INFOCOM SER, P1736, DOI 10.1109/INFCOM.2002.1019427
   Apostolopoulos JG, 2004, IEEE COMMUN MAG, V42, P80, DOI 10.1109/MCOM.2004.1321395
   Begen AC, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, P1583
   Chen ZZ, 2010, IEEE SIGNAL PROC LET, V17, P675, DOI 10.1109/LSP.2010.2046193
   Chow ALH, 2005, INT CONF QUANT EVAL, P63, DOI 10.1109/QEST.2005.41
   Franchi N, 2005, IEEE T CIRC SYST VID, V15, P321, DOI 10.1109/TCSVT.2004.842606
   Frossard P, 2008, P IEEE, V96, P39, DOI 10.1109/JPROC.2007.909876
   Ghareeb M., 2011, 2011 IEEE Symposium on Computers and Communications (ISCC 2011), P824, DOI 10.1109/ISCC.2011.5983944
   Ghareeb M., 2011, 2011 International Conference on Information Networking (ICOIN), P206, DOI 10.1109/ICOIN.2011.5723179
   Guo H, 2008, IEEE T KNOWL DATA EN, V20, P1273, DOI 10.1109/TKDE.2008.18
   Hsu CH, 2008, IEEE T MULTIMEDIA, V10, P457, DOI 10.1109/TMM.2008.917365
   Huang ZX, 2011, IEEE INFOCOM SER, P201, DOI 10.1109/INFCOM.2011.5935009
   Jurca D, 2007, IEEE T MULTIMEDIA, V9, P629, DOI 10.1109/TMM.2006.888017
   Jurca D, 2007, IEEE T MULTIMEDIA, V9, P1227, DOI 10.1109/TMM.2007.902852
   Lee Y, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, CONFERENCE PROCEEDINGS, P2431, DOI 10.1109/ICC.2002.997280
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Liang YJ, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P684
   Ma Z, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1330
   Mao SW, 2005, IEEE INFOCOM SER, P740
   Misra S, 2009, IEEE INFOCOM SER, P558, DOI 10.1109/INFCOM.2009.5061962
   Nguyen T, 2004, IEEE T MULTIMEDIA, V6, P315, DOI 10.1109/TMM.2003.822790
   Odlyzko AM, 2003, PROC SPIE, V5247, P1, DOI 10.1117/12.512942
   Politis I, 2008, GLOB TELECOMM CONF, DOI 10.1109/GLOCOM.2008.ECP.30
   Rosen E., 2001, Multiprotocol Label Switching Architecture
   Schierl T, 2007, IEEE INT SYMP CIRC S, P3455, DOI 10.1109/ISCAS.2007.378370
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Suoheng Li, 2012, 2012 International Conference on Computing, Networking and Communications (ICNC), P944, DOI 10.1109/ICCNC.2012.6167564
   Tao S, 2005, IEEE INFOCOM SER, P2268
   Wang GJ, 2007, IEEE MULTIMEDIA, V14, P74, DOI 10.1109/MMUL.2007.16
   Wang J, 2010, C LOCAL COMPUT NETW, P504, DOI 10.1109/LCN.2010.5735766
   Xia PY, 2011, IEEE T MULTIMEDIA, V13, P366, DOI 10.1109/TMM.2010.2098021
   Zhang W., 2010, P INF THEOR APPL WOR, P1, DOI DOI 10.1109/ICEEE.2010.5660926
NR 34
TC 54
Z9 57
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 758
EP 768
DI 10.1109/TMM.2013.2238908
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500005
DA 2024-07-18
ER

PT J
AU Yang, Y
   Ma, ZG
   Hauptmann, AG
   Sebe, N
AF Yang, Yi
   Ma, Zhigang
   Hauptmann, Alexander G.
   Sebe, Nicu
TI Feature Selection for Multimedia Analysis by Sharing Information Among
   Multiple Tasks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; image classification; multitask feature selection;
   3D motion data annotation
ID IMAGE ANNOTATION; CLASSIFICATION; RETRIEVAL; FRAMEWORK; SPARSITY; WEB
AB While much progress has been made to multi-task classification and subspace learning, multi-task feature selection has long been largely unaddressed. In this paper, we propose a new multi-task feature selection algorithm and apply it to multimedia (e.g., video and image) analysis. Instead of evaluating the importance of each feature individually, our algorithm selects features in a batch mode, by which the feature correlation is considered. While feature selection has received much research attention, less effort has been made on improving the performance of feature selection by leveraging the shared knowledge from multiple related tasks. Our algorithm builds upon the assumption that different related tasks have common structures. Multiple feature selection functions of different tasks are simultaneously learned in a joint framework, which enables our algorithm to utilize the common knowledge of multiple tasks as supplementary information to facilitate decision making. An efficient iterative algorithm is proposed to optimize it, whose convergence is guaranteed. Experiments on different databases have demonstrated the effectiveness of the proposed algorithm.
C1 [Yang, Yi; Hauptmann, Alexander G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
   [Ma, Zhigang; Sebe, Nicu] Univ Trent, Dept Informat Engn & Comp Sci, I-38100 Trento, Italy.
C3 Carnegie Mellon University; University of Trento
RP Yang, Y (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM yiyang@cs.cmu.edu; ma@disi.unitn.it; alex@cs.cmu.edu; sebe@disi.unitn.it
RI Sebe, Niculae/KEC-2000-2024; yang, yang/GVT-5210-2022; Ma,
   Zhigang/H-3543-2015; yang, yang/HGT-7999-2022; yang, yang/GWB-9426-2022;
   Lang, Ming/HIK-0758-2022; Yang, Yi/B-9273-2017
OI Sebe, Niculae/0000-0002-6597-7248; Yang, Yi/0000-0002-0512-880X
FU National Science Foundation [IIS-0917072]; National Institutes of Health
   (NIH) [1RC1MH090021-01]; European Commission [FP7-248984 GLOCAL];
   National Program on Key Basic Research Project of China (973 Program)
   [2010CB327903]; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [0917072] Funding Source: National
   Science Foundation
FX This material is based upon work supported in part by the National
   Science Foundation under Grants No. IIS-0917072, in part by the National
   Institutes of Health (NIH) Grant No. 1RC1MH090021-01, in part by the
   European Commission under the contract FP7-248984 GLOCAL, and in part by
   the National Program on Key Basic Research Project of China (973
   Program) under grant 2010CB327903. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Xian-Sheng Hua.
CR Ando RK, 2005, J MACH LEARN RES, V6, P1817
   [Anonymous], 2009, P BMVC
   [Anonymous], P IEEE WORKSH APPL C
   [Anonymous], P AAAI
   [Anonymous], 2010, P INT C MULT
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Bao L., 2011, P TRECV VID RETR EV
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chen C, 2011, IEEE T VIS COMPUT GR, V17, P1676, DOI 10.1109/TVCG.2010.272
   Chen X, 2012, IEEE T MULTIMEDIA, V14, P3, DOI 10.1109/TMM.2011.2167223
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Dudik M., 2012, AISTATS, P327
   Fan J, 2007, IEEE T MULTIMEDIA, V9, P939, DOI 10.1109/TMM.2007.900143
   Fung GM, 2005, MACH LEARN, V59, P77, DOI 10.1007/s10994-005-0463-6
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Hall M. A., 1999, P AAAI
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Jebara T, 2011, J MACH LEARN RES, V12, P75
   Ji SW, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754431
   Li Z., 2012, P AAAI
   Liu H., 2003, P ICML
   Lyons MJ, 1999, IEEE T PATTERN ANAL, V21, P1357, DOI 10.1109/34.817413
   Ma Z., 2012, P ACM MULT
   Ma Z., 2011, P ACM MULT
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1021, DOI 10.1109/TMM.2012.2187179
   Nie F., 2010, P NIPS, P1
   Ning H., 2008, P CVPR
   Obozinski G., 2006, P WORKSH STRUCT KNOW
   Obozinski G, 2010, STAT COMPUT, V20, P231, DOI 10.1007/s11222-008-9111-x
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Schuldt C., 2004, P ICPR
   Shao L., 2010, P ACM CIVR
   Talavera L., 2005, P IDA
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Yang Y., 2011, P IJCAI
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yang Y, 2012, IEEE T IMAGE PROCESS, V21, P1339, DOI 10.1109/TIP.2011.2169269
   Zhao Z., 2007, P ICML
NR 39
TC 178
Z9 189
U1 0
U2 53
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 661
EP 669
DI 10.1109/TMM.2012.2237023
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900016
DA 2024-07-18
ER

PT J
AU Zhu, XF
   Huang, Z
   Cui, JT
   Shen, HT
AF Zhu, Xiaofeng
   Huang, Zi
   Cui, Jiangtao
   Shen, Heng Tao
TI Video-to-Shot Tag Propagation by Graph Sparse Group Lasso
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Manifold learning; sparse coding; sparse group lasso; structure
   sparsity; video annotation; video tagging
ID ANNOTATION; WEB; REGULARIZATION; FRAMEWORK
AB Traditional approaches to video tagging are designed to propagate tags at the same level, such as assigning the tags of training videos (or shots) to the test videos (or shots), such as generating tags for the test video when the training videos are associated with the tags at the video-level or assigning tags to the test shot when given a collection of annotated shots. This paper focuses on automatical shot tagging given a collection of videos with the tags at the video-level. In other words, we aim to assign specific tags from the training videos to the test shot. The paper solves the V2S issue by assigning the test shot with the tags deriving from parts of the tags in a part of training videos. To achieve the goal, the paper first proposes a novel Graph Sparse Group Lasso (shorted for GSGL) model to linearly reconstruct the visual feature of the test shot with the visual features of the training videos, i.e., finding the correlation between the test shot and the training videos. The paper then proposes a new tagging propagation rule to assign the video-level tags to the test shot by the learnt correlations. Moreover, to effectively build the reconstruction model, the proposed GSGL simultaneously takes several constraints into account, such as the inter-group sparsity, the intra-group sparsity, the temporal-spatial prior knowledge in the training videos and the local structure of the test shot. Extensive experiments on public video datasets are conducted, which clearly demonstrate the effectiveness of the proposed method for dealing with the video-to-shot tag propagation.
C1 [Zhu, Xiaofeng; Huang, Zi; Shen, Heng Tao] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld, Australia.
   [Cui, Jiangtao] Xidian Univ, Sch Comp Sci, Xian, Peoples R China.
C3 University of Queensland; Xidian University
RP Zhu, XF (corresponding author), Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld, Australia.
EM zhux@itee.uq.edu.au; huang@itee.uq.edu.au; shenht@itee.uq.edu.au
RI Zhu, Xiaofeng/HII-5291-2022; Shen, Heng Tao/ABD-5331-2021; Zhu,
   Xiaofeng/F-3601-2016
OI Zhu, Xiaofeng/0000-0001-6840-0578; HUANG, ZI/0000-0002-9738-4949
FU Australia Research Council [DP1094678]; National Natural Science
   Foundation of China [61173089]; Australian Research Council [DP1094678]
   Funding Source: Australian Research Council
FX This work was supported in part by Australia Research Council DP1094678
   and in part by the National Natural Science Foundation of China Under
   Grant No. 61173089. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Chong-Wah Ngo.
CR [Anonymous], P INT C MULT
   [Anonymous], INT C IM VID RETR
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], ARXIV E PRINTS
   [Anonymous], KODAK CONSUMER VIDEO
   [Anonymous], 2009, P 17 ACM INT C MULTI
   [Anonymous], P TRECVID 2007 WORKS
   [Anonymous], P ACM INT C MULT INF
   [Anonymous], 2010, P 18 ACM INT C MULT
   [Anonymous], P INT C MULT
   [Anonymous], 2008, CORR
   [Anonymous], 2009, PROC 17 ACM INT C MU
   [Anonymous], IEEE T PATT IN PRESS
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   Dijun Luo, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P344, DOI 10.1109/ICDM.2010.155
   Ebadollahi S, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P881, DOI 10.1109/ICME.2006.262691
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Gao SH, 2010, PROC CVPR IEEE, P3555, DOI 10.1109/CVPR.2010.5539943
   Golub G.H., 1989, MATRIX COMPUTATIONS
   Hsu WinstonH., 2007, ACM MM
   Jiang W, 2007, INT CONF ACOUST SPEE, P949
   Jiang YG, 2009, IEEE I CONF COMP VIS, P1420, DOI 10.1109/ICCV.2009.5459295
   Li Xirong., 2008, Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval, MIR '08, P180
   Li YN, 2010, IEEE T MULTIMEDIA, V12, P814, DOI 10.1109/TMM.2010.2066960
   Liu J., 2009, PROC INT C UNCERTAIN, P1
   Liu J., 2013, SLEP: Sparse Learning with Efficient Projections
   Liu Y, 2008, PACIIA: 2008 PACIFIC-ASIA WORKSHOP ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION, VOLS 1-3, PROCEEDINGS, P91, DOI 10.1145/1459359.1459372
   Naphade MR, 2002, IEEE T CIRC SYST VID, V12, P40
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Peng J, 2010, ANN APPL STAT, V4, P53, DOI 10.1214/09-AOAS271
   Qi GJ, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1404880.1404883
   Siersdorfer S, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P395, DOI 10.1145/1571941.1572010
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Ulges A, 2010, STUD COMPUT INTELL, V287, P203
   Wang CB, 2006, LECT NOTES COMPUT SC, V4035, P647, DOI 10.1145/1180639.1180774
   Weng M.F., 2008, PROC ACM INT C MULTI, P71, DOI DOI 10.1145/1459359.1459370
   Wu F., 2010, Proceedings of the International Conference on Multimedia (MM'10), P15, DOI DOI 10.1145/1873951.1873957
   Xie L., 2002, Pattern Recognition Letters, P767
   Yang Y, 2011, PROC CVPR IEEE, P881, DOI 10.1109/CVPR.2011.5995499
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2012, IEEE T IMAGE PROCESS, V21, P1339, DOI 10.1109/TIP.2011.2169269
   Yang Y, 2010, IEEE T IMAGE PROCESS, V19, P2761, DOI 10.1109/TIP.2010.2049235
   Yuan M., 2006, Journal of the Royal Statistical Society, Series B, V70, P53
   Zha Z.-J., 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587384
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zha ZJ, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823747
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 51
TC 48
Z9 62
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 633
EP 646
DI 10.1109/TMM.2012.2233723
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900014
DA 2024-07-18
ER

PT J
AU Nie, LQ
   Wang, M
   Gao, Y
   Zha, ZJ
   Chua, TS
AF Nie, Liqiang
   Wang, Meng
   Gao, Yue
   Zha, Zheng-Jun
   Chua, Tat-Seng
TI Beyond Text QA: Multimedia Answer Generation by Harvesting Web
   Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE CQA; medium selection; question answering; reranking
ID VIDEO; PROPAGATION
AB Community question answering (cQA) services have gained popularity over the past years. It not only allows community members to post and answer questions but also enables general users to seek information from a comprehensive set of well-answered questions. However, existing cQA forums usually provide only textual answers, which are not informative enough for many questions. In this paper, we propose a scheme that is able to enrich textual answers in cQA with appropriate media data. Our scheme consists of three components: answer medium selection, query generation for multimedia search, and multimedia data selection and presentation. This approach automatically determines which type of media information should be added for a textual answer. It then automatically collects data from the web to enrich the answer. By processing a large set of QA pairs and adding them to a pool, our approach can enable a novel multimedia question answering (MMQA) approach as users can find multimedia answers by matching their questions with those in the pool. Different from a lot of MMQA research efforts that attempt to directly answer questions with image and video data, our approach is built based on community-contributed textual answers and thus it is able to deal with more complex questions. We have conducted extensive experiments on a multi-source QA dataset. The results demonstrate the effectiveness of our approach.
C1 [Nie, Liqiang; Gao, Yue; Zha, Zheng-Jun; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Kent Ridge 117543, Singapore.
   [Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei, Peoples R China.
C3 National University of Singapore; Hefei University of Technology
RP Nie, LQ (corresponding author), Natl Univ Singapore, Sch Comp, Kent Ridge 117543, Singapore.
EM eric.meng-wang@gmail.com
RI Zha, Zheng-Jun/AAF-8667-2020; Wang, Meng/ITR-8699-2023; Zha,
   Zheng-Jun/AAE-8408-2020
OI Zha, Zheng-Jun/0000-0003-2510-8993
FU NExT Research Center; MDA, Singapore [WBS:R-252-300-001-490]; State Key
   Lab of CAD & CG, Zhejiang University [A1203]
FX Manuscript received January 05, 2012; revised May 07, 2012; accepted
   July 04, 2012. Date of publication November 27, 2012; date of current
   version January 15, 2013. This work was supported in part by the support
   of "NExT Research Center" funded by MDA, Singapore, under the research
   grant WBS:R-252-300-001-490, and the support of the Open Project Program
   of the State Key Lab of CAD & CG (Grant No. A1203), Zhejiang University.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Klara Nahrstedt.
CR Adamic L. A., 2008, P INT WORLD WID WEB
   Agichtein E., 2001, P INT WORKSH WID WEB
   Agichtein E., 2008, P INT C WEB SEARCH W
   Ahmad I., 2003, P ICIP
   Ahonen T., 2004, P EUR C COMP VIS
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI [10.1145/1899412.1899418, DOI 10.1145/1899412.1899418]
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Arguello J., 2009, P ACM INT SIGIR C
   Bendersky M., 2008, P ACM INT SIGIR C
   Cao J., 2004, P INT JOINT C DIG LI
   Chua T. -S., 2009, P ACM WORKSH LARG SC
   Cronen-Townsend S., 2002, P ACM INT SIGIR C
   Cui H, 2007, ACM T INFORM SYST, V25, DOI 10.1145/1229179.1229182
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   GHIAS A, 1995, P ACM INT C MULT
   Harper F. M., 2008, P INT C HUM FACT COM
   Harper F. M., 2009, P INT C HUM FACT COM
   Hsu W. H., 2007, P 15 ACM INT C MULT, P971
   Huang Z., 2008, P INT C EMP METH NAT
   Kacmarcik G., 2005, MULTIMODAL QUESTION
   Kobla V., 1996, P SPIE C
   Lee YS, 2009, J AM SOC INF SCI TEC, V60, P509, DOI 10.1002/asi.21002
   Li B., 2008, P ACM INT SIGIR C
   Li G., 2010, P INT C ADV MULT MOD
   Li GD, 2010, IEEE MULTIMEDIA, V17, P46, DOI 10.1109/MMUL.2010.47
   Li X., 2002, P INT C COMP LING, P1
   Liu Y., 2008, P INT C MULT EXP
   Mollá D, 2007, COMPUT LINGUIST, V33, P41, DOI 10.1162/coli.2007.33.1.41
   NATSEV A, 2005, P ACM INT C MULT
   Nie L., 2011, P ACM INT SIGIR C
   Nie LQ, 2012, ACM T INFORM SYST, V30, DOI 10.1145/2180868.2180875
   Nie Liqiang, 2012, P ACM INT C MULT
   Quarteroni S, 2009, NAT LANG ENG, V15, P73, DOI 10.1017/S1351324908004919
   SiegelCastellan S., 1988, NONPARAMETRIC STAT S
   Singhai N., 2010, International Journal of Computer Applications IJCA, V4, P22, DOI DOI 10.5120/802-1139
   Song F., 1999, P ACM INT CIKM C
   Surdeanu M., 2008, P ASS COMP LING
   Tamura A., 2005, P INT JOINT C NAT LA
   TAMURA H, 1984, PATTERN RECOGN, V17, P29, DOI 10.1016/0031-3203(84)90033-5
   Tang JH, 2009, IEEE T SYST MAN CY B, V39, P409, DOI 10.1109/TSMCB.2008.2006045
   Tian X., 2008, P ACM INT C MULT
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, COMPUT VIS IMAGE UND, V113, P384, DOI 10.1016/j.cviu.2008.08.003
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wang R. C., 2008, P INT C EMP METH NAT
   Wu YC, 2008, IEEE T CIRC SYST VID, V18, P1411, DOI 10.1109/TCSVT.2008.2002831
   YAN R, 2003, P INT C IM VID RETR
   Yang H., 2003, P ACM INT SIGIR C
   Yeh T., 2008, P ACM INT C MULT
   Zha Z.-J., 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587384
   Zha Z. J., 2009, P ACM INT C MULT
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zha ZJ, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823747
   Zhang J., 2003, P INT C COMP INT MUL
   Zoltan G., 2007, QUESTIONING YAHOO AN
   [No title captured]
NR 58
TC 95
Z9 96
U1 2
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 426
EP 441
DI 10.1109/TMM.2012.2229971
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500017
DA 2024-07-18
ER

PT J
AU Wang, SY
   Lin, CK
   Tai, WK
AF Wang, Si-Yuan
   Lin, Cong-Kai
   Tai, Wen-Kai
TI Compressing 3D Trees With Rendering Efficiency Based on Differential
   Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic K-medoids clustering; modified tree edit distance; tree
   compression; tree rendering
ID COMPLEX
AB Trees or forests are indispensable and ubiquitous in virtual outdoor environments. Artists usually use high polygon counts to construct detailed 3D tree models to increase realism. However, large memory spaces are required, and considerable computation power is used for rendering. This paper proposes a compression method for 3D tree models to achieve rendering efficiency with less memory space and high visual fidelity of trees simultaneously. Trees are clustered using automatic K-medoids clustering method, and each member tree in the cluster can be reconstructed based on the representative tree with differential data. An abstract representation of an ordered rooted tree for each tree model is introduced, and the similarity between trees was measured using the modified tree edit distance. Moreover, an LOD priority for each component was associated to facilitate the LOD mechanism by considering the contribution to the visual perception after rendering. For accelerating rendering, GPU was exploited to benefit the LOD mechanism, and the geometry instancing facilitates the rendering for component instances shared among member trees. The effectiveness of compression was tested using four sample forests. As demonstrated by the experimental results, our method can save substantial memory space, retain the visual fidelity of the reconstructed trees, and accelerate the rendering.
C1 [Wang, Si-Yuan; Lin, Cong-Kai; Tai, Wen-Kai] Natl Dong Hwa Univ, Dept Comp Sci & Informat Engn, Shoufeng 974, Hualien, Taiwan.
C3 National Dong Hwa University
RP Wang, SY (corresponding author), Natl Dong Hwa Univ, Dept Comp Sci & Informat Engn, Shoufeng 974, Hualien, Taiwan.
EM wktai@mail.ndhu.edu.tw
RI WANG, SIYUAN/ISS-6364-2023
FU National Science Council, Taiwan (R.O.C.) [NSC 100-2221-E-259-031]
FX Manuscript received February 02, 2012; revised May 23, 2012; accepted
   July 15, 2012. Date of publication November 30, 2012; date of current
   version January 15, 2013. This work was supported in part by the
   National Science Council, Taiwan (R.O.C.) under grant NSC
   100-2221-E-259-031. The associate editor coordinating the review of this
   manuscript and approving it for publication was Weisi Lin.
CR [Anonymous], 2005, Wiley series in probability and statistics
   Biasotti S, 2008, PATTERN RECOGN, V41, P2855, DOI 10.1016/j.patcog.2008.02.003
   Cai S., 2009, PROC INT CONF COMPUT, P1
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cook RL, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239530
   Décoret X, 2003, ACM T GRAPHIC, V22, P689, DOI 10.1145/882262.882326
   Deng QQ, 2010, COMPUT ANIMAT VIRT W, V21, P1, DOI 10.1002/cav.283
   Dengsheng Zhang, 2002, Proceedings of the Fifth Asian Conference on Computer Vision, P652
   Deussen O, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P219, DOI 10.1109/VISUAL.2002.1183778
   Duda R., 1973, Pattern Classification and Scene Analysis
   Fuhrmann A.L., 2005, Objavljeno v Proceedings of the First Eurographics Conference on Natural Phenomena, NPH'05, strani, P57
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Gilet G., 2005, Euro- graphics Workshop on Natural Phenomena, P67
   Gumbau J, 2011, COMPUT GRAPH-UK, V35, P364, DOI 10.1016/j.cag.2010.11.014
   Hanan J., 1992, THESIS
   KLEIN J., 2002, Proceedings of ACM Symposium on Virtual Reality Software and Technology, P137
   Lacewell J. D., 2006, Journal of Graphics Tools, V11, P1
   Lamdan Y., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P238, DOI 10.1109/CCV.1988.589995
   Lluch J, 2010, PROCEDIA COMPUT SCI, V1, P485, DOI 10.1016/j.procs.2010.04.052
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luna F., 2003, INTRODUCTION TO 3D G
   Mantler S., 2007, PROC SYMP INTERACTIV
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Oliapuram N. J., 2010, Proceedings of the Seventh Indian Conference on Computer Vision, Graphics and Image Processing, V10, P197
   Pharr M., 2005, GPU Gems 2: Programming techniques for highperformance graphics and general purpose computation
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   Remolar I, 2004, LECT NOTES COMPUT SC, V3039, P173
   Salvi Marco., 2011, PROC ACM SIGGRAPH S, P119, DOI [10.1145/2018323.2018342, DOI 10.1145/2018323.2018342]
   Schaufler G, 1996, COMPUT GRAPH FORUM, V15, pC227, DOI 10.1111/1467-8659.1530227
   Sintorn E, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024187
   Stamminger M, 2001, SPRING EUROGRAP, P151
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   Tal A, 2007, COMM COM INF SC, V4, P44
   vanRyper W., 1996, ENCYCLOPEDIA OF GRAP
   WALLACE TP, 1980, COMPUT VISION GRAPH, V13, P99, DOI 10.1016/S0146-664X(80)80035-9
   Weber J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P119, DOI 10.1145/218380.218427
   Wilson A, 2003, ACM T GRAPHIC, V22, P678, DOI 10.1145/882262.882325
   Yang M., 2010, 17 ACM S VIRTUAL REA, P83
   ZHANG KZ, 1989, SIAM J COMPUT, V18, P1245, DOI 10.1137/0218082
   Zhang X., 2006, P ACM SIGGRAPH INT C, P331
NR 41
TC 4
Z9 4
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 304
EP 315
DI 10.1109/TMM.2012.2231062
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500007
DA 2024-07-18
ER

PT J
AU Wang, XY
   Kankanhalli, MS
AF Wang, Xiangyu
   Kankanhalli, Mohan S.
TI Multimedia Fusion With Mean-Covariance Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sensor fusion; portfolio theory; data analysis
ID AGGREGATION
AB The number of multimedia applications has been increasing over the past two decades. Multimedia information fusion has therefore attracted significant attention with many techniques having been proposed. However, the uncertainty and correlation among different information sources have not been fully considered in the existing fusion methods. In general, the predictions of individual information source have uncertainty. Furthermore, many information sources in the multimedia systems are correlated with each other. In this paper, we propose a novel multimedia fusion method based on the portfolio theory. Portfolio theory is a widely used financial investment theory dealing with how to allocate funds across securities. The key idea is to maximize the performance of the allocated portfolio while minimize the risk in returns. We adapt this approach to multimedia fusion to derive optimal weights that can achieve good fusion results. The optimization is formulated as a quadratic programming problem. Experimental results with both simulation and real data confirm the theoretical insights and show promising results.
C1 [Wang, Xiangyu; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
   [Kankanhalli, Mohan S.] Natl Univ Singapore, Dept Comp Sci, Singapore 117548, Singapore.
C3 National University of Singapore; National University of Singapore
RP Wang, XY (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
CR Aly R., 2009, P ACM INT C MULT, P233
   [Anonymous], 1994, Data fusion and sensor management: a decentralized information-theoretic approach
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Benmokhtar R., 2008, ACM INT C MULT INF R, P336
   Breiman L., 1996, 460 U CAL BERK
   Bunea F, 2007, ANN STAT, V35, P1674, DOI 10.1214/009053606000001587
   CERNY A, 2003, MATH TECHNIQUES FINA
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Dasarathy B.V., 1994, DECISION FUSION
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Heskes T, 1998, ADV NEUR IN, V10, P266
   Juditsky A, 2000, ANN STAT, V28, P681
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Li M, 2009, LECT NOTES COMPUT SC, V5371, P208
   Markowitz H, 1952, J FINANC, V7, P77, DOI 10.1111/j.1540-6261.1952.tb01525.x
   Mauclair Julie, 2004, 2004 12th European Signal Processing Conference (EUSIPCO), P1285
   Ngo C.-W., 2007, P NIST TRECVID WORKS
   Platt JC, 2000, ADV NEUR IN, P61
   Poh N, 2005, IEEE T SIGNAL PROCES, V53, P4384, DOI 10.1109/TSP.2005.857006
   Punska O., 1999, THESIS U CAMBRIDGE C
   STONE M, 1961, ANN MATH STAT, V32, P1339, DOI 10.1214/aoms/1177704873
   Tax DMJ, 2000, PATTERN RECOGN, V33, P1475, DOI 10.1016/S0031-3203(99)00138-7
   Wang M., 2009, Tech. Rep. MSR-TR-2009-30
   Wang X., 2010, P ACM INT C MULT, P723
   Wu Y., 2004, ACM INT C MULTIMEDIA, P572, DOI DOI 10.1145/1027527.1027665
   Yanagawa A., 2007, 22220068 COL U
NR 26
TC 3
Z9 3
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 120
EP 128
DI 10.1109/TMM.2012.2225027
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600010
DA 2024-07-18
ER

PT J
AU Lee, CT
   Yang, YH
   Chen, HH
AF Lee, Cheng-Te
   Yang, Yi-Hsuan
   Chen, Homer H.
TI Multipitch Estimation of Piano Music by Exemplar-Based Sparse
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 11-15, 2011
CL Univ Ramon Llull, La Salle, Barcelona, SPAIN
SP IEEE, IEEE Signal Proc Soc, IEEE Circuits & Syst Soc, IEEE Comp Soc, IEEE Commun Soc
HO Univ Ramon Llull, La Salle
DE Content retrieval; l(1)-regularized minimization; music transcription;
   pitch estimation; sparse representation
ID FUNDAMENTAL-FREQUENCY ESTIMATION; TRANSCRIPTION; RECOGNITION
AB Pitch, together with other midlevel music features such as rhythm and timbre, holds the promise of bridging the semantic gap between low-level features and high-level semantics for music understanding. This paper investigates the pitch estimation of a piano music signal by exemplar-based sparse representation. A note exemplar is a segment of a piano note, stored in the dictionary. We first describe how to represent a segment of the piano music signal as a linear combination of a small number of note exemplars from a large note exemplar dictionary and then show how the sparse representation problem can be solved by l(1)-regularized minimization. The proposed approach incorporates tuning factor estimation, note candidate selection, and hidden-Markov-model-based smoothing into the estimation process to improve accuracy. Unlike previous approaches, the proposed approach does not require retraining for a new piano. Instead, only a dozen notes of the new piano are needed. This feature is computationally attractive and avoids intense manual labeling. The system performance is evaluated using 70 classical music recordings of two real pianos under different recording conditions. The results show that the proposed system outperforms four state-of-the-art systems.
C1 [Lee, Cheng-Te] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
   [Yang, Yi-Hsuan] Acad Sinica, Res Ctr IT Innovat, Taipei 11529, Taiwan.
   [Chen, Homer H.] Natl Taiwan Univ, Dept Elect Engn, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   [Chen, Homer H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 National Taiwan University; Academia Sinica - Taiwan; National Taiwan
   University; National Taiwan University
RP Lee, CT (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
EM aderleee@gmail.com; yang@citi.sinica.edu.tw; homer@cc.ee.ntu.edu.tw
OI Chen, Homer/0000-0002-8795-1911
CR Abdallah S. A., 2003, AUD ENG SOC 114 CONV
   Abdallah S.A., 2004, P INT C MUS INF RETR, P318
   ABDALLAH SA, 2002, THESIS KINGS COLL LO
   [Anonymous], 2006, COMPUTATIONAL AUDITO
   [Anonymous], 2007, EURASIP J ADV SIG PR
   Bello JP, 2005, IEEE T SPEECH AUDI P, V13, P1035, DOI 10.1109/TSA.2005.851998
   Bello JP, 2006, IEEE T AUDIO SPEECH, V14, P2242, DOI 10.1109/TASL.2006.872609
   Blumensath T, 2006, IEEE T AUDIO SPEECH, V14, P50, DOI 10.1109/TSA.2005.860349
   BRAND M, 1996, COUPLED HIDDEN MARKO
   BREGMAN S, 1990, AUDITORY SCENE ANAL
   Candes E., 2005, IOTA 1 MAGIC RECOVER
   Candès EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Casey MA, 2008, P IEEE, V96, P668, DOI 10.1109/JPROC.2008.916370
   Chu S, 2009, IEEE T AUDIO SPEECH, V17, P1142, DOI 10.1109/TASL.2009.2017438
   Cont A., 2006, P INT C MUSIC INFORM, P206
   de Cheveigné A, 2002, J ACOUST SOC AM, V111, P1917, DOI 10.1121/1.1458024
   Dessein A., 2010, 11 INT SOC MUS INF R, P489
   Dodge C., 1997, COMPUTER MUSIC SYNTH, P80
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Dressler K., 2007, P 8 INT C MUS INF RE, P357
   Duan ZY, 2008, IEEE T AUDIO SPEECH, V16, P766, DOI 10.1109/TASL.2008.919073
   Duan ZY, 2010, IEEE T AUDIO SPEECH, V18, P2121, DOI 10.1109/TASL.2010.2042119
   EMIYA V, 2008, THESIS TELECOM PARIS
   Emiya V, 2010, IEEE T AUDIO SPEECH, V18, P1643, DOI 10.1109/TASL.2009.2038819
   Févotte C, 2009, NEURAL COMPUT, V21, P793, DOI 10.1162/neco.2008.04-08-771
   Gemmeke JF, 2011, IEEE T AUDIO SPEECH, V19, P2067, DOI 10.1109/TASL.2011.2112350
   IBM Corporation and Microsoft Corporation, 1991, MULT PROGR INT DAT S
   Kim SJ, 2007, IEEE J-STSP, V1, P606, DOI 10.1109/JSTSP.2007.910971
   Klapuri A, 2006, SIGNAL PROCESSING ME
   Klapuri A., 2006, Proc. ISMIR, P216
   Klapuri AP, 2003, IEEE T SPEECH AUDI P, V11, P804, DOI 10.1109/TSA.2003.815516
   Lee C.-T., 2010, MULTIPLE FUNDAMENTAL
   Lee CT, 2011, IEEE INT CON MULTI
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Marolt M, 2004, IEEE T MULTIMEDIA, V6, P439, DOI 10.1109/TMM.2004.827507
   *MIDI MAN ASS, 2001, COMPL MIDI 1 0 DET S
   Moorer J.A., 1977, COMPUT MUSIC J, V1, P32
   Olson HarryFerdinand., 1967, Music, Physics and Engineering
   Panagakis Y., 2009, 17 EUR SIGN PROC C G
   Peeters G., 2006, INT CONF ACOUST SPEE, P53
   Plumbley MD, 2010, P IEEE, V98, P995, DOI 10.1109/JPROC.2009.2030345
   Smaragdis P, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P177, DOI 10.1109/ASPAA.2003.1285860
   van Rijsbergen C. J, 1979, Information Retrieval, V2nd
   Vincent E, 2010, IEEE T AUDIO SPEECH, V18, P528, DOI 10.1109/TASL.2009.2034186
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yeh C., 2007, P 8 INT C MUSIC INFO, P393
   Yeh C., 2010, MULTIPLE F0 ESTIMATI
   Yeh C, 2010, IEEE T AUDIO SPEECH, V18, P1116, DOI 10.1109/TASL.2009.2030006
NR 49
TC 29
Z9 31
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 608
EP 618
DI 10.1109/TMM.2012.2191398
PN 1
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 943ZG
UT WOS:000304166300012
DA 2024-07-18
ER

PT J
AU Fang, YM
   Lin, WS
   Lee, BS
   Lau, CT
   Chen, ZZ
   Lin, CW
AF Fang, Yuming
   Lin, Weisi
   Lee, Bu-Sung
   Lau, Chiew-Tong
   Chen, Zhenzhong
   Lin, Chia-Wen
TI Bottom-Up Saliency Detection Model Based on Human Visual Sensitivity and
   Amplitude Spectrum
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Amplitude spectrum; Fourier transform; human visual sensitivity;
   saliency detection; visual attention
ID GUIDED SEARCH; TOP-DOWN; ATTENTION; COLOR; IMAGE; GUIDANCE; SCENE
AB With the wide applications of saliency information in visual signal processing, many saliency detection methods have been proposed. However, some key characteristics of the human visual system (HVS) are still neglected in building these saliency detection models. In this paper, we propose a new saliency detection model based on the human visual sensitivity and the amplitude spectrum of quaternion Fourier transform (QFT). We use the amplitude spectrum of QFT to represent the color, intensity, and orientation distributions for image patches. The saliency value for each image patch is calculated by not only the differences between the QFT amplitude spectrum of this patch and other patches in the whole image, but also the visual impacts for these differences determined by the human visual sensitivity. The experiment results show that the proposed saliency detection model outperforms the state-of-the-art detection models. In addition, we apply our proposed model in the application of image retargeting and achieve better performance over the conventional algorithms.
C1 [Fang, Yuming; Lin, Weisi; Lee, Bu-Sung; Lau, Chiew-Tong] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Chen, Zhenzhong] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
C3 Nanyang Technological University; Nanyang Technological University;
   National Tsing Hua University
RP Fang, YM (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM fa0001ng@ntu.edu.sg; wslin@ntu.edu.sg; ebslee@ntu.edu.sg;
   asctlau@ntu.edu.sg; zzchen@ntu.edu.sg; cwlin@ee.nthu.edu.tw
RI 陈, 震中/C-6857-2014; Lin, Chia-Wen/ABH-6075-2020; Lin, Weisi/A-8011-2012;
   Lee, Francis BS/G-9323-2014; Lin, Weisi/A-3696-2011; Chen,
   Zhenzhong/C-2529-2015; Lin, Chia-Wen/M-4571-2013
OI Lee, Francis BS/0000-0001-7828-7900; Lin, Weisi/0000-0001-9866-1947;
   Lin, Chia-Wen/0000-0002-9097-2318
FU MoE AcRF, Singapore [T208B1218]
FX This work was supported in part by MoE AcRF Tire 2, Singapore, Grant
   Number: T208B1218. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Maja Pantic.
CR [Anonymous], 2007, P IEEE INT C COMP VI
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   BRAUN J, 1990, PERCEPT PSYCHOPHYS, V48, P45, DOI 10.3758/BF03205010
   Bruce N.D., 2006, Adv. Neural Inf. Process. Syst., V18, P5
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Chen ZZ, 2010, IEEE T CIRC SYST VID, V20, P806, DOI 10.1109/TCSVT.2010.2045912
   Ell TA, 2007, IEEE T IMAGE PROCESS, V16, P22, DOI 10.1109/TIP.2006.884955
   Engel S, 1997, NATURE, V388, P68, DOI 10.1038/40398
   Fang Y., 2011, P INT C MULT MOD
   FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379
   Gao D., 2007, P IEEE INT C COMP VI
   Geisler WS, 1998, P SOC PHOTO-OPT INS, V3299, P294, DOI 10.1117/12.320120
   Gonzalez RC., Digital image processing third edition Pearson international edition prepared by Pearson Education
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   GREENSPAN H, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P222, DOI 10.1109/CVPR.1994.323833
   Guo C., 2008, P IEEE INT C COMP VI
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Itti L., 2000, THESIS CALTECH PASAD
   James W., 2007, The principles of psychology, DOI DOI 10.1037/10538-000
   Just Marcel., 1987, The Psychology of Reading and Language Comprehension
   Kanan C, 2009, VIS COGN, V17, P979, DOI 10.1080/13506280902771138
   Liu H., 2007, P ACM INT C MULT
   Liu Hao., 2003, P ACM INT C MULTIMED, P148
   Lu ZK, 2005, IEEE T IMAGE PROCESS, V14, P1928, DOI 10.1109/TIP.2005.854478
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   OPPENHEIM AV, 1981, P IEEE, V69, P529, DOI 10.1109/PROC.1981.12022
   Pashler H., 1997, PSYCHOL ATTENTION
   PASHLER H, 1988, ATTENTION
   PIOTROWSKI LN, 1982, PERCEPTION, V11, P337, DOI 10.1068/p110337
   RECOMMENDATION ITU-R BT, 2002, METH SUBJ ASS QUAL T
   Ren TW, 2009, IEEE INT CON MULTI, P406, DOI 10.1109/ICME.2009.5202520
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Shen HY, 2007, ADV INTEL SYS RES, DOI 10.2991/iske.2007.221
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Valenti R., 2009, P IEEE INT C COMP VI
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wandell B. A, 1995, Foundations of vision
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   WOLFE JM, 1989, J EXP PSYCHOL HUMAN, V15, P419, DOI 10.1037/0096-1523.15.3.419
   Wolfe JM, 2003, J EXP PSYCHOL HUMAN, V29, P483, DOI 10.1037/0096-1523.29.2.483
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
NR 49
TC 127
Z9 141
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 187
EP 198
DI 10.1109/TMM.2011.2169775
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100018
DA 2024-07-18
ER

PT J
AU Benini, S
   Canini, L
   Leonardi, R
AF Benini, Sergio
   Canini, Luca
   Leonardi, Riccardo
TI A Connotative Space for Supporting Movie Affective Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective meaning; connotation; famous movie scenes; movie
   recommendation; video analysis
ID EMOTIONS; MODEL
AB The problem of relating media content to users' affective responses is here addressed. Previous work suggests that a direct mapping of audio-visual properties into emotion categories elicited by films is rather difficult, due to the high variability of individual reactions. To reduce the gap between the objective level of video features and the subjective sphere of emotions, we propose to shift the representation towards the connotative properties of movies, in a space inter-subjectively shared among users. Consequently, the connotative space allows to define, relate, and compare affective descriptions of film videos on equal footing. An extensive test involving a significant number of users watching famous movie scenes suggests that the connotative space can be related to affective categories of a single user. We apply this finding to reach high performance in meeting user's emotional preferences.
C1 [Benini, Sergio; Canini, Luca; Leonardi, Riccardo] Univ Brescia, Dept Informat Engn, I-25123 Brescia, Italy.
C3 University of Brescia
RP Benini, S (corresponding author), Univ Brescia, Dept Informat Engn, I-25123 Brescia, Italy.
EM sergio.benini@ing.unibs.it; luca.canini@ing.unibs.it;
   riccardo.leonardi@ing.unibs.it
RI Leonardi, Riccardo/F-5666-2010
OI Leonardi, Riccardo/0000-0003-0755-1924
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   [Anonymous], WHAT IS GREAT FILM S
   [Anonymous], USER TEST
   [Anonymous], 1991, Grammar of the film language
   Benini S., 2005, P WIAMIS 05 MONTR SW
   Berens P, 2009, J STAT SOFTW, V31, P1, DOI 10.18637/jss.v031.i10
   Castelli CT, 2000, P SOC PHOTO-OPT INS, V3964, P224
   Cotsaces C, 2006, IEEE SIGNAL PROC MAG, V23, P28, DOI 10.1109/MSP.2006.1621446
   Dahlgaard J. J., 2008, TQM J SPECIAL EDITIO, V20
   de Kok I., 2006, P 4 20 STUD C IT ENS
   Fagin R, 2003, SIAM PROC S, P28
   Fontaine JRJ, 2007, PSYCHOL SCI, V18, P1050, DOI 10.1111/j.1467-9280.2007.02024.x
   Gray E.K., 2007, HDB EMOTION ELICITAT
   Greenwald M. K., 1989, Journal of Psychophysiology, V3, P51
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   Heise DR, 2007, EXPRESSIVE ORDER: CONFIRMING SENTIMENTS IN SOCIAL ACTIONS, P3
   Irie G, 2010, IEEE T MULTIMEDIA, V12, P523, DOI 10.1109/TMM.2010.2051871
   Kang H.-B., 2003, P ACM INT C MULT BER
   Kendall M., 1990, Correlation methods
   Kierkels JJM, 2009, IEEE INT CON MULTI, P1436, DOI 10.1109/ICME.2009.5202772
   Mehrabian A, 1996, CURR PSYCHOL, V14, P261, DOI 10.1007/BF02686918
   Osgood C. E., 1957, The measurement of meaning
   Pazzani M. J., 2007, The Adaptive Web. Methods and Strategies of Web Personalization, P325
   Picard RW, 2010, IEEE T AFFECT COMPUT, V1, P11, DOI 10.1109/T-AFFC.2010.10
   Plutchik R, 2001, AM SCI, V89, P344, DOI 10.1511/2001.4.344
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   RUSSELL JA, 1977, J RES PERS, V11, P273, DOI 10.1016/0092-6566(77)90037-X
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Scherer K., 2005, SOCIAL SCI INF   JAN
   SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420
   Smith GregM., 2003, FILM STRUCTURE EMOTI
   Soleymani M., 2008, P 2 ACM WORKSHOP MUL, P32, DOI DOI 10.1145/1460676.1460684
   STEVENS SS, 1946, SCIENCE, V103, P677, DOI 10.1126/science.103.2684.677
   Sun K, 2007, LECT NOTES COMPUT SC, V4738, P594
   Sundaram H, 2002, IEEE T MULTIMEDIA, V4, P482, DOI 10.1109/TMM.2002.802017
   TAN SH, 1995, POETICS, V23, P7
   Tkalcic M, 2010, USER MODEL USER-ADAP, V20, P279, DOI 10.1007/s11257-010-9079-z
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Wang J., 2006, P INT C MULT EXP ICM
   Weaver R.M., 1974, A rhetoric and composition handbook
   Xu M., 2005, P INT C MULT EXP ICM
   Xu M., 2008, Proceeding of the 16th ACM International Conference on Multimedia, P677, DOI DOI 10.1145/1459359.1459457
   Yang YH, 2009, INT CONF ACOUST SPEE, P1657, DOI 10.1109/ICASSP.2009.4959919
NR 45
TC 45
Z9 46
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1356
EP 1370
DI 10.1109/TMM.2011.2163058
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400015
OA Green Published
DA 2024-07-18
ER

PT J
AU Wu, PH
   Hu, YH
AF Wu, Po-Han
   Hu, Yu Hen
TI Optimal Layered Video IPTV Multicast Streaming Over Mobile WiMAX Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE IPTV; multicast; SVC; utility-based; WiMAX
ID BROADCAST
AB A WiMAX radio resource allocation (WRA) problem is investigated in the context of IPTV broadcasting over mobile WiMAX multicast, broadcast services (MBS) channels. The goal is to maximize quality of services, in terms of number of subscribers served, number of IPTV channels carried, and perceived video qualities of individual viewers, subject to constraints on multicast channel capacities and space-time channel quality variations. We present an efficient heuristic algorithm based on the Pareto principle that achieves near-optimal results in polynomial time complexity. Extensive simulations are conducted to compare this new approach against state-of-the-art heuristic algorithms and consistent superior performance has been observed.
C1 [Wu, Po-Han] Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA.
   [Hu, Yu Hen] Univ Wisconsin, Dept Elect & Comp Engn, Madison, WI 53706 USA.
C3 University of Washington; University of Washington Seattle; University
   of Wisconsin System; University of Wisconsin Madison
RP Wu, PH (corresponding author), Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA.
EM phw2@u.washington.edu; hu@engr.wisc.edu
RI HU, YU HEN/Y-3377-2019
OI HU, YU HEN/0000-0003-3427-0677
CR [Anonymous], 2004, IEEE Standard for Local and Metropolitan Area Networks Part 16: Air Interface for Fixed Broadband Wireless Access Systems, P1
   [Anonymous], 2007, Fundamentals of WiMAX: understanding broadband wireless networking
   Deb S, 2008, IEEE INFOCOM SER, P1795
   Huang C.-W., 2009, P IEEE WCNC
   Huang S.-M., 2010, P IEEE VTC2010 SPRIN
   Hwang I.-S., 2011, ISRN COMMUN NETW
   Hwang JN, 2009, MULTIMEDIA NETWORKING: FROM THEORY TO PRACTICE, P1, DOI 10.1017/CBO9780511626654
   Jiang T, 2007, IEEE COMMUN MAG, V45, P78, DOI 10.1109/MCOM.2007.4290318
   Kangasharju J, 2002, IEEE T COMPUT, V51, P622, DOI 10.1109/TC.2002.1009148
   Kelly FP, 1998, J OPER RES SOC, V49, P237, DOI 10.2307/3010473
   Kuo WH, 2007, IEEE ICC, P1754, DOI 10.1109/ICC.2007.293
   Liu JC, 2004, IEEE T MULTIMEDIA, V6, P87, DOI 10.1109/TMM.2003.819753
   Schierl T, 2007, IEEE T CIRC SYST VID, V17, P1204, DOI 10.1109/TCSVT.2007.905528
   She J, 2007, IEEE COMMUN MAG, V45, P87, DOI 10.1109/MCOM.2007.4290319
   So-In C., 2010, J COMPU IN PRESS APR
   Tsitserov D., 2008, P ANN POSTGR S LIV U
   Wang JF, 2007, IEEE J SEL AREA COMM, V25, P712, DOI 10.1109/JSAC.2007.070508
   WiMAX Forum, 2008, WIMAX SYST EV METH V
   Wu PH, 2010, VEH TECHNOL CONFE
   Yang Y.-R., 2000, P IEEE ICNP NOV
NR 20
TC 8
Z9 11
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1395
EP 1403
DI 10.1109/TMM.2011.2168196
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400018
DA 2024-07-18
ER

PT J
AU Gao, Y
   Wang, M
   Zha, ZJ
   Tian, Q
   Dai, QH
   Zhang, NY
AF Gao, Yue
   Wang, Meng
   Zha, Zheng-Jun
   Tian, Qi
   Dai, Qionghai
   Zhang, Naiyao
TI Less is More: Efficient 3-D Object Retrieval With Query View Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Intelligent query; multi-view; representative query view; 3-D object
   retrieval
ID 3D MODEL; SIMILARITY; SEARCH
AB The explosively increasing 3-D objects make their efficient retrieval technology highly desired. Extensive research efforts have been dedicated to view-based 3-D object retrieval for its advantage of using 2-D views to represent 3-D objects. In this paradigm, typically the retrieval is accomplished by matching the views of the query object with the objects in database. However, using all the query views may not only introduce difficulty in rapid retrieval but also degrade retrieval accuracy when there is a mismatch between the query views and the object views in the database. In this work, we propose an interactive 3-D object retrieval scheme. Given a set of query views, we first perform clustering to obtain several candidates. We then incrementally select query views for object matching: in each round of relevance feedback, we only add the query view that is judged to be the most informative one based on the labeling information. In addition, we also propose an efficient approach to learn a distance metric for the newly selected query view and the weights for combining all of the selected query views. We conduct experiments on the National Taiwan University 3D Model database, ETH 3D object collection, and Shape Retrieval Content of Non-Rigid 3D Model, and results demonstrated that our approach not only significantly speeds up the retrieval process but also achieves encouraging retrieval performance.
C1 [Gao, Yue; Dai, Qionghai; Zhang, Naiyao] Tsinghua Univ, Dept Automat, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Wang, Meng] Hefei Univ Technol, Sch Comp & Sci, Hefei 230009, Peoples R China.
   [Zha, Zheng-Jun] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78285 USA.
C3 Tsinghua University; Hefei University of Technology; National University
   of Singapore; University of Texas System; University of Texas at San
   Antonio (UTSA)
RP Gao, Y (corresponding author), Tsinghua Univ, Dept Automat, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM eric.mengwang@gmail.com
RI Gao, Yue/B-3376-2012; Wang, Meng/ITR-8699-2023; Dai,
   Qionghai/ABD-5298-2021; Zha, Zheng-Jun/AAE-8408-2020; Zha,
   Zheng-Jun/AAF-8667-2020
OI Dai, Qionghai/0000-0001-7043-3061; Zha, Zheng-Jun/0000-0003-2510-8993; 
FU National Basic Research Project [2010CB731800]; NSFC [61035002,
   60872056, U0935001]
FX Manuscript received January 16, 2011; revised June 14, 2011; accepted
   June 18, 2011. Date of publication June 27, 2011; date of current
   version September 16, 2011. The work was supported in part by the
   National Basic Research Project (No.2010CB731800) and in part by the
   Project of NSFC (No. 61035002, 60872056, and U0935001). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Ajay Divakaran.
CR AKBAR S., 2006, P INT C INF INT WEB, P77
   Akgül CB, 2010, INT J COMPUT VISION, V89, P392, DOI 10.1007/s11263-009-0294-1
   [Anonymous], P SAMT WORKSH SEM 3
   Ansary TF, 2007, IEEE T MULTIMEDIA, V9, P78, DOI 10.1109/TMM.2006.886359
   Assfalg J, 2007, IEEE T MULTIMEDIA, V9, P589, DOI 10.1109/TMM.2006.886271
   Atmosukarto I, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P334, DOI 10.1109/MMMC.2005.39
   Bustos B., 2004, Proceedings. IEEE Sixth International Symposium on Multimedia Software, P514
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Daras P, 2010, INT J COMPUT VISION, V89, P229, DOI 10.1007/s11263-009-0277-2
   Furuya T., 2008, P ACM INT C IM VID R
   Gao Y, 2011, SIGNAL PROCESS-IMAGE, V26, P39, DOI 10.1016/j.image.2010.10.006
   Gao Y, 2010, NEUROCOMPUTING, V73, P1900, DOI 10.1016/j.neucom.2009.11.050
   Gao Y, 2010, PATTERN RECOGN, V43, P1142, DOI 10.1016/j.patcog.2009.07.012
   GIORGI D, 2010, P ACM WORKSH 3D OBJ
   Hu BK, 2010, PATTERN RECOGN, V43, P2950, DOI 10.1016/j.patcog.2010.02.010
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109
   Kim WY, 2000, SIGNAL PROCESS-IMAGE, V16, P95, DOI 10.1016/S0923-5965(00)00019-9
   Leibe B, 2003, PROC CVPR IEEE, P409
   Leifman G, 2005, VISUAL COMPUT, V21, P865, DOI 10.1007/s00371-005-0341-z
   Leng B, 2008, MULTIMED TOOLS APPL, V40, P135, DOI 10.1007/s11042-007-0188-6
   Leng B, 2007, J ZHEJIANG UNIV-SC A, V8, P1953, DOI 10.1631/jzus.2007.A1953
   Leng BA, 2011, MULTIMED TOOLS APPL, V51, P935, DOI 10.1007/s11042-009-0424-3
   LIAN Z, 2010, P ACM WORKSH 3D OBJ
   Mortara M, 2009, COMPUT GRAPH-UK, V33, P280, DOI 10.1016/j.cag.2009.03.003
   Nah FFH, 2004, BEHAV INFORM TECHNOL, V23, P153, DOI 10.1080/01449290410001669914
   Napoléon T, 2010, EURASIP J IMAGE VIDE, DOI 10.1155/2010/367181
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Page L., 1999, PAGERANK CITATION RA
   Papadakis P., 2008, Computer-Aided Design and Applications Journal, V5, P753
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Rui Y, 2000, PROC CVPR IEEE, P236, DOI 10.1109/CVPR.2000.855825
   Shih JL, 2007, PATTERN RECOGN, V40, P283, DOI 10.1016/j.patcog.2006.04.034
   Steinbach M., 2000, P KDD WORKSH TEXT MI
   Vranic DV, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P757
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
NR 41
TC 157
Z9 164
U1 2
U2 57
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1007
EP 1018
DI 10.1109/TMM.2011.2160619
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300014
DA 2024-07-18
ER

PT J
AU Sharangi, S
   Krishnamurti, R
   Hefeeda, M
AF Sharangi, Somsubhra
   Krishnamurti, Ramesh
   Hefeeda, Mohamed
TI Energy-Efficient Multicasting of Scalable Video Streams Over WiMAX
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Energy efficiency; mobile multimedia; scalable video coding; video
   streaming; WiMAX; wireless scheduling
ID BROADCAST
AB The Multicast/Broadcast Service (MBS) feature of mobile WiMAX network is a promising technology for providing wireless multimedia, because it allows the delivery of multimedia content to large-scale user communities in a cost-efficient manner. In this paper, we consider WiMAX networks that transmit multiple video streams encoded in scalable manner to mobile receivers using the MBS feature. We focus on two research problems in such networks: 1) maximizing the video quality and 2) minimizing energy consumption for mobile receivers. We formulate and solve the substream selection problem to maximize the video quality, which arises when multiple scalable video streams are broadcast to mobile receivers with limited resources. We show that this problem is NP-Complete, and design a polynomial time approximation algorithm to solve it. We prove that the solutions computed by our algorithm are always within a small constant factor from the optimal solutions. In addition, we extend our algorithm to reduce the energy consumption of mobile receivers. This is done by transmitting the selected substreams in bursts, which allows mobile receivers to turn off their wireless interfaces to save energy. We show how our algorithm constructs burst transmission schedules that reduce energy consumption without sacrificing the video quality. Using extensive simulation and mathematical analysis, we show that the proposed algorithm: 1) is efficient in terms of execution time, 2) achieves high radio resource utilization, 3) maximizes the received video quality, and 4) minimizes the energy consumption for mobile receivers.
C1 [Sharangi, Somsubhra; Krishnamurti, Ramesh; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
C3 Simon Fraser University
RP Sharangi, S (corresponding author), Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
EM ssa121a@cs.sfu.ca; ramesh@cs.sfu.ca; mhefeeda@cs.sfu.ca
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   British Columbia Innovation Council
FX Manuscript received May 21, 2010; revised August 24, 2010; accepted
   August 27, 2010. Date of publication September 16, 2010; date of current
   version January 19, 2011. This work was supported in part by the Natural
   Sciences and Engineering Research Council (NSERC) of Canada and in part
   by the British Columbia Innovation Council. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Thinh Nguyen.
CR [Anonymous], GNU LIN PROGR KIT
   [Anonymous], GLOBECOM WORKSH 2008
   Cohen R, 2008, IEEE T MOBILE COMPUT, V7, P346, DOI 10.1109/TMC.2007.70729
   *FRAUNH HHI, FRAUNH HHI JSVM H 26
   Garey M.R., 1979, COMPUTERS INTRACTABI
   GEERT V, 2008, ADV MULTIMEDIA   JAN, P1
   Gray D., 2006, MOBILE WIMAX 1
   HOSEIN P, 2008, P IEEE INT C COM MUN, P271
   Hsu CH, 2009, IEEE INFOCOM SER, P2231, DOI 10.1109/INFCOM.2009.5062148
   Juan HH, 2007, IEEE INT SYMP CIRC S, P3463, DOI 10.1109/ISCAS.2007.378372
   Kim MG, 2008, CONSUM COMM NETWORK, P222, DOI 10.1109/ccnc08.2007.56
   Liao Wei-keng., 2008, High Performance Computing, Networking, Storage and Analysis, P1, DOI DOI 10.1061/40988(323)169
   Lin EYH, 1998, INFOR, V36, P274
   Reguant VD, 2008, I SYMP CONSUM ELECTR, P74
   Seo JB, 2004, VTC2004-FALL: 2004 IEEE 60TH VEHICULAR TECHNOLOGY CONFERENCE, VOLS 1-7, P1169
   SHARANGI S, 2010, P IEEE WORKSH QUAL S
   SHI J, 2006, P IEEE GLOBECOM 06 S, P1
   SINHA P, 1979, OPER RES, V27, P503, DOI 10.1287/opre.27.3.503
   Wang JF, 2007, IEEE J SEL AREA COMM, V25, P712, DOI 10.1109/JSAC.2007.070508
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   ZEMEL E, 1984, INFORM PROCESS LETT, V18, P123, DOI 10.1016/0020-0190(84)90014-0
   LOCAL METROPOLITA 16
   MOBILE VIDEO SERVICE
NR 23
TC 48
Z9 52
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 102
EP 115
DI 10.1109/TMM.2010.2076799
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Uijlings, JRR
   Smeulders, AWM
   Scha, RJH
AF Uijlings, Jasper R. R.
   Smeulders, Arnold W. M.
   Scha, Remko J. H.
TI Real-Time Visual Concept Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bag-of-words; computational efficiency; evaluation; image/video
   retrieval; real-time
ID FEATURES; BAG
AB As datasets grow increasingly large in content-based image and video retrieval, computational efficiency of concept classification is important. This paper reviews techniques to accelerate concept classification, where we show the trade-off between computational efficiency and accuracy. As a basis, we use the Bag-of-Words algorithm that in the 2008 benchmarks of TRECVID and PASCAL lead to the best performance scores. We divide the evaluation in three steps: 1) Descriptor Extraction, where we evaluate SIFT, SURF, DAISY, and Semantic Textons. 2) Visual Word Assignment, where we compare a k-means visual vocabulary with a Random Forest and evaluate subsampling, dimension reduction with PCA, and division strategies of the Spatial Pyramid. 3) Classification, where we evaluate the, chi(2) RBF, and Fast Histogram Intersection kernel for the SVM. Apart from the evaluation, we accelerate the calculation of densely sampled SIFT and SURF, accelerate nearest neighbor assignment, and improve accuracy of the Histogram Intersection kernel. We conclude by discussing whether further acceleration of the Bag-of-Words pipeline is possible.
   Our results lead to a 7-fold speed increase without accuracy loss, and a 70-fold speed increase with 3% accuracy loss. The latter system does classification in real-time, which opens up new applications for automatic concept classification. For example, this system permits five standard desktop PCs to automatically tag for 20 classes all images that are currently uploaded to Flickr.
C1 [Uijlings, Jasper R. R.; Smeulders, Arnold W. M.] Univ Amsterdam, Intelligent Syst Lab Amsterdam, Inst Informat, Amsterdam, Netherlands.
   [Scha, Remko J. H.] Univ Amsterdam, Inst Log Language & Computat, Amsterdam, Netherlands.
C3 University of Amsterdam; University of Amsterdam
RP Uijlings, JRR (corresponding author), Univ Amsterdam, Intelligent Syst Lab Amsterdam, Inst Informat, Amsterdam, Netherlands.
EM jrr.uijlings@uva.nl
CR [Anonymous], 2006, P 20 ANN C NEUR INF
   [Anonymous], 2010, NVIDIA CUDA Programming Guide
   [Anonymous], P INT C MACH LEARN
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2008, IEEE C COMPUTER VISI
   [Anonymous], ACM INT C IM VID
   [Anonymous], P 6 TRECVID WORKSH G
   [Anonymous], 2008, P IEEE C COMP VIS PA
   Ballan L, 2009, LECT NOTES COMPUT SC, V5716, P170, DOI 10.1007/978-3-642-04146-4_20
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bishop Christopher M, 2006, PATTERN RECOGNITION, DOI DOI 10.1117/1.2819119
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Everingham M., 2007, The PASCAL Visual Object Classes Chal- lenge (VOC) Results, DOI 10.1007/s11263-009-0275-4
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   GEMERT J, IEEE T PATT IN PRESS
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Geusebroek JM, 2003, IEEE T IMAGE PROCESS, V12, P938, DOI 10.1109/TIP.2003.812429
   GRABNER M, 2006, P AS C COMP VIS
   He J., 2008, P IEEE CVPR, P1
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maji S, 2008, PROC CVPR IEEE, P2245
   MARSZALEK M, 2007, P ICCV PASC VOC 2007
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K., 2007, P IEEE INT C COMP VI
   Mikolajczyk K., 2005, P IEEE INT C COMP VI
   Moosmann F, 2008, IEEE T PATTERN ANAL, V30, P1632, DOI 10.1109/TPAMI.2007.70822
   Nister David, 2006, CVPR
   Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Phillips J.C., 2008, 2008 SC - International Conference for High Performance Computing, Networking, Storage and Analysis, P1, DOI DOI 10.1145/1413370.1413379
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   SHARP T, 2008, P EUR C COMP VIS
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smeaton A., 2006, P ACM SIGMM INT WORK
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   SNOEK CGM, 2006, P ACM MULT, P101
   SOCHMAN J, 2007, P AS C COMP VIS
   TAHIR M, 2008, P ECCV PASC VOC 2008
   Tuytelaars T., 2007, P IEEE INT C COMP VI
   Uijlings J. R. R., 2009, P ACM INT C IM VID R
   VANDESANDE KEA, 2008, P ACM INT C IM VID R
   WANG L, 2008, P EUR C COMP VIS
   Williamson John, 2004, The Washington Consensus as Policy Prescription for Development, P1
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
NR 46
TC 76
Z9 88
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2010
VL 12
IS 7
BP 665
EP 681
DI 10.1109/TMM.2010.2052027
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 670SV
UT WOS:000283448500005
DA 2024-07-18
ER

PT J
AU Negoescu, RA
   Gatica-Perez, D
AF Negoescu, Radu-Andrei
   Gatica-Perez, Daniel
TI Modeling Flickr Communities Through Probabilistic Topic-Based Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Flickr; probabilistic topic models; social media
ID LATENT SEMANTIC ANALYSIS
AB With the increased presence of digital imaging devices, there also came an explosion in the amount of multimedia content available online. Users have transformed from passive consumers of media into content creators and have started organizing themselves in and around online communities. Flickr has more than 30 million users and over 3 billion photos, and many of them are tagged and public. One very important aspect in Flickr is the ability of users to organize in self-managed communities called groups. This paper examines an unexplored problem, which is jointly analyzing Flickr groups and users. We show that although users and groups are conceptually different, in practice they can be represented in a similar way via a bag-of-tags derived from their photos, which is amenable for probabilistic topic modeling. We then propose a probabilistic topic model representation learned in an unsupervised manner that allows the discovery of similar users and groups beyond direct tag-based strategies, and we demonstrate that higher-level information such as topics of interest are a viable alternative. On a dataset containing users of 10 000 Flickr groups and over 1 milion photos, we show how this common topic-based representation allows for a novel analysis of the groups-users Flickr ecosystem, which results into new insights about the structure of the entities in this social media source. We demonstrate novel practical applications of our topic-based representation, such as similarity-based exploration of entities, or single and multi-topic tag-based search, which address current limitations in the ways Flickr is used today.
C1 [Negoescu, Radu-Andrei] Idiap Res Inst, Lausanne, Switzerland.
   [Negoescu, Radu-Andrei] Ecole Polytech Fed Lausanne, Swiss Fed Inst Technol, Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Negoescu, RA (corresponding author), Idiap Res Inst, Lausanne, Switzerland.
FU Swiss National Science Foundation
FX Manuscript received September 03, 2009; revised December 11, 2009;
   accepted February 22, 2010. Date of publication May 18, 2010; date of
   current version July 16, 2010. This work was supported by the Swiss
   National Science Foundation through the MULTI project. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Qibin Sun.
CR AHERN S, 2007, P 2007 C DIG LIBR JC
   [Anonymous], 2007, P SIGCHI C HUM FACT
   [Anonymous], 2000, P IEEE C COMP VIS PA
   [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], P 12 ACM SIGKDD INT
   [Anonymous], 2007, P SIGCHI C HUM FACT
   [Anonymous], 2007, CVPR
   [Anonymous], P INT C WEBL SOC MED
   [Anonymous], 2005, P INT C COMP VIS
   Berg T.L., 2007, Automatic ranking of iconic images
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   BLEI DM, 2003, ADV NEURAL INFORM PR, V16, P2004
   Davies J., 2007, DISCOURSE, V28, P549, DOI [DOI 10.1080/01596300701625305, 10.1080/01596300701625305]
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   DUBINKO M, 2006, P 15 INT C WORLD WID
   *FLICKR BLOG, 2007, HOL MOL
   *FLICKR BLOG, 2008, BILL
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   HORSTER E, 2007, P INT C IM VID RETR
   JAFFE A, 2006, P 15 INT C WORLD WID
   KAILATH T, 1967, IEEE T COMMUN TECHN, VCO15, P52, DOI 10.1109/TCOM.1967.1089532
   KENNEDY L, 2007, P 15 ACM INT C MULT
   LERMAN K, 2007, P AAAI WORKSH INT TE
   LIENHART R, 2007, P 2007 INT C AC SPEE
   MARLOW C, 2006, P 17 C HYP HYP HYPER
   MILLER AD, 2007, P SIGCHI C HUM FACT
   Monay F, 2007, IEEE T PATTERN ANAL, V29, P1802, DOI 10.1109/TPAMI.2007.1097
   NEGOESCU RA, 2008, P INT C IM VID RETR
   NEGOESCU RA, 2008, P 16 ACM INT C MULT
   NEWMAN D, 2007, ADV NEURAL INFORM PR, V20, P1081
   NOV O, 2008, P 26 SIGCHI C HUM FA
   RATTENBURY T, 2007, P 30 INT C RES DEV I
   ROSENZVI M, 2004, P 20 C UNC ART INT A
   SCHMITZ P, 2006, P 14 ACM INT C MULT
   Schmitz P., 2006, COLLABORATIVE WEB TA
   VANDERWEYDE WM, 2008, G EASTMAN HOUSE MUSE
   VANHOUSE NA, 2007, P HUM FACT COMP SYST
   VANZWOL R, 2007, P INT C WEB INT WI 0
   Wang WM, 2005, ADV ENG INFORM, V19, P5, DOI 10.1016/j.aei.2005.03.002
   WANG Y, 2009, P 5 INT C ALG ASP IN
   WU L, 2008, P 16 ACM INT C MULT
NR 41
TC 21
Z9 25
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2010
VL 12
IS 5
BP 399
EP 416
DI 10.1109/TMM.2010.2050649
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 656DN
UT WOS:000282306500005
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhao, WL
   Wu, XA
   Ngo, CW
AF Zhao, Wan-Lei
   Wu, Xiao
   Ngo, Chong-Wah
TI On the Annotation of Web Videos by Efficient Near-Duplicate Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data-driven; near-duplicate video search; video annotation; web video
ID IMAGES; SCALE
AB With the proliferation of Web 2.0 applications, usersupplied social tags are commonly available in social media as a means to bridge the semantic gap. On the other hand, the explosive expansion of social web makes an overwhelming number of web videos available, among which there exists a large number of near-duplicate videos. In this paper, we investigate techniques which allow effective annotation of web videos from a data-driven perspective. A novel classifier-free video annotation framework is proposed by first retrieving visual duplicates and then suggesting representative tags. The significance of this paper lies in the addressing of two timely issues for annotating query videos. First, we provide a novel solution for fast near-duplicate video retrieval. Second, based on the outcome of near-duplicate search, we explore the potential that the data-driven annotation could be successful when huge volume of tagged web videos is freely accessible online. Experiments on cross sources (annotating Google videos and Yahoo! videos using YouTube videos) and cross time periods (annotating YouTube videos using historical data) show the effectiveness and efficiency of the proposed classifier-free approach for web video tag annotation.
C1 [Zhao, Wan-Lei; Wu, Xiao; Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Wu, Xiao] SW Jiaotong Univ, Dept Comp Sci & Engn, Chengdu, Peoples R China.
C3 City University of Hong Kong; Southwest Jiaotong University
RP Zhao, WL (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM wzhao2@cs.cityu.edu.hk; wuxiaohk@home.swjtu.edu.cn;
   cwngo@cs.cityu.edu.hk
OI Ngo, Chong Wah/0000-0003-4182-8261; Wu, Xiao/0000-0002-8322-8558
FU City University of Hong Kong [7002438]
FX Manuscript received August 30, 2009; revised December 19, 2009 and March
   07, 2010; accepted March 27, 2010. Date of publication May 18, 2010;
   date of current version July 16, 2010. This work was fully supported by
   a grant from the City University of Hong Kong (Project No. 7002438). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Marcel Worring.
CR Ames M., 2007, P SIGCHI C HUMAN FAC, P971
   [Anonymous], P ACM MULT
   [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], 2009, Proceedings of the 18th international conference on World wide web, DOI [10.1145/1526709.1526758, DOI 10.1145/1526709.1526758]
   [Anonymous], P 16 INT C WORLD WID
   Dong W., 2008, PROCEEDING ACM MULTI, P179, DOI DOI 10.1145/1459359.1459384
   DOUZE M, 2008, P TREVCID
   Golder SA, 2006, J INF SCI, V32, P198, DOI 10.1177/0165551506062337
   Hampapur A, 2002, P SOC PHOTO-OPT INS, V4676, P194
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jiang YG, 2009, COMPUT VIS IMAGE UND, V113, P405, DOI 10.1016/j.cviu.2008.10.002
   Joly A, 2008, P 16 ACM INT C MULT, P209
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Karypis G., 2002, CLUTO
   Ke Y., 2004, ACM MULTIMEDIA 04, P869
   Law-To J., 2006, P ACM INT C MULTIMED, P835
   LI X, 2006, P 14 ANN ACM INT C M, P607
   Li XH, 2008, IEEE T SIGNAL PROCES, V56, P675, DOI 10.1109/TSP.2007.907820
   Li Xirong., 2008, Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval, MIR '08, P180
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Moxley E, 2010, IEEE T MULTIMEDIA, V12, P184, DOI 10.1109/TMM.2010.2041101
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song Y, 2008, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE 2007, VOL 6, PTS A AND B, P515, DOI 10.1145/1390334.1390423
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang CB, 2006, LECT NOTES COMPUT SC, V4035, P647, DOI 10.1145/1180639.1180774
   WANG L, 2005, P ACM C MULT, P706
   Wang XJ, 2008, IEEE T PATTERN ANAL, V30, P1919, DOI 10.1109/TPAMI.2008.127
   Wu A. G., 2007, P ACM MM, P218
   Wu X, 2009, IEEE T MULTIMEDIA, V11, P196, DOI 10.1109/TMM.2008.2009673
   Yuan JS, 2004, LECT NOTES COMPUT SC, V3332, P479
   ZHANG D.-Q., 2004, PROC ACM INT C MULTI, P877, DOI DOI 10.1145/1027527.1027730
   Zhao WL, 2009, IEEE T IMAGE PROCESS, V18, P412, DOI 10.1109/TIP.2008.2008900
NR 34
TC 88
Z9 100
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2010
VL 12
IS 5
BP 448
EP 461
DI 10.1109/TMM.2010.2050651
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 656DN
UT WOS:000282306500009
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Pham, PT
   Moens, MF
   Tuytelaars, T
AF Pham, Phi The
   Moens, Marie-Francine
   Tuytelaars, Tinne
TI Cross-Media Alignment of Names and Faces
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-media mining; image annotation
AB In this paper we report on our experiments on aligning names and faces as found in images and captions of online news websites. Developing accurate technologies for linking names and faces is valuable when retrieving or mining information from multimedia collections. We perform exhaustive and systematic experiments exploiting the (a) symmetry between the visual and textual modalities. This leads to different schemes for assigning names to the faces, assigning faces to the names, and establishing name-face link pairs. On top of that, we investigate generic approaches to the use of textual and visual structural information to predict the presence of the corresponding entity in the other modality. The proposed methods are completely unsupervised and are inspired by methods for aligning phrases and words in texts of different languages developed for constructing dictionaries for machine translation. The results are competitive with state-of-the-art performance on the "Labeled Faces in the Wild" dataset in terms of recall values, now reported on the complete dataset, include excellent precision values, and show the value of text and image analysis for identifying the probability of being pictured or named in the alignment process.
C1 [Pham, Phi The; Moens, Marie-Francine] Katholieke Univ Leuven, Dept Comp Sci, B-3001 Heverlee, Belgium.
   [Tuytelaars, Tinne] Katholieke Univ Leuven, Dept Elect Engn, B-3001 Heverlee, Belgium.
C3 KU Leuven; KU Leuven
RP Pham, PT (corresponding author), Katholieke Univ Leuven, Dept Comp Sci, B-3001 Heverlee, Belgium.
RI Moens, Marie-Francine/B-8378-2014; Tuytelaars, Tinne/B-4319-2015
OI Tuytelaars, Tinne/0000-0003-3307-9723
FU Fund for Scientific Research Flanders
FX The work of T. Tuytelaars was supported in part by the Fund for
   Scientific Research Flanders. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Nicu
   Sebe.
CR [Anonymous], 2006, Semi-Supervised Learning, DOI DOI 10.7551/MITPRESS/9780262033589.003.0003
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2008, AAAI
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P C HUM LANG TECHN E
   [Anonymous], P 1 C N AM CHAPT ASS
   Berg TL, 2004, PROC CVPR IEEE, P848
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Brown P. F., 1993, Computational Linguistics, V19, P263
   CHERRY C, 2006, P EACL 2006 COL OH, P145
   DESCHACHT K, 2007, P 45 ANN M ASS COMP, P1000
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Everingham M., 2006, BMVC, V2, P6
   Forsyth, 2005, ADV NEURAL INFORM PR, V17, P137
   GHOSHAL A, 2005, P 28 ANN INT ACM SIG, P544
   GUILLAUMIN M, 2008, P EUR C COMP VIS
   JAIN V, 2007, P INT C COMP VIS
   Jeon J, 2004, LECT NOTES COMPUT SC, V3115, P24
   Jeon J., 2003, Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P119, DOI DOI 10.1145/860435.860459
   Lacoste-Julien S., 2006, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, P112
   Moens Marie-Francine, 2008, Natural Language Engineering, V14, P145, DOI 10.1017/S135132490600430X
   MORI Y, 2000, P CONT BAS MULT INF
   Och FJ, 2003, COMPUT LINGUIST, V29, pc, DOI 10.1162/089120103321337421
   OZKAN D, 2006, P IEEE C COMP VIS PA
   PARK U, 2007, P CVPR
   Strehl A., 2001, J MACH LEARN RES, P583
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Yang J, 2005, Proceedings of the 2005 International Conference on Active Media Technology (AMT 2005), P28
   Yang J, 2004, LECT NOTES COMPUT SC, V3115, P270
   Zhang N., 2007, Tech. Rep. 07-49, P7
NR 30
TC 26
Z9 32
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2010
VL 12
IS 1
BP 13
EP 27
DI 10.1109/TMM.2009.2036232
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 533SJ
UT WOS:000272844800002
OA Green Published
DA 2024-07-18
ER

PT J
AU Vukadinovic, V
   Karlsson, G
AF Vukadinovic, Vladimir
   Karlsson, Gunnar
TI Trade-Offs in Bit-Rate Allocation for Wireless Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 8th ACM/IEEE International Symposium on Modeling, Analysis and
   Simulation of Wireless and Mobile Systems
CY OCT, 2005
CL Montreal, CANADA
SP ACM, IEEE
DE Bit-rate allocation; channel coding; channel estimation; HSDPA;
   perceptual distortion; video streaming
ID RESOURCE-ALLOCATION; TRANSMISSION; CHANNEL; CODES; MODEL
AB One of the central problems in wireless video transmission is the choice of source and channel coding rates to allocate the available transmission rate optimally. In this paper, we present a structural distortion model for video streaming over time-varying fading channels. Based on the model, we study the end-to-end distortion for various bit-rate allocation strategies and channel conditions. We show that the robustness to channel variations is crucial for the streaming performance when frequent bit-rate adaptations are not feasible. It is achieved at the expense of higher source distortion in the encoder. Our findings are illustrated on a practical problem of distortion-optimal selection of transport formats in an adaptive modulation and coding (AMC) scheme used in HSDPA.
C1 [Vukadinovic, Vladimir; Karlsson, Gunnar] Royal Inst Technol, Sch Elect Engn, Stockholm, Sweden.
C3 Royal Institute of Technology
RP Vukadinovic, V (corresponding author), Royal Inst Technol, Sch Elect Engn, Stockholm, Sweden.
EM vvuk@ee.kth.se; gk@ee.kth.se
OI Karlsson, Gunnar/0000-0002-3704-1338
CR *3GPP TS, 2008, 25214 3GPP TS
   Bai F, 2003, IEEE INFOCOM SER, P825
   Barg A, 2002, IEEE T INFORM THEORY, V48, P2568, DOI 10.1109/TIT.2002.800480
   Bouazizi I, 2004, ISCCSP : 2004 FIRST INTERNATIONAL SYMPOSIUM ON CONTROL, COMMUNICATIONS AND SIGNAL PROCESSING, P91
   BROUWER F, 2004, P IEEE ISSSTA 04 SID, P844
   Bystrom M, 2000, IEEE J SEL AREA COMM, V18, P880, DOI 10.1109/49.848242
   Bystrom M, 2004, IEEE T WIREL COMMUN, V3, P258, DOI 10.1109/TWC.2003.821150
   Cheung G, 2000, IEEE T IMAGE PROCESS, V9, P340, DOI 10.1109/83.826773
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   Dai M, 2004, IEEE IMAGE PROC, P1093
   DAI M, 2003, P 13 INT WORKSH NETW, P60
   Ding W, 1996, IEEE T CIRC SYST VID, V6, P12, DOI 10.1109/76.486416
   Dong M, 2004, IEEE T SIGNAL PROCES, V52, P1403, DOI 10.1109/TSP.2004.826182
   Eisenberg Y, 2006, IEEE T IMAGE PROCESS, V15, P289, DOI 10.1109/TIP.2005.860600
   *ETSI TR UMTS, 1998, 3003 ETSI TR UMTS
   Frossard P, 2001, IEEE T IMAGE PROCESS, V10, P1815, DOI 10.1109/83.974566
   GALLAGER RG, 1973, IEEE T INFORM THEORY, V19, P244, DOI 10.1109/TIT.1973.1054971
   Gnavi S, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P517
   GUDMUNDSON M, 1991, ELECTRON LETT, V27, P2145, DOI 10.1049/el:19911328
   Hochwald B, 1997, IEEE T INFORM THEORY, V43, P1412, DOI 10.1109/18.623141
   Katsaggelos AK, 2005, P IEEE, V93, P135, DOI 10.1109/JPROC.2004.839621
   Liang YJ, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P684
   Lin LJ, 1998, IEEE T CIRC SYST VID, V8, P446, DOI 10.1109/76.709411
   Maani E, 2008, IEEE T IMAGE PROCESS, V17, P1663, DOI 10.1109/TIP.2008.2001402
   Médard M, 2000, IEEE T INFORM THEORY, V46, P933, DOI 10.1109/18.841172
   Stankovic V, 2003, IEEE IMAGE PROC, P81
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Wei J, 2004, IEEE SIGNAL PROC LET, V11, P694, DOI 10.1109/LSP.2004.831671
   Weitzen J, 2002, IEEE T VEH TECHNOL, V51, P265, DOI 10.1109/25.994804
   Wu H., 2005, Proceedings of the 15th International Workshop on Network and Operating Systems Support for Digital Audio and Video. NOSSDAV 2005, P111, DOI 10.1145/1065983.1066010
   Xia MH, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 3, PROCEEDINGS, P945
   Zhang R, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P861, DOI 10.1109/ICME.2002.1035918
   Zhang R, 2001, CONF REC ASILOMAR C, P210, DOI 10.1109/ACSSC.2001.986907
NR 35
TC 7
Z9 7
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1105
EP 1113
DI 10.1109/TMM.2009.2026096
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700007
DA 2024-07-18
ER

PT J
AU Ademoye, OA
   Ghinea, G
AF Ademoye, Oluwakemi A.
   Ghinea, Gheorghita
TI Synchronization of Olfaction-Enhanced Multimedia
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
AB This paper presents the results of an experimental study carried out to explore, from an end user perspective, the temporal boundaries within which olfactory data can be used to enhance multimedia applications. Results show the presence of two main synchronization regions, and that olfaction ahead of audiovisual content is more tolerable than olfaction behind content.
C1 [Ademoye, Oluwakemi A.; Ghinea, Gheorghita] Brunel Univ, Uxbridge UB8 3PH, Middx, England.
C3 Brunel University
RP Ademoye, OA (corresponding author), Brunel Univ, Uxbridge UB8 3PH, Middx, England.
RI Ghinea, Gheorghita/AAG-6770-2020
OI Ghinea, Gheorghita/0000-0003-2578-5580; Ademoye,
   Kemi/0000-0001-9597-4497
CR [Anonymous], P CHI 2006
   Blakowski G, 1996, IEEE J SEL AREA COMM, V14, P5, DOI 10.1109/49.481691
   Bodnar A, 2004, P 6 INT C MULTIMODAL, P183, DOI [10.1145/1027933.1027965, DOI 10.1145/1027933.1027965]
   Cevher V, 2007, IEEE T MULTIMEDIA, V9, P715, DOI 10.1109/TMM.2007.893340
   Chastrette M, 2002, OLFACTION, TASTE, AND COGNITION, P100, DOI 10.1017/CBO9780511546389.012
   Herz RS, 2002, OLFACTION, TASTE, AND COGNITION, P160, DOI 10.1017/CBO9780511546389.016
   Hudson R, 2002, OLFACTION, TASTE, AND COGNITION, P408, DOI 10.1017/CBO9780511546389.034
   Hummel T, 2002, OLFACTION, TASTE, AND COGNITION, P441, DOI 10.1017/CBO9780511546389.036
   KAYE N, 2001, THESIS MIT CAMBRIDGE
   Keller A, 2004, CURR BIOL, V14, pR875, DOI 10.1016/j.cub.2004.09.066
   Köster EP, 2002, OLFACTION, TASTE, AND COGNITION, P27, DOI 10.1017/CBO9780511546389.007
   Saito S, 2006, CHEM SENSES, V31, P379, DOI 10.1093/chemse/bjj042
   Sargin ME, 2007, IEEE T MULTIMEDIA, V9, P1396, DOI 10.1109/TMM.2007.906583
   Schaal B, 2002, OLFACTION, TASTE, AND COGNITION, P421, DOI 10.1017/CBO9780511546389.035
   Steinmetz R, 1996, IEEE J SEL AREA COMM, V14, P61, DOI 10.1109/49.481694
NR 15
TC 33
Z9 33
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 561
EP 565
DI 10.1109/TMM.2009.2012927
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300021
OA Green Published
DA 2024-07-18
ER

PT J
AU Plasberg, JH
   Kleijn, WB
AF Plasberg, Jan H.
   Kleijn, W. Bastiaan
TI Feature Selection Under a Complexity Constraint
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Classification; complexity; context awareness; cost; feature selection;
   mutual information
ID MUTUAL INFORMATION; CLASSIFICATION; ACQUISITION; ALGORITHMS; ENTROPY;
   COST
AB Classification on mobile devices is often done in an uninterrupted fashion. This requires algorithms with gentle demands on the computational complexity. The performance of a classifier depends heavily on the set of features used as input variables. Existing feature selection strategies for classification aim at finding a "best" set of features that performs well in terms of classification accuracy, but are not designed to handle constraints on the computational complexity. We demonstrate that an extension of the performance measures used in state-of-the-art feature selection algorithms with a penalty on the feature extraction complexity leads to superior feature sets if the allowed computational complexity is limited. Our solution is independent of a particular classification algorithm.
C1 [Plasberg, Jan H.; Kleijn, W. Bastiaan] Royal Inst Technol KTH, Sch Elect Engn, S-10044 Stockholm, Sweden.
C3 Royal Institute of Technology
RP Plasberg, JH (corresponding author), Royal Inst Technol KTH, Sch Elect Engn, S-10044 Stockholm, Sweden.
EM jan@plasberg.de; bastiaan.kleijn@ee.kth.se
OI Kleijn, W./0000-0002-1973-3920
CR BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224
   Cover T. M., 1991, ELEMENTS INFORM THEO
   COVER TM, 1977, IEEE T SYST MAN CYB, V7, P657, DOI 10.1109/TSMC.1977.4309803
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Egmont-Petersen M, 1999, PATTERN RECOGN LETT, V20, P1439, DOI 10.1016/S0167-8655(99)00112-9
   Eronen AJ, 2006, IEEE T AUDIO SPEECH, V14, P321, DOI 10.1109/TSA.2005.854103
   FERRI FJ, 1994, MACH INTELL PATT REC, V16, P403
   Fleuret F, 2004, J MACH LEARN RES, V5, P1531
   FRASER AM, 1989, IEEE T INFORM THEORY, V35, P245, DOI 10.1109/18.32121
   Grancharov V, 2006, IEEE T AUDIO SPEECH, V14, P1948, DOI 10.1109/TASL.2006.883250
   Hedelin P, 2000, IEEE T SPEECH AUDI P, V8, P385, DOI 10.1109/89.848220
   ISO/IEC 15938-4, 2001, 159384 ISOIEC
   Iswandy K, 2006, ADV RADIO SCI, V4, P135, DOI 10.5194/ars-4-135-2006
   Jain A, 1997, IEEE T PATTERN ANAL, V19, P153, DOI 10.1109/34.574797
   Ji SH, 2007, PATTERN RECOGN, V40, P1474, DOI 10.1016/j.patcog.2006.11.008
   John G.H., 1994, P 11 INT C MACH LEAR, P121
   KLEIJN WB, 2007, BASIC SOURCE CODING
   Kwak N, 2002, IEEE T NEURAL NETWOR, V13, P143, DOI 10.1109/72.977291
   Liu H, 2005, IEEE T KNOWL DATA EN, V17, P491, DOI 10.1109/TKDE.2005.66
   MURPHY KP, BAYES NET TOOLBOX MA
   NELSON GD, 1968, IEEE T SYST SCI CYB, VSSC4, P145, DOI 10.1109/TSSC.1968.300141
   Nilsson M, 2002, INT CONF ACOUST SPEE, P525
   Nilsson M, 2007, IEEE T INFORM THEORY, V53, P2330, DOI 10.1109/TIT.2007.899533
   PACLIK P, 2002, LECT NOTES COMPUTER, P491
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   PELTONEN V, 2002, P IEEE INT C AC SPEE, V1, P1941
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Plasberg JH, 2007, IEEE ICCE, P9
   PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9
   PUDIL P, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL II, P92, DOI 10.1109/ICPR.1992.201729
   SIEDLECKI W, 1989, PATTERN RECOGN LETT, V10, P335, DOI 10.1016/0167-8655(89)90037-8
   Tourassi GD, 2001, MED PHYS, V28, P2394, DOI 10.1118/1.1418724
   YANG H, 1999, P ADV INT DAT AN AID
   Yang J., 1998, FEATURE EXTRACTION C, P117, DOI 10.1007/978-1-4615-5725-8_8
NR 34
TC 6
Z9 8
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 565
EP 571
DI 10.1109/TMM.2009.2012944
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300022
DA 2024-07-18
ER

PT J
AU Konstantinides, JM
   Mademlis, A
   Daras, P
   Mitkas, PA
   Strintzis, MG
AF Konstantinides, John M.
   Mademlis, Athanasios
   Daras, Petros
   Mitkas, Pericles A.
   Strintzis, Michael G.
TI Blind Robust 3-D Mesh Watermarking Based on Oblate Spheroidal Harmonics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D watermarking; blind detection; copyright protection; mesh
   watermarking; spheroidal harmonics
AB In this paper, a novel transform-based, blind and robust 3-D mesh watermarking scheme is presented. The 3-D surface of the mesh is firstly divided into a number of discrete continuous regions, each of which is successively sampled and mapped onto oblate spheroids, using a novel surface parameterization scheme. The embedding is performed in the spheroidal harmonic coefficients of the spheroids, using a novel embedding scheme. Changes made to the transform domain are then reversed back to the spatial domain, thus forming the watermarked 3-D mesh. The embedding scheme presented herein resembles, in principal, the ones using the multiplicative embedding rule (inherently providing high imperceptibility). The watermark detection is blind and by far more powerful than the various correlators typically incorporated by multiplicative schemes. Experimental results have shown that the proposed blind watermarking scheme is competitively robust against similarity transformations, connectivity attacks, mesh simplification and refinement, unbalanced resampling, smoothing and noise addition, even when juxtaposed to the informed ones.
C1 [Konstantinides, John M.; Mademlis, Athanasios; Mitkas, Pericles A.; Strintzis, Michael G.] Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Informat Proc Lab, GR-54006 Thessaloniki, Greece.
   [Daras, Petros; Strintzis, Michael G.] Ctr Res & Technol Hellas, Informat & Telemat Inst, Thermi 57001, Greece.
C3 Aristotle University of Thessaloniki; Centre for Research & Technology
   Hellas
RP Konstantinides, JM (corresponding author), Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Informat Proc Lab, GR-54006 Thessaloniki, Greece.
EM jconnides@yahoo.gr; mademlis@iti.gr; daras@iti.gr; mitkas@eng.auth.gr;
   strintzi@eng.auth.gr
RI Daras, Petros/F-5284-2012
OI Daras, Petros/0000-0003-3814-6710
FU EU; Greek Secretariat For Research and Technology
FX Manuscript received May 02. 2008: revised September 23, 2008. Current
   version published January 09, 2009. This work was supported in part by
   the VICTORY EU project and by the PENED project (co-financed 75% by the
   EU and 25% by the Greek Secretariat For Research and Technology). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Z. Jane Wang.
CR Abramowitz M., 1964, HDB MATH FUNCTION FO
   ALFACE PR, 2007, DATA HIDING MULTIMED, V2, P91
   ALFACE PR, 2005, ICIP, P693
   Benedens O, 2003, PROC SPIE, V5020, P337, DOI 10.1117/12.477299
   Benedens O, 2003, LECT NOTES COMPUT SC, V2578, P177
   BENEDENS O, 2002, 02I002FIGD
   Bors AG, 2006, IEEE T IMAGE PROCESS, V15, P687, DOI 10.1109/TIP.2005.863116
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Cho KK, 2007, CONTR PHENOMENOL, V55, P1
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   COHEN J, 1998, SIGGRAPH 98, P115
   Cox I. J., 2002, Digital Watermarking
   DEROSE TD, 1993, 931005 U WASH DEPT C
   Grafarend EW, 1999, J GEODESY, V73, P611, DOI 10.1007/s001900050272
   Halstead M., 1993, Computer Graphics Proceedings, P35, DOI 10.1145/166117.166121
   Harte T, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P661, DOI 10.1109/ICIP.2002.1039057
   KANAI S, 1998, P 6 IFIP WG 5 2 GEO, P296
   Loop C, 1987, THESIS U UTAH
   Ohbuchi R, 1998, IEEE J SEL AREA COMM, V16, P551, DOI 10.1109/49.668977
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   Praun E, 1999, COMP GRAPH, P49, DOI 10.1145/311535.311540
   RONDAOALFACE P, 2007, ICIP, P465
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   TAUBIN G, 1995, COMPUTER GRAPHICS, V29, P351
   Thong N. C., 1989, Manuscripta Geodaetica, V14, P285
   UCCHEDDU F, 2004, MM SEC 04, P143
   VALETTE S, 1999, ICIP, P171
   WAGNER MG, 2000, GMP 00, P201
   WANG K, 2008, IEEE T MULT IN PRESS
   Yin KK, 2001, COMPUT GRAPH-UK, V25, P409, DOI 10.1016/S0097-8493(01)00065-6
   Yu ZQ, 2003, PATTERN RECOGN, V36, P2603, DOI 10.1016/S0031-3203(03)00086-4
   Zafeiriou S, 2005, IEEE T VIS COMPUT GR, V11, P596, DOI 10.1109/TVCG.2005.71
NR 33
TC 28
Z9 29
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 23
EP 38
DI 10.1109/TMM.2008.2008913
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700003
DA 2024-07-18
ER

PT J
AU Park, SB
   Lee, SU
AF Park, Sung-Bum
   Lee, Sang-Uk
TI Multiscale Representation and Compression of 3-D Point Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D representation and compression; multiscale representation; plane
   decomposition; plane primitive; point data; zerotree coding
AB A compact representation scheme is presented for 3-D point data. To describe underlying surface from raw point samples, we dyadically divide a 3-D domain enclosing whole points. Then, local points in each cube are approximated by a plane patch, yielding a multiscale representation of 3-D surface. To reduce the redundancy between different scale models, the geometry Innovation is evaluated between different scale planes, which reveals the Euclidian distance between planes. Finally, the geometry innovation coefficients are compressed by a zerotree-based encoder. Based on the multiscale plane representation of 3-D geometry and the efficient plane decomposition method, the proposed scheme provides a desirable framework for 3-D point geometry processing.
C1 [Park, Sung-Bum] Samsung Elect Co Ltd, Digital Media R&D Ctr, Multimedia Platform Lab, Suwon, South Korea.
   [Lee, Sang-Uk] Seoul Natl Univ, Sch Elect Engn, Signal Proc Lab, Seoul, South Korea.
C3 Samsung; Samsung Electronics; Seoul National University (SNU)
RP Park, SB (corresponding author), Samsung Elect Co Ltd, Digital Media R&D Ctr, Multimedia Platform Lab, Suwon, South Korea.
EM sbpark@ieee.org; sanguk@ipl.snu.ac.kr
CR Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   *CYB, CYB SAMPL MOD
   Donoho D. L., 2001, MULTISCALE MULTIRESO, V20, P149
   Donoho DL, 1999, ANN STAT, V27, P859, DOI 10.1214/aos/1018031261
   Fleishman S, 2003, ACM T GRAPHIC, V22, P997, DOI 10.1145/944020.944023
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   Huang Y., 2006, EUROGRAPHICS S POINT, P103, DOI DOI 10.2312/SPBG/SPBG06/103-110
   Huang Y, 2008, IEEE T VIS COMPUT GR, V14, P440, DOI 10.1109/TVCG.2007.70441
   Huo XM, 2005, IEEE T IMAGE PROCESS, V14, P1665, DOI 10.1109/TIP.2005.857273
   Kalaiah A, 2005, ACM T GRAPHIC, V24, P348, DOI 10.1145/1061347.1061356
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Ochotta T., 2004, EUROGRAPHICS S POINT, P103
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Schnabel R., 2006, P S POINT BAS GRAPH, V6, P111, DOI DOI 10.2312/SPBG/SPBG06/111-120
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   WASCHBUSCH M., 2004, Proceedings of Eurographics Symposium on Point-Based Graphics 2004, P95
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
   WOO M, 1999, OPENGL 1 2 PROGRAMMI
   ZORIN D., 1996, P SIGGRAPH ANN C COM, P189
NR 20
TC 24
Z9 30
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 177
EP 182
DI 10.1109/TMM.2008.2008868
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700015
DA 2024-07-18
ER

PT J
AU Ahuja, S
   Krunz, M
AF Ahuja, Satyajeet
   Krunz, Marwan
TI Algorithms for Server Placement in Multiple-Description-Based Media
   Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE Global Telecommunications Conference (GLOBECOM 06)
CY NOV 27-DEC 01, 2006
CL San Francisco, CA
SP IEEE Commun Soc (ComSoc)
DE Multiple description coding; path diversity
AB Multiple description coding (MDC) has emerged as a powerful technique for reliable real-time communications over lossy packet networks. In its basic form, it involves encoding a media stream into r substreams that are sent independently from a source to a destination. Each substream (or description) can be decoded independent of the other r - 1 substreams. With every successful reception of a substream, the quality of the decoded signal improves. In this paper, we consider the problem of placing a set of servers in the network such that a desired quality of service can be provided to a community of clients that request MDC-coded traffic. We formulate the server placement (SP) problem, with the goal of identifying the minimum number of server locations that can provide r descriptions to a set of clients such that the delay associated with each path from a chosen server location to a given client is bounded by a given delay constraint and the total "unreliability" associated with the group of paths to a given client is also upper bounded. We show that the SP problem belongs to the class of NP-complete problems. We propose a mixed-integer linear programming (MILP) formulation and an efficient heuristic solution for the SP problem. Simulations are conducted to evaluate the performance of the proposed algorithm and compare it with the optimal solution provided by the MILP solution.
C1 [Ahuja, Satyajeet; Krunz, Marwan] Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA.
C3 University of Arizona
RP Ahuja, S (corresponding author), Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA.
EM ahuja@cce.arizona.edu; krunz@ece.arizona.edu
OI Krunz, Marwan/0000-0001-7137-2985
CR Ahuja R. K., 1993, Network flows: theory, algorithms, and applications
   AHUJA SS, 2008, TRUAECE20081
   Apostolopoulos J, 2002, IEEE INFOCOM SER, P1736, DOI 10.1109/INFCOM.2002.1019427
   APOSTOLOPOULOS J, 2001, P INT WORKSH VIS COM
   Chen Y, 2002, LECT NOTES COMPUT SC, V2429, P306
   Fitzek F.H.P., 2004, PROC WPMC, V2, P524
   GAREY M, 2000, COMPUT INTRACTABILIT
   Korkmaz T, 2003, IEEE ACM T NETWORK, V11, P384, DOI 10.1109/TNET.2003.813047
   Krishnan P, 2000, IEEE ACM T NETWORK, V8, P568, DOI 10.1109/90.879344
   Lee YC, 2003, SIGNAL PROCESS-IMAGE, V18, P337, DOI 10.1016/S0923-5965(02)00138-8
   Lorenz DH, 2001, OPER RES LETT, V28, P213, DOI 10.1016/S0167-6377(01)00069-4
   NI J, 2003, P IEEE ICC C MAY, V2, P854
   ORDA A, 2004, P IEEE INFOCOM C HON
   RODOLAKIS G, 2005, P INT WORKSH QOS MUL
   SINGH R, 1920, SPIE IMAGE VIDEO COM
   Taubman D., 2012, JPEG2000: image compression fundamentals, standards and practice, V642
   Vakali A, 2003, IEEE INTERNET COMPUT, V7, P68, DOI 10.1109/MIC.2003.1250586
   Wang LM, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P345
   WAXMAN BM, 1988, IEEE J SEL AREA COMM, V6, P1617, DOI 10.1109/49.12889
   Zhu WP, 2005, LECT NOTES COMPUT SC, V3719, P386
   FELIX INDEPENDENT MO
NR 21
TC 8
Z9 11
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1382
EP 1392
DI 10.1109/TMM.2008.2004930
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ou, JZ
   Oh, LM
   Fussell, SR
   Blum, T
   Yang, J
AF Ou, Jiazhi
   Oh, Lui Min
   Fussell, Susan R.
   Blum, Tal
   Yang, Jie
TI Predicting Visual Focus of Attention From Intention in Remote
   Collaborative Tasks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer-supported cooperative work; eye tracking; focus of attention;
   keyword spotting; remote collaborative tasks
ID AWARENESS
AB While shared visual space plays a very important role in remote collaboration on physical tasks, it is challenging and expensive to track users' focus of attention (FOA) during these tasks. In this paper, we propose to identify a user's FOA from his/her intention based on task properties, people's actions in the workspace, and conversational content. We employ a conditional Markov model to characterize a subject's FOA. We demonstrate the feasibility of the proposed method using a collaborative laboratory task in which one partner (the helper) instructs another (the worker) on how to assemble online puzzles. We model a helper's FOA using task properties, workers' actions, and conversational content. The accuracy of the model ranged from 65.40% for puzzles with easy-to-name pieces to 74.25% for puzzles with more difficult-to-name pieces. The proposed model can be used to predicate a user's FOA in a remote collaborative task without tracking the user's eye gaze.
C1 [Ou, Jiazhi; Blum, Tal] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.
   [Oh, Lui Min; Fussell, Susan R.; Yang, Jie] Carnegie Mellon Univ, Human Comp Interact Inst, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University; Carnegie Mellon University
RP Ou, JZ (corresponding author), Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.
EM jiazhiou@cmu.edu; kei-thoh@alumni.cmu.edu; sfussell@cmu.edu;
   thlum@bbn.com; yang@cs.cmu.edu
CR [Anonymous], 2001, P SIGCHI C HUMAN FAC, DOI DOI 10.1145/365024.365119
   [Anonymous], GROUNDING COMMUNICAT
   Argyle M., 1976, Gaze and Mutual Gaze
   BRUMITT B, 2000, IEEE PERS COMMUN AUG
   Campana Ellen., 2001, Proceedings of the 2001 workshop on perceptive user interfaces, P1
   CAMPBELL CS, 2001, P ACM C PERC US INT
   Clark H., 1981, Elements of Discourse Understanding, P10
   Clark H., USING LANGUAGE
   Clark HH, 2004, J MEM LANG, V50, P62, DOI 10.1016/j.jml.2003.08.004
   EBERHARD KM, 1995, J PSYCHOLINGUIST RES, V24, P409, DOI 10.1007/BF02143160
   ENDSLEY MR, 1995, HUM FACTORS, V37, P32, DOI 10.1518/001872095779049543
   Freitag D., 1999, A A A I Workshop on Machine Learning for Information Extraction, P31
   Fussell S.R., 2000, P CSCW 2000, P21, DOI DOI 10.1145/358916.358947
   FUSSELL SR, 2003, P SIGCHI C HUM FACT
   Fussell SR, 2004, HUM-COMPUT INTERACT, V19, P273, DOI 10.1207/s15327051hci1903_3
   GAVER W, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P335
   GOOD IJ, 1953, BIOMETRIKA, V40, P237, DOI 10.2307/2333344
   Griffin ZM, 2000, PSYCHOL SCI, V11, P274, DOI 10.1111/1467-9280.00255
   Gullberg M, 2003, MIND'S EYE: COGNITIVE AND APPLIED ASPECTS OF EYE MOVEMENT RESEARCH, P685, DOI 10.1016/B978-044451020-4/50037-2
   Jacob R. J., 1993, ADV HUMAN COMPUTER I, V4, P151
   JOVANOVIC N, 2006, P 11 C EUR CHAPT ASS
   Keysar B, 2000, PSYCHOL SCI, V11, P32, DOI 10.1111/1467-9280.00211
   Kraut R.E., 2002, P 2002 ACM C COMPUTE, P31
   Kraut RE, 2003, HUM-COMPUT INTERACT, V18, P13, DOI 10.1207/S15327051HCI1812_2
   Kuzuoka H., 2000, ACM CSCW, P155
   Kuzuoka H., 1994, P 1994 ACM C COMPUTE, DOI DOI 10.1145/192844.192866
   Lafferty J., 2001, ICML 01 P 18 INT C M
   Land M, 1999, PERCEPTION, V28, P1311, DOI 10.1068/p2935
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   MAGLIO PP, 2000, LNCS, V1948
   McCallum A., 2000, P 17 INT C MACH LEAR, V17, P591
   Monk AF, 2002, DISCOURSE PROCESS, V33, P257, DOI 10.1207/S15326950DP3303_4
   OU J, 2003, P INT C MULT INT VAN
   OU J, DOVE 2 COMBINI UNPUB
   OU J, 2006, ICMI 2006
   OU J, 2005, P ICMI 05 TRENT IT
   Ou J., 2005, P SIGCHI C HUM FACT, P231
   Oudejans RRD, 1999, J EXP PSYCHOL HUMAN, V25, P531, DOI 10.1037/0096-1523.25.2.531
   Pelz JB, 2001, VISION RES, V41, P3587, DOI 10.1016/S0042-6989(01)00245-0
   Qvarfordt P., 2005, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, New York, NY, USA, P221
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   SALVUCCI D, 1999, HUMAN FACTORS COMPUT
   Salvucci D.D., 2000, P 2000 S EYE TRACK R, P71, DOI [DOI 10.1145/355017.355028, 10.1145/355017.355028]
   Stiefelhagen R, 2002, IEEE T NEURAL NETWOR, V13, P928, DOI 10.1109/TNN.2002.1021893
   TANG JC, 1991, INT J MAN MACH STUD, V34, P143, DOI 10.1016/0020-7373(91)90039-A
   Velichkovsky B.M., 1995, Pragmatics Cognition, V3, P199, DOI [10.1075/pc.3.2.02vel, DOI 10.1075/PC.3.2.02VEL]
   Vickers JN, 1996, J EXP PSYCHOL HUMAN, V22, P342, DOI 10.1037/0096-1523.22.2.342
   Whittaker S., 1997, VIDEO MEDIATED COMMU, P23
   [No title captured]
NR 49
TC 9
Z9 9
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1034
EP 1045
DI 10.1109/TMM.2008.2001363
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600008
DA 2024-07-18
ER

PT J
AU Wei, XY
   Ngo, CW
   Jiang, YG
AF Wei, Xiao-Yong
   Ngo, Chong-Wah
   Jiang, Yu-Gang
TI Selection of Concept Detectors for Video Search by Ontology-Enriched
   Semantic Spaces
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic space; ontology; concept-based video search; semantic detectors
ID RETRIEVAL
AB This paper describes the construction and utilization of two novel semantic spaces, namely Ontology-enriched Semantic Space (OSS) and Ontology-enriched Orthogonal Semantic Space (OS2), to facilitate the selection of concept detectors for video search. These two semantic spaces are enriched with ontology knowledge, while emphasizing consistent and uniform comparison of ontological relatedness among concepts for query-to-concept mapping. OS2, in addition to being a linear space like OSS, also guarantees orthogonality of the semantic space. Compared with other ontology reasoning measures, both spaces are capable of providing platforms that offer a global view of concept inter-relatedness, by allowing evaluation of concept similarity in metric spaces. We simulate OSS and OS2 by using LSCOM concepts and experiment search effectiveness with VIREO-374 concept detectors. Empirical observations indicate that the proposed semantic spaces enable more effective selection of concept detectors than eight other existing ontology measures. OS2, in particular, is better in providing a viable and reasonable solution for fusion of multiple concept detectors.
C1 [Wei, Xiao-Yong; Ngo, Chong-Wah; Jiang, Yu-Gang] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Wei, XY (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM xiaoyong@cs.cityu.edu.hk; cwngo@cs.cityu.edu.hk; yjiang@cs.cityu.edu.hk
OI Ngo, Chong Wah/0000-0003-4182-8261
CR [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], INT C RES COMP LING
   [Anonymous], 2007, COLUMBIA U BASELINE
   BERENZWEIG A, 2003, IEEE INT C MULT EXP
   Bhavani SD, 2008, APPL SOFT COMPUT, V8, P555, DOI 10.1016/j.asoc.2007.03.007
   Budanitsky A, 2006, COMPUT LINGUIST, V32, P13, DOI 10.1162/coli.2006.32.1.13
   Campbell Murray, 2006, TRECVID, P175
   CHANG SF, 2006, TRECVID, P99
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Francis N.W., 1982, FREQUENCY ANAL ENGLI
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   HOLLINK L, 2005, ACM INT C MULT MM
   Hoogs A, 2003, PROC CVPR IEEE, P327
   Horn R. A., 2012, MATRIX ANAL
   Jaimes A, 2003, LECT NOTES COMPUT SC, V2728, P248
   Jain A. K., 1988, Algorithms for Clustering Data, P446
   JIANG YG, 2007, INT C IM VID RETR CI
   KENDER J, 2007, IEEE INT C MULT EXP
   Leacock C., 1998, Combining Local Context and WordNet Similarity for Word Sense Identi cation, P265
   LESK Michael, 1986, P 5 ANN INT C SYST D, V5, P24, DOI 10.1145/318723.318728
   LI X, 2007, INT C IM VID RETR CI
   Lin DK, 1997, 35TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 8TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P64
   Lin WH, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P41, DOI 10.1109/ICME.2006.262545
   LUO H, 2006, ACM INT C MULT MM, P57
   Merler M, 2015, IEEE INT CON MULTI
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Natsev A.P., 2004, PROC 10 ACM SIGKDD I, P641
   NEO SY, 2006, INT C IM VID RETR CI
   Pan HX, 2007, ADV INTEL SYS RES, DOI 10.2991/iske.2007.168
   PATWARDHAN S, 2006, C EUR CHAPT ASS COMP
   Penrose R., 1955, Proc. Cambridge Philos. Soc., V51, P406, DOI [10.1017/S0305004100030401, DOI 10.1017/S0305004100030401]
   Resnik P, 1995, INT JOINT CONF ARTIF, P448
   ROMANO JP, 1990, J AM STAT ASSOC, V85, P686, DOI 10.2307/2290003
   Slaney M, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P345, DOI 10.1109/ICME.2002.1035789
   Smeaton AF, 2006, ACM INT WORKSH MULT
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   SNOEK CGM, 2006, TRECVID, P277
   Vleugels J, 2002, PATTERN RECOGN, V35, P69, DOI 10.1016/S0031-3203(00)00120-5
   WANG H, 2006, MULTIMEDIA 06 P 14 A, P109
   WU Y, 2004, IEEE INT C MULT EXP, V2, P1003
   ZHIBIAO W, 1994, ANN M ASS COMP LING, P133
   2007, INT J VERY LARGE DAT, V16, P483
NR 42
TC 19
Z9 19
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1085
EP 1096
DI 10.1109/TMM.2008.2001382
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600012
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Carlsson, N
   Mahanti, A
   Li, ZP
   Eager, D
AF Carlsson, Niklas
   Mahanti, Anirban
   Li, Zongpeng
   Eager, Derek
TI Optimized periodic broadcast of nonlinear media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE nonlinear media; optimization; periodic broadcast; video delivery
AB Conventional video consists of a single sequence of video frames. During a client's playback period, frames are viewed sequentially from some specified starting point. The fixed frame ordering of conventional video enables efficient scheduled broadcast delivery, as well as efficient near on-demand delivery to large numbers of concurrent clients through use of periodic broadcast protocols in which the video file is segmented and transmitted on multiple channels. This paper considers the problem of devising scalable protocols for near on-demand delivery of "nonlinear" media files whose content may have a tree or graph, rather than linear, structure. Such media allows personalization of the media playback according to individual client preferences. We formulate a mathematical model for determination of the optimal periodic broadcast protocol for nonlinear media with piecewise-linear structures. Our objective function allows differing weights to be placed on the startup delays required for differing paths through the media. Studying a number of simple nonlinear structures we provide insight into the characteristics of the optimal solution. For cases in which the cost of solving the optimization model is prohibitive, we propose and evaluate an efficient approximation algorithm.
C1 [Carlsson, Niklas; Eager, Derek] Univ Saskatchewan, Dept Comp Sci, Saskatoon, SK S7N 5C9, Canada.
   [Mahanti, Anirban] Indian Inst Technol Delhi, Dept Comp Sci & Engn, Delhi, India.
   [Li, Zongpeng] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Saskatchewan; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Delhi; University of
   Calgary
RP Carlsson, N (corresponding author), Univ Saskatchewan, Dept Comp Sci, Saskatoon, SK S7N 5C9, Canada.
CR Bradshaw MK, 2003, MULTIMEDIA SYST, V9, P78, DOI 10.1007/s00530-003-0079-2
   Chawathe Y., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P795, DOI 10.1109/INFCOM.2000.832254
   Eager D, 2001, IEEE T KNOWL DATA EN, V13, P742, DOI 10.1109/69.956098
   Gotz D., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, P357, DOI DOI 10.1145/1180639.1180717
   Hu AL, 2001, IEEE INFOCOM SER, P508, DOI 10.1109/INFCOM.2001.916754
   Hua K., 1997, PROC SIGCOMM, P89
   Jannotti J, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P197
   Juhn LS, 1997, IEEE T BROADCAST, V43, P268, DOI 10.1109/11.632927
   Lao L, 2005, LECT NOTES COMPUT SC, V3462, P906
   Mahanti A, 2003, IEEE ACM T NETWORK, V11, P195, DOI 10.1109/TNET.2003.810311
   NIKOLAIDIS I., 1999, P 7 INT S MOD AN SIM, P262
   QUDAH B, 2006, P ACM MULT OCT, P347
   Rost S, 2001, WEB CACHING AND CONTENT DELIVERY, P147
   SHI L, 2006, P ACM MULT OCT, P337
   Viswanathan S, 1996, MULTIMEDIA SYST, V4, P197, DOI 10.1007/s005300050023
   Zhao YP, 2004, IEEE INFOCOM SER, P1522
NR 16
TC 8
Z9 8
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 871
EP 884
DI 10.1109/TMM.2008.922847
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800018
DA 2024-07-18
ER

PT J
AU Xu, CS
   Wang, JJ
   Lu, HQ
   Zhang, YF
AF Xu, Changsheng
   Wang, Jinjun
   Lu, Hanqing
   Zhang, Yifan
TI A novel framework for semantic annotation and personalized retrieval of
   sports video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE annotation; event detection; personalized retrieval; sports video
   analysis; summarization
ID SEGMENTATION
AB Sports video annotation is important for sports video semantic analysis such as event detection and personalization. In this paper, we propose a novel approach for sports video semantic annotation and personalized retrieval. Different from the state of the art sports video analysis methods which heavily rely on audio/visual features, the proposed approach incorporates web-casting text into sports video analysis. Compared with previous approaches, the contributions of our approach include the following. 1) The event detection accuracy is significantly improved due to the incorporation of web-casting text analysis. 2) The proposed approach is able to detect exact event boundary and extract event semantics that are very difficult or impossible to be handled by previous approaches. 3) The proposed method is able to create personalized summary from both general and specific point of view related to particular game, event, player or team according to user's preference. We present the framework of our approach and details of text analysis, video analysis, text/video alignment, and personalized retrieval. The experimental results on event boundary detection in sports video are encouraging and comparable to the manually selected events. The evaluation on personalized retrieval is effective in helping meet users' expectations.
C1 [Xu, Changsheng; Wang, Jinjun] Media Semant, Inst Infocomm Res, Singapore 119613, Singapore.
   [Lu, Hanqing; Zhang, Yifan] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); Chinese Academy of Sciences; Institute of
   Automation, CAS
RP Xu, CS (corresponding author), Media Semant, Inst Infocomm Res, Singapore 119613, Singapore.
EM xucs@i2r.a-star.edu.sg; stuwj2@i2r.a-star.edu.sg; luhq@nlpr.ia.ac.cn;
   yfzhang@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; chen, yue/JXW-9556-2024; zhang,
   yifan/ABB-5853-2021
FU National Sciences Foundation of China [60475010]
FX The work of Lu and Zhang was supported by National Sciences Foundation
   of China under Grant 60475010. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Wen
   Gao.
CR [Anonymous], 1995, P IEEE INT C MULT CO
   Assfalg E, 2003, COMPUT VIS IMAGE UND, V92, P285, DOI 10.1016/j.cviu.2003.06.004
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   BABAGUCHI N, 2003, P PAC RIM C MULT SIN, P940
   BABAGUCHI N, 2000, P ACM MULT 2000 WORK, P205
   BERTINIA M, 2005, P 2 EUR SEM WEB C HE
   CHESHIRE D, 1990, COMPLETE BOOK VIDEO
   Duan L.Y., 2003, Proc. ACM Int. Conf. Multimedia, P33
   Duan LY, 2005, IEEE T MULTIMEDIA, V7, P1066, DOI 10.1109/TMM.2005.858395
   Duan LY, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P709
   Duan LY, 2003, PROC SPIE, V5021, P300, DOI 10.1117/12.476259
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Han M., 2002, Proc. ACM Multimedia, P347, DOI [DOI 10.1145/641007.641081, 10.1145/641007.641081]
   HAUPTMANN A, 2004, P ACM MULT NEW YORK, P668
   JAIMES A, 2003, P IEEE INT CULT EXP
   JAIMES A, 2003, P INT C IM VID RETR
   LI Y, 2006, P INT C PATT REC HON
   Liang J., 2005, International Journal on Document Analysis and Recognition, V7, P84, DOI 10.1007/s10032-004-0138-z
   Lienhart R, 2000, MULTIMEDIA SYST, V8, P69, DOI 10.1007/s005300050006
   Mezaris V, 2004, IEEE T CIRC SYST VID, V14, P606, DOI 10.1109/TCSVT.2004.826768
   Miyauchi S, 2002, INT C PATT RECOG, P1009, DOI 10.1109/ICPR.2002.1048476
   NEPAL S, 2001, P ACM MULT OTT CAN, P61
   NGO CW, 2002, P ACM MULT 2002, P51
   Nitta N, 2005, MULTIMED TOOLS APPL, V25, P59, DOI 10.1023/B:MTAP.0000046382.62218.e1
   NITTA N, 2002, P 8 INT WORKSH MULT, P110
   Otsuka I, 2006, IEEE T CONSUM ELECTR, V52, P168
   Pan H, 2001, INT CONF ACOUST SPEE, P1649, DOI 10.1109/ICASSP.2001.941253
   PAN H, 2002, P INT C AC SPEECH SI
   REIDSMA D, 2003, P ICCS03 JUL S
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   SADKIER DA, 2002, P IEEE INT C MULT EX, V2, P77
   STRINTZIS MG, 2004, P EUR WORKSH INT KNO
   Tan YP, 2000, IEEE T CIRC SYST VID, V10, P133, DOI 10.1109/76.825867
   Wan K, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P591, DOI 10.1109/ICME.2004.1394261
   Wan K, 2004, INT C PATT RECOG, P973, DOI 10.1109/ICPR.2004.1334691
   WANG J, 2005, P INT C AC SPEECH SI
   XIE L, 2003, PATTERN RECOGNIT LET, V24
   Xiong ZY, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P632
   Xu C., 2006, PROC 14 ANN ACM INT, P221, DOI DOI 10.1145/1180639.1180699
   XU H, 2005, P IEEE ICME 05 AMST, P1242
   XU H, 2004, P WORKSH MULT INF RE
   Xu M, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P1526
   Xu M, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P189
   Xu M, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P281
   XU P, 2001, P IEEE INT C MULT EX, P928
   Yang Hui, 2003, P 11 ACM INT C MULT, P632, DOI DOI 10.1145/957013.957146
   Yu X., 2003, PROC 11 ACM INT C MU, P11
   Zhang D., 2002, ACM Multimedia, P315
   ZHONG D, 2001, P IEEE INT MULT EXP
NR 49
TC 86
Z9 92
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 421
EP 436
DI 10.1109/TMM.2008.917346
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100011
DA 2024-07-18
ER

PT J
AU Guo, J
   Wong, EWM
   Chan, S
   Taylor, P
   Zukerman, M
   Tang, KS
AF Guo, Jun
   Wong, Eric W. M.
   Chan, Sammy
   Taylor, Peter
   Zukerman, Moshe
   Tang, Kit-Sang
TI Performance analysis of resource selection schemes for a large scale
   video-on-demand system
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 18th International Teletraffic Congress
CY AUG 31-SEP 05, 2003
CL BERLIN, GERMANY
SP Arcor, Deutsch Telekom, Marconi, Nortel Networks, Siemens, Vodafone
DE blocking probability; fixed-point approximation; resource selection;
   video-on-demand
ID BLOCKING PROBABILITIES
AB The designers of a large scale video-on-demand system face an optimization problem of deciding how to assign movies to multiple disks (servers) such that the request blocking probability is minimized subject to capacity constraints. To solve this problem, it is essential to develop scalable and accurate analytical means to evaluate the blocking performance of the system for a given file assignment. The performance analysis is made more complicated by the fact that the request blocking probability depends also on how disks are selected to serve user requests for multicopy movies. In this paper, we analyze several efficient resource selection schemes. Numerical results demonstrate that our analysis is scalable and sufficiently accurate to support the task of file assignment optimization in such a system.
C1 [Guo, Jun] Univ New S Wales, Sch Engn & Comp Sci, Sydney, NSW 2052, Australia.
   [Wong, Eric W. M.; Chan, Sammy; Tang, Kit-Sang] City Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
   [Taylor, Peter] Univ Melbourne, CUBIN, ARC Ctr Excellence Math & Stat Complex Syst MASCO, Melbourne, Vic, Australia.
   [Taylor, Peter] Univ Melbourne, Dept Math & Stat, Melbourne, Vic, Australia.
   [Zukerman, Moshe] Univ Melbourne, Dept Elect & Elect Engn, ARC Special Res CUBIN, Melbourne, Vic, Australia.
C3 University of New South Wales Sydney; City University of Hong Kong;
   University of Melbourne; University of Melbourne; University of
   Melbourne
RP Guo, J (corresponding author), Univ New S Wales, Sch Engn & Comp Sci, Sydney, NSW 2052, Australia.
EM jguo@cse.unsw.edu.au; eeewong@cityu.edu.hk; eeschan@cityu.edu.hk;
   p.taylor@ms.unimelb.edu.au; m.zukerman@ee.unimelb.edu.au;
   eekstang@cityu.edu
RI Guo, Jun/J-9051-2016; Zukerman, Moshe/AAD-3617-2019
OI Guo, Jun/0000-0002-5645-2132; Zukerman, Moshe/0000-0001-6190-5020; Tang,
   Wallace K.S./0000-0002-5786-418X; WONG, Wing Ming
   Eric/0000-0002-1641-6903; Taylor, Peter/0000-0001-7600-5383; CHAN,
   Sammy/0000-0002-8524-229X
CR Akimaru H, 1999, TELETRAFFIC THEORY A
   AKINPELU JM, 1984, AT&T TECH J, V63, P1261, DOI 10.1002/j.1538-7305.1984.tb00036.x
   [Anonymous], P ACM WWW 04
   [Anonymous], P IEEE ICC 99 JUN
   [Anonymous], 1949, Human behaviour and the principle of least-effort
   Ash G.R., 1998, Dynamic Routing in Telecommunications Networks
   ASH GR, 1993, IEEE T COMMUN, V41, P1748, DOI 10.1109/26.241755
   BOSE SK, 2002, INTRO QUEUEING SYSTE, DOI DOI 10.1007/978-1-4615-0001-8
   Chung SP, 1993, IEEE ACM T NETWORK, V1, P105, DOI 10.1109/90.222911
   CLEARY K, 1995, P INT BROADC CONV AM, P432
   Dan A., 1994, Proceedings ACM Multimedia '94, P15, DOI 10.1145/192593.192614
   DELODDERE D, 1994, IEEE COMMUN MAG, V32, P82, DOI 10.1109/35.281582
   GOULD HW, 1971, COMBINATORIAL IDENTI
   GUO J, 2003, P IEEE ICME 03 BALT, V2, P329
   GUO J, 2006, THESIS U MELBOURNE M
   Kelly F. P., 1991, ANN APPL PROBAB, V1, P319
   KELLY FP, 1986, ADV APPL PROBAB, V18, P473, DOI 10.2307/1427309
   Kolyvas GT, 1997, SECOND IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATIONS, PROCEEDINGS, P96, DOI 10.1109/ISCC.1997.615978
   Little T D., 1995, Multimedia Systems, V2, P280
   MUNKRES J. R., 1984, Elements of Algebraic Topology
   Ross K., 1995, MULTISERVICE LOSS MO
   Sevastyanov BA, 1957, Theory Probab. Appl., V2, P104, DOI 10.1137/1102005
   SINCOSKIE WD, 1991, COMPUT NETWORKS ISDN, V22, P155, DOI 10.1016/0169-7552(91)90007-Y
   SITARAM D., 2000, Multimedia servers: applications, environments, and design
   Tang KS, 2001, IEEE T IND ELECTRON, V48, P891, DOI 10.1109/41.954552
   Tsao SL, 1999, J VIS COMMUN IMAGE R, V10, P197, DOI 10.1006/jvci.1999.0420
   Wolf JL, 1997, MULTIMEDIA SYST, V5, P358, DOI 10.1007/s005300050067
   WONG EWM, 1990, P IEEE INFOCOM 90 9, V3, P934
   Zhao YQ, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P640
NR 29
TC 20
Z9 23
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 153
EP 159
DI 10.1109/TMM.2007.911281
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200015
OA Green Published
DA 2024-07-18
ER

PT J
AU Xiang, SJ
   Huang, JW
AF Xiang, Shijun
   Huang, Jiwu
TI Histogram-based audio watermarking against time-scale modification and
   cropping attacks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 8th International Workshop on Information Hiding
CY JUL 10-12, 2006
CL Alexandria, VA
DE audio watermarking; cropping; histogram; jittering; synchronization; TSM
ID ROBUST
AB In audio watermarking area, the robustness against desynchronization attacks, such as TSM (Time-Scale Modification) and random cropping operations, is still one of the most challenging issues. In this paper, we present a multibit robust audio watermarking solution for such a problem by using the insensitivity of the audio histogram shape and the modified mean to TSM and cropping operations. We address the insensitivity property in both mathematical analysis and experimental testing by representing the histogram shape as the relative relations in the number of samples among groups of three neighboring bins. By reassigning the number of samples in groups of three neighboring bins, the watermark sequence is successfully embedded. In the embedding process, the histogram is extracted from a selected amplitude range by referring to the mean in such a way that the watermark will be able to be resistant to amplitude scaling and avoid exhaustive search in the extraction process. The watermarked audio signal is perceptibly similar to the original one. Experimental results demonstrate that the hidden message is very robust to TSM and random cropping attacks, and also has a satisfactory robustness for those common audio signal processing operations.
C1 Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510275, Peoples R China.
   Sun Yat Sen Univ, Guangdong Key Lab Informat Secur Technol, Guangzhou 510275, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University
RP Xiang, SJ (corresponding author), Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510275, Peoples R China.
EM xiangshijun@gmail.com; isshjw@mail.sysu.edu.cn
RI huang, jw/KVY-9917-2024
CR [Anonymous], 2000, Digital Watermarking
   ARNOLD M, 2000, P IEEE INT C MULT EX, V2, P1010
   Bassia P, 2001, IEEE T MULTIMEDIA, V3, P232, DOI 10.1109/6046.923822
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Coltuc D, 1999, P SOC PHOTO-OPT INS, V3657, P252, DOI 10.1117/12.344674
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   KIROVSKI D, 2001, P 4 INT WORKSH INF H, V2137, P354
   Kirovski D., 2003, IEEE T SGINAL PROCES, V51, P354
   LEE S, 2004, P 2004 PAC RIM C MUL, V3, P340
   Li W, 2006, IEEE T MULTIMEDIA, V8, P60, DOI 10.1109/TMM.2005.861291
   Li W, 2004, LECT NOTES COMPUT SC, V2939, P289
   MAES M, 1998, P 2 INT WORKSH INF H, V1525, P290
   Mansour MF, 2005, IEEE T SPEECH AUDI P, V13, P432, DOI 10.1109/TSA.2005.845816
   Mansour MF, 2001, INT CONF ACOUST SPEE, P1353, DOI 10.1109/ICASSP.2001.941179
   Moulin P, 2005, P IEEE, V93, P2083, DOI 10.1109/JPROC.2005.859599
   Steinebach M, 2001, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P49, DOI 10.1109/ITCC.2001.918764
   Swanson MD, 1998, SIGNAL PROCESS, V66, P337, DOI 10.1016/S0165-1684(98)00014-0
   SYLVAIN B, 2004, P MULT SEC WORKSH, P117
   Tachibana R, 2002, LECT NOTES COMPUT SC, V2532, P647
   Tachibana R, 2001, P SOC PHOTO-OPT INS, V4314, P104, DOI 10.1117/12.435390
   Wang XY, 2006, IEEE T SIGNAL PROCES, V54, P4835, DOI 10.1109/TSP.2006.881258
   Wu CP, 2000, PROC SPIE, V3971, P382, DOI 10.1117/12.384992
   Wu SQ, 2005, IEEE T BROADCAST, V51, P69, DOI 10.1109/TBC.2004.838265
   XIANG S, 2006, P 5 INT WORKSH DIG W, V4283, P226
   XIANG S, 2006, INT J NETWORK SECURI, V3, P230
   XIANG S, 2006, P 8 INT WORKSH INF H
   Zaidi A, 2006, IEEE T SIGNAL PROCES, V54, P570, DOI 10.1109/TSP.2005.861106
   ZMUDZINSKI S, 2005, P SPIE INT C SEC STE, V5681
   2000, SDMI CALL PROPOSALS
   STEP2001
NR 30
TC 99
Z9 114
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1357
EP 1372
DI 10.1109/TMM.2007.906580
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400003
DA 2024-07-18
ER

PT J
AU Cevher, V
   Sankaranarayanan, AC
   McClellan, JH
   Chellappa, R
AF Cevher, Volkan
   Sankaranarayanan, Aswin C.
   McClellan, James H.
   Chellappa, Rama
TI Target tracking using a joint acoustic video system
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE acoustic tracking; multimodal data fusion; particle filtering; visual
   tracking
ID DIRECTION-OF-ARRIVAL; MAXIMUM-LIKELIHOOD
AB In this paper, a multitarget tracking system for collocated video and acoustic sensors is presented. We formulate the tracking problem using a particle filter based on a state-space approach. We first discuss the acoustic state-space formulation whose observations use a sliding window of direction-of-arrival estimates. We then present the video state space that tracks a target's position on the image plane based on online adaptive appearance models. For the joint operation of the filter, we combine the state vectors of the individual modalities and also introduce a time-delay variable to handle the acoustic-video data synchronization issue, caused by acoustic propagation delays. A novel particle, filter proposal strategy for joint state-space tracking is introduced, which places the random support of the joint filter where the final posterior is likely to lie. By using the Kullback-Leibler divergence measure, it is shown that the joint operation of the filter decreases the worst case divergence of the individual modalities. The resulting joint tracking filter is quite robust against video and acoustic occlusions due to our proposal strategy. Computer simulations are presented with synthetic and field data to demonstrate the filter's performance.
C1 Univ Maryland, Dept Elect & Comp Engn, Ctr Automat Res, College Pk, MD 20742 USA.
   Georgia Inst Technol, Sch Elect & Comp Engn, Ctr Signal & Image Proc, Atlanta, GA 30332 USA.
C3 University System of Maryland; University of Maryland College Park;
   University System of Georgia; Georgia Institute of Technology
RP Cevher, V (corresponding author), Univ Maryland, Dept Elect & Comp Engn, Ctr Automat Res, College Pk, MD 20742 USA.
RI Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/B-6573-2012
OI Sankaranarayanan, Aswin/0000-0003-0906-4046; Cevher,
   Volkan/0000-0002-5004-201X
CR ALI SM, 1966, J ROY STAT SOC B, V28, P131
   [Anonymous], 2001, Sequential Monte Carlo methods in practice
   Bar-Shalom Y., 1988, TRACKING DATA ASS
   BERNARDO JM, 1979, J R STAT SOC B, V41, P113
   Cevher V, 2005, IEEE T SIGNAL PROCES, V53, P1, DOI 10.1109/TSP.2004.838947
   CEVHER V, UNPUB IEEE T SIGNAL
   CEVHER V, 2005, P IEEE SSP 2005 BORD
   CEVHER V, 2004, P ICASSP 2004 MONTR
   CEVHER V, 2005, P ICASSP 2005 PHIL P
   Checka N, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P881
   CHELLAPPA R, 2004, P ICASSP 2004 MONTR
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   GATICAPEREZ D, 2003, P ICIP 2003 SEP 14 1
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Isard M., 2000, ACTIVE CONTOURS
   Jepson AD, 2003, IEEE T PATTERN ANAL, V25, P1296, DOI 10.1109/TPAMI.2003.1233903
   Johnson D. H., 1993, ARRAY SIGNAL PROCESS
   LEICHTER I, 2004, P CVPR 2004 JUN JUL
   Li BX, 2002, IEEE T IMAGE PROCESS, V11, P530, DOI 10.1109/TIP.2002.1006400
   Liggins ME, 1997, P IEEE, V85, P95, DOI 10.1109/JPROC.1997.554211
   Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034
   Ripley R.D., 1987, STOCHASTIC SIMULATIO
   Valaee S, 2004, IEEE T SIGNAL PROCES, V52, P1171, DOI 10.1109/TSP.2004.826168
   VERMAAK J, 2001, P ICCV 2001 JUL 7 14
   WAX M, 1985, IEEE T ACOUST SPEECH, V33, P387, DOI 10.1109/TASSP.1985.1164557
   Zhou SHK, 2004, IEEE T IMAGE PROCESS, V13, P1491, DOI 10.1109/TIP.2004.836152
   Zhou YF, 1999, IEEE T SIGNAL PROCES, V47, P2655, DOI 10.1109/78.790648
NR 30
TC 40
Z9 46
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 715
EP 727
DI 10.1109/TMM.2007.893340
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shih, TK
   Wang, TH
   Chang, CY
   Kao, TC
   Hamilton, D
AF Shih, Timothy K.
   Wang, Te-Hua
   Chang, Chih-Yung
   Kao, Tai-Chien
   Hamilton, Douglas
TI Ubiquitous e-learning with multimodal multimedia devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE augmented paper; distance learning; IEEE LOM; mobile devices; SCORM; Web
   service
AB The Sharable Content Object Reference Model (SCORM) is a set of specifications and guidelines for the representation and operation of asynchronous distance learning. Since it was announced in late 1990s, the reference model has been used by software developers and academics in the development of authoring tools, learning management systems, and repositories for distance learning content. To date, most e-learning systems have been based on multimedia and Web technologies on personal computers. Our project, Hard SCORM, advances the field by implementing an integrated system which allows learners to read SCORM-compliant textbooks using multimodal multimedia devices. Hard SCORM employs a pen-like optical character reader device (called Hyper Pen) as an input mechanism. A computer, a personal digital assistant, or a cellular phone can be used for user behavior supervision using the Hard SCORM Machine. With an authoring tool, specially designed tags are printed in textbooks and recognized by Hyper Pen for user navigation control. In this way, users can read hardcopy textbooks in a traditional manner while the process of reading conforms to the SCORM specification. Part of the implemented system (Pocket SCORM on PDA) received the 2005 Brandon Hall Excellence in Learning Awards. The system has also been used by an airline company for online security checking and a high school for online mobile learning.
C1 Tamkang Univ, Dept Comp Sci & Informat Engn, Taipei 25137, Taiwan.
   Natl Dong Hwa Univ, Inst Educ, Hualien, Taiwan.
   Univ Wisconsin, Acad ADL Co Lab, Madison, WI 53703 USA.
C3 Tamkang University; National Dong Hwa University; University of
   Wisconsin System; University of Wisconsin Madison
RP Shih, TK (corresponding author), Tamkang Univ, Dept Comp Sci & Informat Engn, Taipei 25137, Taiwan.
EM tshih@cs.tku.edu.tw; fatty@cs.tku.edu.tw; cychang@mail.tku.edu.tw;
   mkao@mail.ndhu.edu.tw; doug@academiccolab.org
CR Abdullah N.A., 2004, Proceedings of the fifteenth ACM conference on Hypertext hypermedia - HYPERTEXT '04, P183, DOI [10.1145/1012807.1012857, DOI 10.1145/1012807.1012857]
   *ADL SCORM, 2005, ADL SCORM 2004 DOC
   [Anonymous], 2005, 14841122003 IEEE
   [Anonymous], P 14 ACM C HYP HYP N
   ATIF Y, 2003, P 10 IEEE INT C EL C, V3, P1224
   Back M., 2001, ACM C HUMAN FACTORS, P23, DOI DOI 10.1145/365024.365031
   Chu CP, 2004, ITCC 2004: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, VOL 1, PROCEEDINGS, P156
   *IEEE, 2005, IEEE STAND LEARN OBJ
   Kazi SA, 2004, IEEE INTERNATIONAL CONFERENCE ON ADVANCED LEARNING TECHNOLOGIES, PROCEEDINGS, P12, DOI 10.1109/ICALT.2004.1357365
   Li ST, 2005, 19th International Conference on Advanced Information Networking and Applications, Vol 1, Proceedings, P221
   Lin NH, 2004, 24TH INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING SYSTEMS WORKSHOPS, PROCEEDINGS, P274, DOI 10.1109/ICDCSW.2004.1284043
   Liu XF, 2003, CCECE 2003: CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING, VOLS 1-3, PROCEEDINGS, P717
   Luff P., 2004, Computer Supported Cooperative Work Conference Proceedings, P523, DOI 10.1145/1031607.1031695
   Norrie MC, 2003, FOURTH INTERNATIONAL CONFERENCE ON WEB INFORMATION SYSTEMS ENGINEERING, PROCEEDINGS, P209
   OHara K., 1997, Proceedings of the SIGCHI conference on human factors in computing systems, P335
   Shih TK, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P325, DOI 10.1109/ICME.2004.1394194
   Shih TK, 2003, AINA 2003: 17TH INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION NETWORKING AND APPLICATIONS, P408
   Su JM, 2005, 19th International Conference on Advanced Information Networking and Applications, Vol 1, Proceedings, P209
   Vossen G, 2003, SEVENTH INTERNATIONAL DATABASE ENGINEERING AND APPLICATIONS SYMPOSIUM, PROCEEDINGS, P242, DOI 10.1109/IDEAS.2003.1214933
   WANG TH, 2005, P ACM MULT C SING NO
   Yang JTD, 2004, IEEE INTERNATIONAL CONFERENCE ON ADVANCED LEARNING TECHNOLOGIES, PROCEEDINGS, P609, DOI 10.1109/ICALT.2004.1357487
NR 21
TC 15
Z9 17
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 487
EP 499
DI 10.1109/TMM.2006.886265
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100005
DA 2024-07-18
ER

PT J
AU Wang, JJ
   Chng, ES
   Xu, CS
   Lu, HQ
   Tian, Q
AF Wang, Jinjun
   Chng, Engsiong
   Xu, Changsheng
   Lu, Hanqinq
   Tian, Qi
TI Generation of personalized music sports video using multimodal cues
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE event boundary detection; multimodality analysis; music sports video;
   semantic event detection; semi-automatic video editing; sports video
   processing
AB In this paper, we propose a novel automatic approach for personalized music sports video generation. Two research challenges are addressed, specifically the semantic sports video content extraction and the automatic music video composition. For the first challenge, we propose to use multimodal (audio, video, and text) feature analysis and alignment to detect the semantics of events in broadcast sports video. For the second challenge, we introduce the video-centric and music-centric music video composition schemes and proposed a dynamic-programming based algorithm to perform fully or semi-automatic generation of personalized music sports video. The experimental results and user evaluations are promising and show that our system's generated music sports video is comparable to professionally generated ones. Our proposed system greatly facilitates the music sports video editing task for both professionals and amateurs.
C1 Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   Inst Infocomm Res, Singapore 119613, Singapore.
   Chinese Acad Sci, Natl Lab Pattern Recognit, Beijing 100080, Peoples R China.
C3 Nanyang Technological University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R);
   Chinese Academy of Sciences; Institute of Automation, CAS
RP Wang, JJ (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM jjwang@pmail.ntu.edu.sg; aseschng@pmail.ntu.edu.sg;
   xucs@i2r.a-star.edu.sg; luhq@nlpr.ia.ac.cn; tian@i2r.a-star.edu.sg
RI chen, yue/JXW-9556-2024; xu, cj/HJZ-3488-2023; Eng-Siong,
   CHNG/ABH-6779-2020
OI Eng-Siong, CHNG/0000-0001-6257-7399
CR ADAMI N, 2003, P SPIE VCIP 03 JUL, P1296
   Assfalg E, 2003, COMPUT VIS IMAGE UND, V92, P285, DOI 10.1016/j.cviu.2003.06.004
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   BABAGUCHI N, 2003, P IEEE INT C IMAGE P, V1, P13
   Christel M. G., 1998, CHI 98. Human Factors in Computing Systems. CHI 98 Conference Proceedings, P171, DOI 10.1145/274644.274670
   Ekin A, 2003, IEEE IMAGE PROC, P21
   Foote J., 2002, P ACM MULTIMEDIA 200, P553, DOI DOI 10.1145/641007.641119
   GIRGENSOHN A, 2000, P UIST 00, P81
   Hua X. -S., 2004, PROC 12 ANN ACM INT, P472
   Hua X.S., 2003, P 11 ACM INT C MULTI, P490
   Hua XS, 2004, IEEE T CIRC SYST VID, V14, P572, DOI 10.1109/TCSVT.2004.826750
   Leonardi R, 2004, IEEE T CIRC SYST VID, V14, P634, DOI 10.1109/TCSVT.2004.826751
   LI BX, 2003, P INT C EL IM STOR R, P314
   MADDAGE NC, 2004, P 12 ANN ACM INT C M, P112, DOI DOI 10.1145/1027527.1027549
   *MUVEE TECHN PTE L, MUV AUTOPRODUCER
   NITTA N, 2002, P 8 INT WORKSH MULT, P110
   PEKER KA, 2000, P SOC PHOTO-OPT INS, V4676, P318
   Petrushin Valery A, 2000, P C ONLINE S ELECT E
   Pylkkonen Janne., 2004, Proceedings ofthe 8th International Conference on Spoken Language Processing (Interspeech 2004), P385
   RUI Y, 2002, P ACM MULTIMEDIA 02, P105
   Takagi S, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P461
   Tan YP, 2000, IEEE T CIRC SYST VID, V10, P133, DOI 10.1109/76.825867
   UKKONEN E, 1983, LECT NOTES COMPUT SC, V158, P487
   WANG J, 2005, P ACM MULTIMEDIA, P31
   WANG J, 2004, P ACM MULTIMEDIA, P31
   Wang JH, 2004, Proceedings of the World Engineers' Convention 2004: Vol D, Environment Protection and Disaster Mitigation, P599
   Wang JJ, 2004, IEEE IMAGE PROC, P1637
   XIE L, 2003, PATTERN RECOGNITI LE, V24, P767
   Xiong ZY, 2003, IEEE IMAGE PROC, P5
NR 29
TC 24
Z9 24
U1 2
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 576
EP 588
DI 10.1109/TMM.2006.888013
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100012
DA 2024-07-18
ER

PT J
AU Chattopadhyay, S
   Bhandarkar, SM
   Li, K
AF Chattopadhyay, Siddhartha
   Bhandarkar, Suchendra M.
   Li, Kang
TI Model-based power aware compression algorithms for MPEG-4 virtual human
   animation in mobile environments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE animation; mobile communication; multimedia communication; virtual
   reality
AB MPEG-4 body animation parameters (BAP) are used for animation of MPEG-4 compliant virtual human-like characters. Distributed virtual reality applications and networked games on mobile computers require access to locally stored or streamed compressed BAP data. Existing MPEG-4 RAP compression techniques are inefficient for streaming, or storing, RAP data on mobile computers, because: 1) MPEG-4 compressed BAP data entails a significant number of CPU cycles, hence significant, unacceptable power consumption, for the purpose of decompression; 2) the lossy MPEG-4 technique of frame dropping to reduce network throughput during streaming leads to unacceptable animation degradation; and 3) lossy MPEG-4 compression does not exploit structural information in the virtual human model. In this article, we propose two novel algorithms for lossy compression of RAP data, termed as BAP-Indexing and BAP-Sparsing. We demonstrate how an efficient combination of the two algorithms results in a lower network bandwidth requirement and reduced power for data decompression at the client end when compared to MPEG-4 compression. The algorithm exploits the structural information in the virtual human model, thus maintaining visually acceptable quality of the resulting animation upon decompression. Consequently, the hybrid algorithm for RAP data compression is ideal for streaming of motion animation data to power- and network- constrained mobile computers.
C1 Univ Georgia, Athens, GA 30602 USA.
C3 University System of Georgia; University of Georgia
RP Chattopadhyay, S (corresponding author), Univ Georgia, Athens, GA 30602 USA.
EM siddh@cs.uga.edu; suchi@cs.uga.edu; kangli@cs.uga.edu
RI chen, yian/IWM-4310-2023
CR [Anonymous], 1449621999 ISOIEC
   [Anonymous], 1449611999 ISOIEC
   AUBEL A, 1998, P 1 INT C VIRT WORLD, V1434, P14
   Barakonyi I, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P141, DOI 10.1109/ISMAR.2004.11
   CAPIN TK, 1999, P IWSNHC3DI 99 SANT
   CAPIN TK, 2000, P ICME 2000 NEW YORK
   CAPIN TK, 2000, P IEEE INT C MULT EX, V2
   CAPIN TK, P VRAIS 97, P161
   Capin TolgaK., 1999, Avatars in Networked Virtual Environments
   CAVAZZA M, 2003, P INT VIRT AG GERM
   Chandra S, 2003, MULTIMEDIA SYST, V9, P185, DOI 10.1007/s00530-003-0089-0
   CHATTOPADHYAY S, 2006, P ACM MULT COMP NETW
   CHATTOPADHYAY S, 2005, P IEEE INT C MULT CO, P104
   Endo M, 2003, IEEE COMPUT GRAPH, V23, P50, DOI 10.1109/MCG.2003.1159613
   Giacomo T. D, 2003, P INT C CYB, P221
   GUTIERREZ M, 2003, P 9 INT C MULT MOD M
   Hijiri T., 2000, Proceedings Web3D - VRML 2000. Fifth Symposium on the Virtual Reality Modeling Language, P95, DOI 10.1145/330160.330193
   JOSLIN C, 2000, P ACM S VIRT REAL SO, P25
   KRUPPA M, 2003, P SIMVIS, P349
   PAUL JMH, 2000, THESIS U TWENTE ENSC
   Preda M, 2005, 3D MODELING AND ANIMATION: SYNTHESIS AND ANALYSIS TECHNIQUES FOR THE HUMAN BODY, P27
   PREDA M, 2002, MPEG4 JUMP START, pCH9
   PREDA M, 2001, P EUROIMAGE INT C AU, P311
   STEMM M, 1996, P 3 INT WORKSH MOB M
   VACCHETTI L, 2003, P 3DIM 2003 BANFF AL
NR 25
TC 7
Z9 8
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 1
EP 8
DI 10.1109/TMM.2006.886326
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500001
DA 2024-07-18
ER

PT J
AU Ng, B
   Lau, RWH
   Si, A
   Li, FWB
AF Ng, B
   Lau, RWH
   Si, A
   Li, FWB
TI Multiserver support for large-scale distributed virtual environments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive region partitioning; distributed virtual environments;
   multiserver architecture
AB CyberWalk is a distributed virtual walkthrough system that we have developed. It allows users at different geographical locations to share information and interact within a shared virtual environment (VE) via a local network or through the Internet. In this paper, we illustrate that as the number of users exploring the VE increases, the server will quickly become the bottleneck. To enable good performance, CyberWalk utilizes multiple servers and employs an adaptive region partitioning technique to dynamically partition the whole VE into regions. All objects within each region will be managed by one server. Under normal circumstances, when a viewer is exploring a region, the server of that region will be responsible for serving all requests from the viewer. When a viewer is crossing the boundary of two or more regions, the servers of all the regions involved will be serving requests from the viewer since the viewer might be able to view objects within all these regions. This is analogous to evaluating a database query using a parallel database server, which could improve the performance of serving a viewer's request tremendously. We evaluate the performance of this multiserver architecture of CyberWalk via a detail simulation model.
C1 City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   Oracle Corp, Redwood Shores, CA 94065 USA.
   Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
C3 City University of Hong Kong; Oracle; Hong Kong Polytechnic University
RP City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM beatrice@cs.cityu.edu.hk; rynson@cs.cityu.edu.hk; antonio.si@oracle.com;
   csbor@conip.polyu.edu.hk
RI Li, Frederick W. B./AAM-6662-2021
OI Li, Frederick W. B./0000-0002-4283-4228; LAU, Rynson W
   H/0000-0002-8957-8129
CR [Anonymous], Everquest
   [Anonymous], P EUR
   CALVIN J, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P450, DOI 10.1109/VRAIS.1993.380745
   CARLSSON C, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P394, DOI 10.1109/VRAIS.1993.380753
   CASAVANT TL, 1988, IEEE T SOFTWARE ENG, V14, P141, DOI 10.1109/32.4634
   CHAN A, 2005, IN PRESS ACM T INTER, V5, P70
   Chan S, 2000, PROG BIOM O, V1, P23, DOI 10.1117/12.379577
   Chim J, 2003, IEEE T MULTIMEDIA, V5, P503, DOI 10.1109/TMM.2003.819094
   Chim J. H. P., 1998, Proceedings ACM Multimedia 98, P171, DOI 10.1145/290747.290769
   DAS TK, 1997, P ACM S VIRT REAL SO, P157
   DEWITT D, 1988, P ACM SIGMOD CHIC IL
   FALBY JS, 1993, COMPUT GRAPH, V17, P65, DOI 10.1016/0097-8493(93)90052-B
   FUNKHOUSER T, 1995, P S INT 3D GRAPH MON
   GREENHALGH C, 1995, INT CON DISTR COMP S, P27, DOI 10.1109/ICDCS.1995.499999
   Greenhalgh C., 2000, Proceedings of the third international conference on Collaborative virtual environments, P119, DOI DOI 10.1145/351006.351027
   HOPPE H, 1996, P ACM SIGGRAPH NEW O
   Hori M, 2001, 2001 IEEE PACIFIC RIM CONFERENCE ON COMMUNICATIONS, COMPUTERS AND SIGNAL PROCESSING, VOLS I AND II, CONFERENCE PROCEEDINGS, P200, DOI 10.1109/PACRIM.2001.953557
   Katzela I, 1996, IEEE PERS COMMUN, V3, P10, DOI 10.1109/98.511762
   Lea R., 1997, Proceedings VRML 97. Second Symposium on the Virtual Reality Modeling Language, P41, DOI 10.1145/253437.253451
   Singh G., 1995, Proceedings. Virtual Reality Annual International Symposium '95 (Cat. No.95CH35761), P19, DOI 10.1109/VRAIS.1995.512475
   Singh G., 1994, PRESENCE, V3, P19
   WILLEBEEKLEMAIR MH, 1993, IEEE T PARALL DISTR, V4, P979, DOI 10.1109/71.243526
   QUAKE
   ASHERONS CALL
   DIABLO II STARCRAFT
NR 25
TC 14
Z9 19
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1054
EP 1065
DI 10.1109/TMM.2005.858388
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200007
OA Green Submitted, Green Published, Green Accepted
DA 2024-07-18
ER

PT J
AU Erzin, E
   Yemez, Y
   Tekalp, AM
AF Erzin, E
   Yemez, Y
   Tekalp, AM
TI Multimodal speaker identification using an adaptive classifier cascade
   based on modality reliability
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE classifier combining; modality reliability; multimodal speaker
   identification
ID FUSION; FACE; COMBINATION; INFORMATION; SPEECH; VERIFICATION;
   RECOGNITION
AB We present a multimodal open-set speaker identification system that integrates information coming from audio, face and lip motion modalities. For fusion of multiple modalities, we propose a new adaptive cascade rule that favors reliable modality combinations through a cascade of classifiers. The order of the classifiers in the cascade is adaptively determined based on the reliability of each modality combination. A novel reliability measure, that genuinely fits to the open-set speaker identification problem, is also proposed to assess accept or reject decisions of a classifier. A formal framework is developed based on probability of correct decision for analytical comparison of the proposed adaptive rule with other classifier combination rules. The proposed adaptive rule is more robust in the presence of unreliable modalities, and outperforms the hard-level max rule and soft-level weighted summation rule, provided that the employed reliability measure is effective in assessment of classifier decisions. Experimental results that support this assertion are provided.
C1 Koc Univ, Coll Engn, Multimedia Vis & Graph Lab, TR-34450 Istanbul, Turkey.
C3 Koc University
RP Koc Univ, Coll Engn, Multimedia Vis & Graph Lab, TR-34450 Istanbul, Turkey.
EM eerzin@ku.edu.tr; yyemez@ku.edu.tr; mtekalp@ku.edu.tr
RI Tekalp, Murat/AAW-1060-2020; Erzin, Engin/H-1716-2011
OI Erzin, Engin/0000-0002-2715-2368; Tekalp, Ahmet
   Murat/0000-0003-1465-8121
CR Al-Ghoneim K, 1998, PATTERN RECOGN, V31, P2077, DOI 10.1016/S0031-3203(98)00030-2
   ALEXANDRE LA, 2000, P 15 ITN C PATT REC, V2, P3
   Altinçay H, 2000, SPEECH COMMUN, V30, P255, DOI 10.1016/S0167-6393(99)00054-0
   Altinçay H, 2003, PATTERN RECOGN LETT, V24, P1163, DOI 10.1016/S0167-8655(02)00286-6
   Ben-Yacoub S, 1999, IEEE T NEURAL NETWOR, V10, P1065, DOI 10.1109/72.788647
   BENEDIKTSSON JA, 1992, IEEE T SYST MAN CYB, V22, P688, DOI 10.1109/21.156582
   Bloch I, 1996, IEEE T SYST MAN CY A, V26, P52, DOI 10.1109/3468.477860
   BRUNELLI R, 1995, IEEE T PATTERN ANAL, V17, P955, DOI 10.1109/34.464560
   Campbell JP, 1997, P IEEE, V85, P1437, DOI 10.1109/5.628714
   Chatzis V, 1999, IEEE T SYST MAN CY A, V29, P674, DOI 10.1109/3468.798073
   Chaudhari UV, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P712
   Chaudhari UV, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P9
   CIVANLAR MR, 1996, P SPIE PHOT E NOV, P120
   ERZIN E, 2004, DSP IN VEHICLE MOBIL
   Frischholz RW, 2000, COMPUTER, V33, P64, DOI 10.1109/2.820041
   Garcia-Salicetti S, 2003, LECT NOTES COMPUT SC, V2688, P845
   Jourlin P, 1997, PATTERN RECOGN LETT, V18, P853, DOI 10.1016/S0167-8655(97)00070-6
   Kittler J, 2003, IEEE T PATTERN ANAL, V25, P110, DOI 10.1109/TPAMI.2003.1159950
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Neti C, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P619, DOI 10.1109/MMSP.2001.962801
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   RATHA NK, 2001, P INT C ADV PATT REC, P445
   Rogozan A., 1997, P EUR TUT WORKSH AUD, P61
   Sanderson C, 2003, PATTERN RECOGN, V36, P293, DOI 10.1016/S0031-3203(02)00031-6
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   VERMA A, 1999, AUTOMATIC SPEECH REC
   Wark T, 2001, DIGIT SIGNAL PROCESS, V11, P169, DOI 10.1006/dspr.2001.0397
   Yoma NB, 2002, IEEE T SPEECH AUDI P, V10, P158, DOI 10.1109/TSA.2002.1001980
   Zhang D., 2000, AUTOMATED BIOMETRICS
   Zhang J, 1997, P IEEE, V85, P1423, DOI 10.1109/5.628712
NR 30
TC 46
Z9 47
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 840
EP 852
DI 10.1109/TMM.2005.854464
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Vasconcelos, N
   Lippman, A
AF Vasconcelos, N
   Lippman, A
TI A multiresolution manifold distance for invariant image similarity
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affine transformations; face recognition; image similarity; invariance;
   manifold distance; multiresolution; robust estimators; semantic movie
   classification; tangent distance
ID CLASSIFICATION; RECOGNITION
AB Accounting for spatial image transformations is a requirement for multimedia problems such as video classification and retrieval, face/object recognition or the creation of image mosaics from video sequences. We analyze a transformation invariant metric recently proposed in the machine learning literature to measure the distance between image manifolds - the tangent distance (TD) - and show that it is closely related to alignment techniques from the motion analysis literature. Exposing these relationships results in benefits for the two domains. On one hand, it allows leveraging on the knowledge acquired in the alignment literature to build better classifiers. On the other, it provides a new interpretation of alignment techniques as one component of a decomposition that has interesting properties for the classification of video. In particular, we embed the TD into a multiresolution framework that makes it significantly less prone to local minima. The new metric - multiresolution tangent distance (MRTD) - can be easily combined with robust estimation procedures, and exhibits significantly higher invariance to image transformations than the TD and the Euclidean distance (ED). For classification, this translates into significant improvements in face recognition accuracy. For video characterization, it leads to a decomposition of image dissimilarity into "differences due to camera motion" plus "differences due to scene activity" that is useful for classification. Experimental results on a movie database indicate that the distance could be used as a basis for the extraction of semantic primitives such as action and romance.
C1 Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
   MIT, Media Lab, Cambridge, MA 02139 USA.
C3 University of California System; University of California San Diego;
   Massachusetts Institute of Technology (MIT)
RP Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM nuno@ece.ucsd.edu
RI cai, chao/C-4840-2009
OI Vasconcelos, Nuno/0000-0002-9024-4302
CR Aigrain P, 1996, MULTIMED TOOLS APPL, V3, P179, DOI 10.1007/BF00393937
   AMANDAN P, 1993, MOTION ANAL IMAGE SE, pCH1
   [Anonymous], 1981, P DARPA IM UND WORKS
   [Anonymous], J COGN NEUROSCI
   Bertsekas D. P., 1995, NONLINEAR PROGRAMMIN
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Devroye L., 1996, A probabilistic theory of pattern recognition
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Frey B. J., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P416, DOI 10.1109/CVPR.1999.786972
   Huber P., 1981, Robust Statistics
   Irani M, 1996, SIGNAL PROCESS-IMAGE, V8, P327, DOI 10.1016/0923-5965(95)00055-0
   MAO JC, 1992, PATTERN RECOGN, V25, P173, DOI 10.1016/0031-3203(92)90099-5
   MARTIN W, 1986, RECENT THEORIES NARR, pCH5
   Massey M, 1996, IBM SYST J, V35, P557, DOI 10.1147/sj.353.0557
   MONTEGOMERY D, 1992, INTRO LINEAR REGRESS
   MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486
   Ravela S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P608, DOI 10.1109/ICCV.1998.710780
   Rousseeuw P.J., 1987, ROBUST REGRESSION OU
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Rowley HA, 1998, PROC CVPR IEEE, P963, DOI 10.1109/CVPR.1998.698721
   Sawhney HS, 1996, IEEE T PATTERN ANAL, V18, P814, DOI 10.1109/34.531801
   Schiele B, 2000, INT J COMPUT VISION, V36, P31, DOI 10.1023/A:1008120406972
   Schneiderman H, 2000, PROC CVPR IEEE, P746, DOI 10.1109/CVPR.2000.855895
   SIMARD P, 1994, P NEUR INF PROC SYST
   SIMARD PY, 1994, INT C PATT RECOG, P262, DOI 10.1109/ICPR.1994.576916
   SMOLIAR S, 1996, MULTIMEDIA SYSTEMS T
   Sung KK, 1998, IEEE T PATTERN ANAL, V20, P39, DOI 10.1109/34.655648
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Szummer M, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P42, DOI 10.1109/CAIVD.1998.646032
   Vailaya A, 2001, IEEE T IMAGE PROCESS, V10, P117, DOI 10.1109/83.892448
   Vasconcelos N, 1998, PROC CVPR IEEE, P566, DOI 10.1109/CVPR.1998.698662
   Vasconcelos N, 2000, IEEE T IMAGE PROCESS, V9, P3, DOI 10.1109/83.817595
   VASCONCELOS N, 2000, THESIS MIT CAMBRIDGE
   WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981
   Weiss Y, 1997, PROC CVPR IEEE, P520, DOI 10.1109/CVPR.1997.609375
   ZHANG HJ, 1995, P SOC PHOTO-OPT INS, V2417, P389, DOI 10.1117/12.206066
NR 36
TC 36
Z9 43
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 127
EP 142
DI 10.1109/TMM.2004.840596
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zink, M
   Schmitt, J
   Steinmetz, R
AF Zink, M
   Schmitt, J
   Steinmetz, R
TI Layer-encoded video in scalable adaptive streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
AB Combining the concepts of caching and transmission control protocol (TCP)-friendly streaming of layer-encoded video bears the problem that those videos might not be cached in full quality. Therefore, we focus in this work on the scheduling of retransmissions of missing segments of a cached video in a manner that allows clients to receive the content in an improved quality. In a first step, we conducted subjective assessments of variations in layer-encoded video with the goal to validate existing quality metrics, including our own, which are based on certain assumptions. A statistical analysis of the subjective assessment validates these assumptions. We also show that the frequently used peak signal-to-noise ratio (PSNR) is not an appropriate metric for variations in layer-encoded video. With the insight from the subjective assessment we develop heuristics for retransmission scheduling and prove their applicability by conducting a series of simulations.
C1 Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.
   Tech Univ Darmstadt, Fac Elect Engn & Informat Technol, Multimedia Commun Lab, D-64283 Darmstadt, Germany.
C3 University of Massachusetts System; University of Massachusetts Amherst;
   Technical University of Darmstadt
RP Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.
EM zink@cs.umass.edu; jschmitt@informatik.uni-kl.de;
   ralf.steinmetz@kom.tu-darmstadt.de
RI Steinmetz, Patrick R. H./AAD-4093-2022
OI Steinmetz, Ralf/0000-0002-6839-9359; Zink, Michael/0000-0002-0309-9240
CR ALDRIDGE R, 1995, IEE P-VIS IMAGE SIGN, V142, P149, DOI 10.1049/ip-vis:19951937
   Aldridge RP, 1998, IEE P-VIS IMAGE SIGN, V145, P116, DOI 10.1049/ip-vis:19981843
   [Anonymous], P 11 INT WORKSH NETW
   [Anonymous], BT50010 ITUR
   BRADEN R, 1994, INFORMATIONAL RF JUN
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Garey M.R., 1979, COMPUTERS INTRACTABI
   Gringeri S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P113, DOI 10.1145/319463.319478
   Griwodz C, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P349, DOI 10.1145/266180.266386
   GRIWODZ C, 2000, THESIS DARNSTADT U T
   Hartung J., 1998, Proceedings ACM Multimedia 98, P419, DOI 10.1145/290747.290814
   HAYASHI T, 1999, 3 WORKSH MULT SIGN P, P515
   KRASIC C, 2001, ACM MULT DOCT S OTT
   Kuhmünch C, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P1013, DOI 10.1109/ICIP.2001.958668
   Lavington S, 2001, SIGNAL PROCESS-IMAGE, V16, P785, DOI 10.1016/S0923-5965(01)00010-8
   LEE JY, 1998, P ITC CSCC 98 JUL, P245
   Little T. D. C., 1994, IEEE Multimedia, V1, P14, DOI 10.1109/MMUL.1994.318978
   Neff R, 2002, IEEE T CIRC SYST VID, V12, P13, DOI 10.1109/76.981842
   Nelakuditi S., 2000, P 10 INT WORKSH NETW
   Pereira F, 1997, IEEE T CIRC SYST VID, V7, P32, DOI 10.1109/76.554416
   Rejaie R, 1999, COMP COMM R, V29, P189, DOI 10.1145/316194.316222
   Rejaie R., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P980, DOI 10.1109/INFCOM.2000.832273
   REJAIE R, 1999, P 18 ANN JOINT C IEE, P395
   Saparilla D., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P737, DOI 10.1109/INFCOM.2000.832248
   Tan WT, 1999, IEEE T MULTIMEDIA, V1, P172, DOI 10.1109/6046.766738
   Vicisano L, 1998, IEEE INFOCOM SER, P996, DOI 10.1109/INFCOM.1998.662909
   Zink M, 2000, SEVENTH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED SYSTEMS: WORKSHOPS, PROCEEDINGS, P281, DOI 10.1109/PADSW.2000.884594
   ZINK M, 2003, 11 INT WORKSH QUAL S
   ZINK M, 2002, P SPIE ACM C MULT CO, P61
   ZINK M, 2002, P INT C COMM 2002 IC
NR 30
TC 48
Z9 51
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 75
EP 84
DI 10.1109/TMM.2004.840595
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300008
DA 2024-07-18
ER

PT J
AU Clausen, M
   Kurth, F
AF Clausen, M
   Kurth, F
TI A unified approach to content-based and fault-tolerant music recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio identification; content-based indexing and retrieval; music
   information retrieval; music recognition; polyphonic search;
   transposition-invariant search
AB In this paper, we propose a unified approach to fast index-based music recognition. As an important area within the field of music information retrieval (MIR), the goal of music recognition is, given a database of musical pieces and a query document, to locate all occurrences of that document within the database, up to certain possible errors. In particular, the identification of the query with regard to the database becomes possible. The approach presented in this paper is based on a general algorithmic framework for searching complex patterns of objects in large databases. We describe how this approach may be applied to two important music recognition tasks: The polyphonic (musical score-based) search in polyphonic score data and the identification of pulse-code modulation audio material from a given acoustic waveform. We give an overview on the various aspects of our technology including fault-tolerant search methods. Several areas of application are suggested. We describe several prototypic systems we have developed for those applications including the notify! and the audentify! systems for score- and waveform-based music recognition, respectively.
C1 Univ Bonn, Dept Comp Sci 3, D-53117 Bonn, Germany.
C3 University of Bonn
RP Univ Bonn, Dept Comp Sci 3, D-53117 Bonn, Germany.
EM clausen@cs.uni-bonn.de; frank@cs.uni-bonn.de
OI Kurth, Frank/0000-0002-9992-083X; Clausen, Michael/0000-0001-5339-6406
CR ALLMNACHE E, 2001, P 110 AES CONV AMST
   Alt Helmut., 1999, HDB COMPUTATIONAL GE, P121
   [Anonymous], 1977, Signal Analysis
   [Anonymous], 1999, Compressing and Indexing Documents and Images
   BARLOW H, 1991, DICT MUSICAL THEMES
   CANO P, 2002, P 112 AES CONV MUN G
   CLAUSEN M, 2000, P INT S MUS INF RETR
   Dovey M. J., 1999, P AISB 99 S MUS CREA, P48
   DOVEY MJ, 2001, P INT S MUS INF RETR
   Fragoulis D, 2001, IEEE T SIGNAL PROCES, V49, P898, DOI 10.1109/78.912932
   HAITSMA J, 2001, P 2 INT WORKSH CONT
   KURTH F, 2001, P 110 AES CONV AMST
   KURTH F, 2002, P 112 AES CONV MUN G
   LEMSTROM K, 2000, P INT S MUS INF RETR
   MacWilliams F. J., 1978, The Theory of Error-Correcting Codes
   Meredith David, 2001, P 5 WORLD MULT SYST, VX, P61
   VONZURGATHEN J, 2002, MODERN COMPUTER ALGR
   Wolfson HJ, 1997, IEEE COMPUT SCI ENG, V4, P10, DOI 10.1109/99.641604
   Zolzer U.:., 1997, Digital Audio Signal Processing
NR 19
TC 26
Z9 28
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2004
VL 6
IS 5
BP 717
EP 731
DI 10.1109/TMM.2004.834859
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 854XI
UT WOS:000223936800005
DA 2024-07-18
ER

PT J
AU Li, Q
   van der Schaar, M
AF Li, Q
   van der Schaar, M
TI Providing adaptive QoS to layered video over wireless local area
   networks through real-time retry limit adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive control; cross layer design; home communication system;
   multimedia communication; queueing analysis; wireless LAN
AB Robust streaming of video over 802.11 wireless LANs (WLANs) poses many challenges, including coping with packets losses caused by network buffer overflow or link erasures. In this paper, we propose a novel error protection method that can provide adaptive quality-of-service (QoS) to layered coded video by utilizing priority queueing at the network layer and retry-limit adaptation at the link layer. The design of our method is motivated by the observation that the retry limit settings of the MAC layer can be optimized in such a way that the overall packet losses that are caused by either link erasure or buffer overflow are minimized. We developed a real-time retry limit adaptation algorithm to trace the optimal retry limit for both the single-queue (or single-layer) and multiqueue (or multilayer) cases. The video layers are unequally protected over the wireless link by the MAC with different retry limits. In our proposed transmission framework, these retry limits are dynamically adapted depending on the wireless channel conditions and traffic characteristics. Furthermore, the proposed priority queueing discipline is enhanced with packet filtering and purging functionalities that can significantly save bandwidth by discarding obsoleted or un-decodable packets from the buffer. Simulations show that the proposed cross-layer protection mechanism can significantly improve the received video quality.
C1 Philips Res USA, Briarcliff Manor, NY 10510 USA.
   Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA.
C3 Philips; Philips Research; University of California System; University
   of California Davis
RP BrainMedia, New York, NY 10010 USA.
EM qiongli@ieee.org; mvanderschaar@ece.ucdavis.edu
CR Brockett R. W., 1999, Proceedings of the 38th IEEE Conference on Decision and Control (Cat. No.99CH36304), P3077, DOI 10.1109/CDC.1999.831407
   Chou PA, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P587, DOI 10.1109/MMSP.2001.962796
   Cote G., 2001, Compressed Video Over Networks
   GIROD B, 2001, WIRLESS VIDEO BOOK C
   GONG W, 2002, P 42 ANN ALL C COMM
   Gross J., 2002, P EUR WIR 2002 FEB, P762
   *ISO IEC IEE, 1999, 8802111999E ISOIEC I
   KLEINROCK L, 1996, QUEUEING SYSTEMS, V2
   KLEINROCK L, 1995, QUEUEING SYSTEMS, V1
   Li SQ, 1993, IEEE ACM T NETWORK, V1, P522, DOI 10.1109/90.251911
   MA H, 1999, P PACK VID WORKSH 99
   Majumdar A, 2002, IEEE T CIRC SYST VID, V12, P524, DOI 10.1109/TCSVT.2002.800315
   MIAO ZR, 2002, 12 INT PACK VID WORK
   Postel J., 1980, RFC768
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   Shan Y, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P277
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Yan AL, 1999, IEEE T INFORM THEORY, V45, P1588, DOI 10.1109/18.771162
   1996, DARPA FUNDED VINT PR
NR 19
TC 131
Z9 149
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 278
EP 290
DI 10.1109/TMM.2003.822792
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xiang, Z
   Zhang, Q
   Zhu, WW
   Zhang, ZS
   Zhang, YQ
AF Xiang, Z
   Zhang, Q
   Zhu, WW
   Zhang, ZS
   Zhang, YQ
TI Peer-to-peer based multimedia distribution service
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE load balance; multimedia distribution service; peer-to-peer;
   replication; topology-aware
ID NETWORKS
AB Recently, there are many research interests in providing efficient and scalable multimedia distribution service. However, stringent quality-of-service (QoS) requirements for media distribution, as well as dynamically changing and heterogeneous network capacity in today's best effort Internet, bring many challenges. In this paper, we introduce a novel framework for multimedia distribution service based on peer-to-peer (P2P) networks. A topology-aware overlay is proposed in which hosts self-organize into groups. End hosts within the same group have similar network conditions and can easily collaborate with each other to achieve QoS awareness. In order to improve media delivery quality and provide high service availability, we further propose two distributed heuristic replication strategies, intergroup replication and intragroup replication, based on this topology-aware overlay. Specifically, intergroup replication is aimed to improve the efficiency of media content delivery between the group where a request is issued and the group where the content is stored. Also, intragroup replication is targeted at improving the availability of the content. Extensive simulation results show that the latency in our proposed architecture is 20% less than that of the FreeNet and 50% less than that of the randomly replication system. Simulation results also show that the video quality in our system is much better than that in the other two systems. Our P2P-based approach is also distributed, scalable, cost effective, and aware of the performance.
C1 Microsoft Res Asia, Beijing 100080, Peoples R China.
   San Diego Res Ctr, San Diego, CA 92108 USA.
C3 Microsoft; Microsoft Research Asia
RP Microsoft Res Asia, Beijing 100080, Peoples R China.
EM xiangzhe@tsinghua.org.cn; qianz@microsoft.com; wwzhu@microsoft.com;
   zzhang@ieee.org; yzhang@microsoft.com
RI Zhang, Qian/B-9058-2009
OI Zhang, Qian/0000-0001-9205-1881
CR Barabási AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Chen Y, 2002, LECT NOTES COMPUT SC, V2429, P306
   COHEN R, 2002, P IEEE INF 02
   JIN J, MIDDLEWARE 2003
   Kangasharju J, 2002, COMPUT COMMUN, V25, P376, DOI 10.1016/S0140-3664(01)00409-1
   KANT K, ICNP 2001
   LI SP, 1999, MPEG99M5583 ISOIEC J
   Lv Q, 2002, LECT NOTES COMPUT SC, V2429, P94
   PADMANABHAN VN, 2002, MSRTR200237
   QIU L, P IEEE INF, P1587
   Ratnasamy S, 2002, IEEE INFOCOM SER, P1190, DOI 10.1109/INFCOM.2002.1019369
   SRIPANIDKULCHAI K, 2001, POPULARITY GNETELLA
   Tran DA, 2004, IEEE J SEL AREA COMM, V22, P121, DOI 10.1109/JSAC.2003.818803
   TRAN DA, 2002, P ACM MULT C SIGMM 2, P247
   WAXMAN BM, 1988, IEEE J SEL AREA COMM, V6, P1617, DOI 10.1109/49.12889
   XIANG Z, 2001, IEEE GLOB 01 SAN ANT
   XU D, 2002, P IEEE INT C DISTR C
   ZHANG Q, 2001, TESTING SCHEME QOS P
   Zhang XY, 2004, IEEE J SEL AREA COMM, V22, P18, DOI 10.1109/JSAC.2003.818780
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
NR 20
TC 50
Z9 71
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 343
EP 355
DI 10.1109/TMM.2003.822819
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400013
DA 2024-07-18
ER

PT J
AU Pei, SC
   Chou, YZ
AF Pei, SC
   Chou, YZ
TI Novel error concealment method with adaptive prediction to the abrupt
   and gradual scene changes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error concealment; MPEG-2; scene change detection; video communication
ID HIGH-QUALITY VIDEO; COMPRESSED VIDEO; ATM NETWORKS; MPEG VIDEO;
   TRANSMISSION; INTERPOLATION; INFORMATION; ALGORITHM; IMAGES; RECOVERY
AB In this paper, the impact of the scene change on the conventional error concealment method is addressed and a novel error concealment method is proposed to improve the insufficiency of conventional temporal error concealment algorithm due to the occurrence of scene change. Combining with the low complexity scene change detection algorithm using macroblock type information, the corrupt blocks resulting from bit errors are concealed either temporally or spatially depending on whether or not an abrupt scene change is found. In the case of gradual scene change, a novel error concealment method of interpolation and extrapolation is proposed to utilize the linear property of gradual scene change sequence, and effectively reduce the concealment error in comparison with the conventional algorithm. Great improvement about 3 to 5 dB PSNR in average and 6 to 8 dB in some cases is obtained with very little overhead memory and computation.
C1 Natl Taiwan Univ, Dept Elect Engn, Taipei 10617, Taiwan.
C3 National Taiwan University
RP Pei, SC (corresponding author), Natl Taiwan Univ, Dept Elect Engn, Taipei 10617, Taiwan.
EM pei@cc.ee.ntu.edu.tw
CR Al-Mualla M, 1999, ELECTRON LETT, V35, P215, DOI 10.1049/el:19990174
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   ARMAN F, 1993, P SOC PHOTO-OPT INS, V1908, P2, DOI 10.1117/12.143638
   Arman F., 1993, Proceedings ACM Multimedia 93, P267, DOI 10.1145/166266.166297
   ARMAN F, 1994, ACM MULTIMEDIA 94, P97
   Arnold JF, 1999, SIGNAL PROCESS-IMAGE, V14, P607, DOI 10.1016/S0923-5965(98)00059-9
   Boyce JM, 1999, SIGNAL PROCESS-IMAGE, V15, P7, DOI 10.1016/S0923-5965(99)00021-1
   Bystrom M, 1999, IEEE T CIRC SYST VID, V9, P868, DOI 10.1109/76.785725
   CHANG SF, 1995, IEEE J SEL AREA COMM, V13, P1, DOI 10.1109/49.363151
   Chen MJ, 1997, IEEE T CIRC SYST VID, V7, P560, DOI 10.1109/76.585936
   Chung YJ, 1999, IEEE T CIRCUITS-II, V46, P951, DOI 10.1109/82.775393
   Feng J, 1997, IEEE T CONSUM ELECTR, V43, P183, DOI 10.1109/30.585539
   Ford RM, 2000, MULTIMEDIA SYST, V8, P37, DOI 10.1007/s005300050003
   Hong MC, 1999, SIGNAL PROCESS-IMAGE, V14, P473, DOI 10.1016/S0923-5965(98)00061-7
   Kaiser S, 1999, SIGNAL PROCESS-IMAGE, V14, P655, DOI 10.1016/S0923-5965(98)00066-6
   Keck W, 1996, IEEE T CONSUM ELECTR, V42, P411, DOI 10.1109/30.536138
   KWOK W, 1993, IEEE T CONSUM ELECTR, V39, P455, DOI 10.1109/30.234620
   LAM WM, 1995, IEEE T IMAGE PROCESS, V4, P533, DOI 10.1109/83.382489
   Lee PJ, 1999, IEEE T CONSUM ELECTR, V45, P851, DOI 10.1109/30.793622
   LEE XB, 1995, IEEE T IMAGE PROCESS, V4, P259, DOI 10.1109/83.366475
   MENG JH, 1995, P SOC PHOTO-OPT INS, V2419, P14, DOI 10.1117/12.206359
   NAGASAKA A, 1991, VISUAL DATABASE SYST, V2, P119
   NAKAJIMA H, 1994, ELECTRON COMM JPN 3, V77, P12
   Otsuji K., 1993, Proceedings ACM Multimedia 93, P251, DOI 10.1145/166266.166295
   Park JW, 1999, IEEE T CIRC SYST VID, V9, P1003, DOI 10.1109/76.795052
   Parthasarathy V, 1999, IEEE T IMAGE PROCESS, V8, P361, DOI 10.1109/83.748891
   Parthasarathy V, 1997, IEEE T CIRC SYST VID, V7, P358, DOI 10.1109/76.564113
   PATEL, 1996, P I EL ENG VIS IM SI
   Patel NV, 1997, PATTERN RECOGN, V30, P583, DOI 10.1016/S0031-3203(96)00114-8
   PEI SC, 2000, P ICIP SEP, V3, P953
   Pei SC, 1999, IEEE T MULTIMEDIA, V1, P321, DOI 10.1109/6046.807952
   REDMILL DW, 1998, ELECT COMMUN ENG AUG
   Shyu HC, 1999, IEEE T CIRC SYST VID, V9, P937, DOI 10.1109/76.785732
   Suh JW, 1997, IEEE T CONSUM ELECTR, V43, P295, DOI 10.1109/30.628616
   SUN HF, 1995, IEEE T IMAGE PROCESS, V4, P470, DOI 10.1109/83.370675
   Wang JT, 1999, IEEE T CIRC SYST VID, V9, P513, DOI 10.1109/76.754780
   Wang Z, 1998, IEEE T IMAGE PROCESS, V7, P1056, DOI 10.1109/83.701166
   Wei Q, 1997, P SOC PHOTO-OPT INS, V3022, P448, DOI 10.1117/12.263434
   YEO, 1996, P SPIE DIG VID COMPR, V2668, P58
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   Yu GS, 1998, IEEE T CIRC SYST VID, V8, P422, DOI 10.1109/76.709409
   Zeng WJ, 1999, IEEE T CIRC SYST VID, V9, P648, DOI 10.1109/76.767129
   Zhang H., 1995, Multimedia Tools and Applications, V1, P89, DOI 10.1007/BF01261227
   Zhu WW, 1998, IEEE T CIRC SYST VID, V8, P713, DOI 10.1109/76.728413
NR 44
TC 23
Z9 29
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 158
EP 173
DI 10.1109/TMM.2003.819749
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200012
DA 2024-07-18
ER

PT J
AU Kuo, CJ
   Lin, TG
   Huang, RS
   Odeh, SF
AF Kuo, CJ
   Lin, TG
   Huang, RS
   Odeh, SF
TI Facial model estimation from stereo/mono image sequence
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE facial model estimation
ID VIDEO CODING STANDARDS; MOTION
AB Facial model coding is an integral part in MPEG-4 related applications. The generation of the facial model usually requires stereoscopic view of the face in the pre-processing stage. Although facial model can be successfully estimated from two stereo facial images, the occlusion effect and imprecise location of the feature point prohibit obtaining an accurate facial model. In this paper, several facial model estimation (FME) algorithms are proposed in order to find the precise facial model from a stereo or mono image sequence. Since a sequence of images is used to find the facial model, the problem of occlusion effects is less serious. An accurate facial model (within 7.21% error) can still be obtained by our schemes, even without the prior information on the three-dimensional (3-D) position of the head with respect to the camera and the rotation axis/angle of the head's movement. This is the largest error of all FME algorithms presented in this paper when the subject does not wear eyeglasses. In addition, our schemes do not require precise camera parameters and avoid tedious camera calibration, thereby, simplifying the facial model extraction.
C1 Delta Elect Inc, Taoyuan 333, Taiwan.
   Ind Technol Res Inst, Optoelect & Syst Labs, Hsinchu, Taiwan.
   Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.
   Univ Jordan, Dept Comp Engn, Amman 11942, Jordan.
C3 Delta Electronics; Industrial Technology Research Institute - Taiwan;
   University of California System; University of California San Diego;
   University of Jordan
RP Delta Elect Inc, Taoyuan 333, Taiwan.
EM chung.kuo@delta.com.tw
RI Kuo, Chung-Jen/HTO-0059-2023; Huang, Ruey-Song/GZL-0985-2022
OI Huang, Ruey-Song/0000-0003-0385-9358
CR Aizawa K., 1989, Signal Processing: Image Communication, V1, P139, DOI 10.1016/0923-5965(89)90006-4
   AKIMOTO T, 1993, IEEE COMPUT GRAPH, V13, P16, DOI 10.1109/38.232096
   [Anonymous], 1992, R. woods digital image processing
   BEYMER D, 1993, AIM1431 MIT AI
   CHANG SC, 1994, IEEE T CIRCUITS SYST, V4, P257
   Chen XM, 1997, P SOC PHOTO-OPT INS, V3021, P212, DOI 10.1117/12.263514
   CHOW G, 1993, PATTERN RECOGN, V26, P1739, DOI 10.1016/0031-3203(93)90173-T
   Chung JM, 1996, ROBOTICA, V14, P269, DOI 10.1017/S0263574700019585
   Galicia G., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P603, DOI 10.1109/ICIP.1995.537551
   Guenin BM, 1998, P IEEE SEMICOND THER, P55, DOI 10.1109/STHERM.1998.660387
   *ISO IEC, 1998, JTC1SC29WG11N2196 IS
   Kuo CJ, 2000, IEEE T CIRC SYST VID, V10, P813, DOI 10.1109/76.856459
   Kuo CJ, 2002, IEEE T CIRC SYST VID, V12, P183, DOI 10.1109/76.993439
   Lam KM, 1996, PATTERN RECOGN, V29, P771, DOI 10.1016/0031-3203(95)00119-0
   Lengagne R, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P301, DOI 10.1109/AFGR.1996.557281
   OSTERMANN J, 1994, SIGNAL PROCESS-IMAGE, V6, P143, DOI 10.1016/0923-5965(94)90012-4
   RICHARDS W, 1985, J OPT SOC AM A, V2, P343, DOI 10.1364/JOSAA.2.000343
   SCHAFER R, 1995, P IEEE, V83, P907, DOI 10.1109/5.387092
   Shih SW, 1998, IEEE T SYST MAN CY A, V28, P426, DOI 10.1109/3468.686704
   Sikora T, 1997, IEEE SIGNAL PROC MAG, V14, P82, DOI 10.1109/79.618010
   TANG L, 1996, P IEEE INT C IM PROC, P467
   Yokoyama T, 1996, RO-MAN '96 - 5TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN COMMUNICATION, PROCEEDINGS, P335, DOI 10.1109/ROMAN.1996.568859
NR 22
TC 7
Z9 7
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2003
VL 5
IS 1
BP 8
EP 23
DI 10.1109/TMM.2003.808815
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 675HP
UT WOS:000182688200002
DA 2024-07-18
ER

PT J
AU Aafaq, N
   Mian, A
   Akhtar, N
   Liu, W
   Shah, M
AF Aafaq, Nayyer
   Mian, Ajmal
   Akhtar, Naveed
   Liu, Wei
   Shah, Mubarak
TI Dense Video Captioning With Early Linguistic Information Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Context modeling; dense video captioning; event localisation; language
   and vision; video captioning
ID MODELS
AB Dense captioning methods generally detect events in videos first and then generate captions for the individual events. Events are localized solely based on the visual cues while ignoring the associated linguistic information and context. Whereas end-to-end learning may implicitly take guidance from language, these methods still fall short of the power of explicit modeling. In this paper, we propose a Visual-Semantic Embedding (ViSE) Framework that models the word(s)-context distributional properties over the entire semantic space and computes weights for all the n-grams such that higher weights are assigned to the more informative n-grams. The weights are accounted for in learning distributed representations of all the captions to construct a semantic space. To perform the contextualization of visual information and the constructed semantic space in a supervised manner, we design Visual-Semantic Joint Modeling Network (VSJM-Net). The learned ViSE embeddings are then temporally encoded with a Hierarchical Descriptor Transformer (HDT). For caption generation, we exploit a transformer architecture to decode the input embeddings into natural language descriptions. Experiments on the large-scale ActivityNet Captions dataset and YouCook-II dataset demonstrate the efficacy of our method.
C1 [Aafaq, Nayyer; Mian, Ajmal; Akhtar, Naveed; Liu, Wei] Univ Western Australia, Dept Comp Sci & Software Engn, Perth, WA 6009, Australia.
   [Mian, Ajmal] Univ Cent Florida, Orlando, FL 32826 USA.
C3 University of Western Australia; State University System of Florida;
   University of Central Florida
RP Aafaq, N (corresponding author), Univ Western Australia, Dept Comp Sci & Software Engn, Perth, WA 6009, Australia.
EM nayyer.aafaq@research.uwa.edu.au; ajmal.mian@uwa.edu.au;
   naveed.akhtar@uwa.edu.au; wei.liu@uwa.edu.au; shah@crcv.ucf.edu
RI Sahoo, Sarat Kumar/J-8765-2014; AKHTAR, NAVEED/AAT-1283-2020
OI Sahoo, Sarat Kumar/0000-0001-5734-6844; AKHTAR,
   NAVEED/0000-0003-3406-673X; Shah, Mubarak/0000-0001-6172-5572; Liu,
   Wei/0000-0002-7409-0948; Mian, Ajmal/0000-0002-5206-3842
FU Australian Research Council Discovery Project [DP190102443]; Office of
   National Intelligence National Intelligence Postdoctoral Grant -
   Australian Government [NIPG-2021-001]
FX This work is supported in part by the Australian Research Council
   Discovery Project under Grant DP190102443. The work of Naveed Akhtar was
   supported by the Office of National Intelligence National Intelligence
   Postdoctoral Grant NIPG-2021-001funded by the Australian Government.The
   associate editor coordinating the review of this manuscript and
   approving it for publicationwas Dr. Ting Yao.
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Aafaq N, 2020, ACM COMPUT SURV, V52, DOI 10.1145/3355390
   [Anonymous], 1957, STUDIES LINGUISTIC A
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339
   Bojanowski P., 2017, Transactions of the Association for Computational Linguistics, V5, P135, DOI DOI 10.1162/TACLA00051
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   CHURCH KW, 1990, 27TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P76
   Das P, 2013, PROC CVPR IEEE, P2634, DOI 10.1109/CVPR.2013.340
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan X., 2018, ADV NEURAL INFORM PR, P3059
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425
   Fujita Soichiro, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P517, DOI 10.1007/978-3-030-58539-6_31
   Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gella S, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P968
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Gorban A., 2015, THUMOS challenge: Action recognition with a large number of classes
   Hao WL, 2018, AAAI CONF ARTIF INTE, P6894
   Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Iashin Vladimir, 2020, P IEEE CVF C COMP VI, P958
   Jae Sung Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P360, DOI 10.1007/978-3-030-58589-1_22
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Jonghwan Mun, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6581, DOI 10.1109/CVPR.2019.00675
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krishnamoorthy N., 2013, P AAAI C ART INT, P541
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu S, 2021, IEEE T PATTERN ANAL, V43, P3259, DOI 10.1109/TPAMI.2019.2940007
   Luowei Zhou, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6571, DOI 10.1109/CVPR.2019.00674
   Ma CY, 2018, PROC CVPR IEEE, P6790, DOI 10.1109/CVPR.2018.00710
   Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214
   Montes A., 2016, P 1 NIPS WORKSH LARG
   Oppenheim A. V., 1999, DISCRETE TIME SIGNAL, DOI DOI 10.1049/EP.1977.0078
   Pagliardini M., 2018, P C N AM CHAPT ASS C, P528, DOI [10.18653/v1/n18-1049, DOI 10.18653/V1/N18-1049]
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Rahman T, 2019, IEEE I CONF COMP VIS, P8907, DOI 10.1109/ICCV.2019.00900
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tian YC, 2013, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2013.341
   Turney PD, 2010, J ARTIF INTELL RES, V37, P141, DOI 10.1613/jair.2934
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2015, P ANN C N AM CHAPT A, P1494, DOI 10.3115/v1/N15-1173
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang JW, 2018, PROC CVPR IEEE, P7190, DOI 10.1109/CVPR.2018.00751
   Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Wang YF, 2017, PROC CVPR IEEE, P7378, DOI 10.1109/CVPR.2017.780
   Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362
   Wu X, 2018, PROC CVPR IEEE, P6829, DOI 10.1109/CVPR.2018.00714
   Xiong YL, 2018, LECT NOTES COMPUT SC, V11215, P489, DOI 10.1007/978-3-030-01252-6_29
   Xu HJ, 2019, IEEE WINT CONF APPL, P396, DOI 10.1109/WACV.2019.00048
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yang ZL, 2019, ADV NEUR IN, V32
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yu G, 2015, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2015.7298735
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zhang JC, 2020, IEEE T IMAGE PROCESS, V29, P6209, DOI 10.1109/TIP.2020.2988435
   Zhang T., 2020, PROC INT C LEARN REP
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zheng Q., 2020, 2020 CVPR, P13093, DOI 10.1109/CVPR42600.2020.01311
   Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 93
TC 10
Z9 10
U1 2
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2309
EP 2322
DI 10.1109/TMM.2022.3146005
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100055
DA 2024-07-18
ER

PT J
AU Anmulwar, S
   Wang, N
   Huynh, VSH
   Bryant, S
   Yang, JZ
   Tafazolli, RR
AF Anmulwar, Sweta
   Wang, Ning
   Vu San Ha Huynh
   Bryant, Stewart
   Yang, Jinze
   Tafazolli, Regius Rahim
TI HoloSync: Frame Synchronisation for Multi-Source Holographic
   Teleportation Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Synchronization; Teleportation; Production; Servers; Receivers;
   Internet; Point cloud compression; Extended Reality (XR); Frame
   synchronisation; Multi-Source; Teleportation; Holographic-type
   Communication; Edge-computing
ID VIRTUALIZATION; CHALLENGES
AB Live holographic teleportation is an emerging media application that allows Internet users to communicate in a fully immersive environment. One distinguishing feature of such an application is the ability to teleport multiple objects from different network locations into the receiver's field of view at the same time, mimicking the effect of group-based communications in a common physical space. In this case, live teleportation frames originated from different sources must be precisely synchronised at the receiver side to ensure user experiences with eliminated perception of motion misalignment effect. For the very first time in the literature, we quantify the motion misalignment between remote sources with different network contexts in order to justify the necessity of such frame synchronisation operations. Based on this motivation, we propose HoloSync, a novel edge-computing-based scheme capable of achieving controllable frame synchronisation performances for multi-source holographic teleportation applications. We carry out systematic experiments on a real system with the HoloSync scheme in terms of frame synchronisation performances in specific network scenarios, and their sensitivity to different control parameters.
C1 [Anmulwar, Sweta; Wang, Ning; Vu San Ha Huynh; Bryant, Stewart; Tafazolli, Regius Rahim] Univ Surrey, Inst Commun Syst, Guildford GU4 7FJ, Surrey, England.
   [Yang, Jinze] Huawei Technol Co & Ltd, Shenzhen 100095, Guangdong, Peoples R China.
C3 University of Surrey; Huawei Technologies
RP Wang, N (corresponding author), Univ Surrey, Inst Commun Syst, Guildford GU4 7FJ, Surrey, England.
EM s.anmulwar@surrey.ac.uk; n.wang@surrey.ac.uk; v.huynh@surrey.ac.uk;
   s.bryant@surrey.ac.uk; yangjinze@huawei.com; r.tafazolli@surrey.ac.uk
FU 5G/6GIC Innovation Centre industry
FX This work was supported by 5G/6GIC Innovation Centre industry.
CR Anmulwar S, 2021, 2021 IEEE 32ND ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR AND MOBILE RADIO COMMUNICATIONS (PIMRC), DOI 10.1109/PIMRC50174.2021.9569710
   [Anonymous], 2018, ETSI White Paper
   [Anonymous], 2020, Tech. Rep. ITU-T FG-NET-2030
   [Anonymous], 2020, ITU-T Technical report
   [Anonymous], Microsoft hololens
   [Anonymous], 2021, 5G low latency requirements
   Clemm A, 2020, IEEE COMMUN MAG, V58, P93, DOI 10.1109/MCOM.001.1900272
   Ge C, 2017, IEEE T MULTIMEDIA, V19, P2222, DOI 10.1109/TMM.2017.2735301
   Han B, 2020, MOBICOM '20: PROCEEDINGS OF THE 26TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM 2020), P137, DOI 10.1145/3372224.3380888
   Han B, 2015, IEEE COMMUN MAG, V53, P90, DOI 10.1109/MCOM.2015.7045396
   itu.int, FG-NET-2030
   Jeong JB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3687, DOI 10.1145/3394171.3413712
   Jun SH, 2017, ICEC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ELECTRONIC COMMERCE, DOI 10.1145/3154943.3154947
   Karn NK, 2018, MULTIMED TOOLS APPL, V77, P22965, DOI 10.1007/s11042-018-5744-8
   Kowalski M, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P318, DOI 10.1109/3DV.2015.43
   Li R., 2019, P 3 ANN ITU IMT 2020
   Li R, 2018, P 3 ANN ITU IMT 2020, P1
   Maimone A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073624
   Mann S., 2018, arXiv
   Mijumbi R, 2016, IEEE COMMUN SURV TUT, V18, P236, DOI 10.1109/COMST.2015.2477041
   MPEG-DASH, 2022, about us
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Park J., 2018, P IEEE GLOB COMM C, P1
   Park J, 2019, IEEE J EM SEL TOP C, V9, P149, DOI 10.1109/JETCAS.2019.2898622
   Perkins C., 2021, RFC 8834
   Qian F, 2019, HOTMOBILE '19 - PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS, P135, DOI 10.1145/3301293.3302358
   Qualcomm, 2019, Everything you need to know about 5G
   Pham QV, 2020, IEEE ACCESS, V8, P116974, DOI 10.1109/ACCESS.2020.3001277
   Ramadan E, 2021, 5G-MEMU '21: PROCEEDINGS OF THE 1ST WORKSHOP ON 5G MEASUREMENTS, MODELING, AND USE CASES, P27, DOI 10.1145/3472771.3474036
   Schierl T., 2010, RFC 6051
   Selinis I, 2020, 2020 IFIP NETWORKING CONFERENCE AND WORKSHOPS (NETWORKING), P136
   sprint.net, about us
   van der Hooft J, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123081
   van der Hooft J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2405, DOI 10.1145/3343031.3350917
   Vook FW, 2018, CONF REC ASILOMAR C, P1247, DOI 10.1109/ACSSC.2018.8645228
   webRTC, 2022, about us
NR 36
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6245
EP 6257
DI 10.1109/TMM.2022.3207280
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500043
OA hybrid
DA 2024-07-18
ER

PT J
AU Cai, XY
   Liu, S
   Han, JW
   Yang, LB
   Liu, ZG
   Liu, TM
AF Cai, Xiaoyan
   Liu, Sen
   Han, Junwei
   Yang, Libin
   Liu, Zhenguo
   Liu, Tianming
TI ChestXRayBERT: A Pretrained Language Model for Chest Radiology Report
   Summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Radiology; Task analysis; Biomedical imaging; Decoding; Bit error rate;
   Biological system modeling; Transformers; Pre-trained language model;
   chest radiology report; abstractive summarization
AB Automatically generating the "impression " section of a radiology report given the "findings " section can summarize as much salient information of the "findings " section as possible, thus promoting more effective communication between radiologists and referring physicians. To significantly reduce the workload of radiologists, we develop and evaluate a novel framework of abstractive summarization methods to automatically generate the "impression " section of chest radiology reports. Despite recent advancements in natural language process (NLP) field such as BERT and its variants, existing abstractive summarization models and methods could not be directly applied to radiology reports, partly due to domain-specific radiology terminology. In response, we develop a pre-trained language model in the chest radiology domain, named ChestXRayBERT, to solve the problem of automatically summarizing chest radiology reports. Specifically, we first collect radiology-related scientific papers as pre-training corpus and pre-train a ChestXRayBERT on it. Then, an abstractive summarization model is proposed, which consists of the pre-trained ChestXRayBERT and a Transformer decoder. Finally, the model is fine-tuned on chest X-ray reports for the abstractive summarization task. When evaluated on the publicly available OPEN-I and MIMIC-CXR datasets, the performance of our proposed model achieves significant improvement compared with other neural networks-based abstractive summarization models. In general, the proposed ChestXRayBERT demonstrates the feasibility and promise of tailoring and extending advanced NLP techniques to the domain of medical imaging and radiology, as well as in the broader biomedicine and healthcare fields in the future.
C1 [Cai, Xiaoyan; Liu, Sen; Han, Junwei; Yang, Libin] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
   [Liu, Zhenguo] Shaanxi Prov Peoples Hosp, Dept Intens Care Unit, Xian 710068, Peoples R China.
   [Liu, Tianming] Univ Georgia, Dept Comp Sci, Athens, GA 30602 USA.
C3 Northwestern Polytechnical University; Xi'an Medical University;
   University System of Georgia; University of Georgia
RP Yang, LB (corresponding author), Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
EM xiaoyanc@nwpu.edu.cn; lausen@mail.nwpu.edu.cn; jhan@nwpu.edu.cn;
   libiny@nwpu.edu.cn; 379740078@qq.com; tliu@cs.uga.edu
RI Liu, Tianming/GLS-1211-2022; Sen, LAU/AGA-2155-2022; ZHAO,
   S/IWV-4219-2023
OI Liu, Tianming/0000-0003-0942-6748; Sen, LAU/0000-0001-7828-7484; 
FU National Natural Science Foundation of China [61872296, 61772429,
   U20B2065]; MOE (Ministry of Education in China) Project of Humanities
   and Social Sciences [18YJC870001]
FX Manuscript received 21 May 2021; revised 25 September 2021; accepted 29
   November 2021. Date of publication 6 December 2021; date of current
   version 9 March 2023. This work was supported in part by the National
   Natural Science Foundation of China under Grants 61872296, 61772429, and
   U20B2065, and in part by the MOE (Ministry of Education in China)
   Project of Humanities and Social Sciences under Grant 18YJC870001. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Prof. Jinhui Tang. (Xiaoyan Cai and Sen
   Liu contributed equally to this work.)(Corresponding author: Libin
   Yang.)
CR Ayana, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2657-8
   Bastings J., 2017, P 2017 C EMP METH NA, P1957, DOI 10.18653/v1/d17-1209
   Bjorck J, 2018, ADV NEUR IN, V31
   Chakravarty A., 2020, J COMPUT THEOR NANOS, V17, P3867
   Chang E, 2021, 16TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2021), P818
   Chopra Sumit, 2016, P 2016 C N AM CHAPT, P93, DOI DOI 10.18653/V1/N16-1012
   Daumé H, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P305
   Devlin J., 2018, BERT PRE TRAINING DE
   Du YP, 2020, KNOWL-BASED SYST, V199, DOI 10.1016/j.knosys.2020.105964
   Erkan G, 2004, J ARTIF INTELL RES, V22, P457, DOI 10.1613/jair.1523
   Gale W, 2019, I S BIOMED IMAGING, P1275, DOI 10.1109/ISBI.2019.8759236
   Gao Z, 2021, IEEE T IMAGE PROCESS, V30, P767, DOI 10.1109/TIP.2020.3038372
   Gershanik Esteban F, 2011, AMIA Annu Symp Proc, V2011, P465
   Gu JT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1631
   Gururangan Suchin, 2020, P 58 ANN M ASS COMP, P8342, DOI [DOI 10.18653/V1/2020.ACLMAIN.740, 10.18653/v1/2020.aclmain.740]
   Han ZY, 2018, LECT NOTES COMPUT SC, V11073, P185, DOI 10.1007/978-3-030-00937-3_22
   Huang KX, 2020, Arxiv, DOI [arXiv:1904.05342, DOI 10.48550/ARXIV.1904.05342]
   Karn S., 2021, P 2 WORKSHOP DOMAIN, P245
   Kieuvongngam V, 2020, Arxiv, DOI arXiv:2006.01997
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Lee J, 2020, BIOINFORMATICS, V36, P1234, DOI 10.1093/bioinformatics/btz682
   Lewis M., 2020, P 58 ANN M ASS COMP, P7871, DOI 10.18653/v1/2020.acl-main.703
   Li C., 2018, P 2018 C N AM CHAPT, V2, P55, DOI [10.18653/v1/N18-2009, DOI 10.18653/V1/N18-2009]
   Li P., 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, P2091
   Li YZ, 2018, ADV NEUR IN, V31
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu G., 2019, MACHINE LEARNING HEA
   Liu Y, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3730
   Liu ZY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5460
   MacAvaney S, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P1013, DOI 10.1145/3331184.3331319
   McEntyre J, 2001, CAN MED ASSOC J, V164, P1317
   Nallapati R., 2016, P 20 SIGNLL C COMP N, P280, DOI DOI 10.18653/V1/K16-1028
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Paulus Romain, 2018, ICLR
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Peters M, 2018, STUD LATEINAMERIKA, V32, P1, DOI 10.5771/9783845286846
   Pierre-Etienne G., 2012, P 50 ANN M ASS COMP, V2, P354
   Qian T, 2018, P 9 INT WORKSHOP HLT, P204
   Raffel C, 2020, J MACH LEARN RES, V21
   Roberts RJ, 2001, P NATL ACAD SCI USA, V98, P381, DOI 10.1073/pnas.98.2.381
   Rush AlexanderM., 2015, P 2015 C EMP METH NA
   Sachan DS, 2021, 16TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2021), P2647
   See A, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1073, DOI 10.18653/v1/P17-1099
   Sunderrajan S, 2016, IEEE T MULTIMEDIA, V18, P51, DOI 10.1109/TMM.2015.2496139
   Sutskever I, 2014, ADV NEUR IN, V27
   Tan JW, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1171, DOI 10.18653/v1/P17-1108
   Tu ZP, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P76
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XS, 2018, PROC CVPR IEEE, P9049, DOI 10.1109/CVPR.2018.00943
   Park JW, 2020, Arxiv, DOI arXiv:2007.03405
   Wong K. F., 2008, P 22 INT C COMPUTATI, P985, DOI DOI 10.3115/1599081.1599205
   Wu Yonghui, 2017, T ASS COMPUTATIONAL, P339
   Yang ZL, 2019, ADV NEUR IN, V32
   Yu DH, 2020, Arxiv, DOI arXiv:2010.00796
   Yuan JB, 2019, LECT NOTES COMPUT SC, V11769, P721, DOI 10.1007/978-3-030-32226-7_80
   Zhang J., 2020, P 37 INT C MACHINE L
   Zhang Yuhao, 2020, P 58 ANN M ASS COMP, P5108, DOI DOI 10.18653/V1/2020.ACLMAIN.458
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
   Zhou QY, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1095, DOI 10.18653/v1/P17-1101
NR 59
TC 16
Z9 16
U1 5
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 845
EP 855
DI 10.1109/TMM.2021.3132724
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900013
DA 2024-07-18
ER

PT J
AU Chakareski, J
   Corbillon, X
   Simon, G
   Swaminathan, V
AF Chakareski, Jacob
   Corbillon, Xavier
   Simon, Gwendal
   Swaminathan, Viswanathan
TI User Navigation Modeling, Rate-Distortion Analysis, and End-to-End
   Optimization for Viewport-Driven 360° Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Omnidirectional video; quality of experience; viewport-adaptive 360
   degrees video streaming; rate-distortion analysis and optimization; user
   navigation modeling
AB The emerging technologies of Virtual Reality (VR) and 360 degrees video introduce new challenges for state-of-the-art video communication systems. Enormous data volume and spatial user navigation are unique characteristics of 360 degrees videos that necessitate a space-time effective allocation of the available network streaming bandwidth over the 360 degrees video content to maximize the Quality of Experience (QoE) delivered to the user. Towards this objective, we investigate a framework for viewport-driven rate-distortion optimized 360 degrees video streaming that integrates the user view navigation patterns and the spatiotemporal ratedistortion characteristics of the 360 degrees video content to maximize the delivered user viewport video quality, for the given network/system resources. The framework comprises a methodology for assigning dynamic navigation likelihoods over the 360 degrees video spatiotemporal panorama, induced by the user navigation patterns, an analysis and characterization of the 360 degrees video panorama's spatiotemporal rate-distortion characteristics that leverage preprocessed spatial tilling of the content, and an optimization problem formulation and solution that capture and aim to maximize the delivered expected viewport video quality, given a user's navigation patterns, the 360 degrees video encoding/streaming decisions, and the available system/network resources. We formulate a Markov model to capture the navigation patterns of a user over the 360 degrees video panorama and simultaneously extend our actual navigation datasets by synthesizing additional realistic navigation data. Moreover, we investigate the impact of using two different tile sizes for equirectangular tiling of the 360 degrees video panorama. Our experimental results demonstrate the advantages of our framework over the conventional approach of streaming a monolithic uniformly-encoded 360 degrees video and a state-of-the-art navigation-speed based reference method. Considerable average and instantaneous viewport video quality gains of up to 5 dB are demonstrated in the case of five popular 4 K 360 degrees videos. In addition, we explore the impact of two different popular 360 degrees video quality metrics applied to evaluate the streaming performance of our system framework and the two reference methods. Finally, we demonstrate that by exploiting the unequal rate-distortion characteristics of the different spatial sectors of the 360 degrees video panorama, we can enable spatially more uniform and temporally higher 360 degrees video viewport quality delivered to the user, relative to monolithic streaming.
C1 [Chakareski, Jacob] New Jersey Inst Technol, Coll Comp, Newark, NJ 07103 USA.
   [Corbillon, Xavier] Tiledmedia, Rotterdam, Zuid Holland, Netherlands.
   [Simon, Gwendal] Synmedia, Networking, Rennes, France.
   [Swaminathan, Viswanathan] Adobe, Adobe Res, San Jose, CA USA.
C3 New Jersey Institute of Technology; Adobe Systems Inc.
RP Chakareski, J (corresponding author), New Jersey Inst Technol, Coll Comp, Newark, NJ 07103 USA.
EM jakov@jakov.org; xavier.corbillon@gmail.com; gwendal.simon@gmail.com;
   vishy@adobe.com
FU National Science Foundation (NSF) [CCF-2031881, ECCS2032387,
   CNS-2040088, CNS-2032033, CNS-2106150]; National Institutes of Health
   (NIH) [R01EY030470]; Panasonic Chair of Sustainability at theNewJersey
   Institute for Technology
FX The work of Jacob Chakareski has been supported in part by the National
   Science Foundation (NSF) under awards CCF-2031881, ECCS2032387,
   CNS-2040088, CNS-2032033, and CNS-2106150; by the National Institutes of
   Health (NIH) under award R01EY030470; and by the Panasonic Chair of
   Sustainability at theNewJersey Institute for Technology.
CR Afzal S, 2017, VR/AR NETWORK '17: PROCEEDINGS OF THE 2017 WORKSHOP ON VIRTUAL REALITY AND AUGMENTED REALITY NETWORK, P1, DOI 10.1145/3097895.3097896
   Aksu R, 2018, IEEE INT CONF MULTI
   [Anonymous], Wingsuit 360 degree video over dubai
   [Anonymous], Facebook 360: Astunning and captivatingway to share immersive stories, places and experiences
   [Anonymous], Opentrack:Head tracking software
   [Anonymous], YouTube: 360. videos.
   [Anonymous], Elephants on the brink(360 video)
   [Anonymous], Mega coaster: Get ready for the drop (360 video).
   [Anonymous], Scuba diving short film in 360. green island, Taiwan 4 K video quality
   [Anonymous], NYC 360 time-lapse(360 video)
   Apostolopoulos JG, 2012, P IEEE, V100, P974, DOI 10.1109/JPROC.2011.2182069
   Begole Bo, 2016, Forbes Magazine
   Ben Yahia M, 2017, IEEE CONF COMPUT, P677, DOI 10.1109/INFCOMW.2017.8116458
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chakareski J., 2022, ACM Transactions on Multimedia Computing Communications and Applications
   Chakareski J., 2022, arXiv
   Chakareski J, 2021, MMSYS '21: PROCEEDINGS OF THE 2021 MULTIMEDIA SYSTEMS CONFERENCE, P267, DOI 10.1145/3458305.3478447
   Chakareski J, 2020, IEEE T IMAGE PROCESS, V29, P6330, DOI 10.1109/TIP.2020.2986547
   Chakareski J, 2019, IEEE T IMAGE PROCESS, V28, P5977, DOI 10.1109/TIP.2019.2921869
   Chakarothai Jerdvisanop, 2018, 2018 IEEE International Workshop on Electromagnetics: Applications and Student Innovation Competition (iWEM), DOI 10.1109/iWEM.2018.8536621
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Eltobgy O, 2020, IEEE T MULTIMEDIA, V22, P3139, DOI 10.1109/TMM.2020.2973855
   Graf M, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P261, DOI 10.1145/3083187.3084016
   Grand View Research, 2020, VIRT REAL MARK SIZ S
   Hosseini M, 2016, IEEE INT SYM MULTIM, P107, DOI [10.1109/ISM.2016.45, 10.1109/ISM.2016.0028]
   Hou XS, 2021, IEEE T MULTIMEDIA, V23, P716, DOI 10.1109/TMM.2020.2987693
   Khan M, 2019, IEEE INT CONF COMM, DOI 10.1109/iccw.2019.8757099
   Knightly E., 2017, IEEE INT C COMP COMM
   Monnier R., 2017, White Paper., P22
   Moss JD, 2011, HUM FACTORS, V53, P308, DOI 10.1177/0018720811405196
   MPEG-DASH-OMAF standard, 2018, ISO/IEC FDIS 23090-2.
   Ozcinar C, 2017, IEEE IMAGE PROC, P2174, DOI 10.1109/ICIP.2017.8296667
   Petrangeli S, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P225, DOI 10.1145/3083187.3083224
   Piamrat K, 2009, PROCEEDINGS OF THE 2009 SIXTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: NEW GENERATIONS, VOLS 1-3, P1184, DOI 10.1109/ITNG.2009.121
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Sánchez Y, 2015, IEEE IMAGE PROC, P2244, DOI 10.1109/ICIP.2015.7351200
   Simone F. D., 2017, VQEG eLetter, V3, P18
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun LY, 2023, IEEE T MULTIMEDIA, V25, P2636, DOI 10.1109/TMM.2022.3149642
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Xiao MB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P708, DOI 10.1145/3123266.3123339
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
NR 45
TC 2
Z9 2
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5941
EP 5956
DI 10.1109/TMM.2022.3201397
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500022
DA 2024-07-18
ER

PT J
AU Chang, YL
   Li, SM
   Liu, AQ
   Jin, J
   Xiang, W
AF Chang, Yongli
   Li, Sumei
   Liu, Anqi
   Jin, Jie
   Xiang, Wei
TI Coarse-to-Fine Feedback Guidance Based Stereo Image Quality Assessment
   Considering Dominant Eye Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Feature extraction; Image quality; Adaptive systems;
   Streaming media; Data mining; Bidirectional control; Stereo image
   quality assessment; coarse-to-fine feedback guidance; bi-directional
   parallax attention mechanism; convolutional neural network
ID FUNCTIONAL ARCHITECTURE; PREDICTION
AB Considering that the human brain always follows a coarse-to-fine (low-to-high spatial frequency) visual processing and fusion mechanism, we propose a coarse-to-fine feedback guidance based stereo image quality assessment (SIQA) network which considers a coarse-to-fine feedback guidance and adaptive dominant eye mechanism. The proposed network consists of two main sub-network streams, each of which has three branches to extract low, middle and high spatial frequency information in parallel. To better realize the guidance of the high-level features in the low spatial frequency branch to the low-level features in the high spatial frequency branch, an information feedback guidance module (IFGM) is proposed, which realizes a top-down guidance mechanism in each sub-network stream. Simultaneously, according to the theory of ocular dominance in human visual system (HVS), we design an adaptive bi-directional parallax-based binocular fusion module (BPBFM), which synthesizes two types of fusion feature by taking the left and right view features as dominant eye input. Furthermore, in order to obtain the better perceptual quality of stereo images, we design a weighted fusion strategy to weigh the quality scores from the two types of fusion features obtained by using an ensemble model with two multi-layer perceptrons (MLPs). The experimental results on four public stereo image datasets show that the proposed method is superior to the mainstream metrics and achieves an excellent performance.
C1 [Chang, Yongli; Li, Sumei; Liu, Anqi; Jin, Jie] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Xiang, Wei] La Trobe Univ, Sch Engn & Math Sci, Melbourne, Vic 3086, Australia.
C3 Tianjin University; La Trobe University
RP Li, SM (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM chang_yli@163.com; lisumei@tju.edu.cn; liuanqi@tju.edu.cn;
   jinjie@tju.edu.cn; w.xiang@latrobe.edu.au
OI Li, Sumei/0000-0002-4793-3161
FU National Natural Science Foundation of China
FX No Statement Available
CR Bai YQ, 2021, IEEE T MULTIMEDIA, V23, P4259, DOI 10.1109/TMM.2020.3039382
   Chen HW, 2023, IEEE T MULTIMEDIA, V25, P140, DOI 10.1109/TMM.2021.3121875
   Chen L, 2019, SIGNAL PROCESS-IMAGE, V76, P1, DOI 10.1016/j.image.2019.03.011
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   Chen WL, 2021, IEEE T MULTIMEDIA, V23, P1008, DOI 10.1109/TMM.2020.2991546
   Ding Y., 2019, P IEEE VIS COMM IM P, P1
   Ding Y, 2019, IET IMAGE PROCESS, V13, P1608, DOI 10.1049/iet-ipr.2018.5605
   Ding Y, 2018, IEEE ACCESS, V6, P37595, DOI 10.1109/ACCESS.2018.2851255
   Fang YM, 2019, IEEE ACCESS, V7, P132649, DOI 10.1109/ACCESS.2019.2941112
   Fang YM, 2019, J VIS COMMUN IMAGE R, V58, P400, DOI 10.1016/j.jvcir.2018.12.006
   Han YT, 2020, IEEE I C VI COM I PR, P334, DOI 10.1109/vcip49819.2020.9301832
   HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Karimi M, 2019, DIGIT SIGNAL PROCESS, V91, P91, DOI 10.1016/j.dsp.2019.03.004
   Kauffmann L, 2014, FRONT INTEGR NEUROSC, V8, DOI 10.3389/fnint.2014.00037
   Kim J, 2019, IEEE IMAGE PROC, P1745, DOI [10.1109/ICIP.2019.8803183, 10.1109/icip.2019.8803183]
   Li KM, 2015, ELECTRON LETT, V51, P1994, DOI 10.1049/el.2015.2049
   Li S., 2019, P IEEE VIS COMM IM P, P1
   Li SM, 2020, IEEE I C VI COM I PR, P326, DOI 10.1109/vcip49819.2020.9301770
   Li YF, 2019, IEEE ACCESS, V7, P46706, DOI 10.1109/ACCESS.2019.2909073
   Lin YC, 2017, IEEE J-STSP, V11, P89, DOI 10.1109/JSTSP.2016.2632422
   Ling SY, 2021, IEEE T MULTIMEDIA, V23, P4245, DOI 10.1109/TMM.2020.3038305
   Liu Y, 2020, NEUROCOMPUTING, V405, P126, DOI 10.1016/j.neucom.2020.04.049
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Md SK, 2016, 2016 TWENTY SECOND NATIONAL CONFERENCE ON COMMUNICATION (NCC)
   Menon RS, 1997, J NEUROPHYSIOL, V77, P2780, DOI 10.1152/jn.1997.77.5.2780
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2013, SIGNAL PROCESS-IMAGE, V28, P870, DOI 10.1016/j.image.2012.08.004
   Niu YZ, 2019, IEEE ACCESS, V7, P101583, DOI 10.1109/ACCESS.2019.2930707
   Peyrin C, 2010, J COGNITIVE NEUROSCI, V22, P2768, DOI 10.1162/jocn.2010.21424
   Sang Q., 2017, P INT SMART CIT C, P1
   Shao F, 2018, IEEE T CIRC SYST VID, V28, P573, DOI 10.1109/TCSVT.2016.2628082
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shen LL, 2021, NEUROCOMPUTING, V424, P132, DOI 10.1016/j.neucom.2020.10.024
   Shi YQ, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107168
   Si JW, 2022, IEEE T IMAGE PROCESS, V31, P3066, DOI 10.1109/TIP.2022.3164537
   Sim K, 2022, IEEE T MULTIMEDIA, V24, P1389, DOI 10.1109/TMM.2021.3064240
   Sun GM, 2020, IEEE T MULTIMEDIA, V22, P2938, DOI 10.1109/TMM.2020.2965461
   Wan ZL, 2020, IEEE T MULTIMEDIA, V22, P2024, DOI 10.1109/TMM.2019.2950533
   Wang H, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102058
   Wang J., 2014, P INT WORKSH VID PRO, P1
   Wang JH, 2015, IEEE T IMAGE PROCESS, V24, P3400, DOI 10.1109/TIP.2015.2446942
   Wang LG, 2022, IEEE T PATTERN ANAL, V44, P2108, DOI 10.1109/TPAMI.2020.3026899
   Wang LG, 2019, PROC CVPR IEEE, P12242, DOI 10.1109/CVPR.2019.01253
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Xing LY, 2012, IEEE T MULTIMEDIA, V14, P326, DOI 10.1109/TMM.2011.2172402
   Xu J., 2021, IEEE Trans. Instrum. Meas., V70, P1
   Yan J., 2020, P IEEE INT C MULT EX, P1
   Yang JC, 2019, IEEE T MULTIMEDIA, V21, P1750, DOI 10.1109/TMM.2018.2889562
   Yang JC, 2019, IEEE T IMAGE PROCESS, V28, P1314, DOI 10.1109/TIP.2018.2878283
   Ying XY, 2020, IEEE SIGNAL PROC LET, V27, P496, DOI 10.1109/LSP.2020.2973813
   You J., 2012, INT WORKSH VID PROC, P1
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang W, 2016, PATTERN RECOGN, V59, P176, DOI 10.1016/j.patcog.2016.01.034
   Zhou W., 2021, Signal Processing: Image Commun., V91, P1
   Zhou W, 2019, IEEE T IMAGE PROCESS, V28, P3946, DOI 10.1109/TIP.2019.2902831
   Zhou WJ, 2020, IEEE T COMPUT IMAG, V6, P883, DOI 10.1109/TCI.2020.2993640
   Zhou WJ, 2017, PATTERN RECOGN, V71, P207, DOI 10.1016/j.patcog.2017.06.008
NR 59
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8855
EP 8867
DI 10.1109/TMM.2023.3242550
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000018
DA 2024-07-18
ER

PT J
AU Gao, R
   Hou, XS
   Qin, J
   Shen, YM
   Long, Y
   Liu, L
   Zhang, Z
   Shao, L
AF Gao, Rui
   Hou, Xingsong
   Qin, Jie
   Shen, Yuming
   Long, Yang
   Liu, Li
   Zhang, Zhao
   Shao, Ling
TI Visual-Semantic Aligned Bidirectional Network for Zero-Shot Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bidirectional network; generative model; zero-shot learning
ID ADVERSARIAL NETWORK; KERNEL
AB Zero-shot learning (ZSL) aims to recognize unknown categories that are unavailable during training. Recently, generative models have shown the potential to address this challenging problem by synthesizing unseen features conditioned on semantic embeddings such as attributes. However, unidirectional generative models cannot guarantee the effective coupling between visual and semantic spaces. To this end, we propose a visual-semantic aligned bidirectional network with cycle consistency to alleviate the gap between these two spaces, generating unseen features of high quality. More importantly, we incorporate two carefully designed strategies into our bidirectional framework to improve the overall ZSL performance. Specifically, we enhance the intra-domain class divergence in both visual and semantic spaces, and in the meantime, mitigate the inter-domain shift to preserve seen-unseen domain discrimination. Experimental results on four standard benchmarks show the superiority of our framework over existing state-of-the-art methods under both conventional and generalized ZSL settings.
C1 [Qin, Jie] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Gao, Rui; Hou, Xingsong] Xi An Jiao Tong Univ, Dept Elect & Informat Engn, Xian 710049, Peoples R China.
   [Shen, Yuming] Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England.
   [Long, Yang] Univ Durham, Dept Comp Sci, Durham DH1 3LE, England.
   [Liu, Li; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Zhang, Zhao] Hefei Univ Technol, Sch Comp & Informat, Hefei 230601, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Xi'an Jiaotong
   University; University of Oxford; Durham University; Hefei University of
   Technology
RP Qin, J (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM gaorui1013@gmail.com; houxs@mail.xjtu.edu.cn; qinjiebuaa@gmail.com;
   ym_zmxncbv@hotmail.com; yang.long@ieee.org; liuli1213@gmail.com;
   cszzhang@gmail.com; ling.shao@ieee.org
RI Qin, Jie/AAM-3315-2020; Zhang, Zhao/B-5136-2010; Shao, Ling/D-3535-2011
OI Qin, Jie/0000-0002-0306-534X; Zhang, Zhao/0000-0002-5703-7969; Gao,
   Rui/0000-0002-4351-6555
FU NSFC [61872286, 62072151]; Key R&D Program of Shaanxi Province of China
   [2020ZDLGY04-05, S2021-YF-YBSF-0094]; Anhui Provincial Natural Science
   Fund for Distinguished Young Scholars [2008085J30]; MRC [MR/S003916/1,
   MR/S003916/2] Funding Source: UKRI
FX This work was supported in part by NSFC under Grants 61872286 and
   62072151, in part by the Key R&D Program of Shaanxi Province of China
   under Grants 2020ZDLGY04-05 and S2021-YF-YBSF-0094, and in part by the
   Anhui Provincial Natural Science Fund for Distinguished Young Scholars
   under Grant 2008085J30. The guest editor coordinating the review of this
   manuscript and approving it for publication was Mr. Chuang Gan.
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111
   Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bai S, 2016, IEEE T MULTIMEDIA, V18, P1351, DOI 10.1109/TMM.2016.2557071
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Ding X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4026
   Ding ZM, 2019, PROC CVPR IEEE, P6184, DOI 10.1109/CVPR.2019.00635
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Frome A., 2013, P ADV NEUR INF PROC
   Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354
   Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17
   Gan C, 2016, INT J COMPUT VISION, V120, P61, DOI 10.1007/s11263-016-0893-6
   Gao R., 2018, P EUR C COMP VIS WOR, P3665
   Gao R, 2020, IEEE T IMAGE PROCESS, V29, P3665, DOI 10.1109/TIP.2020.2964429
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Gu Y.-C., 2020, ARXIV
   Guan JC, 2021, IEEE T PATTERN ANAL, V43, P2510, DOI 10.1109/TPAMI.2020.2965534
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   He D., 2016, P C NEUR INF PROC SY, P183
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089
   Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184
   Gulrajani I, 2017, ADV NEUR IN, V30
   Ji Z, 2020, KNOWL-BASED SYST, V197, DOI 10.1016/j.knosys.2020.105847
   Kingma D. P., 2014, PROC INT C LEARN REP, P3577
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li J., 2021, P IEEE INT C CYB, P1
   Liu Y, 2020, IEEE COMPUT SOC CONF, P4053, DOI 10.1109/CVPRW50498.2020.00478
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P4788, DOI 10.1109/TIP.2020.2975980
   Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Mishra A, 2018, IEEE COMPUT SOC CONF, P2269, DOI 10.1109/CVPRW.2018.00294
   Ni J., 2019, Advances in neural information processing systems, P6146
   Norouzi M, 2014, PROC INT C LEARN REP
   Pambala AK, 2020, IEEE WINT CONF APPL, P1226, DOI [10.1109/WACV45572.2020.9093625, 10.1109/wacv45572.2020.9093625]
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Qin J, 2016, IEEE SIGNAL PROC LET, V23, P1667, DOI 10.1109/LSP.2016.2612247
   Radford A., 2015, ARXIV
   Romera-Paredes Bernardino, 2015, ICML
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Socher R., 2013, P 2013 C EMP METH NA, V2013, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791
   Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113
   Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252
   Tong B, 2019, PROC CVPR IEEE, P11459, DOI 10.1109/CVPR.2019.01173
   Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450
   Verma VK, 2017, LECT NOTES ARTIF INT, V10535, P792, DOI 10.1007/978-3-319-71246-8_48
   Xian Y., 2017, IEEE T PATTERN ANAL, V41, P2251
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Xing Y, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102961
   Xu B., 2015, PROC INT C MACH LEAR, P204
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Ye SQ, 2017, IEEE INT WORKS MACH, DOI 10.1109/TPAMI.2017.2762295
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang HF, 2019, NEUROCOMPUTING, V329, P12, DOI 10.1016/j.neucom.2018.10.043
   Zhang HF, 2019, IEEE T IMAGE PROCESS, V28, P506, DOI 10.1109/TIP.2018.2869696
   Zhang L, 2020, IEEE T CIRC SYST VID, V30, P2843, DOI 10.1109/TCSVT.2020.2984666
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
   Zhang ZM, 2016, LECT NOTES COMPUT SC, V9911, P533, DOI 10.1007/978-3-319-46478-7_33
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 79
TC 7
Z9 7
U1 6
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1649
EP 1664
DI 10.1109/TMM.2022.3145666
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100004
DA 2024-07-18
ER

PT J
AU Hua, GG
   Liao, MX
   Tian, SS
   Zhang, YH
   Zou, WB
AF Hua, Guoguang
   Liao, Muxin
   Tian, Shishun
   Zhang, Yuhang
   Zou, Wenbin
TI Multiple Relational Learning Network for Joint Referring Expression
   Comprehension and Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Feature extraction; Multitasking; Visualization;
   Convolutional neural networks; Predictive models; Image segmentation;
   Referring expression comprehension; referring expression segmentation;
   multi-task learning
AB Multi-task learning is a successful learning framework which improves the performance of prediction models by leveraging knowledge among related tasks. Referring expression comprehension (REC) and segmentation (RES) are highly relevant tasks, which both are language-guided visual recognition tasks. However, their relations have not yet been fully exploited in previous works. In this paper, a Multiple Relational Learning Network (MRLN) is proposed for multi-task learning of REC and RES. First, a feature-feature interaction learning module is introduced to handle the complicated interactions among features. Moreover, we propose a feature-task dependence learning module, which associates the related features with target tasks. Furthermore, a task-task relationship learning module is designed, which captures the relationships among tasks automatically and guides the REC and RES fine-tuning adaptively. To verify our proposed approach, experiments are conducted on three benchmark datasets, i.e., RefCOCO, RefCOCO+, and RefCOCOg. Extensive experiments demonstrate that the multiple relationships are more appealing since it alleviates the prediction inconsistency issue in multi-task setup. In addition, the experimental results report the significant performance gains of MRLN over most existing methods, i.e., up to 83.46% for REC and 63.62% for RES over state-of-the-art methods, which demonstrate the validity and superiority of MRLN.
C1 [Hua, Guoguang; Liao, Muxin; Tian, Shishun; Zhang, Yuhang; Zou, Wenbin] Shenzhen Univ, Coll Elect & Informat Engn, Guangdong Key Lab Intelligent Informat Proc, Shenzhen Key Lab Adv Machine Learning & Applicat, Shenzhen 518060, Peoples R China.
C3 Shenzhen University
RP Zou, WB (corresponding author), Shenzhen Univ, Coll Elect & Informat Engn, Guangdong Key Lab Intelligent Informat Proc, Shenzhen Key Lab Adv Machine Learning & Applicat, Shenzhen 518060, Peoples R China.
EM huaguoguang2021@email.szu.edu.cn; seabearlmx@gmail.com;
   stian@szu.edu.cn; zhangyuhang2021@email.szu.edu.cn; wzou@szu.edu.cn
RI liao, muxin/AAS-5785-2020; Zhang, Yuhang/JJF-7201-2023
OI liao, muxin/0000-0002-8461-1946; Zhang, Yuhang/0000-0001-6404-9952;
   Tian, Shishun/0000-0002-7616-8382
FU National Natural Science Foundation of China
FX No Statement Available
CR Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Y., 2019, P BRIT MACH VIS C, P1
   Chung J., 2014, NIPS 2014 WORKSH DEE
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fu CY, 2019, Arxiv, DOI arXiv:1901.03353
   Gen Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10031, DOI 10.1109/CVPR42600.2020.01005
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hong Y., 2020, Advances in Neural Information Processing Systems, P7685
   Hu R., 2017, P IEEE COMP VIS PATT, P4555
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Kingma D. P., 2014, arXiv
   Li QZ, 2022, NEUROCOMPUTING, V467, P99, DOI 10.1016/j.neucom.2021.09.066
   Li RY, 2018, PROC CVPR IEEE, P5745, DOI 10.1109/CVPR.2018.00602
   Li XY, 2018, IEEE T MULTIMEDIA, V20, P2749, DOI 10.1109/TMM.2018.2811621
   Liao MX, 2022, NEUROCOMPUTING, V500, P938, DOI 10.1016/j.neucom.2022.05.059
   Liao Y., 2020, P IEEE CVF C COMP VI, P10877, DOI 10.24963/ijcai.2018/155
   Lin L, 2022, IEEE T MULTIMEDIA, V24, P1922, DOI 10.1109/TMM.2021.3074008
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2017, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2017.143
   Liu DQ, 2019, IEEE I CONF COMP VIS, P4672, DOI 10.1109/ICCV.2019.00477
   Loshchilov I., 2016, P INT C LEARN REPR
   Luo G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1274, DOI 10.1145/3394171.3414006
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Margffoy-Tuay E, 2018, LECT NOTES COMPUT SC, V11215, P656, DOI 10.1007/978-3-030-01252-6_39
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Nekrasov V, 2019, IEEE INT CONF ROBOT, P7101, DOI [10.1109/icra.2019.8794220, 10.1109/ICRA.2019.8794220]
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Qiu HQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4171, DOI 10.1145/3394171.3413850
   Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Santoro A, 2017, ADV NEUR IN, V30
   Shi HC, 2021, IEEE T MULTIMEDIA, V23, P995, DOI 10.1109/TMM.2020.2991504
   Sun MY, 2023, IEEE T MULTIMEDIA, V25, P2446, DOI 10.1109/TMM.2022.3147385
   Sun MJ, 2023, IEEE T MULTIMEDIA, V25, P1611, DOI 10.1109/TMM.2021.3139467
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Yang C, 2020, Arxiv, DOI arXiv:2012.10890
   Yang Z., 2016, P 2016 C N AM CHAPT, P1480, DOI [DOI 10.18653/V1/N16-1174, 10.18653/v1/n16-1174]
   Yang ZY, 2019, IEEE I CONF COMP VIS, P4682, DOI 10.1109/ICCV.2019.00478
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Ye LW, 2020, IEEE T MULTIMEDIA, V22, P3224, DOI 10.1109/TMM.2020.2971171
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Yuankai Qi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9979, DOI 10.1109/CVPR42600.2020.01000
   Zhang Y, 2010, AAAI CONF ARTIF INTE, P667
   Zhang YT, 2017, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2017.122
   Zhengyuan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P387, DOI 10.1007/978-3-030-58568-6_23
   Zhou YY, 2023, IEEE T NEUR NET LEAR, V34, P134, DOI 10.1109/TNNLS.2021.3090426
NR 57
TC 6
Z9 6
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8805
EP 8816
DI 10.1109/TMM.2023.3241802
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000064
DA 2024-07-18
ER

PT J
AU Huo, SW
   Zhou, Y
   Wang, RL
   Xiang, W
   Kung, SY
AF Huo, Shuwei
   Zhou, Yuan
   Wang, Ruolin
   Xiang, Wei
   Kung, Sun-Yuan
TI Semantic Relevance Learning for Video-Query Based Video Moment Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video moment retrieval; video query; fine-grained feature interaction;
   semantic relevance measurement
ID TEMPORAL ACTION LOCALIZATION
AB The task of video-query based video moment retrieval (VQ-VMR) aims to localize the segment in the reference video, which matches semantically with a short query video. This is a challenging task due to the rapid expansion and massive growth of online video services. With accurate retrieval of the target moment, we propose a new metric to effectively assess the semantic relevance between the query video and segments in the reference video. We also develop a new VQ-VMR framework to discover the intrinsic semantic relevance between a pair of input videos. It comprises two key components: a Fine-grained Feature Interaction (FFI) module and a Semantic Relevance Measurement (SRM) module. Together they can effectively deal with both the spatial and temporal dimensions of videos. First, the FFI module computes the semantic similarity between videos at a local frame level, mainly considering the spatial information in the videos. Subsequently, the SRM module learns the similarity between videos from a global perspective, taking into account the temporal information. We have conducted extensive experiments on two key datasets which demonstrate noticeable improvements of the proposed approach over the state-of-the-art methods.
C1 [Huo, Shuwei; Zhou, Yuan; Wang, Ruolin] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Xiang, Wei] La Trobe Univ, Sch Comp Engn & Math Sci, Melbourne, Vic 3086, Australia.
   [Xiang, Wei] James Cook Univ, Coll Sci & Engn, Cairns, Qld 4878, Australia.
   [Kung, Sun-Yuan] Princeton Univ, Elect Engn Dept, Princeton, NJ 08540 USA.
C3 Tianjin University; La Trobe University; James Cook University;
   Princeton University
RP Zhou, Y (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM huosw@tju.edu.cn; zhouyuan@tju.edu.cn; szx2048067511@163.com;
   w.xiang@latrobe.edu.au; kung@princeton.edu
OI Huo, Shuwei/0000-0002-7290-7838
FU National Key Research and Development Program of China
FX No Statement Available
CR Bojanowski P, 2015, IEEE I CONF COMP VIS, P4462, DOI 10.1109/ICCV.2015.507
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9620, DOI 10.1109/ICCV48922.2021.00950
   Chou CL, 2015, IEEE T MULTIMEDIA, V17, P382, DOI 10.1109/TMM.2015.2391674
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feng Y., 2018, PROC EUR C COMPUT VI
   Feng Y, 2019, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2019.00138
   Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Huang JB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7179, DOI 10.1109/ICCV48922.2021.00711
   Huang YH, 2020, AAAI CONF ARTIF INTE, V34, P11077
   Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kingma D, 2014, ICLR P, V2014, P1
   Lin DH, 2014, PROC CVPR IEEE, P2657, DOI 10.1109/CVPR.2014.340
   Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Ma SG, 2013, IEEE I CONF COMP VIS, P2744, DOI 10.1109/ICCV.2013.341
   Minuk Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P156, DOI 10.1007/978-3-030-58604-1_10
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Pramono RRA, 2022, IEEE T MULTIMEDIA, V24, P625, DOI 10.1109/TMM.2021.3056892
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sun C, 2022, IEEE T MULTIMEDIA, V24, P274, DOI 10.1109/TMM.2021.3050067
   Sun XY, 2021, IEEE T IMAGE PROCESS, V30, P5589, DOI 10.1109/TIP.2021.3086591
   Tang H., 2021, P 2 ACM INT C MULT A, P1
   Tapaswi M, 2015, PROC CVPR IEEE, P1827, DOI 10.1109/CVPR.2015.7298792
   Tellex Stefanie., 2009, P ACM INT C IM VID R, P38
   Wang GM, 2022, IEEE T MULTIMEDIA, V24, P1221, DOI 10.1109/TMM.2022.3142420
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yu-Gang Jiang, 2016, IEEE Transactions on Big Data, V2, P32, DOI 10.1109/TBDATA.2016.2530714
   Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337
   Zeng YW, 2021, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR46437.2021.00225
   Zhai YH, 2022, IEEE T MULTIMEDIA, V24, P1857, DOI 10.1109/TMM.2021.3073235
   Zhang H, 2022, IEEE T PATTERN ANAL, V44, P4252, DOI 10.1109/TPAMI.2021.3060449
   Zhang H, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P685, DOI 10.1145/3404835.3462874
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhou Y, 2021, IEEE T MULTIMEDIA, V23, P4363, DOI 10.1109/TMM.2020.3042077
NR 49
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9290
EP 9301
DI 10.1109/TMM.2023.3250088
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200021
DA 2024-07-18
ER

PT J
AU Krivokuca, M
   Miandji, E
   Guillemot, C
   Chou, PA
AF Krivokuca, Maja
   Miandji, Ehsan
   Guillemot, Christine
   Chou, Philip A. A.
TI Compression of Plenoptic Point Cloud Attributes Using 6-D Point Clouds
   and 6-D Transforms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Cameras; Encoding; Rate-distortion; Transform
   coding; Surface treatment; Light fields; GFT; plenoptic point clouds;
   point cloud compression; RAHT; separable transforms; surface light
   fields
ID EFFICIENT REPRESENTATION; FIELD
AB In this paper, we introduce a novel 6-D representation of plenoptic point clouds, enabling joint, non-separable transform coding of plenoptic signals defined along both spatial and angular (viewpoint) dimensions. This 6-D representation, which is built in a global coordinate system, can be used in both multi-camera studio capture and video fly-by capture scenarios, with various viewpoint (camera) arrangements and densities. We show that both the Region-Adaptive Hierarchical Transform (RAHT) and the Graph Fourier Transform (GFT) can be extended to the proposed 6-D representation to enable the non-separable transform coding. Our method is applicable to plenoptic data with either dense or sparse sets of viewpoints, and to complete or incomplete plenoptic data, while the state-of-the-art RAHT-KLT method, which is separable in spatial and angular dimensions, is applicable only to complete plenoptic data. The "complete " plenoptic data refers to data that has, for each spatial point, one colour for every viewpoint (ignoring any occlusions), while "incomplete " data has colours only for the visible surface points at each viewpoint. We demonstrate that the proposed 6-D RAHT and 6-D GFT compression methods are able to outperform the state-of-the-art RAHT-KLT method on 3-D objects with various levels of surface specularity, and captured with different camera arrangements and different degrees of viewpoint sparsity.
C1 [Krivokuca, Maja; Guillemot, Christine] Inria, SIROCCO, Ctr Rech Rennes Bretagne Atlantique, Rennes, France.
   [Krivokuca, Maja] InterDigital, Cesson Sevigne F-35510, France.
   [Miandji, Ehsan] Linkopings Univ, Dept Sci & Technol, S-58183 Linkoping, Sweden.
   [Chou, Philip A. A.] Google Inc, Percept Res, Seattle, WA USA.
C3 Universite de Rennes; Inria; Linkoping University; Google Incorporated
RP Krivokuca, M (corresponding author), Inria, SIROCCO, Ctr Rech Rennes Bretagne Atlantique, Rennes, France.; Krivokuca, M (corresponding author), InterDigital, Cesson Sevigne F-35510, France.
EM majakri01@gmail.com; ehsan.miandji@liu.se; christine.guillemot@inria.fr;
   philchou@msn.com
RI Miandji, Ehsan/ACN-7116-2022
OI Chou, Philip Andrew/0000-0002-7242-0210
FU EU H2020 Research and Innovation Programme [694122]
FX This work was supported by the EU H2020 Research and Innovation
   Programme under Grant 694122 (ERC advanced grant CLIM).
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   BADER M, 2012, Spacefilling curves: an introduction with applications in scientific computing
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cao C, 2019, PROCEEDINGS WEB3D 2019: THE 24TH INTERNATIONAL ACM CONFERENCE ON 3D WEB TECHNOLOGY, DOI 10.1145/3329714.3338130
   Chen WC, 2002, ACM T GRAPHIC, V21, P447, DOI 10.1145/566570.566601
   Chou PA, 2020, IEEE T IMAGE PROCESS, V29, P2203, DOI 10.1109/TIP.2019.2908095
   de Queiroz RL, 2016, IEEE T IMAGE PROCESS, V25, P3947, DOI 10.1109/TIP.2016.2575005
   Ebrahimi T, 2016, IEEE MULTIMEDIA, V23, P14, DOI 10.1109/MMUL.2016.64
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   Hannuksela MM, 2015, IEEE IMAGE PROC, P2154
   Ihrke I, 2016, IEEE SIGNAL PROC MAG, V33, P59, DOI 10.1109/MSP.2016.2582220
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Koppal S. J., 2014, Lambertian Reflectance, P441, DOI [DOI 10.1007/978-0-387-31439-6_534, 10.1007/978-0-387-31439-6534]
   Krivokuca M, 2020, INT CONF ACOUST SPEE, P1978, DOI [10.1109/icassp40776.2020.9053862, 10.1109/ICASSP40776.2020.9053862]
   Krivokuda M., 2018, ISOIEC JTC1SC29 WG11
   Lasserre S., 2019, ISOIEC JTC1SC29WG11
   Li L, 2020, IEEE DATA COMPR CONF, P378, DOI 10.1109/DCC47342.2020.00053
   Malvar HS, 2006, IEEE DATA COMPR CONF, P23
   Miandji E., 2013, PROC SIGGRAPH ASIA T, P1
   Miller G., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P281
   Naik D., 2020, PROC IEEE INT WORKSH, P1
   Naik D., 2019, PROC EUR LIGHT FIELD
   Ortega A, 2018, P IEEE, V106, P808, DOI 10.1109/JPROC.2018.2820126
   Pavez E., 2021, PROC IEEE INT C IMAG
   Pavez E, 2020, IEEE IMAGE PROC, P2726, DOI 10.1109/ICIP40778.2020.9191183
   Sandri G, 2018, Arxiv, DOI arXiv:1805.09146
   Sandri G, 2018, IEEE IMAGE PROC, P1153, DOI 10.1109/ICIP.2018.8451367
   Sandri G, 2019, IEEE T IMAGE PROCESS, V28, P1419, DOI 10.1109/TIP.2018.2877486
   Sandri GP, 2019, IEEE SIGNAL PROC LET, V26, P1369, DOI 10.1109/LSP.2019.2931425
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Souto AL, 2020, IEEE IMAGE PROC, P2701, DOI 10.1109/ICIP40778.2020.9191205
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Wood DN, 2000, COMP GRAPH, P287, DOI 10.1145/344779.344925
   Zhang C, 2014, IEEE IMAGE PROC, P2066, DOI 10.1109/ICIP.2014.7025414
   Zhang X, 2019, IEEE J EM SEL TOP C, V9, P163, DOI 10.1109/JETCAS.2018.2883479
   Zhang X, 2018, IEEE IMAGE PROC, P2595, DOI 10.1109/ICIP.2018.8451269
NR 37
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 593
EP 607
DI 10.1109/TMM.2021.3129341
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, F
   Wu, YX
   Bai, HH
   Lin, WS
   Cong, RM
   Zhao, Y
AF Li, Feng
   Wu, Yixuan
   Bai, Huihui
   Lin, Weisi
   Cong, Runmin
   Zhao, Yao
TI Learning Detail-Structure Alternative Optimization for Blind
   Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind image super-resolution; convolutional neural networks;
   detail-structure alternative optimization; detail restoration; structure
   modulation
ID IMAGE SUPERRESOLUTION; NETWORKS
AB Existing convolutional neural networks (CNN) based image super-resolution (SR) methods have achieved impressive performance on bicubic kernel, which is not valid to handle unknown degradations in real-world applications. Recent blind SR methods suggest to reconstruct SR images relying on blur kernel estimation. However, their results still remain visible artifacts and detail distortion due to the estimation errors. To alleviate these problems, in this paper, we propose an effective and kernel-free network, namely DSSR, which enables recurrent detail-structure alternative optimization without blur kernel prior incorporation for blind SR. Specifically, in our DSSR, a detail-structure modulation module (DSMM) is built to exploit the interaction and collaboration of image details and structures. The DSMM consists of two components: a detail restoration unit (DRU) and a structure modulation unit (SMU). The former aims at regressing the intermediate HR detail reconstruction from LR structural contexts, and the latter performs structural contexts modulation conditioned on the learned detail maps at both HR and LR spaces. Besides, we use the output of DSMM as the hidden state and design our DSSR architecture from a recurrent convolutional neural network (RCNN) view. In this way, the network can alternatively optimize the image details and structural contexts, achieving co-optimization across time. Moreover, equipped with the recurrent connection, our DSSR allows low- and high-level feature representations complementary by observing previous HR details and contexts at every unrolling time. Extensive experiments on synthetic datasets and real-world images demonstrate that our method achieves the state-of-the-art against existing methods.
C1 [Li, Feng; Wu, Yixuan; Bai, Huihui; Cong, Runmin; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Li, Feng; Wu, Yixuan; Bai, Huihui; Cong, Runmin; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Beijing Jiaotong University; Nanyang Technological University
RP Bai, HH (corresponding author), Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.; Bai, HH (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM l1feng@bjtu.edu.cn; wuyixuan@bjtu.edu.cn; hhbai@bjtu.edu.cn;
   wslin@ntu.edu.sg; rmcong@bjtu.edu.cn; yzhao@bjtu.edu.cn
RI Wu, Yixuan/GQG-9308-2022; Lin, Wei/D-3353-2012; Lin, Weisi/A-3696-2011
OI Zhao, Yao/0000-0002-8581-9554; Lin, Weisi/0000-0001-9866-1947; Bai,
   Huihui/0000-0002-3879-8957
FU Fundamental Research Funds for the Central Universities [2019YJS031];
   National Natural Science Foundation of China [61972023, 62120106009,
   62002014]; Beijing Nova Program [Z201100006820016]; Beijing Municipal
   Natural Science Foundation [4222013]; China Scholarship Council
   [202007090046]; National Key Ramp;D Program of China [2021ZD0112100]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant 2019YJS031, in partby the National
   Natural Science Foundation of China under Grants 61972023, 62120106009,
   and 62002014, in part by the Beijing Nova Program under Grant
   Z201100006820016, in part by the Beijing Municipal Natural Science
   Foundation under Grant 4222013, in part by the China Scholarship Council
   underGrant 202007090046, and in part by the National Key R & D Program
   of China under grant 2021ZD0112100.
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cornillère V, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356575
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170
   Guo YW, 2021, IEEE T IMAGE PROCESS, V30, P2669, DOI 10.1109/TIP.2021.3051767
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He ZY, 2022, IEEE T MULTIMEDIA, V24, P2877, DOI 10.1109/TMM.2021.3090166
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Ji XZ, 2020, IEEE COMPUT SOC CONF, P1914, DOI 10.1109/CVPRW50498.2020.00241
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Kim J, 2016, PROC CVPR IEEE, P1646, DOI 10.1109/CVPR.2016.182
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim JH, 2020, Arxiv, DOI arXiv:1811.12043
   Kim SY, 2021, PROC CVPR IEEE, P10606, DOI 10.1109/CVPR46437.2021.01047
   Kingma D. P., 2014, arXiv
   Kligler S. B., 2019, P C ADV NEUR INF PRO, P1
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Li S., 2020, Caching mechanism for mobile edge computing in V2I networks, DOI DOI 10.1002/ETT.3689
   Li Z., 2019, PROC BRIT MACH VIS C, P1
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu YQ, 2022, IEEE T MULTIMEDIA, V24, P2259, DOI 10.1109/TMM.2021.3078615
   Maas AL, 2013, PROC INT C MACH LEAR
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Michaeli T, 2013, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2013.121
   Pan JS, 2018, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2018.00324
   Pan JS, 2018, IEEE T PATTERN ANAL, V40, P2315, DOI 10.1109/TPAMI.2017.2753804
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shocher A, 2019, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2019.00459
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Soh JW, 2020, PROC CVPR IEEE, P3513, DOI 10.1109/CVPR42600.2020.00357
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang LG, 2021, PROC CVPR IEEE, P10576, DOI 10.1109/CVPR46437.2021.01044
   Wang X., 2018, P NTIRE CHALL
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Yan YT, 2022, IEEE T MULTIMEDIA, V24, P1473, DOI 10.1109/TMM.2021.3065731
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yu JH, 2018, Arxiv, DOI arXiv:1808.08718
   Yu-Syuan Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12493, DOI 10.1109/CVPR42600.2020.01251
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 58
TC 10
Z9 10
U1 8
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2825
EP 2838
DI 10.1109/TMM.2022.3152090
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600029
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, DN
   Zhang, CY
   Song, Y
   Huang, H
   Wang, CY
   Barnett, M
   Cai, WD
AF Liu, Dongnan
   Zhang, Chaoyi
   Song, Yang
   Huang, Heng
   Wang, Chenyu
   Barnett, Michael
   Cai, Weidong
TI Decompose to Adapt: Cross-Domain Object Detection Via Feature
   Disentanglement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Object detection; Task analysis; Training;
   Visualization; Minimization; Data mining; Automatic drive; domain
   adaption; feature disentanglement; object detection
AB Recent advances in unsupervised domain adaptation (UDA) techniques have witnessed great success in cross-domain computer vision tasks, enhancing the generalization ability of data-driven deep learning architectures by bridging the domain distribution gaps. For the UDA-based cross-domain object detection methods, the majority of them alleviate the domain bias by inducing the domain-invariant feature generation via adversarial learning strategy. However, their domain discriminators have limited classification ability due to the unstable adversarial training process. Therefore, the extracted features induced by them cannot be perfectly domain-invariant and still contain domain-private factors, bringing obstacles to further alleviate the cross-domain discrepancy. To tackle this issue, we design a Domain Disentanglement Faster-RCNN (DDF) to eliminate the source-specific information in the features for detection task learning. Our DDF method facilitates the feature disentanglement at the global and local stages, with a Global Triplet Disentanglement (GTD) module and an Instance Similarity Disentanglement (ISD) module, respectively. By outperforming state-of-the-art methods on four benchmark UDA object detection tasks, our DDF method is demonstrated to be effective with wide applicability.
C1 [Liu, Dongnan; Zhang, Chaoyi; Cai, Weidong] Univ Sydney, Sch Comp Sci, Sydney, NSW 2008, Australia.
   [Song, Yang] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2008, Australia.
   [Huang, Heng] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15261 USA.
   [Wang, Chenyu; Barnett, Michael] Univ Sydney, Brain & Mind Ctr, Sydney, NSW 2050, Australia.
C3 University of Sydney; University of New South Wales Sydney; Pennsylvania
   Commonwealth System of Higher Education (PCSHE); University of
   Pittsburgh; University of Sydney
RP Liu, DN (corresponding author), Univ Sydney, Sch Comp Sci, Sydney, NSW 2008, Australia.
EM dongnan.liu@sydney.edu.au; czha5168@uni.sydney.edu.au;
   yang.song1@unsw.edu.au; henghuanghh@gmail.com;
   chenyu.wang@sydney.edu.au; michael@sydneyneurology.com.au;
   tom.cai@sydney.edu.au
RI Wang, Chenyu/IUO-3169-2023; LIU, DONGNAN/ISU-7922-2023; Song,
   Yangyi/JBJ-7119-2023; guo, ppdop/KAL-9865-2024; Zhang,
   Chaoyi/ABF-4443-2021; Cai, Tingwei Bill/AAJ-8822-2020
OI Song, Yangyi/0000-0002-3649-014X; Wang, Chenyu/0000-0001-7135-7662;
   Song, Yang/0000-0003-1283-1672; Cai, Weidong/0000-0003-3706-8896; Liu,
   Dongnan/0000-0001-8102-3949; Barnett, Michael/0000-0002-2156-8864
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Belghazi MI, 2018, PR MACH LEARN RES, V80
   Ben-David S., 2007, ADV NEURAL INFORM PR, V19
   Bousmalis K, 2016, ADV NEUR IN, V29
   Cai Q, 2019, PROC CVPR IEEE, P11449, DOI 10.1109/CVPR.2019.01172
   Cai RC, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2060
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chang-Dong Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11721, DOI 10.1109/CVPR42600.2020.01174
   Chattopadhyay Prithvijit, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P301, DOI 10.1007/978-3-030-58545-7_18
   Chen Chaoqi, 2020, P IEEE CVF C COMP VI, P8869, DOI DOI 10.1109/CVPR42600.2020.00889
   Chen X, 2016, ADV NEUR IN, V29
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Cheng-Chun Hsu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P733, DOI 10.1007/978-3-030-58545-7_42
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JH, 2021, PROC CVPR IEEE, P4089, DOI 10.1109/CVPR46437.2021.00408
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gholami B, 2020, IEEE T IMAGE PROCESS, V29, P3993, DOI 10.1109/TIP.2019.2963389
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   He ZW, 2019, IEEE I CONF COMP VIS, P6667, DOI 10.1109/ICCV.2019.00677
   Higgins I., 2016, BETA VAE LEARNING BA
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Huang J., 2021, ICCV, P8988
   Huang JX, 2021, PROC CVPR IEEE, P6887, DOI 10.1109/CVPR46437.2021.00682
   Huang JX, 2021, PROC CVPR IEEE, P10128, DOI 10.1109/CVPR46437.2021.01000
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525
   Johnson-Roberson Matthew, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P746, DOI 10.1109/ICRA.2017.7989092
   Kim T, 2019, PROC CVPR IEEE, P12448, DOI 10.1109/CVPR.2019.01274
   Kurmi VK, 2019, PROC CVPR IEEE, P491, DOI 10.1109/CVPR.2019.00058
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li YS, 2019, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR.2019.00710
   Lin C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8751, DOI 10.1109/ICCV48922.2021.00865
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu DN, 2021, IEEE T MED IMAGING, V40, P154, DOI 10.1109/TMI.2020.3023466
   Liu DN, 2020, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR42600.2020.00430
   Mathieu M, 2016, ADV NEUR IN, V29
   Minghao Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12352, DOI 10.1109/CVPR42600.2020.01237
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Paszke A., 2017, NIPS W
   Peng X., 2019, INT C MACH LEARN, P5102
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Shen ZQ, 2021, INT J COMPUT VISION, V129, P761, DOI 10.1007/s11263-020-01394-z
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Sindagi Vishwanath A., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P763, DOI 10.1007/978-3-030-58568-6_45
   Su P., 2020, EUR C COMP VIS, P403
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   VS Vibashan, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P4514, DOI 10.1109/CVPR46437.2021.00449
   Wang Y., 2021, P IEEE CVF C COMP VI, P9603
   Wu A, 2022, IEEE T PATTERN ANAL, V44, P4178, DOI 10.1109/TPAMI.2021.3060446
   Xingchao Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P756, DOI 10.1007/978-3-030-58539-6_45
   Yosinski J, 2014, ADV NEUR IN, V27
   Zhen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P54, DOI 10.1007/978-3-030-58604-1_4
   Zheng Y., 2020, P IEEE CVF C COMP VI, P13766
   Zhenwei He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P309, DOI 10.1007/978-3-030-58586-0_19
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu XG, 2019, PROC CVPR IEEE, P687, DOI 10.1109/CVPR.2019.00078
   Zhuang CF, 2020, AAAI CONF ARTIF INTE, V34, P13122
NR 71
TC 13
Z9 15
U1 5
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1333
EP 1344
DI 10.1109/TMM.2022.3141614
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100024
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, H
   Hu, MH
   Chen, YZ
   Li, QL
   Zhai, GT
   Yang, SX
   Zhang, XP
   Yang, XK
AF Liu, Hang
   Hu, Menghan
   Chen, Yuzhen
   Li, Qingli
   Zhai, Guangtao
   Yang, Simon X.
   Zhang, Xiao-Ping
   Yang, Xiaokang
TI Angel's Girl for Blind Painters: An Efficient Painting Navigation System
   Validated by Multimodal Evaluation Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Assistive device; multimodal evaluation metrics; painting navigation
   system; QR code positioning; visually impaired artists
ID PEOPLE; MOOD; AID
AB For people who ardently love painting but unfortunately have visual impairments, holding a paintbrush to create a work is a very difficult task. People in this special group are eager to pick up the paintbrush, like Leonardo da Vinci, to create and make full use of their own talents. Therefore, to maximally bridge this gap, we propose a painting navigation system called "Angle's Eyes" to assist blind people in artistic creation. The proposed system is composed of cognitive system and guidance system. The system adopts drawing board positioning based on QR code, brush navigation based on target detection and bush real-time positioning. Meanwhile, we design a simple yet efficient position information coding rule to remind the user of the current brush tip position. In addition, we design a criterion to efficiently judge whether the brush reaches the target or not. The numerous experiments are conducted to optimize and test the performance of the system. The results of real-world scenario experiments demonstrate that the developed system has great potential to help blind people with painting. This work also demonstrates that it is practicable for the blind people to feel the world through the brush in their hands. In the future, we plan to deploy "Angle's Eyes" on the phone to make it more portable. The demo video of the proposed painting navigation system is available at https://doi.org/10.6084/m9.figshare.9760004.v1.
C1 [Liu, Hang; Hu, Menghan; Chen, Yuzhen; Li, Qingli] East China Normal Univ, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200241, Peoples R China.
   [Hu, Menghan] Chongqing Inst East China Normal Univ, Chongqing Key Lab Precis Opt, Chongqing 401120, Peoples R China.
   [Zhai, Guangtao; Yang, Xiaokang] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
   [Yang, Simon X.] Univ Guelph, Sch Engn, Adv Robot & Intelligent Syst Lab, Guelph, ON N1G 2W1, Canada.
   [Zhang, Xiao-Ping] Ryerson Univ, Dept Elect Comp & Biomed Engn, Toronto, ON M5B 2K3, Canada.
C3 East China Normal University; Shanghai Jiao Tong University; University
   of Guelph; Toronto Metropolitan University
RP Hu, MH (corresponding author), East China Normal Univ, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200241, Peoples R China.; Hu, MH (corresponding author), Chongqing Inst East China Normal Univ, Chongqing Key Lab Precis Opt, Chongqing 401120, Peoples R China.
EM 10172100137@stu.ecnu.edu.cn; mhhu@ce.ecnu.edu.cn;
   51205904112@stu.ecnu.edu.cn; qlli@cs.ecnu.edu.cn;
   zhaiguangtao@sjtu.edu.cn; syang@uoguelph.ca; xzhang@ee.ryerson.ca;
   xkyang@sjtu.edu.cn
RI Yang, Simon X./A-4399-2008; Zhang, Xiao-Ping (Steven)/B-1436-2016; Yang,
   Xiaokang/C-6137-2009; yuan, lin/JDW-7387-2023; Hu,
   Menghan/AAK-7153-2021; Zhai, Guangtao/X-5949-2019
OI Yang, Simon X./0000-0002-6888-7993; Zhang, Xiao-Ping
   (Steven)/0000-0001-5241-0069; Yang, Xiaokang/0000-0003-4029-3322; Hu,
   Menghan/0000-0002-8557-8930; Zhai, Guangtao/0000-0001-8165-9322
FU National Natural Science Foundation of China [61831015]; Natural Science
   Foundation Project of CQ [cstc2021jcyj-msxmX0816]; Shanghai Education
   Development Foundation and Shanghai Municipal Education Commission
   [19CG27]; Science and Technology Commission of Shanghai Municipality
   [19511120100]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61831015, in part by the Natural Science
   Foundation Project of CQ cstc2021jcyj-msxmX0816, in part by the
   "Chenguang Program" supported by Shanghai Education Development
   Foundation and Shanghai Municipal Education Commission under Grant
   19CG27, and in part by the Science and Technology Commission of Shanghai
   Municipality under Grant 19511120100. The Associate Editor coordinating
   the review of this manuscript and approving it for publication was Dr
   Noel Codella.
CR Ainsworth S, 2011, SCIENCE, V333, P1096, DOI 10.1126/science.1204153
   Bacik J, 2017, INTEL SERV ROBOT, V10, P185, DOI 10.1007/s11370-017-0219-8
   Bornschein J, 2017, 10TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2017), P21, DOI 10.1145/3056540.3056542
   Bornschein J, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173689
   Brooke J., 1996, USABILITY EVALUATION, P189, DOI DOI 10.1201/9781498710411-35
   Cacciamani L, 2017, NEUROBIOL LEARN MEM, V141, P101, DOI 10.1016/j.nlm.2017.03.013
   Caine K, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P981, DOI 10.1145/2858036.2858498
   Cordeiro NH, 2019, ENG APPL ARTIF INTEL, V81, P180, DOI 10.1016/j.engappai.2019.02.016
   De Petrillo L, 2005, ART THER, V22, P205, DOI 10.1080/07421656.2005.10129521
   Drake JE, 2011, ART THER, V28, P26, DOI 10.1080/07421656.2011.557032
   Fan J. E., 2015, Translational Issues in Psychological Science, V1, P170, DOI DOI 10.1037/TPS0000037
   Fernandes MA, 2018, CURR DIR PSYCHOL SCI, V27, P302, DOI 10.1177/0963721418755385
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   Horton EL, 2017, ASSIST TECHNOL, V29, P28, DOI 10.1080/10400435.2016.1176083
   Hu MH, 2019, INT J ROBOT AUTOM, V34, P580, DOI 10.2316/J.2019.206-0302
   ILLUMENATE and N. C. L. OUTLET, LIGHT SOL HOM BUS ON
   Jeamwatthanachai W, 2019, BRIT J VISUAL IMPA, V37, P140, DOI 10.1177/0264619619833723
   Jiang B, 2019, IEEE INTERNET THINGS, V6, P1375, DOI 10.1109/JIOT.2018.2842229
   KaewTraKulPong P, 2002, VIDEO-BASED SURVEILLANCE SYSTEMS: COMPUTER VISION AND DISTRIBUTED PROCESSING, P135
   Kamel H. M., 1999, CHI 99 EXTENDED ABST, P222
   Kayukawa S, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382925
   Kayukawa S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300282
   Kolarik AJ, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0175750
   Li B, 2019, IEEE T MOBILE COMPUT, V18, P702, DOI 10.1109/TMC.2018.2842751
   Li LY, 2017, IEEE T CYBERNETICS, V47, P841, DOI 10.1109/TCYB.2016.2530407
   Likova LT, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00044
   LYON JG, 1995, APHASIOLOGY, V9, P33, DOI 10.1080/02687039508248687
   Martinez M, 2017, IEEE INT CONF COMP V, P1424, DOI 10.1109/ICCVW.2017.169
   Murata M, 2019, PERVASIVE MOB COMPUT, V57, P14, DOI 10.1016/j.pmcj.2019.04.003
   Pravin M, 2018, INT CONF COMPUT
   Prescher D, 2014, LECT NOTES COMPUT SC, V8548, P26, DOI 10.1007/978-3-319-08599-9_5
   Ranasinghe A, 2016, IEEE T CYBERNETICS, V46, P568, DOI 10.1109/TCYB.2015.2409772
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Saleem Y, 2022, IEEE T EMERG TOP COM, V10, P280, DOI 10.1109/TETC.2020.3014722
   Stankiewicz O, 2018, IEEE T MULTIMEDIA, V20, P2182, DOI 10.1109/TMM.2018.2790162
   Tan E., 2002, NATURE, V415
   Toni L, 2015, IEEE T MULTIMEDIA, V17, P1604, DOI 10.1109/TMM.2015.2450020
   U. E. P. A. O. of Noise Abatement and Control, 1974, INF LEV ENV NOIS REQ
   Vinter A, 2012, RES DEV DISABIL, V33, P1819, DOI 10.1016/j.ridd.2012.05.001
   W. H. Organization, BLINDN VIS IMP
   Wammes JD, 2016, Q J EXP PSYCHOL, V69, P1752, DOI 10.1080/17470218.2015.1094494
   Yatani K., STAT METHODS HCI RES
   Zhang XC, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9050989
   Zhao DY, 2022, IEEE T CYBERNETICS, V52, P508, DOI 10.1109/TCYB.2020.2977999
NR 44
TC 1
Z9 1
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2415
EP 2429
DI 10.1109/TMM.2022.3146767
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100063
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, JH
   Guo, JY
   Xu, D
AF Liu, Jiaheng
   Guo, Jinyang
   Xu, Dong
TI GeometryMotion-Transformer: An End-to-End Framework for 3D Action
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D action recognition; point cloud; transformer
ID GRAPH
AB In this work, we propose a new end-to-end optimized two-stream framework called GeometryMotion-Transformer (GMT) for 3D action recognition. We first observe that the existing 3D action recognition approaches cannot well extract motion representations from point cloud sequences. Specifically, when extracting motion representations, the existing approaches do not explicitly consider one-to-one correspondence among frames. Besides, the existing methods only extract the single-scale motion representations, which cannot well model the complex motion patterns of moving objects in point cloud sequences. To address these issues, we first propose the feature extraction module (FEM) to generate one-to-one correspondence among frames without using the voxelization process, and explicitly extract both geometry and multi-scale motion representations from raw point clouds. Moreover, we also observe the existing two-stream 3D action recognition approaches simply concatenate or add the geometry and motion features, which cannot well exploit the relationship between two-steam features. To this end, we also propose an improved transformer-based feature fusion module (FFM) to effectively fuse the two-stream features. Based on the proposed FEM and FFM, we build our GMT for 3D action recognition. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of our backbone GMT.
C1 [Liu, Jiaheng] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Guo, Jinyang] Beihang Univ, State Key Lab Software Dev Environm, Inst Artificial Intelligence, Beijing 100191, Peoples R China.
   [Xu, Dong] Univ Hong Kong, Dept Comp Sci, Pokfulam, Hong Kong, Peoples R China.
C3 Beihang University; Beihang University; University of Hong Kong
RP Xu, D (corresponding author), Univ Hong Kong, Dept Comp Sci, Pokfulam, Hong Kong, Peoples R China.
EM liujiaheng@buaa.edu.cn; jinyangguo@buaa.edu.cn; dongxu@hku.hk
OI Guo, Jinyang/0000-0003-1956-3367
FU National Key Research and Development Project of China [2018AAA0101900]
FX This work was supported by the National Key Research and Development
   Project of China under Grant 2018AAA0101900. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Liang Lin. (Jiaheng Liu and Jinyang Guo contributed
   equally to this work.) (Corresponding author: Dong Xu.)
CR Bilen H, 2018, IEEE T PATTERN ANAL, V40, P2799, DOI 10.1109/TPAMI.2017.2769085
   Caetano C, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909840
   Cai DG, 2022, PROC CVPR IEEE, P16443, DOI 10.1109/CVPR52688.2022.01597
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen C, 2019, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR.2019.00513
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Cheng Y.-B, 2021, PROC 2 ACM INT C MUL, P1
   Chuang Gan, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P5589, DOI 10.1109/CVPR.2018.00586
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan HD, 2022, PROC CVPR IEEE, P2959, DOI 10.1109/CVPR52688.2022.00298
   Fan H., 2021, INT C LEARN REPR
   Fan HH, 2021, PROC CVPR IEEE, P14199, DOI 10.1109/CVPR46437.2021.01398
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gan C, 2016, LECT NOTES COMPUT SC, V9907, P849, DOI 10.1007/978-3-319-46487-9_52
   Gan C, 2016, INT J COMPUT VISION, V120, P61, DOI 10.1007/s11263-016-0893-6
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Guo JY, 2022, IEEE T CIRC SYST VID, V32, P3659, DOI 10.1109/TCSVT.2021.3105820
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Heeseung Kwon, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P345, DOI 10.1007/978-3-030-58517-4_21
   Huang WB, 2019, IEEE T IMAGE PROCESS, V28, P1773, DOI 10.1109/TIP.2018.2877936
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Li J, 2022, IEEE T ENG MANAGE, V69, P1902, DOI 10.1109/TEM.2019.2940702
   Li JN, 2018, ADV NEUR IN, V31
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Liu JH, 2022, IEEE T IMAGE PROCESS, V31, P5287, DOI 10.1109/TIP.2022.3193290
   Liu JH, 2021, IEEE T CIRC SYST VID, V31, P4711, DOI 10.1109/TCSVT.2021.3101847
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P1453, DOI 10.1109/TPAMI.2019.2898954
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu XY, 2019, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2019.00062
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P1978, DOI 10.1109/TIP.2022.3147032
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Long X, 2022, IEEE T PATTERN ANAL, V44, P2140, DOI 10.1109/TPAMI.2020.3029554
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Lu JS, 2019, ADV NEUR IN, V32
   Misra I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2886, DOI 10.1109/ICCV48922.2021.00290
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Pan XR, 2021, PROC CVPR IEEE, P7459, DOI 10.1109/CVPR46437.2021.00738
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shaw P., 2018, P 2018 NAACL, V2, P464, DOI [DOI 10.18653/V1/N18-2074, 10.18653/v1/N18-2074]
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Simonyan K, 2014, ADV NEUR IN, V27
   Su R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1513, DOI 10.1109/ICCV48922.2021.00156
   Tran D, 2019, IEEE I CONF COMP VIS, P5551, DOI 10.1109/ICCV.2019.00565
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YC, 2020, PROC CVPR IEEE, P508, DOI 10.1109/CVPR42600.2020.00059
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xiao Y, 2019, INFORM SCIENCES, V480, P287, DOI 10.1016/j.ins.2018.12.050
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao L., 2021, P IEEECVF INT C COMP, P2928
   Zhao LC, 2021, IEEE T CIRC SYST VID, V31, P4735, DOI 10.1109/TCSVT.2021.3102025
NR 74
TC 2
Z9 2
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5649
EP 5661
DI 10.1109/TMM.2022.3198011
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500001
DA 2024-07-18
ER

PT J
AU Lu, M
   Chen, T
   Dai, ZY
   Wang, D
   Ding, DD
   Ma, Z
AF Lu, Ming
   Chen, Tong
   Dai, Zhenyu
   Wang, Dong
   Ding, Dandan
   Ma, Zhan
TI Decoder-Side Cross Resolution Synthesis for Video Compression
   Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video coding; cross resolution synthesis; super resolution; deep
   learning
ID SUPERRESOLUTION
AB This paper proposes a decoder-side Cross Resolution Synthesis (CRS) module to pursue better compression efficiency beyond the latest Versatile Video Coding (VVC), where we encode intra frames at original high resolution (HR), compress inter frames at a lower resolution (LR), and then super-resolve decoded LR inter frames with the help from preceding HR intra and neighboring LR inter frames. For a LR inter frame, a motion alignment and aggregation network (MAN) is devised to produce temporally aggregated motion representation to best guarantee the temporal smoothness; Another texture compensation network (TCN) is utilized to generate texture representation from decoded HR intra frame for better augmenting spatial details; Finally, a similarity-driven fusion engine synthesizes motion and texture representations to upscale LR inter frames for the removal of compression and resolution re-sampling noises. We enhance the VVC using proposed CRS, showing averaged 8.76% and 11.93% Bjontegaard Delta Rate (BD-Rate) gains against the latest VVC anchor in Random Access (RA) and Low-delay P (LDP) settings respectively. In addition, experimental comparisons to the state-of-the-art super-resolution (SR) based VVC enhancement methods, and ablation studies are conducted to further report superior efficiency and generalization of the proposed algorithm. All materials will be made to public at https://njuvision.github.io/CRS for reproducible research.
C1 [Lu, Ming; Chen, Tong; Ma, Zhan] Nanjing Univ, Nanjing 210023, Peoples R China.
   [Dai, Zhenyu; Wang, Dong] OPPO Inc, Nanjing, Peoples R China.
   [Ding, Dandan] Hangzhou Normal Univ, Hangzhou 310030, Peoples R China.
C3 Nanjing University; Hangzhou Normal University
RP Ma, Z (corresponding author), Nanjing Univ, Nanjing 210023, Peoples R China.
EM luming@smail.nju.edu.cn; tong@smail.nju.edu.cn; daizhenyu@oppo.com;
   wangdong@oppo.com; dandanding@hznu.edu.cn; mazhan@nju.edu.cn
RI Ma, Zhan/HKW-2859-2023; CHEN, TONG/HLX-4285-2023; Lu, Ming/IWM-1207-2023
OI Ma, Zhan/0000-0003-3686-4057; CHEN, TONG/0000-0001-5020-6099; Lu,
   Ming/0000-0002-5044-8802
FU National Natural Science Foundation of China [62022038, U20A20184,
   62171174]
FX This work was supported by the National Natural Science Foundation
   ofChina under Grants 62022038, U20A20184, and 62171174.
CR Alshina E., 2021, JVET DOC, V0023
   Barreto D, 2007, MULTIDIM SYST SIGN P, V18, P59, DOI 10.1007/s11045-007-0019-y
   Bjotegaard G., 2001, VCEGM33
   Brandi F, 2008, IEEE INT SYMP CIRC S, P1608, DOI 10.1109/ISCAS.2008.4541741
   Brandi F, 2008, IEEE IMAGE PROC, P321, DOI 10.1109/ICIP.2008.4711756
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   Chen H, 2021, IEEE T MULTIMEDIA, V23, P584, DOI 10.1109/TMM.2020.2985538
   Chen T., 2017, P IEEE VCIP DEC, P1, DOI DOI 10.1109/VCIP.2017.8305033
   Cheng M, 2021, IEEE T PATTERN ANAL, V43, P3275, DOI 10.1109/TPAMI.2020.2983371
   Chujoh T., 2021, V0073 JVET
   Deng JN, 2020, AAAI CONF ARTIF INTE, V34, P10696
   Fischer K, 2020, IEEE INT WORKSH MULT, DOI 10.1109/MMSP48831.2020.9287136
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho MM, 2020, LECT NOTES COMPUT SC, V11961, P99, DOI 10.1007/978-3-030-37731-1_9
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Kotra A., 2021, V0096 JVET
   Li TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921877
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu HJ, 2021, IEEE T CIRC SYST VID, V31, P3182, DOI 10.1109/TCSVT.2020.3035680
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Lu M, 2019, IEEE IMAGE PROC, P934, DOI [10.1109/ICIP.2019.8803049, 10.1109/icip.2019.8803049]
   Rassool R, 2017, IEEE INT SYM BROADB, P351
   Rippel O, 2019, IEEE I CONF COMP VIS, P3453, DOI 10.1109/ICCV.2019.00355
   Shen MM, 2011, IEEE T CIRC SYST VID, V21, P755, DOI 10.1109/TCSVT.2011.2130390
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Song L, 2013, INT WORK QUAL MULTIM, P34, DOI 10.1109/QoMEX.2013.6603201
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tong JC, 2019, IEEE IMAGE PROC, P929, DOI [10.1109/icip.2019.8803786, 10.1109/ICIP.2019.8803786]
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang R, 2018, PROC CVPR IEEE, P6664, DOI 10.1109/CVPR.2018.00697
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 39
TC 4
Z9 4
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2097
EP 2110
DI 10.1109/TMM.2022.3142414
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100039
DA 2024-07-18
ER

PT J
AU Ren, XA
   Zhang, DM
   Bao, XG
   Zhang, YD
AF Ren, Xuena
   Zhang, Dongming
   Bao, Xiuguo
   Zhang, Yongdong
TI S<SUP>2</SUP>-Net:Semantic and Saliency Attention Network for Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human semantic mask; person Re-ID; saliency attention; semantic
   attention
AB Person re-identification is still a challenging task when moving objects or another person occludes the probe person. Mainstream methods based on even partitioning apply an off-the-shelf human semantic parsing to highlight the non-collusion part. In this paper, we apply an attention branch to learn the human semantic partition to avoid misalignment introduced by even partitioning. In detail, we propose a semantic attention branch to learn 5 human semantic maps. We also note that some accessories or belongings, such as a hat, bag, may provide more informative clues to improve the person Re-ID. Human semantic parsing, however, usually treats non-human parts as distractions and discards them. To fetch the missing clues, we design a branch to capture the salient non-human parts. Finally, we merge the semantic and saliency attention to build an end-to-end network, named as S-2-Net. Specifically, to further improve Re-ID, we develop a trade-off weighting scheme between semantic and saliency attention and set the right weight with the actual scene. The extensive experiments show that S-2-Net gets the competitive performance. S-2-Net achieves 87.4% mAP on Market1501 and obtains 79.3%/56.1% rank-1/mAP on MSMT17 without semantic supervision. The source codes are available at https://github.com/upgirlnana/S2Net.
C1 [Ren, Xuena] Chinese Acad Sci, Inst Informat Engn, Beijing 100045, Peoples R China.
   [Ren, Xuena] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100045, Peoples R China.
   [Zhang, Dongming; Bao, Xiuguo] Natl Comp Network Emergency Response Tech Team Coo, Beijing 100029, Peoples R China.
   [Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS
RP Zhang, DM (corresponding author), Natl Comp Network Emergency Response Tech Team Coo, Beijing 100029, Peoples R China.
EM renxuena@iie.ac.cn; zhdm@cert.org.cn; baoxiuguo@139.com;
   zhyd73@ustc.edu.cn
OI Zhang, Dongming/0000-0002-1237-7177
FU National Key Research and Development Plan of China [2018YFB0804202];
   National Natural Science Foundation of China [61672495, U1736218]
FX This work was supported in part by the National Key Research and
   Development Plan of China under Grant 2018YFB0804202, and in part by the
   National Natural Science Foundation of China under Grants 61672495 and
   U1736218.
CR Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Cai HL, 2019, IEEE COMPUT SOC CONF, P1555, DOI 10.1109/CVPRW.2019.00197
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen XS, 2020, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR42600.2020.00336
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Fan X, 2019, LECT NOTES COMPUT SC, V11362, P19, DOI 10.1007/978-3-030-20890-5_2
   Fang PF, 2019, IEEE I CONF COMP VIS, P8029, DOI 10.1109/ICCV.2019.00812
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Ge YX, 2018, ADV NEUR IN, V31
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2018, Arxiv, DOI arXiv:1810.07399
   He LX, 2020, Arxiv, DOI arXiv:2006.02631
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Huang Bingyue, 2015, P INT C MULT MOD
   Jin GQ, 2019, INT CONF ACOUST SPEE, P3842, DOI [10.1109/ICASSP.2019.8683044, 10.1109/icassp.2019.8683044]
   Jin GQ, 2017, IEEE IMAGE PROC, P4262, DOI 10.1109/ICIP.2017.8297086
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Li W., 2021, arXiv
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Lingxiao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P357, DOI 10.1007/978-3-030-58604-1_22
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo XZ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102719
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Quispe R, 2019, IMAGE VISION COMPUT, V92, DOI 10.1016/j.imavis.2019.07.009
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Subramaniam A, 2016, ADV NEUR IN, V29
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang Y, 2018, PROC CVPR IEEE, P8042, DOI [10.1109/CVPR.2018.00839, 10.1109/CVPR.2018.00736]
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu SX, 2016, IEEE WINT CONF APPL
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yang F, 2019, PATTERN RECOGN, V86, P143, DOI 10.1016/j.patcog.2018.08.015
   Yao HT, 2017, IEEE INT CON MULTI, P1440, DOI 10.1109/ICME.2017.8019485
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang X, 2018, Arxiv, DOI arXiv:1711.08184
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou SP, 2019, IEEE I CONF COMP VIS, P8039, DOI 10.1109/ICCV.2019.00813
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhuo JX, 2019, Arxiv, DOI arXiv:1907.03253
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 76
TC 6
Z9 6
U1 5
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4387
EP 4399
DI 10.1109/TMM.2022.3174768
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W3PP1
UT WOS:001090784100003
DA 2024-07-18
ER

PT J
AU Song, MS
   Um, GM
   Lee, HK
   Seo, J
   Kim, W
AF Song, Minsoo
   Um, Gi-Mun
   Lee, Hee Kyung
   Seo, Jeongil
   Kim, Wonjun
TI Dynamic Residual Filtering With Laplacian Pyramid for Instance
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Laplace equations; Kernel; Shape; Image restoration; Convolution; Image
   segmentation; Image reconstruction; Instance segmentation; dynamic
   residual filtering; Laplacian pyramid-based decomposition;
   spatially-aware convolution filters
AB Various studies have been conducted on instance segmentation and made great strides over the past few years. Most recently, instance-specific mask generation via dynamic kernel predictions has shown the significant performance improvement even without bounding boxes as well as anchors. However, this scheme still does not fully consider dynamic properties since the size of the receptive field is not enough to cover the spatially-meaningful range due to memory limitations. Furthermore, the single-fused feature often fails to grasp complicated boundaries for objects of different sizes. In this article, we propose the dynamic residual filtering method with the Laplacian pyramid, which separately restores the global layout and local boundaries of instance masks. Specifically, we firstly apply the Laplacian pyramid-based decomposition scheme to features encoded from the backbone and subsequently restore sub-band mask residuals from coarse to fine pyramid levels. To do this, we design spatially-aware convolution filters to progressively capture the residual form of mask features at each level of the Laplacian pyramid while holding deformable receptive fields with dynamic offset information. This is fairly desirable since global and local properties of mask features can be accurately restored with keeping the spatial flexibility through the invertible process of the Laplacian reconstruction. Experimental results on the COCO dataset demonstrate that our proposed method achieves the state-of-the-art performance, i.e., 42.7% AP. The code and model are publicly available at: https://github.com/tjqansthd/LapMask.
C1 [Song, Minsoo; Kim, Wonjun] Konkuk Univ, Dept Elect & Elect Engn, Seoul 05029, South Korea.
   [Um, Gi-Mun; Lee, Hee Kyung; Seo, Jeongil] Elect & Telecommun Res Inst, Immers Media Res Sect, Daejeon 34129, South Korea.
C3 Konkuk University; Electronics & Telecommunications Research Institute -
   Korea (ETRI)
RP Kim, W (corresponding author), Konkuk Univ, Dept Elect & Elect Engn, Seoul 05029, South Korea.
EM tjqansthd@konkuk.ac.kr; gmum@etri.re.kr; lhk95@etri.re.kr;
   seoji@etri.re.kr; wonjkim@konkuk.ac.kr
RI Kim, Wonjun/JXN-3386-2024
FU Institute of Information and Communications Technology Planning and
   Evaluation(IITP) - Korea government (MSIT) [2018-0-00207]; Immersive
   Media Research Laboratory
FX This work was supported by Institute of Information and Communications
   Technology Planning and Evaluation(IITP) funded by Korea government
   (MSIT) under Grant 2018-0-00207, Immersive Media Research Laboratory.
CR Bolya D, 2022, IEEE T PATTERN ANAL, V44, P1108, DOI 10.1109/TPAMI.2020.3014297
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Canovas B, 2021, ETRI J, V43, P617, DOI 10.4218/etrij.2021-0061
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32
   Dai P, 2019, IEEE T MULTIMEDIA, V21, P1709, DOI 10.1109/TMM.2018.2885922
   De Brabandere B, 2016, ADV NEUR IN, V29
   Ding H, 2021, PROC CVPR IEEE, P8274, DOI 10.1109/CVPR46437.2021.00818
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ghiasi G, 2016, LECT NOTES COMPUT SC, V9907, P519, DOI 10.1007/978-3-319-46487-9_32
   Guo RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7137, DOI 10.1109/ICCV48922.2021.00707
   Hao Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8570, DOI 10.1109/CVPR42600.2020.00860
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Lee Y, 2020, P IEEE CVF C COMP VI
   Li Y., 2020, Adv. Neural Inf. Proces. Syst., V33, P1218
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Mehta S, 2019, PROC CVPR IEEE, P9182, DOI 10.1109/CVPR.2019.00941
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Neven D, 2019, PROC CVPR IEEE, P8829, DOI 10.1109/CVPR.2019.00904
   Nirkin Y, 2021, PROC CVPR IEEE, P4060, DOI 10.1109/CVPR46437.2021.00405
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   Paszke Adam, 2017, NIPS W
   Rufeng Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10223, DOI 10.1109/CVPR42600.2020.01024
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shao L, 2014, IEEE T CYBERNETICS, V44, P817, DOI 10.1109/TCYB.2013.2273174
   Sofiiuk K, 2019, IEEE I CONF COMP VIS, P7354, DOI 10.1109/ICCV.2019.00745
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10777, DOI 10.1109/ICCV48922.2021.01062
   Wang X., 2020, Advances in Neural information processing systems
   Wu Y., 2019, Detectron 2
   Xie E., 2019, P 2020 IEEE CVF C CO, P12190
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Xu WQ, 2019, IEEE I CONF COMP VIS, P5167, DOI 10.1109/ICCV.2019.00527
   Yang B, 2019, ADV NEUR IN, V32
   Yin CX, 2022, IEEE T MULTIMEDIA, V24, P4183, DOI 10.1109/TMM.2021.3114541
   Yu F., 2015, ARXIV
   Yuqing Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9310, DOI 10.1109/CVPR42600.2020.00933
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P352, DOI 10.1109/TMM.2021.3126430
   Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 55
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6892
EP 6903
DI 10.1109/TMM.2022.3215306
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000014
DA 2024-07-18
ER

PT J
AU Su, MY
   Gu, GH
   Ren, XL
   Fu, H
   Zhao, Y
AF Su, Mingyue
   Gu, Guanghua
   Ren, Xianlong
   Fu, Hao
   Zhao, Yao
TI Semi-Supervised Knowledge Distillation for Cross-Modal Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Knowledge engineering; Codes; Predictive models; Data models;
   Cows; Bridges; Cross-modal retrieval; semi-supervised hash learning;
   konwledge distillation; triplet ranking loss
ID REPRESENTATION
AB Deep hashing methods have achieved tremendous success in cross-modal retrieval, due to its low storage consumption and fast retrieval speed. Supervised cross-modal hashing methods have achieved substantial advancement by incorporating semantic information. However, to a great extent, supervised methods rely on large-scale labeled cross-modal training data which are laborious to obtain. Moreover, most cross-modal hashing methods only handle two modalities of image and text, without taking the scene of multiple modalities into consideration. In this paper, we propose a novel semi-supervised approach called semi-supervised knowledge distillation for cross-modal hashing (SKDCH) to overcome the above-mentioned challenges, which enables guiding a supervised method using outputs produced by a semi-supervised method for multimodality retrieval. Specifically, we utilize teacher-student optimization to propagate knowledge. Furthermore, we improves triplet ranking loss to better mitigate the heterogeneity gap, which increases the discriminability of our proposed approach. Extensive experiments executed on two benchmark datasets validate that the proposed SKDCH surpasses the state-of-the-art methods.
C1 [Su, Mingyue; Gu, Guanghua; Ren, Xianlong; Fu, Hao] Yanshan Univ, Sch Informat Sci & Engn, Hebei Key Lab Informat Transmiss & Signal Proc, Qinhuangdao 066000, Peoples R China.
   [Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
C3 Yanshan University; Beijing Jiaotong University
RP Gu, GH (corresponding author), Yanshan Univ, Sch Informat Sci & Engn, Hebei Key Lab Informat Transmiss & Signal Proc, Qinhuangdao 066000, Peoples R China.
EM guguanghua@ysu.edu.cn
OI Zhao, Yao/0000-0002-8581-9554
FU National Natural Science Foundation of China [62072394]; Natural Science
   Foundation of Hebei province [F2021203019]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62072394 and in part by Natural Science
   Foundation of Hebei province under Grant F2021203019. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Prof. L. Nie.
CR [Anonymous], 2016, PROC INT JOINT C ART, DOI DOI 10.5555/3060832.3060890
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Cai Q, 2019, PROC CVPR IEEE, P11449, DOI 10.1109/CVPR.2019.01172
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Chen X, 2016, ADV NEUR IN, V29
   Chen YT, 2018, AAAI CONF ARTIF INTE, P2852
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Finn C, 2016, ADV NEUR IN, V29
   Furlanello T, 2018, PR MACH LEARN RES, V80
   Hinton G., 2015, COMPUT SCI, V2
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Hu HT, 2020, PROC CVPR IEEE, P3120, DOI 10.1109/CVPR42600.2020.00319
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Komodakis N, 2017, P ICLR
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Lin QB, 2021, IEEE T MULTIMEDIA, V23, P550, DOI 10.1109/TMM.2020.2984081
   Liong VE, 2017, IEEE I CONF COMP VIS, P4097, DOI 10.1109/ICCV.2017.439
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Rupnik J., 2010, P C DATA MINING DATA, P1
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Tarvainen A, 2017, ADV NEUR IN, V30
   Wang XH, 2019, PROC CVPR IEEE, P3551, DOI 10.1109/CVPR.2019.00367
   Wei Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P791, DOI 10.1145/2623330.2623688
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Yang CL, 2019, AAAI CONF ARTIF INTE, P5628
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yu L, 2019, PROC CVPR IEEE, P2902, DOI 10.1109/CVPR.2019.00302
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang J, 2020, IEEE T CYBERNETICS, V50, P489, DOI 10.1109/TCYB.2018.2868826
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhang J, 2018, AAAI CONF ARTIF INTE, P539
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhou JT, 2018, IEEE T NEUR NET LEAR, V29, P6191, DOI 10.1109/TNNLS.2018.2827036
NR 46
TC 10
Z9 10
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 662
EP 675
DI 10.1109/TMM.2021.3129623
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800025
DA 2024-07-18
ER

PT J
AU Tang, ZY
   Zhang, RM
   Peng, ZL
   Chen, JR
   Lin, L
AF Tang, Ziyi
   Zhang, Ruimao
   Peng, Zhanglin
   Chen, Jinrui
   Lin, Liang
TI Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video-based person re-ID; transformer; spatial temporal modeling; deep
   representation learning
ID ATTENTION; NETWORK
AB In recent years, the Transformer architecture has shown its superiority in the video-based person re-identification task. Inspired by video representation learning, these methods mainly focus on designing modules to extract informative spatial and temporal features. However, they are still limited in extracting local attributes and global identity information, which are critical for the person re-identification task. In this paper, we propose a novel Multi-Stage Spatial-Temporal Aggregation Transformer (MSTAT) with two novel designed proxy embedding modules to address the above issue. Specifically, MSTAT consists of three stages to encode the attribute-associated, the identity-associated, and the attribute-identity-associated information from the video clips, respectively, achieving the holistic perception of the input person. We combine the outputs of all the stages for the final identification. In practice, to save the computational cost, the Spatial-Temporal Aggregation (STA) modules are first adopted in each stage to conduct the self-attention operations along the spatial and temporal dimensions separately. We further introduce the Attribute-Aware and Identity-Aware Proxy embedding modules (AAP and IAP) to extract the informative and discriminative feature representations at different stages. All of them are realized by employing newly designed self-attention operations with specific meanings. Moreover, temporal patch shuffling is also introduced to further improve the robustness of the model. Extensive experimental results demonstrate the effectiveness of the proposed modules in extracting the informative and discriminative information from the videos, and illustrate the MSTAT can achieve state-of-the-art accuracies on various standard benchmarks.
C1 [Tang, Ziyi; Zhang, Ruimao; Chen, Jinrui] Chinese Univ Hong Kong, Shenzhen 518172, Peoples R China.
   [Tang, Ziyi] Sun Yat Sen Univ, Guangzhou 510006, Peoples R China.
   [Peng, Zhanglin] Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
   [Lin, Liang] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
C3 The Chinese University of Hong Kong, Shenzhen; Sun Yat Sen University;
   University of Hong Kong; Sun Yat Sen University
RP Zhang, RM (corresponding author), Chinese Univ Hong Kong, Shenzhen 518172, Peoples R China.
EM tangziyi@cuhk.edu.cn; ruimao.zhang@ieee.org;
   zhanglin.peng@connect.hku.hk; 120090765@link.cuhk.edu.cn;
   linliang@ieee.org
RI peng, zhanglin/W-6217-2019; Lin, Liang/IQR-8601-2023
OI peng, zhanglin/0000-0002-1195-5895; Lin, Liang/0000-0003-2248-3755;
   Zhang, Ruimao/0000-0001-9511-7532
FU Young Scientists Fund of the National Natural Science Foundation of
   China [62106154]; National Key R&D Program of China [2021ZD0111600];
   Natural Science Foundation of Guangdong Province, China (General
   Program) [2022A1515011524]; Guangdong Basic and Applied Basic Research
   Foundation [2017A030312006]; CCF-Tencent Open Fund [RAGR20210115];
   Shenzhen Science and Technology Program [ZDSYS20211021111415025];
   Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese
   University of Hong Kong (Shenzhen)
FX This work was supported in part by the Young Scientists Fund of the
   National Natural Science Foundation of China under Grant 62106154, in
   part by the National Key R&D Program of China under Grant 2021ZD0111600,
   in part by the Natural Science Foundation of Guangdong Province, China
   (General Program) under Grant 2022A1515011524, in part by Guangdong
   Basic and Applied Basic Research Foundation under Grant 2017A030312006,
   in part by CCF-Tencent Open Fund under Grant RAGR20210115, in part by
   Shenzhen Science and Technology Program under Grant
   ZDSYS20211021111415025, and in part by the Guangdong Provincial Key
   Laboratory of Big Data Computing, The Chinese University of Hong Kong
   (Shenzhen).
CR Aich A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P152, DOI 10.1109/ICCV48922.2021.00022
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Ba J. L., 2016, Layer normalization
   Bak S, 2017, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR.2017.171
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Bolle RM, 2005, FOURTH IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P15, DOI 10.1109/AUTOID.2005.48
   Carreira J., 2018, A short note about kinetics-600
   Chai TR, 2022, IEEE T CIRC SYST VID, V32, P7951, DOI 10.1109/TCSVT.2022.3189027
   Chang ZG, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675368
   Chen CQ, 2022, IEEE T CIRC SYST VID, V32, P6100, DOI 10.1109/TCSVT.2022.3157130
   Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128
   Chen GY, 2020, IEEE T IMAGE PROCESS, V29, P6963, DOI 10.1109/TIP.2020.2995272
   Chen GY, 2019, IEEE T IMAGE PROCESS, V28, P4192, DOI 10.1109/TIP.2019.2908062
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P9887, DOI 10.1109/TPAMI.2021.3131222
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Chen X., 2021, Oh-former: Omni-relational high-order transformer for person re-identification
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Chung D, 2017, IEEE I CONF COMP VIS, P1992, DOI 10.1109/ICCV.2017.218
   Dehghan A, 2015, PROC CVPR IEEE, P4091, DOI 10.1109/CVPR.2015.7299036
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dong Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P323, DOI 10.1007/978-3-030-58604-1_20
   Dosovitskiy A., 2021, PROC INT C LEARN
   Fang PF, 2021, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV48630.2021.00051
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Guangyi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P660, DOI 10.1007/978-3-030-58598-3_39
   Guo MH, 2023, IEEE T PATTERN ANAL, V45, P5436, DOI 10.1109/TPAMI.2022.3211006
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han K., 2021, Adv. Neural Inf. Process. Syst., V34, P15908, DOI DOI 10.48550/ARXIV.2103.00112
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   He TY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1470, DOI 10.1109/ICCV48922.2021.00152
   Hermans A., 2017, In defense of the triplet loss for person re-identification
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou RB, 2019, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR.2019.00735
   Karanam S, 2015, IEEE I CONF COMP VIS, P4516, DOI 10.1109/ICCV.2015.513
   Li JN, 2019, AAAI CONF ARTIF INTE, P8618
   Li M., 2018, PROC EUR C COMPUT, P737
   Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liao XY, 2019, LECT NOTES COMPUT SC, V11366, P620, DOI 10.1007/978-3-030-20876-9_39
   Liu C.-T., 2019, PROC BRIT MACH VIS
   Liu CT, 2021, IEEE COMPUT SOC CONF, P1491, DOI 10.1109/CVPRW53098.2021.00165
   Liu CX, 2012, LECT NOTES COMPUT SC, V7583, P391, DOI 10.1007/978-3-642-33863-2_39
   Liu H, 2018, IEEE T CIRC SYST VID, V28, P2788, DOI 10.1109/TCSVT.2017.2715499
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu X., 2021, A video is worth three views: Trigeminal transformers for video-based person re-identification
   Liu XH, 2021, PROC CVPR IEEE, P13329, DOI 10.1109/CVPR46437.2021.01313
   Liu YH, 2019, AAAI CONF ARTIF INTE, P8786
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Nambiar A, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3243043
   Paszke A, 2019, ADV NEUR IN, V32
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Ruibing Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P388, DOI 10.1007/978-3-030-58595-2_24
   Shi YX, 2021, IEEE T MULTIMEDIA, V23, P4376, DOI 10.1109/TMM.2020.3042068
   Song WR, 2019, IEEE ACCESS, V7, P8508, DOI 10.1109/ACCESS.2019.2890836
   Subramaniam A, 2019, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2019.00065
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang S., 2020, Linformer: Self-attention with linear complexity
   Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wu AC, 2017, IEEE T IMAGE PROCESS, V26, P2588, DOI 10.1109/TIP.2017.2675201
   Wu DM, 2022, IEEE T INF FOREN SEC, V17, P115, DOI 10.1109/TIFS.2021.3075894
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu SX, 2016, IEEE WINT CONF APPL
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Xia B, 2019, IEEE I CONF COMP VIS, P3759, DOI 10.1109/ICCV.2019.00386
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xinqian Gu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P228, DOI 10.1007/978-3-030-58536-5_14
   Xiong R., 2020, INT C MACHINE LEARNI, P10524
   Xu SJ, 2017, IEEE I CONF COMP VIS, P4743, DOI 10.1109/ICCV.2017.507
   Yan YC, 2016, LECT NOTES COMPUT SC, V9910, P701, DOI 10.1007/978-3-319-46466-4_42
   Ye M, 2022, IEEE T INF FOREN SEC, V17, P386, DOI 10.1109/TIFS.2021.3139224
   Ye M, 2022, IEEE T IMAGE PROCESS, V31, P379, DOI 10.1109/TIP.2021.3131937
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2020, IEEE T IMAGE PROCESS, V29, P9387, DOI 10.1109/TIP.2020.2998275
   Yu F., 2020, Devil's in the details: Aligning visual clues for conditional embedding in person re-identification
   Yu ZX, 2022, IEEE T MULTIMEDIA, V24, P4482, DOI 10.1109/TMM.2021.3119133
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang JF, 2018, PROC CVPR IEEE, P6781, DOI 10.1109/CVPR.2018.00709
   Zhang L, 2021, IEEE T PATTERN ANAL, V43, P1460, DOI 10.1109/TPAMI.2020.2976969
   Zhang P, 2021, IEEE T MULTIMEDIA, V23, P3562, DOI 10.1109/TMM.2020.3028461
   Zhang RM, 2019, IEEE T IMAGE PROCESS, V28, P4870, DOI 10.1109/TIP.2019.2911488
   Zhang T., 2021, Spatiotemporal transformer for video-based person re-identification
   Zhang YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13557, DOI [10.1109/iccv48922.2021.01332, 10.1109/ICCV48922.2021.01332]
   Zhang YY, 2022, LECT NOTES COMPUT SC, V13674, P462, DOI 10.1007/978-3-031-19781-9_27
   Zhang ZZ, 2020, INT CONF CONDIT MON, P404, DOI 10.1109/CVPR42600.2020.01042
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng WS, 2016, IEEE T PATTERN ANAL, V38, P591, DOI 10.1109/TPAMI.2015.2453984
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717
NR 105
TC 6
Z9 6
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7917
EP 7929
DI 10.1109/TMM.2022.3231103
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400024
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, L
   Li, Q
   Zhou, SP
   Zheng, NN
AF Wang, Le
   Li, Qing
   Zhou, Sanping
   Zheng, Nanning
TI Multi-Panda Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Target tracking; Visualization; Filtering; Task
   analysis; Switches; Predictive models; Fine-grained feature; Multi-panda
   tracking; tracking-by-detection
ID DATA ASSOCIATION; MULTITARGET; VISION
AB Multi-Panda Tracking (MPT) is a video-based tracking task for panda individuals, which is conducive to the observation and measurement of distribution and status of pandas. Different from tracking general objects such as pedestrians and vehicles, MPT is extremely challenging due to the indistinguishable appearances and diversified postures of pandas. In this case, existing tracking methods cannot appropriately tackle with the excessive occlusion between different panda individuals, hence suffering from identity switch, missing and inaccurate detections. To address these problems, we propose a simple yet effective MPT framework in the tracking-by-detection paradigm, which is benefited both from a short-term prediction filtering module and a discriminative feature learning network. In particular, the short-term prediction filtering module introduces similarity learning to enhance the temporal consistency among detections, which is capable of supplementing the missing detections and discarding false positive detections. Besides, the discriminative feature learning network leverages a two-branch network to learn both local and global discriminative features, so as to distinguish different panda individuals with a very similar appearance with a subtle difference. To evaluate the proposed method, we annotate a large-scale MPT dataset, named PANDA2021, which is particularly challenging due to the similar appearance and dramatic occlusion between panda individuals. Experiments on PANDA2021 demonstrate that the proposed MPT method significantly outperforms the competing methods. Moreover, experimental results on pedestrian tracking dataset MOT16 further demonstrate that the proposed MPT method achieves comparative performance with competing methods.
C1 [Wang, Le; Li, Qing; Zhou, Sanping; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University
RP Zhou, SP (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
EM lewang@mail.xjtu.edu.cn; liqing_k@stu.xjtu.edu.cn;
   spzhou@mail.xjtu.edu.cn; nnzheng@mail.xjtu.edu.cn
OI Wang, Le/0000-0001-6636-6396
FU National Key R&D Program of China [2018AAA0101400]; NSFC [62088102,
   61976171, 62106192]; China Post-doctoral Science Foundation
   [2020M683490]; Natural Science Foundation of Shaanxi Province
   [2021JQ-054]; Fundamental Research Funds for the Central Universities
   [XTR042021005]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0101400, in part by NSFC under Grants 62088102,
   61976171, and 62106192, in part by China Post-doctoral Science
   Foundation under Grant 2020M683490, in part by the Natural Science
   Foundation of Shaanxi Province under Grant 2021JQ-054, and in part by
   the Fundamental Research Funds for the Central Universities under Grant
   XTR042021005.
CR Andriluka M, 2010, PROC CVPR IEEE, P623, DOI 10.1109/CVPR.2010.5540156
   [Anonymous], 2016, IEEE T SMART GRID, DOI DOI 10.1109/ISGTEUROPE.2016.7856217
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Betke M, 2007, PROC CVPR IEEE, P192
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Choi W, 2010, LECT NOTES COMPUT SC, V6314, P553, DOI 10.1007/978-3-642-15561-1_40
   Chu P, 2019, IEEE I CONF COMP VIS, P6171, DOI 10.1109/ICCV.2019.00627
   Chu Q, 2020, AAAI CONF ARTIF INTE, V34, P10672
   Dai P, 2019, IEEE T MULTIMEDIA, V21, P1709, DOI 10.1109/TMM.2018.2885922
   Dendorfer P., 2020, arXiv
   Ess A, 2008, PROC CVPR IEEE, P1857
   Ferris J, 2009, PALG STUD THEAT PERF, P1
   Fu ZY, 2019, IEEE T MULTIMEDIA, V21, P2277, DOI 10.1109/TMM.2019.2902480
   Gadde R, 2016, LECT NOTES COMPUT SC, V9905, P597, DOI 10.1007/978-3-319-46448-0_36
   Gao TZ, 2022, IEEE T MULTIMEDIA, V24, P995, DOI 10.1109/TMM.2021.3062489
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Ghosh S, 2018, IEEE GLOB CONF SIG, P26, DOI 10.1109/GlobalSIP.2018.8646671
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu YL, 2016, PROC CVPR IEEE, P5704, DOI 10.1109/CVPR.2016.615
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Kalman R.E., 1960, J BASIC ENG-T ASME, V82, P35, DOI [DOI 10.1115/1.3662552, 10.1115/1.3662552]
   Keuper M, 2020, IEEE T PATTERN ANAL, V42, P140, DOI 10.1109/TPAMI.2018.2876253
   Khan Z, 2004, LECT NOTES COMPUT SC, V2034, P279
   Kim C, 2015, IEEE I CONF COMP VIS, P4696, DOI 10.1109/ICCV.2015.533
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Leal-Taix‚ L, 2015, Arxiv, DOI arXiv:1504.01942
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li K, 2008, MED IMAGE ANAL, V12, P546, DOI 10.1016/j.media.2008.06.001
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu WL, 2013, IEEE T PATTERN ANAL, V35, P1704, DOI 10.1109/TPAMI.2012.242
   Luo WH, 2014, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2014.168
   Luo Yan, 2020, P IEEE CVF C COMP VI, P14065
   Maksai A, 2019, PROC CVPR IEEE, P4634, DOI 10.1109/CVPR.2019.00477
   Meijering E, 2009, SEMIN CELL DEV BIOL, V20, P894, DOI 10.1016/j.semcdb.2009.07.004
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Pang B, 2020, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR42600.2020.00634
   Paszke A, 2019, ADV NEUR IN, V32
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Revaud J, 2016, INT J COMPUT VISION, V120, P300, DOI 10.1007/s11263-016-0908-3
   Ristani E, 2018, PROC CVPR IEEE, P6036, DOI 10.1109/CVPR.2018.00632
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sanchez-Matilla R, 2016, LECT NOTES COMPUT SC, V9914, P84, DOI 10.1007/978-3-319-48881-3_7
   Shao S, 2018, Arxiv, DOI [arXiv:1805.00123, DOI 10.48550/ARXIV.1805.00123]
   Spampinato Concetto, 2012, Proceedings of the International Conference on Computer Vision Theory and Applications. VISAPP 2012, P409
   Spampinato C, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P514
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Sun SJ, 2021, IEEE T PATTERN ANAL, V43, P104, DOI 10.1109/TPAMI.2019.2929520
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang L, 2021, IEEE T IMAGE PROCESS, V30, P2837, DOI 10.1109/TIP.2021.3055627
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Wen LY, 2020, COMPUT VIS IMAGE UND, V193, DOI 10.1016/j.cviu.2020.102907
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu L, 2019, IEEE T CYBERNETICS, V49, P1791, DOI 10.1109/TCYB.2018.2813971
   Wu Z, 2012, PROC CVPR IEEE, P1948, DOI 10.1109/CVPR.2012.6247896
   Xing JL, 2011, IEEE T IMAGE PROCESS, V20, P1652, DOI 10.1109/TIP.2010.2102045
   Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409
   Xu YH, 2020, PROC CVPR IEEE, P6786, DOI 10.1109/CVPR42600.2020.00682
   Yan B, 2021, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR46437.2021.00525
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yu FW, 2016, LECT NOTES COMPUT SC, V9914, P36, DOI 10.1007/978-3-319-48881-3_3
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7
   Zhou QQ, 2019, IEEE T MULTIMEDIA, V21, P1183, DOI 10.1109/TMM.2018.2875360
   Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23
NR 75
TC 0
Z9 0
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 706
EP 720
DI 10.1109/TMM.2021.3130414
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900003
DA 2024-07-18
ER

PT J
AU Yan, JW
   Wang, JJ
   Li, Q
   Wang, CM
   Pu, SL
AF Yan, Jingwei
   Wang, Jingjing
   Li, Qiang
   Wang, Chunmao
   Pu, Shiliang
TI Weakly Supervised Regional and Temporal Learning for Facial Action Unit
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gold; Task analysis; Face recognition; Feature extraction;
   Representation learning; Optical imaging; Facial muscles; Facial action
   unit recognition; regional and temporal feature learning; weakly
   supervised learning
AB Automatic facial action unit (AU) recognition is a challenging task due to the scarcity of manual annotations. To alleviate this problem, a large amount of efforts has been dedicated to exploiting various weakly supervised methods which leverage numerous unlabeled data. However, many aspects with regard to some unique properties of AUs, such as the regional and relational characteristics, are not sufficiently explored in previous works. Motivated by this, we take the AU properties into consideration and propose two auxiliary AU related tasks to bridge the gap between limited annotations and the model performance in a self-supervised manner via the unlabeled data. Specifically, to enhance the discrimination of regional features with AU relation embedding, we design a task of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a single image based optical flow estimation task is proposed to leverage the dynamic change of facial muscles and encode the motion information into the global feature representation. Based on these two self-supervised auxiliary tasks, local features, mutual relation and motion cues of AUs are better captured in the backbone network. Furthermore, by incorporating semi-supervised learning, we propose an end-to-end trainable framework named weakly supervised regional and temporal learning (WSRTL) for AU recognition. Extensive experiments on BP4D and DISFA demonstrate the superiority of our method and new state-of-the-art performances are achieved.
C1 [Yan, Jingwei; Wang, Jingjing; Li, Qiang; Wang, Chunmao; Pu, Shiliang] Hikvis Res Inst, Hangzhou 310051, Peoples R China.
RP Wang, JJ (corresponding author), Hikvis Res Inst, Hangzhou 310051, Peoples R China.
EM yanjingwei@hikvision.com; wangjingjing9@hikvision.com;
   liqiang23@hikvision.com; wangchunmao@hikvision.com;
   pushiliang.hri@hikvision.com
RI Wang, Jingjing/GLT-7562-2022; Li, Qiang/AAR-9738-2021
OI Li, Qiang/0000-0002-6736-3389; Yan, Jingwei/0000-0001-9014-2050; Pu,
   Shiliang/0000-0001-5269-7821
CR [Anonymous], 2015, P 3 INT C LEARN REPR
   Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Berthelot D, 2019, ADV NEUR IN, V32
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chang WY, 2017, IEEE COMPUT SOC CONF, P1963, DOI 10.1109/CVPRW.2017.246
   Chu WS, 2016, Arxiv, DOI arXiv:1608.00911
   Corneanu C, 2018, LECT NOTES COMPUT SC, V11216, P309, DOI 10.1007/978-3-030-01258-8_19
   Dosovitskiy A, 2014, ADV NEUR IN, V27
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ekman Paul, 1997, What the Face Reveals: Basic and Applied Studies of Spontaneous Expression using the Facial Action Coding System (FACS)
   Gidaris S., 2018, P 6 INT C LEARNING R
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grandvalet Y., 2005, CAP, V367, P281
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jaiswal S, 2016, IEEE WINT CONF APPL
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Kolesnikov A, 2019, PROC CVPR IEEE, P1920, DOI 10.1109/CVPR.2019.00202
   Li C.-L., 2020, Advances in neural information processing systems, V33, P596
   Li GB, 2019, AAAI CONF ARTIF INTE, P8594
   Li W, 2017, PROC CVPR IEEE, P6766, DOI 10.1109/CVPR.2017.716
   Li Y, 2019, PROC CVPR IEEE, P10916, DOI 10.1109/CVPR.2019.01118
   Liu ZL, 2020, LECT NOTES COMPUT SC, V11962, P489, DOI 10.1007/978-3-030-37734-2_40
   Lu L., 2020, PROC BRIT MACH VIS C, P1
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Niu XS, 2019, PROC CVPR IEEE, P11909, DOI 10.1109/CVPR.2019.01219
   Niu Xuesong, 2019, NIPS, P909
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Paszke A, 2019, ADV NEUR IN, V32
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng GZ, 2019, AAAI CONF ARTIF INTE, P8827
   Peng GZ, 2018, PROC CVPR IEEE, P2188, DOI 10.1109/CVPR.2018.00233
   Samuli L., 2017, ICLR, P1
   Shao ZW, 2018, LECT NOTES COMPUT SC, V11217, P725, DOI 10.1007/978-3-030-01261-8_43
   Song TF, 2021, AAAI CONF ARTIF INTE, V35, P5993
   Tarvainen A, 2017, ADV NEUR IN, V30
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang SF, 2017, PATTERN RECOGN, V61, P78, DOI 10.1016/j.patcog.2016.07.028
   Wiles O., 2018, BRIT MACH VIS C
   Yan JW, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2005, DOI 10.1109/ICASSP39728.2021.9413551
   Yang JJ, 2016, INT C PATT RECOG, P4089, DOI 10.1109/ICPR.2016.7900274
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   [张醒 Zhang Xing], 2013, [火工品, Initiators & Pyrotechnics], P1
   Zhang Y, 2019, IEEE I CONF COMP VIS, P733, DOI 10.1109/ICCV.2019.00082
   Zhang Y, 2018, PROC CVPR IEEE, P5108, DOI 10.1109/CVPR.2018.00536
   Zhao KL, 2018, PROC CVPR IEEE, P2090, DOI 10.1109/CVPR.2018.00223
   Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369
NR 51
TC 1
Z9 1
U1 4
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1760
EP 1772
DI 10.1109/TMM.2022.3160061
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU You, JK
   Wang, YG
   Zhu, GP
   Wu, LG
   Zhang, HL
   Kwong, S
AF You, Jinkun
   Wang, Yuan-Gen
   Zhu, Guopu
   Wu, Ligang
   Zhang, Hongli
   Kwong, Sam
TI Estimating the Secret Key of Spread Spectrum Watermarking Based on
   Equivalent Keys
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Equivalent keys; Monte Carlo simulation; secret key estimation; spread
   spectrum watermarking; watermarking security
ID SECURITY; SCHEMES
AB The security of spread spectrum (SS) watermarking largely depends on the difficulty of estimating its secret key. Some estimators have been proposed to estimate the secret key in the known-message attack (KMA) scenario. However, the estimation accuracies of existing estimators are not satisfactory when the number of observations is not large enough. Currently, it is still a challenging and open problem to design more effective estimators. In this paper, we propose an equivalent keys (EK)-based estimator to estimate the secret key for both the traditional and more secure SS watermarking methods. Equivalent keys form an equivalent region, which is the intersection of a unit hypersphere and a hypercone. According to the Monte Carlo simulation, we find that the secret key can be estimated by adding up the equivalent keys uniformly sampled from the equivalent region. Thus, the proposed estimator selects equivalent keys from randomly-generated vectors by exploiting the pairs of watermarked signals and their embedded messages. A theoretical analysis is performed for the proposed estimator to evaluate the estimation accuracy. Experimental results verify the theoretical analysis and show the superiority of the proposed estimator over existing estimation methods. Furthermore, this paper also shows the insecurity of the more secure SS watermarking methods in the KMA scenario from a practical perspective for the first time.
C1 [You, Jinkun; Zhu, Guopu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [You, Jinkun] Univ Chinese Acad Sci, Shenzhen Coll Adv Technol, Shenzhen 518055, Peoples R China.
   [Wang, Yuan-Gen] Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 510006, Peoples R China.
   [Zhu, Guopu; Zhang, Hongli] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Wu, Ligang] Harbin Inst Technol, Dept Control Sci & Engn, Harbin 150001, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Guangzhou University; Harbin Institute of Technology;
   Harbin Institute of Technology; City University of Hong Kong
RP Zhu, GP (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
EM jk.you@siat.ac.cn; wangyg@gzhu.edu.cn; guopu.zhu@hit.edu.cn;
   ligangwu@hit.edu.cn; zhanghongli@hit.edu.cn; cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012; Wu, Ligang/K-2922-2012; liu, sha/JXL-6600-2024
OI Kwong, Sam/0000-0001-7484-7261; Wang, Yuan-Gen/0000-0003-3010-4196; Zhu,
   Guopu/0000-0001-7956-5343; You, Jinkun/0000-0003-1991-1851
FU National Key Research and Development Program of China [2020YFB1406902];
   National Natural Science Foundation of China [62172402, 62033005,
   61872350, 61872099]; Natural Science Foundation of Heilongjiang Province
   [ZD2021F001]; Tip-top Scientific and Technical Innovative Youth Talents
   of Guangdong Special Support Program [2019TQ05X696]; Basic Research
   Program of Shenzhen [JCYJ20170818163403748]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB1406902, in part by the
   National Natural Science Foundation of China under Grants 62172402,
   62033005, 61872350, and 61872099, in part by the Natural Science
   Foundation of Heilongjiang Province under Grant ZD2021F001, in part by
   the Tip-top Scientific and Technical Innovative Youth Talents of
   Guangdong Special Support Program under Grant 2019TQ05X696, and in part
   by the Basic Research Program of Shenzhen under Grant
   JCYJ20170818163403748.
CR [Anonymous], 1883, Journal des sciences militaires
   Bas P., 2006, PROC 8 WORKSHOP MULT, P80
   Bas P., 2007, Break our Watermarking System, V2nd
   Bas P, 2013, IEEE T INF FOREN SEC, V8, P1306, DOI 10.1109/TIFS.2013.2267960
   Bloom JA, 1999, P IEEE, V87, P1267, DOI 10.1109/5.771077
   Cao J, 2012, IEEE T INF FOREN SEC, V7, P821, DOI 10.1109/TIFS.2012.2184093
   Cao JA, 2010, LECT NOTES COMPUT SC, V6387, P249, DOI 10.1007/978-3-642-16435-4_19
   Cayre F, 2005, IEEE T SIGNAL PROCES, V53, P3976, DOI 10.1109/TSP.2005.855418
   Cayre F, 2008, IEEE T INF FOREN SEC, V3, P1, DOI 10.1109/TIFS.2007.916006
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   DIFFIE W, 1976, IEEE T INFORM THEORY, V22, P644, DOI 10.1109/TIT.1976.1055638
   Doérr G, 2004, IEEE T SIGNAL PROCES, V52, P2955, DOI 10.1109/TSP.2004.833867
   Furon Teddy, 2013, Information Hiding. 14th International Conference, IH 2012 Revised Selected Papers, P207, DOI 10.1007/978-3-642-36373-3_14
   Hua G, 2020, IEEE SIGNAL PROC LET, V27, P770, DOI 10.1109/LSP.2020.2986154
   Hua G, 2019, IEEE T CIRC SYST VID, V29, P625, DOI 10.1109/TCSVT.2018.2809585
   Huang Y, 2019, IEEE T MULTIMEDIA, V21, P2447, DOI 10.1109/TMM.2019.2907475
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Kalker T, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P201, DOI 10.1109/MMSP.2001.962734
   Kang XG, 2011, IEEE T MULTIMEDIA, V13, P181, DOI 10.1109/TMM.2010.2098850
   Lu CS, 2000, IEEE T MULTIMEDIA, V2, P209, DOI 10.1109/6046.890056
   Malvar HS, 2003, IEEE T SIGNAL PROCES, V51, P898, DOI 10.1109/TSP.2003.809385
   Mathon B, 2014, IEEE T IMAGE PROCESS, V23, P1694, DOI 10.1109/TIP.2014.2305873
   Mathon B, 2008, LECT NOTES COMPUT SC, V5284, P325
   Moulin P, 2003, IEEE T SIGNAL PROCES, V51, P1098, DOI 10.1109/TSP.2003.809370
   Nakashima Y, 2009, IEEE T MULTIMEDIA, V11, P443, DOI 10.1109/TMM.2009.2012938
   Pérez-Freire L, 2007, PROC SPIE, V6505, DOI 10.1117/12.704176
   Pérez-Freire L, 2009, IEEE T INF FOREN SEC, V4, P2, DOI 10.1109/TIFS.2008.2009603
   Subramanyam AV, 2012, IEEE T MULTIMEDIA, V14, P703, DOI 10.1109/TMM.2011.2181342
   Valizadeh A, 2012, IEEE T INF FOREN SEC, V7, P1127, DOI 10.1109/TIFS.2012.2199312
   Wang YG, 2021, IEEE T CIRC SYST VID, V31, P76, DOI 10.1109/TCSVT.2020.2971590
   Wang YG, 2018, IEEE T CYBERNETICS, V48, P2307, DOI 10.1109/TCYB.2017.2735989
   Wang YG, 2018, IEEE T IMAGE PROCESS, V27, P2063, DOI 10.1109/TIP.2018.2795745
   Xiang Y, 2015, IEEE-ACM T AUDIO SPE, V23, P2228, DOI 10.1109/TASLP.2015.2476755
   You JK, 2022, IEEE T CIRC SYST VID, V32, P483, DOI 10.1109/TCSVT.2021.3065199
NR 34
TC 1
Z9 1
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2459
EP 2473
DI 10.1109/TMM.2022.3147379
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600001
DA 2024-07-18
ER

PT J
AU Zhang, HD
   Ban, YX
   Guo, ZM
   Chen, K
   Zhang, XG
AF Zhang, Haodan
   Ban, Yixuan
   Guo, Zongming
   Chen, Ken
   Zhang, Xinggong
TI RAM360: Robust Adaptive Multi-Layer 360° Video Streaming With Lyapunov
   Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lyapunov optimization; multi-layer 360 degrees video streaming
ID PREDICTION
AB Viewport-adaptive streaming approaches are emerging as the most promising way to deliver high-quality 360(circle) videos. The viewport prediction techniques are developed to reduce bandwidth waste and improve users' Quality of Experience (QoE). However, the viewport prediction result is only reliable with a short prediction window, i.e., a short playback buffer, which conflicts with maintaining a long buffer to minimize the stall ratio. To deal with this problem, we present RAM360, a Robust Adaptive Multi-layer 360(circle) video streaming system, to ensure high viewport quality and low stall ratio concurrently. We make three technical contributions. First, we design a QoE-driven robust multi-layer streaming framework, where each chunk is encoded into multiple independent layers with different quality levels. The client can dynamically decide which chunk and which layer to download according to their QoE contributions. Thus, the client can enhance the low-quality chunks (including the mistakenly predicted ones) in time to improve the viewport quality. Meanwhile, the client can adaptively download new chunks to the buffer to decrease the risk of stall. Second, we establish a novel model as users' QoE metric throughout the playback progress, aiming to guide the client's download theoretically. Third, we utilize the Lyapunov optimization theory to solve the QoE optimization problem online while assuring our algorithm's near-optimality. We demonstrate that RAM360 can significantly outperform the existing schemes regarding the QoE (related to viewport quality, viewport quality oscillation, and stall ratio) through extensive experiments on public datasets.
C1 [Zhang, Haodan; Ban, Yixuan; Guo, Zongming; Zhang, Xinggong] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
   [Chen, Ken] Univ Paris 13, F-75006 Paris, France.
C3 Peking University; Universite Paris 13
RP Zhang, XG (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
EM pkuzhd@pku.edu.cn; banyixuan@pku.edu.cn; guozongming@pku.edu.cn;
   ken.chen@univ-paris13.fr; zhangxg@pku.edu.cn
FU National Key R&D Program of China [2019YFB1802701]; NSFC [U21B2012]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2019YFB1802701 and in part by NSFC under Grant U21B2012.
CR Almquist M, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P258, DOI 10.1145/3204949.3204970
   Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   Ban Y., 2018, P INT C MULT EXP, P1
   Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V1
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Camacho E.F., 1995, Model Predictive Control in the Process Industry, V1st
   Chao FY, 2021, IEEE T MULTIMEDIA, V23, P1811, DOI 10.1109/TMM.2020.3003642
   Chen JY, 2021, IEEE T MULTIMEDIA, V23, P3853, DOI 10.1109/TMM.2020.3033127
   Cheng Q, 2022, IEEE T MULTIMEDIA, V24, P1529, DOI 10.1109/TMM.2021.3067205
   Duanmu F, 2017, VR/AR NETWORK '17: PROCEEDINGS OF THE 2017 WORKSHOP ON VIRTUAL REALITY AND AUGMENTED REALITY NETWORK, P13, DOI 10.1145/3097895.3097898
   Duanmu F, 2017, IEEE INT SYMP CIRC S
   Eltobgy O, 2020, IEEE T MULTIMEDIA, V22, P3139, DOI 10.1109/TMM.2020.2973855
   Fan CL, 2020, IEEE T MULTIMEDIA, V22, P744, DOI 10.1109/TMM.2019.2931807
   Guan Y, 2019, SIGCOMM '19 - PROCEEDINGS OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P394, DOI 10.1145/3341302.3342063
   Hou XS, 2021, IEEE T MULTIMEDIA, V23, P716, DOI 10.1109/TMM.2020.2987693
   Jiang ZQ, 2020, IEEE T VEH TECHNOL, V69, P2157, DOI 10.1109/TVT.2019.2960866
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Long KX, 2021, IEEE T MULTIMEDIA, V23, P3670, DOI 10.1109/TMM.2020.3029880
   Maniotis P, 2022, IEEE T MULTIMEDIA, V24, P386, DOI 10.1109/TMM.2021.3052339
   Maniotis P, 2020, IEEE T MULTIMEDIA, V22, P2382, DOI 10.1109/TMM.2019.2957993
   Nasrabadi AT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1689, DOI 10.1145/3123266.3123414
   Neely M. J., 2010, STOCHASTIC NETWORK O
   Puterman ML., 2014, MARKOV DECISION PROC, DOI DOI 10.1002/9780470316887
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Qian F, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P99, DOI 10.1145/3241539.3241565
   Qiao ML, 2021, IEEE T MULTIMEDIA, V23, P748, DOI 10.1109/TMM.2020.2987682
   Song JR, 2020, IEEE T MULTIMEDIA, V22, P2366, DOI 10.1109/TMM.2019.2957976
   Spiteri K, 2020, IEEE ACM T NETWORK, V28, P1698, DOI 10.1109/TNET.2020.2996964
   Sun LY, 2019, IEEE J EM SEL TOP C, V9, P43, DOI 10.1109/JETCAS.2019.2898877
   Sun LY, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P162, DOI 10.1145/3204949.3204978
   support.google.com/, Encoder settings for live 360 degree videos
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210
   Xie L, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P564, DOI 10.1145/3240508.3240556
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Yadav PK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3724, DOI 10.1145/3394171.3413550
   Zhang B, 2010, IEEE ACM T NETWORK, V18, P229, DOI 10.1109/TNET.2009.2024083
   Zhang YX, 2019, IEEE INFOCOM SER, P1252, DOI [10.1109/INFOCOM.2019.8737361, 10.1109/infocom.2019.8737361]
   Zhao JB, 2017, P IEEE VIRT REAL ANN, P313, DOI 10.1109/VR.2017.7892302
   Zhu YC, 2020, IEEE T MULTIMEDIA, V22, P2331, DOI 10.1109/TMM.2019.2957986
NR 40
TC 6
Z9 7
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4225
EP 4239
DI 10.1109/TMM.2022.3172550
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200012
DA 2024-07-18
ER

PT J
AU Zhang, L
   Huang, H
AF Zhang, Lei
   Huang, Hua
TI Image Stitching With Manifold Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Alignment; distortion; general linear group; image stitching; manifold
ID ALIGNMENT; FEATURES; WARPS
AB Image stitching usually relies on spatial transformations to perform the overlap alignment and distortion mitigation. This paper presents a manifold optimization method to seek these transformations. The purpose is not to present a new formulation of image stitching, as the proposed method uses common transformations such as homography to align feature correspondences in the overlap and similarity transformations to preserve the shape. Instead, the proposed method is based on a new treatment of these transformations as elements of a prescribed matrix manifold. Its advantage lies in its more effective and efficient optimization in the manifold domain. Specifically, spatially varying homographies are computed by an efficient second-order minimization (ESM) of the geometric error of aligning feature correspondences, but with their intrinsic manifold parameterization. To mitigate the distortion, the interpolation between homography and similarity transformation is performed on a general matrix manifold. These on-manifold operations improve the stitching quality with fewer ghosting and distortion artifacts. The experiments show our manifold optimization for image stitching outperforms other methods.
C1 [Zhang, Lei] Beijing Inst Technol, Sch Comp, Beijing Key Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Huang, Hua] Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.
C3 Beijing Institute of Technology; Beijing Normal University
RP Huang, H (corresponding author), Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.
EM leizhang@bit.edu.cn; huahuang@bnu.edu.cn
RI Huang, Hua/M-9684-2013
OI Huang, Hua/0000-0003-2587-1702
FU National Natural Science Foundation of China [61922014, 62132012]
FX & nbsp;This work was supported by the National Natural Science
   Foundation of China under Grants 61922014 and 62132012. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Prof. Engin Erzin.& nbsp;
CR Abeo TA, 2019, IEEE ACCESS, V7, P38123, DOI 10.1109/ACCESS.2019.2906244
   Benhimane S, 2007, INT J ROBOT RES, V26, P661, DOI 10.1177/0278364907080252
   Bernard F, 2017, PROC CVPR IEEE, P1436, DOI 10.1109/CVPR.2017.157
   Bertel T, 2019, IEEE T VIS COMPUT GR, V25, P1828, DOI 10.1109/TVCG.2019.2898799
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   Chum O, 2005, COMPUT VIS IMAGE UND, V97, P86, DOI 10.1016/j.cviu.2004.03.004
   Gao J., 2013, Eurographics (Short Papers), P45, DOI DOI 10.2312/CONF/EG2013/SHORT/045-048
   Gao JH, 2011, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2011.5995433
   Guo H, 2016, IEEE T IMAGE PROCESS, V25, P5491, DOI 10.1109/TIP.2016.2607419
   Hall BC, 2004, Lie groups, Lie algebras and representations, an elementary introduction
   Hamel T, 2011, IEEE DECIS CONTR P, P7902
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Hartley RI, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P469, DOI 10.1109/ICCV.1998.710760
   Higham NJ, 2005, SIAM J MATRIX ANAL A, V26, P1179, DOI 10.1137/04061101X
   HOWE R, 1983, AM MATH MON, V90, P600, DOI 10.2307/2323277
   Jia JY, 2008, IEEE T PATTERN ANAL, V30, P617, DOI 10.1109/TPAMI.2007.70729
   Jia Q, 2021, PROC CVPR IEEE, P12181, DOI 10.1109/CVPR46437.2021.01201
   Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650
   Kopf J, 2009, COMPUT GRAPH FORUM, V28, P1083, DOI 10.1111/j.1467-8659.2009.01485.x
   Levin A, 2004, LECT NOTES COMPUT SC, V2034, P377
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Li N, 2018, IEEE T MULTIMEDIA, V20, P1365, DOI 10.1109/TMM.2017.2771566
   Liao TL, 2020, IEEE T IMAGE PROCESS, V29, P724, DOI 10.1109/TIP.2019.2934344
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Lin KM, 2016, LECT NOTES COMPUT SC, V9907, P370, DOI 10.1007/978-3-319-46487-9_23
   Lin KM, 2016, COMPUT GRAPH FORUM, V35, P479, DOI 10.1111/cgf.12848
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Madhusudana PC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921858
   Nie L, 2022, IEEE T CIRC SYST VID, V32, P4460, DOI 10.1109/TCSVT.2021.3125736
   Nie L, 2021, IEEE T IMAGE PROCESS, V30, P6184, DOI 10.1109/TIP.2021.3092828
   Nie YW, 2018, IEEE T IMAGE PROCESS, V27, P164, DOI 10.1109/TIP.2017.2736603
   Nie YW, 2014, IEEE T VIS COMPUT GR, V20, P1303, DOI 10.1109/TVCG.2013.2297931
   Qian YT, 2013, IEEE IMAGE PROC, P1341, DOI 10.1109/ICIP.2013.6738276
   Rosenhahn B, 2008, PROC CVPR IEEE, P1381
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Tang WK, 2005, IEEE T MULTIMEDIA, V7, P280, DOI 10.1109/TMM.2005.843811
   Zaragoza J, 2013, PROC CVPR IEEE, P2339, DOI 10.1109/CVPR.2013.303
   Zhang F, 2015, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2015.7298811
   Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423
   Zhang HY, 2012, J MATH IMAGING VIS, V44, P80, DOI 10.1007/s10851-011-0312-0
   Zhang L, 2017, IEEE T IMAGE PROCESS, V26, P2219, DOI 10.1109/TIP.2017.2676354
NR 45
TC 5
Z9 5
U1 4
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3469
EP 3482
DI 10.1109/TMM.2022.3161839
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200041
DA 2024-07-18
ER

PT J
AU Zhu, JC
   Zhang, XY
   Fang, X
   Wang, YX
   Tan, PL
   Liu, JN
AF Zhu, Jinchao
   Zhang, Xiaoyu
   Fang, Xian
   Wang, Yuxuan
   Tan, Panlong
   Liu, Junnan
TI Perception-and-Regulation Network for Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Regulation; Object detection; Feature extraction;
   Convolution; Logic gates; Task analysis; Salient object detection;
   convolutional neural networks; attention mechanism; global perception
ID MODEL
AB Effective fusion of different types of features is the key to salient object detection (SOD). The majority of the existing network structure designs are based on the subjective experience of scholars, and the process of feature fusion does not consider the relationship between the fused features and the highest-level features. In this paper, we focus on the feature relationship and propose a novel global attention unit, which we term the "perception-and-regulation" (PR) block, that adaptively regulates the feature fusion process by explicitly modelling the interdependencies between features. The perception part uses the structure of the fully connected layers in the classification networks to learn the size and shape of the objects. The regulation part selectively strengthens and weakens the features to be fused. An imitating eye observation module (IEO) is further employed to improve the global perception capabilities of the network. The imitation of foveal vision and peripheral vision enables the IEO to scrutinize highly detailed objects and to organize a broad spatial scene to better segment objects. Sufficient experiments conducted on the SOD datasets demonstrate that the proposed method performs favourably against the 29 state-of-the-art methods.
C1 [Zhu, Jinchao; Zhang, Xiaoyu; Wang, Yuxuan; Tan, Panlong] Nankai Univ, Coll Artificial Intelligence, Tianjin 300350, Peoples R China.
   [Zhu, Jinchao] Tsinghua Univ, Dept Automat, BNRist, Tianjin 300350, Peoples R China.
   [Fang, Xian] Nankai Univ, Coll Comp Sci, Tianjin 300350, Peoples R China.
   [Liu, Junnan] Harbin Engn Univ, Coll Intelligent Syst Sci & Engn, Harbin 150001, Peoples R China.
C3 Nankai University; Tsinghua University; Nankai University; Harbin
   Engineering University
RP Zhang, XY (corresponding author), Nankai Univ, Coll Artificial Intelligence, Tianjin 300350, Peoples R China.
EM jczhu@mail.nankai.edu.cn; zhangxiaoyu@nankai.edu.cn;
   xianfang@mail.nankai.edu.cn; 843434046@qq.com; 296055178@qq.com;
   1070715836@hrbeu.edu.cn
RI Fang, Xian/KMY-0913-2024; Liu, Junnan/ABD-5144-2021
OI Fang, Xian/0000-0001-5161-2574; wang, yu xuan/0000-0002-5743-2029; Zhu,
   Jinchao/0000-0003-2821-4847
FU National Natural Science Foundation of China [U21B6001, 62103204];
   Tianjin Graduate Scientific Research Innovation Project [2021YJSO2S02]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant U21B6001, in part by the National
   Natural Science Foundation of China under Grant 62103204, and in part by
   the Tianjin Graduate Scientific Research Innovation Project under Grant
   2021YJSO2S02. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Palaiahnakote
   Shivakumara.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Q, 2021, AAAI CONF ARTIF INTE, V35, P1063
   Chen Q, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107740
   Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3763, DOI 10.1109/TIP.2020.2965989
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Eymond C, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-69329-9
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]
   Islam MA, 2017, PROC CVPR IEEE, P4877, DOI 10.1109/CVPR.2017.518
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang ZL, 2013, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR.2013.266
   Klein DA, 2011, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2011.6126499
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu W., 2015, ARXIV150604579
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2021, IEEE T IMAGE PROCESS, V30, P3804, DOI 10.1109/TIP.2021.3065239
   Liu Y, 2021, IEEE T CYBERNETICS, V51, P4439, DOI 10.1109/TCYB.2020.3035613
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Mahadevan V, 2009, PROC CVPR IEEE, P1007, DOI 10.1109/CVPRW.2009.5206573
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Mohammadi S, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107303
   Park J., 2018, BRIT MACH VIS C, P147
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qiao SY, 2021, PROC CVPR IEEE, P10208, DOI 10.1109/CVPR46437.2021.01008
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K., 2014, CORR
   Stewart EEM, 2020, J VISION, V20, DOI 10.1167/jov.20.12.2
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang TT, 2019, IEEE I CONF COMP VIS, P8837, DOI 10.1109/ICCV.2019.00893
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yu SY, 2021, AAAI CONF ARTIF INTE, V35, P3234
   Zhang D., 2020, Adv. Neural Info. Process. Syst., V33, P12236
   Zhang J, 2022, IEEE T PATTERN ANAL, V44, P5761, DOI 10.1109/TPAMI.2021.3073564
   Zhang J, 2021, IEEE T PATTERN ANAL, V43, P2866, DOI 10.1109/TPAMI.2020.3046486
   Zhang Jing, 2020, P IEEECVF C COMPUTER, P12546, DOI DOI 10.1109/CVPR42600.2020.01256
   Zhang L, 2019, PROC CVPR IEEE, P6017, DOI 10.1109/CVPR.2019.00618
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhao X., 2020, P EUR C COMP VIS, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhou H., 2020, P IEEE C COMP VIS PA, P9141, DOI 10.1109/CVPR42600.2020.00916
NR 84
TC 1
Z9 1
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6525
EP 6537
DI 10.1109/TMM.2022.3210366
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500063
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, TZ
   Pan, HH
   Wang, ZD
   Gao, HJ
AF Gao, Tianze
   Pan, Huihui
   Wang, Zidong
   Gao, Huijun
TI A CRF-Based Framework for Tracklet Inactivation in Online Multi-Object
   Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Pipelines; Inference algorithms; Detectors; Training;
   Machine learning; Head; Conditional random field; online multi-object
   tracking; tracklet inactivation
ID MULTITARGET
AB Online multi-object tracking (MOT) is an active research topic in the domain of computer vision. Although many previously proposed algorithms have exhibited decent results, the issue of tracklet inactivation has not been sufficiently studied. Simple strategies such as using a fixed threshold on classification scores are adopted, yielding undesirable tracking mistakes and limiting the overall performance. In this paper, a conditional random field (CRF) based framework is put forward to tackle the tracklet inactivation issue in online MOT problems. A discrete CRF which exploits the intra-frame relationship between tracking hypotheses is developed to improve the robustness of tracklet inactivation. Separate sets of feature functions are designed for the unary and binary terms in the CRF, which take into account various tracking challenges in practical scenarios. To handle the problem of varying CRF nodes in the MOT context, two strategies named as hypothesis filtering and dummy nodes are employed. In the proposed framework, the inference stage is conducted by using the loopy belief propagation algorithm, and the CRF parameters are determined by utilizing the maximum likelihood estimation method followed by slight manual adjustment. Experimental results show that the tracker combined with the CRF-based framework outperforms the baseline on the MOT16 and MOT17 benchmarks. The extensibility of the proposed framework is further validated by an extensive experiment.
C1 [Gao, Tianze; Pan, Huihui; Gao, Huijun] Harbin Inst Technol, Res Inst Intelligent Control & Syst, Harbin 150001, Peoples R China.
   [Wang, Zidong] Brunel Univ London, Dept Comp Sci, Uxbridge UB8 3PH, Middx, England.
   [Pan, Huihui] Ningbo Inst Intelligent Equipment Technol Co Ltd, Ningbo 315200, Peoples R China.
C3 Harbin Institute of Technology; Brunel University
RP Gao, HJ (corresponding author), Harbin Inst Technol, Res Inst Intelligent Control & Syst, Harbin 150001, Peoples R China.
EM gao2990026796@gmail.com; huihuipan@hit.edu.cn; Zidong.Wang@brunet.ac.uk;
   hjgao@hit.edu.cn
RI Gao, Huijun/B-6853-2013; Wang, Zidong/H-1523-2011; GAO,
   Tianze/HCI-5451-2022
OI Wang, Zidong/0000-0002-9576-7401; GAO, Tianze/0000-0003-0837-6638; Pan,
   Huihui/0000-0002-8931-1774
FU National Natural Science Foundation of China [U1964201, 61803120,
   61790562]
FX This work was supported in part by the National Natural Science
   Foundation ofChina underGrantsU1964201, 61803120, and 61790562. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Marco Carli. (Corresponding author:
   Huijun Gao.)
CR [Anonymous], 2019, CVPR
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Chen JH, 2017, IEEE COMPUT SOC CONF, P2143, DOI 10.1109/CVPRW.2017.266
   Chen L, 2018, IEEE INT CON MULTI, DOI 10.1097/PEC.0000000000001553
   Chu P, 2019, IEEE I CONF COMP VIS, P6171, DOI 10.1109/ICCV.2019.00627
   Chu P, 2019, IEEE WINT CONF APPL, P161, DOI 10.1109/WACV.2019.00023
   Chu Q, 2017, IEEE I CONF COMP VIS, P4846, DOI 10.1109/ICCV.2017.518
   Ciaparrone G, 2020, NEUROCOMPUTING, V381, P61, DOI 10.1016/j.neucom.2019.11.023
   Dai P, 2019, IEEE T MULTIMEDIA, V21, P1709, DOI 10.1109/TMM.2018.2885922
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Fang K, 2018, IEEE WINT CONF APPL, P466, DOI 10.1109/WACV.2018.00057
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu ZY, 2019, IEEE T MULTIMEDIA, V21, P2277, DOI 10.1109/TMM.2019.2902480
   Heili A, 2014, IEEE T IMAGE PROCESS, V23, P3040, DOI 10.1109/TIP.2014.2324292
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Karunasekera H, 2019, IEEE ACCESS, V7, P104423, DOI 10.1109/ACCESS.2019.2932301
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Lu YY, 2017, IEEE I CONF COMP VIS, P2363, DOI 10.1109/ICCV.2017.257
   Mekonnen AA, 2019, IEEE T CIRC SYST VID, V29, P996, DOI 10.1109/TCSVT.2018.2817609
   Milan A., 2016, ARXIV160300831
   Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033
   Pearl J., 1988, UNI
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41
   Sheng H, 2019, IEEE T CIRC SYST VID, V29, P3660, DOI 10.1109/TCSVT.2018.2881123
   Song YM, 2019, IEEE ACCESS, V7, P165103, DOI 10.1109/ACCESS.2019.2953276
   Sutton C, 2012, FOUND TRENDS MACH LE, V4, P267, DOI 10.1561/2200000013
   Tian W, 2020, IEEE T INTELL TRANSP, V21, P374, DOI 10.1109/TITS.2019.2892413
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Xiang J, 2021, IEEE T CIRC SYST VID, V31, P275, DOI 10.1109/TCSVT.2020.2975842
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409
   Yang B, 2012, PROC CVPR IEEE, P2034, DOI 10.1109/CVPR.2012.6247907
   Yang F, 2016, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2016.234
   Yoon K, 2020, IEEE ACCESS, V8, P38060, DOI 10.1109/ACCESS.2020.2975912
   Yoon Y.-C., 2020, INFORM SCIENCES
   Yu FW, 2016, LECT NOTES COMPUT SC, V9914, P36, DOI 10.1007/978-3-319-48881-3_3
   Zhou H, 2019, IEEE T CIRC SYST VID, V29, P1011, DOI 10.1109/TCSVT.2018.2825679
   Zhou QQ, 2019, IEEE T MULTIMEDIA, V21, P1183, DOI 10.1109/TMM.2018.2875360
   Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23
NR 46
TC 11
Z9 12
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 995
EP 1007
DI 10.1109/TMM.2021.3062489
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100037
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jing, PG
   Zhang, J
   Nie, LQ
   Ye, S
   Liu, J
   Su, YT
AF Jing, Peiguang
   Zhang, Jing
   Nie, Liqiang
   Ye, Shu
   Liu, Jing
   Su, Yuting
TI Tripartite Graph Regularized Latent Low-Rank Representation for Fashion
   Compatibility Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Task analysis; Correlation; Semantics; Clothing;
   Visualization; Industries; Correlation; fashion compatibility; graph
   regularization; latent low-rank representation
ID THRESHOLDING ALGORITHM; REGRESSION; RETRIEVAL
AB In recent years, an increasing online shopping demand has greatly promoted the innovation and development of the fashion industry. Visual fashion analysis has become a prospective research topic in computer vision and multimedia fields. Among these studies, fashion compatibility analysis is required in many real applications, such as fashion recommendation, matching, and retrieval. However, learning fashion compatibility is nontrivial, not only due to the uncertain and sparse dependencies among fashion items but also the latent and mutual associations among multiple factors such as color, texture, style, and functionality. To better predict fashion compatibility, in this paper, we proposed a tripartite graph regularized latent low-rank representation method, named TGRLLR, for fashion compatibility prediction. In TGRLLR, to learn more low-dimensional and effective representations, we considered the latent low-rank representation by decomposing the original feature matrix in both the column and row directions to tackle the problem of insufficient observations. On this basis, we simultaneously exploited different regularization strategies to encode the structured correlations among features, the high-order relationships among items, and the geometrical structures of outfits for more informative representations. Extensive experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed method compared with state-of-the-art methods.
C1 [Jing, Peiguang; Zhang, Jing; Ye, Shu; Liu, Jing; Su, Yuting] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250000, Peoples R China.
C3 Tianjin University; Shandong University
RP Su, YT (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM pgjing@tju.edu.cn; jjzhang_2019@tju.edu.cn; nieliqiang@gmail.com;
   yeshu330@outlook.com; jliu_tju@tju.edu.cn; ytsu@tju.edu.cn
OI Jing, Peiguang/0000-0003-2648-7358
FU NationalNatural Science Foundation of China [61802277]; Tianjin
   Municipal Natural Science Foundation [20JCQNJC01210]; China Postdoctoral
   Science Foundation [2019M651038]
FX This work was supported in part by the NationalNatural Science
   Foundation of China under Grant 61802277, in part by Tianjin Municipal
   Natural Science Foundation under Grant 20JCQNJC01210, and in part by
   China Postdoctoral Science Foundation Funded Project under Grant
   2019M651038. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhu Liu.
CR Ak KE, 2018, IEEE WINT CONF APPL, P1671, DOI 10.1109/WACV.2018.00186
   [Anonymous], 2000, Em: Proceedings of the 2000 ACM workshops on Multimedia, DOI DOI 10.1145/357744.357758
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Cai X, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1124
   Chen L, 2018, AAAI CONF ARTIF INTE, P2103
   Cucurull G, 2019, PROC CVPR IEEE, P12609, DOI 10.1109/CVPR.2019.01290
   Drucker H, 1997, ADV NEUR IN, V9, P155
   Fang XZ, 2018, IEEE T NEUR NET LEAR, V29, P5228, DOI 10.1109/TNNLS.2018.2796133
   Fang XZ, 2018, IEEE T NEUR NET LEAR, V29, P1006, DOI 10.1109/TNNLS.2017.2648880
   Ge YY, 2019, PROC CVPR IEEE, P5332, DOI 10.1109/CVPR.2019.00548
   Gu XL, 2019, IEEE T MULTIMEDIA, V21, P1524, DOI 10.1109/TMM.2018.2876822
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   Hou M, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4681
   Hsiao WL, 2018, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR.2018.00748
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Huang S, 2015, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2015.7298638
   Jo SY, 2019, INT CONF BIG DATA, P664, DOI 10.1109/bigcomp.2019.8679117
   Kang WC, 2017, IEEE DATA MINING, P207, DOI 10.1109/ICDM.2017.30
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kuang ZH, 2019, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2019.00316
   Lee H, 2019, IEEE WINT CONF APPL, P462, DOI 10.1109/WACV.2019.00055
   Li S, 2016, IEEE T NEUR NET LEAR, V27, P2160, DOI 10.1109/TNNLS.2015.2464090
   Lin Z., 2009, Technical Report (No. UILU-ENG-09-2215
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Mall U, 2019, IEEE I CONF COMP VIS, P411, DOI 10.1109/ICCV.2019.00050
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Nie LQ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P591, DOI 10.1145/2733373.2806217
   Nie LQ, 2015, IEEE T KNOWL DATA EN, V27, P396, DOI 10.1109/TKDE.2014.2330813
   Ren ZW, 2020, IEEE T IMAGE PROCESS, V29, P2094, DOI 10.1109/TIP.2019.2938859
   Saha A, 2018, IEEE WINT CONF APPL, P557, DOI 10.1109/WACV.2018.00067
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song XM, 2018, ACM/SIGIR PROCEEDINGS 2018, P5, DOI 10.1145/3209978.3209996
   Song XM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P753, DOI 10.1145/3123266.3123314
   Stockman G, 2003, COMPUTER VISION
   STRICKER M, 1995, P SOC PHOTO-OPT INS, V2410, P381, DOI 10.1117/12.205308
   Valle D., 2018, IEEE INT JOINT C NEU, P1, DOI DOI 10.1109/EPIM.2018.8756397
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Verma S, 2018, IEEE IMAGE PROC, P500, DOI 10.1109/ICIP.2018.8451164
   Wen J, 2018, NEURAL NETWORKS, V108, P83, DOI 10.1016/j.neunet.2018.08.007
   Wen J, 2018, NEURAL NETWORKS, V102, P36, DOI 10.1016/j.neunet.2018.02.002
   Xie LF, 2018, IEEE T IMAGE PROCESS, V27, P5261, DOI 10.1109/TIP.2018.2855426
   Yang X, 2019, AAAI CONF ARTIF INTE, P403
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yin M, 2016, IEEE T PATTERN ANAL, V38, P504, DOI 10.1109/TPAMI.2015.2462360
   Yin RP, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3434, DOI 10.1145/3308558.3313739
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
   Zhang Y, 2010, PROCEEDINGS OF THE ASME 29TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2010, VOL 6, P733
   Zhang Z, 2017, IEEE T IMAGE PROCESS, V26, P1466, DOI 10.1109/TIP.2017.2651396
   Zou XX, 2019, IEEE COMPUT SOC CONF, P296, DOI 10.1109/CVPRW.2019.00039
NR 51
TC 9
Z9 9
U1 4
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1277
EP 1287
DI 10.1109/TMM.2021.3062736
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200003
DA 2024-07-18
ER

PT J
AU Li, M
   Miao, ZJ
   Zhang, XP
   Xu, WR
   Ma, C
   Xie, NW
AF Li, Min
   Miao, Zhenjiang
   Zhang, Xiao-Ping
   Xu, Wanru
   Ma, Cong
   Xie, Ningwei
TI Rhythm-Aware Sequence-to-Sequence Learning for Labanotation Generation
   With Gesture-Sensitive Graph Convolutional Encoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Motion segmentation; Data models; Skeleton; Hidden Markov models;
   Feature extraction; Decoding; Rhythm; Attention mechanism; graph
   convolutional network; labanotation generation; sequence-to-sequence
   model
AB Labanotation is a professional dance notation system widely used in dance education and choreography preservation. Automatically generatingLabanotation dance scores from motion capture data can save a huge amount of manual time and effort. Recently, the sequence-to-sequence (seq2seq) model is applied to the automatic Labanotation generation. This model is based on an encoder-decoder structure, which encodes the input motion sequence to a fixed-length vector and then decodes it to generate the target sequence. However, the encoding of spatial skeleton structure of motion data is not considered in the existing work. Besides, it is challenging to align between the input motion data and the output Laban symbol sequences due to the severe imbalance of sequence lengths. Therefore, in this paper, we present a new seq2seq model for more effective Labanotation generation. In the encoder, we propose a new gesture-sensitive graph convolutional network with learned adaptive joint weights and non-physical connections to learn both spatial and temporal patterns from motion data sequences. In the decoder, we exploit motion rhythm information and propose a novel rhythm-aware attention mechanism to learn a good alignment between motion sequences and Laban symbol sequences, so that we can focus on relevant parts of the input motion sequence without searching in the whole sequence when predicting a target Laban symbol. Extensive experiments on two real-world datasets show that the proposed method achieves a better performance compared with the state-of-the-art approaches on the task of automatic Labanotation generation.
C1 [Li, Min; Miao, Zhenjiang; Xu, Wanru; Ma, Cong; Xie, Ningwei] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Zhang, Xiao-Ping] Ryerson Univ, Dept Elect Comp & Biomed Engn, Toronto, ON M5B 2K3, Canada.
C3 Beijing Jiaotong University; Toronto Metropolitan University
RP Li, M; Miao, ZJ (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM 16112066@bjtu.edu.cn; zjmiao@bjtu.edu.cn; xzhang@ee.ryerson.ca;
   xuwanru@bjtu.edu.cn; 13112063@bjtu.edu.cn; 18120323@bjtu.edu.cn
RI Xie, Ningwei/AFR-0083-2022; Zhang, Xiao-Ping (Steven)/B-1436-2016
OI Zhang, Xiao-Ping (Steven)/0000-0001-5241-0069; Li,
   Min/0000-0002-5304-1477
FU National Natural Science Foundation of China (NSFC) [61672089, 61703436,
   61572064, 62006015]; Center of Ethnic and Folk Literature and Art
   Development (CEFLA) Audio-Video Restoration and Evaluation Key
   Laboratory of Ministry of Culture and Tourism of China; Natural Sciences
   and EngineeringResearch Council of Canada (NSERC) [RGPIN-2020-04661]
FX This work is supported in part by the National Natural Science
   Foundation of China (NSFC) under Grants 61672089, 61703436, 61572064,
   and 62006015; in part by the Center of Ethnic and Folk Literature and
   Art Development (CEFLA) Audio-Video Restoration and Evaluation Key
   Laboratory of Ministry of Culture and Tourism of China; and in part by
   theNatural Sciences and EngineeringResearch Council of Canada (NSERC),
   under Grant RGPIN-2020-04661.
CR Alpert, 2003, LABAN WRITER 4 4
   Chen H., 2005, IEEE 7th Workshop on Multimedia Signal Processing, P1
   Chiu CC, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4774, DOI 10.1109/ICASSP.2018.8462105
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Choensawat W, 2016, SPRINGER TRAC ADV RO
   Choensawat W, 2015, MULTIMED TOOLS APPL, V74, P10823, DOI 10.1007/s11042-014-2209-6
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Gao X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P601, DOI 10.1145/3343031.3351170
   Graves A, 2014, PR MACH LEARN RES, V32, P1764
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Guest A. H, 2014, Labanotation: the system of analyzing and recording movement, V4th
   Guo H, 2014, COMM COM INF SC, V483, P426
   Hachimura K, 2001, ROBOT AND HUMAN COMMUNICATION, PROCEEDINGS, P122, DOI 10.1109/ROMAN.2001.981889
   Hao SS, 2019, IEEE IMAGE PROC, P4265, DOI [10.1109/icip.2019.8803659, 10.1109/ICIP.2019.8803659]
   Hunt F., LED LINTEL WINDOWS M
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Kipf T.N., 2017, ICLR 2017
   Kojima K, 2002, IEEE ROMAN 2002, PROCEEDINGS, P59, DOI 10.1109/ROMAN.2002.1045598
   Li C., 2017, 2017 IEEE INT C MULT, P585, DOI DOI 10.1109/ICMEW.2017.8026287
   Li C, 2018, IEEE INT CONF SENS, P1, DOI 10.1109/TFUZZ.2018.2878200
   Li CL, 2018, AAAI CONF ARTIF INTE, P3482
   Li M., 2020, PROC IEEE INT C MULT, P1
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li M, 2020, INT CONF ACOUST SPEE, P4517, DOI [10.1109/ICASSP40776.2020.9054302, 10.1109/icassp40776.2020.9054302]
   Li M, 2019, IEEE ACCESS, V7, P161561, DOI 10.1109/ACCESS.2019.2951588
   Li M, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P793, DOI 10.1109/ACPR.2017.55
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Niepert M, 2016, PR MACH LEARN RES, V48
   OptiTrack, OPT MOT CAPT SYST AP
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Sutskever I, 2014, ADV NEUR IN, V27
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Thakkar K. C., 2018, PRAC BRIZ MACH VIS C, P270
   Valsesia D, 2021, IEEE T MULTIMEDIA, V23, P402, DOI 10.1109/TMM.2020.2976627
   Wang JJ, 2018, INT C PATT RECOG, P854, DOI 10.1109/ICPR.2018.8545306
   Wilke L, 2005, COMPUT ANIMAT VIRT W, V16, P201, DOI 10.1002/cav.90
   Wu JX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P827, DOI 10.1145/3343031.3350938
   Xie N., 2020, IEEE INT C MULTIMEDI, P1
   Xie N., 2019, LECT NOTES COMPUT, P554
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang H, 2019, IEEE INT CON MULTI, P412, DOI 10.1109/ICME.2019.00078
   Zhang XY, 2019, J PHYS CONF SER, V1229, DOI 10.1088/1742-6596/1229/1/012031
   Zhang XY, 2018, INT CONF SIGN PROCES, P510, DOI 10.1109/ICSP.2018.8652386
   Zhou ZM, 2016, INT CONF SIGN PROCES, P1031, DOI 10.1109/ICSP.2016.7877986
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
NR 52
TC 3
Z9 3
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1488
EP 1502
DI 10.1109/TMM.2021.3066115
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0F4IB
UT WOS:000777324000001
DA 2024-07-18
ER

PT J
AU Liu, HB
   Li, JG
   Li, DA
   See, J
   Lin, WY
AF Liu, Huabin
   Li, Jianguo
   Li, Dian
   See, John
   Lin, Weiyao
TI Learning Scale-Consistent Attention Part Network for Fine-Grained Image
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image recognition; Task analysis; Logic gates; Location awareness;
   Visualization; Training; Object detection; Fine-grained image
   recognition; scale-consistent; attention part
AB Discriminative region localization and feature learning are crucial for fine-grained visual recognition. Existing approaches solve this issue by attention mechanism or part based methods while neglecting consistency between attention and local parts, as well as the rich relation information among parts. This paper proposes a Scale-consistent Attention Part Network (SCAPNet) to address that issue, which seamlessly integrates three novel modules: grid gate attention unit (gGAU), scale-consistent attention part selection (SCAPS), and part relation modeling (PRM). The gGAU module represents the grid region at a certain fine-scale with middle layer CNN features and produces hard attention maps with the lightweight Gumbel-Max based gate. The SCAPS module utilizes attention to guide part selection across multi-scales and keep the selection scale-consistent. The PRM module utilizes the self-attention mechanism to build the relationship among parts based on their appearance and relative geo-positions. SCAPNet can be learned in an end-to-end way and demonstrates state-of-the-art accuracy on several publicly available fine-grained recognition datasets (CUB-200-2011, FGVC-Aircraft, Veg200, and Fru92).
C1 [Liu, Huabin; Lin, Weiyao] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Li, Jianguo] Ant Financial Serv Grp, Beijing 101100, Peoples R China.
   [Li, Dian] Tencent Technol Beijing Co Ltd, Beijing 100080, Peoples R China.
   [See, John] Heriot Watt Univ, Sch Math & Comp Sci, Putrajaya 62200, Malaysia.
C3 Shanghai Jiao Tong University; Tencent
RP Lin, WY (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
EM huabinliu@sjtu.edu.cn; lijg.zero@antgroup.com; goodli@tencent.com;
   j.see@hw.ac.uk; wylin@sjtu.edu.cn
RI lin, yuxi/HKF-6212-2023; See, John/C-8633-2013
OI See, John/0000-0003-3005-4109; Li, Dian/0000-0002-7183-0090; Liu,
   Huabin/0000-0001-9174-1696; Lin, Weiyao/0000-0001-8307-7107
FU National Key Research, and Development Program of China
   [2018AAA0100400]; National Natural Science Foundation of China
   [61971277]; Open Research Project of the State Key Laboratory of Media
   Convergence and Communication, Communication University of China
FX This work was supported in part by the National Key Research, and
   Development Program of China under Grant 2018AAA0100400, in part by the
   National Natural Science Foundation of China underGrant 61971277, and in
   part by the Open Research Project of the State Key Laboratory of Media
   Convergence and Communication, Communication University of China.
CR Bengio Y., 2013, ARXIV
   BUZZELLI M, 2018, IEEE I C CONS ELECT
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352
   Dubey A, 2018, LECT NOTES COMPUT SC, V11216, P71, DOI 10.1007/978-3-030-01258-8_5
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   He XT, 2019, INT J COMPUT VISION, V127, P1235, DOI 10.1007/s11263-019-01176-2
   He XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P627, DOI 10.1145/3123266.3123319
   He XT, 2017, AAAI CONF ARTIF INTE, P4075
   Hou SH, 2017, IEEE I CONF COMP VIS, P541, DOI 10.1109/ICCV.2017.66
   Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Huang C, 2016, IEEE T MULTIMEDIA, V18, P2372, DOI 10.1109/TMM.2016.2602060
   Jang E., 2016, ARXIV161101144
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Li ZC, 2017, IEEE INT CONF COMP V, P1199, DOI 10.1109/ICCVW.2017.145
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu Y, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278091
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Lu P, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1880, DOI 10.1145/3219819.3220036
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Rodríguez P, 2020, IEEE T MULTIMEDIA, V22, P502, DOI 10.1109/TMM.2019.2928494
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Shroff P, 2020, IEEE COMPUT SOC CONF, P3782, DOI 10.1109/CVPRW50498.2020.00442
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedaldi A., 2013, Technical report
   Veit A, 2018, LECT NOTES COMPUT SC, V11205, P3, DOI 10.1007/978-3-030-01246-5_1
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Xu H, 2019, PROC CVPR IEEE, P9290, DOI 10.1109/CVPR.2019.00952
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Zhang LB, 2019, IEEE I CONF COMP VIS, P8330, DOI 10.1109/ICCV.2019.00842
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
NR 53
TC 14
Z9 15
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2902
EP 2913
DI 10.1109/TMM.2021.3090274
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000017
DA 2024-07-18
ER

PT J
AU Liu, XN
   Li, H
   Zhu, C
AF Liu, Xiaoning
   Li, Hui
   Zhu, Ce
TI Joint Contrast Enhancement and Exposure Fusion for Real-World Image
   Dehazing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Atmospheric modeling; Histograms; Visualization;
   Transforms; Scattering; Image enhancement; Image dehazing;
   multi-exposure fusion; structural patch decomposition; nighttime scene;
   image enhancement
ID WEATHER
AB Due to the complexity of real environment and potential defects of current simulation datasets, either prior-based or deep learning-based single image dehazing methods may not work well in certain scenarios. In this work, we propose an efficient joint contrast enhancement and exposure fusion (CEEF) framework to formulate image dehazing task as a problem of enhancing local visibility and global contrast. In the contrast enhancement stage, several intermediate images are generated through two pre-processing steps. Specifically, gamma correction (GC) is used to adjust local visibility of an input hazy image. To address the issue of applying adaptive histogram equalization (AHE) to each color channel independently, we introduce color-preserving AHE (CP-AHE) to improve global contrast of the input hazy image. In the fusion stage, we develop a fast structural patch decomposition-based fusion strategy with an adaptive kernel size to fuse the inputs obtained by GC and CP-AHE. Extensive experiments on the real-world datasets demonstrate superiority of the proposed method to state-of-the-art methods in terms of visual and quantitative evaluation. Particularly for nighttime hazy scenes, our approach is shown to retain fine details and reduce color artifacts against three latest nighttime defogging methods. Moreover, we discuss potential applications of our CP-AHE in low-light enhancement and image editing.
C1 [Liu, Xiaoning; Zhu, Ce] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Li, Hui] Vivo Mobile Commun Co Ltd, Imaging Algorithm Res Dept, Shenzhen 518101, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Li, H (corresponding author), Vivo Mobile Commun Co Ltd, Imaging Algorithm Res Dept, Shenzhen 518101, Peoples R China.
EM liuxiaoning2016@sina.com; lihui@vivo.com; eczhu@uestc.edu.cn
RI Liu, Xiaoning/IVH-5642-2023
OI Liu, Xiaoning/0000-0001-8060-5832; Li, Hui/0000-0002-5401-3013
FU National Natural Science Foundation of China [U19A2052, 62020106011]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U19A2052 and 62020106011.
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   Ancuti C, 2020, IEEE T IMAGE PROCESS, V29, P6264, DOI 10.1109/TIP.2020.2988203
   Berman D, 2020, IEEE T PATTERN ANAL, V42, P720, DOI 10.1109/TPAMI.2018.2882478
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen C, 2016, LECT NOTES COMPUT SC, V9906, P576, DOI 10.1007/978-3-319-46475-6_36
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Deng ZJ, 2019, IEEE I CONF COMP VIS, P2453, DOI 10.1109/ICCV.2019.00254
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dudhane A, 2020, PROC CVPR IEEE, P4563, DOI 10.1109/CVPR42600.2020.00462
   Fang FM, 2020, IEEE T MULTIMEDIA, V22, P2537, DOI 10.1109/TMM.2019.2958755
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Galdran A, 2018, SIGNAL PROCESS, V149, P135, DOI 10.1016/j.sigpro.2018.03.008
   Galdran A, 2018, PROC CVPR IEEE, P8212, DOI 10.1109/CVPR.2018.00857
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo JM, 2017, IEEE T IMAGE PROCESS, V26, P4217, DOI 10.1109/TIP.2017.2706526
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu HM, 2020, IEEE T MULTIMEDIA, V22, P1485, DOI 10.1109/TMM.2019.2944260
   Ju MY, 2020, IEEE T IMAGE PROCESS, V29, P3104, DOI 10.1109/TIP.2019.2957852
   Kim SE, 2020, IEEE T IMAGE PROCESS, V29, P1985, DOI 10.1109/TIP.2019.2948279
   Kou F, 2018, J VIS COMMUN IMAGE R, V53, P235, DOI 10.1016/j.jvcir.2018.03.020
   Lebrun M, 2015, IEEE T IMAGE PROCESS, V24, P3149, DOI 10.1109/TIP.2015.2439041
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P5805, DOI 10.1109/TIP.2020.2987133
   Li H, 2018, COMPUT VIS IMAGE UND, V168, P37, DOI 10.1016/j.cviu.2017.11.001
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Li Y, 2017, COMPUT VIS IMAGE UND, V165, P1, DOI 10.1016/j.cviu.2017.09.003
   Li Y, 2015, IEEE I CONF COMP VIS, P226, DOI 10.1109/ICCV.2015.34
   Li ZG, 2018, IEEE T IMAGE PROCESS, V27, P442, DOI 10.1109/TIP.2017.2750418
   Li ZG, 2017, IEEE T IMAGE PROCESS, V26, P1243, DOI 10.1109/TIP.2017.2651366
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P5432, DOI 10.1109/TIP.2015.2482903
   Li ZW, 2015, PROC CVPR IEEE, P4988, DOI 10.1109/CVPR.2015.7299133
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu X, 2019, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR.2019.00717
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Pan JS, 2021, IEEE T PATTERN ANAL, V43, P2449, DOI 10.1109/TPAMI.2020.2969348
   Pan JS, 2018, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2018.00324
   Pang YW, 2020, PROC CVPR IEEE, P5930, DOI 10.1109/CVPR42600.2020.00597
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Salazar-Colores S, 2019, IEEE T IMAGE PROCESS, V28, P2357, DOI 10.1109/TIP.2018.2885490
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Sulami M, 2014, IEEE INT CONF COMPUT
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Wu QB, 2020, IEEE T IMAGE PROCESS, V29, P2583, DOI 10.1109/TIP.2019.2949392
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Yeh CH, 2020, IEEE T IMAGE PROCESS, V29, P3153, DOI 10.1109/TIP.2019.2957929
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2355, DOI 10.1145/3394171.3413763
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P72, DOI 10.1109/TIP.2019.2922837
   Zhang J, 2017, PROC CVPR IEEE, P7016, DOI 10.1109/CVPR.2017.742
   Zhang Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P582, DOI 10.1145/3240508.3240595
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhao SY, 2020, IEEE T IMAGE PROCESS, V29, P6947, DOI 10.1109/TIP.2020.2995264
   Zhu FY, 2016, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2016.52
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 72
TC 40
Z9 41
U1 8
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3934
EP 3946
DI 10.1109/TMM.2021.3110483
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400019
DA 2024-07-18
ER

PT J
AU Lu, YW
   Li, DS
   Wang, WJ
   Lai, ZH
   Zhou, J
   Li, XL
AF Lu, Yuwu
   Li, Desheng
   Wang, Wenjing
   Lai, Zhihui
   Zhou, Jie
   Li, Xuelong
TI Discriminative Invariant Alignment for Unsupervised Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Feature extraction; Manifolds; Degradation; Neural
   networks; Kernel; Data structures; Domain adaptation; subspace learning;
   maximum margin criterion
ID FRAMEWORK; RECOGNITION; KERNEL
AB As one of the most prevalent branches of transfer learning, domain adaptation is dedicated to generalizing the knowledge of a source domain to a target domain to perform machine learning tasks. In domain adaptation, the key strategy is to overcome the shift between different domains and learn shared features with domain invariance. However, most existing methods focus on extracting the common features of the source and target domains, and do not consider the shift problem of class center in the target domain caused by this process. Specifically, when we align the domain distributions, we often ignore the inherent feature attributes of the data, or under the guidance of false pseudo-labels, cause the target domain data to be far away from the class center after projection. This is not conducive to classification task. To address these problems, in this study, we propose a novel domain adaptation method, referred to as discriminative invariant alignment (DIA), for image representation. DIA enriches the knowledge matrix by combining the class discriminative information of the source domain and local data structure information of the target domain into a new framework. By introducing the maximum margin criterion of the source domain, the classification boundaries are expanded. To verify the performance of the proposed method, we compared DIA with several state-of-the-art methods on five benchmark databases. The experimental results show that DIA is superior to the state-of-the-art methods.
C1 [Lu, Yuwu; Li, Desheng; Wang, Wenjing; Lai, Zhihui; Zhou, Jie] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Lu, Yuwu; Li, Desheng; Wang, Wenjing; Lai, Zhihui; Zhou, Jie] Shenzhen Univ, Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [Lu, Yuwu; Li, Desheng; Wang, Wenjing; Lai, Zhihui; Zhou, Jie] Shenzhen Univ, Guangdong Lab Artificial Intelligence & Digital E, Shenzhen 518060, Peoples R China.
   [Li, Xuelong] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Peoples R China.
C3 Shenzhen University; Shenzhen University; Shenzhen University; Guangming
   Laboratory; Northwestern Polytechnical University
RP Lu, YW (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.; Lu, YW (corresponding author), Shenzhen Univ, Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
EM luyuwu2008@163.com; lidesheng2019@email.szu.edu.cn;
   wangwenjing2018@email.szu.edu.cn; lai_zhi_hui@163.com; jie_jpu@163.com;
   xuelong_li@nwpu.edu.cn
RI Li, Xue-long/AFU-6301-2022; Lai, Zhihui/R-1000-2019
OI Li, Xue-long/0000-0003-2037-2525; Lai, Zhihui/0000-0002-4388-3080; Zhou,
   Jie/0000-0001-5882-3649
FU National Natural Science Foundation of China [61871470, U1801262,
   61732011]; Guangdong Basic and Applied Basic Research Foundation
   [2019A1515011493, 2021A1515011861]; Major Project of the New Generation
   of Artificial Intelligence of China [2018AAA0102900]; Natural Science
   Foundation of Shenzhen University [2019046]; Science Foundation of
   Shenzhen [JCYJ20160422144110140]
FX This work was supported in part by the National Natural Science
   Foundation of China Under Grants 61871470, U1801262, and 61732011, in
   part by the Guangdong Basic and Applied Basic Research Foundation under
   Grants 2019A1515011493 and 2021A1515011861, in part by the Major Project
   of the New Generation of Artificial Intelligence of China under Grant
   2018AAA0102900, in part by the Natural Science Foundation of Shenzhen
   University underGrant 2019046, and in part by the Science Foundation of
   Shenzhen under Grant JCYJ20160422144110140.
CR [Anonymous], 2019, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2019.00309
   Baktashmotlagh M, 2016, J MACH LEARN RES, V17
   Banerjee S, 2007, ICMLA 2007: SIXTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P148
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Cao Y, 2018, AAAI CONF ARTIF INTE, P2795
   Chen YD, 2018, IEEE T MULTIMEDIA, V20, P3212, DOI 10.1109/TMM.2018.2834867
   Ding M, 2016, IEEE T IMAGE PROCESS, V25, P776, DOI 10.1109/TIP.2015.2507445
   Fan ZZ, 2011, IEEE T NEURAL NETWOR, V22, P1119, DOI 10.1109/TNN.2011.2152852
   Fang C, 2013, IEEE I CONF COMP VIS, P1657, DOI 10.1109/ICCV.2013.208
   Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368
   Fu Q, 2018, IEEE T MULTIMEDIA, V20, P2114, DOI 10.1109/TMM.2018.2791803
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gao JY, 2021, IEEE T CYBERNETICS, V51, P4822, DOI 10.1109/TCYB.2020.3034316
   Ghifary M, 2017, IEEE T PATTERN ANAL, V39, P1414, DOI 10.1109/TPAMI.2016.2599532
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Hu Q., 2021, ARXIV201108435V3
   Khalighi S, 2017, IEEE T CYBERNETICS, V47, P3280, DOI 10.1109/TCYB.2016.2616119
   Li HF, 2006, IEEE T NEURAL NETWOR, V17, P157, DOI 10.1109/TNN.2005.860852
   Li S, 2018, IEEE T IMAGE PROCESS, V27, P4260, DOI 10.1109/TIP.2018.2839528
   Liang J, 2019, IEEE T PATTERN ANAL, V41, P1027, DOI 10.1109/TPAMI.2018.2832198
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Lu YW, 2021, IEEE T MULTIMEDIA, V23, P2056, DOI 10.1109/TMM.2020.3007340
   Lu YW, 2018, IEEE T CIRC SYST VID, V28, P3345, DOI 10.1109/TCSVT.2017.2749980
   Ma XH, 2019, PROC CVPR IEEE, P8258, DOI 10.1109/CVPR.2019.00846
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Mezghani A, 2012, INT CONF FRONT HAND, P399, DOI 10.1109/ICFHR.2012.155
   Olivetti E, 2013, INT WORKSHOP PATTERN, P128, DOI 10.1109/PRNI.2013.41
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Qi G.-J., 2012, P SIAM INT C DATA MI, P528
   Qi GJ, 2022, IEEE T PATTERN ANAL, V44, P2168, DOI 10.1109/TPAMI.2020.3031898
   Quanz B, 2012, IEEE T KNOWL DATA EN, V24, P1789, DOI 10.1109/TKDE.2012.75
   Ren CX, 2020, IEEE T CYBERNETICS, V50, P821, DOI 10.1109/TCYB.2018.2874219
   Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130
   Sun B., 2015, BMVC
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Tsai YHH, 2016, PROC CVPR IEEE, P5081, DOI 10.1109/CVPR.2016.549
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Wang JD, 2017, IEEE DATA MINING, P1129, DOI 10.1109/ICDM.2017.150
   Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083
   Wang Q, 2019, IEEE T IMAGE PROCESS, V28, P4376, DOI 10.1109/TIP.2019.2910667
   Wang X., 2019, ARXIV191109265V1
   Wang X, 2018, IEEE T IMAGE PROCESS, V27, P121, DOI 10.1109/TIP.2017.2756825
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xu YH, 2017, IEEE T KNOWL DATA EN, V29, P1158, DOI 10.1109/TKDE.2017.2669193
   Yan K, 2018, IEEE T CYBERNETICS, V48, P288, DOI 10.1109/TCYB.2016.2633306
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhang L, 2016, IEEE T IMAGE PROCESS, V25, P1177, DOI 10.1109/TIP.2016.2516952
   Zhang LH, 2019, PROC CVPR IEEE, P2542, DOI 10.1109/CVPR.2019.00265
   Zhang Y., 2019, PR MACH LEARN RES, V97, P7404
NR 58
TC 25
Z9 25
U1 4
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1871
EP 1882
DI 10.1109/TMM.2021.3073258
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200008
DA 2024-07-18
ER

PT J
AU Ma, JY
   Peng, CL
   Tian, X
   Jiang, JJ
AF Ma, Jiayi
   Peng, Chengli
   Tian, Xin
   Jiang, Junjun
TI DBDnet: A Deep Boosting Strategy for Image Denoising
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Boosting; Noise reduction; Image denoising; Task analysis; Deep
   learning; Learning systems; Noise measurement; Image denoising; deep
   boosting; deep learning; Gaussian noise; real noise; image restoration
ID SPARSE; REMOVAL
AB In this paper, we propose a new deep network architecture named deep boosting denoising net (DBDnet) for image denoising. It is a residual learning network that can generate a noise map from a noisy observation. In detail, it first generates a coarse noise map via a simple structure, and then updates the noise map gradually via a boosting function. The motivation of our DBDnet stems from the observation that the noise map recovered by any algorithm cannot ideally equal the ground-truth noise map, which typically contains noise. We call this noise NoN, i.e., noise of noise map. Based on this observation, we formulate the denoising as a process of reducing NoN, and the role of DBDnet is to eliminate the NoN from the coarse noise map. In particular, we analyze the process of reducing NoN theoretically, and propose an NoN eliminating module to simulate it accordingly. We evaluate the proposed DBDnet on images polluted by different levels of additive white Gaussian noise and real noise. Experiment results demonstrate that our DBDnet can attain better denoising performance compared with state-of-the-art methods on several kinds of image denoising tasks. In particular, for the Gaussian denoising and real image denoising tasks, the average improvements of the PSNR values brought by our DBDnet are about 0.25 dB and 1.01 dB, respectively. In addition, we find and verify that the deep boosting insight can be easily introduced into the state-of-the-art image denoising network, and promotes its denoising performance. Our code is publicly available at https://github.com/jiayi-ma/DBDNet.
C1 [Ma, Jiayi; Peng, Chengli; Tian, Xin] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
   [Jiang, Junjun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
C3 Wuhan University; Harbin Institute of Technology
RP Tian, X (corresponding author), Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
EM jyma2010@gmail.com; pengcl@whu.edu.cn; xin.tian@whu.edu.cn;
   jiangjunjun@hit.edu.cn
RI Ma, Jiayi/Y-2470-2019; Jiang, Junjun/L-7087-2019
OI Ma, Jiayi/0000-0003-3264-3265; Jiang, Junjun/0000-0002-5694-505X
FU National Natural Science Foundation of China [61773295, 61971315];
   Natural Science Foundation of Hubei Province [2019CFA037]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61773295 and 61971315, and in part by
   the Natural Science Foundation of Hubei Province under Grant 2019CFA037.
CR Bailer C, 2015, IEEE I CONF COMP VIS, P4015, DOI 10.1109/ICCV.2015.457
   Basaeed E, 2016, KNOWL-BASED SYST, V99, P19, DOI 10.1016/j.knosys.2016.01.028
   Bregman L. M., 1967, USSR COMP MATH MATH, V7, P200, DOI [10.1016/0041-5553(67)90040-7, DOI 10.1016/0041-5553(67)90040-7]
   Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129
   Burger H., 2012, CVPR
   Buzzard GT, 2018, SIAM J IMAGING SCI, V11, P2001, DOI 10.1137/17M1122451
   Charest MR, 2006, 2006 40TH ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-4, P452, DOI 10.1109/CISS.2006.286510
   Chen C, 2020, IEEE T PATTERN ANAL, V42, P3071, DOI 10.1109/TPAMI.2019.2921548
   Chen C, 2018, LECT NOTES COMPUT SC, V11215, P3, DOI 10.1007/978-3-030-01252-6_1
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Ding GG, 2019, IEEE T IMAGE PROCESS, V28, P3752, DOI 10.1109/TIP.2019.2902115
   Dong WS, 2019, IEEE T PATTERN ANAL, V41, P2305, DOI 10.1109/TPAMI.2018.2873610
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Du Y, 2021, IEEE T MULTIMEDIA, V23, P2139, DOI 10.1109/TMM.2020.3008057
   El Helou M, 2020, IEEE T IMAGE PROCESS, V29, P4885, DOI 10.1109/TIP.2020.2976814
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Han S., 2016, ADV NEURAL INF PROCE, P109
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jain V., 2009, PROC ADV NEURAL INFO, P769
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Kumar K, 2017, 7TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION TECHNOLOGY (ICCCT - 2017), P100, DOI 10.1145/3154979.3154998
   Kumar K, 2017, 2017 INTERNATIONAL CONFERENCE ON INNOVATIONS IN ELECTRONICS, SIGNAL PROCESSING AND COMMUNICATION (IESC), P106, DOI 10.1109/IESPC.2017.8071874
   Li S. Z., 2009, Markov random field modeling in image analysis
   Liu D, 2020, IEEE T IMAGE PROCESS, V29, P3695, DOI 10.1109/TIP.2020.2964518
   Luan SZ, 2018, IEEE T IMAGE PROCESS, V27, P4357, DOI 10.1109/TIP.2018.2835143
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Malladi SRSP, 2021, IEEE T MULTIMEDIA, V23, P2297, DOI 10.1109/TMM.2020.3009502
   Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329
   Moghaddam B, 2007, IEEE I CONF COMP VIS, P2073, DOI 10.1109/cvpr.2007.383092
   Moghimi M., 2016, PROC BRIT MACH VIS C
   Nam S, 2016, PROC CVPR IEEE, P1683, DOI 10.1109/CVPR.2016.186
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   Plötz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Ren C, 2019, IEEE T MULTIMEDIA, V21, P731, DOI 10.1109/TMM.2018.2866362
   Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Singh M, 2019, IET COMPUT VIS, V13, P578, DOI 10.1049/iet-cvi.2018.5814
   Talebi H, 2013, IEEE T IMAGE PROCESS, V22, P1468, DOI 10.1109/TIP.2012.2231691
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tian CW, 2020, NEURAL NETWORKS, V121, P461, DOI 10.1016/j.neunet.2019.08.022
   Tukey J.W., 1977, EXPLORATORY DATA ANA
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Wang B, 2016, INT C PATT RECOG, P1400, DOI 10.1109/ICPR.2016.7899833
   Wu CH, 2017, IEEE WINT CONF APPL, P540, DOI 10.1109/WACV.2017.66
   Xie Y., 2020, Adv. Neural. Inf. Process. Syst, V33, P20320
   Yue ZS, 2019, ADV NEUR IN, V32
   Zamir SW, 2020, PROC CVPR IEEE, P2693, DOI 10.1109/CVPR42600.2020.00277
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhao R., 2020, PROC BRIT MACH VIS C
   Zhuo S, 2019, PROC IEEE INT C COMP
NR 57
TC 20
Z9 21
U1 5
U2 52
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3157
EP 3168
DI 10.1109/TMM.2021.3094058
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000037
DA 2024-07-18
ER

PT J
AU Wang, DM
   Zhou, GL
   Yan, Y
   Chen, HY
   Chen, QJ
AF Wang, Deming
   Zhou, Guangliang
   Yan, Yi
   Chen, Huiyi
   Chen, Qijun
TI GeoPose: Dense Reconstruction Guided 6D Object Pose Estimation With
   Geometric Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 6D object pose estimation; reconstruction guidance; dense 2D-3D
   correspondences; geometric consistency
ID DESCRIPTORS
AB 6D object pose estimation for texture-less objects from RGB images remains challenging, especially in occlusion scenarios. Instead of localizing sparse keypoints by regressing their image coordinates or heatmaps, which are sensitive to occlusion, we introduce GeoPose, a novel reconstruction guided pose estimation pipeline that predicts dense correspondences and leverages geometric consistency effectively. We first design a dense reconstruction network (ReconNet) to reconstruct pixel-wise object coordinates in normalization space. Dense 2D-3D correspondences are generated intuitively by our explicit parameterization for 3D object models, which dismisses keypoint selection efforts. These 2D-3D correspondences are then utilized to estimate 6D poses by the PnP algorithm with RANSAC iterations. Furthermore, a novel Cycle Loss is proposed to provide 3D prior supervision, which significantly correlates with the pose estimation task by guiding geometric consistency between reconstruction (pixel to 3D) and reprojection (3D to pixel). In addition, a training data augmentation method is proposed to handle the insufficiency of 6D datasets, the acquisition of which is error-prone and time-consuming. Extensive experiments demonstrate that, compared with existing RGB-based methods, our GeoPose can achieve state-of-the-art (SOTA) 6D pose estimation performance on the LINEMOD, Occlusion LINEMOD and T-LESS datasets.
C1 [Wang, Deming; Zhou, Guangliang; Yan, Yi; Chen, Qijun] Tongji Univ, Coll Elect & Informat Engn, 4800 Caon Highway, Shanghai 201804, Peoples R China.
   [Chen, Huiyi] Tisch Sch Arts Univ, New York, NY 10003 USA.
C3 Tongji University
RP Chen, QJ (corresponding author), Tongji Univ, Coll Elect & Informat Engn, 4800 Caon Highway, Shanghai 201804, Peoples R China.
EM wangdeming@tongji.edu.cn; tj_zgl@tongji.edu.cn; seanyan@tongji.edu.cn;
   hc2446@nyu.edu; qjchen@tongji.edu.cn
OI , Guangl/0000-0001-7845-7068; Wang, Deming/0000-0003-3486-4176
FU National Natural Science Foundation of China [61733013, 61673300,
   62073245]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61733013, 61673300,and 62073245. The
   associate editor coordinating the review of this manuscriptand approving
   it for publication was Dr. Alexandros (Alexis) Michael Tourapis
CR [Anonymous], CoRR abs/1511.07122
   [Anonymous], 2008, BMVC
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Brachmann E, 2016, PROC CVPR IEEE, P3364, DOI 10.1109/CVPR.2016.366
   Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Cao Z, 2016, IEEE INT CONF ROBOT, P2441, DOI 10.1109/ICRA.2016.7487396
   Chen X., 2021, IEEE C COMP VIS PATT, P3034
   Do TT, 2018, Arxiv, DOI [arXiv:1802.10367, 10.48550/ARXIV.1802.10367, DOI 10.48550/ARXIV.1802.10367]
   Doumanoglou A, 2016, PROC CVPR IEEE, P3583, DOI 10.1109/CVPR.2016.390
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gu CH, 2010, LECT NOTES COMPUT SC, V6315, P408
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinterstoisser S, 2012, IEEE T PATTERN ANAL, V34, P876, DOI 10.1109/TPAMI.2011.206
   Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326
   Hinterstoisser V., 2012, P COMP VIS ACCV 2012, P548
   Hodan T, 2018, LECT NOTES COMPUT SC, V11214, P19, DOI 10.1007/978-3-030-01249-6_2
   Hodan T, 2017, IEEE WINT CONF APPL, P880, DOI 10.1109/WACV.2017.103
   Hodan T, 2016, LECT NOTES COMPUT SC, V9915, P606, DOI 10.1007/978-3-319-49409-8_52
   Hodan T, 2015, IEEE INT C INT ROBOT, P4421, DOI 10.1109/IROS.2015.7354005
   Hongping Cai, 2013, Computer Vision Systems. 9th International Conference, ICVS 2013. Proceedings: LNCS 7963, P103, DOI 10.1007/978-3-642-39402-7_11
   Karimi-Rouzbahani H, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13756-8
   Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169
   Kehl W, 2016, LECT NOTES COMPUT SC, V9907, P205, DOI 10.1007/978-3-319-46487-9_13
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Liu YP, 2019, IEEE T MULTIMEDIA, V21, P2776, DOI 10.1109/TMM.2019.2913321
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Oberweger M, 2018, LECT NOTES COMPUT SC, V11219, P125, DOI 10.1007/978-3-030-01267-0_8
   Park K, 2019, IEEE I CONF COMP VIS, P7667, DOI 10.1109/ICCV.2019.00776
   Pavlakos G., 2017, P IEEE C COMPUTER VI, P2011, DOI DOI 10.1109/CVPR.2017.139
   Peng SD, 2022, IEEE T PATTERN ANAL, V44, P3212, DOI 10.1109/TPAMI.2020.3047388
   Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469
   Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413
   Rangesh A, 2020, IEEE T INTELL VEHICL, V5, P449, DOI 10.1109/TIV.2020.2966074
   Redmon J., 2017, P IEEE C COMP VIS PA, P7263
   Rios-Cabrera R, 2013, IEEE I CONF COMP VIS, P2048, DOI 10.1109/ICCV.2013.256
   Rothganger F, 2006, INT J COMPUT VISION, V66, P231, DOI 10.1007/s11263-005-3674-1
   Rublee E., 2011, INT C COMP VIS, P24, DOI DOI 10.1109/ICCV.2011.6126544
   Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43
   Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038
   Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346
   Wang D, 2019, IEEE T MULTIMEDIA, V21, P2071, DOI 10.1109/TMM.2019.2892004
   Wang H, 2019, PROC CVPR IEEE, P2637, DOI 10.1109/CVPR.2019.00275
   Wanqing Zhao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14122, DOI 10.1109/CVPR42600.2020.01414
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Zakharov S, 2019, IEEE I CONF COMP VIS, P1941, DOI 10.1109/ICCV.2019.00203
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhou GL, 2021, IEEE T MULTIMEDIA, V23, P1630, DOI 10.1109/TMM.2020.3001533
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
   Zhu ML, 2014, IEEE INT CONF ROBOT, P3936, DOI 10.1109/ICRA.2014.6907430
NR 52
TC 3
Z9 4
U1 6
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4394
EP 4408
DI 10.1109/TMM.2021.3117092
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 6O4GT
UT WOS:000890201600001
DA 2024-07-18
ER

PT J
AU Wang, GM
   Xu, X
   Shen, FM
   Lu, HM
   Ji, YL
   Shen, HT
AF Wang, Gongmian
   Xu, Xing
   Shen, Fumin
   Lu, Huimin
   Ji, Yanli
   Shen, Heng Tao
TI Cross-Modal Dynamic Networks for Video Moment Retrieval With Text Query
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Proposals; Location awareness; Task analysis;
   Visualization; Semantics; Pipelines; Cross-modal alignment; temporal
   localization; video analysis; video moment retrieval
ID LOCALIZATION; ATTENTION; LANGUAGE
AB Video moment retrieval with text query aims to retrieve the most relevant segment from the whole video based on the given text query. It is a challenging cross-modal alignment task due to the huge gap between visual and linguistic modalities and the noise generated by manual labeling of time segments. Most of the existing works only use language information in the cross-modal fusion stage, neglecting that language information plays an important role in the retrieval stage. Besides, these works roughly compress the visual information in the video clips to reduce the computation cost which loses subtle video information in the long video. In this paper, we propose a novel model termed Cross-modal Dynamic Networks (CDN) which dynamically generates convolution kernel by visual and language features. In the feature extraction stage, we also propose a frame selection module to capture the subtle video information in the video segment. By this approach, the CDN can reduce the impact of the visual noise without significantly increasing the computation cost and leads to a better video moment retrieval result. The experiments on two challenge datasets, i.e., Charades-STA and TACoS, show that our proposed CDN method outperforms a bundle of state-of-the-art methods with more accurately retrieved moment video clips. The implementation code and extensive instruction of our proposed CDN method are provided at https://github.com/CFM-MSG/Code_CDN.
C1 [Wang, Gongmian; Xu, Xing; Shen, Fumin; Ji, Yanli; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.
   [Wang, Gongmian; Xu, Xing; Shen, Fumin; Ji, Yanli; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Lu, Huimin] Kyushu Inst Technol, Dept Mech & Control Engn, Kitakyushu, Fukuoka 8048550, Japan.
   [Shen, Heng Tao] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Kyushu Institute of
   Technology; Peng Cheng Laboratory
RP Shen, FM (corresponding author), Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.; Shen, FM (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM whwanggongmian@gmail.com; xing.xu@uestc.edu.cn; fumin.shen@gmail.com;
   dr.huimin.lu@ieee.org; yanliji@uestc.edu.cn; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021; Shen, Fumin/R-2121-2016
OI Lu, Huimin/0000-0001-9794-3221
FU National Natural Science Foundation of China [62072080, 61976049];
   Sichuan Science and Technology Program, China [2019ZDZX0008,
   2019YFG0533, 2020YFS0057]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072080 and 61976049, and in part by
   the Sichuan Science and Technology Program, China under Grants
   2019ZDZX0008, 2019YFG0533, and 2020YFS0057. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Dan Zeng.
CR [Anonymous], 2019, P 2019 INT C MULT RE, DOI DOI 10.1145/3323873.3325019
   Ansari Aasif., 2015, International Journal of Computer Applications, V112, P13
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Chen YY, 2018, LECT NOTES COMPUT SC, V11217, P367, DOI 10.1007/978-3-030-01261-8_22
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Fan CY, 2019, PROC CVPR IEEE, P1999, DOI 10.1109/CVPR.2019.00210
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Ghosh S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1984
   He DL, 2019, AAAI CONF ARTIF INTE, P8393
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu W., 2019, Neural Inf. Process. Syst., P536
   Liu Y., 2019, P BRIT MACH VIS C, P279
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Rehg M., 2019, TRIPPING TIME EFFICI
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rodriguez Cristian, 2020, WACV, P2464
   Rohrbach M, 2012, LECT NOTES COMPUT SC, V7572, P144, DOI 10.1007/978-3-642-33718-5_11
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song XM, 2018, LECT NOTES COMPUT SC, V11165, P340, DOI 10.1007/978-3-030-00767-6_32
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang L., 2014, THUMOS14 ACTION RECO, V1
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Wu AM, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1029
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu X, 2020, IEEE T NEUR NET LEAR, V31, P5412, DOI 10.1109/TNNLS.2020.2967597
   Xu X, 2022, IEEE T PATTERN ANAL, V44, P3030, DOI 10.1109/TPAMI.2020.3045530
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, CVPR
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhu Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10665, DOI 10.1109/CVPR42600.2020.01068
NR 50
TC 14
Z9 14
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1221
EP 1232
DI 10.1109/TMM.2022.3142420
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800018
DA 2024-07-18
ER

PT J
AU Wang, X
   Lai, SQ
   Chai, ZH
   Zhang, XJ
   Qian, XM
AF Wang, Xuan
   Lai, Shenqi
   Chai, Zhenhua
   Zhang, Xingjun
   Qian, Xueming
TI SPGNet: Serial and Parallel Group Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; Computer architecture; Hardware; Task analysis;
   Computational modeling; Feature extraction; Complexity theory;
   Lightweight; NPU; image classification; object detection; person
   re-identification
AB Neural-network Processing Units (NPU), which specializes in the acceleration of deep neural networks (DNN), is of great significance to latency-sensitive areas like robotics or edge computing. However, there are few works focusing on the network design for NPU in recent studies. Most of the popular lightweight structures (e.g. MobileNet) are designed with depthwise convolution, which has less computation in theory but is not friendly to existing hardwares, and the speed tested on NPU is not always satisfactory. Even under similar FLOPs (the number of multiply-accumulates), vanilla convolution operation is always faster than depthwise one. In this paper, we will propose a novel architecture named Serial and Parallel Group Network (SPGNet), which can capture discriminative multi-scale information and at the same time keep the structure compact. Extensive evaluations have been conducted on different computer vision tasks, e.g. image classification (CIFAR and ImageNet), object detection (PASCAL VOC and MS COCO) and person re-identification (Market-1501 and DukeMTMC-ReID). The experimental results show that our proposed SPGNet can achieve comparable performance with the state-of-the-art networks while the speed is 120% faster than MobileNetV2 under similar FLOPS and over 300% faster than GhostNet with similar accuracy on NPU.
C1 [Wang, Xuan] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, SMILES LAB, Xian 710049, Peoples R China.
   [Lai, Shenqi; Chai, Zhenhua] Meituan, Beijing 100102, Peoples R China.
   [Zhang, Xingjun] Xi An Jiao Tong Univ, Dept Comp Sci & Engn, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Key Lab Intelligient Networks & Network Secur, SMILES LAB, Minist Educ, Xianning West Rd, Xian 710049, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong
   University
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, Key Lab Intelligient Networks & Network Secur, SMILES LAB, Minist Educ, Xianning West Rd, Xian 710049, Shaanxi, Peoples R China.
EM 2236135021@qq.com; laishenqi@stu.xjtu.edu.cn; chaizhenhua@meituan.com;
   xjzhang@mail.xjtu.edu.cn; qianxm@mail.xjtu.edu.cn
RI Chai, Zhenhua/HTM-6798-2023
OI Chai, Zhenhua/0000-0003-1979-1167; Wang, Xuan/0000-0002-5803-2086
FU NSFC [61772407, 61732008]
FX This work was supported in part by NSFC under Grants 61772407 and
   61732008 and in part by Meituan.
CR [Anonymous], P INT C MACH LEARN L
   Cai Han, 2019, INT C LEARN REPR
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Gholami A, 2018, IEEE COMPUT SOC CONF, P1719, DOI 10.1109/CVPRW.2018.00215
   Han CC, 2020, IEEE T CIRC SYST VID, V30, P3433, DOI 10.1109/TCSVT.2019.2957467
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Howard A. G., 2017, arXiv
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Iandola F. N., 2016, ARXIV160207360, DOI 10.1007/978-3-319-24553-9
   Li X, 2022, IEEE T CIRC SYST VID, V32, P1792, DOI 10.1109/TCSVT.2021.3082635
   Liu CB, 2020, IEEE T MULTIMEDIA, V22, P1785, DOI 10.1109/TMM.2019.2954747
   Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339, 10.1109/ICCV.2019.00339D\]
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Radosavovic Ilija, 2020, P IEEE CVF C COMP VI, P10428
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen YH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3397
   Shizhen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P647, DOI 10.1007/978-3-030-58539-6_39
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang J., 2018, PROC BRIT MACH VIS C
   Wang R. J., 2018, P 2018 ANN C NEUR IN, P1963
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wu BC, 2019, PROC CVPR IEEE, P10726, DOI 10.1109/CVPR.2019.01099
   Wu HB, 2020, IEEE T MULTIMEDIA, V22, P2293, DOI 10.1109/TMM.2019.2953814
   Xie GT, 2018, PROC CVPR IEEE, P8847, DOI 10.1109/CVPR.2018.00922
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang FX, 2020, IEEE T MULTIMEDIA, V22, P2444, DOI 10.1109/TMM.2019.2957928
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Zhai HJ, 2021, IEEE T CIRC SYST VID, V31, P742, DOI 10.1109/TCSVT.2020.2991171
   Zhang L, 2021, IEEE T CIRC SYST VID, V31, P1490, DOI 10.1109/TCSVT.2020.3002956
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang T, 2017, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2017.469
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou D., 2020, P COMP VIS ECCV 2020, P680, DOI DOI 10.1007/978-3-030-58580-8_40
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou K, 2016, DESTECH TRANS COMP
   Zhuang BH, 2019, PROC CVPR IEEE, P413, DOI 10.1109/CVPR.2019.00050
NR 56
TC 11
Z9 11
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2804
EP 2814
DI 10.1109/TMM.2021.3088639
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000009
DA 2024-07-18
ER

PT J
AU Wang, XJ
   Shao, F
   Jiang, QP
   Fu, ZQ
   Meng, XC
   Gu, K
   Ho, YS
AF Wang, Xuejin
   Shao, Feng
   Jiang, Qiuping
   Fu, Zhenqi
   Meng, Xiangchao
   Gu, Ke
   Ho, Yo-Sung
TI Combining Retargeting Quality and Depth Perception Measures for Quality
   Evaluation of Retargeted Stereopairs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Visualization; Three-dimensional displays; Stereo image
   processing; Measurement; Distortion measurement; Feature extraction;
   Quality assessment; Stereoscopic image retargeting; Superpixel-based
   method; Depth perception
ID IMAGES
AB Stereoscopic Image Retargeting (SIR) aims to adapt stereoscopic images and videos to 3D display devices with various aspect ratios by emphasizing the important content while retaining surrounding context with minimal visual distortion. To address the issue of SIR evaluation, this paper presents a new objective quality assessment method for retargeted stereopairs by combining image quality and depth perception measures. Specifically, the image quality measure is conducted between the source and retargeted intermediate views generated by the view synthesis method to characterize the geometric distortion and content loss of the retargeted stereopair, while several depth-aware features are extracted to measure the visual comfort/discomfort and depth sensation when human views a 3D scene. Then, the extracted features are integrated into an overall perceptual quality prediction. Experiment results on NBU SIRQA and SIRD databases verify the superiority of our method.
C1 [Wang, Xuejin; Shao, Feng; Jiang, Qiuping; Meng, Xiangchao] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Fu, Zhenqi] Xiamen Univ, Sch Informat Sci & Engn, Xiamen 361005, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Engn Res Ctr Intelligent Percept & Autonomous Con, Minist Educ,Beijing Artificial Intelligence Inst, Beijing Lab Smart Environm Protect,Beijing Key La, Beijing 100124, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol GIST, Sch Informat & Commun, Gwangju 500712, South Korea.
C3 Ningbo University; Xiamen University; Beijing University of Technology;
   Beijing University of Technology; Gwangju Institute of Science &
   Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM 1020468620@qq.com; shaofeng@nbu.edu.cn; jiangqiuping@nbu.edu.cn;
   920012597@qq.com; mengxiangchao@nbu.edu.cn; guke.doctor@gmail.com;
   hoyo@gist.ac.kr
RI Jiang, Qiuping/AAL-8273-2020
OI Qiuping, Jiang/0000-0002-6025-9343; Fu, Zhenqi/0000-0003-2950-7190; HO,
   YO-SUNG/0000-0002-7220-1034
FU Natural Science Foundation of China [R18F010008]; K. C. Wong Magna Fund
   in Ningbo University
FX This work was supported by the Natural Science Foundation of China under
   Grants 62071261, 61901236, 41801252, 62076013, and 62021003, and Natural
   Science Foundation of China under Grant R18F010008. It was also
   sponsored by K. C. Wong Magna Fund in Ningbo University. The associate
   editor coordinating the reviewof this manuscript and approving it for
   publication was Dr. Hantao Liu.(
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2012, P SPIE
   [Anonymous], 2010, P INT WORKSH VID PRO
   [Anonymous], 2003, FINAL REPORT VIDEO Q
   Avidan S., 2007, ACM SIGGRAPH 2007 PA, P10, DOI [10.1145/1275808.1276390, DOI 10.1145/1275808.1276390]
   Baker L, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P782, DOI [10.1109/VR46266.2020.1581313146787, 10.1109/VR46266.2020.000-2]
   Basgoze Z., 2020, J VISUAL-JAPAN, V20, P1
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen YX, 2015, NEUROCOMPUTING, V151, P645, DOI 10.1016/j.neucom.2014.05.089
   Dekel T, 2013, IEEE T PATTERN ANAL, V35, P2513, DOI 10.1109/TPAMI.2013.46
   Fang YM, 2014, IEEE J EM SEL TOP C, V4, P95, DOI 10.1109/JETCAS.2014.2298919
   Fu ZQ, 2021, IEEE T MULTIMEDIA, V23, P2100, DOI 10.1109/TMM.2020.3008054
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hsu CC, 2014, IEEE J-STSP, V8, P377, DOI 10.1109/JSTSP.2014.2311884
   Jiang QP, 2019, IEEE T CIRC SYST VID, V29, P323, DOI 10.1109/TCSVT.2017.2783938
   Jiang QP, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.4.043002
   Karimi M, 2017, J VIS COMMUN IMAGE R, V43, P108, DOI 10.1016/j.jvcir.2016.12.011
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   Kim D, 2011, IEEE T CIRC SYST VID, V21, P231, DOI 10.1109/TCSVT.2011.2106275
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lee K, 2015, IEEE J-STSP, V9, P533, DOI 10.1109/JSTSP.2015.2393296
   Li B, 2015, IEEE T IMAGE PROCESS, V24, P2811, DOI 10.1109/TIP.2015.2431441
   Li LD, 2021, IEEE T MULTIMEDIA, V23, P2757, DOI 10.1109/TMM.2020.3016124
   Liang Y, 2017, IEEE T VIS COMPUT GR, V23, P1099, DOI 10.1109/TVCG.2016.2517641
   Lin JX, 2015, INT WORK QUAL MULTIM
   Lin SS, 2014, IEEE T CIRC SYST VID, V24, P759, DOI 10.1109/TCSVT.2013.2291282
   Lin SS, 2013, IEEE T VIS COMPUT GR, V19, P1677, DOI 10.1109/TVCG.2013.75
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu Y., 2016, P IEEE INT C MULT EX, P1
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Maalouf A, 2011, INT CONF ACOUST SPEE, P1161
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Niu YZ, 2021, IEEE T CIRC SYST VID, V31, P972, DOI 10.1109/TCSVT.2020.2998087
   Oliveira SAF, 2018, COMPUT VIS IMAGE UND, V168, P172, DOI 10.1016/j.cviu.2017.11.011
   Park J, 2015, IEEE T IMAGE PROCESS, V24, P1101, DOI 10.1109/TIP.2014.2383327
   Park J, 2014, IEEE J-STSP, V8, P415, DOI 10.1109/JSTSP.2014.2311885
   Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Shao F, 2021, IEEE T SYST MAN CY-S, V51, P3053, DOI 10.1109/TSMC.2019.2917496
   Shao F, 2017, IEEE T IMAGE PROCESS, V26, P4790, DOI 10.1109/TIP.2017.2721546
   Shao F, 2016, J DISP TECHNOL, V12, P22, DOI 10.1109/JDT.2015.2446973
   Shao F, 2015, IEEE T IMAGE PROCESS, V24, P2971, DOI 10.1109/TIP.2015.2436332
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Simakov D., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587842
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Wang Q, 2016, NEUROCOMPUTING, V173, P1798, DOI 10.1016/j.neucom.2015.09.057
   Wang WG, 2017, IEEE T VIS COMPUT GR, V23, P2014, DOI 10.1109/TVCG.2016.2600594
   Ya M.L., 2014, 2014 S DESIGN TEST I, P1
   Yue GH, 2019, IEEE T IND ELECTRON, V66, P3784, DOI 10.1109/TIE.2018.2851984
   Zhang YB, 2016, IEEE T IMAGE PROCESS, V25, P4286, DOI 10.1109/TIP.2016.2585884
   Zhou Y, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351198
   Zhou Y, 2019, IEEE T IMAGE PROCESS, V28, P4566, DOI 10.1109/TIP.2019.2912463
   Zhou Y, 2017, IEICE T INF SYST, VE100D, P1929, DOI 10.1587/transinf.2016EDL8255
   Zhu L., 2017, CHINA MULTIMEDIA
NR 56
TC 6
Z9 7
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2422
EP 2434
DI 10.1109/TMM.2021.3081259
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600015
DA 2024-07-18
ER

PT J
AU Zhang, KH
   Luo, WH
   Ma, L
   Ren, WQ
   Li, HD
AF Zhang, Kaihao
   Luo, Wenhan
   Ma, Lin
   Ren, Wenqi
   Li, Hongdong
TI Disentangled Feature Networks for Facial Portrait and Caricature
   Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Faces; Generative adversarial networks; Streaming media; Image
   reconstruction; Feature extraction; Decoding; Videos; Adversarial
   portrait mapping modules; facial caricature; facial portraits;
   four-stream disentangled feature networks
ID IMAGE
AB Facial portrait is an artistic form which draws faces by emphasizing discriminative or prominent parts of faces via various kinds of drawing tools. However, the complex interplay between the different facial factors, such as facial parts, background, and drawing styles, and the significant domain gap between natural facial images and their portrait counterparts makes the task challenging. In this paper, a flexible four-stream Disentangled Feature Networks (DFN) is proposed to learn disentangled feature representation of different facial factors and generate plausible portraits with reasonable exaggerations and richness in style. Four factors are encoded as embedding features, and combined to reconstruct facial portraits. Meanwhile, to make the process fully automatic (without manually specifying either portrait style or exaggerating form), we propose a new Adversarial Portrait Mapping Module (APMM) to map noise to the embedding feature space, as proxies for portrait style and exaggerating. Thanks to the proposed DFN and APMM, we are able to manipulate the portrait style and facial geometric structures to generate a large number of portraits. Extensive experiments on two public datasets show that our proposed methods can generate a diverse set of artistic portraits.
C1 [Zhang, Kaihao; Li, Hongdong] Australian Natl Univ, Coll Engn & Comp Sci, Canberra, ACT 2601, Australia.
   [Luo, Wenhan] Tencent, Shenzhen 518057, Peoples R China.
   [Ma, Lin] Meituan, Beijing 100000, Peoples R China.
   [Ren, Wenqi] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China.
C3 Australian National University; Tencent; Chinese Academy of Sciences;
   Institute of Information Engineering, CAS
RP Luo, WH (corresponding author), Tencent, Shenzhen 518057, Peoples R China.
EM super.khzhang@gmail.com; whluo.china@gmail.com; forest.linma@gmail.com;
   rwq.renwenqi@gmail.com; hongdong.li@anu.edu.au
RI Zhang, Kaihao/HGC-0368-2022; Luo, Wenhan/GZL-0535-2022; Ren,
   Wenqi/L-8724-2019
OI li, hongdong/0000-0003-4125-1554; Luo, Wenhan/0000-0002-5697-4168
FU ARC Centre of Excellence for Robotics Vision [CE140100016];
   ARC-Discovery [DP 190102261]; ARC-LIEF [190100080]; Baidu on autonomous
   driving
FX This work was supported in part by the ARC Centre of Excellence for
   Robotics Vision (CE140100016), ARC-Discovery (DP 190102261) and ARC-LIEF
   (190100080) grants, aswell as a research grant from Baidu on autonomous
   driving. The Associate Editor coordinating the review of this manuscript
   and approving it for publication was Prof. Jianguo Zhang
CR Akleman E ., 1997, ACM SIGGRAPH
   Akleman E., 2000, P VIS, P1
   [Anonymous], ADV NEUR IN
   [Anonymous], INT C MACH LEARN
   Brennan S. E ., 2007, CARICATURE GENERATOR
   Cao K., 2018, SIGGRAPH ASIA
   Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126
   Chen H ., 2002, P ACM INT C MULT, P1
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Ganin Y, 2016, LECT NOTES COMPUT SC, V9906, P311, DOI 10.1007/978-3-319-46475-6_20
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gonçalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021
   Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648
   Huang HZ, 2017, PROC CVPR IEEE, P7044, DOI 10.1109/CVPR.2017.745
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huo J., 2018, PROC BRIT MACHINE VI, P1
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jaderberg M, 2015, ADV NEUR IN, V28
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim T, 2017, PR MACH LEARN RES, V70
   Lewiner T ., 2011, COMPUT GRAPH-UK
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li W., 2018, ARXIV181100445
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Liang L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P386, DOI 10.1109/PCCGA.2002.1167882
   Lin CH, 2018, PROC CVPR IEEE, P9455, DOI 10.1109/CVPR.2018.00985
   Liu J., 2006, ACM MULTIMEDIA, P683
   Liu MY, 2017, ADV NEUR IN, V30
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma L., ADV NEURAL INF PROCE
   Mo Z., 2004, ACM SIGGRAPH 2004 SK, P57
   Radford A., 2015, ARXIV151106434
   Romero A., 2015, ICLR, P1, DOI DOI 10.48550/ARXIV.1412.6550
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3
   Shi YC, 2019, PROC CVPR IEEE, P10754, DOI 10.1109/CVPR.2019.01102
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Taigman Y., 2017, INT C LEARN REPR ICL, P1
   Tseng CC, 2007, LECT NOTES COMPUT SC, V4843, P314
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Vondrick C., 2016, ADV NEURAL INFORM PR, P613, DOI DOI 10.13016/M26GIH-TNYZ
   Vondrick C, 2017, PROC CVPR IEEE, P2992, DOI 10.1109/CVPR.2017.319
   Wu R ., ARXIV190701424, P2019
   Xiong W, 2018, PROC CVPR IEEE, P2364, DOI 10.1109/CVPR.2018.00251
   Yang F, 2012, PROC CVPR IEEE, P861, DOI 10.1109/CVPR.2012.6247759
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang KH, 2019, IEEE T IMAGE PROCESS, V28, P291, DOI 10.1109/TIP.2018.2867733
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 58
TC 3
Z9 4
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1378
EP 1388
DI 10.1109/TMM.2021.3064273
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200011
DA 2024-07-18
ER

PT J
AU Zhang, L
   Shen, JL
   Zhang, J
   Xu, JS
   Li, ZB
   Yao, YZ
   Yu, LT
AF Zhang, Lu
   Shen, Jialie
   Zhang, Jian
   Xu, Jingsong
   Li, Zhibin
   Yao, Yazhou
   Yu, Litao
TI Multimodal Marketing Intent Analysis for Effective Targeted Advertising
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Media; Advertising; Feature extraction; Social networking (online); Task
   analysis; Springs; Visualization; Multimodal; marketing intent analysis;
   targeted advertising
AB People's daily information sharing and acquisition through the Internet has become more and more popular. The comprehensive multimodal marketing advertorial generated by 'We Media' accounts besides the normal social news is gaining its importance on social media platforms. In order to achieve effective advertising, the marketing intent understanding is a key step towards generating targeted advertising strategies (push advertorials to specific people at a specific time). However, advertorials in real are usually designed to pretend as normal social news with a wide range of contents. This poses big challenges to the platforms on accurately recognizing and analyzing the marketing intents behind the advertorials. As a pioneering study, we address this new problem of multimodal-based marketing intent analysis and answer three core questions: (1) does a piece of social news contain marketing intent? (2) what is the topic of marketing intent? (3) what is the extent of marketing intent? Towards this end, we propose a novel Multimodal-based Marketing Intent Analysis scheme (MMIA) to estimate the marketing intent embedded in the multimodal contents. Specifically, a novel supervised neural autoregressive model (SmiDocNADE) is proposed to enhance the discriminative capacity of the learned hidden features so that a single system is capable of solving the three questions. In order to effectively model inter-correlations between images and text in advertorials, we fuse multimodal data and extract features by Graph Convolution Networks as an enhancement to SmiDocNADE. The extensive evaluations demonstrate the advantages of our proposed system in multimodal-based marketing intent analysis from multiple aspects.
C1 [Zhang, Lu; Zhang, Jian; Xu, Jingsong; Li, Zhibin; Yu, Litao] Univ Technol Sydney, Fac Engn & Informat Technol, Ultimo, NSW 2007, Australia.
   [Shen, Jialie] Queens Univ Belfast, Sch Elect Elect Engn & Comp Sci, Belfast BT7 1NN, Antrim, North Ireland.
   [Yao, Yazhou] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210014, Jiangsu, Peoples R China.
C3 University of Technology Sydney; Queens University Belfast; Nanjing
   University of Science & Technology
RP Zhang, J (corresponding author), Univ Technol Sydney, Fac Engn & Informat Technol, Ultimo, NSW 2007, Australia.
EM lu.zhang-5@student.uts.edu.au; jialie@gmail.com; Jian.Zhang@uts.edu.au;
   jingsong.xu@uts.edu.au; Zhibin.Li@student.uts.edu.au;
   yazhou.yao@njust.edu.cn; litao.yu@uts.edu.au
RI Li, Zhibin/KCL-0684-2024
OI Li, Zhibin/0000-0002-4226-267X; Zhang, Lu/0000-0002-2225-9772; Zhang,
   Jian/0000-0002-7240-3541; Yao, Yazhou/0000-0002-0337-9410
CR [Anonymous], 2018, SOHU second algorithm competition
   [Anonymous], 2020, JIEBA CHINESE WORD S
   [Anonymous], 2019, the global state of digital in 2019 report
   Ashkan A, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P800, DOI 10.1145/1571941.1572135
   Chan HP, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1191, DOI 10.1145/3397271.3401039
   Chang HH, 2018, ONLINE INFORM REV, V42, P697, DOI 10.1108/OIR-08-2016-0214
   Chen C, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P1265, DOI 10.1145/3331184.3331370
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Chen W, 2015, IEEE I CONF COMP VIS, P3298, DOI 10.1109/ICCV.2015.377
   Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPR.2009.5206800, 10.1109/CVPRW.2009.5206800]
   Dai H.K., 2006, Proceedings of the 15th international conference on World Wide Web, WWW '06, P829
   Das A, 2013, INT CONF INTELL SYST, P74, DOI 10.1109/ISDA.2013.6920711
   Ding XY, 2019, IEEE T MULTIMEDIA, V21, P124, DOI 10.1109/TMM.2018.2851389
   Gao Y, 2021, IEEE T MULTIMEDIA, V23, P784, DOI 10.1109/TMM.2020.2990085
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Guo Q., 2010, Proceedings of the 19th international conference on World wide web, WWW '10, P1107
   Gupta P, 2019, AAAI CONF ARTIF INTE, P6505
   Hamilton WL, 2017, ADV NEUR IN, V30
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks B., 2017, How to write an advertorial: Layout guidelines
   Hollerit B, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P629
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jegelka S, 2018, P INT C LEARN REPR
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Kim S, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P2878, DOI 10.1145/3366423.3380052
   Kipf TN, 2017, INT C LEARN REPR
   Kotler P., 2012, Marketing
   Larochelle H., 2012, ADV NEURAL INFORM PR, P2708
   Larochelle H., 2011, P 14 INT C ARTIFICIA, P29
   Lauly S, 2017, J MACH LEARN RES, V18
   Lee J, 2019, PR MACH LEARN RES, V97
   Lei XJ, 2016, IEEE T MULTIMEDIA, V18, P1910, DOI 10.1109/TMM.2016.2575738
   Liang X., IEEE ACCESS, V7, p94 869
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Liu TL, 2020, IEEE T MULTIMEDIA, V22, P1098, DOI 10.1109/TMM.2019.2936805
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu LH, 2020, IEEE T MULTIMEDIA, V22, P524, DOI 10.1109/TMM.2019.2930344
   Mazloom M., 2016, P ACM MULT, P197, DOI [10.1145/2964284.2967210, 10]
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Netzer O, 2012, MARKET SCI, V31, P521, DOI 10.1287/mksc.1120.0713
   Newman N., 2022, Reuters Institute Digital News Report 2022
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1962, DOI 10.1109/TMM.2020.3006371
   Porshnev A, 2013, INT CONF DAT MIN WOR, P440, DOI 10.1109/ICDMW.2013.111
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P1231, DOI 10.1109/TMM.2013.2261481
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Shishkin A, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P1145
   Shu K, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1051, DOI 10.1145/3397271.3401121
   Srivastava N., 2013, P NIPS, V3, P8
   Stelzner MichaelA., 2013, 2013 Social Media Marketing Industry Report: How Marketers are Using Social Media to Grow Their Businesses
   Valsesia D, 2021, IEEE T MULTIMEDIA, V23, P402, DOI 10.1109/TMM.2020.2976627
   Verbeek J., 2010, Proc. ACM Multimedia Information Retrieval, P537
   Wang J., 2013, Proceedings of the Conference on Empirical Methods in Natural Language, P1337
   Wang JC, 2020, AAAI CONF ARTIF INTE, V34, P9177
   Wang L., 2011, Proceedings of the International ACM Conference on Information and Knowledge Management, P599
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang WL, 2019, IND MARKET MANAG, V81, P160, DOI 10.1016/j.indmarman.2017.11.006
   Wang YF, 2019, FUTURE INTERNET, V11, DOI 10.3390/fi11070155
   Xu BJ, 2020, IEEE T MULTIMEDIA, V22, P1423, DOI 10.1109/TMM.2019.2943753
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Ying R, 2018, ADV NEUR IN, V31
   You QZ, 2015, IEEE T MULTIMEDIA, V17, P2271, DOI 10.1109/TMM.2015.2487863
   Zhang L., 2020, P INT IEEE C MULT EX, P1
   Zhang W, 2011, EXPERT SYST APPL, V38, P2758, DOI 10.1016/j.eswa.2010.08.066
   Zheng Y, 2016, IEEE T PATTERN ANAL, V38, P1056, DOI 10.1109/TPAMI.2015.2476802
   Zheng Y, 2014, PROC CVPR IEEE, P1370, DOI 10.1109/CVPR.2014.178
   Zhou J, 2018, ARTIF CELL NANOMED B, V46, pS1016, DOI 10.1080/21691401.2018.1442841
NR 66
TC 18
Z9 19
U1 4
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1830
EP 1843
DI 10.1109/TMM.2021.3073267
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200006
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhang, XY
   Zhang, FH
AF Zhang, Xingyuan
   Zhang, Fuhai
TI Differentiable Spatial Regression: A Novel Method for 3D Hand Pose
   Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Pose estimation; Decoding; Two dimensional
   displays; Neural networks; Cameras; Space heating; Convolutional neural
   networks; deep learning; hand pose estimation; image recognition
ID NETWORK
AB 3D Hand pose estimation from a single depth image is an essential topic in computer vision and human-computer interaction. Although the rising of deep learning boosts the accuracy a lot, the problem is still hard to solve due to the complex structure of the human hand. Two existing types of methods with deep learning, i.e. the regression-based and detection-based methods, either lose spatial information of the hand structure or lack direct supervision of the joint coordinates. In this paper, we propose a novel Differentiable Spatial Regression method which combines the advantages of these two types of methods to overcome each other's shortcomings. Our method uses spatial-form representation (SFR) to maintain spatial information and differentiable decoder to establish a direct supervision. Following the procedure suggested by our method, a particular model named SRNet is designed which uses a combination of 2D heatmaps and local offset maps as SFRs. Two modules named Plane Regression and Depth Regression are designed as differentiable decoder to regress plane coordinates and depth coordinates respectively. Ablation study demonstrates the superiority of our method over the two combined methods since the differentiable decoder leads to better SFRs learned by the network itself other than human design. Extensive experiments on four public datasets demonstrate that SRNet is comparable with the state-of-the-art models.
C1 [Zhang, Xingyuan; Zhang, Fuhai] Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150001, Peoples R China.
C3 Harbin Institute of Technology
RP Zhang, FH (corresponding author), Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150001, Peoples R China.
EM wizardicarus@gmail.com; zfhhit@hit.edu.cn
OI Zhang, Xingyuan/0000-0001-9965-4490
FU National Natural Science Foundation of China [62073097]; Natural Science
   Foundation of Heilongjiang Province of China [LC2017022]; Postdoctoral
   Scientific Research Developmental Fund of Heilongjiang Province of China
   [LBH-Q17071]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62073097, in part by the Natural Science
   Foundation of Heilongjiang Province of China under Grant LC2017022, and
   in part by the Postdoctoral Scientific Research Developmental Fund of
   Heilongjiang Province of China under Grant LBH-Q17071. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Sebastian Knorr. (Corresponding author: Fuhai
   Zhang.)
CR Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44
   Chen TY, 2018, PATTERN RECOGN, V80, P1, DOI 10.1016/j.patcog.2018.02.029
   Chen XH, 2020, NEUROCOMPUTING, V395, P138, DOI 10.1016/j.neucom.2018.06.097
   Chen XH, 2018, IEEE ACCESS, V6, P43425, DOI 10.1109/ACCESS.2018.2863540
   Chengde Wan, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P5147, DOI 10.1109/CVPR.2018.00540
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Ge LH, 2018, LECT NOTES COMPUT SC, V11217, P489, DOI 10.1007/978-3-030-01261-8_29
   Ge LH, 2018, PROC CVPR IEEE, P8417, DOI 10.1109/CVPR.2018.00878
   Ge LH, 2019, IEEE T PATTERN ANAL, V41, P956, DOI 10.1109/TPAMI.2018.2827052
   Ge LH, 2018, IEEE T IMAGE PROCESS, V27, P4422, DOI 10.1109/TIP.2018.2834824
   Ge LH, 2017, PROC CVPR IEEE, P5679, DOI 10.1109/CVPR.2017.602
   Guo HK, 2017, IEEE IMAGE PROC, P4512, DOI 10.1109/ICIP.2017.8297136
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Hu ZX, 2019, NEUROCOMPUTING, V333, P157, DOI 10.1016/j.neucom.2018.12.065
   Iqbal U, 2018, LECT NOTES COMPUT SC, V11215, P125, DOI 10.1007/978-3-030-01252-6_8
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Liu SW, 2020, IEEE ACCESS, V8, P53072, DOI 10.1109/ACCESS.2020.2979507
   Luo CW, 2019, IEEE T MULTIMEDIA, V21, P2473, DOI 10.1109/TMM.2019.2903724
   Moon G, 2018, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2018.00533
   Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343
   Nibali A., 2018, CORR
   Oberweger M, 2017, IEEE INT CONF COMP V, P585, DOI 10.1109/ICCVW.2017.75
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Sun X, 2015, PROC CVPR IEEE, P824, DOI 10.1109/CVPR.2015.7298683
   Supancic JS, 2018, INT J COMPUT VISION, V126, P1180, DOI 10.1007/s11263-018-1081-7
   Tang DH, 2017, IEEE T PATTERN ANAL, V39, P1374, DOI 10.1109/TPAMI.2016.2599170
   Tang DH, 2014, PROC CVPR IEEE, P3786, DOI 10.1109/CVPR.2014.490
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Ulyanov Dmitry, 2016, arXiv
   Wang GJ, 2018, J VIS COMMUN IMAGE R, V55, P404, DOI 10.1016/j.jvcir.2018.04.005
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu XK, 2018, LECT NOTES COMPUT SC, V11220, P246, DOI 10.1007/978-3-030-01270-0_15
   Wu Y, 2021, IEEE T SYST MAN CY-S, V51, P326, DOI [10.1109/TMC.2018.2812722, 10.1109/TSMC.2018.2871100]
   Yuan SX, 2018, PROC CVPR IEEE, P2636, DOI 10.1109/CVPR.2018.00279
   Zhou X., 2016, IJCAI, P2421
NR 38
TC 12
Z9 12
U1 2
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 166
EP 176
DI 10.1109/TMM.2020.3047552
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, JX
   Chen, L
   Zhou, YC
AF Chen, Junxin
   Chen, Lei
   Zhou, Yicong
TI Universal Chosen-Ciphertext Attack for a Family of Image Encryption
   Schemes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encryption; Nonlinear dynamical systems; Standards; Technological
   innovation; Matrices; Cryptanalysis; substitution and permutation;
   modular addition; chosen-ciphertext attack
ID ONLY MULTIMEDIA CIPHERS; QUANTITATIVE CRYPTANALYSIS; CHAOTIC SYSTEM;
   ALGORITHM; SECURITY; MAP
AB In recent decades, there has been considerable popularity in employing nonlinear dynamics and permutation-substitution structures for image encryption. Three procedures generally exist in such image encryption schemes: the key schedule module for producing encryption elements, permutation for image scrambling and substitution for pixel modification. This paper cryptanalyzes a family of image encryption schemes that adopt pixel-level permutation and modular addition-based substitution. The security analysis first reveals a common defect in the studied image encryption schemes. Specifically, the mapping from the differentials of the ciphertexts to those of the plaintexts is found to be linear and independent of the key schedules, permutation techniques and encryption rounds. On this theory basis, a universal chosen-ciphertext attack is further proposed. Experimental results demonstrate that the proposed attack can recover the plaintexts of the studied image encryption schemes without a security key or any encryption elements. Related cryptographic discussions are also given.
C1 [Chen, Junxin] Northeastern Univ, Coll Med & Biol Informat Engn, Shenyang 110004, Peoples R China.
   [Chen, Junxin] Minist Educ, Key Lab Intelligent Comp Med Image, Shenyang 110004, Peoples R China.
   [Chen, Junxin; Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Chen, Lei] Nsfocus Informat Technol Co Ltd, Beijing 100089, Peoples R China.
   [Chen, Lei] Tsinghua Univ, Res Inst Informat Technol RIIT, Beijing 100084, Peoples R China.
C3 Northeastern University - China; University of Macau; Tsinghua
   University
RP Zhou, YC (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
EM chenjx@bmie.neu.edu.cn; clei@bupt.edu.cn; yicongzhou@um.edu.mo
RI Zhou, Yicong/A-8017-2009; Chen, Junxin/ABC-1747-2020
OI Zhou, Yicong/0000-0002-4487-6384; Chen, Junxin/0000-0003-4745-8361
FU National Natural Science Foundation of China [61802055, 61771121];
   Fundamental Research Funds for the Central Universities [N2019001];
   China Postdoctoral Science Foundation [2019M660511]; Science and
   Technology Development Fund, Macau SAR [189/2017/A3]; University of
   Macau [MYRG2018-00136-FST]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61802055 and 61771121, in part by the Fundamental
   Research Funds for the Central Universities (N2019001), in part by China
   Postdoctoral Science Foundation (2019M660511), in part by the Science
   and Technology Development Fund, Macau SAR (File no. 189/2017/A3), and
   in part by the University of Macau (File no. MYRG2018-00136-FST). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Abdulmotaleb El Saddik.
CR Abd El-Latif AA, 2018, IEEE ACCESS, V6, P1073, DOI 10.1109/ACCESS.2017.2777869
   Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   [Anonymous], [No title captured]
   Chen GR, 2004, CHAOS SOLITON FRACT, V21, P749, DOI 10.1016/j.chaos.2003.12.022
   Chen L, 2017, NONLINEAR DYNAM, V87, P1797, DOI 10.1007/s11071-016-3153-y
   Chen L, 2015, COMPUT BIOL MED, V65, P69, DOI 10.1016/j.compbiomed.2015.07.024
   Chen WH, 2016, IEEE T NEUR NET LEAR, V27, P2696, DOI 10.1109/TNNLS.2015.2512849
   Diab H, 2018, IEEE ACCESS, V6, P42227, DOI 10.1109/ACCESS.2018.2858839
   Dzwonkowski M, 2015, IEEE T IMAGE PROCESS, V24, P4614, DOI 10.1109/TIP.2015.2467317
   Enayatifar R, 2017, OPT LASER ENG, V90, P146, DOI 10.1016/j.optlaseng.2016.10.006
   Fouda JSAE, 2014, COMMUN NONLINEAR SCI, V19, P578, DOI 10.1016/j.cnsns.2013.07.016
   Fridrich J, 1998, INT J BIFURCAT CHAOS, V8, P1259, DOI 10.1142/S021812749800098X
   He JH, 2018, IEEE T MULTIMEDIA, V20, P2645, DOI 10.1109/TMM.2018.2817065
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Hua ZY, 2019, IEEE ACCESS, V7, P8660, DOI 10.1109/ACCESS.2018.2890116
   Hua ZY, 2018, SIGNAL PROCESS, V149, P148, DOI 10.1016/j.sigpro.2018.03.010
   Hua ZY, 2018, SIGNAL PROCESS, V144, P134, DOI 10.1016/j.sigpro.2017.10.004
   Hua ZY, 2017, INFORM SCIENCES, V396, P97, DOI 10.1016/j.ins.2017.02.036
   Hua ZY, 2015, INFORM SCIENCES, V297, P80, DOI 10.1016/j.ins.2014.11.018
   Jolfaei A, 2016, IEEE T INF FOREN SEC, V11, P235, DOI 10.1109/TIFS.2015.2489178
   Kaur M, 2018, ELECTRON LETT, V54, P562, DOI 10.1049/el.2017.4426
   Kaur M, 2020, ARCH COMPUT METHOD E, V27, P15, DOI 10.1007/s11831-018-9298-8
   Lan RS, 2018, SIGNAL PROCESS, V147, P133, DOI 10.1016/j.sigpro.2018.01.026
   Li CQ, 2019, J INF SECUR APPL, V48, DOI 10.1016/j.jisa.2019.102361
   Li CQ, 2011, SIGNAL PROCESS, V91, P949, DOI 10.1016/j.sigpro.2010.09.014
   Li M, 2018, IEEE MULTIMEDIA, V25, P92, DOI 10.1109/MMUL.2018.112142439
   Li PY, 2018, IEEE T MULTIMEDIA, V20, P1960, DOI 10.1109/TMM.2017.2786860
   Li SJ, 2008, SIGNAL PROCESS-IMAGE, V23, P212, DOI 10.1016/j.image.2008.01.003
   Lin ZS, 2015, IEEE T CIRC SYST VID, V25, P1203, DOI 10.1109/TCSVT.2014.2369711
   Mannai O, 2015, NONLINEAR DYNAM, V82, P107, DOI 10.1007/s11071-015-2142-x
   Norouzi B, 2014, MULTIMED TOOLS APPL, V71, P1469, DOI 10.1007/s11042-012-1292-9
   Özkaynak F, 2018, NONLINEAR DYNAM, V92, P305, DOI 10.1007/s11071-018-4056-x
   Parvin Z., 2016, MULTIMEDIA TOOLS APP, V75, p10 631
   Praveenkumar P, 2015, SECUR COMMUN NETW, V8, P3335, DOI 10.1002/sec.1257
   Preishuber M, 2018, IEEE T INF FOREN SEC, V13, P2137, DOI 10.1109/TIFS.2018.2812080
   Solak E, 2010, INT J BIFURCAT CHAOS, V20, P1405, DOI 10.1142/S0218127410026563
   Tong XJ, 2012, J SYST SOFTWARE, V85, P850, DOI 10.1016/j.jss.2011.10.051
   Wang XY, 2014, NONLINEAR DYNAM, V75, P345, DOI 10.1007/s11071-013-1070-x
   Wu XL, 2017, IEEE ACCESS, V5, P6429, DOI 10.1109/ACCESS.2017.2692043
   Xie EY, 2017, SIGNAL PROCESS, V132, P150, DOI 10.1016/j.sigpro.2016.10.002
   Yang YG, 2015, SCI REP-UK, V5, DOI 10.1038/srep07784
   Yu F., 2018, ARXIV181211693
   Zanin M, 2014, INFORM SCIENCES, V270, P288, DOI 10.1016/j.ins.2014.02.131
   Zhang LY, 2018, IEEE T CYBERNETICS, V48, P1163, DOI 10.1109/TCYB.2017.2682561
   Zhang YS, 2013, NONLINEAR DYNAM, V72, P751, DOI 10.1007/s11071-013-0750-x
   Zhou YC, 2015, IEEE T CYBERNETICS, V45, P2001, DOI 10.1109/TCYB.2014.2363168
   Zhu ZL, 2011, INFORM SCIENCES, V181, P1171, DOI 10.1016/j.ins.2010.11.009
NR 47
TC 34
Z9 36
U1 1
U2 83
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2372
EP 2385
DI 10.1109/TMM.2020.3011315
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, Y
   Gong, MG
   Xie, Y
   Qin, AK
AF Gao, Yuan
   Gong, Maoguo
   Xie, Yu
   Qin, A. K.
TI An Attention-Based Unsupervised Adversarial Model for Movie Review Spam
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Review
DE Movie reviews; spam detection; attention mechanism; generative
   adversarial networks (GANs)
ID SENTIMENT
AB With the prevalence of the Internet, online reviews have become a valuable information resource for people. However, the authenticity of online reviews remains a concern, and deceptive reviews have become one of the most urgent network security problems to be solved. Review spams will mislead users into making suboptimal choices and inflict their trust in online reviews. Most existing research manually extracted features and labeled training samples, which are usually complicated and time-consuming. This paper focuses primarily on a neglected emerging domain - movie review, and develops a novel unsupervised spam detection model with an attention mechanism. By extracting the statistical features of reviews, it is revealed that users will express their sentiments on different aspects of movies in reviews. An attention mechanism is introduced in the review embedding, and the conditional generative adversarial network is exploited to learn users' review style for different genres of movies. The proposed model is evaluated on movie reviews crawled from Douban, a Chinese online community where people could express their feelings about movies. The experimental results demonstrate the superior performance of the proposed approach.
C1 [Gao, Yuan; Gong, Maoguo; Xie, Yu; Qin, A. K.] Xidian Univ, Minist Educ, Key Lab Intelligent Percept & Image Understand, Sch Elect Engn, Xian 710071, Peoples R China.
   [Qin, A. K.] Swinburne Univ Technol, Dept Comp Sci & Software Engn, Melbourne, Vic 3122, Australia.
C3 Xidian University; Swinburne University of Technology
RP Gong, MG (corresponding author), Xidian Univ, Minist Educ, Key Lab Intelligent Percept & Image Understand, Sch Elect Engn, Xian 710071, Peoples R China.
EM cn_gaoyuan@foxmail.com; gong@ieee.org; sxlljcxy@gmail.com;
   kqin@swin.edu.au
RI Qin, Kai/AAH-4943-2021; Gao, Yuan/AAP-7481-2021
OI Qin, Kai/0000-0001-6631-1651; Gao, Yuan/0000-0002-2990-9205
FU National Key Research and Development Program of China [2017YFB0802200];
   Australian Research Council [LP170100416, LP180100114, DP200102611];
   Australian Research Council [DP200102611, LP180100114, LP170100416]
   Funding Source: Australian Research Council
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFB0802200 and in part by
   Australian Research Council under Grants LP170100416, LP180100114, and
   DP200102611.
CR An J., 2015, Special Lecture on IE, V2, P1
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Arora S., 2016, T ASS COMPUTATIONAL, V4, P385, DOI [10.1162/tacl_a_00106, DOI 10.1162/TACL_A_00106]
   Arora S, 2019, 5 INT C LEARN REPR I
   Breunig M. M., 2000, SIGMOD Record, V29, P93, DOI 10.1145/335191.335388
   Chua AYK, 2016, COMPUT HUM BEHAV, V54, P547, DOI 10.1016/j.chb.2015.08.057
   Crawford M., 2015, J BIG DATA, V2, P23, DOI [10.1186/s40537-015-0029-9, DOI 10.1186/S40537-015-0029-9]
   DEMIREL EU, 2018, ARTS MARK, V8, P80, DOI DOI 10.1108/AAM-06-2017-0011
   Dewang RK, 2018, J INTELL INF SYST, V50, P231, DOI 10.1007/s10844-017-0454-7
   Diao QM, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P193, DOI 10.1145/2623330.2623758
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P2281, DOI 10.1109/TMM.2015.2491019
   Fawzi A, 2017, IEEE SIGNAL PROC MAG, V34, P50, DOI 10.1109/MSP.2017.2740965
   Forman George, 2010, ACM SIGKDD Explorations Newsletter, DOI DOI 10.1145/1882471.1882479
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hardin J, 2004, COMPUT STAT DATA AN, V44, P625, DOI 10.1016/S0167-9473(02)00280-3
   Hu N, 2011, DECIS SUPPORT SYST, V50, P627, DOI 10.1016/j.dss.2010.08.013
   Hu N, 2011, DECIS SUPPORT SYST, V50, P614, DOI 10.1016/j.dss.2010.08.012
   Hussain N, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9050987
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Jindal N., 2008, WSDM 08, P219, DOI [DOI 10.1145/1341531.1341560, 10.1145/1341531.1341560]
   Khurshid F, 2017, 2017 12TH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS AND KNOWLEDGE ENGINEERING (IEEE ISKE), DOI 10.1109/ISKE.2017.8258755
   Lee SY, 2018, PROD OPER MANAG, V27, P393, DOI 10.1111/poms.12805
   Legoux R, 2016, INT J RES MARK, V33, P357, DOI 10.1016/j.ijresmar.2015.07.003
   Li LY, 2017, NEUROCOMPUTING, V254, P33, DOI 10.1016/j.neucom.2016.10.080
   Li M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P661, DOI 10.1145/2623330.2623612
   Li XL, 2019, INFORM MANAGE-AMSTER, V56, P172, DOI 10.1016/j.im.2018.04.007
   Lim E.P., 2010, P ACM INT C INFORM K, P939, DOI DOI 10.1145/1871437.1871557
   Liu F, 2008, ACTA HORTIC, V792, P413, DOI 10.17660/ActaHortic.2008.792.48
   Liu Y, 2005, IEEE IJCNN, P849
   Ma HX, 2019, ELECTRON COMMER R A, V35, DOI 10.1016/j.elerap.2019.100840
   Mayzlin D, 2014, AM ECON REV, V104, P2421, DOI 10.1257/aer.104.8.2421
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Ott M., 2011, ACL HLT 2011 P 49 AN, V1, P309, DOI DOI 10.1145/2567948.2577293
   Poria S, 2016, KNOWL-BASED SYST, V108, P42, DOI 10.1016/j.knosys.2016.06.009
   Quinn M, 2019, FED CONF COMPUT SCI, P81, DOI 10.15439/2019F274
   Ravanbakhsh M, 2017, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2017.8296547
   Rayana S, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P985, DOI 10.1145/2783258.2783370
   Rosario AB, 2016, J MARKETING RES, V53, P297, DOI 10.1509/jmr.14.0380
   SAIDANI N, 2017, P 3 INT C E TECHN, P136, DOI DOI 10.1007/978-3-319-59041-7_8
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Schölkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   Shehnepoor S, 2017, IEEE T INF FOREN SEC, V12, P1585, DOI 10.1109/TIFS.2017.2675361
   Sun Q, 2016, 2016 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING, APPLICATIONS AND TECHNOLOGIES (BDCAT), P262, DOI 10.1145/3006299.3006325
   Tang DY, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1555
   Thet TT, 2010, J INF SCI, V36, P823, DOI 10.1177/0165551510388123
   Vidanagama DU, 2020, ARTIF INTELL REV, V53, P1323, DOI 10.1007/s10462-019-09697-5
   Xue H, 2019, ACM J DATA INF QUAL, V11, DOI 10.1145/3305258
   Yang J, 2016, DECIS SUPPORT SYST, V89, P66, DOI 10.1016/j.dss.2016.06.009
   Yang M, 2017, AAAI CONF ARTIF INTE, P5011
   Zhao GS, 2019, IEEE T MULTIMEDIA, V21, P771, DOI 10.1109/TMM.2018.2863598
   Zhuang L., 2006, P 15 ACM INT C INF K, P43, DOI [DOI 10.1145/1183614.1183625, 10.1145/1183614.1183625]
NR 52
TC 35
Z9 37
U1 6
U2 53
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 784
EP 796
DI 10.1109/TMM.2020.2990085
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ghazalian, R
   Aghagolzadeh, A
   Andargoli, SMH
AF Ghazalian, Reza
   Aghagolzadeh, Ali
   Andargoli, Seyed Mehdi Hosseini
TI Energy Optimization and QoE Satisfaction for Wireless Visual Sensor
   Networks in Multi Target Tracking Scenario
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of experience; Target tracking; Energy consumption;
   Visualization; Cameras; Optimization; Wireless communication; Convex
   Optimization; Focal Length; Multi Target Tracking; Wireless Visual
   Sensor Network
ID LIFETIME MAXIMIZATION; CAMERA SELECTION
AB Nowadays, by emerging new technologies, demand for a wireless visual sensor networks (WVSN) has been significantly increased. By providing the vital visual data from such networks, they play a substantial role in the surveillance applications. The wireless visual sensors (VSes) are the main component of these networks, which are equipped with camera and transceiver module. Energy optimization and satisfying the quality of the captured visual data, are two main contradictable issues in this area of research, especially in the target tracking applications. Therefore, these two issues have been simultaneously investigated in this paper. The coverage of the tracked targets and the quality of the captured visual data are considered as the quality of experienced (QoE). The desired threshold of QoE is defined by user. To optimize energy consumption with regard to QoE constraints in a multi target tracking scenario, an effective VSes selection is proposed. This method has been developed based on the convex optimization framework. Besides, the focal length of the VSes is set optimally by executing the proposed algorithm. Simulation results show efficiency of the proposed method compared with the optimal method (exhaustive search).
C1 [Ghazalian, Reza; Aghagolzadeh, Ali; Andargoli, Seyed Mehdi Hosseini] Babol Noshirvani Univ Technol, Dept Elect & Comp Engn, Babol 47148, Iran.
C3 Babol Noshirvani University of Technology
RP Aghagolzadeh, A (corresponding author), Babol Noshirvani Univ Technol, Dept Elect & Comp Engn, Babol 47148, Iran.
EM rezaghazalian@bzte.ac.ir; aghagol@nit.ac.ir; smh_andargoli@nit.ac.ir
RI Hosseini Andargoli, Seyed Mehdi/AFM-7448-2022; , Reza/Y-5947-2019
OI Hosseini Andargoli, Seyed Mehdi/0000-0002-8277-7002; ,
   Reza/0000-0002-4112-4961
FU Babol Noshirvani University of Technology [BNUT/389059/98]
FX This work was supported by the Babol Noshirvani University of Technology
   under Grant BNUT/389059/98.
CR Amiri SM, 2011, IET COMMUN, V5, P2443, DOI 10.1049/iet-com.2010.0952
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cai YL, 2009, IEEE T COMPUT, V58, P1259, DOI 10.1109/TC.2009.40
   Chen KM, 2019, IEICE T COMMUN, VE102B, P528, DOI 10.1587/transcom.2018EBP3079
   Dai R, 2012, IEEE T MULTIMEDIA, V14, P1469, DOI 10.1109/TMM.2012.2194992
   Fan CL, 2018, INT J DISTRIB SENS N, V14, DOI 10.1177/1550147718765980
   Fusco G., 2009, 6th annual IEEE communications society conference on sensor, mesh and ad hoc communications and networks, P1
   Ghazalian R, 2017, IEEE SENS J, V17, P4056, DOI 10.1109/JSEN.2017.2702121
   Ghazalian R, 2014, 2014 7TH INTERNATIONAL SYMPOSIUM ON TELECOMMUNICATIONS (IST), P312, DOI 10.1109/ISTEL.2014.7000720
   Hameed A, 2016, IEEE T MULTIMEDIA, V18, P764, DOI 10.1109/TMM.2016.2525862
   Hooshmand M, 2013, J NETW COMPUT APPL, V36, P409, DOI 10.1016/j.jnca.2012.04.017
   Hoseini S. M., 2012, 2012 2nd International eConference on Computer and Knowledge Engineering (ICCKE 2012), P260, DOI 10.1109/ICCKE.2012.6395389
   Hosseini M, 2012, 2012 SIXTH INTERNATIONAL SYMPOSIUM ON TELECOMMUNICATIONS (IST), P697, DOI 10.1109/ISTEL.2012.6483076
   Idoudi M, 2018, IEEE SENS J, V18, P5915, DOI 10.1109/JSEN.2018.2838676
   Jin YQ, 2017, INT CON DISTR COMP S, P1301, DOI 10.1109/ICDCS.2017.236
   Khernane N, 2018, COMPUT COMMUN, V124, P1, DOI 10.1016/j.comcom.2018.04.012
   Kim S, 2016, DIGIT SIGNAL PROCESS, V50, P135, DOI 10.1016/j.dsp.2015.12.007
   Liu FM, 2015, IEEE T COMPUT, V64, P3051, DOI 10.1109/TC.2015.2401032
   Liu FM, 2013, IEEE WIREL COMMUN, V20, P14
   Lyon R F., 2006, Online Monograph
   Sheng ZG, 2015, IEEE T VEH TECHNOL, V64, P1156, DOI 10.1109/TVT.2014.2322653
   Wang P, 2013, IEEE T MULTIMEDIA, V15, P684, DOI 10.1109/TMM.2012.2236304
   Wang P, 2011, IEEE T MULTIMEDIA, V13, P388, DOI 10.1109/TMM.2010.2100374
   Yap FGH, 2014, SENSORS-BASEL, V14, P3506, DOI 10.3390/s140203506
   Yen HH, 2013, INT WIREL COMMUN, P1516, DOI 10.1109/IWCMC.2013.6583781
   Yetgin H, 2017, IEEE COMMUN SURV TUT, V19, P828, DOI 10.1109/COMST.2017.2650979
   Zannat H, 2016, J NETW COMPUT APPL, V75, P1, DOI 10.1016/j.jnca.2016.08.015
   Zhang T, 2015, INT CON DISTR COMP S, P113, DOI 10.1109/ICDCS.2015.20
NR 28
TC 5
Z9 5
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 823
EP 834
DI 10.1109/TMM.2020.2990077
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SN5MI
UT WOS:000658333200001
DA 2024-07-18
ER

PT J
AU Li, XR
   Zhou, FM
   Xu, CX
   Ji, JQ
   Yang, G
AF Li, Xirong
   Zhou, Fangming
   Xu, Chaoxi
   Ji, Jiaqi
   Yang, Gang
TI SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoding; Computational modeling; Semantics; Vocabulary; Bit error rate;
   Task analysis; Benchmark testing; Ad-hoc video search; cross-modal
   representation learning; sentence encoder assembly; multiple space
   learning
ID EMBEDDINGS
AB Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search (AVS), is a core theme in multimedia data management and retrieval. The success of AVS counts on cross-modal representation learning that encodes both query sentences and videos into common spaces for semantic similarity computation. Inspired by the initial success of previously few works in combining multiple sentence encoders, this paper takes a step forward by developing a new and general method for effectively exploiting diverse sentence encoders. The novelty of the proposed method, which we term Sentence Encoder Assembly (SEA), is two-fold. First, different from prior art that uses only a single common space, SEA supports text-video matching in multiple encoder-specific common spaces. Such a property prevents the matching from being dominated by a specific encoder that produces an encoding vector much longer than other encoders. Second, in order to explore complementarities among the individual common spaces, we propose multi-space multi-loss learning. As extensive experiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD) show, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to implement. All this makes SEA an appealing solution for AVS and promising for continuously advancing the task by harvesting new sentence encoders.
C1 [Li, Xirong; Zhou, Fangming; Xu, Chaoxi; Ji, Jiaqi; Yang, Gang] Renmin Univ China, Key Lab Data Engn & Knowledge Engn, Beijing 100872, Peoples R China.
   [Li, Xirong; Zhou, Fangming; Xu, Chaoxi; Ji, Jiaqi; Yang, Gang] Renmin Univ China, AI & Media Comp Lab, Sch Informat, Beijing 100872, Peoples R China.
C3 Renmin University of China; Renmin University of China
RP Li, XR (corresponding author), Renmin Univ China, Key Lab Data Engn & Knowledge Engn, Beijing 100872, Peoples R China.
EM xirong.li@gmail.com; fangming_zhou@ruc.edu.cn; xcx@ruc.edu.cn;
   2019104238@ruc.edu.cn; yanggang@ruc.edu.cn
RI Li, Xirong/AAD-3347-2019
OI Li, Xirong/0000-0002-0220-8310; Xu, Chaoxi/0000-0003-4883-1703
FU NSFC [61672523]; Beijing Natural Science Foundation [4202033];
   Fundamental Research Funds for the Central Universities; Research Funds
   of Renmin University of China [18XNLG19]
FX This work was supported in part by NSFC (No. 61672523), Beijing Natural
   Science Foundation (No. 4202033), the Fundamental Research Funds for the
   Central Universities and the Research Funds of Renmin University of
   China (No. 18XNLG19). The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Raouf
   Hamzaoui.
CR Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Awad G., 2019, P TRECVID WORKSH
   Awad G, 2017, P TRECVID WORKSH
   Awad G., 2018, TRECVID WORKSH
   Awad G, 2016, P TRECVID WORKSH
   Awad G, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P3, DOI 10.1145/3078971.3079044
   Bastan M, 2018, P TRECVID WORKSH
   Berns F, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P334, DOI 10.1145/3323873.3325051
   Bishop Christopher M., 2006, Pattern Recognition and Machine Learning, V4
   Chang X, 2018, P TRECVID WORKSH
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen S., 2020, P IEEE CVF C COMP VI, P10638
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Galanopoulos Damianos, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P336, DOI 10.1145/3372278.3390737
   Habibian A, 2017, IEEE T PATTERN ANAL, V39, P2089, DOI 10.1109/TPAMI.2016.2627563
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Joulin A, 2016, LECT NOTES COMPUT SC, V9911, P67, DOI 10.1007/978-3-319-46478-7_5
   Kobayashi K, 2019, P TRECVID WORKSH
   Kobayashi K, 2017, P TRECVID WORKSH
   Kratochvíl M, 2020, LECT NOTES COMPUT SC, V11962, P790, DOI 10.1007/978-3-030-37734-2_71
   Le D.-D, 2016, P TRECVID WORKSH
   Li X, 2018, P TRECVID WORKSH
   Li X, 2019, P TRECVID WORKSH
   Li XR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1786, DOI 10.1145/3343031.3350906
   Li YC, 2016, PROC CVPR IEEE, P4641, DOI 10.1109/CVPR.2016.502
   Liang J, 2016, P TRECVID WORKSH
   Liu Y., 2019, P BRIT MACH VIS C, P279
   Lokoc J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2553, DOI 10.1145/3394171.3414002
   Lokoc J, 2018, IEEE T MULTIMEDIA, V20, P3361, DOI 10.1109/TMM.2018.2830110
   Lu YJ, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P127, DOI 10.1145/2911996.2912015
   Luo H, ARXIV200206353, V2020
   Markatopoulou F, 2016, P TRECVID WORKSH
   Markatopoulou F, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P412, DOI 10.1145/3078971.3079041
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Mithun NC, 2019, INT J MULTIMED INF R, V8, P3, DOI 10.1007/s13735-018-00166-3
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Nguyen P, 2017, P TRECVID WORKSH
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Paszke A, 2019, ADV NEUR IN, V32
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Rossetto L, 2019, LECT NOTES COMPUT SC, V11295, P349, DOI 10.1007/978-3-030-05710-7_29
   Rossetto Luca, 2020, IEEE T MULTIMEDIA
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Snoek CGM, 2017, TRECVID
   Snoek D. C, 2017, P TRECVID WORKSH
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang W., 2020, IEEE T MULTIMEDIA
   Wray M, 2019, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2019.00054
   Wu X, 2019, P TRECVID WORKSH
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1339, DOI 10.1145/3397271.3401151
   Yilmaz Emine, 2006, Proceedings of the 2006 ACM CIKM International Conference on Information and Knowledge Management, Arlington, Virginia, USA, November 6-11, 2006, P102, DOI [10.1145/1183614.1183633 (cit. on p. 34, DOI 10.1145/1183614.1183633(CIT.ONP.34]
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23
NR 61
TC 23
Z9 24
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4351
EP 4362
DI 10.1109/TMM.2020.3042067
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900034
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Nie, WZ
   Jia, WW
   Li, WH
   Liu, AA
   Zhao, SC
AF Nie, Wei-Zhi
   Jia, Wen-Wu
   Li, Wen-Hui
   Liu, An-An
   Zhao, Si-Cheng
TI 3D Pose Estimation Based on Reinforce Learning for 2D Image-Based 3D
   Model Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Two dimensional displays;
   Computational modeling; Shape; Feature extraction; Adaptation models;
   Characteristic View; pose estimation; reinforcement learning; 2D image;
   3D object
ID OBJECT RETRIEVAL; NEURAL-NETWORKS
AB In this paper, we propose a novel characteristic view selection model (CVSM) to address the 2D image-based 3D object retrieval problem. This work includes two key contributions: 1) we propose a novel reinforcement learning model to estimate the 3D pose based on a 2D image; and 2) we render the pose-specific model to generate a representative angle view for retrieval applications. First, we define state, policy, action and reward functions to train an agent with the reinforcement learning framework, by which the agent can effectively reduce the computational cost of the characteristic view selection and directly obtain the 3D model pose. Second, to resolve the problem of computing similarity in the cross-domain between the virtual 3D model view and the real query image, we project them into the skeleton domain, and the skeleton information can effectively bridge the gap between the image and 3D model view for cross-media retrieval. To demonstrate the performance of our approach, we compare with some classic 3D pose estimation methods using the popular Pascal3D dataset. To demonstrate the performance of our approach in model retrieval, we collect a new dataset that includes pairs of 2D images and 3D objects, where 3D objects are based on the ModelNet40 dataset and 2D images are based on the ImageNet dataset, and we experiment with our method using the SHREC 2018 and SHREC 2019 databases. The experimental results demonstrate the superiority of our method.
C1 [Nie, Wei-Zhi; Jia, Wen-Wu; Li, Wen-Hui; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Zhao, Si-Cheng] Univ Calif Berkeley, Berkeley, CA 94720 USA.
C3 Tianjin University; University of California System; University of
   California Berkeley
RP Li, WH; Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM weizhinie@tju.edu.cn; jiaww111@126.com; liwenhui@tju.edu.cn;
   anan0422@gmail.com; schzhao@gmail.com
RI Nie, Weizhi/ABF-5316-2021; LI, Wenhui/JCD-9947-2023; Lu,
   Wang/JVO-0416-2024; Zeng, Yun/JFK-6190-2023
OI nie, weizhi/0000-0002-0578-8138
FU National Natural Science Foundation of China [61872267, 61772359,
   61702471]; Natural Science Foundation of Tianjin [19JCQNJC00500];
   Tianjin New Generation Artificial Intelligence Major Program
   [19ZXZNGX00110, 18ZXZNGX00150]; Open Project Program of the State Key
   Lab of CAD & CG, Zhejiang University [A2005, A2012]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872267, 61772359, and 61702471, in
   part by the Natural Science Foundation of Tianjin under Grant
   19JCQNJC00500, in part by the Tianjin New Generation Artificial
   Intelligence Major Program under Grant 19ZXZNGX00110, 18ZXZNGX00150, and
   in part by the Open Project Program of the State Key Lab of CAD & CG,
   Zhejiang University under Grants A2005 and A2012.
CR Abdul-Rashid H., 2018, EUR WORKSH 3D OBJ RE
   Anjulan A, 2009, IEEE T CIRC SYST VID, V19, P63, DOI 10.1109/TCSVT.2008.2005801
   [Anonymous], 2012, NEURIPS
   Armagan A, 2017, PROC CVPR IEEE, P4590, DOI 10.1109/CVPR.2017.488
   Aubry M, 2015, IEEE I CONF COMP VIS, P2875, DOI 10.1109/ICCV.2015.329
   Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487
   Bosche F, 2008, AUTOMAT CONSTR, V17, P499, DOI 10.1016/j.autcon.2007.09.001
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Crivellaro A, 2015, IEEE I CONF COMP VIS, P4391, DOI 10.1109/ICCV.2015.499
   Dai GX, 2018, IEEE T IMAGE PROCESS, V27, P3374, DOI 10.1109/TIP.2018.2817042
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Furuya Takahiko, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P543, DOI 10.1109/3DV.2014.72
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong BQ, 2013, IEEE T MULTIMEDIA, V15, P369, DOI 10.1109/TMM.2012.2231059
   Grabner A, 2018, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2018.00319
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Guétat G, 2006, IEEE T INF TECHNOL B, V10, P362, DOI 10.1109/TITB.2005.863875
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Li WQ, 2020, ENERGY TECHNOL-GER, V8, DOI 10.1002/ente.201900871
   Li Yong, 2015, Advanced Technology of Electrical Engineering and Energy, V34, P1
   Liu AA, 2019, IEEE T CIRC SYST VID, V29, P868, DOI 10.1109/TCSVT.2018.2810191
   Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Massa F, 2016, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2016.648
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597
   Mu PP, 2018, FRONT INFORM TECH EL, V19, P1397, DOI 10.1631/FITEE.1601764
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pavlakos G., 2017, P IEEE C COMPUTER VI, P2011, DOI DOI 10.1109/CVPR.2017.139
   Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen W, 2017, IEEE T IMAGE PROCESS, V26, P5298, DOI 10.1109/TIP.2017.2735182
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Tulsiani S, 2015, IEEE I CONF COMP VIS, P64, DOI 10.1109/ICCV.2015.16
   Tulsiani S, 2015, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2015.7298758
   Wong HS, 2007, IEEE T MULTIMEDIA, V9, P1026, DOI 10.1109/TMM.2007.898915
   Wu BT, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3946
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101
   Xie J, 2017, IEEE T MULTIMEDIA, V19, P2463, DOI 10.1109/TMM.2017.2698200
   Yeh JS, 2005, BIOINFORMATICS, V21, P3056, DOI 10.1093/bioinformatics/bti458
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhu F, 2016, AAAI CONF ARTIF INTE, P3683
   2017, IEEE T CYBERN, V47, P4342
NR 51
TC 19
Z9 19
U1 3
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1021
EP 1034
DI 10.1109/TMM.2020.2991532
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300015
DA 2024-07-18
ER

PT J
AU Nie, WZ
   Ren, MJ
   Liu, AA
   Mao, ZD
   Nie, J
AF Nie, Wei-Zhi
   Ren, Min-Jie
   Liu, An-An
   Mao, Zhendong
   Nie, Jie
TI M-GCN: Multi-Branch Graph Convolution Network for 2D Image-based on 3D
   Model Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Two dimensional displays;
   Computational modeling; Visualization; Feature extraction; Predictive
   models; Cross-domain retrieval; 3D model retrieval; multi-head
   attention; multiple graphs
AB 2D image based 3D model retrieval is a challenging research topic in the field of 3D model retrieval. The huge gap between two modalities - 2D image and 3D model, extremely constrains the retrieval performance. In order to handle this problem, we propose a novel multi-branch graph convolution network (M-GCN) to address the 2D image based 3D model retrieval problem. First, we compute the similarity between 2D image and 3D model based on visual information to construct one cross-modalities graph model, which can provide the original relationship between image and 3D model. However, this relationship is not accurate because of the difference of modalities. Thus, the multi-head attention mechanism is employed to generate a set of fully connected edge-weighted graphs, which can predict the hidden relationship between 2D image and 3D model to further strengthen the correlation for the embedding generation of nodes. Finally, we apply the max-pooling operation to fuse the multi-graphs information and generate the fusion embeddings of nodes for retrieval. To validate the performance of our method, we evaluated M-GCN on the MI3DOR dataset, Shrec 2018 track and Shrec 2014 track. The experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods.
C1 [Nie, Wei-Zhi; Ren, Min-Jie; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Mao, Zhendong] Univ Sci & Technol China, Sch Elect Informat Engn, Hefei 230052, Peoples R China.
   [Nie, Jie] Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Ocean University of China
RP Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Nie, J (corresponding author), Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
EM weizhinie@tju.edu.cn; renminjie@tju.edu.cn; anan0422@gmail.com;
   maozhendong2008@gmail.com; niejie@ouc.edu.cn
RI Nie, Weizhi/ABF-5316-2021; Nie, Jie/ABG-9228-2021; lu,
   lala/GQQ-3784-2022
OI Nie, Jie/0000-0003-4952-7666; lu, lala/0000-0002-6080-8074; nie,
   weizhi/0000-0002-0578-8138
FU National Natural Science Foundation of China [61772359, 61572356,
   61872267, 61902277]; 2019 Tianjin New Generation Artificial Intelligence
   Major Program [18ZXZNGX00150, 19ZXZNGX00110]; Open Project Program of
   the State Key Lab of CAD & CG, Zhejiang University [A2005, A2012];
   Tianjin Science Foundation for Young Scientists of China [19JCQNJC00500]
FX Manuscript received December 6, 2019; revised February 29, 2020 and June
   11, 2020; accepted June 23, 2020. Date of publication July 3, 2020; date
   of current version June 25, 2021. This work was supported in part by the
   National Natural Science Foundation of China under Grants 61772359,
   61572356, 61872267, and 61902277, in part by the grant of 2019 Tianjin
   New Generation Artificial Intelligence Major Program under Grants
   18ZXZNGX00150 and 19ZXZNGX00110, in part by the Open ProjectProgram of
   the State Key Lab of CAD & CG, Zhejiang University under Grants A2005
   and A2012, and in part by the Tianjin Science Foundation for Young
   Scientists of China under Grant 19JCQNJC00500. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Wen-Huang Cheng.
CR Abdul-Rashid H., 2019, EUROGRAPHICSWORKSHOP
   Abdul-Rashid Hameed, 2018, 11 EUROGRAPHICS WORK, P37
   [Anonymous], 2012, P 3DOR
   Aubry M, 2015, IEEE I CONF COMP VIS, P2875, DOI 10.1109/ICCV.2015.329
   Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487
   Battaglia P.W., 2018, ARXIV 180601261
   Bosche F, 2008, AUTOMAT CONSTR, V17, P499, DOI 10.1016/j.autcon.2007.09.001
   Cai HY, 2018, IEEE T KNOWL DATA EN, V30, P1616, DOI 10.1109/TKDE.2018.2807452
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cucurull G., 2018, PROC INT C LEARN REP
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Fisher M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818057
   Furuya T, 2013, 2013 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P274, DOI 10.1109/CW.2013.60
   Ghorbani M, 2019, PROCEEDINGS OF THE 2019 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2019), P208, DOI 10.1145/3341161.3342942
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Guétat G, 2006, IEEE T INF TECHNOL B, V10, P362, DOI 10.1109/TITB.2005.863875
   Guo YC, 2018, IEEE T IMAGE PROCESS, V27, P949, DOI 10.1109/TIP.2017.2766445
   Guo ZJ, 2019, T ASSOC COMPUT LING, V7, P297, DOI 10.1162/tacl_a_00269
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426
   He XW, 2019, IEEE I CONF COMP VIS, P7514, DOI 10.1109/ICCV.2019.00761
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P5814, DOI 10.1109/TIP.2016.2614132
   Lee T, 2018, INT CONF 3D VISION, P258, DOI 10.1109/3DV.2018.00038
   Li B, 2014, EUR WORKSH 3D OBJ RE, P121
   Li B, 2015, COMPUT VIS IMAGE UND, V131, P1, DOI 10.1016/j.cviu.2014.10.006
   Li B, 2014, COMPUT VIS IMAGE UND, V119, P57, DOI 10.1016/j.cviu.2013.11.008
   Li CS, 2020, IEEE ACCESS, V8, P105634, DOI 10.1109/ACCESS.2020.2999520
   Li Wenhui, 2019, PROC 3DOR EUROGRAPHI, P103
   Lin ZJ, 2017, IEEE T CYBERNETICS, V47, P4342, DOI 10.1109/TCYB.2016.2608906
   Liu AA, 2018, IEEE T CYBERNETICS, V48, P916, DOI 10.1109/TCYB.2017.2664503
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Mu PP, 2018, FRONT INFORM TECH EL, V19, P1397, DOI 10.1631/FITEE.1601764
   Nie WZ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3344684
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Perozzi B., 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732
   Pham Q., 2018, P EUR WORKSH 3D OBJ, P45
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Savva M., 2017, P WORKSH 3D OBJ RETR, P39, DOI DOI 10.2312/3DOR.20171050
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   Tatsuma A., 2012, Signal Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Pacific, P1
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wong HS, 2007, IEEE T MULTIMEDIA, V9, P1026, DOI 10.1109/TMM.2007.898915
   Wu BT, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3946
   Wu GS, 2019, IEEE T IND ELECTRON, V66, P9868, DOI 10.1109/TIE.2018.2873547
   Wu GS, 2019, IEEE T IMAGE PROCESS, V28, P1993, DOI 10.1109/TIP.2018.2882155
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu C, 2019, IEEE I CONF COMP VIS, P3731, DOI 10.1109/ICCV.2019.00383
   Yang Y, 2019, IEEE T MULTIMEDIA, V21, P809, DOI 10.1109/TMM.2018.2867742
   Yeh JS, 2005, BIOINFORMATICS, V21, P3056, DOI 10.1093/bioinformatics/bti458
   Yuan ZW, 2016, PROC SPIE, V10033, DOI 10.1117/12.2243849
   Zhuang CY, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P499, DOI 10.1145/3178876.3186116
   Zou CQ, 2014, IEEE SIGNAL PROC LET, V21, P966, DOI 10.1109/LSP.2014.2321764
NR 60
TC 26
Z9 28
U1 4
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1962
EP 1976
DI 10.1109/TMM.2020.3006371
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100011
DA 2024-07-18
ER

PT J
AU Wang, XJ
   Shao, F
   Jiang, QP
   Meng, XC
   Ho, YS
AF Wang, Xuejin
   Shao, Feng
   Jiang, Qiuping
   Meng, Xiangchao
   Ho, Yo-Sung
TI Measuring Coarse-to-Fine Texture and Geometric Distortions for Quality
   Assessment of DIBR-Synthesized Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Distortion measurement; Image edge detection; Feature
   extraction; Image color analysis; Color; Quality assessment; view
   synthesis; DIBR; texture distortion; geometric distortion;
   coarse-to-fine
ID VIEW SYNTHESIS; STEREOSCOPIC IMAGES; EDGE INTENSITY; COLOR; PREDICTION;
   PLUS
AB A synthesized view can be generated via Depth-Image-Based Rendering (DIBR) technique using one (or more) color images and the associated depth maps. However, several artifacts may occur in the synthesized views due to the imperfect color images, depth maps or texture inpainting techniques, which cannot be effectively estimated by the conventional quality metrics designed for natural images. In this paper, a new quality metric is proposed to evaluate DIBR-synthesized images by measuring texture and geometric distortions. The artifacts are first analyzed on different phases of the synthesis process, and the associated features are extracted to estimate the degree of texture and geometric distortions from both coarse and fine scales. Finally, individual quality scores are aggregated into an overall quality via regression. Experimental results on three publicly available DIBR datasets demonstrate the superiority of the proposed method over the state-of-the-art quality models.
C1 [Wang, Xuejin; Shao, Feng; Jiang, Qiuping; Meng, Xiangchao] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol GIST, Sch Informat & Commun, Gwangju 500712, South Korea.
C3 Ningbo University; Gwangju Institute of Science & Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM 1020468620@qq.com; shaofeng@nbu.edu.cn; jiangqiuping@nbu.edu.cn;
   mengxiangchao@nbu.edu.cn; hoyo@gist.ac.kr
RI Jiang, Qiuping/AAL-8273-2020
OI HO, YO-SUNG/0000-0002-7220-1034; Qiuping, Jiang/0000-0002-6025-9343
FU Natural Science Foundation of China [61622109, 61901236, 41801252];
   Zhejiang Natural Science Foundation of China [R18F010008]; Natural
   Science Foundation of Ningbo [2019A610097, 2019A610098, 2017A610112]; K.
   C. Wong Magna Fund in Ningbo University
FX This work was supported in part by the Natural Science Foundation of
   China underGrants 61622109, 61901236, and 41801252, in part by the
   Zhejiang Natural Science Foundation of China under Grant R18F010008, and
   in part by the Natural Science Foundation of Ningbo (2019A610097,
   2019A610098, and 2017A610112). It was also sponsored by K. C. Wong Magna
   Fund in Ningbo University. The Associate Editor coordinating the review
   of this manuscript and approving it for publication was Dr. Mohammed
   Daoudi.
CR Ahn I, 2013, IEEE T BROADCAST, V59, P614, DOI 10.1109/TBC.2013.2281658
   [Anonymous], 2013, 3D TV SYSTEM DEPTH I, DOI DOI 10.1007/978-1-4419-9964-1_15
   [Anonymous], 2015, P IEEE TRUE VIS CAPT
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Battisti F, 2015, SIGNAL PROCESS-IMAGE, V30, P78, DOI 10.1016/j.image.2014.10.005
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   Conze PH, 2012, PROC SPIE, V8288, DOI 10.1117/12.908762
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Farid MS, 2017, IEEE INT CON MULTI, P505, DOI 10.1109/ICME.2017.8019307
   Farid MS, 2015, IEEE IMAGE PROC, P3720, DOI 10.1109/ICIP.2015.7351499
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Fu ZQ, 2018, IEEE ACCESS, V6, P12008, DOI 10.1109/ACCESS.2018.2808322
   Gu K, 2017, IEEE IMAGE PROC, P745, DOI 10.1109/ICIP.2017.8296380
   Gu K, 2018, IEEE T IMAGE PROCESS, V27, P394, DOI 10.1109/TIP.2017.2733164
   Gu K, 2016, IEEE T BROADCAST, V62, P446, DOI 10.1109/TBC.2015.2511624
   Gu K, 2014, IEEE IMAGE PROC, P506, DOI 10.1109/ICIP.2014.7025101
   Herzog R, 2012, COMPUT GRAPH FORUM, V31, P545, DOI 10.1111/j.1467-8659.2012.03055.x
   Jakhetiya V, 2019, IEEE T IND INFORM, V15, P4120, DOI 10.1109/TII.2018.2888861
   Jantet V, 2011, IEEE IMAGE PROC, P125, DOI 10.1109/ICIP.2011.6115662
   Jiang QP, 2019, IEEE T IMAGE PROCESS, V28, P1866, DOI 10.1109/TIP.2018.2881828
   Kauff P, 2007, SIGNAL PROCESS-IMAGE, V22, P217, DOI 10.1016/j.image.2006.11.013
   Köppel M, 2010, IEEE IMAGE PROC, P1809, DOI 10.1109/ICIP.2010.5652138
   Lee K, 2015, IEEE J-STSP, V9, P533, DOI 10.1109/JSTSP.2015.2393296
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Lin CH, 2009, IMAGE VISION COMPUT, V27, P658, DOI 10.1016/j.imavis.2008.07.004
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Luo GB, 2016, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2016.197
   Milani S, 2013, IEEE IMAGE PROC, P408, DOI 10.1109/ICIP.2013.6738084
   Mori Y., 2008, JTC1SC29WG11 ISO IEC
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   Müller K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/438148
   MULLEN KT, 1985, J PHYSIOL-LONDON, V359, P381, DOI 10.1113/jphysiol.1985.sp015591
   Ndjiki-Nya P, 2010, IEEE INT CON MULTI, P424, DOI 10.1109/ICME.2010.5583559
   Ponomarenko N., 2007, P 3 INT WORKSH VID P
   Rodrigues F, 2019, IEEE T MULTIMEDIA, V21, P1737, DOI 10.1109/TMM.2018.2888830
   Ryu S, 2014, IEEE IMAGE PROC, P585, DOI 10.1109/ICIP.2014.7025117
   Sandic-Stankovic D., 2015, P IEEE 7 INT WORKSH, P1
   Sandic-Stankovic D, 2016, J ELECTR ENG-SLOVAK, V67, P3, DOI 10.1515/jee-2016-0001
   Shao F, 2018, IEEE T MULTIMEDIA, V20, P2605, DOI 10.1109/TMM.2018.2817072
   Shao F, 2018, IEEE T MULTIMEDIA, V20, P659, DOI 10.1109/TMM.2017.2748460
   Shao F, 2016, IEEE T IMAGE PROCESS, V25, P2059, DOI 10.1109/TIP.2016.2538462
   Shao F, 2015, IEEE T IMAGE PROCESS, V24, P2971, DOI 10.1109/TIP.2015.2436332
   Shao F, 2014, IEEE J EM SEL TOP C, V4, P106, DOI 10.1109/JETCAS.2014.2298314
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   Shao H, 2009, 3DTV CONF, P25, DOI 10.1109/3DTV.2009.5069619
   Solh M., 2011, IEEE International Conference on Multimedia and Expo (ICME), P1
   Solh M, 2012, IEEE J-STSP, V6, P495, DOI 10.1109/JSTSP.2012.2204723
   Song R, 2015, J INF SCI ENG, V31, P1593
   Sun C, 2012, IEEE I C EMBED SOFTW, P1391, DOI 10.1109/HPCC.2012.204
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   Tian SS, 2018, IEEE T IMAGE PROCESS, V27, P1652, DOI 10.1109/TIP.2017.2781420
   Tian SS, 2017, INT CONF ACOUST SPEE, P1248, DOI 10.1109/ICASSP.2017.7952356
   Tong HHY, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P428, DOI 10.1109/ICIP.1998.999032
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Wang XC, 2019, COMPUT VIS MEDIA, V5, P193, DOI 10.1007/s41095-019-0131-6
   Wang XJ, 2019, IEEE ACCESS, V7, P10242, DOI 10.1109/ACCESS.2019.2891070
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Yue GH, 2019, IEEE T IMAGE PROCESS, V28, P2075, DOI 10.1109/TIP.2018.2875913
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhou Y, 2018, MULTIMED TOOLS APPL, V77, P21033, DOI 10.1007/s11042-017-5543-7
   Zhou Y, 2017, IEICE T INF SYST, VE100D, P1929, DOI 10.1587/transinf.2016EDL8255
   Zhou Y, 2016, IEEE IMAGE PROC, P1012, DOI 10.1109/ICIP.2016.7532510
   Zhu C, 2016, IEEE T BROADCAST, V62, P82, DOI 10.1109/TBC.2015.2475697
NR 69
TC 8
Z9 8
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1173
EP 1186
DI 10.1109/TMM.2020.2993942
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XG0UQ
UT WOS:000724477100002
DA 2024-07-18
ER

PT J
AU Yang, JC
   Liu, TL
   Jiang, B
   Lu, W
   Meng, QG
AF Yang, Jiachen
   Liu, Tianlin
   Jiang, Bin
   Lu, Wen
   Meng, Qinggang
TI Panoramic Video Quality Assessment Based on Non-Local Spherical CNN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Virtual reality; neural network model; panoramic video; quality
   assessment; spatiotemporal information
ID VIRTUAL-REALITY; SALIENCY
AB Panoramic video and stereoscopic panoramic video are essential carriers of virtual reality content, so it is very crucial to establish their quality assessment models for the standardization of virtual reality industry. However, it is very challenging to evaluate the quality of the panoramic video at present. One reason is that the spatial information of the panoramic video is warped due to the projection process, and the conventional video quality assessment (VQA) method is difficult to deal with this problem. Another reason is that the traditional VQA method is problematic to capture the complex global time information in the panoramic video. In response to the above questions, this paper presents an end-to-end neural network model to evaluate the quality of panoramic video and stereoscopic panoramic video. Compared to other panoramic video quality assessment methods, our proposed method combines spherical convolutional neural networks (CNN) and non-local neural networks, which can effectively extract complex spatiotemporal information of the panoramic video. We evaluate the method in two databases, VRQ-TJU and VR-VQA48. Experiments show the effectiveness of different modules in our method, and our method outperforms state-of-the-art other related methods.
C1 [Yang, Jiachen; Liu, Tianlin; Jiang, Bin] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Lu, Wen] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Meng, Qinggang] Loughborough Univ, Dept Comp Sci, Loughborough LE11 3TU, Leics, England.
C3 Tianjin University; Xidian University; Loughborough University
RP Jiang, B (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM yangjiachen@tju.edu.cn; liutianlin@tju.edu.cn; jiangbin@tju.edu.cn;
   luwen@xidian.edu.cn; q.meng@lboro.ac.uk
RI Yang, Jiachen/ABH-5032-2020; Meng, Qinggang/B-9207-2018
OI Yang, Jiachen/0000-0003-2558-552X; Meng, Qinggang/0000-0002-9483-5724
FU National Natural Science Foundation of China [61871283]; Foundation of
   Pre-Research on Equipment of China [61403120103]; Major Civil-Military
   Integration Projet in Tianjin [18ZXJMTG00170]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61871283, in part by Foundation of
   Pre-Research on Equipment of China under Grant 61403120103, and in part
   by Major Civil-Military Integration Projet in Tianjin under Grant
   18ZXJMTG00170.
CR Abbas A, 2017, PROC SPIE, V10396, DOI 10.1117/12.2271944
   Alekseevskii D. V, 1985, J SOVIET MATH, V28, P924
   [Anonymous], 2007, 2007 IEEE INT C IM P
   Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P2392, DOI 10.1109/TIP.2016.2545863
   Bo Zhang, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P163, DOI 10.1109/ICMEW.2017.8026226
   Chang SM, 2013, HUM-CENTRIC COMPUT I, V3, DOI 10.1186/2192-1962-3-14
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   Chen ZZ, 2018, SIGNAL PROCESS, V146, P66, DOI 10.1016/j.sigpro.2018.01.004
   Cohen T. S., 2018, P INT C LEARN REPR
   Eickenberg M, 2017, NEUROIMAGE, V152, P184, DOI 10.1016/j.neuroimage.2016.10.001
   Engelke U, 2011, IEEE SIGNAL PROC MAG, V28, P50, DOI 10.1109/MSP.2011.942473
   Freina L, 2015, ELEARN SOFTW EDUC, P133, DOI 10.12753/2066-026X-15-020
   Fu CW, 2009, IEEE T MULTIMEDIA, V11, P634, DOI 10.1109/TMM.2009.2017626
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Kingma D. P., 2014, arXiv
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P3329, DOI 10.1109/TIP.2016.2568752
   Li YM, 2016, IEEE T CIRC SYST VID, V26, P1044, DOI 10.1109/TCSVT.2015.2430711
   Liu Y, 2016, SIGNAL PROCESS, V125, P237, DOI 10.1016/j.sigpro.2016.01.019
   MacQuarrie A, 2017, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VR.2017.7892230
   Manasa K, 2016, IEEE T IMAGE PROCESS, V25, P2480, DOI 10.1109/TIP.2016.2548247
   Mulyadi, 2018, 2018 6TH INTERNATIONAL CONFERENCE ON CYBER AND IT SERVICE MANAGEMENT (CITSM), P37
   Nasrabadi AT, 2017, P IEEE VIRT REAL ANN, P347, DOI 10.1109/VR.2017.7892319
   Ng KT, 2005, IEEE T CIRC SYST VID, V15, P82, DOI 10.1109/TCSVT.2004.839989
   Pei SC, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440172
   Rahaman DMM, 2018, IEEE T IMAGE PROCESS, V27, P1190, DOI 10.1109/TIP.2017.2772858
   Schölkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565
   Singla A, 2017, INT WORK QUAL MULTIM
   Snyder J. P., 1989, An album of map projections (U.S. Geological Survey Professional Paper No. 1453)
   Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072
   Sun YL, 2017, IEEE SIGNAL PROC LET, V24, P1408, DOI 10.1109/LSP.2017.2720693
   Tsakalides, 2018, 9 VIS INF PROC COMM
   Turner CJ, 2016, IEEE T HUM-MACH SYST, V46, P882, DOI 10.1109/THMS.2016.2596099
   Ullah A, 2018, IEEE ACCESS, V6, P1155, DOI 10.1109/ACCESS.2017.2778011
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang SZ, 2016, IEEE T MED IMAGING, V35, P1046, DOI 10.1109/TMI.2015.2506902
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weissig C, 2012, LECT NOTES COMPUT SC, V7131, P671
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Xu M, 2019, IEEE T CIRC SYST VID, V29, P3516, DOI [10.1109/TCSVT.2018.2886277, 10.1080/17445302.2018.1558727]
   Yang JC, 2018, IEEE ACCESS, V6, P38669, DOI 10.1109/ACCESS.2018.2854922
   Yang JC, 2018, IEEE T BROADCAST, V64, P341, DOI 10.1109/TBC.2018.2789583
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Zakharchenko V, 2016, PROC SPIE, V9970, DOI 10.1117/12.2235885
   Zare A., 2016, P 24 ACM INT C MULT, P601, DOI DOI 10.1145/2964284.2967292
   Zhang W, 2016, PATTERN RECOGN, V59, P176, DOI 10.1016/j.patcog.2016.01.034
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P1275, DOI 10.1109/TIP.2017.2651410
   Zhang YX, 2018, IEEE T BROADCAST, V64, P461, DOI 10.1109/TBC.2018.2811627
   Zhu KF, 2015, IEEE T CIRC SYST VID, V25, P533, DOI 10.1109/TCSVT.2014.2363737
   Zhu Z, 2018, IEEE T IMAGE PROCESS, V27, P2952, DOI 10.1109/TIP.2018.2808766
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 53
TC 18
Z9 22
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 797
EP 809
DI 10.1109/TMM.2020.2990075
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200019
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhang, J
   Mei, KZ
   Zheng, Y
   Fan, JP
AF Zhang, Ji
   Mei, Kuizhi
   Zheng, Yu
   Fan, Jianping
TI Integrating Part of Speech Guidance for Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Predictive models; Semantics; Feature extraction; Task
   analysis; Computer vision; Speech processing; Part of speech; image
   captioning; multi-task learning
ID RECOGNITION; ATTENTION; LANGUAGE
AB To generate an image caption, firstly, the content of the image should be fully understood; and then the semantic information contained in the image should be described using a phrase or statement that conforms to certain grammatical rules. Thus, it requires techniques from both computer vision and natural language processing to connect the two different media forms together, which is highly challenging. To adaptively adjust the effect of visual information and language information on the captioning process, in this paper, the part of speech information is proposed to novelly integrate with image captioning models based on the encoder-decoder framework. First, a part of speech prediction network is proposed to analyze and model the part of speech sequences for the words in natural language sentences; then, different mechanisms are proposed to integrate the part of speech guidance information with merge-based and inject-based image captioning models, respectively; finally, according to the integrated frameworks, a multi-task learning paradigm is proposed to facilitate model training. Experiments are conducted on two widely used image captioning datasets, Flickr30 k and COCO, and the results have validated that the image captions generated by the proposed method contain more accurate visual information and comply with language habits and grammar rules better.
C1 [Zhang, Ji; Mei, Kuizhi] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710054, Shaanxi, Peoples R China.
   [Zheng, Yu] Xidian Univ, Sch Cyber Engn, Xian 710071, Shaanxi, Peoples R China.
   [Fan, Jianping] Univ N Carolina, Charlotte, NC 28262 USA.
C3 Xi'an Jiaotong University; Xidian University; University of North
   Carolina; University of North Carolina Charlotte
RP Mei, KZ (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710054, Shaanxi, Peoples R China.
EM zhang_ji@stu.xjtu.edu.cn; meikuizhi@mail.xjtu.edu.cn;
   yuzheng.xidian@gmail.com; jfan@uncc.edu
RI Zheng, Yu/GRJ-5808-2022; Mei, Kuizhi/B-2284-2015
OI Mei, Kuizhi/0000-0002-8119-3726
FU National Key Research and Development Plan [2016YFB1001004]; Guangdong
   Science and Technology Project [2017B010123003]; National Natural
   Science Foundation of China [61772161, 61906143]
FX This work was supported in part by the National Key Research and
   Development Plan under Grant 2016YFB1001004, in part by Guangdong
   Science and Technology Project 2017B010123003, and in part by the
   National Natural Science Foundation of China under Grants 61772161 and
   61906143.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2013, EMNLP
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.90
   Ba J., 2014, ARXIV PREPRINT ARXIV
   Bahdanau Dzmitry, 2014, NEURAL MACHINE TRANS
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Choi Y, 2011, P 15 C COMPUTATIONAL, P220
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deshpande A, 2019, PROC CVPR IEEE, P10687, DOI 10.1109/CVPR.2019.01095
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   He XW, 2019, PATTERN RECOGN LETT, V119, P229, DOI 10.1016/j.patrec.2017.10.018
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li XX, 2017, INT C COMP SUPP COOP, P378, DOI 10.1109/CSCWD.2017.8066724
   Liu F, 2011, MECHANIKA, P449
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Maji S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3177, DOI 10.1109/CVPR.2011.5995631
   Mao Junhua, 2014, CoRR
   Mao Junhua., 2014, Explain images with multimodal recurrent neural networks
   Mason R, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P592
   Medress M. F., 1977, Artificial Intelligence, V9, P307, DOI 10.1016/0004-3702(77)90026-1
   Mitchell M, 2012, P 13 C EUR CHAPT ASS, P747
   Mnih V., 2014, Neural Information Processing Systems, P2204
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   OW PS, 1988, INT J PROD RES, V26, P35, DOI 10.1080/00207548808947840
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Tanti M, 2018, NAT LANG ENG, V24, P467, DOI 10.1017/S1351324918000098
   Ushiku Y, 2015, IEEE I CONF COMP VIS, P2668, DOI 10.1109/ICCV.2015.306
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Xiao XY, 2019, IEEE T MULTIMEDIA, V21, P2942, DOI 10.1109/TMM.2019.2915033
   Xu X, 2015, IEEE ICC, P2048, DOI 10.1109/ICC.2015.7248627
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang Z, 2016, INT CONF ACOUST SPEE, P3236, DOI 10.1109/ICASSP.2016.7472275
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhou BL, 2014, ADV NEUR IN, V27
NR 61
TC 29
Z9 29
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 92
EP 104
DI 10.1109/TMM.2020.2976552
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600008
DA 2024-07-18
ER

PT J
AU Zhu, YH
   Min, WQ
   Jiang, SQ
AF Zhu, Yaohui
   Min, Weiqing
   Jiang, Shuqiang
TI Attribute-Guided Feature Learning for Few-Shot Image Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image recognition; Training; Task analysis; Semantics; Standards;
   Measurement; Visualization; Attribute learning; few-shot learning; image
   recognition
AB Few-shot image recognition has become an essential problem in the field of machine learning and image recognition, and has attracted more and more research attention. Typically, most few-shot image recognition methods are trained across tasks. However, these methods are apt to learn an embedding network for discriminative representations of training categories, and thus could not distinguish well for novel categories. To establish connections between training and novel categories, we use attribute-related representations for few-shot image recognition and propose an attribute-guided two-layer learning framework, which is capable of learning general feature representations. Specifically, few-shot image recognition trained over tasks and attribute learning trained over images share the same network in a multi-task learning framework. In this way, few-shot image recognition learns feature representations guided by attributes, and is thus less sensitive to novel categories compared with feature representations only using category supervision. Meanwhile, the multi-layer features associated with attributes are aligned with category learning on multiple levels respectively. Therefore we establish a two-layer learning mechanism guided by attributes to capture more discriminative representations, which are complementary compared with a single-layer learning mechanism. Experimental results on CUB-200, AWA and MiniImageNet datasets demonstrate our method effectively improves the performance.
C1 [Zhu, Yaohui; Min, Weiqing; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, CAS, Beijing 100190, Peoples R China.
   [Zhu, Yaohui; Min, Weiqing; Jiang, Shuqiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, CAS, Beijing 100190, Peoples R China.
EM yaohui.zhu@vipl.ict.ac.cn; minweiqing@ict.ac.cn; sqjiang@ict.ac.cn
OI Zhu, Yaohui/0009-0009-4841-1195; Zhu, Yaohui/0000-0003-4091-4782
FU National Natural Science Foundation of China [61532018, 61972378,
   U1936203, U19B2040]; Beijing Natural Science Foundation [L182054];
   National Program for Special Support of Eminent Professionals; National
   Program for Support of Top-Notch Young Professionals
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61532018, 61972378, U1936203,
   andU19B2040, in part by BeijingNatural Science Foundation under Grant
   L182054, in part by National Program for Special Support of Eminent
   Professionals andNational Program for Support of Top-NotchYoung
   Professionals.
CR [Anonymous], 2018, SEMANTIC FEATURE AUG
   [Anonymous], 2012, 20 ACM INT C MULT
   [Anonymous], 2011, CNSTR2010001 CAL I T
   Bertinetto L., 2016, Advances in neural information processing systems, P523
   Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429
   Chen Y, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE), P1467, DOI 10.1109/ICISCE.2017.306
   Chu WH, 2019, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR.2019.00641
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dixit M, 2017, PROC CVPR IEEE, P3328, DOI 10.1109/CVPR.2017.355
   Finn C, 2017, PR MACH LEARN RES, V70
   Grant E., 2018, 6 INT C LEARN REPR
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jamal MA, 2019, PROC CVPR IEEE, P11711, DOI 10.1109/CVPR.2019.01199
   Jayaraman D, 2014, PROC CVPR IEEE, P1629, DOI 10.1109/CVPR.2014.211
   Jiang HJ, 2017, IEEE I CONF COMP VIS, P4233, DOI 10.1109/ICCV.2017.453
   Jiang S., 2020, ACM T MULTIMEDIA COM
   Kingma D. P., 2014, arXiv
   Kovashka A, 2012, PROC CVPR IEEE, P2973, DOI 10.1109/CVPR.2012.6248026
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lee YK, 2018, I C INF COMM TECH CO, P1292, DOI 10.1109/ICTC.2018.8539493
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779
   Li Z., 2017, Meta-sgd: Learning to learn quickly for few-shot learning
   Liang KM, 2015, IEEE I CONF COMP VIS, P2506, DOI 10.1109/ICCV.2015.288
   Mishra N., 2018, INT C LEARN REPR
   Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220
   Munkhdalai T., 2018, INT C MACH LEARN, P3664
   Munkhdalai Tsendsuren, 2017, Proc Mach Learn Res, V70, P2554
   Naik D. K., 1992, IJCNN International Joint Conference on Neural Networks (Cat. No.92CH3114-6), P437, DOI 10.1109/IJCNN.1992.287172
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755
   Ravi S., 2016, INT C LEARNING REPRE
   Roy D, 2019, IEEE T MULTIMEDIA, V21, P1672, DOI 10.1109/TMM.2018.2887021
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Santoro A., 2016, ICML, P1842
   Satorras V.G., 2018, ICLR
   Schmidhuber J., 1987, Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta- ...  hook ...
   Snell J., 2017, ADV NEURAL INFORM PR, V30, P4077
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van der Spoel E, 2015, ICML DEEP LEARNING W, V7, P956, DOI 10.1017/CBO9781107415324
   Vinyals O., 2016, ADV NEURAL INFORM PR, P3630, DOI DOI 10.48550/ARXIV.1606.04080
   Wang YJ, 2010, PRODUCTION GRIDS IN ASIA: APPLICATIONS, DEVELOPMENTS AND GLOBAL TIES, P155, DOI 10.1007/978-1-4419-0046-3_13
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yang SF, 2015, IEEE I CONF COMP VIS, P1215, DOI 10.1109/ICCV.2015.144
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhou F., 2018, Technical report
   Zhou L, 2019, PROCEEDINGS OF THE 40TH ACM SIGPLAN CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION (PLDI '19), P1149, DOI 10.1145/3314221.3314584
NR 58
TC 43
Z9 47
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1200
EP 1209
DI 10.1109/TMM.2020.2993952
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200004
DA 2024-07-18
ER

PT J
AU Yu, J
   Zhang, WF
   Lu, YH
   Qin, ZC
   Hu, Y
   Tan, JL
   Wu, Q
AF Yu, Jing
   Zhang, Weifeng
   Lu, Yuhang
   Qin, Zengchang
   Hu, Yue
   Tan, Jianlong
   Wu, Qi
TI Reasoning on the Relation: Enhancing Visual Representation for Visual
   Question Answering and Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Cognition; Task analysis; Knowledge discovery; Semantics;
   Correlation; Information retrieval; Visual relational reasoning; visual
   attention; visual question answering; cross-modal information retrieval
AB Cross-modal analysis has become a promising direction for artificial intelligence. Visual representation is crucial for various cross-modal analysis tasks that require visual content understanding. Visual features which contain semantical information can disentangle the underlying correlation between different modalities, thus benefiting the downstream tasks. In this paper, we propose a Visual Reasoning and Attention Network (VRANet) as a plug-and-play module to capture rich visual semantics and help to enhance the visual representation for improving cross-modal analysis. Our proposed VRANet is built based on the bilinear visual attention module which identifies the critical objects. We propose a novel Visual Relational Reasoning (VRR) module to reason about pair-wise and inner-group visual relationships among objects guided by the textual information. The two modules enhance the visual features at both relation level and object level. We demonstrate the effectiveness of the proposed VRANet by applying it to both Visual Question Answering (VQA) and Cross-Modal Information Retrieval (CMIR) tasks. Extensive experiments conducted on VQA 2.0, CLEVR, CMPlaces, and MS-COCO datasets indicate superior performance comparing with state-of-the-art work.
C1 [Yu, Jing; Hu, Yue; Tan, Jianlong] Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China.
   [Yu, Jing; Hu, Yue; Tan, Jianlong] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100093, Peoples R China.
   [Zhang, Weifeng] Jiaxing Univ, Coll Math Phys & Informat Engn, Jiaxing 314000, Peoples R China.
   [Lu, Yuhang] Alibaba Grp, Hangzhou 310052, Peoples R China.
   [Qin, Zengchang] Beihang Univ, Sch ASEE, Intelligent Comp & Machine Learning Lab, Beijing 100191, Peoples R China.
   [Wu, Qi] Univ Adelaide, Australian Ctr Robot Vis, Adelaide, SA 5005, Australia.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Jiaxing University; Alibaba Group; Beihang University; University
   of Adelaide; Australian Centre for Robotic Vision
RP Zhang, WF (corresponding author), Jiaxing Univ, Coll Math Phys & Informat Engn, Jiaxing 314000, Peoples R China.
EM yujing02@iie.ac.cn; 1574940537@qq.com; yuhanglu@iie.ac.cn;
   zcqin@buaa.edu.cn; huyue@iie.ac.cn; jianglongtan@iie.ac.cn;
   qi.wu01@adelaide.edu.au
RI Wu, Qi/ABD-6304-2021; Hu, Yue/HGE-1673-2022
OI Wu, Qi/0000-0003-3631-256X; Yu, Jing/0000-0002-3966-511X; Zhang,
   Weifeng/0000-0002-0280-2336
FU National Key Research and Development Program [2017YFB0803301]
FX This work is supported by the National Key Research and Development
   Program under Grant 2017YFB0803301.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2009, Advances in neural information processing systems
   [Anonymous], 2018, IEEE T NEUR NET LEAR, DOI DOI 10.1109/TNNLS.2018.2817340
   [Anonymous], 2016, P INT C LEARN REPR
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2017, INT C LEARNING REPRE
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bahdanau D., 2014, 3 INT C LEARN REPR
   Bai S., 2018, EMPIRICAL EVALUATION
   Cao QX, 2018, PROC CVPR IEEE, P7249, DOI 10.1109/CVPR.2018.00757
   Castrejon L, 2016, PROC CVPR IEEE, P2940, DOI 10.1109/CVPR.2016.321
   Chaudhary C., 2018, IEEE T MULTIMEDIA, V20, P1372
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P2371, DOI 10.1109/TMM.2018.2796248
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Faghri F., 2018, P BRIT MACH VIS C
   Fan HQ, 2018, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2018.00118
   Fisher Y., 2014, P INT C LEARN REPR
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Huang Y, 2019, IEEE I CONF COMP VIS, P5773, DOI 10.1109/ICCV.2019.00587
   Huang Y, 2017, PROC CVPR IEEE, P7254, DOI 10.1109/CVPR.2017.767
   Husdon D., 2018, P INT C LEARN REPR
   Jin WK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1193, DOI 10.1145/3343031.3351065
   Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Jun SH, 2017, ICEC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ELECTRONIC COMMERCE, DOI 10.1145/3154943.3154947
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Ke X, 2019, IEEE T MULTIMEDIA, V21, P2093, DOI 10.1109/TMM.2019.2895511
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar BGV, 2016, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2016.581
   Lee K, 2018, 2018 INTERNATIONAL CONFERENCE ON ELECTRONICS, INFORMATION, AND COMMUNICATION (ICEIC), P214
   Lei J., 2019, ARXIV190411574
   Lei J, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1369
   Li S, 2017, IEEE I CONF COMP VIS, P1908, DOI 10.1109/ICCV.2017.209
   Liang JW, 2018, PROC CVPR IEEE, P6135, DOI 10.1109/CVPR.2018.00642
   Liang JW, 2019, IEEE T PATTERN ANAL, V41, P1893, DOI 10.1109/TPAMI.2018.2890628
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin X, 2016, LECT NOTES COMPUT SC, V9906, P261, DOI 10.1007/978-3-319-46475-6_17
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Ma Z, 2015, PR MACH LEARN RES, V37, P169
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Niu ZX, 2017, IEEE I CONF COMP VIS, P1899, DOI 10.1109/ICCV.2017.208
   Noh H, 2016, PROC CVPR IEEE, P30, DOI 10.1109/CVPR.2016.11
   Osman A, 2019, COMPUT VIS IMAGE UND, V185, P24, DOI 10.1016/j.cviu.2019.05.001
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Ren S., 2017, IEEETransactionsonPatternAnalysisandMachineIntelligence, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Santoro A., 2017, ADV NEURAL INFORM PR, V30, P4967
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang P, 2017, PROC CVPR IEEE, P3909, DOI 10.1109/CVPR.2017.416
   Wu C., 2019, ARXIV190510226
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yu J, 2018, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, 2018, VOL 2A
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhang WF, 2018, NEURAL PROCESS LETT, V48, P1503, DOI 10.1007/s11063-017-9753-9
   Zhang YX, 2018, AEBMR ADV ECON, V68, P707
   Zhang Z, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4383
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhao Z, 2019, IEEE T IMAGE PROCESS, V28, P3860, DOI 10.1109/TIP.2019.2902106
   Zhou B., 2015, ARXIV151202167V2
NR 68
TC 57
Z9 62
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3196
EP 3209
DI 10.1109/TMM.2020.2972830
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700014
DA 2024-07-18
ER

PT J
AU Yan, CG
   Shao, BY
   Zhao, H
   Ning, RX
   Zhang, YD
   Xu, F
AF Yan, Chenggang
   Shao, Biyao
   Zhao, Hao
   Ning, Ruixin
   Zhang, Yongdong
   Xu, Feng
TI 3D Room Layout Estimation From a Single RGB Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Layout; Three-dimensional displays; Estimation; Two dimensional
   displays; Image edge detection; Topology; Image reconstruction; Indoor
   scene; 2D topology; 3D topology; convolutional neural networks;
   nonlinear optimization
ID SHAPE; SCENES; MODEL
AB 3D layout is crucial for scene understanding and reconstruction, and very useful in applications like real estate and furniture design. In this paper, we propose a fully automatic solution to estimate 3D layout of an indoor scene from a single 2D image. Our technique contains two key components. Firstly, we train a neural network that directly estimates room structure lines from the input image. Secondly, we propose a novel technique to automatically identify the layout topology of an input image, followed by a nonlinear optimization with equality constraints to estimate the final 3Dlayout of a scene. Based on our knowledge, this is the first fully automatic technique to achieve single image-based 3D layout estimation of an indoor scene. We evaluate our method on the public datasets LSUN, Hedau and 3DGP and the results show that the proposed method achieves accurate 3D layout reconstruction on various images with different layout topologies.
C1 [Yan, Chenggang; Shao, Biyao; Ning, Ruixin] Hangzhou Dianzi Univ, Dept Automat, Hangzhou 310018, Peoples R China.
   [Yan, Chenggang] Shandong Univ, Sch Mech Elect & Informat Engn, Weihai 264209, Peoples R China.
   [Zhao, Hao] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Anhui, Peoples R China.
   [Xu, Feng] Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
   [Xu, Feng] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
C3 Hangzhou Dianzi University; Shandong University; Tsinghua University;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Tsinghua University; Tsinghua University
RP Xu, F (corresponding author), Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.; Xu, F (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
EM cgyan@hdu.edu.cn; 161060024@hdu.edu.cn; hao.zhao@intel.com;
   ningruixin_work@163.com; zhyd73@ustc.edu.cn; feng-xu@tsinghua.edu.cn
FU Zhejiang Province Nature Science Foundation of China [LR17F030006,
   Q19F010030]; National Nature Science Foundation of China [61931008,
   61671196, 61701149, 61801157, 61971268, 61901145, 61901150, 61972123];
   National Natural Science Major Foundation of Research Instrumentation of
   PR China [61427808]; 111 Project [D17019]
FX This work is supported by Zhejiang Province Nature Science Foundation of
   China (LR17F030006, Q19F010030), National Nature Science Foundation of
   China (61931008, 61671196, 61701149, 61801157, 61971268, 61901145,
   61901150, 61972123), the National Natural Science Major Foundation of
   Research Instrumentation of PR China under Grants 61427808, 111 Project,
   No. D17019. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. G.-J. Qi
CR Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Choi WG, 2013, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2013.12
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Coughlan J. M., 1999, P IEEE INT C COMPUTE, DOI DOI 10.1109/ICCV.1999.790349
   Coughlan JM, 2003, NEURAL COMPUT, V15, P1063, DOI 10.1162/089976603765202668
   Criminisi A, 2000, INT J COMPUT VISION, V40, P123, DOI 10.1023/A:1026598000963
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dasgupta S, 2016, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2016.73
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Del Pero L, 2013, PROC CVPR IEEE, P153, DOI 10.1109/CVPR.2013.27
   Del Pero L, 2012, PROC CVPR IEEE, P2719, DOI 10.1109/CVPR.2012.6247994
   Delage E, 2007, SPRINGER TRAC ADV RO, V28, P305
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Favaro P, 2000, LECT NOTES COMPUT SC, V1842, P755
   Han F, 2003, FIRST IEEE INTERNATIONAL WORKSHOP ON HIGHER-LEVEL KNOWLEDGE IN 3D MODELING AND MOTION ANALYSIS, PROCEEDINGS, P12
   Hedau V, 2009, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2009.5459411
   Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y
   Huang C, 2016, IEEE T MULTIMEDIA, V18, P2372, DOI 10.1109/TMM.2016.2602060
   Huang JC, 2005, IEEE T MULTIMEDIA, V7, P538, DOI 10.1109/TMM.2005.843346
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   KOSAKA A, 1992, CVGIP-IMAG UNDERSTAN, V56, P271, DOI 10.1016/1049-9660(92)90045-5
   Lee CY, 2017, IEEE I CONF COMP VIS, P4875, DOI 10.1109/ICCV.2017.521
   Mallya A, 2015, IEEE I CONF COMP VIS, P936, DOI 10.1109/ICCV.2015.113
   Marder-Eppstein E., 2016, P ACM SIGGRAPH 2016, P25
   Qi GJ, 2016, PROC CVPR IEEE, P2267, DOI 10.1109/CVPR.2016.249
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Saito H, 2003, IEEE T MULTIMEDIA, V5, P303, DOI 10.1109/TMM.2003.813283
   Sankar A, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P415, DOI 10.1145/3126594.3126629
   Schindler G., 2004, IEEE COMPUT SOCCONF, V1, pI
   Schwing AG, 2012, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2012.6248006
   Shum HY, 1998, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.1998.698641
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Sturm P. F., 1999, BMVC99. Proceedings of the 10th British Machine Vision Conference, P265
   Tang JH, 2015, IEEE T MULTIMEDIA, V17, P1899, DOI 10.1109/TMM.2015.2476660
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655
   Wang HY, 2013, COMMUN ACM, V56, P92, DOI [10.1145/2436258.2436276, 10.1145/2436256.2436276]
   Xie HT, 2019, IEEE T MULTIMEDIA, V21, P1248, DOI 10.1109/TMM.2018.2872898
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xie HT, 2019, PATTERN RECOGN, V85, P109, DOI 10.1016/j.patcog.2018.07.031
   Yan CH, 2020, INT FORUM ALLERGY RH, V10, P806, DOI 10.1002/alr.22579
   Yan CG, 2019, IEEE T MULTIMEDIA, V21, P2675, DOI 10.1109/TMM.2019.2903448
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhang WD, 2017, IEEE T MULTIMEDIA, V19, P935, DOI 10.1109/TMM.2016.2642780
   Zhang Y., 2015, LARGE SCALE SCENE UN, V15
   Zhang YD, 2017, PROC CVPR IEEE, P5057, DOI 10.1109/CVPR.2017.537
   Zhang YD, 2014, LECT NOTES COMPUT SC, V8694, P668, DOI 10.1007/978-3-319-10599-4_43
   Zhao H, 2017, PROC CVPR IEEE, P870, DOI 10.1109/CVPR.2017.99
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou ZH, 2017, IEEE T MULTIMEDIA, V19, P2651, DOI 10.1109/TMM.2017.2703954
   Zou CH, 2018, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2018.00219
NR 54
TC 139
Z9 141
U1 9
U2 83
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 3014
EP 3024
DI 10.1109/TMM.2020.2967645
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900020
DA 2024-07-18
ER

PT J
AU Yan, Y
   Huang, Y
   Chen, S
   Shen, CH
   Wang, HZ
AF Yan, Yan
   Huang, Ying
   Chen, Si
   Shen, Chunhua
   Wang, Hanzi
TI Joint Deep Learning of Facial Expression Synthesis and Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gallium nitride; Face recognition; Databases; Generative adversarial
   networks; Deep learning; Training data; Generators; Facial expression
   recognition; facial expression synthesis; convolutional neural networks
   (CNNs); generative adversarial net (GAN)
ID NETWORKS; MANIFOLD; IMAGES
AB Recently, deep learning based facial expression recognition (FER) methods have attracted considerable attention and they usually require large-scale labelled training data. Nonetheless, the publicly available facial expression databases typically contain a small amount of labelled data. In this paper, to overcome the above issue, we propose a novel joint deep learning of facial expression synthesis and recognition method for effective FER. More specifically, the proposed method involves a two-stage learning procedure. Firstly, a facial expression synthesis generative adversarial network (FESGAN) is pre-trained to generate facial images with different facial expressions. To increase the diversity of the training images, FESGAN is elaborately designed to generate images with new identities from a prior distribution. Secondly, an expression recognition network is jointly learned with the pre-trained FESGAN in a unified framework. In particular, the classification loss computed from the recognition network is used to simultaneously optimize the performance of both the recognition network and the generator of FESGAN. Moreover, in order to alleviate the problem of data bias between the real images and the synthetic images, we propose an intra-class loss with a novel real data-guided back-propagation (RDBP) algorithm to reduce the intra-class variations of images from the same class, which can significantly improve the final performance. Extensive experimental results on public facial expression databases demonstrate the superiority of the proposed method compared with several state-of-the-art FER methods.
C1 [Yan, Yan; Huang, Ying; Wang, Hanzi] Xiamen Univ, Fujian Key Lab Sensing & Comp Smart City, Sch Informat, Xiamen 361005, Peoples R China.
   [Chen, Si] Xiamen Univ Technol, Sch Comp & Informat Engn, Xiamen 361024, Peoples R China.
   [Shen, Chunhua] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
C3 Xiamen University; Xiamen University of Technology; University of
   Adelaide
RP Yan, Y (corresponding author), Xiamen Univ, Fujian Key Lab Sensing & Comp Smart City, Sch Informat, Xiamen 361005, Peoples R China.
EM yanyan@xmu.edu.cn; ying_hwang@qq.com; chensi@xmut.edu.cn;
   chunhua.shen@adelaide.edu.au; hanzi.wang@xmu.edu.cn
RI wang, handong/HLH-5739-2023; wang, hao/HSE-7975-2023; Wang,
   Han/GPW-9809-2022
OI Chen, Si/0000-0002-5631-7942; Shen, Chunhua/0000-0002-8648-8718
FU National Key R&D Program of China [2017YFB1302400]; National Natural
   Science Foundation of China [61571379, U1605252, 61872307]; Natural
   Science Foundation of Fujian Province of China [2017J01127, 2018J01576]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFB1302400, in part by the National Natural Science
   Foundation of China under Grants 61571379, U1605252, and 61872307, and
   in part by the Natural Science Foundation of Fujian Province of China
   under Grants 2017J01127 and 2018J01576.
CR Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606
   [Anonymous], 2018, ABS180201822 CORR
   [Anonymous], 2017, ARXIV PREPRINT ARXIV
   [Anonymous], INT C MACH LEARN ATL
   [Anonymous], 2014, P 3 INT C LEARN REPR
   [Anonymous], 2010, PROC 3 INT WORKSHOP
   [Anonymous], 2016, ARXIV161109961
   [Anonymous], 2010, TECH REP
   [Anonymous], 2014, ARXIV14117923
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2007, P 24 INT C MACHINE L
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   Arjovsky M, 2017, arXiv preprint arXiv:1701.07875
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Bishop C, 2007, RECOGNITION PATTERN
   Blanz V, 2003, COMPUT GRAPH FORUM, V22, P641, DOI 10.1111/1467-8659.t01-1-00712
   Bozorgtabar B., 2019, ARXIV190508090
   Chang FJ, 2019, INT J COMPUT VISION, V127, P930, DOI 10.1007/s11263-019-01151-x
   Chen X, 2016, ADV NEUR IN, V29
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dailey MN, 2002, J COGNITIVE NEUROSCI, V14, P1158, DOI 10.1162/089892902760807177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding H, 2018, AAAI CONF ARTIF INTE, P6781
   Ding H, 2017, IEEE INT CONF AUTOMA, P118, DOI 10.1109/FG.2017.23
   Eleftheriadis S, 2015, IEEE T IMAGE PROCESS, V24, P189, DOI 10.1109/TIP.2014.2375634
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Haxby JV, 2000, TRENDS COGN SCI, V4, P223, DOI 10.1016/S1364-6613(00)01482-0
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XF, 2004, ADV NEUR IN, V16, P153
   Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593
   Hu Y, 2008, PROC CVPR IEEE, P85
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kan MN, 2012, LECT NOTES COMPUT SC, V7572, P808, DOI 10.1007/978-3-642-33718-5_58
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Kingma D. P., 2014, TRACK 2014 PROC 2 IN, P1
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li M., 2016, ARXIV161005586
   Li Shan, 2018, arXiv:1804.08348
   Liu MY, 2015, LECT NOTES COMPUT SC, V9006, P143, DOI 10.1007/978-3-319-16817-3_10
   Liu MY, 2015, NEUROCOMPUTING, V159, P126, DOI 10.1016/j.neucom.2015.02.011
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Liu P, 2014, PROC CVPR IEEE, P1805, DOI 10.1109/CVPR.2014.233
   Liu P, 2014, LECT NOTES COMPUT SC, V8692, P151, DOI 10.1007/978-3-319-10593-2_11
   Liu ZC, 2001, COMP GRAPH, P271
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Martinez, 2016, ADV FACE DETECTION F, P63, DOI [DOI 10.1007/978-3-319-25958-1_4, 10.1007/978-3-319-25958-1_4]
   Meng ZB, 2017, IEEE INT CONF AUTOMA, P558, DOI 10.1109/FG.2017.140
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Radford A., 2015, ARXIV151106434
   Ranzato M, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995710
   Reed S, 2014, PR MACH LEARN RES, V32, P1431
   Ren S., 2017, IEEETransactionsonPatternAnalysisandMachineIntelligence, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Song LX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P627, DOI 10.1145/3240508.3240612
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Susskind J., 2008, AFFECT COMPUT
   Tawari A, 2013, IEEE T MULTIMEDIA, V15, P1543, DOI 10.1109/TMM.2013.2266635
   Testa RL, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3292652
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Ulyanov Dmitry, 2016, arXiv
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Xie WC, 2017, IEEE T MULTIMEDIA, V19, P279, DOI 10.1109/TMM.2016.2614429
   Xu ZS, 2017, 2017 2ND IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND APPLICATIONS (ICCIA), P214, DOI 10.1109/CIAPP.2017.8167210
   Yang F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964955
   Yang HY, 2018, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2018.00231
   Zhang KH, 2017, IEEE T IMAGE PROCESS, V26, P4193, DOI 10.1109/TIP.2017.2689999
   Zhang QS, 2006, IEEE T VIS COMPUT GR, V12, P48, DOI 10.1109/TVCG.2006.9
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhao XY, 2016, LECT NOTES COMPUT SC, V9906, P425, DOI 10.1007/978-3-319-46475-6_27
   Zhong GQ, 2010, AAAI CONF ARTIF INTE, P679
   Zhong L, 2012, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR.2012.6247974
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
NR 86
TC 23
Z9 25
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2792
EP 2807
DI 10.1109/TMM.2019.2962317
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, YP
   Kong, DH
   Wang, SF
   Li, JH
   Yin, BC
AF Wu, Yongpeng
   Kong, Dehui
   Wang, Shaofan
   Li, Jinghua
   Yin, Baocai
TI An Unsupervised Real-Time Framework of Human Pose Tracking From Range
   Image Sequences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pose tracking; visible hybrid model; componentwise correspondence
   optimization; dynamic database lookup
AB Pose tracking from range image sequences remains a difficult task due to strong noise and serious self-occlusion of human body. Existing work either rely on extremely large and precisely annotated datasets, or rely on accurate human mesh model and GPU acceleration. In this paper, we propose an unsupervised real-time framework of pose tracking from range image sequences. Our framework consists of a visible hybrid model (VHM), a componentwise correspondence optimization (CCO) and a dynamic database lookup (DDL). VHM consists of component sphere sets and component visible spherical point sets which exhibits both simplicity and high accuracy. CCO converts the matching between VHM and input point cloud into several subproblems regarding local rotations of components and a global translation of body abdominal joint, each of which has an efficient closed form solution. DDL is designed to recover correct pose when tracking fails, which effectively mitigates accumulative error during tracking. Experiments on SMMC, PDT, EVAL datasets indicate that our framework not only achieves better or competitive precision compared with state-of-the-art methods, but also produces real-time efficiency in personal computers without GPU acceleration.
C1 [Wu, Yongpeng; Kong, Dehui; Wang, Shaofan; Li, Jinghua] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing Artificial Intelligence Inst,Fac Informat, Beijing 100124, Peoples R China.
   [Yin, Baocai] Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
C3 Beijing University of Technology; Dalian University of Technology
RP Wang, SF (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing Artificial Intelligence Inst,Fac Informat, Beijing 100124, Peoples R China.
EM b201861012@emails.bjut.edu.cn; kdh@bjut.edu.cn; wangshaofan@bjut.edu.cn;
   lijinghua@bjut.edu.cn; ybc@dlut.edu.cn
OI WANG, SHAOFAN/0000-0002-3045-624X
FU National Natural Science Foundation of China [61772049, 61632006,
   61876012, U1811463]; Beijing Natural Science Foundation [4172003,
   4202003]; Beijing Educational Committee [KM201710005022]
FX This work is supported in part by National Natural Science Foundation of
   China under Grants 61772049, 61632006, 61876012, and U1811463, in part
   by Beijing Natural Science Foundation under Grant 4172003, and 4202003,
   and in part by Beijing Educational Committee under Grant KM201710005022.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Cees G. M. Snoek.
CR Martinez AA, 2017, INT SYMP COMPUT EDUC
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2013, P BRIT MACH VIS C, DOI DOI 10.5244/C.27.4
   Baak A, 2011, IEEE I CONF COMP VIS, P1092, DOI 10.1109/ICCV.2011.6126356
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Bregler C, 2004, INT J COMPUT VISION, V56, P179, DOI 10.1023/B:VISI.0000011203.00237.9b
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610
   Ding M, 2015, CHINESE WAY, P57
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Ganapathi V, 2012, LECT NOTES COMPUT SC, V7577, P738, DOI 10.1007/978-3-642-33783-3_53
   Ganapathi V, 2010, PROC CVPR IEEE, P755, DOI 10.1109/CVPR.2010.5540141
   González A, 2010, MATH GEOSCI, V42, P49, DOI 10.1007/s11004-009-9257-x
   Helten T, 2013, IEEE I CONF COMP VIS, P1105, DOI 10.1109/ICCV.2013.141
   Helten T, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P279, DOI 10.1109/3DV.2013.44
   Jung HY, 2015, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2015.7298861
   Kadu H, 2014, IEEE T MULTIMEDIA, V16, P2191, DOI 10.1109/TMM.2014.2360793
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Li M, 2016, IEEE T MULTIMEDIA, V18, P2293, DOI 10.1109/TMM.2016.2614228
   Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23
   Marcos-Ramiro A, 2015, IEEE T MULTIMEDIA, V17, P1721, DOI 10.1109/TMM.2015.2464152
   Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170
   Nie BX, 2017, IEEE I CONF COMP VIS, P3467, DOI 10.1109/ICCV.2017.373
   Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138
   Plagemann C, 2010, IEEE INT CONF ROBOT, P3108, DOI 10.1109/ROBOT.2010.5509559
   Pons-Moll G, 2015, INT J COMPUT VISION, V113, P163, DOI 10.1007/s11263-015-0818-9
   Qian C, 2014, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.2014.145
   Ruijiang L., 2010, Experimental Technology and Management, V27, P52, DOI 10.16791/j.cnki.sjg.2010.09.016
   Schmidt T, 2015, AUTON ROBOT, V39, P239, DOI 10.1007/s10514-015-9462-z
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Swinbank R, 2006, Q J ROY METEOR SOC, V132, P1769, DOI 10.1256/qj.05.227
   Taylor J, 2012, PROC CVPR IEEE, P103, DOI 10.1109/CVPR.2012.6247664
   Tekin Bugra, 2016, P BRIT MACH VIS C 20
   Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603
   Vasileiadis M, 2017, IEEE INT CONF COMP V, P1363, DOI 10.1109/ICCVW.2017.162
   Wei XL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366207
   Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22
   Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Ye M, 2016, IEEE T PATTERN ANAL, V38, P1517, DOI 10.1109/TPAMI.2016.2557783
   Ye M, 2011, IEEE I CONF COMP VIS, P731, DOI 10.1109/ICCV.2011.6126310
   Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537
   Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51
NR 43
TC 6
Z9 7
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2177
EP 2190
DI 10.1109/TMM.2019.2953380
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500021
DA 2024-07-18
ER

PT J
AU Kostic, Z
   Jevremovic, A
AF Kostic, Zona
   Jevremovic, Aleksandar
TI What Image Features Boost Housing Market Predictions?
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Entropy; Predictive models;
   Satellites; Metadata; Image segmentation; Deep image features; image
   segmentation; entropy; CNN; real-estate; price predictions; DOM;
   boosting models
AB The attractiveness of a property is one of the most interesting, yet challenging, categories to model. Image characteristics are used to describe certain attributes, and to examine the influence of visual factors on the price or timeframe of the listing. In this paper, we propose a set of techniques for the extraction of visual features for efficient numerical inclusion in modern-day predictive algorithms. We discuss techniques such as Shannon's entropy, calculating the center of gravity, employing image segmentation, and using Convolutional Neural Networks. After comparing these techniques as applied to a set of property-related images (indoor, outdoor, and satellite), we conclude the following: (i) the entropy is the most efficient single-digit visual measure for housing price prediction; (ii) image segmentation is the most important visual feature for the prediction of housing lifespan; and (iii) deep image features can be used to quantify interior characteristics and contribute to captivation modeling. The set of 40 image features selected here carries a significant amount of predictive power and outperforms some of the strongest metadata predictors. Without any need to replace a human expert in a real-estate appraisal process, we conclude that the techniques presented in this paper can efficiently describe visible characteristics, thus introducing perceived attractiveness as a quantitative measure into the predictive modeling of housing.
C1 [Kostic, Zona] Harvard Univ, Ilarvard John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Jevremovic, Aleksandar] Singidunum Univ, Fac Informat & Comp, Belgrade 160622, Serbia.
C3 Harvard University
RP Kostic, Z (corresponding author), Harvard Univ, Ilarvard John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
EM zonakostic@seas.harvard.edu; ajevremovic@gmail.com
RI Jevremovic, Aleksandar/AAZ-5331-2021
OI Kostic, Zona/0000-0003-1757-6260; Jevremovic,
   Aleksandar/0000-0002-5564-8344
CR [Anonymous], ANN REPORT MLS PIN H
   [Anonymous], 2011, Public transit's impact on housing costs: a review of the literature
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], HEDONIC MODELING REA
   [Anonymous], ARXIV150602371
   [Anonymous], INT J COMPUT INF ENG
   [Anonymous], 2017, Workshop on ML Systems at NIPS
   [Anonymous], 2018, ARXIV180707155
   [Anonymous], 2016, Machine learning for causal inference: An application to air quality impacts on house prices
   [Anonymous], 2017, ARXIV170701700
   [Anonymous], 2018, ARXIV180311227
   [Anonymous], 2012, ADV K MEANS CLUSTERI, DOI DOI 10.1007/978-3-642-29807-3
   [Anonymous], 2018, INT J COMPUTER INFOR
   [Anonymous], 2017, ARXIV170705489
   [Anonymous], 2016, P 22 ACM SIGKDD INT
   [Anonymous], 1978, MILLENNIUM FILM J, DOI DOI 10.1177/03058298780070030601
   Arietta SM, 2014, IEEE T VIS COMPUT GR, V20, P2624, DOI 10.1109/TVCG.2014.2346446
   ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663
   Bency AJ, 2017, IEEE WINT CONF APPL, P320, DOI 10.1109/WACV.2017.42
   Benefield JD, 2011, J REAL ESTATE FINANC, V43, P401, DOI 10.1007/s11146-009-9219-6
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dubin R., 1999, J REAL ESTATE LIT, V7, P79, DOI [DOI 10.1023/A:1008690521599, DOI 10.1080/10835547.1999.12090079]
   Guyon Isabelle, 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616
   He B, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7060203
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Jolliffe IT, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0202
   Jung SY, 2005, IEEE T KNOWL DATA EN, V17, P834, DOI 10.1109/TKDE.2005.86
   Ke GL, 2017, ADV NEUR IN, V30
   Khosla A, 2014, PROC CVPR IEEE, P3710, DOI 10.1109/CVPR.2014.474
   Nascimento DSC, 2009, LECT NOTES COMPUT SC, V5863, P512, DOI 10.1007/978-3-642-10677-4_58
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Shin Y, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/149702
   Tsai DY, 2008, J DIGIT IMAGING, V21, P338, DOI 10.1007/s10278-007-9044-5
   Wang CZ, 2018, IOP CONF SER-MAT SCI, V322, DOI 10.1088/1757-899X/322/5/052053
   You QZ, 2017, IEEE T MULTIMEDIA, V19, P2751, DOI 10.1109/TMM.2017.2710804
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 38
TC 13
Z9 14
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1904
EP 1916
DI 10.1109/TMM.2020.2966890
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hu, WP
   Hu, HF
AF Hu, Weipeng
   Hu, Haifeng
TI Disentangled Spectrum Variations Networks for NIR-VIS Face Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE NIR-VIS face recognition; deep learning; adversarial training;
   disentangled spectrum variations; orthogonality constraint
ID REGRESSION
AB Surveillance cameras often capture near infrared images since it provides a low-cost and effective solution to acquire high-quality images under low-light environments. However, visual versus near infrared (VIS-NIR) heterogeneous face recognition (HFR) is still a challenging issue in computer vision community due to the gap between sensing patterns of different spectrums as well as the lack of sufficient training samples. To solve the above problem, in this paper, we present an effective Disentangled Spectrum Variations Networks (DSVNs) for VIS-NIR HFR. Two key strategies are introduced to the DSVNs for disentangling spectrum variations between two domains: Spectrum-adversarial Discriminative Feature Learning (SaDFL) and Step-wise Spectrum Orthogonal Decomposition (SSOD). The SaDFL consists of Identity-Discriminative subnetwork (IDNet) and Auxiliary Spectrum Adversarial subnetwork (ASANet). On the one hand, the IDNet is composed of a generator $G_H$ and a discriminator $D_U$ for extracting identity-discriminative feature. On the other hand, the ASANet is built by a generator $G_H$ and a discriminator $D_M$ for eliminating modality-variant spectrum information under the guidance of the discriminator $D_M$. The identity-label and modality-label HFR datasets are used to train the DSVNs with triplet loss. Both IDNet and ASANet can jointly enhance the domain-invariant feature representations via an adversarial learning. Furthermore, to disentangle spectrum variations effectively as well as making identity information and modality information unrelated to each other, we present a new topology of connection block called Disentangled Spectrum Variations (DSV). An orthogonality constraint is imposed to DSV at the convolution level for channel-wise orthogonal decomposition between the modality-invariant identity information and modality-variant spectrum information. In particular, the SSOD is built by stacking multiple modularized mirco-block DSV, and thereby enjoys the benefits of disentangling spectrum variation step by step. Moreover, we investigate the similarity calculation method to further improve the HFR performance. To sum up, the designed DSVNs leads to a purification of identity information as well as an elimination of modality information. Extensive experiments are carried out on two challenging NIR-VIS HFR datasets CASIA NIR-VIS 2.0 and Oulu-CASIA NIR-VIS, demonstrating the superiority of the proposed method.
C1 [Hu, Weipeng; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510275, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510275, Peoples R China.
EM huwp5@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn
RI Weipeng, Hu/AAS-1819-2020
OI Weipeng, Hu/0000-0003-2886-7346; Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; Science and Technology Program of Guangzhou [201704020180];
   Natural Science Foundation of Guangdong [2017A030311029]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61673402, Grant 61273270, and Grant
   60802069, in part by theNatural Science Foundation of Guangdong under
   Grant 2017A030311029, and in part by the Science and Technology Program
   of Guangzhou under Grant 201704020180. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Elisa Ricci.
CR Alex AT, 2013, IEEE SYS MAN CYBERN, P1211, DOI 10.1109/SMC.2013.210
   [Anonymous], 2014, ARXIV14117923
   Boyd S., 2009, Convex Optimization, P215
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41
   Chen J, 2009, PROC CVPR IEEE, P156, DOI 10.1109/CVPRW.2009.5206832
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Gong DH, 2017, IEEE T IMAGE PROCESS, V26, P2079, DOI 10.1109/TIP.2017.2651380
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   He R, 2019, IEEE T PATTERN ANAL, V41, P1761, DOI 10.1109/TPAMI.2018.2842770
   He R, 2017, AAAI CONF ARTIF INTE, P2000
   Huang DA, 2013, IEEE I CONF COMP VIS, P2496, DOI 10.1109/ICCV.2013.310
   Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129
   Huang XS, 2013, IEEE T IMAGE PROCESS, V22, P353, DOI 10.1109/TIP.2012.2215617
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jin Y, 2015, IEEE T INF FOREN SEC, V10, P640, DOI 10.1109/TIFS.2015.2390414
   Juefei-Xu Felix, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P141, DOI 10.1109/CVPRW.2015.7301308
   Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740
   Klare B., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1513, DOI 10.1109/ICPR.2010.374
   Klare BF, 2013, IEEE T PATTERN ANAL, V35, P1410, DOI 10.1109/TPAMI.2012.229
   Lei Z., 2008, PROC IEEE C COMPUT V, P1
   Lei Z, 2009, PROC CVPR IEEE, P1123, DOI 10.1109/CVPRW.2009.5206860
   Lezama J, 2017, PROC CVPR IEEE, P6807, DOI 10.1109/CVPR.2017.720
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Li SZ, 2013, IEEE COMPUT SOC CONF, P348, DOI 10.1109/CVPRW.2013.59
   Liu XM, 2016, AER ADV ENG RES, V116, P1, DOI 10.1145/2875194.2875248
   Liu X, 2017, FRONT COMPUT SCI-CHI, V11, P208, DOI 10.1007/s11704-016-6076-3
   Lu JW, 2018, IEEE T PATTERN ANAL, V40, P1979, DOI 10.1109/TPAMI.2017.2737538
   Ouyang SX, 2016, IMAGE VISION COMPUT, V56, P28, DOI 10.1016/j.imavis.2016.09.001
   Peng CL, 2017, IEEE T PATTERN ANAL, V39, P301, DOI 10.1109/TPAMI.2016.2542816
   Reale C, 2016, IEEE COMPUT SOC CONF, P320, DOI 10.1109/CVPRW.2016.47
   Saxena S, 2016, LECT NOTES COMPUT SC, V9915, P483, DOI 10.1007/978-3-319-49409-8_40
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shao M, 2017, IEEE T NEUR NET LEAR, V28, P451, DOI 10.1109/TNNLS.2016.2517014
   Sharma A., 2013, P IEEE C COMP VIS PA, P593
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Song LX, 2018, AAAI CONF ARTIF INTE, P7355
   Song YB, 2014, LECT NOTES COMPUT SC, V8694, P800, DOI 10.1007/978-3-319-10599-4_51
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Wang R, 2009, LECT NOTES COMPUT SC, V5558, P319, DOI 10.1007/978-3-642-01793-3_33
   Wang W, 2015, IEEE INFOCOM SER
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu X, 2018, AAAI CONF ARTIF INTE, P1679
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Yi DW, 2015, INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ARTIFICIAL INTELLIGENCE (ICCSAI 2014), P1
   Zhang W, 2011, PROC CVPR IEEE, P513, DOI 10.1109/CVPR.2011.5995324
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhu JY, 2014, IEEE T INF FOREN SEC, V9, P501, DOI 10.1109/TIFS.2014.2299977
NR 51
TC 18
Z9 20
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1234
EP 1248
DI 10.1109/TMM.2019.2938685
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200010
DA 2024-07-18
ER

PT J
AU Liu, X
   Dohler, M
   Deng, YS
AF Liu, Xun
   Dohler, Mischa
   Deng, Yansha
TI Vibrotactile Quality Assessment: Hybrid Metric Design Based on SNR and
   SSIM
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Signal to noise ratio; Haptic interfaces; Quality
   assessment; Protocols; Vibrations; Codecs; Mulsemedia; haptics;
   vibrotactile; quality of experience; subjective tests; objective metrics
ID IMAGE; EXPERIENCE; PERCEPTION; REDUCTION
AB The emerging mulsemedia (MULtiple SEnsorial MEDIA) introduces new sensorial data (haptic, olfaction, gustation, etc.), significantly augmenting the conventional audio-visual communication. This can be used in many areas, such as immersive entertainment and innovative education. Previous research has been dedicated to evaluating the impact of other sensorial data on conventional multimedia; however, standalone quality evaluation of new sensorial data, especially vibrotactile data (a type of haptic data), has not been covered. To the best of our knowledge, this paper is the first to empirically demonstrate that the common statistical metrics in audio and visual domains, i.e. signal-to-noise ratio (SNR) and Structural SIMilarity (SSIM), are highly correlated with human vibrotactile perception as well. To be specific, we propose a testing protocol for vibrotactile quality evaluation and conduct subjective experiments. The results suggest that SNR and SSIM are applicable to vibrotactile quality assessment. We also consider a practical scenario where the quality of vibrotactile data varies with time. Based on the validation of SNR and SSIM in the first part, we present an objective metric as a hybrid composition of SNR and SSIM. Instead of assessing the quality of data using an overall score, the hybrid metric evaluates the quality in a time-varying manner. Subjective experiments are conducted and the results demonstrate that the correlation coefficient can be significantly increased using the hybrid metric.
C1 [Liu, Xun; Dohler, Mischa; Deng, Yansha] Kings Coll London, Dept Informat, London WC2B 4BG, England.
C3 University of London; King's College London
RP Liu, X (corresponding author), Kings Coll London, Dept Informat, London WC2B 4BG, England.
EM xun.2.liu@kcl.ac.uk; mischa.dohler@kcl.ac.uk; yansha.deng@kcl.ac.uk
RI Deng, Yansha/ABD-2830-2020; Dohler, Mischa/G-8670-2012
OI Deng, Yansha/0000-0003-1001-7036; Dohler, Mischa/0000-0001-9583-2923
CR Adler H.E, 1966, Elemente der Psychophysik Elements of psychophysics, V1
   [Anonymous], 2017, 2017 IEEE INT S BROA, DOI DOI 10.1109/BMSB.2017.7986129
   [Anonymous], 2012, PRINCIPLES NEURAL SC
   BAK P, 1987, PHYS REV LETT, V59, P381, DOI 10.1103/PhysRevLett.59.381
   BELL J, 1994, PROG NEUROBIOL, V42, P79, DOI 10.1016/0301-0082(94)90022-1
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chaudhari R., 2011, 2011 IEEE World Haptics Conference (WHC 2011), P539, DOI 10.1109/WHC.2011.5945543
   Chaudhari R., 2012, P ACM MULT NAR JAP O, P409, DOI [10.1145/2393347.2393407, DOI 10.1145/2393347.2393407]
   Chaudhariu R, 2015, IEEE J-STSP, V9, P462, DOI 10.1109/JSTSP.2014.2374574
   Hasserjian RP, 2019, PATHOBIOLOGY, V86, P7, DOI 10.1159/000489702
   Hinterseer P, 2008, IEEE T SIGNAL PROCES, V56, P588, DOI 10.1109/TSP.2007.906746
   ITU-R, 2001, METH OBJ MEAS PERC A
   ITU-R, 2014, METH SUBJ ASS INT QU
   ITU-R, 1994, METH SUBJ ASS SMALL
   ITU-T, 1996, METH OBJ SUBJ ASS QU
   Kandadai S, 2008, INT CONF ACOUST SPEE, P221, DOI 10.1109/ICASSP.2008.4517586
   Kirsch J., 2018, 2018 IEEE INT S HAPT, P1
   Klatzky Roberta., 2008, Haptic Rendering: Foundations, Algorithms, and Applications, P7
   Kretschmer M, 2010, IEEE GLOBE WORK, P27, DOI 10.1109/GLOCOMW.2010.5700326
   Kuchenbecker KJ, 2006, IEEE T VIS COMPUT GR, V12, P219, DOI 10.1109/TVCG.2006.32
   Landin N, 2010, LECT NOTES COMPUT SC, V6192, P79, DOI 10.1007/978-3-642-14075-4_12
   LAWRENCE DA, 1993, IEEE T ROBOTIC AUTOM, V9, P624, DOI 10.1109/70.258054
   Liu X., 2017, PHARM RES, P1, DOI DOI 10.1007/S11356-017-0289-3
   Maiero J, 2017, IEEE INT CON MULTI, P169, DOI 10.1109/ICME.2017.8019519
   McMahan William., 2010, PROC IEEE HAPTICS S, P141
   MERMELSTEIN P, 1979, J ACOUST SOC AM, V66, P1664, DOI 10.1121/1.383638
   Murray N, 2018, IEEE T BROADCAST, V64, P539, DOI 10.1109/TBC.2018.2825297
   Murray N, 2017, IEEE T SYST MAN CY-S, V47, P2503, DOI 10.1109/TSMC.2016.2531654
   Okamoto S., 2010, Proceedings of the 2010 IEEE/SICE International Symposium on System Integration (SII 2010), P384, DOI 10.1109/SII.2010.5708356
   Okamoto S, 2011, IEEE INT C INT ROBOT, P3060, DOI 10.1109/IROS.2011.6048298
   Okamoto S, 2009, IEEE T HAPTICS, V2, P73, DOI 10.1109/ToH.2009.17
   Okamura AM, 2001, IEEE-ASME T MECH, V6, P245, DOI 10.1109/3516.951362
   Rix AW, 2001, INT CONF ACOUST SPEE, P749, DOI 10.1109/ICASSP.2001.941023
   Sakr N., 2007, HAVE 2007 - IEEE International Workshop on Haptic Audio Visual Environments and their Applications, P27
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Waltl Markus, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P124, DOI 10.1109/QOMEX.2010.5517704
   Wang Z, 2005, PROC SPIE, V5666, P149, DOI 10.1117/12.597306
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Waters G, 1988, TECH REP
   Yang W, 1999, ENHANCED MODIFIED BA
   Yoshioka T, 2009, ADV ROBOTICS, V23, P747, DOI 10.1163/156855309X431703
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P957, DOI 10.1109/TMM.2015.2431915
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
   Zielinski S., 2007, Audio Engineering Society Convention 123
NR 46
TC 13
Z9 13
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 921
EP 933
DI 10.1109/TMM.2019.2936305
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, SF
   Hao, LF
   Ji, Q
AF Wang, Shangfei
   Hao, Longfei
   Ji, Qiang
TI Knowledge-Augmented Multimodal Deep Regression Bayesian Networks for
   Emotion Video Tagging
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Tagging; Bayes methods; Feature extraction; Grammar;
   Emotion recognition; Knowledge engineering; Regression Bayesian network;
   Multi-modal deep network; Domain knowledge; Emotion video tagging
AB The immanent dependencies between audio and visual modalities extracted from video content and the well-established film grammar (i.e., domain knowledge) are important for emotion video recognition and regression. However, these tools have yet to be exploited successfully. Therefore, we propose a multimodal deep regression Bayesian network (MMDRBN) to capture the relationship between audio and visual modalities for emotion video tagging. We then modify the structure of the MMDRBN to incorporate domain knowledge. A regression Bayesian network (RBN) is formed from one latent layer, one visible layer and directed links from the latent layer to the visible layer. RBN is able to fully represent the data, since it captures the dependencies not only among the visible variables but also among the latent variables given visible variables. For the MMDRBN, first, we learn several layers of RBNs using audio and visual modalities, and then stack these RBNs to form two deep networks. A joint representation is obtained from the top layers of the two deep networks, capturing the deep dependencies between audio and visual modalities. We also summarize the main audio and visual elements used by filmmakers to convey emotions and formulate them as semantical meaningful middle-level representation, i.e., attributes. Through these attributes, we construct the knowledge-augmented MMDRBN, which learns a hybrid middle-level video representation using video data and the summarized attributes. Experimental results of both emotion recognition and regression from videos on the LIRIS-ACCEDE database demonstrate that the proposed model can successfully capture the intrinsic connections between audio and visual modalities, and integrate the middle-level representation learning from video data and semantical attributes summarized from film grammar. Thus, it achieves superior performance on emotion video tagging compared to state-of-the-art methods.
C1 [Wang, Shangfei] Univ Sci & Technol China, Key Lab Comp & Commun Software Anhui Prov, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.
   [Wang, Shangfei] Univ Sci & Technol China, Sch Data Sci, Hefei 230027, Peoples R China.
   [Hao, Longfei] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.
   [Ji, Qiang] Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, Troy, NY 12180 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS; Rensselaer Polytechnic Institute
RP Wang, SF (corresponding author), Univ Sci & Technol China, Key Lab Comp & Commun Software Anhui Prov, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.; Wang, SF (corresponding author), Univ Sci & Technol China, Sch Data Sci, Hefei 230027, Peoples R China.
EM sfwang@ustc.edu.cn; hlf101@mai.ustc.edu.cn; qji@ecse.rpi.edu
OI Hao, Longfei/0000-0002-0333-4546; wang, shangfei/0000-0003-1164-9895
FU Anhui Science and Technology Agency [1804a09020038]; National Science
   Foundation of China [917418129, 61473270]
FX This work was supported in part by the project from Anhui Science and
   Technology Agency (1804a09020038) and in part by the National Science
   Foundation of China under Grants 917418129 and 61473270.
CR Acar Esra, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P303, DOI 10.1007/978-3-319-04114-8_26
   Akaho S., 2006, ARXIVCS0609071
   Andrew G., 2013, P ICML, P1247
   [Anonymous], 1985, HERBERT ROBBINS SELE, DOI DOI 10.1007/978-1-4612-5110-1_9
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], 2011, P ICML
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], 1958, An introduction to multivariate statistical analysis
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   Baveye Y, 2015, IEEE T AFFECT COMPUT, V6, P43, DOI 10.1109/TAFFC.2015.2396531
   Chen SY, 2016, INT C PATT RECOG, P295, DOI 10.1109/ICPR.2016.7899649
   Chen TF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P769, DOI 10.1145/3123266.3123352
   Dai Q., 2015, MEDIAEVAL 2015 WORKS
   Gan Q, 2017, AAAI CONF ARTIF INTE, P4039
   Gan Q, 2017, IEEE I CONF COMP VIS, P5123, DOI 10.1109/ICCV.2017.547
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Lam V., 2015, MEDIAEVAL 2015 WORKS
   Li Ming, 2016, ARXIV161001206
   Mironica I., 2015, MEDIAEVAL 2015 WORKS
   Mukhopadhyay D, 2015, I CONF VLSI DESIGN, P14, DOI 10.1109/VLSID.2015.115
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251
   Scherer KR, 2003, SPEECH COMMUN, V40, P227, DOI 10.1016/S0167-6393(02)00084-5
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Sun SL, 2013, NEURAL COMPUT APPL, V23, P2031, DOI 10.1007/s00521-013-1362-6
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Wang SF, 2015, IEEE T AFFECT COMPUT, V6, P410, DOI 10.1109/TAFFC.2015.2432791
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Watanapa SC, 2008, IEICE T INF SYST, VE91D, P1562, DOI 10.1093/ietisy/e91-d.5.1562
   Yi Y. W. Hanli., 2015, MEDIAEVAL 2015 WORKS
   Yuille A.L., 2005, ADV INFORM PROCESSIN, V17, P1593
   Zettl Herbert., 2013, Sight, sound, motion: Applied media aesthetics
NR 47
TC 15
Z9 15
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 1084
EP 1097
DI 10.1109/TMM.2019.2934824
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400021
DA 2024-07-18
ER

PT J
AU Fotiadou, K
   Tsagkatakis, G
   Tsakalides, P
AF Fotiadou, Konstantina
   Tsagkatakis, Grigorios
   Tsakalides, Panagiotis
TI Snapshot High Dynamic Range Imaging via Sparse Representations and
   Feature Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Imaging; Dynamic range; Image reconstruction; Computer science;
   Streaming media; Lighting; Optimization; High dynamic range imaging;
   deep learning; sparse stacked autoencoders; sparse representations
ID QUALITY ASSESSMENT; ENHANCEMENT; SUPERRESOLUTION; AUTOENCODERS;
   ALGORITHM; NETWORK; MODEL
AB Bracketed High Dynamic Range (HDR) imaging architectures acquire a sequence of Low Dynamic Range (LDR) images in order to either produce a HDR image or an "optimally" exposed LDR image, achieving impressive results under static camera and scene conditions. However, in real world conditions, ghost-like artifacts and noise effects limit the quality of HDR reconstruction. We address these limitations by introducing a post-acquisition snapshot HDR enhancement scheme that generates a bracketed sequence from a small set of LDR images, and in the extreme case, directly from a single exposure. We achieve this goal via a sparse-based approach where transformations between differently exposed images are encoded through a dictionary learning process, while we learn appropriate features by employing a stacked sparse autoencoder (SSAE) based framework. Via experiments with real images, we demonstrate the improved performance of our method over the state-of-the-art, while our single-shot based HDR formulation provides a novel paradigm for the enhancement of LDR imaging and video sequences.
C1 [Fotiadou, Konstantina; Tsakalides, Panagiotis] Univ Crete, Dept Comp Sci, Iraklion 74100, Greece.
   [Fotiadou, Konstantina; Tsagkatakis, Grigorios; Tsakalides, Panagiotis] Inst Comp Sci FORTH, GR-70013 Iraklion, Greece.
C3 University of Crete
RP Fotiadou, K (corresponding author), Univ Crete, Dept Comp Sci, Iraklion 74100, Greece.
EM kfot@ics.forth.gr; greg@ics.forth.gr; tsakalid@ics.forth.gr
RI Tsagkatakis, Grigorios/Z-1265-2019; Tsakalides, Panagiotis/O-2063-2015
OI Tsagkatakis, Grigorios/0000-0001-6498-9450; 
FU DEDALE project within the H2020 Framework Program of the European
   Commission [665044]
FX This work was partially funded by the DEDALE project, contract 665044,
   within the H2020 Framework Program of the European Commission. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Han Hu.
CR Aguerrebere C, 2014, IEEE INT CONF COMPUT
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], P IS T SPIE ELECT IM
   [Anonymous], HDMHDR 2014 PROJECT
   [Anonymous], 2013, K SPARSE AUTOENCODER
   [Anonymous], P 30 INT C MACH LEAR
   [Anonymous], ARXIV180304815
   [Anonymous], P ACM SIGGRAPH COURS
   [Anonymous], LDR DATASET
   [Anonymous], 2009, SIGNAL PROCESSING AD
   [Anonymous], 2015, P IEEE C COMP VIS PA
   [Anonymous], STANFORD DEEP LEARNI
   [Anonymous], P OPT PHOT INT SOC O
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P2343, DOI 10.1111/j.1467-8659.2009.01541.x
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Debevec P.E., 2008, P 24 ANN C COM GRAPH, P1
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong X, 2011, IEEE INT CON MULTI
   Dong YY, 2016, IEEE T MULTIMEDIA, V18, P549, DOI 10.1109/TMM.2016.2522639
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1
   François E, 2016, IEEE T CIRC SYST VID, V26, P63, DOI 10.1109/TCSVT.2015.2461911
   Fu HY, 2012, INT C PATT RECOG, P3656
   Grossberg MD, 2004, IEEE T PATTERN ANAL, V26, P1272, DOI 10.1109/TPAMI.2004.88
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Heidrich Wolfgang, ERIK REINHARD
   Hosseini-Asl E, 2016, IEEE T NEUR NET LEAR, V27, P2486, DOI 10.1109/TNNLS.2015.2479223
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Jiang QP, 2019, IEEE T CIRC SYST VID, V29, P323, DOI 10.1109/TCSVT.2017.2783938
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   Kaur M, 2011, INT J ADV COMPUT SC, V2, P63
   Khan IR, 2018, IEEE T IND ELECTRON, V65, P3469, DOI 10.1109/TIE.2017.2760247
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Le Q. V., 2011, P 28 INT C INT C MAC, P265
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Lemme A, 2012, NEURAL NETWORKS, V33, P194, DOI 10.1016/j.neunet.2012.05.003
   Lin YT, 2017, IEEE T MULTIMEDIA, V19, P196, DOI 10.1109/TMM.2016.2605499
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Marwah K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461914
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Moriwaki K., 2018, ARXIV181207134
   Nayar SK, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1168
   Nayar SK, 2000, PROC CVPR IEEE, P472, DOI 10.1109/CVPR.2000.855857
   Ng A, 2011, ELGAR LAW TECH SOC, P1
   Papandreou G, 2015, PROC CVPR IEEE, P390, DOI 10.1109/CVPR.2015.7298636
   Rana A, 2019, IEEE T MULTIMEDIA, V21, P256, DOI 10.1109/TMM.2018.2839885
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222
   Shan Q, 2010, IEEE T VIS COMPUT GR, V16, P663, DOI 10.1109/TVCG.2009.92
   Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277
   Song Y, 2016, APPL OPTICS, V55, P10084, DOI 10.1364/AO.55.010084
   Su Z, 2014, IEEE T MULTIMEDIA, V16, P988, DOI 10.1109/TMM.2014.2305914
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Tocci MD, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964936
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Tsagarakis G., 2012, U POWER ENG C UPEC 2, P1
   Wang SQ, 2016, IEEE T MULTIMEDIA, V18, P219, DOI 10.1109/TMM.2015.2510326
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Z, 2018, IEEE T CIRC SYST VID, V28, P550, DOI 10.1109/TCSVT.2016.2611944
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Yamasaki A, 2008, INT C PATT RECOG, P1267
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang WM, 2016, IEEE T MULTIMEDIA, V18, P313, DOI 10.1109/TMM.2016.2515997
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Zhang FL, 2018, IEEE T MULTIMEDIA, V20, P1987, DOI 10.1109/TMM.2018.2790163
   [张华 ZHANG Hua], 2011, [高分子通报, Polymer Bulletin], P1
   Zhao H, 2015, IEEE INT CONF COMPUT
NR 75
TC 6
Z9 8
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 688
EP 703
DI 10.1109/TMM.2019.2933333
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700010
DA 2024-07-18
ER

PT J
AU Bai, JL
   Li, ZF
   Ni, BB
   Wang, MS
   Yang, XK
   Hu, CP
   Gao, W
AF Bai, Jiale
   Li, Zefan
   Ni, Bingbing
   Wang, Minsi
   Yang, Xiaokang
   Hu, Chuanping
   Gao, Wen
TI Loopy Residual Hashing: Filling the Quantization Gap for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary codes; Quantization (signal); Semantics; Image retrieval; Feature
   extraction; Visualization; Hamming distance; Deep Hashing; Recurrent
   Neural Networks; Saliency; Residual
ID LEARNING BINARY-CODES; SCALABLE IMAGE; NETWORK
AB Hashing has been widely used in large-scale image retrieval based on approximate nearest neighbor search. Most learning-to-hashing methods adopt a two-stage algorithm to generate binary codes. First, original images are mapped into continuous visual features. Then, binary codes are generated by quantization step or separate projection. Nevertheless, these methods are sensitive to quantization operation, i.e., thresholding. To explicitly address this issue, this study proposes a novel feature quantization scheme with a loopy recurrent neural network, called loopy residual hashing, for the purpose of high accuracy in image retrieval. Instead of one-off thresholding-based feature binarization, the proposed approach performs an iterative threshold-then-approximate operation, which calculates the quantization residual after each thresholding step and then imitates another round of binarization to further approximate the coding residual. The resulting sequences of binary codes possess higher representation accuracy and extensive experiments on image retrieval demonstrate its superior discriminative capability over the prior art. In the meantime, theoretical approximation error analysis is given.
C1 [Bai, Jiale; Li, Zefan; Ni, Bingbing; Wang, Minsi; Hu, Chuanping] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Yang, Xiaokang] Shanghai Jiao Tong Univ, AI Inst, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Yang, Xiaokang] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
   [Gao, Wen] Peking Univ, Dept Comp Sci & Technol, Beijing 100091, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai
   Jiao Tong University; Peking University
RP Ni, BB (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
EM baijiale@sjtu.edu.cn; leezf@sjtu.edu.cn; nibingbing@sjtu.edu.cn;
   mswang1994@gmail.com; xkyang@sjtu.edu.cn; cphu@vip.sina.com;
   wgao@pku.edu.cn
RI Yang, Xiaokang/C-6137-2009
OI Yang, Xiaokang/0000-0003-4029-3322
FU National Science Foundation of China [U1611461]; SJTU-BIGO LIVE; China's
   Thousand Talent Program
FX This work was supported in part by the National Science Foundation of
   China under Grant U1611461, in part by SJTU-BIGO LIVE, and in part by
   the China's Thousand Talent Program. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Marco Bertini.
CR [Anonymous], IEEE T CYBERN
   [Anonymous], 2009, NIPS
   Bai JL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P208, DOI 10.1145/3123266.3123280
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Dai Q, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1247, DOI 10.1145/2964284.2964331
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Deng J., 2012, ILSVRC-2012
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Gu SM, 2013, INT CONF MACH LEARN, P108, DOI 10.1109/ICMLC.2013.6890453
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu MQ, 2018, IEEE T IMAGE PROCESS, V27, P545, DOI 10.1109/TIP.2017.2749147
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li WJ, 2016, IJCAI, P1711
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Lin GS, 2013, IEEE I CONF COMP VIS, P2552, DOI 10.1109/ICCV.2013.317
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu L, 2015, IEEE T IMAGE PROCESS, V24, P956, DOI 10.1109/TIP.2015.2390975
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2012, P IEEE, V100, P2624, DOI 10.1109/JPROC.2012.2197809
   Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P2352, DOI 10.1109/TIP.2017.2678163
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Norouzi M.E., 2011, ICML
   Pan YW, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P53, DOI 10.1145/2766462.2767725
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R., 2007, AISTATS, V2, P412
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Song JK, 2018, AAAI CONF ARTIF INTE, P394
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song JK, 2018, PATTERN RECOGN, V75, P175, DOI 10.1016/j.patcog.2017.03.021
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2013, IEEE I CONF COMP VIS, P3032, DOI 10.1109/ICCV.2013.377
   Wang QF, 2014, LECT NOTES COMPUT SC, V8691, P378, DOI 10.1007/978-3-319-10578-9_25
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Ye ZD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P852, DOI 10.1145/3240508.3240560
   Yuan X., 2018, P EUR C COMP VIS, P134
   Zhai DM, 2018, IEEE T MULTIMEDIA, V20, P675, DOI 10.1109/TMM.2017.2749160
   Zhang J, 2018, AAAI CONF ARTIF INTE, P539
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhu L, 2018, IEEE T NEUR NET LEAR, V29, P5264, DOI 10.1109/TNNLS.2018.2797248
NR 64
TC 12
Z9 12
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 215
EP 228
DI 10.1109/TMM.2019.2922130
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000019
DA 2024-07-18
ER

PT J
AU Mai, SJ
   Xing, SL
   Hu, HF
AF Mai, Sijie
   Xing, Songlong
   Hu, Haifeng
TI Locally Confined Modality Fusion Network With a Global Perspective for
   Multimodal Human Affective Computing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal human affective computing; locally confined cross-modality
   interaction; global cross-modality interaction; bidirectional
   multiconnected LSTM
ID SENTIMENT ANALYSIS; SPEECH; REPRESENTATIONS; RECOGNITION
AB In this paper, we propose a novel multimodal fusion framework, called the locally confined modality fusion network (LMFN), that contains a bidirectional multiconnected LSTM (BM-LSTM) to address the multimodal human affective computing problem. In the LMFN, we introduce a generic fusion structure that explores both local and global fusion to obtain an integral comprehension of information. Specifically, we partition the feature vector corresponding to each modality into multiple segments and learn every local interaction through a tensor fusion procedure. Global interaction is then modeled by learning the dependence between local tensors via an originally designed BM-LSTM architecture, establishing a direct connection of cells and states of local tensors that are several time steps apart. With the LMFN, we achieve advantages over other methods in the following aspects: 1) local interactions are successfully modeled using a feasible vector segmentation procedure that can explore cross-modal dynamics in a more specialized manner; 2) global interactions are modeled to obtain an integral view of multimodal information using BM-LSTM, which guarantees an adequate flow of information; and 3) our general fusion structure is highly extendable by applying other local and global fusion methods. Experiments show that the LMFN yields state-of-the-art results. Moreover, the LMFN achieves higher efficiency compared to other models by applying the outer product as the fusion method.
C1 [Mai, Sijie; Xing, Songlong; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Engn, Guangzhou 51027, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Engn, Guangzhou 51027, Peoples R China.
EM maisj@mail2.sysu.edu.cn; xingslong@mail2.sysu.edu.cn;
   huhaif@mail.sysu.edu.cn
OI Xing, Songlong/0000-0002-2734-1695; Hu, Haifeng/0000-0002-4884-323X;
   Mai, Sijie/0000-0001-9763-375X
FU National Natural Science Foundation of China [61673402]; Natural Science
   Foundation of Guangdong [2017A030311029]; Science and Technology Program
   of Guangzhou [201704020180]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61673402, in part by the Natural Science
   Foundation of Guangdong under Grant 2017A030311029, and in part by the
   Science and Technology Program of Guangzhou under Grant 201704020180.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. James She.
CR Alm CO, 2005, P C HUM LANG TECHN E, P579, DOI DOI 10.3115/1220575.1220648
   [Anonymous], 2016, P INT C LEARN REPR
   [Anonymous], 2015, P INT C NEUR INF PRO
   [Anonymous], 2011, P 13 INT C MULT INT
   [Anonymous], 2019, WORLD WIDE WEB, DOI DOI 10.1007/s11280-018-0541-x
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2001, FIELD GUIDE DYNAMICA
   [Anonymous], 2012, ARXIV12125701V1
   Arjovsky M, 2016, PR MACH LEARN RES, V48
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Barezi Elham J, 2018, ARXIV181112624
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Cambria E, 2016, IEEE INTELL SYST, V31, P102, DOI 10.1109/MIS.2016.31
   Campos V., 2018, INT C LEARN REPR, P1
   Castellano G, 2008, LECT NOTES COMPUT SC, V4868, P92, DOI 10.1007/978-3-540-85099-1_8
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI 10.1145/3136755.3136801
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   D'Mello SK, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2682899
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Eddy SR, 1996, CURR OPIN STRUC BIOL, V6, P361, DOI 10.1016/S0959-440X(96)80056-X
   Emerich S, 2009, ISSCS 2009: INTERNATIONAL SYMPOSIUM ON SIGNALS, CIRCUITS AND SYSTEMS, VOLS 1 AND 2, PROCEEDINGS,, P297, DOI 10.1109/ISSCS.2009.5206101
   Eyben Florian, 2010, P 18 ACM INT C MULT, P1459
   GOUDREAU MW, 1994, IEEE T NEURAL NETWOR, V5, P511, DOI 10.1109/72.286928
   Gu Y, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P537, DOI 10.1145/3240508.3240714
   Gu Y, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2225
   Gu Y, 2017, LECT NOTES ARTIF INT, V10233, P260, DOI 10.1007/978-3-319-57351-9_30
   Guo WZ, 2019, IEEE ACCESS, V7, P63373, DOI 10.1109/ACCESS.2019.2916887
   Hazarika Devamanyu, 2018, Proc Conf, V2018, P2122, DOI 10.18653/v1/n18-1193
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hussain, 2015, SENTIC COMPUTING, P23, DOI DOI 10.1007/978-3-319-23654-4_2
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kampman O, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P606
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kumar K, 2017, 7TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION TECHNOLOGY (ICCCT - 2017), P100, DOI 10.1145/3154979.3154998
   Kumar K, 2018, MULTIMED TOOLS APPL, V77, P26635, DOI 10.1007/s11042-018-5882-z
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Kumar K, 2016, 2016 12TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P119, DOI 10.1109/SITIS.2016.27
   Kumar S, 2018, IEEE IND APPLIC SOC
   Liang PP, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P2599
   Liang PP, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P472, DOI 10.1145/3242969.3243019
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Lipton Z. C., 2015, ARXIV
   Liu B, 2011, DATA CENTRIC SYST AP, P459, DOI 10.1007/978-3-642-19460-3_11
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Majumder N, 2018, KNOWL-BASED SYST, V161, P124, DOI 10.1016/j.knosys.2018.07.041
   Majumder N., 2019, P AAAI C ART INT
   Mikolov T, 2013, P WORKSHOP ICLR 2013
   Mishra S, 2019, I S BIOMED IMAGING, P57
   Mo XL, 2018, EUR J ORG CHEM, V2018, P150, DOI 10.1002/ejoc.201701324
   Moniz Joel Ruben Antony, 2017, PMLR, V77, P530, DOI [10.48550/arXiv.1801.10308, DOI 10.48550/ARXIV.1801.10308]
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   Oliva JB, 2017, PR MACH LEARN RES, V70
   OLSON DR, 1977, HARVARD EDUC REV, V47, P257, DOI 10.17763/haer.47.3.8840364413869005
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Pascanu R., 2013, INT C MACH LEARN, P1310
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Pham H., 2018, P ACL GRAND CHALL WO
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Poria S, 2017, IEEE DATA MINING, P1033, DOI 10.1109/ICDM.2017.134
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.178, 10.1109/ICDM.2016.0055]
   Poria Soujanya, 2018, ARXIV181002508
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Rajagopalan SS, 2016, LECT NOTES COMPUT SC, V9911, P338, DOI 10.1007/978-3-319-46478-7_21
   Ranganathan H, 2016, IEEE WINT CONF APPL
   Rosas VP, 2013, IEEE INTELL SYST, V28, P38, DOI 10.1109/MIS.2013.9
   Rozgic V, 2012, ASIAPAC SIGN INFO PR
   Sebe N, 2010, IEEE T MULTIMEDIA, V12, P477, DOI 10.1109/TMM.2010.2052315
   Sharma S, 2017, 2017 CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGY (CICT)
   Sharma S, 2017, LECT NOTES COMPUT SC, V10597, P373, DOI 10.1007/978-3-319-69900-4_47
   Singh H, 2017, 7TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION TECHNOLOGY (ICCCT - 2017), P29, DOI 10.1145/3154979.3154996
   Soltani R., 2016, ARXIV160500064
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Strapparava C, 2010, STUD COMPUT INTELL, V301, P21
   Su RF, 2017, INT CONF ASIAN LANG, P40
   Sukhbaatar S, 2015, ADV NEURAL INFORM PR, V28, P2440
   Sun SL, 2013, NEURAL COMPUT APPL, V23, P2031, DOI 10.1007/s00521-013-1362-6
   Tao F, 2018, INTERSPEECH, P1244
   Tao F, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2906, DOI 10.1109/ICASSP.2018.8461750
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Tsai Y. H. H., 2019, P INT C LEARN REPR
   Wang, 2016, P 2016 C EMP METH NA, P938
   Wang C., 2017, ARXIV PREPRINT ARXIV
   Wang HH, 2017, IEEE INT CON MULTI, P949, DOI 10.1109/ICME.2017.8019301
   Wang Mingxuan, 2018, P 27 INT C COMPUTATI, P1464
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wöllmer M, 2013, IEEE INTELL SYST, V28, P46, DOI 10.1109/MIS.2013.34
   Wu CH, 2011, IEEE T AFFECT COMPUT, V2, P10, DOI 10.1109/T-AFFC.2010.16
   Wu L., 2017, ARXIV170905769
   Yuan J, 2008, J ACOUST SOC AM, V124, P2078, DOI 10.1121/1.2968700
   YUHAS BP, 1989, IEEE COMMUN MAG, V27, P65, DOI 10.1109/35.41402
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5642
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang B, 2018, IMMUNOL INVEST, V47, P689, DOI 10.1080/08820139.2018.1480034
   Zhang Y., 2019, ARXIV190601004
NR 103
TC 47
Z9 52
U1 2
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 122
EP 137
DI 10.1109/TMM.2019.2925966
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000012
DA 2024-07-18
ER

PT J
AU Shin, J
   Kim, M
   Paik, J
   Lee, S
AF Shin, Joongchol
   Kim, Minseo
   Paik, Joonki
   Lee, Sangkeun
TI Radiance-Reflectance Combined Optimization and Structure-Guided $\ell
   _0$-Norm for Single Image Dehazing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE l(0)-norm; gradient; dehazing; edge-preserving filtering; optimization;
   guided filtering
ID CONTRAST ENHANCEMENT; REAL-TIME; VISIBILITY; FRAMEWORK; FEATURES
AB Outdoor images are subject to degradation regarding contrast and color because atmospheric particles scatter incoming light to a camera. Existing haze models that employ model-based dehazing methods cannot avoid the dehazing artifacts. These artifacts include color distortion and overenhancement around object boundaries because of the incorrect transmission estimation from a depth error in the skyline and the wrong haze information, especially in bright objects. To overcome this problem, we present a novel optimization-based dehazing algorithm that combines radiance and reflectance components with an additional refinement using a structure-guided $\ell _0$-norm filter. More specifically, we first estimate a weak reflectance map and optimize the transmission map based on the estimated reflectance map. Next, we estimate the structure-guided $\ell _0$ transmission map to remove the dehazing artifacts. The experimental results show that the proposed method outperforms state-of-the-art algorithms in terms of qualitative and quantitative measures compared with simulated image pairs. In addition, the real-world enhancement results demonstrate that the proposed method can provide a high-quality image without undesired artifacts. Furthermore, the guided $\ell _0$-norm filter can remove textures while preserving edges for general image enhancement algorithms.
C1 [Shin, Joongchol; Kim, Minseo; Paik, Joonki; Lee, Sangkeun] Chung Ang Univ, Dept Images, Seoul 06974, South Korea.
C3 Chung Ang University
RP Lee, S (corresponding author), Chung Ang Univ, Dept Images, Seoul 06974, South Korea.
EM your2759@daum.net; kmse706@cau.ac.kr; paikj@cau.ac.kr; sangkny@gmail.com
RI Kim, minseo/HHS-8532-2022; Paik, Joonki/AAN-7017-2020; Paik,
   Joonki/D-7635-2012
OI Paik, Joonki/0000-0002-8593-7155; Shin, Joongchol/0000-0003-3818-6587
FU Institute for Information & Communications Technology Promotion grant -
   Korea government (MSIT) [2017-0-00250]; Intelligent Defense Boundary
   Surveillance Technology Using Collaborative Reinforced Learning of
   Embedded Edge Camera and Image Analysis; National Research Foundation of
   Korea - Korea government (MSIP) [NRF-2014R1A2A1A11049986]
FX This work was supported in part by the Institute for Information &
   Communications Technology Promotion grant funded by the Korea government
   (MSIT) 2017-0-00250, in part by the Intelligent Defense Boundary
   Surveillance Technology Using Collaborative Reinforced Learning of
   Embedded Edge Camera and Image Analysis, and in part by the National
   Research Foundation of Korea grant funded by the Korea government (MSIP)
   NRF-2014R1A2A1A11049986. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Jingdong Wang.
CR Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   [Anonymous], 1952, VISION ATMOSPHERE
   [Anonymous], 2018, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2018.2807593
   [Anonymous], P IEEE WORKSH COL PH
   [Anonymous], INDIAN J SCI TECHNOL
   Berman D, 2017, IEEE INT CONF COMPUT, P115
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Breiman L., 2001, Mach. Learn., V45, P5
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Chen C, 2016, LECT NOTES COMPUT SC, V9906, P576, DOI 10.1007/978-3-319-46475-6_36
   Cho C, 2016, IEEE T IMAGE PROCESS, V25, P1617, DOI 10.1109/TIP.2016.2526785
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Feris RS, 2012, IEEE T MULTIMEDIA, V14, P28, DOI 10.1109/TMM.2011.2170666
   Gelfand I.M., 2000, Calculus of Variations
   Gonzalez RC, 2009, DIGITAL IMAGE PROCES, DOI 10.1117/1.3115362
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Jeong S, 2013, IEEE ICCE, P376, DOI 10.1109/ICCE.2013.6486936
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jiang H, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8100844
   Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Kim TK, 1998, IEEE T CONSUM ELECTR, V44, P82, DOI 10.1109/30.663733
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Saini M, 2012, IEEE T MULTIMEDIA, V14, P555, DOI 10.1109/TMM.2012.2186957
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Shin Y, 2015, IET IMAGE PROCESS, V9, P662, DOI 10.1049/iet-ipr.2014.0437
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Xie J, 2015, IEEE T MULTIMEDIA, V17, P1525, DOI 10.1109/TMM.2015.2457678
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xueyang Fu, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1190, DOI 10.1109/ICASSP.2014.6853785
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zhou ZH, 2017, IEEE T MULTIMEDIA, V19, P2651, DOI 10.1109/TMM.2017.2703954
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 58
TC 48
Z9 50
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 30
EP 44
DI 10.1109/TMM.2019.2922127
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000005
DA 2024-07-18
ER

PT J
AU Zhang, XD
   Gao, XB
   Lu, W
   He, LH
AF Zhang, Xiaodan
   Gao, Xinbo
   Lu, Wen
   He, Lihuo
TI A Gated Peripheral-Foveal Convolutional Neural Network for Unified Image
   Aesthetic Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Logic gates; Visualization; Convolutional neural
   networks; Task analysis; Deep learning; Image resolution; Visual
   aesthetic quality assessment; image aesthetics; deep learning
AB Learning fine-grained details is a key issue in image aesthetic assessment. Most of the previous methods extract the fine-grained details via random cropping strategy, which may undermine the integrity of semantic information. Extensive studies show that humans perceive fine-grained details with a mixture of foveal vision and peripheral vision. Fovea has the highest possible visual acuity and is responsible for seeing the details. The peripheral vision is used for perceiving the broad spatial scene and selecting the attended regions for the fovea. Inspired by these observations, we propose a gated peripheral-foveal convolutional neural network. It is a dedicated double-subnet neural network (i.e., a peripheral subnet and a foveal subnet). The former aims to mimic the functions of peripheral vision to encode the holistic information and provide the attended regions. The latter aims to extract fine-grained features on these key regions. Considering that the peripheral vision and foveal vision play different roles in processing different visual stimuli, we further employ a gated information fusion network to weigh their contributions. The weights are determined through the fully connected layers followed by a sigmoid function. We conduct comprehensive experiments on the standard Aesthetic Visual Analysis (AVA) dataset and Photo.net dataset for unified aesthetic prediction tasks: 1) aesthetic quality classification; 2) aesthetic score regression; and 3) aesthetic score distribution prediction. The experimental results demonstrate the effectiveness of the proposed method.
C1 [Zhang, Xiaodan; Lu, Wen; He, Lihuo] Xidian Univ, Sch Elect Engn, Video & Image Proc Syst Lab, Xian 710071, Shaanxi, Peoples R China.
   [Gao, Xinbo] Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
C3 Xidian University; Xidian University
RP Gao, XB (corresponding author), Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
EM xdanzhang@stu.xidian.edu.cn; xbgao@mail.xidian.edu.cn;
   luwen@mail.xidian.edu.cn; lihuo.he@gmail.com
RI Zhang, Xiaodan/GQR-0608-2022; Gao, Xinbo/Q-8622-2016
OI Zhang, Xiaodan/0000-0001-8192-0666; Gao, Xinbo/0000-0003-1443-0776
FU National Natural Science Foundation of China [61432014, 61772402,
   U1605252, 61671339]; National Key Research and Development Program of
   China [2016QY01W0200]; National High-Level Talents Special Support
   Program of China [CS31117200001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61432014, 61772402, U1605252, and
   61671339; in part by the National Key Research and Development Program
   of China under Grant 2016QY01W0200; and in part by National High-Level
   Talents Special Support Program of China under Grant CS31117200001. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Hantao Liu.
CR [Anonymous], LEARNED PERCEPTUAL I
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, P INT C LEARN REP IC
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], ROBUST DEEP MULTIMOD
   Chamaret C, 2013, IEEE COMPUT SOC CONF, P961, DOI 10.1109/CVPRW.2013.161
   Chen JS, 2016, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2016.61
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Gould S, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2115
   Guo LH, 2014, NEUROCOMPUTING, V143, P14, DOI 10.1016/j.neucom.2014.06.029
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiao ZC, 2018, PATTERN RECOGN, V76, P582, DOI 10.1016/j.patcog.2017.12.002
   Jin B, 2016, IEEE IMAGE PROC, P2291, DOI 10.1109/ICIP.2016.7532767
   Jin X, 2018, AAAI CONF ARTIF INTE, P77
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Kucer M, 2018, IEEE T IMAGE PROCESS, V27, P5100, DOI 10.1109/TIP.2018.2845100
   Larson AM, 2009, J VISION, V9, DOI 10.1167/9.10.6
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Ludwig CJH, 2014, P NATL ACAD SCI USA, V111, pE291, DOI 10.1073/pnas.1313553111
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Murray N., 2017, CoRR
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   Samii A, 2015, COMPUT GRAPH FORUM, V34, P141, DOI 10.1111/cgf.12465
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Tian XM, 2015, IEEE T MULTIMEDIA, V17, P2035, DOI 10.1109/TMM.2015.2479916
   Wang PQ, 2017, J VISION, V17, DOI 10.1167/17.4.9
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zhang JM, 2016, LECT NOTES COMPUT SC, V9908, P543, DOI 10.1007/978-3-319-46493-0_33
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhang XD, 2018, NEUROCOMPUTING, V311, P260, DOI 10.1016/j.neucom.2018.05.071
   Zhao MQ, 2016, SIGNAL PROCESS-IMAGE, V47, P511, DOI 10.1016/j.image.2016.05.009
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 44
TC 56
Z9 59
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2815
EP 2826
DI 10.1109/TMM.2019.2911428
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ma, CS
   Yan, ZS
   Chen, CW
AF Ma, Changsha
   Yan, Zhisheng
   Chen, Chang Wen
TI SSPA-LBS: Scalable and Social-Friendly Privacy-Aware Location-Based
   Services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social media sharing; privacy; access control; SCP-ABE; scalable media
   format
ID ANONYMITY; PROTOCOL; MODEL
AB Privacy-aware location-based service (PA-LBS) preserves LBS users' privacy but undesirably sacrifices service quality. In order to balance the two factors with satisfactory user experience, existing frameworks are faced with two barriers, that is, scalability and social-friendliness. First, existing schemes do not enable LBS users to flexibly scale their privacy level on service provision. Such a lack of scalability easily results in either unacceptable service-quality degradation or insufficient privacy protection and fails to meet dynamic user requirements. Second, existing schemes handle privacy protection by merely considering the trust relationship between users and servers but ignore the complex trust relationships among users. As a result, users cannot preserve privacy in location-based social services that involve user-to-user interactions. In this paper, we present the first scalable and social-friendly PA-LBS system. In particular, we propose a novel camouflage algorithm with a formal privacy guarantee that enables LBS users to expose their location information by scaling two privacy related factors, that is, camouflage range and place type. Furthermore, we apply the scalable ciphertext policy attribute-based encryption algorithm to enable LBS users to effectively control the access from other users to their location information. Moreover, we also demonstrated the operational efficiency of the proposed system through successful implementations on Android devices.
C1 [Ma, Changsha; Chen, Chang Wen] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
   [Yan, Zhisheng] Georgia State Univ, Dept Comp Sci, Atlanta, GA 30303 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Buffalo; University System of Georgia; Georgia State University
RP Yan, ZS (corresponding author), Georgia State Univ, Dept Comp Sci, Atlanta, GA 30303 USA.
EM changsha@buffalo.edu; zyan@gsu.edu; chencw@buffalo.edu
OI Chen, Chang Wen/0000-0002-6720-234X
FU National Science Foundation [ECCS1405594]
FX This work was supported by the National Science Foundation under Grant
   ECCS1405594. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Sanjeev Mehrotra.
   (Corresponding author: Zhisheng Yan.)
CR Andrs Miguel E., 2013, P 2013 ACM SIGSAC C, P901, DOI DOI 10.1145/2508859.2516735
   [Anonymous], ENCY CRYPTOGRAPHY SE
   [Anonymous], 2009, FULLY HOMOMORPHIC EN, DOI 10.1145/1536414.1536440
   Ardagna C., 2006, IEEE TRANS DEPENDABL, V8, P13
   Bethencourt J, 2007, P IEEE S SECUR PRIV, P321, DOI 10.1109/sp.2007.11
   Bisio I, 2013, IEEE T MULTIMEDIA, V15, P858, DOI 10.1109/TMM.2013.2239631
   Bordenabe NE, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P251, DOI 10.1145/2660267.2660345
   Chow CY, 2007, LECT NOTES COMPUT SC, V4605, P258
   Dewri R, 2013, IEEE T MOBILE COMPUT, V12, P2360, DOI 10.1109/TMC.2012.208
   Duckham M, 2005, LECT NOTES COMPUT SC, V3468, P152
   Dwork C., 2006, P INT C AUT LANG PRO, P338
   Gkoulalas-Divanis A., 2010, SIGKDD Explorations Newslett., V12, P3, DOI DOI 10.1145/1882471.1882473
   Golle P, 2009, LECT NOTES COMPUT SC, V5538, P390, DOI 10.1007/978-3-642-01516-8_26
   Kido H., 2005, 21 INT C DAT ENG WOR, P1248
   Lien IT, 2013, IEEE T INF FOREN SEC, V8, P863, DOI 10.1109/TIFS.2013.2252011
   Liu ZG, 2017, IEEE T MULTIMEDIA, V19, P874, DOI 10.1109/TMM.2016.2636750
   Ma CS, 2016, IEEE INT CON MULTI, DOI 10.1109/ICME.2016.7552906
   Ma CS, 2014, PROCEDIA COMPUT SCI, V34, P352, DOI 10.1016/j.procs.2014.07.036
   MAURER C, 1994, CONTEMPORARY OCULAR MOTOR AND VESTIBULAR RESEARCH: A TRIBUTE TO DAVID A. ROBINSON, P271
   Peddinti S., 2011, PROC 10 ANN ACM WORK, P143
   Shokri R., 2012, Em: Proceedings of the 2012 ACM conference on Computer and communications security, P617, DOI [DOI 10.1145/2382196.2382261, 10.1145/2382196.2382261]
   Shokri R, 2011, P IEEE S SECUR PRIV, P247, DOI 10.1109/SP.2011.18
   Solanas A, 2008, COMPUT COMMUN, V31, P1181, DOI 10.1016/j.comcom.2008.01.007
   Solanas A, 2007, LECT NOTES COMPUT SC, V4582, P362
   Sweeney L, 2002, INT J UNCERTAIN FUZZ, V10, P557, DOI 10.1142/S0218488502001648
   Wang XY, 2015, IEEE T MULTIMEDIA, V17, P409, DOI 10.1109/TMM.2014.2385473
   Xu T, 2009, CCS'09: PROCEEDINGS OF THE 16TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P348
   Yiu ML, 2008, PROC INT CONF DATA, P366, DOI 10.1109/ICDE.2008.4497445
   Zang H., 2011, P 17 ANN INT C MOB C, P145, DOI DOI 10.1145/2030613.2030630
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
NR 30
TC 16
Z9 18
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 2146
EP 2156
DI 10.1109/TMM.2019.2892300
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700020
OA hybrid
DA 2024-07-18
ER

PT J
AU Jia, J
   Zhou, SP
   Yin, YF
   Wu, BY
   Chen, W
   Meng, FB
   Wang, YF
AF Jia, Jia
   Zhou, Suping
   Yin, Yufeng
   Wu, Boya
   Chen, Wei
   Meng, Fanbo
   Wang, Yanfeng
TI Inferring Emotions From Large-Scale Internet Voice Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion; Internet voice data; deep sparse neural network; long
   short-term memory
ID RECOGNITION; FEATURES; MOOD
AB As voice dialog applications (VDAs, e.g., Siri,(1) Cortana,(2) Google Now(3)) are increasing in popularity, inferring emotions from the large-scale internet voice data generated from VDAs can help give a more reasonable and humane response. However, the tremendous amounts of users in large-scale internet voice data lead to a great diversity of users accents and expression patterns. Therefore, the traditional speech emotion recognition methods, which mainly target acted corpora, cannot effectively handle the massive and diverse amount of internet voice data. To address this issue, we carry out a series of observations, find suitable emotion categories for large-scale internet voice data, and verify the indicators of the social attributes (query time, query topic, and users location) and emotion inferring. Based on our observations, two different strategies are employed to solve the problem. First, a deep sparse neural network model that uses acoustic information, textual information, and three indicators (a temporal indicator, descriptive indicator, and geo-social indicator) as the input is proposed. Then, to capture the contextual information, we propose a hybrid emotion inference model that includes long short-term memory to capture the acoustic features and a latent dirichlet allocation to extract text features. Experiments on 93 000 utterances collected from the Sogou Voice Assistant(4) (Chinese Siri) validate the effectiveness of the proposed methodologies. Furthermore, we compare the two methodologies and give their advantages and disadvantages.
C1 [Jia, Jia; Zhou, Suping; Yin, Yufeng; Wu, Boya] Tsinghua Univ, Dept Comp Sci & Technol, Beijing Natl Res Ctr Informat Sci & Technol, Key Lab Pervas Comp,Minist Educ, Beijing 100084, Peoples R China.
   [Chen, Wei; Meng, Fanbo; Wang, Yanfeng] Sogou Corp, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Jia, J (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing Natl Res Ctr Informat Sci & Technol, Key Lab Pervas Comp,Minist Educ, Beijing 100084, Peoples R China.
EM jjia@tsinghua.edu.cn; 1874504489@qq.com; yinyf15@mails.tsinghua.edu.cn;
   wuboya10@gmail.com; chenweibj8871@sogou-inc.com;
   mengfanbosi0935@sogou-inc.com; wangyanfeng@sogou-inc.com
RI Wang, Yan-Feng/ABB-8063-2020; jia, jia/JKJ-5720-2023; Chen,
   Wei/E-6592-2015
FU National Key Research and Development Plan [2016YFB1001200]; Innovation
   Method Fund of China [2016IM010200]; National Natural and Science
   Foundation of China [61521002]; Tiangong Institute for Intelligent
   Computing, Tsinghua University, Beijing, China
FX This work was supported in part by the National Key Research and
   Development Plan (2016YFB1001200), in part by the Innovation Method Fund
   of China (2016IM010200), in part by the National Natural and Science
   Foundation of China (61521002), and in part by the Tiangong Institute
   for Intelligent Computing, Tsinghua University, Beijing, China.
CR ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209
   [Anonymous], 2016, P 24 ACM INT C MULT
   [Anonymous], 2014, ASE BIGDATA SOCIALCO
   [Anonymous], 2008, ADV NEURAL INFORM PR, DOI DOI 10.1109/ICISS.2008.7
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2012, J MACH LEARN RES, DOI DOI 10.1162/NECO_A_00311
   [Anonymous], 2005, P INT 2005 LISB PORT
   [Anonymous], 2012, P 20 ACM INT C MULT
   Balabantaray RC., 2012, IJAIS, V4, P48, DOI DOI 10.5120/IJAIS12-450651
   Baldi P., 2012, P INT C UNS TRANSF L, P37
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bollen J., 2009, COMPUT SCI, V44, P2365
   Bollen J, 2011, J COMPUT SCI-NETH, V2, P1, DOI 10.1016/j.jocs.2010.12.007
   Cheng X., 2012, P 2012 INT C COMP AP, P1222, DOI [10.2991/ICCASM.2012.311, DOI 10.2991/ICCASM.2012.311]
   Cui D., 2007, THESIS
   Phung D, 2013, IEEE T MULTIMEDIA, V15, P1316, DOI 10.1109/TMM.2013.2264274
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   Emerich Simina, 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P1617
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gao L., 2014, P 3 RENEWABLE POWER, P1
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Han K, 2014, INTERSPEECH, P223
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang ZW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P801, DOI 10.1145/2647868.2654984
   Kamath Krishna., 2013, P 22 INT C WORLD WID, P667, DOI DOI 10.1145/2488388.2488447
   Kim E., 2009, Web Ecology, V3, P1
   Le D, 2015, INT CONF AFFECT, P146, DOI 10.1109/ACII.2015.7344564
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2
   Lee CM, 2005, IEEE T SPEECH AUDI P, V13, P293, DOI 10.1109/TSA.2004.838534
   Li B, 2011, EPD CONGRESS 2011, P621
   Li N, 2015, INT CONF AFFECT, P84, DOI 10.1109/ACII.2015.7344555
   Lin CH, 2012, IEEE T KNOWL DATA EN, V24, P1134, DOI 10.1109/TKDE.2011.48
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Meddeb M, 2015, INT CONF INTELL SYST, P46, DOI 10.1109/ISDA.2015.7489165
   Mei J., 1996, TONGYICI CILIN VERSI
   Ngiam J., 2012, INT C MACH LEARN, P689
   Nwe TL, 2003, SPEECH COMMUN, V41, P603, DOI 10.1016/S0167-6393(03)00099-2
   OConnor B, 2010, ICWSM, P122, DOI DOI 10.1609/ICWSM.V4I1.14031
   PAN Y, 2013, INT J SMART HOME, V1
   Qi GJ, 2015, IEEE T MULTIMEDIA, V17, P1873, DOI 10.1109/TMM.2015.2485538
   Ramakrishnan S, 2013, TELECOMMUN SYST, V52, P1467, DOI 10.1007/s11235-011-9624-z
   Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58
   Schuller B, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P401
   Sedaaghi MH, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P461, DOI 10.1109/MMSP.2007.4412916
   Shah M, 2013, INT CONF ACOUST SPEE, P2553, DOI 10.1109/ICASSP.2013.6638116
   Stuhlsatz A, 2011, INT CONF ACOUST SPEE, P5688
   Tang J, 2012, IEEE T AFFECT COMPUT, V3, P132, DOI 10.1109/T-AFFC.2011.23
   Tao YS, 2015, INT CONF AFFECT, P362, DOI 10.1109/ACII.2015.7344596
   Thapliyal N., 2012, INT J ADV RES COMPUT, V1, P65
   Trabelsi Imen, 2013, International Journal of Image, Graphics and Signal Processing, V5, P8, DOI 10.5815/ijigsp.2013.09.02
   Wang DG, 2007, IEEE T AUDIO SPEECH, V15, P690, DOI 10.1109/TASL.2006.881703
   Wang XH, 2015, IEEE T AFFECT COMPUT, V6, P286, DOI 10.1109/TAFFC.2015.2400917
   Weninger Felix, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4623, DOI 10.1109/ICASSP.2014.6854478
   Wöllmer M, 2013, IMAGE VISION COMPUT, V31, P153, DOI 10.1016/j.imavis.2012.03.001
   Wöllmer M, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P2362
   Wöllmer M, 2011, INT CONF ACOUST SPEE, P4860
   Wollmer M., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P36, DOI 10.1109/ASRU.2011.6163902
   Wu B., 2010, INFOCOM IEEE Conference on Computer Communications Workshops , 2010, P1
   Wu Z, 2016, MULTIMED SYST APPL, P1, DOI 10.1007/978-3-319-40991-7
   Yang Y, 2014, AAAI CONF ARTIF INTE, P306
   Yao XW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1211, DOI 10.1145/2733373.2806319
   Yin HZ, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1543, DOI 10.1145/2588555.2593685
   Yu D., 2013, INT C LEARN REPR SCO
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao S., 2015, IEEE T MULTIMEDIA, V19, P632
   Zheng WQ, 2015, INT CONF AFFECT, P827, DOI 10.1109/ACII.2015.7344669
   Zhou X, 2016, 2016 INT IEEE CONFERENCES ON UBIQUITOUS INTELLIGENCE & COMPUTING, ADVANCED & TRUSTED COMPUTING, SCALABLE COMPUTING AND COMMUNICATIONS, CLOUD AND BIG DATA COMPUTING, INTERNET OF PEOPLE, AND SMART WORLD CONGRESS (UIC/ATC/SCALCOM/CBDCOM/IOP/SMARTWORLD), P841, DOI [10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0133, 10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.42]
   Zhu Ren, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P327, DOI 10.1007/978-3-319-04114-8_28
NR 70
TC 10
Z9 11
U1 2
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1853
EP 1866
DI 10.1109/TMM.2018.2887016
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700019
DA 2024-07-18
ER

PT J
AU Liu, YT
   Gu, K
   Wang, SQ
   Zhao, DB
   Gao, W
AF Liu, Yutao
   Gu, Ke
   Wang, Shiqi
   Zhao, Debin
   Gao, Wen
TI Blind Quality Assessment of Camera Images Based on Low-Level and
   High-Level Statistical Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality assessment (IQA); no-reference (NR)/blind; neural network;
   free-energy principle; natural image statistics
ID FREE-ENERGY PRINCIPLE; STRUCTURAL SIMILARITY; SHARPNESS ASSESSMENT
AB Camera images in reality are easily affected by various distortions, such as blur, noise, blockiness, and the like, which damage the quality of images. The complexity of distortions in camera images raises significant challenge for precisely predicting their perceptual quality. In this paper, we present an image quality assessment (IQA) approach that aims to solve this challenging problem to some extent. In the proposed method, we first extract the low-level and high-level statistical features, which can capture the quality degradations effectively. On the one hand, the first kind of statistical features are extracted from the locally mean subtracted and contrast normalized coefficients, which denote the low-level features in the early human vision. On the other hand, the recently proposed brain theory and neuroscience, especially the free-energy principle, reveal that the human brain tries to explain its encountered visual scenes through an inner creative model, with which the brain can produce the projection for the image. Then, the quality of perceptions can be reflected by the divergence between the image and its brain projection. Based on this, we extract the second type of features from the brain perception mechanism, which represent the high-level features. The low-level and high-level statistical features can play a complementary role in quality prediction. After feature extraction, we design a neural network to integrate all the features and convert them to the final quality score. Extensive tests performed on two real camera image datasets prove the validity of our method and its advantageous predicting ability over the competitive IQA models.
C1 [Liu, Yutao; Zhao, Debin; Gao, Wen] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
   [Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.
   [Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Key Lab Machine Percept, Beijing 100871, Peoples R China.
C3 Harbin Institute of Technology; Beijing University of Technology; City
   University of Hong Kong; Peking University; Peking University
RP Gu, K (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
EM yt.liu@hit.edu.cn; guke.doctor@gmail.com; shiqwang@cityu.edu.hk;
   dbzhao@hit.edu.cn; wgao@pku.edu.cn
RI Zhao, Debin/JEP-0204-2023; Gu, Ke/AAJ-9684-2021
FU Major State Basic Research Development Program of China (973 Program)
   [2015CB351804]; National Science Foundation of China [61703009]; Young
   Elite Scientist Sponsorship Program, China Association for Science and
   Technology [2017QNRC001]; Young Top-Notch Talents Team Program of
   Beijing Excellent Talents Funding [2017000026833ZK40]
FX This work was supported in part by the Major State Basic Research
   Development Program of China (973 Program) under Grant 2015CB351804, in
   part by the National Science Foundation of China under Grant 61703009,
   and in part by the Young Elite Scientist Sponsorship Program by China
   Association for Science and Technology under Grant 2017QNRC001, and in
   part by Young Top-Notch Talents Team Program of Beijing Excellent
   Talents Funding (2017000026833ZK40).
CR [Anonymous], 2010, Categorical image quality (CSIQ) database
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2014, IEEE IMAGE PROC, P511, DOI 10.1109/ICIP.2014.7025102
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Gu K, 2013, IEEE INT CON MULTI
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   Hou WL, 2015, IEEE T NEUR NET LEAR, V26, P1275, DOI 10.1109/TNNLS.2014.2336852
   Hu B, 2017, SIGNAL PROCESS-IMAGE, V58, P165, DOI 10.1016/j.image.2017.08.003
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Li LD, 2017, IEEE ACCESS, V5, P2163, DOI 10.1109/ACCESS.2017.2661858
   Li LD, 2016, IEEE T IMAGE PROCESS, V25, P3775, DOI 10.1109/TIP.2016.2577891
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Lia LD, 2016, SIGNAL PROCESS-IMAGE, V48, P81, DOI 10.1016/j.image.2016.09.005
   Liu HT, 2011, IEEE T CIRC SYST VID, V21, P971, DOI 10.1109/TCSVT.2011.2133770
   Liu HT, 2009, IEEE IMAGE PROC, P3097, DOI 10.1109/ICIP.2009.5414466
   Liu HT, 2010, IEEE T CIRC SYST VID, V20, P529, DOI 10.1109/TCSVT.2009.2035848
   Liu H, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/263540
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Liu YT, 2017, J VIS COMMUN IMAGE R, V46, P70, DOI 10.1016/j.jvcir.2017.03.007
   Liu YT, 2016, IEEE INT SYMP CIRC S, P1586, DOI 10.1109/ISCAS.2016.7538867
   Marziliano P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P57, DOI 10.1109/ICIP.2002.1038902
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moore M., 1974, PHYS B, V25
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Ng A., 2012, UFLDL tutorial"
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Rehman A, 2012, IEEE T IMAGE PROCESS, V21, P3378, DOI 10.1109/TIP.2012.2197011
   Rohaly A.M., 2000, ITU T STANDARDS CONT, P9
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh H.R., 2006, LIVE image quality assessment database release 2
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Soundararajan R, 2012, IEEE T IMAGE PROCESS, V21, P517, DOI 10.1109/TIP.2011.2166082
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Vu CT, 2012, IEEE T IMAGE PROCESS, V21, P934, DOI 10.1109/TIP.2011.2169974
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2006, IEEE T IMAGE PROCESS, V15, P1680, DOI 10.1109/TIP.2005.864165
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu QB, 2015, IEEE IMAGE PROC, P339, DOI 10.1109/ICIP.2015.7350816
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zoran D, 2009, IEEE I CONF COMP VIS, P2209, DOI 10.1109/ICCV.2009.5459476
NR 60
TC 50
Z9 52
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 135
EP 146
DI 10.1109/TMM.2018.2849602
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700012
DA 2024-07-18
ER

PT J
AU Guo, YM
   Liu, Y
   Lao, SY
   Bakker, EM
   Bai, L
   Lew, MS
AF Guo, Yanming
   Liu, Yu
   Lao, Songyang
   Bakker, Erwin M.
   Bai, Liang
   Lew, Michael S.
TI Bag of Surrogate Parts Feature for Visual Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks (CNNs); convolutional layers; surrogate
   part; scale pooling; global-part prediction; visual recognition
AB Convolutional neural networks (CNNs) have attracted significant attention in visual recognition. Several recent studies have shown that, in addition to the fully connected layers, the features derived from the convolutional layers of CNNs can also achieve promising performance in image classification tasks. In this paper, we propose a new feature from the convolutional layers, called Bag of Surrogate Parts (BoSP), and its spatial variant, Spatial-BoSP (S-BoSP). The main idea is, we assume the feature maps in the convolutional layers as surrogate parts, and densely sample and assign image regions to these surrogate parts by observing the activation values. Together with BoSP/S-BoSP, we further propose another two schemes to enhance the performance: scale pooling and global-part prediction. Scale pooling aims to handle the objects with different scales and deformations, and global-part prediction combines the predictions of global and part features. By conducting extensive experiments on generic object, fine-grained object and scene datasets, we find the proposed scheme can not only achieve superior performance to the fully connected feature, but also produces competitive or, in some cases, remarkably better performance than the state of the art.
C1 [Guo, Yanming; Liu, Yu; Bakker, Erwin M.; Lew, Michael S.] Leiden Univ, Leiden Inst Adv Comp Sci, NL-2300 RA Leiden, Netherlands.
   [Lao, Songyang; Bai, Liang] Natl Univ Def Technol, Coll Informat Syst & Management, Changsha 410073, Hunan, Peoples R China.
C3 Leiden University; Leiden University - Excl LUMC; National University of
   Defense Technology - China
RP Lew, MS (corresponding author), Leiden Univ, Leiden Inst Adv Comp Sci, NL-2300 RA Leiden, Netherlands.
EM y.guo@liacs.leidenuniv.nl; y.liu@liacs.leidenuniv.nl;
   laosongyang@vip.sina.com; e.m.bakker@liacs.leidenuniv.nl; xabpz@163.com;
   m.s.lew@liacs.leidenuniv.nl
CR Agrawal P, 2014, LECT NOTES COMPUT SC, V8695, P329, DOI 10.1007/978-3-319-10584-0_22
   [Anonymous], INT J COMPUT VIS
   [Anonymous], 2003, TECH REP
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], TECH REP
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], P INT C LEARN REPR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, 2015 IEEE INT C, DOI DOI 10.1109/ICCVW.2015.45
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2014, P 31 INT C INT C MAC
   [Anonymous], 2015, PROCIEEE CONFCOMPUT
   Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150
   Bai S, 2016, IEEE T MULTIMEDIA, V18, P1351, DOI 10.1109/TMM.2016.2557071
   Cimpoi M, 2016, INT J COMPUT VISION, V118, P65, DOI 10.1007/s11263-015-0872-3
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donggeun Yoo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P71, DOI 10.1109/CVPRW.2015.7301274
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fernando B, 2012, PROC CVPR IEEE, P3434, DOI 10.1109/CVPR.2012.6248084
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116
   Guo YR, 2016, LECT NOTES COMPUT SC, V9993, P1, DOI 10.1007/978-3-319-47118-1_1
   Herranz L, 2016, PROC CVPR IEEE, P571, DOI 10.1109/CVPR.2016.68
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Joe Yue-Hei Ng, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P53, DOI 10.1109/CVPRW.2015.7301272
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124
   Kim YD, 2016, PROC CVPR IEEE, P5318, DOI 10.1109/CVPR.2016.574
   Kulkarni P, 2016, LECT NOTES COMPUT SC, V9912, P329, DOI 10.1007/978-3-319-46484-8_20
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Liu LQ, 2017, IEEE T PATTERN ANAL, V39, P2305, DOI 10.1109/TPAMI.2016.2637921
   Liu LQ, 2015, PROC CVPR IEEE, P4749, DOI 10.1109/CVPR.2015.7299107
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Perronnin F., 2007, PROC IEEE C COMPUT V, P1
   Philbin J., 2008, PROC IEEE C COMPUT V, P1
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   Sharma G, 2015, IEEE I CONF COMP VIS, P1296, DOI 10.1109/ICCV.2015.153
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Wu JX, 2016, AAAI CONF ARTIF INTE, P2237
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xie LX, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P3, DOI 10.1145/2671188.2749289
   Yang SF, 2015, IEEE I CONF COMP VIS, P1215, DOI 10.1109/ICCV.2015.144
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 53
TC 11
Z9 11
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1525
EP 1536
DI 10.1109/TMM.2017.2766842
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400019
DA 2024-07-18
ER

PT J
AU Otsuka, K
AF Otsuka, Kazuhiro
TI Behavioral Analysis of Kinetic Telepresence for Small Symmetric
   Group-to-Group Meetings
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Teleconferencing; multimodal interactions; non-verbal communication;
   telepresence
ID GAZE; PERCEPTION; ATTENTION; MOVEMENT; MOTION
AB Nonverbal behavior analysis revealed the effect of MMSpace, a kinetic telepresence developed for social telepresence, on small symmetric group-to-group conversations. MMSpace consists of kinetic avatars, equipped with flat projection screen panels as faces, that can change their pose and position automatically to mirror the remote user's head motions. The advantage is the realistic kinetic expression of human head movements, which form gestures like nodding and indicate the focus of visual attention, through the use of four degree-of-freedom low-latency precision actuators. Another feature is the support of eye contact among remote participants, which is made possible by the avatar's kinetic pose changes and by adaptive camera selection for orienting the user's face toward the remote addressee. Its limitation is its room-scale infrastructure and restricted participant positions. Targeting a symmetric 2 x 2 setting, participants' nonverbal behaviors, including gaze directions and head gestures, were compared among three conditions, MMSpace with/without physical motions and face-to-face settings. There was a significant difference between the conditions in terms of the duration of glance/mutual glances, total gaze transition time, amount of head gesturing, and co-occurrences of head gestures in the remote participants. The results indicate that the avatar's physical motion can elicit longer (mutual) glances with a shorter total transition time and more (co-) occurrences of head gestures, and it makes MMSpace-based conversations closer, in terms of these nonverbal statistics, to face-to-face ones compared with those of a static version of MMSpace without physical motion.
C1 [Otsuka, Kazuhiro] NTT Corp, NTT Commun Sci Labs, Atsugi, Kanagawa 2430198, Japan.
C3 Nippon Telegraph & Telephone Corporation
RP Otsuka, K (corresponding author), NTT Corp, NTT Commun Sci Labs, Atsugi, Kanagawa 2430198, Japan.
EM otsuka@ieee.org
RI Otsuka, Kazuhiro/AAP-7814-2020
OI Otsuka, Kazuhiro/0000-0003-4352-3955
CR Adalgeirsson SO, 2010, ACMIEEE INT CONF HUM, P15, DOI 10.1109/HRI.2010.5453272
   Al Moubayed Samer, 2012, Cognitive Behavioural Systems (COST 2012). International Training School. Revised Selected Papers, P114, DOI 10.1007/978-3-642-34584-5_9
   Alejandro Jaimes, 2006, P 14 ANN ACM INT C M, P855, DOI [10.1145/1180639.1180829, DOI 10.1145/1180639.1180829]
   [Anonymous], 1992, P SIGCHI C HUMAN FAC, DOI DOI 10.1145/142750.142769
   Argyle M., 1976, Gaze and Mutual Gaze
   Argyle Michael, 1975, BODILY COMMUNICATION
   Bailenson JN, 2003, PERS SOC PSYCHOL B, V29, P819, DOI 10.1177/0146167203029007002
   Breazeal C., 2007, 2007 2nd Annual Conference on Human-Robot Interaction (HRI), P153
   Chartrand TL, 1999, J PERS SOC PSYCHOL, V76, P893, DOI 10.1037/0022-3514.76.6.893
   Chen M., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P49, DOI 10.1145/503376.503386
   Cohen J., 1988, STAT POWER ANAL BEHA
   Delaherche E, 2012, IEEE T AFFECT COMPUT, V3, P349, DOI 10.1109/T-AFFC.2012.12
   Dong W, 2013, IEEE T MULTIMEDIA, V15, P83, DOI 10.1109/TMM.2012.2225039
   DUNCAN S, 1972, J PERS SOC PSYCHOL, V23, P283, DOI 10.1037/h0033031
   Goodwin C., 1981, CONVERSATIONAL ORG I
   Gorga S., 2010, P INT C MULT INT WOR
   Guizzo E, 2010, IEEE SPECTRUM, V47, P26, DOI 10.1109/MSPEC.2010.5557512
   Hecht H, 2014, PSIHOLOGIJA, V47, P287, DOI 10.2298/PSI1403287H
   Heider F, 1944, AM J PSYCHOL, V57, P243, DOI 10.2307/1416950
   Heyes C, 2011, PSYCHOL BULL, V137, P463, DOI 10.1037/a0022288
   Heylen D., 2005, P AISB, P45
   Ishii H., 1992, P SIGCHI C HUM FACT, P3525, DOI [10.1145/142750.142977, DOI 10.1145/142750.142977]
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   KENDON A, 1967, ACTA PSYCHOL, V26, P22, DOI 10.1016/0001-6918(67)90005-4
   Kim K, 2012, P SIGCHI C HUM FACT, P2531, DOI DOI 10.1145/2207676.2208640
   KRAUSS RM, 1967, J ACOUST SOC AM, V41, P286, DOI 10.1121/1.1910338
   Kumano S, 2017, IEEE T MULTIMEDIA, V19, P107, DOI 10.1109/TMM.2016.2608002
   Kumano S, 2015, IEEE T AFFECT COMPUT, V6, P324, DOI 10.1109/TAFFC.2015.2417561
   Lee J, 2010, IEEE T MULTIMEDIA, V12, P552, DOI 10.1109/TMM.2010.2051874
   Lincoln P, 2009, INT SYM MIX AUGMENT, P27, DOI 10.1109/ISMAR.2009.5336503
   LIVINGSTONE M, 1988, SCIENCE, V240, P740, DOI 10.1126/science.3283936
   MAYNARD SK, 1987, J PRAGMATICS, V11, P589, DOI 10.1016/0378-2166(87)90181-0
   Misawa K, 2012, PROCEEDINGS OF THE INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, P394, DOI 10.1145/2254556.2254632
   Mori M., 1970, Energy, V7, P33, DOI [DOI 10.1109/MRA.2012.2192811, 10.1109/MRA.2012.2192811]
   Naimark M., 1979, TALKING HEAD PROJECT
   Nguyen D., 2005, Proceedings of the sigchi conference on human factors in computing systems, P799
   Okada K., 1994, Transcending Boundaries, CSCW '94. Proceedings of the Conference on Computer Supported Cooperative Work, P385, DOI 10.1145/192844.193054
   Otsuka K., 2012, CHI'12 Extended Abstracts on Human Factors in Computing Systems, P2243
   Otsuka K., 2016, HOLOGRAPHY LIKE KINE
   Otsuka K., 2016, ABOUT MMSPACE
   Otsuka K., 2011, P ACM MULT 11, P763
   Otsuka K., 2005, P 7 INT C MULT INT, P191, DOI DOI 10.1145/1088463.1088497
   Otsuka K, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P389
   Otsuka K, 2016, P IEEE VIRT REAL ANN, P19, DOI 10.1109/VR.2016.7504684
   Otsuka K, 2011, IEEE SIGNAL PROC MAG, V28, P127, DOI 10.1109/MSP.2011.941100
   Oyekoya O., 2012, P SIGCHI C HUMAN FAC, P2551, DOI DOI 10.1145/2207676.2208642
   Pan Y, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2173, DOI 10.1145/2556288.2557320
   Regenbrecht H, 2004, PRESENCE-TELEOP VIRT, V13, P338, DOI 10.1162/1054746041422334
   Regenbrecht H, 2015, COMMUN ASSOC INF SYS, V37, P965
   Revolve Robotics, 2012, KUBI
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Savicki V, 2000, CYBERPSYCHOL BEHAV, V3, P817, DOI 10.1089/10949310050191791
   Schermerhorn P, 2008, P 3 ACM IEEE INT C H, P263, DOI [DOI 10.1145/1349822.1349857, 10.1145/1349822.1349857]
   Schulze R., 2012, APPL FERROELECTRICS, P1, DOI DOI 10.1109/ISAF.2012.6297771
   Sellen A. J., 1992, CHI '92 Conference Proceedings. ACM Conference on Human Factors in Computing Systems. Striking a Balance, P49, DOI 10.1145/142750.142756
   SELLEN AJ, 1995, HUM-COMPUT INTERACT, V10, P401, DOI 10.1207/s15327051hci1004_2
   SHAPIRO KL, 1987, ACTA PSYCHOL, V66, P157, DOI 10.1016/0001-6918(87)90031-X
   Short J., 1976, The social psychology of telecommunications
   Sirkin D, 2012, ACMIEEE INT CONF HUM, P57
   Sirkin D, 2011, LECT NOTES COMPUT SC, V6946, P162, DOI 10.1007/978-3-642-23774-4_16
   Stiefelhagen R, 2002, IEEE T NEURAL NETWOR, V13, P928, DOI 10.1109/TNN.2002.1021893
   Tan KH, 2011, IEEE T MULTIMEDIA, V13, P466, DOI 10.1109/TMM.2011.2130516
   Vertegaal R., 2003, ACM CHI, P521, DOI 10.1145/642611.642702
   Xiao B, 2015, IEEE T MULTIMEDIA, V17, P1107, DOI 10.1109/TMM.2015.2432671
   Yankelovich Nicole., 2007, CHI'07 extended abstracts on Human factors in computing systems, P2789, DOI DOI 10.1145/1240866
NR 65
TC 7
Z9 8
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1432
EP 1447
DI 10.1109/TMM.2017.2771396
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400012
DA 2024-07-18
ER

PT J
AU Zhang, YF
   Cao, CQ
   Cheng, J
   Lu, HQ
AF Zhang, Yifan
   Cao, Congqi
   Cheng, Jian
   Lu, Hanqing
TI EgoGesture: A New Dataset and Benchmark for Egocentric Hand Gesture
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Benchmark; dataset; egocentric vision; gesture recognition; first-person
   view
ID REAL-TIME
AB Gesture is a natural interface in human-computer interaction, especially interacting with wearable devices, such as VR/AR helmet and glasses. However, in the gesture recognition community, it lacks of suitable datasets for developing egocentric (first-person view) gesture recognition methods, in particular in the deep learning era. In this paper, we introduce a new benchmark dataset named EgoGesture with sufficient size, variation, and reality to be able to train deep neural networks. This dataset contains more than 24 000 gesture samples and 3 000 000 frames for both color and depth modalities from 50 distinct subjects. We design 83 different static and dynamic gestures focused on interaction with wearable devices and collect them from six diverse indoor and outdoor scenes, respectively, with variation in background and illumination. We also consider the scenario when people perform gestures while they are walking. The performances of several representative approaches are systematically evaluated on two tasks: gesture classification in segmented data and gesture spotting and recognition in continuous data. Our empirical study also provides an in-depth analysis on input modality selection and domain adaptation between different scenes.
C1 [Zhang, Yifan; Cao, Congqi; Cheng, Jian; Lu, Hanqing] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Zhang, Yifan; Cao, Congqi; Cheng, Jian; Lu, Hanqing] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Cheng, J (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.; Cheng, J (corresponding author), Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
EM yfzhang@nlpr.ia.ac.cn; congqi.cao@nlpr.ia.ac.cn; jcheng@nlpr.ia.ac.cn;
   luhq@nlpr.ia.ac.cn
RI zhang, yifan/ABB-5853-2021; , chengjian/KGL-5551-2024
OI , chengjian/0000-0003-1289-2758
FU National Natural Science Foundation of China [61332016, 61572500]; Youth
   Innovation Promotion Association CAS
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61332016 and Grant 61572500, and in part
   by the Youth Innovation Promotion Association CAS.
CR [Anonymous], ECCV CHALEARN LOOK P
   [Anonymous], 2016, PROC IEEE COMPUT VIS
   [Anonymous], 2013, ARXIV13080850 CORR
   [Anonymous], PAC RIM S IM VID TEC
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2014, P EUR C COMP VIS
   Bambach S, 2015, IEEE I CONF COMP VIS, P1949, DOI 10.1109/ICCV.2015.226
   Bao BK, 2012, IEEE T IMAGE PROCESS, V21, P3794, DOI 10.1109/TIP.2012.2192742
   Baraldi Lorenzo., 2014, P IEEE C COMP VIS PA, P688
   Camgoz NC, 2016, INT C PATT RECOG, P49, DOI 10.1109/ICPR.2016.7899606
   Cao C., 2017, P IEEE C COMP VIS PA, P3763
   Cao C., 2016, INT JOINT C ART INT, V1, P3
   Crispim CF Jr, 2016, IEEE T PATTERN ANAL, V38, P1598, DOI 10.1109/TPAMI.2016.2537323
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Escalera S, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P365
   Gonzalez-Diaz I., 2015, HLTH MONITORING PERS, P161
   Jang Y, 2017, IEEE T HUM-MACH SYST, V47, P113, DOI 10.1109/THMS.2016.2611824
   Jiang F, 2015, J MACH LEARN RES, V16, P227
   Jiang YG, 2015, IEEE T IMAGE PROCESS, V24, P3781, DOI 10.1109/TIP.2015.2456412
   Karaman S, 2014, MULTIMED TOOLS APPL, V69, P743, DOI 10.1007/s11042-012-1117-x
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kim TK, 2009, IEEE T PATTERN ANAL, V31, P1415, DOI 10.1109/TPAMI.2008.167
   Kurakin A, 2012, EUR SIGNAL PR CONF, P1975
   Liu L., 2013, 23 INT JOINT C ART I
   Lui YM, 2012, J MACH LEARN RES, V13, P3297
   Malgireddy MR, 2013, J MACH LEARN RES, V14, P2189
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Memo A, 2018, MULTIMED TOOLS APPL, V77, P27, DOI 10.1007/s11042-016-4223-3
   Mitra S, 2007, IEEE T SYST MAN CY C, V37, P311, DOI 10.1109/TSMCC.2007.893280
   Molchanov P., 2016, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, P4207, DOI DOI 10.1109/CVPR.2016.456
   Ohn-Bar E, 2014, IEEE T INTELL TRANSP, V15, P2368, DOI 10.1109/TITS.2014.2337331
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Rogez G, 2015, IEEE I CONF COMP VIS, P3889, DOI 10.1109/ICCV.2015.443
   Ruffieux S, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P483, DOI 10.1145/2522848.2532590
   Sharp T, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3633, DOI 10.1145/2702123.2702179
   Simonyan K., 2014, 14091556 ARXIV
   Starner T, 1998, IEEE T PATTERN ANAL, V20, P1371, DOI 10.1109/34.735811
   Wan CD, 2016, LECT NOTES COMPUT SC, V9907, P554, DOI 10.1007/978-3-319-46487-9_34
   Wan J, 2016, IEEE T PATTERN ANAL, V38, P1626, DOI 10.1109/TPAMI.2015.2513479
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang J, 2012, LECT NOTES COMPUT SC, V7573, P872, DOI 10.1007/978-3-642-33709-3_62
   Wang PC, 2016, INT C PATT RECOG, P13, DOI 10.1109/ICPR.2016.7899600
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Zhu Y, 2014, IMAGE VISION COMPUT, V32, P453, DOI 10.1016/j.imavis.2014.04.005
NR 47
TC 163
Z9 173
U1 6
U2 58
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1038
EP 1050
DI 10.1109/TMM.2018.2808769
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400002
DA 2024-07-18
ER

PT J
AU Kim, K
   Jung, SW
AF Kim, Kyumok
   Jung, Seung-Won
TI Interactive Image Segmentation Using Semi-transparent Wearable Glasses
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hand gesture; interactive image segmentation; semi-transparent glasses;
   wearable device
AB Since it is difficult to automatically and precisely extract an object of interest, interactive image segmentation techniques exploit user-provided segmentation seeds. In previous interactive segmentation applications, the segmentation seeds are typically provided by mouse clicks or finger touches. In this paper, the segmentation of an object is studied from the scene that the user sees through semi-transparent wearable glasses. In this application scenario, a front-view camera is used to obtain the segmentation seeds from the user's fingertip position. In particular, two segmentation methodologies called transparent segmentation and semi-transparent segmentation are considered to determine an effective segmentation scheme for the wearable glasses. Extensive user studies are performed to evaluate the user preferences and the segmentation accuracies of the two methodologies.
C1 [Kim, Kyumok; Jung, Seung-Won] Dongguk Univ, Dept Multimedia Engn, Seoul 100715, South Korea.
C3 Dongguk University
RP Jung, SW (corresponding author), Dongguk Univ, Dept Multimedia Engn, Seoul 100715, South Korea.
EM kyoomok@gmail.com; swjung83@gmail.com
OI Jung, Seung-Won/0000-0002-0319-4467
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Science, ICT & Future Planning
   [NRF-2017R1D1A1A09000558]
FX This work was supported by the Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Science, ICT & Future Planning (NRF-2017R1D1A1A09000558). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Winston Hu.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Adamek T., 2006, THESIS
   ADAMS R, 1994, IEEE T PATTERN ANAL, V16, P641, DOI 10.1109/34.295913
   Andrade F., 2015, P 20 S SIGN PROC IM, P1
   [Anonymous], 2016, SOURCE CODE SLIC SUP
   [Anonymous], 2016, SOURCE CODE GRABCUT
   [Anonymous], 2016, Epson Moverio BT-200 Next Generation Smart Glasses
   [Anonymous], 2016, MICROSOFT HOLOLENS
   [Anonymous], P IEEE INT S MULT IS
   [Anonymous], 2016, SOURCE CODE FINDHOMO
   Arsenio A., 2003, P 3 INT WORKSH EP RO, P1
   Bai JJ, 2014, PROC CVPR IEEE, P392, DOI 10.1109/CVPR.2014.57
   Bhattacharyya Anil, 1943, B CALCUTTA MATH SOC, V35, P99
   Bouguet J., 2000, OPENCV DOC INTEL MIC
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Byungsung Lee, 2010, Proceedings of the Seventh International Conference on Information Technology: New Generations (ITNG 2010), P398, DOI 10.1109/ITNG.2010.36
   Ding JJ, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P740, DOI 10.1109/ICDSP.2015.7251974
   Dominguez SM, 2006, IEEE T MULTIMEDIA, V8, P956, DOI 10.1109/TMM.2006.879872
   Gallo I, 2014, 2014 27TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI), P73, DOI 10.1109/SIBGRAPI.2014.35
   Huang ZP, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P341, DOI 10.1145/2733373.2806266
   Jalaliniya S, 2014, 2014 11TH INTERNATIONAL CONFERENCE ON WEARABLE AND IMPLANTABLE BODY SENSOR NETWORKS WORKSHOPS (BSN WORKSHOPS), P50, DOI 10.1109/BSN.Workshops.2014.14
   Jiang HQ, 2015, IEEE T MULTIMEDIA, V17, P3, DOI 10.1109/TMM.2014.2368273
   Johansson J, 2015, J VISION, V15, DOI 10.1167/15.9.21
   Keaton T, 2002, SIXTH INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P75, DOI 10.1109/ISWC.2002.1167221
   Keaton T, 1999, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES (CBAIVL'99) - PROCEEDINGS, P69, DOI 10.1109/IVL.1999.781126
   Kocejko T., 2016, P 9 INT C HUM SYST I, P476
   Korinke C., 2016, P ACM MULT C, P717
   Korinke C, 2015, PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA (MUM 2015), P84, DOI 10.1145/2836041.2836049
   Li KQ, 2015, IEEE T MULTIMEDIA, V17, P994, DOI 10.1109/TMM.2015.2433795
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   McGuinness K, 2010, PATTERN RECOGN, V43, P434, DOI 10.1016/j.patcog.2009.03.008
   Powers DMW, 2020, J MACH LEARN TECHNOL, P37, DOI DOI 10.9735/2229-3981
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sener O., 2012, P 2 ACM INT WORKSH I, P9
   Shanhe Yi, 2016, 2016 IEEE Conference on Computer Communications: Workshops (INFOCOM WKSHPS), P1017, DOI 10.1109/INFCOMW.2016.7562233
   Shi R., 2011, P VIS COMM IM PROC, P1
   Sun YP, 2016, IEEE T MULTIMEDIA, V18, P171, DOI 10.1109/TMM.2015.2496246
   Tang M, 2013, IEEE I CONF COMP VIS, P1769, DOI 10.1109/ICCV.2013.222
   Tung YC, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3327, DOI 10.1145/2702123.2702214
   Wang T, 2016, IEEE T MULTIMEDIA, V18, P2358, DOI 10.1109/TMM.2016.2600441
   Wang T, 2015, ADV INTEL SYS RES, V117, P1
   Wu Yunfen., 2013, Electrodiagnosis in New Frontiers of Clinical Research, P1
   Yang WX, 2010, IEEE T IMAGE PROCESS, V19, P2470, DOI 10.1109/TIP.2010.2048611
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zucco J. E., 2016, FRONTIER ICT, V3, P1
NR 45
TC 6
Z9 6
U1 3
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 208
EP 223
DI 10.1109/TMM.2017.2728318
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700017
DA 2024-07-18
ER

PT J
AU Tsai, TJ
   Prätzlich, T
   Müller, M
AF Tsai, T. J.
   Praetzlich, Thomas
   Mueller, Meinard
TI Known-Artist Live Song Identification Using Audio Hashprints
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio matching; cover song; fingerprinting; live performance; song
   identification
ID COMPUTER VISION; ROBUST
AB The goal of live song identification is to allow concertgoers to identify a live performance by recording a few seconds of the performance on their cell phone. This paper proposes a multistep approach to address this problem for popular bands. In the first step, GPS data are used to associate the audio query with a concert in order to infer who the musical artist is. This reduces the search space to a dataset containing the artist's studio recordings. In the next step, the known-artist search is solved by representing the audio as a sequence of binary codes called hashprints, which can be efficiently matched against the database using a two-stage cross-correlation approach. The hashprint representation is derived from a set of spectrotemporal filters that are learned in an unsupervised artist-specific manner. On the Gracenote live song identification benchmark, the proposed system outperforms five other baseline systems and improves the mean reciprocal rank of the previous state of the art from 0.68 to 0.79, while simultaneously reducing the average runtime per query from 10 to 0.9 s. We conduct extensive analyses of major factors affecting system performance.
C1 [Tsai, T. J.] Harvey Mudd Coll, Dept Engn, Claremont, CA 91711 USA.
   [Praetzlich, Thomas; Mueller, Meinard] Audio Labs Erlangen, D-91058 Erlangen, Germany.
C3 Claremont Colleges; Harvey Mudd College
RP Tsai, TJ (corresponding author), Harvey Mudd Coll, Dept Engn, Claremont, CA 91711 USA.
EM ttsai@hmc.edu; thomas.praetzlich@audiolabs-erlangen.de;
   meinard.mueller@audiolabs-erlangen.de
RI Mueller, Meinard/U-2097-2019
OI Mueller, Meinard/0000-0001-6062-7524
FU German Research Foundation [DFG MU 2686/7-1]
FX The work of T. Pratzlich was supported by the German Research Foundation
   under Grant DFG MU 2686/7-1. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Cha
   Zhang.
CR Allamanche E., 2001, P INT S MUS INF RETR, P197
   Anguera X., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P455, DOI 10.1109/ICME.2012.137
   Baluja S, 2008, PATTERN RECOGN, V41, P3467, DOI 10.1016/j.patcog.2008.05.006
   Baluja S, 2007, INT CONF ACOUST SPEE, P213
   Bertin-Mahieux T., 2011, ISMIR, P591
   Bertin-Mahieux T., 2012, INT SOC MUSIC INFORM
   Bertin-Mahieux T, 2011, 2011 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS (WASPAA), P117, DOI 10.1109/ASPAA.2011.6082307
   Casey M, 2008, IEEE T AUDIO SPEECH, V16, P1015, DOI 10.1109/TASL.2008.925883
   Coover Bob, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1394, DOI 10.1109/ICASSP.2014.6853826
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng L, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P1692
   Downie J., 2008, P 9 INT C MUS INF RE, P468
   Ellis D., 2007, P MUS INF RETR EV EX
   Ellis D., 2015, Robust Landmark-Based Audio Fingerprinting
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Fenet S., 2013, Proc. International Society for Music Information Retrieval (ISMIR), P569
   Fenet S., 2011, Proc. International Society for Music Information Retrieval (ISMIR), P121
   George J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P436, DOI 10.1109/ICDSP.2015.7251909
   Grosche P, 2012, INT CONF ACOUST SPEE, P473, DOI 10.1109/ICASSP.2012.6287919
   Haitsma J., 2001, International Workshop on Content-Based Multimedia Indexing, V4, P117
   Haitsma J., 2002, P ISMIR 2002 3 INT C, P107
   Haykin S.O., 1996, Adaptive Filter Theory, V4th
   Herre J, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P127, DOI 10.1109/ASPAA.2001.969559
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hu N, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P185, DOI 10.1109/ASPAA.2003.1285862
   Humphrey E. J., 2013, PROC INT SOC MUSIC I, P4
   Jang D, 2009, IEEE T INF FOREN SEC, V4, P995, DOI 10.1109/TIFS.2009.2034452
   Ke Y, 2005, PROC CVPR IEEE, P597
   Khadkevich Maksim, 2013, Proc. International Society for Music Information Retrieval (ISMIR), P233
   Kim S, 2007, INT CONF ACOUST SPEE, P241
   Kurth F, 2008, IEEE T AUDIO SPEECH, V16, P382, DOI 10.1109/TASL.2007.911552
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Malekesmaeili M, 2014, SIGNAL PROCESS, V98, P308, DOI 10.1016/j.sigpro.2013.11.023
   Muller M., 2015, Fundamentals of Music Processing. Audio, Analysis, Algorithms, Applications, DOI [10.1007/978-3-319-21945-5, DOI 10.1007/978-3-319-21945-5]
   Muller M, 2005, P 6 INT C MUSIC INFO, P288
   Nagano H, 2015, INT CONF ACOUST SPEE, P2324, DOI 10.1109/ICASSP.2015.7178386
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Osmalsky Julien, 2015, INT SOC MUS INF RETR
   Over P, 2011, P TRECVID
   Rael C., 2015, Proc. International Society for Music Information Retrieval (ISMIR), P234
   Rafii Zafar, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P644, DOI 10.1109/ICASSP.2014.6853675
   Ravuri S, 2010, INT CONF ACOUST SPEE, P65, DOI 10.1109/ICASSP.2010.5496214
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Schorkhuber C., 2010, P 7 SOUND MUS COMP C, P3
   Schreiber H, 2014, IEEE T MULTIMEDIA, V16, P1654, DOI 10.1109/TMM.2014.2318517
   Seo JS, 2006, IEEE SIGNAL PROC LET, V13, P209, DOI 10.1109/LSP.2005.863678
   Seo JS, 2005, INT CONF ACOUST SPEE, P213
   Serra J., 2007, P MUS INFRETR EV EXC
   Serrà J, 2010, STUD COMPUT INTELL, V274, P307
   Serrà J, 2009, NEW J PHYS, V11, DOI 10.1088/1367-2630/11/9/093017
   Shi Y., 2011, Proc. Interspeech, P2485
   Six J, 2014, 15 INT SOC MUS INF R
   Sonnleitner R, 2014, INT CONF DIGIT AUDIO, P173
   Sonnleitner R, 2016, IEEE-ACM T AUDIO SPE, V24, P409, DOI 10.1109/TASLP.2015.2509248
   Sukittanon S, 2002, INT CONF ACOUST SPEE, P1773
   Tsai TJ, 2016, IEEE-ACM T AUDIO SPE, V24, P833, DOI 10.1109/TASLP.2016.2526787
   Tsai T.J., 2016, Proc. International Society for Music Information Retrieval Conference (ISMIR), P427
   Voorhees Ellen M., 2001, Trec, V7, P361, DOI DOI 10.1017/S1351324901002789
   Wang A., 2003, P 4 INT SOC MUSIC IN, P7, DOI DOI 10.1109/IITAW.2009.110
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Younessian E., 2010, P TRECVID
NR 61
TC 3
Z9 3
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1569
EP 1582
DI 10.1109/TMM.2017.2669864
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800014
DA 2024-07-18
ER

PT J
AU Rainer, B
   Petscharnig, S
   Timmerer, C
   Hellwagner, H
AF Rainer, Benjamin
   Petscharnig, Stefan
   Timmerer, Christian
   Hellwagner, Hermann
TI Statistically Indifferent Quality Variation: An Approach for Reducing
   Multimedia Distribution Cost for Adaptive Video Streaming Services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive video streaming; MPEG-DASH; quality of experience
AB Forecasts predict that Internet traffic will continue to grow in the near future. A huge share of this traffic is caused by multimedia streaming. The quality of experience (QoE) of such streaming services is an important aspect and inmost cases the goal is to maximize the bit rate which-in some cases-conflicts with the requirements of both consumers and providers. For example, in mobile environments users may prefer a lower bit rate to come along with their data plan. Likewise, providers aim at minimizing bandwidth usage in order to reduce costs by transmitting less data to users while maintaining a high QoE. Today's adaptive video streaming services try to serve users with the highest bit rates that consequently results in high QoE. In practice, however, some of these high bit rate representations may not differ significantly in terms of perceived video quality compared to lower bit rate representations. In this paper, we present a novel approach to determine the statistically indifferent quality variation of adjacent video representations for adaptive video streaming services by adopting standard objective quality metrics and existing QoE models. In particular, whenever the quality variation between adjacent representations is imperceptible from a statistical point of view, the representation with higher bit rate can be substituted with a lower bit rate representation. As expected, this approach results in savings with respect to bandwidth consumption while still providing a high QoE for users. The approach is evaluated subjectively with a crowdsourcing study. Additionally, we highlight the benefits of our approach, by providing a case study that extrapolates possible savings for providers.
C1 [Rainer, Benjamin; Petscharnig, Stefan; Timmerer, Christian; Hellwagner, Hermann] Alpen Adria Univ Klagenfurt, Inst Informat Technol, A-9020 Klagenfurt, Austria.
   [Timmerer, Christian] Bitmovin Inc, Res & Standardizat, Palo Alto, CA 94301 USA.
C3 University of Klagenfurt
RP Rainer, B (corresponding author), Alpen Adria Univ Klagenfurt, Inst Informat Technol, A-9020 Klagenfurt, Austria.
EM benjamin.ainer@itec.aau.at; stefan.petscharnig@itec.aau.at;
   christian.timmerer@bitmovin.com; hermann.hellwagner@itec.aau.at
RI Rainer, Benjamin/AAG-7407-2019
OI Rainer, Benjamin/0000-0003-1954-019X; Timmerer,
   Christian/0000-0002-0031-5243
FU Austrian Science Fund under the CHIST-ERA project CONCERT (A
   Context-Adaptive Content Ecosystem Under Uncertainty) [I1402]; Austrian
   Research Promotion Agency under the project Advanced Ultra High
   Definition Dymamic Adaptive Streaming over HTTP; Austrian Science Fund
   (FWF) [I1402] Funding Source: Austrian Science Fund (FWF)
FX This work was supported in part by the Austrian Science Fund under the
   CHIST-ERA project CONCERT (A Context-Adaptive Content Ecosystem Under
   Uncertainty), project number I1402, and in part by the Austrian Research
   Promotion Agency under the project Advanced Ultra High Definition
   Dymamic Adaptive Streaming over HTTP. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Xiaoqing Zhu.
CR [Anonymous], 2013, EUROPEAN NETWORK QUA
   [Anonymous], 2010, SINTEL DURIAN OPEN M
   [Anonymous], 2012, Tears of Steel - Mango Open Movie Project
   [Anonymous], 2016, CISC VIS NETW IND GL
   [Anonymous], 2008, Subjective video quality assessment methods for multimedia applications. Recommendation P.910
   Hossfeld T, 2015, COMPUT NETW, V81, P320, DOI 10.1016/j.comnet.2015.02.015
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   Li Z., 2014, Proceedings of the 5th ACM Multimedia Systems Conference, MMSys '14, P248
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Lin YC, 2015, IEEE IMAGE PROC, P907, DOI 10.1109/ICIP.2015.7350931
   Moorthy AK, 2010, IEEE T CIRC SYST VID, V20, P587, DOI 10.1109/TCSVT.2010.2041829
   Park J, 2013, IEEE T IMAGE PROCESS, V22, P610, DOI 10.1109/TIP.2012.2219551
   Pechard Stephane, 2008, P INT WORKSH IM MED
   Pinson M., 2002, TECH REP
   Poletiek F. H., 2013, HYPOTHESIS TESTING B
   Rainer B, 2013, INT WORK QUAL MULTIM, P24, DOI 10.1109/QoMEX.2013.6603196
   Sandvine, 2015, GLOB INT PHEN AFR MI
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Toni L, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700294
   Toni Laura., 2014, Proceedings of_the_5th_ACM_Multimedia_Systems_Conference, P271
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu CC, 2013, IEEE T MULTIMEDIA, V15, P1121, DOI 10.1109/TMM.2013.2241043
   Xu YD, 2014, IEEE T MULTIMEDIA, V16, P813, DOI 10.1109/TMM.2014.2300041
NR 24
TC 22
Z9 23
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 849
EP 860
DI 10.1109/TMM.2016.2629761
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500015
DA 2024-07-18
ER

PT J
AU González-Díaz, I
   Birinci, M
   Díaz-de-María, F
   Delp, EJ
AF Gonzalez-Diaz, Ivan
   Birinci, Murat
   Diaz-de-Maria, Fernando
   Delp, Edward J.
TI Neighborhood Matching for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geometric verification; image retrieval; neighborhood matching (NM);
   robust estimation
ID OBJECT RETRIEVAL; SCALE; LOCALIZATION; QUANTIZATION; GEOMETRY; SEARCH;
   MODEL
AB In the last few years, large-scale image retrieval has attracted a lot of attention from the multimedia community. Usual approaches addressing this task first generate an initial ranking of the reference images using fast approximations that do not take into consideration the spatial arrangement of local features in the image (e.g., the bag-of-words paradigm). The top positions of the rankings are then re-estimatedwith verificationmethods that deal with more complex information, such as the geometric layout of the image. This verification step allows pruning of many false positives at the expense of an increase in the computational complexity, whichmay prevent its application to large-scale retrieval problems. This paper describes a geometric method known as neighborhood matching (NM), which revisits the keypointmatching process by considering a neighborhood around each keypoint and improves the efficiency of a geometric verification step in the image search system. Multiple strategies are proposed and compared to incorporate NM into a large-scale image retrieval framework. A detailed analysis and comparison of these strategies and baseline methods have been investigated. The experiments show that the proposed method not only improves the computational efficiency, but also increases the retrieval performance and outperforms state-of-the-artmethods in standard datasets, such as the Oxford 5 k and 105 k datasets, for which the spatial verification step has a significant impact on the system performance.
C1 [Gonzalez-Diaz, Ivan; Diaz-de-Maria, Fernando] Univ Carlos III Madrid, Dept Signal Theory & Commun, Madrid 28045, Spain.
   [Birinci, Murat] Tampere Univ Technol, Dept Signal Proc, Tampere 33720, Finland.
   [Delp, Edward J.] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
C3 Universidad Carlos III de Madrid; Tampere University; Purdue University
   System; Purdue University
RP González-Díaz, I (corresponding author), Univ Carlos III Madrid, Dept Signal Theory & Commun, Madrid 28045, Spain.
EM igonzalez@tsc.uc3m.es; murat.birinci@tut.fi; fdiaz@tsc.uc3m.es;
   ace@ecn.purdue.edu
RI Delp, Edward J/C-3616-2013; de María, Fernando Díaz/E-8048-2011; Díaz,
   Iván González/L-5103-2014
OI de María, Fernando Díaz/0000-0002-6437-914X; Díaz, Iván
   González/0000-0003-4644-8479; Delp, Edward/0000-0002-2909-7323
FU Spanish Ministry of Economy and Competitiveness [TEC2014-53390-P,
   TEC2014-61729-EXP]
FX This work was supported in part by the National Grant TEC2014-53390-P
   and National Grant TEC2014-61729-EXP of the Spanish Ministry of Economy
   and Competitiveness.
CR [Anonymous], 2012, OFC NFOEC
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Birinci M, 2011, 2011 IEEE DIGITAL SIGNAL PROCESSING WORKSHOP AND IEEE SIGNAL PROCESSING EDUCATION WORKSHOP (DSP/SPE), P157, DOI 10.1109/DSP-SPE.2011.5739204
   Bunke H, 2012, PATTERN RECOGN LETT, V33, P811, DOI 10.1016/j.patrec.2011.04.017
   Chou CL, 2015, IEEE T MULTIMEDIA, V17, P382, DOI 10.1109/TMM.2015.2391674
   Chum O., 2004, Proc. of the Asian Conference on Computer Vision ACCV, V2, P812
   Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   Duchenne O, 2011, IEEE I CONF COMP VIS, P1792, DOI 10.1109/ICCV.2011.6126445
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gibert J, 2012, PATTERN RECOGN, V45, P3072, DOI 10.1016/j.patcog.2012.01.009
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   González-Díaz I, 2014, IEEE T MULTIMEDIA, V16, P169, DOI 10.1109/TMM.2013.2286083
   Gordo A., 2016, P 14 EUR C COMP VIS
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Huang J, 2011, IEEE T MULTIMEDIA, V13, P653, DOI 10.1109/TMM.2011.2127463
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Kristo, 2013, IEEE IMAGE PROC, P3354, DOI 10.1109/ICIP.2013.6738691
   Lampert CH, 2009, IEEE I CONF COMP VIS, P987, DOI 10.1109/ICCV.2009.5459359
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li W, 2015, IEEE T MULTIMEDIA, V17, P967, DOI 10.1109/TMM.2015.2428996
   Li XC, 2015, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR.2015.7299151
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9
   Mikulík A, 2010, LECT NOTES COMPUT SC, V6313, P1
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Perd'och M, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPRW.2009.5206529
   Philbin J., OXFORD BUILDING DATA
   Philbin J., 2008, P CVPR, P1
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Shen XH, 2012, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR.2012.6248031
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Wei SK, 2013, IEEE T CYBERNETICS, V43, P2216, DOI 10.1109/TCYB.2013.2245890
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Xie HT, 2011, IEEE T MULTIMEDIA, V13, P1319, DOI 10.1109/TMM.2011.2167224
   Xie LX, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P3, DOI 10.1145/2671188.2749289
   Xie LX, 2015, IEEE T IMAGE PROCESS, V24, P4287, DOI 10.1109/TIP.2015.2432673
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Yi Ren, 2014, 2014 4th International Conference on Image Processing Theory, Tools and Applications (IPTA). Proceedings, P1, DOI 10.1109/IPTA.2014.7001967
   Zhang E., 2009, AVERAGE PRECISION EN, P192
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zheng L., 2014, CORR
   Zheng YT, 2009, VISUAL COMPUT, V25, P13, DOI 10.1007/s00371-008-0294-0
   Zhong ZY, 2015, IEEE T MULTIMEDIA, V17, P1391, DOI 10.1109/TMM.2015.2446201
   Zhou W, 2013, PHYSIOL REP, V1, DOI 10.1002/phy2.110
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
NR 56
TC 13
Z9 13
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 544
EP 558
DI 10.1109/TMM.2016.2616298
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400010
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Ren, JF
   Jiang, XD
   Yuan, JS
   Magnenat-Thalmann, N
AF Ren, Jianfeng
   Jiang, Xudong
   Yuan, Junsong
   Magnenat-Thalmann, Nadia
TI Sound-Event Classification Using Robust Texture Features for Robot
   Hearing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Band-dependent local binary pattern; band-independent local binary
   pattern; multi-channel local binary pattern; robot hearing; sound-event
   classification
ID LOCAL BINARY PATTERNS; AUTOMATIC SPEECH RECOGNITION; VISUAL RECOGNITION;
   FACE RECOGNITION; AUDIO CLASSIFICATION; IMAGE FEATURE; LBP FEATURES;
   SUBSPACE; DESCRIPTORS; MECHANISM
AB Sound-event classification often utilizes timefrequency analysis, which produces an image-like spectrogram. Recent approaches such as spectrogram image features and subband power distribution image features extract the image local statistics such as mean and variance from the spectrogram. They have demonstrated good performance. However, we argue that such simple image statistics cannot well capture the complex texture details of the spectrogram. Thus, we propose to extract the local binary pattern (LBP) from the logarithm of the Gammatonelike spectrogram. However, the LBP feature is sensitive to noise. After analyzing the spectrograms of sound events and the audio noise, we find that the magnitude of pixel differences, which is discarded by the LBP feature, carries important information for sound-event classification. We thus propose a multichannel LBP feature via pixel difference quantization to improve the robustness to the audio noise. In view of the differences between spectrograms and natural images, and the reliability issues of LBP features, we propose two projection-based LBP features to better capture the texture information of the spectrogram. To validate the proposed multichannel projection-based LBP features for robot hearing, we have built a new sound-event classification database, the NTUSEC database, in the context of social interaction between human and robot. It is publicly available to promote research on soundevent classification in a social context. The proposed approaches are compared with the state of the art on the RWCP database and the NTU-SEC database. They consistently demonstrate superior performance under various noise conditions.
C1 [Ren, Jianfeng; Jiang, Xudong; Yuan, Junsong] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Magnenat-Thalmann, Nadia] Nanyang Technol Univ, Inst Media Innovat, Singapore 637553, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Ren, JF (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM jfren@ntu.edu.sg; exdjiang@ntu.edu.sg; jsyuan@ntu.edu.sg;
   nadiathalmann@ntu.edu.sg
RI Jiang, Xudong/B-1555-2008; Ren, Jianfeng/D-4160-2017; Yuan,
   Junsong/R-4352-2019; Thalmann, Nadia/AAK-5195-2021
OI Jiang, Xudong/0000-0002-9104-2315; Thalmann, Nadia/0000-0002-1459-5960;
   Yuan, Junsong/0000-0002-7901-8793
FU Singapore National Research Foundation under its International Research
   Centre Singapore Funding Initiative; Singapore Future Systems and
   Technology Directorate [MINDEF-NTU-DIRP/2014/01]
FX This work was supported in part by the Singapore National Research
   Foundation under its International Research Centre Singapore Funding
   Initiative and administered by the Interactive Digital Media Programme
   Office, and in part by the Singapore Future Systems and Technology
   Directorate under Project MINDEF-NTU-DIRP/2014/01.
CR [Anonymous], 1997, The HTK book
   [Anonymous], 2000, ISMIR
   [Anonymous], 2001, P COST G6 C DIG AUD
   Arashloo SR, 2014, IEEE T MULTIMEDIA, V16, P2099, DOI 10.1109/TMM.2014.2362855
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Chen ZF, 2014, IEEE T MULTIMEDIA, V16, P1000, DOI 10.1109/TMM.2014.2307692
   Chu S, 2009, IEEE T AUDIO SPEECH, V17, P1142, DOI 10.1109/TASL.2009.2017438
   Costa YMG, 2012, SIGNAL PROCESS, V92, P2723, DOI 10.1016/j.sigpro.2012.04.023
   Cowling M, 2003, PATTERN RECOGN LETT, V24, P2895, DOI 10.1016/S0167-8655(03)00147-8
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Dennis J, 2013, IEEE T AUDIO SPEECH, V21, P367, DOI 10.1109/TASL.2012.2226160
   Dennis J, 2011, IEEE SIGNAL PROC LET, V18, P130, DOI 10.1109/LSP.2010.2100380
   Fang YC, 2008, INT C WAVEL ANAL PAT, P373, DOI 10.1109/ICWAPR.2008.4635807
   Geng C, 2011, PATTERN RECOGN, V44, P2565, DOI 10.1016/j.patcog.2011.03.011
   Gerosa L., 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P1216
   Ghoraani B, 2011, IEEE T AUDIO SPEECH, V19, P2197, DOI 10.1109/TASL.2011.2118753
   Guo GD, 2003, IEEE T NEURAL NETWOR, V14, P209, DOI 10.1109/TNN.2002.806626
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Tran HD, 2011, IEEE T AUDIO SPEECH, V19, P1556, DOI 10.1109/TASL.2010.2093519
   Jianfeng Ren, 2015, Pattern Recognition, V48, P3180, DOI 10.1016/j.patcog.2015.02.001
   Jiang XD, 2011, IEEE SIGNAL PROC MAG, V28, P16, DOI 10.1109/MSP.2010.939041
   Jiang XD, 2009, IEEE T PATTERN ANAL, V31, P931, DOI 10.1109/TPAMI.2008.258
   Kim MJ, 2012, IEEE T MULTIMEDIA, V14, P1390, DOI 10.1109/TMM.2012.2195481
   Kleinschmidt M, 2002, ACTA ACUST UNITED AC, V88, P416
   Kobayashi Takumi, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3052, DOI 10.1109/ICASSP.2014.6854161
   Liao S, 2009, IEEE T IMAGE PROCESS, V18, P1107, DOI 10.1109/TIP.2009.2015682
   Lyon RF, 2010, IEEE SIGNAL PROC MAG, V27, P131, DOI 10.1109/MSP.2010.937498
   Matsui T, 2011, EUR SIGNAL PR CONF, P724
   Muhammad G., 2009, 2009 Fourth International Conference on Embedded and Multimedia Computing, P1
   Nakamura Satoshi, 2000, LREC
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Paraskevas I, 2003, PROCEEDINGS EC-VIP-MC 2003, VOLS 1 AND 2, P187
   Qian S, 1999, IEEE SIGNAL PROC MAG, V16, P52, DOI 10.1109/79.752051
   Ren J., 2016, P SOC PHOTO-OPT INS, V10011
   Ren JF, 2015, IEEE SIGNAL PROC LET, V22, P2373, DOI 10.1109/LSP.2015.2481435
   Ren JF, 2013, IEEE IMAGE PROC, P3680, DOI 10.1109/ICIP.2013.6738759
   Ren JF, 2015, IEEE T IMAGE PROCESS, V24, P1893, DOI 10.1109/TIP.2015.2409554
   Ren JF, 2014, IEEE SIGNAL PROC LET, V21, P1346, DOI 10.1109/LSP.2014.2336252
   Ren JF, 2013, IEEE IMAGE PROC, P2494, DOI 10.1109/ICIP.2013.6738514
   Ren JF, 2013, INT CONF ACOUST SPEE, P2400, DOI 10.1109/ICASSP.2013.6638085
   Ren JF, 2013, IEEE T IMAGE PROCESS, V22, P4049, DOI 10.1109/TIP.2013.2268976
   Satpathy A, 2014, IEEE T IMAGE PROCESS, V24, P1953, DOI 10.1109/TIP.2014.2310123
   Satpathy A, 2014, IEEE T IMAGE PROCESS, V23, P287, DOI 10.1109/TIP.2013.2264677
   Slaney M., 1998, Interval Res. Corp. Tech. Rep, V10, P1194
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Temko A., 2005, P ICASSP 05, V5, P505
   Valero X, 2012, IEEE T MULTIMEDIA, V14, P1684, DOI 10.1109/TMM.2012.2199972
   VARGA A, 1993, SPEECH COMMUN, V12, P247, DOI 10.1016/0167-6393(93)90095-3
   Wang JC, 2014, IEEE T AUTOM SCI ENG, V11, P607, DOI 10.1109/TASE.2013.2285131
   Wang WC, 2010, 2010 8TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P6375, DOI 10.1109/WCICA.2010.5554337
   Weninger F, 2011, INT CONF ACOUST SPEE, P337
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224
   Xiao Y, 2014, IEEE T IMAGE PROCESS, V23, P823, DOI 10.1109/TIP.2013.2295756
   Yan SC, 2007, INT CONF ACOUST SPEE, P629
   Ye JX, 2013, INT CONF ACOUST SPEE, P808, DOI 10.1109/ICASSP.2013.6637760
   Yu G., 2008, CORR
   Yuan JS, 2011, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2011.5995476
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
NR 60
TC 43
Z9 46
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 447
EP 458
DI 10.1109/TMM.2016.2618218
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400003
DA 2024-07-18
ER

PT J
AU Yang, J
   Zhu, KJ
   Ran, YY
   Cai, WZ
   Yang, EZ
AF Yang, Jian
   Zhu, Kunjie
   Ran, Yongyi
   Cai, Weizhe
   Yang, Enzhong
TI Joint Admission Control and Routing Via Approximate Dynamic Programming
   for Streaming Video Over Software-Defined Networking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Admission control; approximate dynamic programming; routing algorithm;
   software-defined networking (SDN); video streaming service
ID QOS-AWARE VIRTUALIZATION; WIRELESS NETWORKS; PACKET NETWORKS;
   ACCESS-CONTROL; QUALITY; OPTIMIZATION; ALGORITHM; SERVICES; SYSTEM
AB This paper considers the optimization problem of joint admission control and routing for the video streaming service in wired software-defined networking (SDN). With the aid of the network operating system, SDN is able to support the dynamic nature of future network functions and intelligent applications. Against this changing network landscape, we rely on Flow Visor-based virtualization in the context of OpenFlow-based wired SDN to design an open optimization architecture for the joint admission control and routing, which supports flexible and agile deployment of advanced joint admission control and routing strategies. Following this architecture, we interpret the joint admission control and routing problem into the Markov decision process for maximizing the overall " revenue." In order to solve the issue of the curses of dimensionality, we invoke the function approximation technique in the context of approximate dynamic programming to conceive an online learning framework. By applying kernel-based autonomous feature extraction into the function approximation, we develop an approximate dynamic programming-based joint admission control and routing for video streaming service, which is apt to be implemented in the proposed open architecture. An emulation platform based on Flow Visor, POX, and Mininet is constructed for demonstrating the success of the proposed solution. The experimental results are presented to show the performance improvement of the proposed scheme by comparing it with the Q-learning algorithm and open shortest path first-based benchmark scheme.
C1 [Yang, Jian; Zhu, Kunjie; Ran, Yongyi; Cai, Weizhe; Yang, Enzhong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Yang, J (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
EM jianyang@ustc.edu.cn; zkj4873@mail.ustc.edu.cn; yyran@ustc.edu.cn;
   cwz@mail.ustc.edu.cn; ezhyang@mail.ustc.edu.cn
FU State Key Program of National NSF of China [61233003]; National NSF of
   China [61573329]; Fundamental Research Funds for the Central
   Universities; Youth Innovation Promotion Association CAS
FX This work was supported in part by the State Key Program of National NSF
   of China under Grant 61233003, in part by the National NSF of China
   under Grant 61573329, in part by the Fundamental Research Funds for the
   Central Universities, and in part by the Youth Innovation Promotion
   Association CAS.
CR Alwakeel S., 2011, P IEEE INT C MULT CO, P1
   [Anonymous], 1996, LIDSP2349 MIT
   [Anonymous], 2014, P IEEE NETW OP MAN S
   [Anonymous], 2016, CISCO VISUAL NETWORK
   Bakiras S, 2002, IEEE T BROADCAST, V48, P281, DOI 10.1109/TBC.2002.806194
   Bertsekas D.P., 1996, Athena Scientific, V7, P15
   Bertsekas DP, 2017, DYNAMIC PROGRAMMING
   Blasco P, 2013, IEEE T WIREL COMMUN, V12, P1872, DOI 10.1109/TWC.2013.030413.121120
   Cetinkaya C, 2001, IEEE T MULTIMEDIA, V3, P69, DOI 10.1109/6046.909595
   Chen C, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2337277
   Chen LJ, 2005, IEEE INFOCOM SER, P2212
   Dhurandher SK, 2015, IEEE SYST J, V9, P595, DOI 10.1109/JSYST.2013.2296336
   Domzal J, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING, NETWORKING AND COMMUNICATIONS (ICNC), P265, DOI 10.1109/ICCNC.2015.7069352
   Elsayed KMF, 2005, IEEE T MULTIMEDIA, V7, P563, DOI 10.1109/TMM.2005.846779
   Engel Y, 2004, IEEE T SIGNAL PROCES, V52, P2275, DOI 10.1109/TSP.2004.830985
   Haykin S. S., 2008, ADAPTIVE FILTER THEO
   Hew SL, 2007, IEEE T AUTOMAT CONTR, V52, P1442, DOI 10.1109/TAC.2007.902746
   Huang D., 2011, REINFORCEMENT LEARNI, P535
   Ifeachor E.C., 2002, Digital signal processing: a practical approach
   Jain R, 2013, IEEE COMMUN MAG, V51, P24, DOI 10.1109/MCOM.2013.6658648
   Latré S, 2011, IEEE COMMUN MAG, V49, P94, DOI 10.1109/MCOM.2011.6094011
   Latré S, 2009, 2009 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT - WORKSHOPS, P161, DOI 10.1109/INMW.2009.5195955
   Lin SC, 2016, COMPUT NETW, V96, P69, DOI 10.1016/j.comnet.2015.08.003
   Liu CH, 2014, IEEE T WIREL COMMUN, V13, P604, DOI 10.1109/TWC.2013.010214.121856
   Lu M.-H., 2007, PACKET VIDEO 2007, P52
   Marbach P, 2000, IEEE J SEL AREA COMM, V18, P197, DOI 10.1109/49.824797
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Mellouk A, 2007, INT J COMMUN SYST, V20, P1113, DOI 10.1002/dac.858
   Miyata S., 2013, P IEEE PAC RIM C COM, P165
   Mundur P, 2005, IEEE T CIRC SYST VID, V15, P844, DOI 10.1109/TCSVT.2005.848351
   Namazi Siamak., 2013, Sanctions and Medical Supply Shortages in Iran, P1, DOI DOI 10.1109/SDN4FNS.2013.6702551
   Nasser N, 2009, IEEE T MULTIMEDIA, V11, P786, DOI 10.1109/TMM.2009.2017612
   Neely MJ, 2005, IEEE J SEL AREA COMM, V23, P89, DOI 10.1109/JSAC.2004.837349
   Oulai D, 2007, IEEE COMMUN LETT, V11, P216, DOI 10.1109/LCOMM.2007.061618
   Park H, 2009, IEEE T CIRC SYST VID, V19, P1781, DOI 10.1109/TCSVT.2009.2026983
   Porxas AX, 2015, IEEE ICC, P5771, DOI 10.1109/ICC.2015.7249242
   Puterman Martin L., 2005, MARKOV DECISION PROC, pi, DOI 10.1002/
   Qadir QM, 2015, IEEE T MULTIMEDIA, V17, P711, DOI 10.1109/TMM.2015.2416637
   Sezer S, 2013, IEEE COMMUN MAG, V51, P36, DOI 10.1109/MCOM.2013.6553676
   Sutton R., 1998, Reinforcement Learning: An Introduction
   Testolin Alberto, 2014, 2014 13th Annual Mediterranean Ad Hoc Networking Workshop (MED-HOC-NET), P31, DOI 10.1109/MedHocNet.2014.6849102
   Tseng YH, 2008, IEEE T MULTIMEDIA, V10, P470, DOI 10.1109/TMM.2008.917423
   Van Vaerenbergh S., 2012, Machine Learning for Signal Processing (MLSP), 2012 IEEE International Workshop on, P1
   Wang WJ, 2003, NEUROCOMPUTING, V55, P643, DOI 10.1016/S0925-2312(02)00632-X
   Weber S, 2005, IEEE ACM T NETWORK, V13, P1275, DOI 10.1109/TNET.2005.860105
   Wu DL, 2010, IEEE T CIRC SYST VID, V20, P1721, DOI 10.1109/TCSVT.2010.2057014
   Yu R, 2007, IEEE VTS VEH TECHNOL, P2776, DOI 10.1109/VETECS.2007.570
   Zhang SY, 2010, IEEE T WIREL COMMUN, V9, P1370, DOI 10.1109/TWC.2010.04.081437
   ZIMMERMAN R, 2003, P ACM MULT C, P75
NR 49
TC 16
Z9 16
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 619
EP 631
DI 10.1109/TMM.2016.2629280
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400016
DA 2024-07-18
ER

PT J
AU Alam, MR
   Bennamoun, M
   Togneri, R
   Sohel, F
AF Alam, Mohammad Rafiqul
   Bennamoun, Mohammed
   Togneri, Roberto
   Sohel, Ferdous
TI A Joint Deep Boltzmann Machine (jDBM) Model for Person Identification
   Using Mobile Phone Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual biometrics; deep Boltzmann machines; joint features
AB We propose an audio-visual person identification approach based on a joint deep Boltzmannmachine (jDBM) model. The proposed jDBM model is trained in three steps: 1) learning the unimodal DBM models corresponding to the speech and facial image modalities, 2) learning the shared layer parameters using a joint restricted Boltzmann machine (jRBM) model, and 3) the fine-tuning of the jDBM model after the initialization with the parameters of the unimodal DBMs and the shared layer. The activation probabilities of the units of the shared layer are used as the joint features and a logistic regression classifier is used for the combined speech and facial image recognition. We show that by learning the shared layer parameters using a jRBM, a higher accuracy can be achieved compared to the greedy layer-wise initialization. The performance of our proposed model is also compared with a state-of-the art support vector machine (SVM), deep belief network (DBN), and the deep auto-encoder (DAE) models. In addition, our experimental results show that the joint representations obtained from the proposed jDBM model are robust to noise and missing information. Experiments were carried out on the challenging MOBIO database, which includes audiovisual data captured using mobile phones.
C1 [Alam, Mohammad Rafiqul; Bennamoun, Mohammed] Univ Western Australia, Dept Comp Sci & Software Engn, Crawley, WA 6009, Australia.
   [Togneri, Roberto] Univ Western Australia, Dept Elect Elect & Comp Engn, Crawley, WA 6009, Australia.
   [Sohel, Ferdous] Murdoch Univ, Sch Engn & Informat Technol, Murdoch, WA 6150, Australia.
C3 University of Western Australia; University of Western Australia;
   Murdoch University
RP Alam, MR (corresponding author), Univ Western Australia, Dept Comp Sci & Software Engn, Crawley, WA 6009, Australia.
EM mohammad.alam@research.uwa.edu.au; mohammed.bennamoun@uwa.edu.au;
   roberto.togneri@uwa.edu.au; F.Sohel@murdoch.edu.au
RI Sohel, Ferdous/C-2428-2013; Togneri, Roberto/C-2466-2013; Bennamoun,
   Mohammed/C-2789-2013
OI Sohel, Ferdous/0000-0003-1557-4907; Bennamoun,
   Mohammed/0000-0002-6603-3257; Togneri, Roberto/0000-0002-3778-4633
FU Australian Research Council [DP110103336, DE120102960]
FX This work was supported by the Australian Research Council under Grant
   DP110103336 and Grant DE120102960.
CR Alam M. R., 2015, P 7 PAC RIM S IM VID, P631
   Aleksic PS, 2006, P IEEE, V94, P2025, DOI 10.1109/JPROC.2006.886017
   [Anonymous], 2015, 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA), Paris, DOI 10.1109/iisa.2015.7387997
   [Anonymous], 2010, Proceedings of the International Conference on Machine Learning ICML'10
   [Anonymous], ODYSSEY
   [Anonymous], P OD
   [Anonymous], P BIOM TECHN FOR SCI
   [Anonymous], 2013, Biometrics (ICB), 2013 International Conference on
   [Anonymous], 2013, SPEECH LANG PROCESS
   Bishop Christopher M, 2006, PATTERN RECOGNITION, DOI DOI 10.1117/1.2819119
   Brookes M., 1997, SOFTWARE, P47
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Duong N, 2015, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2015.7299111
   Cho K., 2011, Proceedings of the 28th International Conference on Machine Learning, ICML, P105
   Cho K.H., 2013, Matlab code for restricted/deep Boltzmann machines and autoencoders
   Cho K, 2013, LECT NOTES COMPUT SC, V8131, P106, DOI 10.1007/978-3-642-40728-4_14
   Desjardins G., 2010, P 13 INT C ARTIFICIA, ppp 145
   Ghahabi Omid, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1700, DOI 10.1109/ICASSP.2014.6853888
   Hinton G. E., 2012, 12070580 ARXIV
   Khoury E, 2013, INT CONF BIOMETR
   Liu W, 2015, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR.2015.7298920
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   McCool C, 2012, IEEE INT CONF MULTI, P635, DOI 10.1109/ICMEW.2012.116
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Salakhutdinov R., 2009, ARTIFICIAL INTELLIGE, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sui C, 2015, IEEE I CONF COMP VIS, P154, DOI 10.1109/ICCV.2015.26
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Togneri R, 2011, IEEE CIRC SYST MAG, V11, P23, DOI 10.1109/MCAS.2011.941079
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
NR 32
TC 19
Z9 22
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 317
EP 326
DI 10.1109/TMM.2016.2615524
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800008
DA 2024-07-18
ER

PT J
AU Chou, CL
   Chen, HT
   Lee, SY
AF Chou, Chien-Li
   Chen, Hua-Tsung
   Lee, Suh-Yin
TI Multimodal Video-to-Near-Scene Annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Near-duplicate segment alignment; near-duplicate video retrieval;
   near-scene detection; near-scene annotation; video annotation
ID COPY-DETECTION; LOCALIZATION; RETRIEVAL; FRAMEWORK; SIMILARITY
AB Traditional video annotation approaches focus on annotating keyframes/shots or whole videos with semantic keywords. However, the extraction processes of keyframes/shots might lack semantic meanings, and it is hard to use a few keywords to describe the content of a long video with multiple topics. In this work, near- scenes, which contain similar concepts, topics, or semantic meanings, are designed for better video content understanding and annotation. We propose a novel framework of hierarchical video-to-near-scene annotation not only to preserve but also to purify the semantic meanings of near- scenes. To detect near-scenes, a pattern- based prefix tree is first constructed to fast retrieve near- duplicate videos. Then, the videos containing similar near- duplicate segments and similar keywords are clustered with consideration of multimodal features including visual and textual features. To enhance the precision of near-scene detection, a pattern-to-intensity- mark (PIM) method is proposed to perform precise frame- level near- duplicate segment alignment. For each near- scene, a video-to-concept distribution model is designed to analyze the representativeness of keywords and discriminations of clusters by the proposed potential term frequency and inverse document frequency and entropy. Tags are ranked according to video-to-concept distribution scores, and the tags with the highest scores are propagated to near-scenes detected. Extensive experiments demonstrate that the proposed PIM outperforms state-of-the-art approaches compared in terms of quality segments and quality frames for near-scene detection. Furthermore, the proposed framework of hierarchical video- to-near- scene annotation can achieve high quality of near-scene annotation in terms of mean average precision.
C1 [Chou, Chien-Li; Lee, Suh-Yin] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 30010, Taiwan.
   [Chou, Chien-Li] MediaTek Inc, Hsinchu 30010, Taiwan.
   [Chen, Hua-Tsung] Natl Chiao Tung Univ, Informat & Commun Technol Labs, Hsinchu 30010, Taiwan.
C3 National Yang Ming Chiao Tung University; Mediatek Incorporated;
   National Yang Ming Chiao Tung University
RP Chen, HT (corresponding author), Natl Chiao Tung Univ, Informat & Commun Technol Labs, Hsinchu 30010, Taiwan.
EM fallwind@cs.nctu.edu.tw; huatsung@cs.nctu.edu.tw; sylee@cs.nctu.edu.tw
FU [MOST-103-2221E-009-154];  [MOST-103-2218-E-009-020]; 
   [MOST-105-2221-E-009-065];  [ICTL103-Q528];  [ATU-103-W958]
FX This work was supported in part by MOST-103-2221E-009-154,
   MOST-103-2218-E-009-020, MOST-105-2221-E-009-065, ICTL103-Q528, and
   ATU-103-W958.
CR Altadmri A, 2014, MULTIMED TOOLS APPL, V72, P1167, DOI 10.1007/s11042-013-1363-6
   [Anonymous], 2009, ACM INT C IM VID RET
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], P INT C MULT
   [Anonymous], 2009, P ACM INT C MULT, DOI DOI 10.1145/1631272.1631295
   [Anonymous], P IEEE INT C MULT EX
   Bergroth L, 2000, SPIRE 2000: SEVENTH INTERNATIONAL SYMPOSIUM ON STRING PROCESSING AND INFORMATION RETRIEVAL - PROCEEDINGS, P39, DOI 10.1109/SPIRE.2000.878178
   Bianco S, 2015, COMPUT VIS IMAGE UND, V131, P88, DOI 10.1016/j.cviu.2014.06.015
   Chamasemani Fereshteh Falah, 2015, Journal of Applied Sciences, V15, P256, DOI 10.3923/jas.2015.256.263
   Chaudhry R, 2009, PROC CVPR IEEE, P1932, DOI 10.1109/CVPRW.2009.5206821
   Chou C.-L., 2014, IEEE International Conference on Multimedia and Expo (ICME), P1
   Chou CL, 2015, IEEE T MULTIMEDIA, V17, P382, DOI 10.1109/TMM.2015.2391674
   Douze M, 2010, IEEE T MULTIMEDIA, V12, P257, DOI 10.1109/TMM.2010.2046265
   Esmaeili MM, 2011, IEEE T INF FOREN SEC, V6, P213, DOI 10.1109/TIFS.2010.2097593
   Karasev V, 2014, PROC CVPR IEEE, P2131, DOI 10.1109/CVPR.2014.273
   Lavrenko V., 2004, P 2004 IEEE INT C AC
   Law-To J., 2007, Muscle-VCD-2007: a live benchmark for video copy detection
   Ligang Zheng, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2537, DOI 10.1109/ICIP.2011.6116179
   Markatopoulou Fotini, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P1, DOI 10.1007/978-3-319-04114-8_1
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Min HS, 2012, IEEE T CIRC SYST VID, V22, P1174, DOI 10.1109/TCSVT.2012.2197080
   Moxley E, 2010, IEEE T MULTIMEDIA, V12, P184, DOI 10.1109/TMM.2010.2041101
   Moxley E, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P685, DOI 10.1109/ICME.2008.4607527
   Nakamura Y, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P393, DOI 10.1145/266180.266391
   Ngo C.W., 2009, Proc. of TRECVID, P415
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Pedersen T, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P1024
   Rasheed Z., 2003, IEEE T CIRCUITS SYST, V1, P1
   Ren W, 2005, LECT NOTES COMPUT SC, V3687, P693
   Roopalakshmi R, 2013, SIGNAL PROCESS, V93, P2339, DOI 10.1016/j.sigpro.2012.06.004
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
   Shang L., 2010, Proceedings of the International Conference on Multimedia, P531
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Su JH, 2011, IEEE T MULTIMEDIA, V13, P530, DOI 10.1109/TMM.2011.2129502
   Sun S S, 2011, P IEEE ICC, P1
   Tian YH, 2015, ACM T INFORM SYST, V33, DOI 10.1145/2699662
   Tian YH, 2013, IEEE MULTIMEDIA, V20, P72, DOI 10.1109/MMUL.2012.62
   Tseng VS, 2005, IEEE ACM T COMPUT BI, V2, P355, DOI 10.1109/TCBB.2005.56
   Vijayanarasimhan S, 2012, LECT NOTES COMPUT SC, V7576, P496, DOI 10.1007/978-3-642-33715-4_36
   Vondrick C, 2013, INT J COMPUT VISION, V101, P184, DOI 10.1007/s11263-012-0564-1
   Wang C., 2008, ACM SIGIR, P355, DOI DOI 10.1145/1390334.1390396
   Wang FS, 2007, LECT NOTES COMPUT SC, V4351, P279
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang XJ, 2012, P IEEE, V100, P2705, DOI 10.1109/JPROC.2012.2193109
   Wei SK, 2015, MULTIMEDIA SYST, V21, P207, DOI 10.1007/s00530-014-0398-5
   Wu A. G., 2007, P ACM MM, P218
   Wu GL, 2013, IEEE MULTIMEDIA, V20, P47, DOI 10.1109/MMUL.2013.13
   Wu X, 2009, IEEE T MULTIMEDIA, V11, P196, DOI 10.1109/TMM.2008.2009673
   Wu ZP, 2014, INT J MULTIMED INF R, V3, P1, DOI 10.1007/s13735-013-0049-1
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yi J, 2013, IEEE T MULTIMEDIA, V15, P1400, DOI 10.1109/TMM.2013.2250266
   Yilmaz E., 2008, P 31 ANN INT ACM SIG
   Yu-Gang Jiang, 2016, IEEE Transactions on Big Data, V2, P32, DOI 10.1109/TBDATA.2016.2530714
   Zhang J. R., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P842, DOI 10.1109/ICME.2012.111
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
   Zhou XM, 2012, IEEE T MULTIMEDIA, V14, P1220, DOI 10.1109/TMM.2012.2194481
   Zhu L., 2016, P 2016 IEEE C COMP V
   Zhu XF, 2013, IEEE T MULTIMEDIA, V15, P633, DOI 10.1109/TMM.2012.2233723
NR 60
TC 6
Z9 8
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 354
EP 366
DI 10.1109/TMM.2016.2614426
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800011
DA 2024-07-18
ER

PT J
AU Florea, R
   Munteanu, A
   Lu, SP
   Schelkens, P
AF Florea, Ruxandra
   Munteanu, Adrian
   Lu, Shao-Ping
   Schelkens, Peter
TI Wavelet-Based <i>L</i><sub>∞</sub> Semi-regular Mesh Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE L-infinity coding; near-lossless compression; semi-regular meshes;
   subdivision-based wavelets
ID LOSSLESS IMAGE COMPRESSION; TRANSMISSION; WATERMARKING; MODELS; SCHEME
AB Polygonal meshes are popular three-dimensional virtual representations employed in a wide range of applications. Users have very high expectations with respect to the accuracy of these virtual representations, fueling a steady increase in the processing power and performance of graphics processing hardware. This accuracy is closely related to how detailed the virtual representations are. The more detailed these representations become, the higher the amount of data that will need to be displayed, stored, or transmitted. Efficient compression techniques are of critical importance in this context. State-of-the-art compression performance of semi-regular mesh coding systems has been achieved through the use of subdivision-based wavelet coding techniques. However, the vast majority of these codecs are optimized with respect to the L-2 distortion metric, i.e., the average error. This makes them unsuitable for applications where each input signal sample has a certain significance. To alleviate this problem, we propose to optimize the mesh codec with respect to the L-infinity metric, which allows for the control of the local reconstruction error. This paper proposes novel data-dependent formulations for the L-infinity distortion. The proposed L-infinity estimators are incorporated in a state-of-the-art wavelet-based semi-regular mesh codec. The resulting coding system offers scalability in L-infinity sense. The experiments demonstrate the advantages of L-infinity coding in providing a tight control on the local reconstruction error. Furthermore, the proposed data-dependent L-infinity approaches significantly improve estimation accuracy, reducing the classical low-rate gap between the estimated and actual L-infinity distortion observed for previous L-infinity estimators.
C1 [Florea, Ruxandra; Munteanu, Adrian; Lu, Shao-Ping; Schelkens, Peter] Vrije Univ Brussel, Dept Elect & Informat, B-1050 Brussels, Belgium.
C3 Vrije Universiteit Brussel
RP Florea, R (corresponding author), Vrije Univ Brussel, Dept Elect & Informat, B-1050 Brussels, Belgium.
EM fruxandr@etrovub.be; acmuntea@etrovub.be; splu@etrovub.be;
   pschelke@etrovub.be
RI Munteanu, Adrian/HKO-9955-2023; Schelkens, Peter/B-7831-2008
OI Munteanu, Adrian/0000-0001-7290-0428; Schelkens,
   Peter/0000-0003-0908-1655
FU 3DLicornea project - Brussels Region (Brussels Institute for Research
   and Innovation - Innoviris)
FX This work was supported by the 3DLicornea project funded by the Brussels
   Region (Brussels Institute for Research and Innovation - Innoviris).
CR Ahn JK, 2013, IEEE T MULTIMEDIA, V15, P485, DOI 10.1109/TMM.2012.2235417
   Alecu A, 2004, IEEE SIGNAL PROC LET, V11, P367, DOI 10.1109/LSP.2003.822599
   Alecu A, 2006, IEEE T IMAGE PROCESS, V15, P2499, DOI 10.1109/TIP.2006.877416
   Alregib G, 2005, ACM T GRAPHIC, V24, P182, DOI 10.1145/1061347.1061349
   Andries B., 2016, VISUAL COMPUT, V32, P1
   [Anonymous], 1993, Modeling in Computer Graphics
   Ansari R, 1998, J ELECTRON IMAGING, V7, P486, DOI 10.1117/1.482591
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   Avcibas I, 2002, IEEE SIGNAL PROC LET, V9, P312, DOI 10.1109/LSP.2002.804129
   Aviles M., 2005, PROGRESSIVE LOWER TR, V3767
   Bajaj C. L., 1998, TECH REP
   Berjón D, 2013, SIGNAL PROCESS-IMAGE, V28, P181, DOI 10.1016/j.image.2012.10.013
   Bors AG, 2006, IEEE T IMAGE PROCESS, V15, P687, DOI 10.1109/TIP.2005.863116
   Cernea DC, 2008, IEEE T MULTIMEDIA, V10, P503, DOI 10.1109/TMM.2008.917407
   CHEN KS, 1994, IEEE T MED IMAGING, V13, P538, DOI 10.1109/42.310885
   Chuah S, 2013, IEEE T IMAGE PROCESS, V22, P5271, DOI 10.1109/TIP.2013.2286324
   Denis L, 2010, IEEE T MULTIMEDIA, V12, P773, DOI 10.1109/TMM.2010.2058094
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Florea R, 2012, EUR SIGNAL PR CONF, P764
   Florea R, 2012, IEEE INT CONF MULTI, P37, DOI 10.1109/ICMEW.2012.14
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   Guskov I, 2007, GRAPH MODELS, V69, P1, DOI 10.1016/j.gmod.2006.05.001
   Hammond P, 2007, ARCH DIS CHILD, V92, P1120, DOI 10.1136/adc.2006.103507
   Karray L, 1998, IEEE T IMAGE PROCESS, V7, P621, DOI 10.1109/83.668016
   Ke LG, 1998, IEEE T IMAGE PROCESS, V7, P225, DOI 10.1109/83.660999
   Khalip J, 2016, LIT Z, P1
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   KHODAKOVSKY A, 2003, COMPRESSION NORMAL M
   Kim MS, 2005, LECT NOTES COMPUT SC, V3710, P313
   Klein R, 1996, IEEE VISUAL, P311, DOI 10.1109/VISUAL.1996.568124
   Lavoue G., 2006, SPIE APPL DIG IM PRO
   Lavoué G, 2010, IEEE T MULTIMEDIA, V12, P636, DOI 10.1109/TMM.2010.2060475
   Li JK, 1998, P IEEE, V86, P1052, DOI 10.1109/5.687829
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Mekuria R, 2014, IEEE T MULTIMEDIA, V16, P1809, DOI 10.1109/TMM.2014.2331919
   Morán F, 2004, IEEE T CIRC SYST VID, V14, P937, DOI 10.1109/TCSVT.2004.830663
   Munteanu A, 1999, IEEE Trans Inf Technol Biomed, V3, P176, DOI 10.1109/4233.788579
   Munteanu A, 1999, INT J IMAG SYST TECH, V10, P76, DOI 10.1002/(SICI)1098-1098(1999)10:1<76::AID-IMA9>3.0.CO;2-0
   Munteanu A, 2010, IEEE T VIS COMPUT GR, V16, P513, DOI 10.1109/TVCG.2009.90
   Pajarola R, 2000, IEEE T VIS COMPUT GR, V6, P79, DOI 10.1109/2945.841122
   Papoulis A., 1964, PROBABILITY RANDOM V
   Park SB, 2006, IEEE T MULTIMEDIA, V8, P885, DOI 10.1109/TMM.2006.879914
   Payan F, 2006, IEEE T VIS COMPUT GR, V12, P649, DOI 10.1109/TVCG.2006.73
   Payan F, 2005, COMPUT AIDED GEOM D, V22, P466, DOI 10.1016/j.cagd.2005.04.001
   Peng J., 2005, ELSEVIER J VIS COMMU, V16, P668
   Peng JL, 2010, COMPUT GRAPH FORUM, V29, P2029, DOI 10.1111/j.1467-8659.2010.01789.x
   Ronfard R., 1996, Computer Graphics Forum, V15, pC67, DOI 10.1111/1467-8659.1530067
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Salomie IA, 2004, IEEE T CIRC SYST VID, V14, P950, DOI 10.1109/TCSVT.2004.830665
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Soucy M, 1996, COMPUT VIS IMAGE UND, V63, P1, DOI 10.1006/cviu.1996.0001
   Taubin G., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P123, DOI 10.1145/280814.280834
   Taubman D., 2012, JPEG2000: image compression fundamentals, standards and practice, V642
   Tian DH, 2007, IEEE T MULTIMEDIA, V9, P736, DOI 10.1109/TMM.2007.893341
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   Valette S, 2009, COMPUT GRAPH FORUM, V28, P1301, DOI 10.1111/j.1467-8659.2009.01507.x
   Vasic B, 2013, IEEE T MULTIMEDIA, V15, P1532, DOI 10.1109/TMM.2013.2265673
   Weber GH, 2007, IEEE T VIS COMPUT GR, V13, P1416, DOI 10.1109/TVCG.2007.70601
   Wu X., 1996, SC29WG1N256 ISOIEC
   Wu XL, 2000, IEEE T IMAGE PROCESS, V9, P536, DOI 10.1109/83.841931
   Wu XL, 1997, IEEE T COMMUN, V45, P437, DOI 10.1109/26.585919
NR 62
TC 6
Z9 6
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 236
EP 250
DI 10.1109/TMM.2016.2614483
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800002
DA 2024-07-18
ER

PT J
AU Kazemi, M
   Iqbal, R
   Shirmohammadi, S
AF Kazemi, Mohammad
   Iqbal, Razib
   Shirmohammadi, Shervin
TI Redundancy Allocation Based on the Weighted Mismatch-Rate Slope for
   Multiple Description Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple description coding (MDC); rate-distortion optimization;
   redundancy allocation; video coding; video transmission
ID BIT ALLOCATION; DISTORTION; TRANSMISSION; H.264/AVC
AB Multiple description coding (MDC) is a robust coding technique for video transmission over error prone networks, whereby the video is encoded into multiple descriptions with some redundancy between the descriptions. This redundancy leads to error resiliency in the case of packet loss during the network transport. However, the amount of this redundancy has a critical role in MDC performance. Therefore, a crucial problem in MDC is to find what the optimum amount of redundancy budget is, and then how this redundancy budget can be optimally allocated to the frames. To solve this problem, we propose a scheme in which the redundancy budget is allocated to the frames based on the weighted mismatch-rate slopes so that this additional bitrate can attain maximum distortion reduction. The redundancy is added gradually so that fine tuning of the utilized bitrate is achievable. We have verified our proposed scheme by implementing it in H.264/AVC reference software JM16.0, and running experiments against two representative reference methods. Our experiments show that our scheme not only minimizes the end-to-end distortion with a rate-distortion performance that is better than the reference methods, especially for high PLRs, but also entirely uses the available bandwidth, unlike the reference methods.
C1 [Kazemi, Mohammad] Univ Isfahan, Dept Elect Engn, Esfahan 8174673441, Iran.
   [Iqbal, Razib] Missouri State Univ, Dept Comp Sci, Springfield, MO 65897 USA.
   [Shirmohammadi, Shervin] Univ Ottawa, Sch Elect Engn & Comp Sci, Distributed & Collaborat Virtual Environm Res Lab, Ottawa, ON K1N 6N5, Canada.
   [Shirmohammadi, Shervin] Istanbul Sehir Univ, Multimedia Syst Lab, TR-34662 Istanbul, Turkey.
C3 University of Isfahan; Missouri State University; University of Ottawa;
   Istanbul Sehir University
RP Kazemi, M (corresponding author), Univ Isfahan, Dept Elect Engn, Esfahan 8174673441, Iran.
EM m.kazemi@eng.ui.ac.ir; riqbal@missouristate.edu; shervin@eecs.uottawa.ca
RI Kazemi, Mohammad/G-7733-2017; Shirmohammadi, Shervin/E-6945-2012
OI Shirmohammadi, Shervin/0000-0002-3973-4445; Iqbal,
   Razib/0000-0002-9293-2993
CR [Anonymous], 1996, ACTA NUMER
   Apostolpoulos JG, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P352, DOI 10.1109/ICIP.2000.899393
   Bai HH, 2014, IEEE T CIRC SYST VID, V24, P1390, DOI 10.1109/TCSVT.2014.2315770
   Carnpana O, 2008, IEEE T CIRC SYST VID, V18, P268, DOI 10.1109/TCSVT.2008.918113
   Chang CY, 2012, IEEE SYST J, V6, P414, DOI 10.1109/JSYST.2011.2163986
   Chen ZF, 2012, IEEE T IMAGE PROCESS, V21, P1123, DOI 10.1109/TIP.2011.2168411
   Fraunhofer Heinrich Hertz Institute Berlin Germany, 2015, H 264 AVC REFERENCE
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   Huszak A., 2010, COMMUNICATIONS ICC 2, P1, DOI DOI 10.1109/ISCCSP.2010.5463469
   Kamaci N, 2005, IEEE T CIRC SYST VID, V15, P994, DOI 10.1109/TCSVT.2005.852400
   Kamnoonwatana N, 2012, IEEE T CIRC SYST VID, V22, P1, DOI 10.1109/TCSVT.2011.2129251
   Kazemi M, 2015, SIGNAL PROCESS-IMAGE, V36, P95, DOI 10.1016/j.image.2015.06.006
   Kazemi M, 2014, MULTIMEDIA SYST, V20, P283, DOI 10.1007/s00530-013-0319-z
   Kazemi M, 2012, IEEE T CIRC SYST VID, V22, P202, DOI 10.1109/TCSVT.2011.2159431
   Kim K, 2006, SIGNAL PROCESS-IMAGE, V21, P293, DOI 10.1016/j.image.2005.11.002
   Lam E., 1999, IEEE T CIRCUITS SYST, V9, P608
   Li X, 2009, IEEE T CIRC SYST VID, V19, P193, DOI 10.1109/TCSVT.2008.2009255
   Lin CY, 2011, IEEE T CIRC SYST VID, V21, P589, DOI 10.1109/TCSVT.2011.2129270
   Park J, 2009, IEEE T MULTIMEDIA, V11, P1062, DOI 10.1109/TMM.2009.2026084
   Puri R., 1999, Proceedings of the 33rd Asilomar Confe. Signals, Systems, V1, P342
   Reibman A., 2002, P PACK VID C
   Reibman AR, 2002, IEEE T CIRC SYST VID, V12, P193, DOI 10.1109/76.993440
   Shirani S, 2006, IEEE T MULTIMEDIA, V8, P411, DOI 10.1109/TMM.2005.864349
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Temlyakov VN, 2015, CONSTR APPROX, V41, P269, DOI 10.1007/s00365-014-9272-0
   Tillo T, 2008, IEEE T CIRC SYST VID, V18, P59, DOI 10.1109/TCSVT.2007.913751
   Tillo T, 2007, IEEE T IMAGE PROCESS, V16, P1269, DOI 10.1109/TIP.2007.891799
   Wang HS, 2009, IEEE T IMAGE PROCESS, V18, P225, DOI 10.1109/TIP.2008.2008743
   Wang JP, 2008, IEEE T MULTIMEDIA, V10, P1393, DOI 10.1109/TMM.2008.2004935
   Wang Y, 2005, P IEEE, V93, P57, DOI 10.1109/JPROC.2004.839618
   Wang Y, 2006, IEEE T CIRC SYST VID, V16, P716, DOI 10.1109/TCSVT.2006.875203
   Williamson D. P., 2011, DESIGN APPROXIMATION, DOI DOI 10.1017/CBO9780511921735
   Xiao JM, 2012, IEEE T MULTIMEDIA, V14, P1298, DOI 10.1109/TMM.2012.2194274
   Xu YY, 2013, IEEE T CIRC SYST VID, V23, P1523, DOI 10.1109/TCSVT.2013.2249018
   ZHANG R, 2000, IEEE J SEL AREAS COM, V18
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 38
TC 11
Z9 11
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 54
EP 66
DI 10.1109/TMM.2016.2607342
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200005
DA 2024-07-18
ER

PT J
AU Kazemi, R
   Perez-González, F
   Akhaee, MA
   Behnia, F
AF Kazemi, Reza
   Perez-Gonzalez, Fernando
   Akhaee, Mohammad Ali
   Behnia, Fereydoon
TI Data Hiding Robust to Mobile Communication Vocoders
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data hiding; mobile communication vocoder; nonlinearity; spread
   spectrum; watermarking
ID SPREAD-SPECTRUM WATERMARKING; INFORMATION
AB The swift growth of cellular mobile networks in recent years has made voice channels almost accessible everywhere. Besides, data hiding has recently attracted significant attention due to its ability to imperceptibly embed side information that can be used for signal enhancement, security improvement, and two-way authentication purposes. In this regard, we aim at proposing efficient schemes for hiding data in the widespread voice channel of cellular networks. To this aim, our first contribution is to model the channel accurately by considering a linear filter plus a nonlinear scaling function. This model is validated through experiments with true speech signals. Then we leverage on this model to propose two additive and multiplicative data hiding methods based on the spread spectrum techniques. In addition, inspired by the concept of M-ary biorthogonal codes, we develop novel schemes that significantly outperform the previous ones. The performance of all the methods that we present is assessed mathematically and cross-validated with simulations. These are later extended to true speech signals where the results evidence an excellent performance as predicted by the theory. Finally, we assess the imperceptibility by means of both subjective and objective benchmarks and show that the perceptual impact of our watermarks is acceptable.
C1 [Kazemi, Reza; Behnia, Fereydoon] Sharif Univ Technol, Dept Elect & Elect Engn, Tehran 1136511155, Iran.
   [Perez-Gonzalez, Fernando] Univ Vigo SPAIN, Signal Theory & Commun Dept, EE Telecomunicac, Vigo 36310, Spain.
   [Akhaee, Mohammad Ali] Univ Tehran, Coll Engn, Sch Elect & Comp Engn, Tehran 1417466191, Iran.
C3 Sharif University of Technology; Universidade de Vigo; University of
   Tehran
RP Kazemi, R (corresponding author), Sharif Univ Technol, Dept Elect & Elect Engn, Tehran 1136511155, Iran.
EM rezakazemi.reza@gmail.com; fperez@gts.uvigo.es; Akhaee@ut.ac.ir;
   behnia@sharif.edu
RI Pérez-González, Fernando/GLT-6608-2022; verijkazemi, reza/ABF-7583-2021
OI Pérez-González, Fernando/0000-0002-0568-1373; 
FU Spanish Ministry of Economy and Competitiveness and the European
   Regional Development Fund (ERDF) under Project TACTICA and Project
   COMPASS [TEC2013-47020-C2-1-R]; Galician Regional Government and ERDF
   under "Project Consolidation of Research Units" [GRC2013/009]; Project
   REdTEIC [R2014/037]; AtlantTIC
FX This work was supported in part by the Spanish Ministry of Economy and
   Competitiveness and the European Regional Development Fund (ERDF) under
   Project TACTICA and Project COMPASS (TEC2013-47020-C2-1-R), and in part
   by the Galician Regional Government and ERDF under "Project
   Consolidation of Research Units" (GRC2013/009), Project REdTEIC
   (R2014/037), and Project AtlantTIC. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Xiao-Ping Zhang.
CR [Anonymous], 301704 ETSI EN
   [Anonymous], 126071 ETSI EN
   [Anonymous], WORLDS PREMIER SUPPL
   [Anonymous], 2013, P INT C E W DES TEST
   [Anonymous], P IS T SPIE EL IM
   [Anonymous], MOBILE TELECOMMUNICA
   [Anonymous], 11 AES CONV NEW YORK
   [Anonymous], IMSI CATCHER CHAIR C
   Aoki N, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P608, DOI 10.1109/IIH-MSP.2008.122
   Baras C, 2006, IEEE T AUDIO SPEECH, V14, P1772, DOI 10.1109/TASL.2006.879808
   Bologna Mauro, 2014, Proceedings of the NATO Advanced Study Institute on Detection of Chemical, Biological, Radiological and Nuclear Agents for the Prevention of Terrorism. Mass Spectrometry and Allied Topics., P1, DOI 10.1007/978-94-017-9238-7_1
   Boloursaz M, 2012, 2012 SIXTH INTERNATIONAL SYMPOSIUM ON TELECOMMUNICATIONS (IST), P1021, DOI 10.1109/ISTEL.2012.6483136
   Buttyán L, 2000, IEEE T COMMUN, V48, P373, DOI 10.1109/26.837039
   Cheng Q, 2001, IEEE T MULTIMEDIA, V3, P273, DOI 10.1109/6046.944472
   Cheng Q, 2009, IEEE T CIRC SYST VID, V19, P978, DOI 10.1109/TCSVT.2009.2020255
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Dittmann J, 2005, P SOC PHOTO-OPT INS, V5681, P607, DOI 10.1117/12.586579
   Eltholth, 2009, UBIQUITOUS COMPUT CO, V3, P54
   Er F. C., 2011, 2011 7th International Conference on Information Assurance and Security (IAS), P13, DOI 10.1109/ISIAS.2011.6122787
   Garofolo John, 1993, TIMIT ACOUSTIC PHONE
   Gazor S, 2003, IEEE SIGNAL PROC LET, V10, P204, DOI 10.1109/LSP.2003.813679
   Huang S, 2011, INT J PLANT PROD, V5, P135
   Huang YF, 2011, IET COMMUN, V5, P929, DOI 10.1049/iet-com.2010.0348
   Huang YF, 2011, IEEE T INF FOREN SEC, V6, P296, DOI 10.1109/TIFS.2011.2108649
   Huang YF, 2012, IEEE T INF FOREN SEC, V7, P1865, DOI 10.1109/TIFS.2012.2218599
   Kirovski D, 2003, IEEE T SIGNAL PROCES, V51, P1020, DOI 10.1109/TSP.2003.809384
   Kokkinakis K, 2005, INT CONF ACOUST SPEE, P381
   LaDue CK, 2008, IEEE T VEH TECHNOL, V57, P2205, DOI 10.1109/TVT.2007.912322
   Mazurczyk W, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543587
   Mazurczyk W, 2007, J INF ASSUR SECUR, V2, P226
   Merhav N, 2008, IEEE T INFORM THEORY, V54, P255, DOI 10.1109/TIT.2007.911210
   Pérez-Freire L, 2009, IEEE T INF FOREN SEC, V4, P2, DOI 10.1109/TIFS.2008.2009603
   Pérez-González F, 2003, IEEE T SIGNAL PROCES, V51, P960, DOI 10.1109/TSP.2003.809368
   Piotrowski Zbigniew, 2013, Przeglad Elektrotechniczny, V89, P196
   Proakis J.G., 1995, Communications and signal processing
   Singh J, 2009, INF SECUR J, V18, P99, DOI 10.1080/19393550902791424
   Su YM, 2006, 2006 IMACS: Multiconference on Computational Engineering in Systems Applications, Vols 1 and 2, P11
   Toorani M, 2008, INT CONF NEXT GEN, P576, DOI 10.1109/NGMAST.2008.88
   Valizadeh A., 2009, PROC GLOBECOM IEEE G, P1
   Valizadeh A, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-88
   Valizadeh A, 2012, IEEE T INF FOREN SEC, V7, P1127, DOI 10.1109/TIFS.2012.2199312
   Wang CY, 2007, IEEE INT SYM MULTIM, P255, DOI 10.1109/ISM.2007.33
   Wu ZJ, 2006, LECT NOTES COMPUT SC, V4113, P1139, DOI 10.1007/11816157_141
   Xiao B., 2008, PROC IEEE GLOBECOM G, P1
   Yan DQ, 2009, FUND INFORM, V97, P1, DOI 10.3233/FI-2009-190
NR 45
TC 9
Z9 9
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2345
EP 2357
DI 10.1109/TMM.2016.2599149
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200003
DA 2024-07-18
ER

PT J
AU Lee, J
   Lee, K
   Han, C
   Kim, T
   Chong, S
AF Lee, Joohyun
   Lee, Kyunghan
   Han, Choongwoo
   Kim, Taehoon
   Chong, Song
TI Resource-Efficient Mobile Multimedia Streaming With Adaptive Network
   Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Communication energy saving; Markov decision process; mobile video
   streaming; resource efficiency
ID SCALABLE VIDEO; ALLOCATION
AB From the advancements of mobile display and network infrastructure, mobile users can enjoy high quality-mobile video streaming anywhere, anytime. However, most mobile users are still reluctant to use high quality video streaming when they are mobile due to costly cellular data and high energy consumption. In this work, we develop scheduling algorithms for resource-efficient mobile video streaming, which minimize the weighted sum objective of cellular cost and energy consumption. We first model the scheduling problem as a Markov decision process and propose an optimal scheduling algorithm based on dynamic programming. Then, we derive a heuristic algorithm that approximates the optimal algorithm. To evaluate the performance of proposed algorithms, we run simulation over YouTube video traces with audience retention graphs and mobility/connectivity traces in public transportation (e.g., commuting). Through extensive simulations, we show that our proposed scheduling algorithm has negligible performance loss compared to the optimal scheduling algorithm, where it saves 59% of cellular cost and 41% of energy compared to the YouTube default scheduler. We also implement our scheduling algorithm on an Android platform, and experimentally evaluate the performance compared to existing streaming policies.
C1 [Lee, Joohyun] Korea Adv Inst Sci & Technol, Daejeon 34141, South Korea.
   [Lee, Joohyun] Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USA.
   [Lee, Kyunghan; Kim, Taehoon] Ulsan Natl Inst Sci & Technol, Sch Elect & Comp Engn, Ulsan 44919, South Korea.
   [Han, Choongwoo] Korea Adv Inst Sci & Technol, Sch Comp, Daejeon 34141, South Korea.
   [Chong, Song] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); University
   System of Ohio; Ohio State University; Ulsan National Institute of
   Science & Technology (UNIST); Korea Advanced Institute of Science &
   Technology (KAIST); Korea Advanced Institute of Science & Technology
   (KAIST)
RP Lee, J (corresponding author), Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USA.
EM lee.7119@osu.edu; khlee@unist.ac.kr; cwhan.tunz@kaist.ac.kr;
   tunz@kaist.ac.kr; songchong@kaist.edu
RI Chong, Song/C-1567-2011; Lee, Kyunghan/AAO-9449-2021; Lee,
   Kyunghan/AAM-9618-2020; Lee, Joohyun/G-1608-2018
OI Lee, Kyunghan/0000-0001-8647-1476; Lee, Kyunghan/0000-0001-8647-1476;
   Lee, Joohyun/0000-0002-7698-1568
FU Institute for Information & Communications Technology Promotion (IITP)
   [B0126-16-1064]; Research on Near-Zero Latency Network for 5G Immersive
   Service [B0190-16-2017]; Ministry of Science, ICT & Future Planning
   [NRF-2014R1A1A1006330]; Ulsan National Institute of Science and
   Technology [1.160085.01]
FX This work was supported in part by the Institute for Information &
   Communications Technology Promotion (IITP) under Grant B0126-16-1064,
   Research on Near-Zero Latency Network for 5G Immersive Service, and
   Grant B0190-16-2017, Resilient/Fault-Tolerant Autonomic Networking Based
   on Physicality, Relationship and Service Semantic of IoT Devices, in
   part by the Ministry of Science, ICT & Future Planning under Grant
   NRF-2014R1A1A1006330, and in part by the Ulsan National Institute of
   Science and Technology under Grant 1.160085.01. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shiwen Mao.
CR Almowuena S, 2016, IEEE T MULTIMEDIA, V18, P102, DOI 10.1109/TMM.2015.2502067
   [Anonymous], 2012, Cisco Visual Networking Index: Global Mobile Data Traffic Forecast Update
   [Anonymous], 2008, Proc. ACM WiNTECH
   [Anonymous], 2014, PRIC PLANS AT T
   [Anonymous], 2012, AN SMARTPH US PATT C
   [Anonymous], 2016, YOUTUBE AN REP API G
   BALASUBRAMANIAN A., 2010, Proceedings of MobiSys, P209, DOI DOI 10.1145/1814433.1814456
   Bertsekas D., 2007, Dynamic programming and optimal control, V2
   Bokani A, 2013, 2013 20TH INTERNATIONAL PACKET VIDEO WORKSHOP (PV)
   Chen J., 2013, Proceedings of the 19th Annual International Conference on Mobile Computing Networking, P389
   Chen Jing, 2012, Proceedings of the 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE), P121
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Chuah SP, 2012, IEEE T MULTIMEDIA, V14, P1324, DOI 10.1109/TMM.2012.2193560
   Dimatteo S., 2011, 2011 IEEE 8th International Conference on Mobile Ad-Hoc and Sensor Systems, P192, DOI 10.1109/MASS.2011.26
   Finamore A., 2011, ACM IMC, P345, DOI DOI 10.1145/2068816.2068849
   Go Y, 2015, IEEE T MULTIMEDIA, V17, P1646, DOI 10.1109/TMM.2015.2451951
   Guruprasad R, 2015, IEEE T MULTIMEDIA, V17, P1630, DOI 10.1109/TMM.2015.2436821
   Hamblen M., 2012, COMPUTER WORLD
   Hoque M. A., 2013, CORR
   Hoque MA, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2556942
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Jurca D, 2007, IEEE T MULTIMEDIA, V9, P1227, DOI 10.1109/TMM.2007.902852
   Lee K, 2012, IEEE ACM T NETWORK, V20, P515, DOI 10.1109/TNET.2011.2172984
   Moon Y., 2015, PROC ANN INT C MOBIL, P419
   Powell W. B., 2007, APPROXIMATE DYNAMIC, V703
   Schulman A, 2010, MOBICOM 10 & MOBIHOC 10: PROCEEDINGS OF THE 16TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING AND THE 11TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P85
   Shakil SR, 2013, 2013 2ND INTERNATIONAL CONFERENCE ON ADVANCES IN ELECTRICAL ENGINEERING (ICAEE 2013), P374, DOI 10.1109/ICAEE.2013.6750366
   Sharangi S, 2011, IEEE T MULTIMEDIA, V13, P102, DOI 10.1109/TMM.2010.2076799
   Siekkinen M, 2016, IEEE ACM T NETWORK, V24, P1489, DOI 10.1109/TNET.2015.2415873
   Singhal C, 2014, IEEE T MOBILE COMPUT, V13, P1522, DOI 10.1109/TMC.2013.138
   Theiss B., 2014, CONNECT MAGAZINE
   Wang A, 2012, PROCEEDINGS OF 2012 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND NETWORK TECHNOLOGY (ICCSNT 2012), P708, DOI 10.1109/ICCSNT.2012.6526032
   Xiaomeng Chen, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P151, DOI 10.1145/2745844.2745875
   Xu Qiang., 2013, ACM MOBISYS
   Yao J, 2012, IEEE T MOBILE COMPUT, V11, P603, DOI 10.1109/TMC.2011.97
   Zhu TX, 2012, PROCEEDINGS OF THE 4TH (2012) INTERNATIONAL CONFERENCE ON FINANCIAL RISK AND CORPORATE FINANCE MANAGEMENT, VOLS I AND II, P279
NR 36
TC 5
Z9 5
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2517
EP 2527
DI 10.1109/TMM.2016.2604565
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200017
DA 2024-07-18
ER

PT J
AU Gaddam, VR
   Riegler, M
   Eg, R
   Griwodz, C
   Halvorsen, P
AF Gaddam, Vamsidhar Reddy
   Riegler, Michael
   Eg, Ragnhild
   Griwodz, Carsten
   Halvorsen, Pal
TI Tiling in Interactive Panoramic Video: Approaches and Evaluation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia system; panorama; tiling; user studies; video
ID QUALITY ASSESSMENT; DISPLAY; SYSTEM; SIZE
AB Interactive panoramic systems are currently on the rise. However, one of the major challenges in such a system is the overhead involved in transferring a full-quality panorama to the client when only a part of the panorama is used to extract a virtual view. Thus, such a system should maximize the user experience while simultaneously minimizing the bandwidth required. In this paper, we apply tiling to deliver different quality levels for different parts of the panorama. Tiling has traditionally been applied to the delivery of very high-resolution content to clients. Here, we apply similar ideas in a real-time interactive panoramic video system. A major challenge lies in the movement of such a virtual view, for which clients' regions of interest change dynamically and independently from each other. We show that our algorithms, which progressively increase in quality toward the point of the view, manage to (i) reduce the bandwidth requirement and (ii) provide a similar quality of experience (QoE) compared to a full panorama system.
C1 [Gaddam, Vamsidhar Reddy] Univ Oslo, Dept Informat, N-0373 Oslo, Norway.
   [Riegler, Michael; Eg, Ragnhild; Griwodz, Carsten; Halvorsen, Pal] Simula Res Lab, Media Dept, N-1325 Lysaker, Norway.
C3 University of Oslo
RP Gaddam, VR (corresponding author), Univ Oslo, Dept Informat, N-0373 Oslo, Norway.
EM vamsidhg@ifi.uio.no; michael@simula.no; rage@simula.no; griff@simula.no;
   paalh@ifi.uio.no
RI Riegler, Michael A/E-5443-2015; Eg, Ragnhild/AAQ-5857-2021
OI Eg, Ragnhild/0000-0002-9550-0424; Halvorsen, Pal/0000-0003-2073-7029
CR [Anonymous], P ACM SIGMM C MULT S
   [Anonymous], 2008, 5104 RFC
   [Anonymous], P ACM IMMERSIVEME 15
   [Anonymous], 2002, 3264 RFC
   [Anonymous], 1998, 2250 RFC
   [Anonymous], 2008, ITUTJ247 IEEE
   [Anonymous], 2013, JTC1SC29WG11M30022 I
   [Anonymous], P WORKSH NETW OP SYS
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], 2005, Digital Video Quality: Vision Models and Metrics
   [Anonymous], 2011, Image Processing (ICIP), 2011 18th IEEE International Conference on
   Boulos F, 2009, IEEE IMAGE PROC, P3109, DOI 10.1109/ICIP.2009.5414458
   Brunnstrom Kjell, 2009, IEEE Signal Processing Magazine, V26, P96, DOI 10.1109/MSP.2009.932162
   Carr P, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P74, DOI 10.1109/DICTA.2009.62
   Carr Peter., 2013, Proceedings of the 21st ACM International Conference on Multimedia, MM '13, P193, DOI DOI 10.1145/2502081.2502086
   Dahud H., 2015, VIDEO WARS YOUTUBE V
   Dosselmann R, 2011, SIGNAL IMAGE VIDEO P, V5, P81, DOI 10.1007/s11760-009-0144-1
   Farid MS, 2014, IEEE IMAGE PROC, P3233, DOI 10.1109/ICIP.2014.7025654
   Farid MS, 2015, IEEE T IMAGE PROCESS, V24, P205, DOI 10.1109/TIP.2014.2374533
   Fehn C, 2006, IEEE INT SYM MULTIM, P291
   Feipeng Liu, 2012, Advances in Multimedia Information Processing - PCM 2012. 13th Pacific-Rim Conference on Multimedia. Proceedings, P251, DOI 10.1007/978-3-642-34778-8_23
   FLEISS JL, 1971, PSYCHOL BULL, V76, P378, DOI 10.1037/h0031619
   Foote Eric., 2013, Proceedings of the 21st ACM International Conference on Multimedia (New York, NY, USA), MM'13, Association for Computing Machinery, P163
   Gaddam VR, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P204, DOI 10.1109/PCS.2015.7170076
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   IEEE, 1998, IEEE, P1, DOI [10.1109/IEEESTD.1998.88286, DOI 10.1109/IEEESTD.1998.88286]
   Karczewicz M, 2003, IEEE T CIRC SYST VID, V13, P637, DOI 10.1109/TCSVT.2003.814969
   Kimata H., 2012, 2012 IEEE 1st Global Conference on Consumer Electronics (GCCE 2012), P574, DOI 10.1109/GCCE.2012.6379918
   Kimata H, 2011, 2011 TECHNICAL SYMPOSIUM AT ITU TELECOM WORLD (ITU WT), P45
   Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453
   Kreuzberger C., 2015, Proceedings of the 6th ACM Multimedia Systems Conference, P213
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Li Z., 2014, Proceedings of the 5th ACM Multimedia Systems Conference, MMSys '14, P248
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Mavlankar A, 2009, IEEE IMAGE PROC, P3061, DOI 10.1109/ICIP.2009.5414201
   Mavlankar A, 2010, SIGNALS COMMUN TECHN, P431, DOI 10.1007/978-3-642-12802-8_19
   McCanne S., 1996, Computer Communication Review, V26, P117, DOI 10.1145/248157.248168
   Niamut O., 2012, P NEM SUMM, P69
   Pechard S, 2006, IEEE IMAGE PROC, P409, DOI 10.1109/ICIP.2006.312480
   Pitrey Y., 2011, P EUR ITV WORKSH QOE
   Raaen K., 2014, 2014 13th Annual Workshop on Network and Systems Support for Games, P1
   Riiser H, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240137
   Sánchez Y, 2015, IEEE IMAGE PROC, P2244, DOI 10.1109/ICIP.2015.7351200
   Sánchez-Hernández JJ, 2015, IEEE T MULTIMEDIA, V17, P1829, DOI 10.1109/TMM.2015.2470595
   Shum HY, 2005, IEEE T MULTIMEDIA, V7, P85, DOI 10.1109/TMM.2004.840591
   Sim J, 2005, PHYS THER, V85, P257
   Statista, 2014, TOP 10 INT TRAFF SER
   Takaki Y, 2011, OPT EXPRESS, V19, P4129, DOI 10.1364/OE.19.004129
   Takaki Y, 2010, OPT EXPRESS, V18, P8824, DOI 10.1364/OE.18.008824
   Tang WK, 2005, IEEE T MULTIMEDIA, V7, P280, DOI 10.1109/TMM.2005.843811
   Tzavidas S, 2005, IEEE T MULTIMEDIA, V7, P880, DOI 10.1109/TMM.2005.854430
   Unanue I., 2011, P REC ADV VID COD, P3
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xinding Sun, 2005, IEEE Transactions on Multimedia, V7, P981, DOI 10.1109/TMM.2005.854388
   Xu W, 2013, MULTIMEDIA SYST, V19, P407, DOI 10.1007/s00530-013-0316-2
   Yokoi T., 2005, P 1 INT C RAILW OP M, P1
   Zhao Q, 2013, IEEE T MULTIMEDIA, V15, P1745, DOI 10.1109/TMM.2013.2280249
NR 59
TC 119
Z9 131
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1819
EP 1831
DI 10.1109/TMM.2016.2586304
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800012
DA 2024-07-18
ER

PT J
AU Zhao, GS
   Qian, XM
   Xie, X
AF Zhao, Guoshuai
   Qian, Xueming
   Xie, Xing
TI User-Service Rating Prediction by Exploring Social Users' Rating
   Behaviors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data mining; recommender system; social networks; social user behavior
ID RECOMMENDATION; INFERENCE
AB With the boom of social media, it is a very popular trend for people to share what they are doing with friends across various social networking platforms. Nowadays, we have a vast amount of descriptions, comments, and ratings for local services. The information is valuable for new users to judge whether the services meet their requirements before partaking. In this paper, we propose a user-service rating prediction approach by exploring social users' rating behaviors. In order to predict user-service ratings, we focus on users' rating behaviors. In our opinion, the rating behavior in recommender system could be embodied in these aspects: 1) when user rated the item, 2) what the rating is, 3) what the item is, 4) what the user interest that we could dig from his/her rating records is, and 5) how the user's rating behavior diffuses among his/her social friends. Therefore, we propose a concept of the rating schedule to represent users' daily rating behaviors. In addition, we propose the factor of interpersonal rating behavior diffusion to deep understand users' rating behaviors. In the proposed user-service rating prediction approach, we fuse four factors-user personal interest (related to user and the item's topics), interpersonal interest similarity (related to user interest), interpersonal rating behavior similarity (related to users' rating behavior habits), and interpersonal rating behavior diffusion (related to users' behavior diffusions)-into a unified matrix-factorized framework. We conduct a series of experiments in the Yelp dataset and Douban Movie dataset. Experimental results show the effectiveness of our approach.
C1 [Zhao, Guoshuai; Qian, Xueming] Xi An Jiao Tong Univ, SMILES Lab, Xian 710049, Peoples R China.
   [Qian, Xueming] Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China.
   [Xie, Xing] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Xi'an Jiaotong University; Microsoft Research Asia; Microsoft
RP Zhao, GS (corresponding author), Xi An Jiao Tong Univ, SMILES Lab, Xian 710049, Peoples R China.; Qian, XM (corresponding author), Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China.; Xie, X (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM zgs2012@stu.xjtu.edu.cn; qianxm@mail.xjtu.edu.cn; xing.xie@microsoft.com
RI Zhao, Guoshuai/AAN-1271-2020
OI Zhao, Guoshuai/0000-0003-4392-8450; Xie, Xing/0000-0002-8608-8482
FU Program 973 [2012CB316400]; NSFC [61373113, 61173109, 61332018];
   Microsoft Research Asia
FX This work was supported in part by the Program 973 under Grant
   2012CB316400, in part by the NSFC under Grant 61373113, Grant 61173109,
   and Grant 61332018, and in part by Microsoft Research Asia. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Ebroul Izquierdo.
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   [Anonymous], 2011, P 5 ACM C REC SYST
   [Anonymous], P IEEE INT C MULT BI
   [Anonymous], 2011, P ACM C REC SYST REC, DOI DOI 10.1145/2043932.2043947
   Bao J, 2015, GEOINFORMATICA, V19, P525, DOI 10.1007/s10707-014-0220-8
   Bedi P., 2007, PROC IJCAI 07, P2677
   Bell RM, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P95
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1283, DOI 10.1109/TMM.2013.2265077
   Chen Y, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P1013
   Chen ZS, 2011, IEEE T MULTIMEDIA, V13, P1371, DOI 10.1109/TMM.2011.2166380
   Cheng C., 2012, P AAAI C ART INT, P17
   Cho E., 2011, P 17 ACM SIGKDD INT, P1082, DOI DOI 10.1145/2020408.2020579
   Cui P, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P185
   Deshpande M, 2004, ACM T INFORM SYST, V22, P143, DOI 10.1145/963770.963776
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P1031, DOI 10.1109/TMM.2015.2430819
   Feng H, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1521
   Feng H, 2014, NEUROCOMPUTING, V129, P409, DOI 10.1016/j.neucom.2013.09.018
   Gui-Rong Xue, 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P114
   Guoshuai Zhao, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8326, P181, DOI 10.1007/978-3-319-04117-9_17
   Harvey M., 2011, P 20 ACM INT C INFOR, P699, DOI DOI 10.1145/2063576.2063680
   Herlocker J, 2002, INFORM RETRIEVAL, V5, P287, DOI 10.1023/A:1020443909834
   Herlocker JL, 2004, ACM T INFORM SYST, V22, P5, DOI 10.1145/963770.963772
   Hu LK, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P345, DOI 10.1145/2600428.2609593
   Huang JM, 2010, FRONT ARTIF INTEL AP, V215, P601, DOI 10.3233/978-1-60750-606-5-601
   Jahrer M., 2010, P 16 ACM SIGKDD INT, P693, DOI [DOI 10.1145/1835804.1835893, 10.1145/1835804.1835893]
   Jamali M., 2010, P 4 ACM C REC SYST, P135, DOI DOI 10.1145/1864708.1864736
   Jamali M, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P397
   Jiang Meng, 2012, P 21 ACM INT C INFOR, P45
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Keshavan RH, 2010, J MACH LEARN RES, V11, P2057
   Koenigstein N., 2013, Proceedings of the 7th ACM conference on Recommender systems, P419, DOI 10.1145/2507157.2507208
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Koren Y, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P447
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lee K, 2014, IEEE T MULTIMEDIA, V16, P1201, DOI 10.1109/TMM.2014.2311012
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   Liu, 2013, P 7 ACM C REC SYST, P93, DOI DOI 10.1145/2507157.2507182
   Liu N N, 2009, P 18 ACM C INF KNOWL, P759, DOI DOI 10.1145/1645953.1646050
   Liu Q, 2012, IEEE T SYST MAN CY B, V42, P218, DOI 10.1109/TSMCB.2011.2163711
   Ma H., 2011, Proceedings of the 4th ACM International Conference on Web Search and Data Mining, P287
   Ma H, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P203, DOI 10.1145/1571941.1571978
   Ma Hao, 2008, P CIKM08 C INFORM KN, P931
   Mao K, 2015, IEEE T MULTIMEDIA, V17, P396, DOI 10.1109/TMM.2015.2392562
   Massa P, 2007, RECSYS 07: PROCEEDINGS OF THE 2007 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P17
   Min WQ, 2015, IEEE T MULTIMEDIA, V17, P1787, DOI 10.1109/TMM.2015.2463226
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Paterek A., 2007, P KDD CUP WORKSH, V2007, P5, DOI [DOI 10.1145/1557019.1557072, 10.1145/1557019.1557072]
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Resnick P., 1994, Transcending Boundaries, CSCW '94. Proceedings of the Conference on Computer Supported Cooperative Work, P175, DOI 10.1145/192844.192905
   Richardson M., 2002, P 8 ACM SIGKDD INT C, P61, DOI DOI 10.1145/775047.775057
   Sarwar B., 2001, P 10 INT C WORLD WID, P285, DOI 10.1145/371920.372071
   Servia-Rodríguez S, 2013, IEEE T MULTIMEDIA, V15, P1296, DOI 10.1109/TMM.2013.2265168
   Wang J., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P501, DOI 10.1145/1148170.1148257
   Wang XY, 2015, IEEE T MULTIMEDIA, V17, P409, DOI 10.1109/TMM.2014.2385473
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Yang XW, 2011, IEEE INFOCOM SER, P551, DOI 10.1109/INFCOM.2011.5935224
   Yang Xiwang., 2012, P 18 ACM SIGKDD INT, P1267, DOI [10.1145/2339530.2339728, DOI 10.1145/2339530.2339728]
   Zahálka J, 2015, IEEE T MULTIMEDIA, V17, P2235, DOI 10.1109/TMM.2015.2480007
   Zhao GS, 2015, 2015 1ST IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P228, DOI 10.1109/BigMM.2015.67
   Zhou Y, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P338
NR 61
TC 88
Z9 90
U1 1
U2 59
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 496
EP 506
DI 10.1109/TMM.2016.2515362
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600015
DA 2024-07-18
ER

PT J
AU Tan, WM
   Yan, B
   Li, K
   Tian, Q
AF Tan, Weimin
   Yan, Bo
   Li, Ke
   Tian, Qi
TI Image Retargeting for Preserving Robust Local Feature: Application to
   Mobile Visual Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature preserving; image retargeting; mobile visual search
ID RETRIEVAL; QUERY; SIFT
AB With the sharp increasing of mobile devices, conducting search on mobile devices becomes pervasive, and one of the most popular applications is mobile visual search. To achieve low bit-rate visual search, most of the existing works focus on addressing local descriptor coding and BoW histogram compression. In this paper, we extend the concept of image retargeting and propose a new image resizing approach that is devoted to preserving the robust local features in the query image while resizing it. Based on the extended concept, we introduce a novel mobile-visual-search scheme that conducts the proposed approach to reduce the size of the query image for achieving low bit-rate visual search. Extensive experiments on Oxford 5 K and Flickr 100k datasets show that our approach obtains superior retrieval performance than state-of-the-art image resizing approaches at the similar query size; meanwhile, it is cost effective in terms of processing time.
C1 [Tan, Weimin; Yan, Bo; Li, Ke] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 201203, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Fudan University; University of Texas System; University of Texas at San
   Antonio (UTSA)
RP Tan, WM (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 201203, Peoples R China.
EM byan@fudan.edu.cn; qitian@cs.utsa.edu
RI Yan, Bo/AFQ-7025-2022
OI Yan, Bo/0000-0002-7775-1270
FU NSFC [61370158, 61522202, 61429201]; ARO [W911NF-15-1-0290,
   W911NF-12-1-0057]; NEC Laboratories of America
FX This work was supported in part by the NSFC under Grant 61370158 and
   Grant 61522202. The work of Q. Tian was supported in part by the ARO
   under Grant W911NF-15-1-0290 and Grant W911NF-12-1-0057, in part by the
   NSFC under Grant 61429201, and in part by the Faculty Research Awards of
   the NEC Laboratories of America.
CR [Anonymous], 2009, P 5 INT C MOB MULT C
   [Anonymous], 2007, ACM T GRAPH
   [Anonymous], 1999, N1359 JTC1SC29WG1 IS
   [Anonymous], 2006, P 14 ACM INT C MULT
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chandrasekhar Vijay, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2504, DOI 10.1109/CVPRW.2009.5206733
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Chen L, 2012, IEEE T MULTIMEDIA, V14, P1057, DOI 10.1109/TMM.2012.2187435
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Duan LY, 2014, IEEE T MULTIMEDIA, V16, P346, DOI 10.1109/TMM.2013.2293063
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Ke Y, 2004, PROC CVPR IEEE, P506
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Nister David, 2006, CVPR
   Panozzo D, 2012, COMPUT GRAPH FORUM, V31, P229, DOI 10.1111/j.1467-8659.2012.03001.x
   Qi H, 2014, IEEE T MULTIMEDIA, V16, P1963, DOI 10.1109/TMM.2014.2345026
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   SEBER G. A., 2009, Multivariate observations, V252
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Spath H., 1985, The cluster dissection and analysis theory fortran programs examples
   Squire DM, 2000, PATTERN RECOGN LETT, V21, P1193, DOI 10.1016/S0167-8655(00)00081-7
   Wang Y, 2011, PROCEEDINGS OF THE 4TH CONFERENCE ON SYSTEMS SCIENCE, MANAGEMENT SCIENCE AND SYSTEMS DYNAMICS, SSMSSD10, VOL 4, P73
   Xiao Y, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON EVOLVABLE SYSTEMS (ICES), P1, DOI [10.1109/USNC-URSI-NRSM.2014.6927950, 10.1109/ICES.2014.7008715]
   Xie HT, 2014, IEEE T MULTIMEDIA, V16, P1104, DOI 10.1109/TMM.2014.2305909
   Xue Y., 2013, PROC IEEE INT TEST C, P1
   Yan B, 2015, IEEE T CIRC SYST VID, V25, P15, DOI 10.1109/TCSVT.2014.2329374
   Yan B, 2014, IEEE T MULTIMEDIA, V16, P272, DOI 10.1109/TMM.2013.2286112
   Yan B, 2013, IEEE T CIRC SYST VID, V23, P313, DOI 10.1109/TCSVT.2012.2203740
   Yen TC, 2011, IEEE T IMAGE PROCESS, V20, P2339, DOI 10.1109/TIP.2011.2114357
   Zhang SL, 2014, IEEE T IMAGE PROCESS, V23, P3671, DOI 10.1109/TIP.2014.2330794
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
NR 35
TC 28
Z9 29
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2016
VL 18
IS 1
BP 128
EP 137
DI 10.1109/TMM.2015.2500727
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CZ5JW
UT WOS:000367139700012
DA 2024-07-18
ER

PT J
AU Ding, CX
   Tao, DC
AF Ding, Changxing
   Tao, Dacheng
TI Robust Face Recognition via Multimodal Deep Face Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks (CNNs); deep learning; face recognition;
   multimodal system
AB Face images appearing in multimedia applications, e.g., social networks and digital entertainment, usually exhibit dramatic pose, illumination, and expression variations, resulting in considerable performance degradation for traditional face recognition algorithms. This paper proposes a comprehensive deep learning framework to jointly learn face representation using multimodal information. The proposed deep learning structure is composed of a set of elaborately designed convolutional neural networks (CNNs) and a three-layer stacked auto-encoder (SAE). The set of CNNs extracts complementary facial features from multimodal data. Then, the extracted features are concatenated to form a high-dimensional feature vector, whose dimension is compressed by SAE. All of the CNNs are trained using a subset of 9,000 subjects from the publicly available CASIA-WebFace database, which ensures the reproducibility of this work. Using the proposed single CNN architecture and limited training data, 98.43% verification rate is achieved on the LFW database. Benefitting from the complementary information contained in multimodal data, our small ensemble system achieves higher than 99.0% recognition rate on LFW using publicly available training set.
C1 [Ding, Changxing; Tao, Dacheng] Univ Technol Sydney, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW 2007, Australia.
   [Ding, Changxing; Tao, Dacheng] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
C3 University of Technology Sydney; University of Technology Sydney
RP Ding, CX (corresponding author), Univ Technol Sydney, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW 2007, Australia.
EM changxing.ding@student.uts.edu.au; dacheng.tao@uts.edu.au
RI Ding, Changxing/L-7075-2019; Tao, Dacheng/A-5449-2012
OI Tao, Dacheng/0000-0001-7225-5449
FU Australian Research Council [FT-130101457, DP-140102164]
FX This work was supported by the Australian Research Council under Project
   FT-130101457 and Project DP-140102164. The guest editor co-ordinating
   the review of this manuscript and approving it for publication was Dr.
   Guo-Jun Qi.
CR Ahonen T, 2008, ICPR2009, P1, DOI DOI 10.1109/ICPR.2008.4761847
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], KNOWL INF SYST
   [Anonymous], 2014, CORR
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587383
   [Anonymous], 2015, CORR
   [Anonymous], IEEE T NEUR IN PRESS
   [Anonymous], CORR
   [Anonymous], 2013, Biometrics (ICB), 2013 International Conference on
   [Anonymous], IEEE T PATT IN PRESS
   [Anonymous], 2014, CORR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2007, Technical Report
   Arashloo SR, 2014, IEEE T MULTIMEDIA, V16, P2099, DOI 10.1109/TMM.2014.2362855
   Beveridge JR, 2015, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2015.7163156
   Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41
   Choi JY, 2011, IEEE T MULTIMEDIA, V13, P14, DOI 10.1109/TMM.2010.2087320
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346
   Ding CX, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2845089
   Ding CX, 2016, IEEE T PATTERN ANAL, V38, P518, DOI 10.1109/TPAMI.2015.2462338
   Ding CX, 2015, IEEE T IMAGE PROCESS, V24, P980, DOI 10.1109/TIP.2015.2390959
   Eaton E, 2014, KNOWL INF SYST, V38, P231, DOI 10.1007/s10115-012-0577-7
   Hsieh CK, 2009, IEEE T MULTIMEDIA, V11, P600, DOI 10.1109/TMM.2009.2017606
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kannala J, 2012, INT C PATT RECOG, P1363
   Lei Z, 2014, IEEE T PATTERN ANAL, V36, P289, DOI 10.1109/TPAMI.2013.112
   Ojansivu V, 2008, LECT NOTES COMPUT SC, V5099, P236, DOI 10.1007/978-3-540-69905-7_27
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Prince S. J., 2007, PROC IEEE INT C COMP, P1
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Qi GJ, 2009, IEEE T PATTERN ANAL, V31, P1880, DOI 10.1109/TPAMI.2008.218
   Simonyan K., 2014, CORR
   Sun Y, 2014, ADV NEUR IN, V27
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Xu C, 2014, IEEE T PATTERN ANAL, V36, P1559, DOI 10.1109/TPAMI.2013.2296528
   Yi D, 2013, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2013.454
   You D, 2011, IEEE T PATTERN ANAL, V33, P631, DOI 10.1109/TPAMI.2010.173
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
NR 42
TC 300
Z9 320
U1 6
U2 190
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 2049
EP 2058
DI 10.1109/TMM.2015.2477042
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400017
OA Green Published, Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Liu, LB
   Wang, D
   Zhu, M
   Wang, YS
   Yin, SY
   Cao, P
   Yang, J
   Wei, SJ
AF Liu, Leibo
   Wang, Dong
   Zhu, Min
   Wang, Yansheng
   Yin, Shouyi
   Cao, Peng
   Yang, Jun
   Wei, Shaojun
TI An Energy-Efficient Coarse-Grained Reconfigurable Processing Unit for
   Multiple-Standard Video Decoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coarse-grained reconfigurable array; reconfigurable computing; video
   decoding
ID MEDIA PROCESSOR; H.264 DECODER; PERFORMANCE; PARALLEL; TIME
AB A coarse-grained reconfigurable processing unit (RPU) consisting of 16 x 16 multi-functional processing elements (PEs) interconnected by an area-efficient line-switched mesh connect (LSMC) routing is implemented on a 5.4 mm X 3.1 mm die in TSMC 65 nm LP1P8M CMOS technology. A hierarchical configuration context (HCC) organization scheme is proposed to reduce the implementation overhead and the energy dissipation spent on fast reconfiguration. The proposed RPU is integrated into two system-on-a-chips (SoCs), targeting multiple-standard video decoding. The high-performance chip, comprising two RPU processors (named REMUS_HPP), can decode 1920 X 1080 H.264 video streams at 30 frames per second (fps) under 200 MHz. REMUS_HPP achieves a 25% performance gain over the XPP-III reconfigurable processor with only 280 mW power consumption, resulting in a 14.3 x improvement on energy efficiency. The other chip (named REMUS_LPP), targeting low power applications, integrates only one RPU processor. REMUS_LPP can decode 720 x 480 H.264 video streams at 35fps with 24.5 mW under 75 MHz, achieving a 76% reduction in power dissipation and a 3.96x improvement on energy efficiency compared with the ADRES reconfigurable processor.
C1 [Liu, Leibo; Zhu, Min; Yin, Shouyi; Wei, Shaojun] Tsinghua Univ, Inst Microelect, Beijing 100084, Peoples R China.
   [Wang, Dong] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Wang, Yansheng] China Elect Technol Grp Corp, Beijing 100846, Peoples R China.
   [Cao, Peng; Yang, Jun] Southeast Univ, Natl ASIC Syst Engn Res Ctr, Nanjing 211189, Jiangsu, Peoples R China.
C3 Tsinghua University; Beijing Jiaotong University; China Electronics
   Technology Group; Southeast University - China
RP Wang, D (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM liulb@tsinghua.edu.cn; wangdong@bjtu.edu.cn; zhumin@tsinghua-wx.org;
   kazaam_wys@163.com; yinsy@tsinghua.edu.cn; caopeng@seu.edu.cn;
   dragon@seu.edu.cn; wsj@tsinghua.edu.cn
RI 0, 0/KEE-7704-2024; yang, jun/GQH-0635-2022; Liu, Leibo/AAE-1973-2020
OI Liu, Leibo/0000-0001-7548-4116
FU NNSF of China [60803018, 61106022]; National 863 Program of China
   [2009AA011702]
FX This work was supported by the NNSF of China under Grant 60803018 and
   Grant 61106022, and by the National 863 Program of China under Grant
   2009AA011702. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Shahram Shirani.
   (Corresponding author: Dong Wang.)
CR Aigner G., 2000, An overview of the SUIF2 compiler infrastruc- ture
   [Anonymous], 2013, H265 ITUT
   [Anonymous], 2006, WHIT PAP VID DEC XPP
   [Anonymous], 2009, MONOGR TXB PURE APPL
   [Anonymous], 2006, Tech. rep.
   Asanovic K, 2009, COMMUN ACM, V52, P56, DOI 10.1145/1562764.1562783
   Audio Video Coding Standard Workgroup of China (AVS), 2000902 AVS GBT
   Bader DA, 2006, PROC INT CONF PARAL, P523, DOI 10.1109/ICPP.2006.34
   Che S, 2008, J PARALLEL DISTR COM, V68, P1370, DOI 10.1016/j.jpdc.2008.05.014
   Compton K, 2002, ACM COMPUT SURV, V34, P171, DOI 10.1145/508352.508353
   Dou C., 2012, P 2012 SPRING C ENG, P1
   Furini M, 2001, IEEE T MULTIMEDIA, V3, P33, DOI 10.1109/6046.909592
   Ganesan MKAN, 2007, I C FIELD PROG LOGIC, P467, DOI 10.1109/FPL.2007.4380691
   Geng TS, 2010, IEICE T INF SYST, VE93D, P3223, DOI 10.1587/transinf.E93.D.3223
   Hameed R, 2010, CONF PROC INT SYMP C, P37, DOI 10.1145/1816038.1815968
   Hamzeh M, 2012, DES AUT CON, P1280
   Han K, 2013, DES AUT TEST EUROPE, P1579
   Hui Xu, 2012, 2012 IEEE Symposium on VLSI Circuits, P150, DOI 10.1109/VLSIC.2012.6243834
   Jianbin Fang, 2011, 2011 International Conference on Parallel Processing, P216, DOI 10.1109/ICPP.2011.45
   Kitaoka T, 2003, LECT NOTES COMPUT SC, V2778, P171
   Liu B, 2013, IEEE T COMPUT, V62, P536, DOI 10.1109/TC.2011.251
   Liu Dajiang., 2013, DESIGN AUTOMATION C, P1
   Liu SF, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P1, DOI 10.1109/GSIS.2013.6714728
   Maestre R., 2002, IEEE Circuits and Systems Magazine, V2, P48, DOI 10.1109/MCAS.2002.1173134
   Mei B, 2008, J SIGNAL PROCESS SYS, V51, P225, DOI 10.1007/s11265-007-0152-8
   Mori T, 2009, IEEE J SOLID-ST CIRC, V44, P2957, DOI 10.1109/JSSC.2009.2028936
   Park SJ, 2007, PROCEEDINGS OF THE HPCMP USERS GROUP CONFERENCE 2007, P350
   Park Y, 2013, ISSCC DIG TECH PAP I, V56, P160, DOI 10.1109/ISSCC.2013.6487681
   Pescador F, 2013, IEEE T CONSUM ELECTR, V59, P391, DOI 10.1109/TCE.2013.6531122
   Pescador F, 2008, IEEE T CONSUM ELECTR, V54, P145, DOI 10.1109/TCE.2008.4470037
   Pouchet L.-N., 2011, Polybench: The polyhedral benchmark suite
   Rossi D, 2010, IEEE J SOLID-ST CIRC, V45, P1615, DOI 10.1109/JSSC.2010.2048149
   Sato T, 2005, 2005 IEEE VLSI-TSA International Symposium on VLSI Design, Automation & Test (VLSI-TSA-DAT), Proceedings of Technical Papers, P323, DOI 10.1109/VDAT.2005.1500086
   Singh H, 2000, IEEE T COMPUT, V49, P465, DOI 10.1109/12.859540
   Smith M.D., 2002, An introduction to machine suif and its portable libraries for analysis and optimization
   Taherkhani Saeid, 2010, Proceedings of the 2010 IEEE 10th International Conference on Computer and Information Technology (CIT 2010), P1257, DOI 10.1109/CIT.2010.227
   Texas Instruments, 2005, SPRA962F
   Totoni E., 2012, 2012 IEEE International Symposium on Performance Analysis of Systems & Software (ISPASS), P78, DOI 10.1109/ISPASS.2012.6189208
   Tripp J., 2007, P 11 ANN WORKSH HIGH
   Tseng PC, 2005, P IEEE, V93, P184, DOI 10.1109/JPROC.2004.839622
   Yin CY, 2009, IEICE T ELECTRON, VE92C, P1284, DOI 10.1587/transele.E92.C.1284
   Yin SY, 2013, IEICE T INF SYST, VE96D, P1582, DOI 10.1587/transinf.E96.D.1582
NR 42
TC 30
Z9 35
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1706
EP 1720
DI 10.1109/TMM.2015.2463735
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400003
OA Bronze
DA 2024-07-18
ER

PT J
AU Marcos-Ramiro, A
   Pizarro, D
   Marron-Romera, M
   Gatica-Perez, D
AF Marcos-Ramiro, Alvaro
   Pizarro, Daniel
   Marron-Romera, Marta
   Gatica-Perez, Daniel
TI Let Your Body Speak: Communicative Cue Extraction on Natural Interaction
   Using RGBD Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Markerless motion capture; nonverbal cues; rgb-d fusion; social
   interaction
ID POSE ESTIMATION; BEHAVIOR; CAPTURE
AB Employment interviews are relevant scenarios for the study of social interaction. In this setting, social skills play an important role, even though the interactions between potential employers and candidates are often limited. One fundamental aspect of social interaction is the use of nonverbal communication, which affects how we are socially perceived. We present a method to automatically extract body communicative cues from one-on-one conversations recorded with Kinect devices. First, we find the three-dimensional position of hands and head of the subject, and, aided by training data, we infer the upper body pose. Then, we use the inferred poses to perform action recognition and build person-specific activity descriptors. We evaluate our system with both domain-specific and public, generic datasets, and show competitive performance.
C1 [Marcos-Ramiro, Alvaro; Pizarro, Daniel; Marron-Romera, Marta] Univ Alcala de Henares, Dept Elect, Madrid 28871, Spain.
   [Gatica-Perez, Daniel] Idiap Res Inst, Social Comp Grp, CH-1920 Martigny, Switzerland.
   [Gatica-Perez, Daniel] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
C3 Universidad de Alcala; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne
RP Marcos-Ramiro, A (corresponding author), Univ Alcala de Henares, Dept Elect, Madrid 28871, Spain.
EM amarcos@depeca.uah.es; pizarro@depeca.uah.es; marta@depeca.uah.es;
   gatica@idiap.ch
RI Marta, Marrón Romera/AAD-9385-2021; Marta, Marrón-Romera/T-5343-2017
OI Marta, Marrón Romera/0000-0001-7723-2262; Marta,
   Marrón-Romera/0000-0001-7723-2262; Pizarro, Daniel/0000-0003-0622-4884
FU University of Alcala FPI Program; SNSF SONVB Project; SNSF UBImpressed
   Project; Spanish Ministry of Economy and Competitiveness
   [TIN2013-47630-C2-1-R]
FX This work was supported by the University of Alcala FPI Program, the
   SNSF SONVB and UBImpressed Projects, and the Spanish Ministry of Economy
   and Competitiveness under Project SPACES-UAH (TIN2013-47630-C2-1-R). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jiebo Luo.
CR [Anonymous], 2011, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2011.5995316, 10.1109/CVPR.2011.5995316]
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Curhan JR, 2007, J APPL PSYCHOL, V92, P802, DOI 10.1037/0021-9010.92.3.802
   Eichner M, 2012, INT J COMPUT VISION, V99, P190, DOI 10.1007/s11263-012-0524-9
   Feese S, 2011, IEEE INT SYM WRBL CO, P119, DOI 10.1109/ISWC.2011.31
   Ferrari V, 2009, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2009.5206495
   FORBES RJ, 1980, J OCCUP PSYCHOL, V53, P65, DOI 10.1111/j.2044-8325.1980.tb00007.x
   Gall J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1969, DOI 10.1109/CVPR.2011.5995582
   Gall J, 2012, P EUR C COMPUT VIS, P425
   Gatica-Perez D, 2009, IMAGE VISION COMPUT, V27, P1775, DOI 10.1016/j.imavis.2009.01.004
   Gijsenij A, 2011, IEEE T IMAGE PROCESS, V20, P2475, DOI 10.1109/TIP.2011.2118224
   Hernandez Javier., 2013, Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on, P1, DOI DOI 10.1109/FG.2013.6553742
   Huayan Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2433, DOI 10.1109/CVPR.2011.5995722
   IMADA AS, 1977, J APPL PSYCHOL, V62, P295, DOI 10.1037/0021-9010.62.3.295
   Knapp M.L., 2013, Cengage Learning
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Nguyen L, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P289
   Liu YB, 2011, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2011.5995424
   Lopez-Mendez A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P391, DOI 10.1109/ICCVW.2011.6130268
   Marcos- Ramiro A., 2014, THESIS U ALCALA MADR
   Marcos-Ramiro A., 2013, 10 IEEE INT C WORKSH, P1, DOI DOI 10.1109/FG.20120133.6553741
   McNeill D., 1992, Hand and Mind: What Gestures Reveal about Thought
   McNeill D., 2005, GESTURE THOUGHT, DOI DOI 10.7208/CHICAGO/9780226514642.001.0001
   Mehrabian A, 1972, COMMUNICATION
   Morency L.-P., 2008, ICMI '08: Proceedings of the 10th international conference on Multimodal interfaces, P181, DOI DOI 10.1145/1452392.1452426
   Natarajan P, 2010, PROC CVPR IEEE, P2006, DOI 10.1109/CVPR.2010.5539876
   Pantic M, 2007, LECT NOTES COMPUT SC, V4451, P47
   Pentland Alex, 2008, HONEST SIGNALS THEY
   Sanabria-Macías F, 2013, IEEE IND ELEC, P7825, DOI 10.1109/IECON.2013.6700440
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471
   Scheffler C, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.53
   Scherer S, 2013, IEEE INT CONF AUTOMA
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Taylor J, 2012, PROC CVPR IEEE, P103, DOI 10.1109/CVPR.2012.6247664
   Urtasun R., 2006, THESIS ECOLE POLITEC
   Wu D, 2012, COMPUT GRAPH FORUM, V31, P2019, DOI 10.1111/j.1467-8659.2012.03194.x
   Yang Y., 2011, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, page, V1385-1392, P2011, DOI DOI 10.1109/CVPR.2011.5995741
   Yao A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.67
   Yao A, 2012, INT J COMPUT VISION, V100, P16, DOI 10.1007/s11263-012-0532-9
   Yin Y, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P489, DOI 10.1145/2522848.2532588
NR 41
TC 19
Z9 22
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1721
EP 1732
DI 10.1109/TMM.2015.2464152
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400004
DA 2024-07-18
ER

PT J
AU Valsesia, D
   Coluccia, G
   Bianchi, T
   Magli, E
AF Valsesia, Diego
   Coluccia, Giulio
   Bianchi, Tiziano
   Magli, Enrico
TI Large-Scale Image Retrieval Based on Compressed Camera Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image forensics; image search and retrieval; photo response
   non-uniformity (PRNU); random projections
ID SENSOR; ORIGIN
AB Retrieving pictures from large collections according to a specific criterion is an increasingly relevant task. An important, but so far overlooked, such criterion is the retrieval of pictures acquired by a specific camera. Instead of relying on metadata, which can be absent or easily manipulated, a forensic tool is exploited, namely the photo response non-uniformity (PRNU) of the camera sensor. Recent works showed that random projections can be used to significantly compress the PRNU, enabling operation on very large scales, previously impossible due to the size of the PRNU and to the complexity of the matching operations. In this paper, we propose efficient techniques for management and retrieval of images employing the PRNU, and test them on a database of 1174 cameras and half a million pictures downloaded from the Internet.
C1 [Valsesia, Diego; Coluccia, Giulio; Bianchi, Tiziano; Magli, Enrico] Politecn Torino, Dept Elect & Telecommun, I-2410129 Turin, Italy.
C3 Polytechnic University of Turin
RP Valsesia, D (corresponding author), Politecn Torino, Dept Elect & Telecommun, I-2410129 Turin, Italy.
EM diego.valsesia@polito.it; giulio.coluccia@polito.it;
   tiziano.bianchi@polito.it; enrico.magli@polito.it
RI VALSESIA, DIEGO/U-3135-2019; Bianchi, Tiziano/E-4796-2011
OI VALSESIA, DIEGO/0000-0003-1997-2910; Bianchi,
   Tiziano/0000-0002-3965-3522
FU European Research Council under the European Community [279848]
FX This work was supported by the European Research Council under the
   European Community's Seventh Framework Programme under Grant
   FP7/2007-2013 and ERC Grant Agreement 279848. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Ramesh Jain.
CR Amerini I, 2014, SIGNAL PROCESS-IMAGE, V29, P831, DOI 10.1016/j.image.2014.07.003
   [Anonymous], 2012, CAM FING SOFTW
   [Anonymous], MULTIMEDIA TOOLS APP
   [Anonymous], 2010, 2010 IEEE INT WORKSH
   Bayram S, 2012, IEEE T INF FOREN SEC, V7, P1404, DOI 10.1109/TIFS.2012.2192272
   Caldelli Roberto., 2010, 2010 IEEE International Workshop on Information Forensics and Security, P1
   Chen M, 2008, IEEE T INF FOREN SEC, V3, P74, DOI 10.1109/TIFS.2007.916285
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   DAVID HA, 1970, ORDER STAT
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Fridrich J, 2009, IEEE SIGNAL PROC MAG, V26, P26, DOI 10.1109/MSP.2008.931078
   Gloe T., 2012, Proceedings of the on Multimedia and security, P109
   Goljan M., 2009, P SOC PHOTO-OPT INS, V7254
   Goljan M., 2013, P SPIE MEDIA WATERWA, V8665
   Goljan M., 2010, P SPIE MEDIA FORENSI, V7541, p[754, 108, 108]
   Hinrichs A, 2011, RANDOM STRUCT ALGOR, V39, P391, DOI 10.1002/rsa.20360
   Hu Y., 2009, Proceedings of Int Conf Computer Science Applications, V23, P1, DOI DOI 10.1109/IAS.2009.5324874
   Hu YP, 2012, ADV MATER SCI ENG, V2012, DOI 10.1155/2012/185905
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Johnson W.B., 1984, C MODERN ANAL PROBAB, V26
   Kremerskothen K., 2011, 6000000000
   Li CT, 2010, IEEE INT SYMP CIRC S, P3429, DOI 10.1109/ISCAS.2010.5537850
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lukás J, 2005, PROC SPIE, V5685, P249, DOI 10.1117/12.587105
   Lukás J, 2006, IEEE T INF FOREN SEC, V1, P205, DOI 10.1109/TIFS.2006.873602
   Penatti OAB, 2012, J VIS COMMUN IMAGE R, V23, P359, DOI 10.1016/j.jvcir.2011.11.002
   Rauhut H., 2009, P SIGN PROC AD SPARS, P1
   TEUHOLA J, 1978, INFORM PROCESS LETT, V7, P308, DOI 10.1016/0020-0190(78)90024-8
   Valsesia D, 2015, INT CONF ACOUST SPEE, P1697, DOI 10.1109/ICASSP.2015.7178260
   Valsesia D, 2015, IEEE T INF FOREN SEC, V10, P1472, DOI 10.1109/TIFS.2015.2415461
   Wang JQ, 2006, IEEE T SYST MAN CY B, V36, P413, DOI 10.1109/TSMCB.2005.859085
NR 32
TC 23
Z9 23
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1439
EP 1449
DI 10.1109/TMM.2015.2455417
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000005
DA 2024-07-18
ER

PT J
AU Kuanar, SK
   Ranga, KB
   Chowdhury, AS
AF Kuanar, Sanjay K.
   Ranga, Kunal B.
   Chowdhury, Ananda S.
TI Multi-View Video Summarization Using Bipartite Matching Constrained
   Optimum-Path Forest Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bipartite matching; Gaussian entropy; multi-view video summarization;
   optimum-path forest; visual bag of words
ID SCENE DETECTION; COLOR; TRANSFORM; FRAMEWORK
AB The task of multi-view video summarization is to efficiently represent the most significant information from a set of videos captured for a certain period of time by multiple cameras. The problem is highly challenging because of the huge size of the data, presence of many unimportant frames with low activity, inter-view dependencies, and significant variations in illumination. In this paper, we propose a graph-theoretic solution to the above problems. Semantic feature in form of visual bag of words and visual features like color, texture, and shape are used to model shot representative frames after temporal segmentation. Gaussian entropy is then applied to filter out frames with low activity. Inter-view dependencies are captured via bipartite graph matching. Finally, the optimum-path forest algorithm is applied for the clustering purpose. Subjective as well as objective evaluations clearly indicate the effectiveness of the proposed approach.
C1 [Kuanar, Sanjay K.; Ranga, Kunal B.; Chowdhury, Ananda S.] Jadavpur Univ, Dept Elect & Telecommun Engn, Kolkata 700032, India.
C3 Jadavpur University
RP Kuanar, SK (corresponding author), Jadavpur Univ, Dept Elect & Telecommun Engn, Kolkata 700032, India.
EM sanjay.kuanar@gmail.com; kunalranga@gmail.com;
   aschowdhury@etce.jdvu.ac.in
RI ; Kuanar, Dr. Sanjay/C-7247-2019
OI Chowdhury, Ananda/0000-0002-5799-3467; Kuanar, Dr.
   Sanjay/0000-0001-8229-2956
CR Adjeroh DA, 1997, J VIS COMMUN IMAGE R, V8, P182, DOI 10.1006/jvci.1997.0349
   Amel A. M., 2010, J TELECOMMUN, V2, P54
   [Anonymous], 2001, PATTERN CLASSIFICATI
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Ben-Hur A, 2010, METHODS MOL BIOL, V609, P223, DOI 10.1007/978-1-60327-241-4_13
   Bollegala D, 2011, IEEE T KNOWL DATA EN, V23, P977, DOI 10.1109/TKDE.2010.172
   BRODER AZ, 1994, SIAM J COMPUT, V23, P324, DOI 10.1137/S0097539790190144
   Cappabianco FAM, 2012, COMPUT VIS IMAGE UND, V116, P1047, DOI 10.1016/j.cviu.2012.06.002
   Chasanis VT, 2009, IEEE T MULTIMEDIA, V11, P89, DOI 10.1109/TMM.2008.2008924
   Chowdhury AS, 2012, INT C PATT RECOG, P3108
   Cotsaces C, 2006, IEEE SIGNAL PROC MAG, V23, P28, DOI 10.1109/MSP.2006.1621446
   De Leo C, 2014, ACM T SENSOR NETWORK, V10, DOI 10.1145/2530285
   Falcao AX, 2004, IEEE T PATTERN ANAL, V26, P19, DOI 10.1109/TPAMI.2004.1261076
   Fu Y, 2014, CORR
   Fu YW, 2010, IEEE T MULTIMEDIA, V12, P717, DOI 10.1109/TMM.2010.2052025
   Han B., 2011, 2011 IEEE WORKSH APP, P51
   Hannon J., 2011, P 16 INT C INT US IN, P335, DOI DOI 10.1145/1943403.1943459
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Jianguo Wang, 2011, Proceedings of the 2011 2nd International Conference on Control, Instrumentation, and Automation (ICCIA), P1, DOI 10.1109/ICCIAutom.2011.6183890
   Kuanar SK, 2013, J VIS COMMUN IMAGE R, V24, P1212, DOI 10.1016/j.jvcir.2013.08.003
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1959, DOI 10.1109/ICME.2004.1394645
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Macqueen J., 1967, Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, P297
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Mohanta P. P., 2010, P 7 IND C COMP VIS G, P464
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Napoletano P., 2014, CORR
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Ou SH, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2331916
   Papa J. P., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4162, DOI 10.1109/ICPR.2010.1012
   Papadimitrou CH, 1982, COMBINATORIAL OPTIMI
   Paschos G, 2001, IEEE T IMAGE PROCESS, V10, P932, DOI 10.1109/83.923289
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Ping Li, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2473, DOI 10.1109/ICIP.2011.6116162
   Rasheed Z, 2005, IEEE T MULTIMEDIA, V7, P1097, DOI 10.1109/TMM.2005.858392
   Rocha LM, 2009, INT J IMAG SYST TECH, V19, P50, DOI 10.1002/ima.20191
   Sasongko J., 2008, P TRECVID BBC RUSH S, P1
   Shokoufandeh Ali, 1999, P ICCV WORKSH GRAPH, V2, P1
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Zabih R, 1999, MULTIMEDIA SYST, V7, P119, DOI 10.1007/s005300050115
   Zhu XB, 2014, MACH VISION APPL, V25, P145, DOI 10.1007/s00138-013-0519-8
NR 45
TC 40
Z9 44
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1166
EP 1173
DI 10.1109/TMM.2015.2443558
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000004
DA 2024-07-18
ER

PT J
AU Fang, Q
   Sang, JT
   Xu, CS
   Hossain, MS
AF Fang, Quan
   Sang, Jitao
   Xu, Changsheng
   Hossain, M. Shamim
TI Relational User Attribute Inference in Social Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attribute relation; latent SVM (LSVM); user attribute inference
AB Nowadays, more and more people are engaged in social media to generate multimedia information, i.e., creating text and photo profiles and posting multimedia messages. Such multimodal social networking activities reveal multiple user attributes such as age, gender, and personal interest. Inferring user attributes is important for user profiling, retrieval, and personalization. Existing work is devoted to inferring user attributes independently and ignores the dependency relations between attributes. In this work, we investigate the problem of relational user attribute inference by exploring the relations between user attributes and extracting both lexical and visual features from online user-generated content. We systematically study six types of user attributes: gender, age, relationship, occupation, interest, and emotional orientation. In view of methodology, we propose a relational latent SVM (LSVM) model to combine a rich set of user features, attribute inference, and attribute relations in a unified framework. In the model, one attribute is selected as the target attribute and others are selected as the auxiliary attributes to assist the target attribute inference. The model infers user attributes and attribute relations simultaneously. Extensive experiments conducted on a collected dataset from Google+ with full attribute annotations demonstrate the effectiveness of the proposed approach in user attribute inference and attribute-based user retrieval.
C1 [Fang, Quan; Sang, Jitao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, SWE Dept, Riyadh 12372, Saudi Arabia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; King Saud
   University
RP Fang, Q (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM qfang@nlpr.ia.ac.cn; jtsang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn;
   mshossain@ksu.edu.sa
RI Guizani, Mohsen/AAX-4534-2021; xu, cj/HJZ-3488-2023; Hossain, M.
   Shamim/K-1362-2014
OI Guizani, Mohsen/0000-0002-8972-8094; Hossain, M.
   Shamim/0000-0001-5906-9422
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009, 61432019, 61332016,
   61303176]; Beijing Natural Science Foundation [4131004]; Deanship of
   Scientific Research at King Saud University under the International
   Research Group Program [IRG 14-18]
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2012CB316304, in part by the National Natural
   Science Foundation of China under Grant 61225009, Grant 61432019, Grant
   61332016, and Grant 61303176, in part by the Beijing Natural Science
   Foundation under Grant 4131004, and by the Deanship of Scientific
   Research at King Saud University under the International Research Group
   Program IRG 14-18. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. K. Selcuk Candan.
CR [Anonymous], 2012, P 2012 INT MEAS C, DOI DOI 10.1145/2398776.2398794
   [Anonymous], 2013, P 51 ANN M ASS COMPU
   [Anonymous], 2012, ICWSM
   [Anonymous], 2010, P 19 ACM INT C INFOR, DOI DOI 10.1145/1871437.1871535
   [Anonymous], P 2 INT WORKSH SEARC
   [Anonymous], 2005, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics
   [Anonymous], 2010, EMNLP
   [Anonymous], 2008, Cambridge Series in Statistical and Probabilistic Mathematics
   Bergsma S., 2013, P 2013 C N AM CHAPT, P1010
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Burger JohnD., 2006, AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs, P15
   Cohen R., 2013, Proceedings of the Seventh International AAAI Conference on Weblogs and Social Media, V7, P91, DOI [10.1609/icwsm.v7i1.14434, DOI 10.1609/ICWSM.V7I1.14434, https://doi.org/10.1609/icwsm.v7i1.14434]
   Conover M. D., 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P192, DOI 10.1109/PASSAT/SocialCom.2011.34
   Culotta A, 2015, AAAI CONF ARTIF INTE, P72
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Do T. M. T., 2009, ICML, P34
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Felzenszwalb P., 2008, PROC IEEE C COMPUT V, P1
   Filatova E, 2012, DATA KNOWL ENG, V76-78, P39, DOI 10.1016/j.datak.2012.04.001
   Garera N., 2009, Proceedings of EACL, P300
   Garera Nikesh., 2009, P JOINT C 47 ANN M A, V2, P710
   Herring SC, 2006, J SOCIOLING, V10, P439, DOI 10.1111/j.1467-9841.2006.00287.x
   Mann G.S., 2005, P ASS COMPUTATIONAL, P483
   Mislove A., 2010, P 3 ACM INT C WEB SE, P251, DOI [DOI 10.1145/1718487.1718519, 10.1145/1718487.1718519]
   Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79, DOI 10.3115/1118693.1118704
   Pennacchiotti M, 2011, KDD '11, P430, DOI DOI 10.1145/2020408.2020477
   Sarawgi R., 2011, P 15 C COMP NAT LANG, P78
   Tan Chenhao., 2011, P 17 ACM SIGKDD INT, P1397, DOI DOI 10.1145/2020408.2020614
   Thomas M, 2006, P 2006 C EMP METH NA, P327
   Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273
   Van Durme B., 2012, Proceedings of EMNLP, P48
   Volkova S, 2015, AAAI CONF ARTIF INTE, P2325
   Volkova Svitlana, 2013, EMNLP, P1815
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Weber I, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P523
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Yu K., 2005, P 43 ANN M ASS COMPU, P499, DOI [10.3115/1219840.1219902, DOI 10.3115/1219840.1219902]
   Yu T., 2009, P 26 ANN INT C MACH, P1169, DOI DOI 10.1145/1553374.1553523
   Zheleva E., 2009, P 18 INT C WORLD WID, P531, DOI [10.1145/1526709.1526781, DOI 10.1145/1526709.1526781]
   Zhou L., 2004, P C EMPIRICAL METHOD, P434
   Zhu Jianke., 2008, P 16 ACM INT C MULTI, P41
NR 43
TC 52
Z9 57
U1 3
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 1031
EP 1044
DI 10.1109/TMM.2015.2430819
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300010
DA 2024-07-18
ER

PT J
AU Wang, Z
   Zhu, WW
   Chen, MH
   Sun, LF
   Yang, SQ
AF Wang, Zhi
   Zhu, Wenwu
   Chen, Minghua
   Sun, Lifeng
   Yang, Shiqiang
TI CPCDN: Content Delivery Powered by Context and User Intelligence
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content delivery; data mining; user behavior; QoS
AB There is an unprecedented trend that content providers (CPs) are building their own content delivery networks (CDNs) to provide a variety of content services to their users. By exploiting powerful CP-level information in content distribution, these CP-built CDNs open up a whole new design space and are changing the content delivery landscape. In this paper, we adopt a measurement-based approach to understanding why, how, and how much CP-level intelligences can help content delivery. We first present a measurement study of the CDN built by Tencent, a largest content provider based in China. We observe new characteristics and trends in content delivery which pose great challenges to the conventional content delivery paradigm and motivate the proposal of CPCDN, a CDN powered by CP-aware information. We then reveal the benefits obtained by exploiting two indispensable CP-level intelligences, namely context intelligence and user intelligence, in content delivery. Inspired by the insights learnt from the measurement studies, we systematically explore the design space of CPCDN and present the novel architecture and algorithms to address the new content delivery challenges that have arisen. Our results not only demonstrate the potential of CPCDN in pushing content delivery performance to the next level, but also identify new research problems calling for further investigation.
C1 [Wang, Zhi] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518057, Peoples R China.
   [Zhu, Wenwu; Sun, Lifeng; Yang, Shiqiang] Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Chen, Minghua] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Tsinghua University; Tsinghua Shenzhen International Graduate School;
   Tsinghua University; Chinese University of Hong Kong
RP Wang, Z (corresponding author), Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518057, Peoples R China.
EM wangzhi@sz.tsinghua.edu.cn; wwzhu@tsinghua.edu.cn;
   minghua@ie.cuhk.edu.hk; sunlf@tsinghua.edu.cn; yangshq@tsinghua.edu.cn
RI yang, shiqiang/AAH-5484-2019; Chen, Minghua/A-7476-2012
OI Chen, Minghua/0000-0003-4763-0037
FU National Basic Research Program of China (973) [2011CB302206]; National
   Natural Science Foundation of China [61210008, 61272231, 61402247];
   SZSTI [JCYJ20140417115840259]; Tsinghua-Tencent Joint Laboratory for
   Internet Innovation Technology, Beijing Key Laboratory of Networked
   Multimedia; University Grants Committee of the Hong Kong Special
   Administrative Region, China under General Research Fund Project
   [14201014]
FX This work was supported in part by the National Basic Research Program
   of China (973) under Grant 2011CB302206, the National Natural Science
   Foundation of China under Grant 61210008, Grant 61272231, and Grant
   61402247, SZSTI under Grant JCYJ20140417115840259, the research fund of
   Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology,
   Beijing Key Laboratory of Networked Multimedia, and the University
   Grants Committee of the Hong Kong Special Administrative Region, China
   under General Research Fund Project 14201014. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Pal Halvorsen.
CR Ager B., 2011, Proceedings of the 2011 ACM SIGCOMM Conference on Internet Measurement Conference, IMC '11, P585
   Bakshy E., 2011, P 4 ACM INT C WEB SE, P65
   Brewington BE, 2000, COMPUT NETW, V33, P257, DOI 10.1016/S1389-1286(00)00045-1
   Cha M, 2009, WWW 09 P 18 INT WORL, DOI DOI 10.1145/1526709.1526806
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chen FF, 2012, IEEE INFOCOM SER, P433, DOI 10.1109/INFCOM.2012.6195782
   Datta A, 2004, ACM T DATABASE SYST, V29, P403, DOI 10.1145/1005566.1005571
   Dhillon I. S., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P269, DOI 10.1145/502512.502550
   Di Marco A, 2013, COMPUT LINGUIST, V39, P709, DOI 10.1162/COLI_a_00148
   Ellison NB, 2007, J COMPUT-MEDIAT COMM, V12, P1143, DOI 10.1111/j.1083-6101.2007.00367.x
   Frank B., 2010, P 10 ACM SIGCOMM C I, P22
   Huberman B., 2008, COMPUT RES REPOSITOR, Vabs/0812.1045
   Kommula S., 2007, U.S. Patent, Patent No. [7,254,626, 7254626]
   Kwak H., WWW'10, DOI DOI 10.1145/1772690.1772751
   Lazar I., 2001, IT Professional, V3, P47, DOI 10.1109/6294.946620
   Lemlouma T, 2003, 2003 SYMPOSIUM ON APPLICATIONS AND THE INTERNET, PROCEEDINGS, P190, DOI 10.1109/SAINT.2003.1183048
   Li Haitao., 2012, P 22 INT WORKSHOP NE, P83
   Li Z., 2012, Proc. of ACM Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV), P33, DOI DOI 10.1145/2229087.2229097
   Liu HH, 2012, ACM SIGCOMM COMP COM, V42, P371, DOI 10.1145/2377677.2377753
   O'reilly T., 2009, WHAT IS WEB 2 0
   Pujol JM, 2010, ACM SIGCOMM COMP COM, V40, P375, DOI 10.1145/1851275.1851227
   Scellato S., 2011, Proceedings of the 20th International Conference on World Wide Web, P457
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Verbert K, 2012, IEEE T LEARN TECHNOL, V5, P318, DOI 10.1109/TLT.2012.11
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wang Z, 2012, IEEE INFOCOM SER, P2901, DOI 10.1109/INFCOM.2012.6195726
   Wang Zhi., 2012, Proceedings of the 20th ACM international conference on Multimedia, MM '12, P29
   Wei JB, 2011, IEEE T PARALL DISTR, V22, P773, DOI 10.1109/TPDS.2010.131
   Yin H., 2009, Proceedings of ACM International Conference on Multimedia, P25
NR 29
TC 23
Z9 25
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2015
VL 17
IS 1
BP 92
EP 103
DI 10.1109/TMM.2014.2365364
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AX6QW
UT WOS:000347047400009
DA 2024-07-18
ER

PT J
AU Yang, Y
   Yang, LJ
   Wu, GS
   Li, SP
AF Yang, Yang
   Yang, Linjun
   Wu, Gangshan
   Li, Shipeng
TI Image Relevance Prediction Using Query-Context Bag-of-Object Retrieval
   Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Common object discovery; image search reranking; object vocabulary; web
   image search
ID FEEDBACK
AB Image search reranking and image research result summarization are two effective approaches which enhance text-based image search results using visual information. Since the existing approaches optimize search relevance in terms of average performance, they usually cannot achieve satisfactory results for some particular classes of queries, like "object queries," which is defined as the queries with the intent of searching for some kinds of objects. One possible reason is that the generic approaches such as [40], [43], [46] are mostly built based on the global statistics of images as features while ignoring the fact that the relevance between the image and the query sometimes depends on an image patch instead of the whole image. In this paper, we therefore design a novel bag-of-object retrieval model to predict image relevance, which is particularly effective for object queries. First, we construct an object vocabulary containing query-relative objects by mining frequent object patches from the result image collection of the expanded query set. After representing each image as a bag of objects, our retrieval model can be derived from a risk-minimization framework for language modeling. To demonstrate the effectiveness of the proposed model, this paper also present two related applications: for image search reranking, we adopt a supervised framework to combine multiple ranking features from different assumptions; for image search result summarization, we propose a two-step ranking process which optimizes not only representativeness but also image attractiveness. The experimental results show that the proposed methods can significantly outperform the existing approaches.
C1 [Yang, Yang; Wu, Gangshan] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210093, Jiangsu, Peoples R China.
   [Yang, Linjun] Microsoft Corp, Redmond, WA 98052 USA.
   [Li, Shipeng] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Nanjing University; Microsoft; Microsoft; Microsoft Research Asia
RP Yang, Y (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210093, Jiangsu, Peoples R China.
EM charlie.yang.nju@gmail.com; linjuny@microsoft.com; gswu@nju.edu.cn;
   spli@nju.edu.cn
RI Li, Shipeng/AAA-3374-2020
OI Li, Shipeng/0000-0001-5368-4256
FU NSFC of China [61321491]; 863 Program of China [2011AA01A202]; National
   Special Fund [2011ZX05035-004-004HZ]
FX This work is supported by the NSFC of China under Grant 61321491, the
   863 Program of China under Grant 2011AA01A202, and the National Special
   Fund under Grant 2011ZX05035-004-004HZ. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Xiao-Ping Zhang.
CR [Anonymous], P ACM MULT
   [Anonymous], 2006, P ACMSIGKDD INT C KN
   [Anonymous], P NIPS
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], 2010, P ACM INT C MULT
   [Anonymous], 2010, ADV NEURAL PROCESSIN
   [Anonymous], P 20 ACM INT C MULT
   [Anonymous], IMAGE VIDEO RETRIEVA
   Bian W, 2010, IEEE T IMAGE PROCESS, V19, P545, DOI 10.1109/TIP.2009.2035223
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X
   Chowdhury G. G., 2010, Introduction to modern information retrieval
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Deselaers T, 2010, LECT NOTES COMPUT SC, V6314, P452, DOI 10.1007/978-3-642-15561-1_33
   Fan J., 2008, Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval, ser. MIR'08, Vancouver, British Columbia, P358, DOI [DOI 10.1145/1460096.1460155, 10.1145/1460096.1460155]
   Feng J, 2011, IEEE I CONF COMP VIS, P1028, DOI 10.1109/ICCV.2011.6126348
   Fergus R, 2005, IEEE I CONF COMP VIS, P1816
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Fritz M., 2008, CVPR, P1
   Hochbaum DS, 2009, IEEE I CONF COMP VIS, P269, DOI 10.1109/ICCV.2009.5459261
   Hohl L, 2004, IEEE IMAGE PROC, P1609
   Hsu WinstonH., 2007, ACM MM
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Joachims T, 1999, ADVANCES IN KERNEL METHODS, P169
   Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Kolmogorov V., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91
   Krapac J, 2010, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR.2010.5540092
   Lafferty J., 2001, SIGIR, P111, DOI DOI 10.1145/383952.383970
   Lee YJ, 2010, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2010.5540237
   Li WJ, 2009, PROC CVPR IEEE, P1666, DOI 10.1109/CVPRW.2009.5206796
   Li YX, 2012, IEEE T MULTIMEDIA, V14, P1618, DOI 10.1109/TMM.2012.2199292
   Li YF, 2009, LECT NOTES ARTIF INT, V5782, P15
   Liu R, 2010, LECT NOTES COMPUT SC, V5996, P485
   Liu Y, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P297, DOI 10.1109/ICME.2008.4607430
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Raguram Rahul, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4562959
   Schroff F, 2007, IEEE I CONF COMP VIS, P2120
   Simon I., 2007, IEEE11TH INT C COMPU, P1
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Tian X., 2008, ACM INT C MULTIMEDIA, P131, DOI DOI 10.1145/1459359.1459378.ISBN
   Tian XM, 2010, LECT NOTES COMPUT SC, V5916, P163, DOI 10.1007/978-3-642-11301-7_19
   Tian Xinmei, 2011, P ACM INT C MULT, P363
   Vicente S, 2010, LECT NOTES COMPUT SC, V6312, P465, DOI 10.1007/978-3-642-15552-9_34
   Yang LJ, 2013, IEEE MULTIMEDIA, V20, P13, DOI 10.1109/MMUL.2012.30
   Yang LJ, 2011, IEEE T MULTIMEDIA, V13, P1295, DOI 10.1109/TMM.2011.2162399
   Yang Y.H., 2008, ACM International Conference on Multimedia (MM), P199
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
NR 50
TC 10
Z9 11
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1700
EP 1712
DI 10.1109/TMM.2014.2326836
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200018
DA 2024-07-18
ER

PT J
AU Lee, K
   Lee, K
AF Lee, Kibeom
   Lee, Kyogu
TI Using Dynamically Promoted Experts for Music Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Algorithm Design and Analysis; Information Retrieval; Recommender
   Systems
AB Recommender systems have become an invaluable asset to online services with the ever-growing number of items and users. Most systems focused on recommendation accuracy, predicting likable items for each user. Such methods tend to generate popular and safe recommendations, but fail to introduce users to potentially risky, yet novel items that could help in increasing the variety of items consumed by the users. This is known as popularity bias, which is predominant in methods that adopt collaborative filtering. Recently, however, recommenders have started to improve their methods to generate lists that encompass diverse items that are both accurate and novel through specific novelty-driven algorithms or hybrid recommender systems. In this paper, we propose a recommender system that uses the concepts of Experts to find both novel and relevant recommendations. By analyzing the ratings of the users, the algorithm promotes special Experts from the user population to create novel recommendations for a target user. Thus, different users are promoted dynamically to Experts depending on who the recommendations are for. The system used data collected from Last.fm and was evaluated with several metrics. Results show that the proposed system outperforms matrix factorization methods in finding novel items and performs on par in finding simultaneously novel and relevant items. This system can also provide a means to popularity bias while preserving the advantages of collaborative filtering.
C1 [Lee, Kibeom; Lee, Kyogu] Seoul Natl Univ, Dept Transdisciplinary Studies, Seoul, South Korea.
C3 Seoul National University (SNU)
RP Lee, K (corresponding author), Seoul Natl Univ, Dept Transdisciplinary Studies, Seoul, South Korea.
EM kiblee@snu.ac.kr; kglee@snu.ac.kr
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Education, Science and Technology
   [2011-0013476]
FX This work was supported by the Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Education, Science and Technology (2011-0013476). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Tao Li.
CR Akiyama T., 2010, PRSAT@ RecSys, P3
   Amatriain X, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P532, DOI 10.1145/1571941.1572033
   [Anonymous], 2001, Proc. IEEE International Conference on Multimedia and Expo
   Aucouturier J.-J., 2002, P 3 INT S MUS INF OC
   Burke R, 2002, USER MODEL USER-ADAP, V12, P331, DOI 10.1023/A:1021240730564
   Celma O., 2008, P 2 KDD WORKSH LARG, P1
   Dhillon IS, 2003, P 9 ACM SIGKDD INT C, P89, DOI DOI 10.1145/956750.956764
   GOLDBERG D, 1992, COMMUN ACM, V35, P61, DOI 10.1145/138859.138867
   Herlocker J. L., 2000, CSCW 2000. ACM 2000 Conference on Computer Supported Cooperative Work, P241, DOI 10.1145/358916.358995
   Hoffman M., 2008, Proc. ISMIR, P349
   Kamahara J, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P433, DOI 10.1109/MMMC.2005.5
   Kibeom Lee, 2013, Journal of Computing Science and Engineering, V7, P21, DOI 10.5626/JCSE.2013.7.1.21
   Kim SW, 2009, ONLINE INFORM REV, V33, P584, DOI 10.1108/14684520910969970
   Konstan JA, 1997, COMMUN ACM, V40, P77, DOI 10.1145/245108.245126
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Kumar A., 2012, INT J COMPUT APPL JA, V37, P7
   Lee K., 2010, P IEEE SIL NAN WORKS, P47
   Lin Chen., 2012, Proceedings of the 21st ACM international conference on Information and knowledge management, P1607
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   McFee B, 2011, J MACH LEARN RES, V12, P491
   MCNEE SM, 2006, EXT ABSTR 2006 ACM C
   McNee SM, 2002, P 2002 ACM C COMP SU, P116, DOI [DOI 10.1145/587078.587096, 10.1145/587078.587096]
   McNee SM, 2006, CHI 06 HUM FACT COMP, P1097, DOI DOI 10.1145/1125451.1125659
   Murakami T, 2008, LECT NOTES ARTIF INT, V4914, P40
   Nanopoulos A, 2010, IEEE T AUDIO SPEECH, V18, P407, DOI 10.1109/TASL.2009.2033973
   Onuma K, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P657
   Resnick P., 1994, Transcending Boundaries, CSCW '94. Proceedings of the Conference on Computer Supported Cooperative Work, P175, DOI 10.1145/192844.192905
   Shan HH, 2008, IEEE DATA MINING, P530, DOI 10.1109/ICDM.2008.91
   Shao B, 2009, IEEE T AUDIO SPEECH, V17, P1602, DOI 10.1109/TASL.2009.2020893
   Slaney M., 2008, Proc. ISMIR, P313
   Vargas S., 2011, P 5 ACM C RECOMMENDE, P109, DOI DOI 10.1145/2043932.2043955
   Vozalis MG, 2005, 5th International Conference on Intelligent Systems Design and Applications, Proceedings, P464, DOI 10.1109/ISDA.2005.25
   Wang C., 2010, P 18 ACM INT C MULT, P391, DOI [10.1145/ 1873951.1874005, DOI 10.1145/1873951.1874005]
   Zhang Yuan Cao, 2012, P 5 ACM INT C WEB SE, P13, DOI [10.1145/2124295.2124300, DOI 10.1145/2124295.2124300]
   Zhou T, 2010, P NATL ACAD SCI USA, V107, P4511, DOI 10.1073/pnas.1000488107
   Ziegler Cai-Nicolas, 2005, P 14 INT C WORLD WID, P22, DOI DOI 10.1145/1060745.1060754
NR 36
TC 28
Z9 31
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1201
EP 1210
DI 10.1109/TMM.2014.2311012
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600003
DA 2024-07-18
ER

PT J
AU Deng, C
   Ji, RR
   Tao, DC
   Gao, XB
   Li, XL
AF Deng, Cheng
   Ji, Rongrong
   Tao, Dacheng
   Gao, Xinbo
   Li, Xuelong
TI Weakly Supervised Multi-Graph Learning for Robust Image Reranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attributes; co-occurred patterns; multiple graphs; visual reranking;
   weakly supervised learning
ID MODELS
AB Visual reranking has been widely deployed to refine the traditional text-based image retrieval. Its current trend is to combine the retrieval results from various visual features to boost reranking precision and scalability. And its prominent challenge is how to effectively exploit the complementary property of different features. Another significant issue raises from the noisy instances, from manual or automatic labels, which makes the exploration of such complementary property difficult. This paper proposes a novel image reranking by introducing a new Co-Regularized MultiGraph Learning (Co-RMGL) framework, in which intra-graph and inter-graph constraints are integrated to simultaneously encode the similarity in a single graph and the consistency across multiple graphs. To deal with the noisy instances, weakly supervised learning via co-occurred visual attribute is utilized to select a set of graph anchors to guide multiple graphs alignment and fusion, and to filter out those pseudo labeling instances to highlight the strength of individual features. After that, a learned edge weighting matrix from a fused graph is used to reorder the retrieval results. We evaluate our approach on four popular image retrieval data sets and demonstrate a significant improvement over state-of-the-art methods.
C1 [Deng, Cheng; Gao, Xinbo] Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China.
   [Ji, Rongrong] Xiamen Univ, Sch Informat Sci & Technol, Dept Cognit Sci, Xiamen 31005, Fujian, Peoples R China.
   [Tao, Dacheng] Univ Technol Sydney, Fac Engn & Informat Technol, Ctr Quantum Computat & Intelligent Syst, Broadway, NSW 2007, Australia.
   [Li, Xuelong] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Ctr OPT IMagery Anal & Learning OP TIMAL, State Key Lab Transient Opt & Photon, Xian 710119, Shaanxi, Peoples R China.
C3 Xidian University; Xiamen University; University of Technology Sydney;
   Chinese Academy of Sciences; Xi'an Institute of Optics & Precision
   Mechanics, CAS; State Key Laboratory of Transient Optics & Photonics
RP Ji, RR (corresponding author), Xiamen Univ, Sch Informat Sci & Technol, Dept Cognit Sci, Xiamen 31005, Fujian, Peoples R China.
EM chdeng@mail.xidian.edu.cn; rrji@xmu.edu.cn; dacheng.tao@uts.edu.au;
   xbgao@mail.xidian.cn; xuelong_li@opt.ac.cn
RI li, xiang/GWM-6319-2022; Li, Xuelong/Z-3785-2019; Tao,
   Dacheng/A-5449-2012; Li, Xuelong/ABF-3381-2020
OI Tao, Dacheng/0000-0001-7225-5449; Li, Xuelong/0000-0002-0019-4197
FU National Basic Research Program of China (973 Program) [2012CB316400];
   National Natural Science Foundation of China [61125106, 61125204,
   61101250, 61373076]; Program for New Century Excellent Talents in
   University [NCET-12-0917]; Program for New Scientific and Technological
   Star of Shaanxi Province [2012KJXX-24]; Shaanxi Key Innovation Team of
   Science and Technology [2012KCT-02, 2012KCT-04]; Fundamental Research
   Funds for the Central Universities [2013121026]; 985 Project of Xiamen
   University
FX This work was supported by the National Basic Research Program of China
   (973 Program) (Grant No. 2012CB316400), the National Natural Science
   Foundation of China (Grant Nos.: 61125106, 61125204, 61101250, and
   61373076), the Program for New Century Excellent Talents in University
   (NCET-12-0917), the Program for New Scientific and Technological Star of
   Shaanxi Province (No. 2012KJXX-24), the Shaanxi Key Innovation Team of
   Science and Technology (Grant Nos.: 2012KCT-02 and 2012KCT-04), the
   Fundamental Research Funds for the Central Universities (No.
   2013121026), and the 985 Project of Xiamen University. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Selcuk Candan.
CR Agrawal T., P INT C VER LARG DAT, V199, P487
   [Anonymous], 2012, INT J COMPUT VISION, DOI DOI 10.1007/s11263-011-0472-9
   [Anonymous], P ACM MULT
   [Anonymous], ACM MULTIMEDIA
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Chen X, 2012, ANN APPL STAT, V6, P719, DOI 10.1214/11-AOAS514
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   Crandall DJ, 2006, LECT NOTES COMPUT SC, V3951, P16
   Cui J., 2008, MM 08, P997
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Fergus R, 2007, INT J COMPUT VISION, V71, P273, DOI 10.1007/s11263-006-8707-x
   Fu F. X., 2010, P IEEE INT C COMP VI, P2949
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Hsu WH, 2007, IEEE MULTIMEDIA, V14, P14, DOI 10.1109/MMUL.2007.61
   Jain V., 2011, International Conference on World Wide Web, P277, DOI DOI 10.1145/1963405.1963447
   Jegou H., P EUR C COMP VIS, V200, P304
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Krapac J, 2010, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR.2010.5540092
   Kumar N, 2011, IEEE T PATTERN ANAL, V33, P1962, DOI 10.1109/TPAMI.2011.48
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Liu W., 2010, P INT C MACH LEARN
   Liu W, 2011, PROC CVPR IEEE, P849, DOI 10.1109/CVPR.2011.5995315
   Liu YA, 2011, IEEE T MULTIMEDIA, V13, P280, DOI 10.1109/TMM.2010.2103931
   Liu Y, 2010, IEEE T CIRC SYST VID, V20, P749, DOI 10.1109/TCSVT.2010.2045801
   Lu JY, 2012, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2012.6248033
   Mikulík A, 2010, LECT NOTES COMPUT SC, V6313, P1
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Nister David, 2006, CVPR
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Parikh D, 2011, PROC CVPR IEEE, P1681, DOI 10.1109/CVPR.2011.5995451
   Philbin J., 2008, P CVPR, P1
   Qin DF, 2011, PROC CVPR IEEE, P777, DOI 10.1109/CVPR.2011.5995373
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Tang XO, 2012, IEEE T PATTERN ANAL, V34, P1342, DOI 10.1109/TPAMI.2011.242
   Tian XM, 2010, IEEE T IMAGE PROCESS, V19, P805, DOI 10.1109/TIP.2009.2035866
   Tian XM, 2011, IEEE T MULTIMEDIA, V13, P639, DOI 10.1109/TMM.2011.2111363
   Torresani L., P EUR C COMP VIS, V201, P776
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang F, 2008, IEEE T KNOWL DATA EN, V20, P55, DOI 10.1109/TKDE.2007.190672
   Wang JD, 2009, IEEE T PATTERN ANAL, V31, P1600, DOI 10.1109/TPAMI.2008.216
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang XG, 2011, PROC CVPR IEEE, P857, DOI 10.1109/CVPR.2011.5995399
   Xue G.-R., 2004, CIKM, P118
   Yang LJ, 2012, IEEE T MULTIMEDIA, V14, P871, DOI 10.1109/TMM.2012.2187778
   Zhang ST, 2012, LECT NOTES COMPUT SC, V7573, P660, DOI 10.1007/978-3-642-33709-3_47
NR 49
TC 49
Z9 54
U1 0
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 785
EP 795
DI 10.1109/TMM.2014.2298841
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500018
DA 2024-07-18
ER

PT J
AU Zhang, LM
   Gao, Y
   Xia, YJ
   Lu, K
   Shen, JL
   Ji, RR
AF Zhang, Luming
   Gao, Yue
   Xia, Yingjie
   Lu, Ke
   Shen, Jialie
   Ji, Rongrong
TI Representative Discovery of Structure Cues for Weakly-Supervised Image
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Structure cues; graphlet; weakly supervised; segmentation; active
   learning
ID RECOGNITION; RETRIEVAL
AB Weakly-supervised image segmentation is a challenging problem with multidisciplinary applications in multimedia content analysis and beyond. It aims to segment an image by leveraging its image-level semantics (i.e., tags). This paper presents a weakly-supervised image segmentation algorithm that learns the distribution of spatially structural superpixel sets from image-level labels. More specifically, we first extract graphlets from a given image, which are small-sized graphs consisting of superpixels and encapsulating their spatial structure. Then, an efficient manifold embedding algorithm is proposed to transfer labels from training images into graphlets. It is further observed that there are numerous redundant graphlets that are not discriminative to semantic categories, which are abandoned by a graphlet selection scheme as they make no contribution to the subsequent segmentation. Thereafter, we use a Gaussian mixture model (GMM) to learn the distribution of the selected post-embedding graphlets (i.e., vectors output from the graphlet embedding). Finally, we propose an image segmentation algorithm, termed representative graphlet cut, which leverages the learned GMM prior to measure the structure homogeneity of a test image. Experimental results show that the proposed approach outperforms state-of-the-art weakly-supervised image segmentation methods, on five popular segmentation data sets. Besides, our approach performs competitively to the fully-supervised segmentation models.
C1 [Zhang, Luming; Gao, Yue] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
   [Xia, Yingjie] Hangzhou Normal Univ, Hangzhou Inst Serv Engn, Hangzhou, Zhejiang, Peoples R China.
   [Lu, Ke] Chinese Acad Sci, Grad Univ, Beijing 10049, Peoples R China.
   [Shen, Jialie] Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore.
   [Ji, Rongrong] Xiamen Univ, Dept Cognit Sci, Xiamen, Peoples R China.
C3 National University of Singapore; Hangzhou Normal University; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Singapore Management University; Xiamen University
RP Gao, Y (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
EM kevin.gaoy@gmail.com
RI Lei, Ming/JAD-1050-2023; SHEN, Jialie/E-8573-2012; zhang,
   lu/GRO-2969-2022; Shen, Jialie/AAX-6851-2020
FU Singapore National Research Foundation under its International Research
   Centre@Singapore Funding Initiative; Nature Science Foundation of China
   [61373076]; Fundamental Research Funds for the Central Universities
   [2013121026]; 985 Project of Xiamen University; Natural Science
   Foundation of China [61002009]; Key Science and Technology Program of
   Zhejiang Province of China [2012C01035-1]; Zhejiang Provincial Natural
   Science Foundation of China [LZ13F020004]
FX This work was supported by the Singapore National Research Foundation
   under its International Research Centre@Singapore Funding Initiative and
   administered by the IDM Programme Office. This work was also supported
   by the Nature Science Foundation of China (No. 61373076), the
   Fundamental Research Funds for the Central Universities (No.
   2013121026), and the 985 Project of Xiamen University. Natural Science
   Foundation of Chinaunder grant number 61002009, Key Science and
   Technology Program of Zhejiang Province of Chinaunder grant number
   2012C01035-1, and Zhejiang Provincial Natural Science Foundation of
   Chinaunder grant number LZ13F020004 (Corresponding author: Y. Gao). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Cees Snoek.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], P CVPR
   [Anonymous], P CVPR
   [Anonymous], 2010, ACM MULTIMEDIA
   Castelhano MS, 2009, J VISION, V9, DOI 10.1167/9.3.6
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Golub G.H., 1989, MATRIX COMPUTATIONS
   Hoiem D., 2009, WORLD LITERATURE TOD, V24, P1
   Keselman Y, 2005, IEEE T PATTERN ANAL, V27, P1141, DOI 10.1109/TPAMI.2005.139
   Kim S., 2011, Advances in Neural Information Processing Systems 24 (NIPS), P1530
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248
   Li YX, 2012, IEEE T MULTIMEDIA, V14, P1618, DOI 10.1109/TMM.2012.2199292
   Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1662, DOI 10.1109/TMM.2012.2199293
   Ni B., T MULTIMEDI IN PRESS
   Platt JC, 2000, ADV NEUR IN, P61
   Rital S, 2009, FUND INFORM, V96, P153, DOI 10.3233/FI-2009-172
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   STRICKER M, 1995, P SOC PHOTO-OPT INS, V2410, P381, DOI 10.1117/12.205308
   Verbeek J., 2007, PROC IEEE C COMPUTER, P1, DOI DOI 10.1109/CVPR.2007.383098
   VEZHNEVETS A, 2010, PROC CVPR IEEE, P3249, DOI DOI 10.1109/CVPR.2010.5540060
   Vezhnevets A, 2012, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2012.6247757
   Vezhnevets A, 2012, PROC CVPR IEEE, P3162, DOI 10.1109/CVPR.2012.6248050
   Vezhnevets A, 2011, IEEE I CONF COMP VIS, P643, DOI 10.1109/ICCV.2011.6126299
   WERMAN M, 1995, IEEE T PATTERN ANAL, V17, P810, DOI 10.1109/34.400572
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zhang SL, 2013, IEEE T IMAGE PROCESS, V22, P2889, DOI 10.1109/TIP.2013.2251650
NR 32
TC 180
Z9 186
U1 3
U2 71
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 470
EP 479
DI 10.1109/TMM.2013.2293424
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800015
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Xu, JJ
   Jagadeesh, V
   Ni, ZF
   Sunderrajan, S
   Manjunath, BS
AF Xu, Jiejun
   Jagadeesh, Vignesh
   Ni, Zefeng
   Sunderrajan, Santhoshkumar
   Manjunath, B. S.
TI Graph-Based Topic-Focused Retrieval in Distributed Camera Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distributed camera network; diverse and relevant ranking; graph-based
   modeling; information search and retrieval
ID VIDEO; SIMILARITY
AB Wide-area wireless camera networks are being increasingly deployed in many urban scenarios. The large amount of data generated from these cameras pose significant information processing challenges. In this work, we focus on representation, search and retrieval of moving objects in the scene, with emphasis on local camera node video analysis. We develop a graph model that captures the relationships among objects without the need to identify global trajectories. Specifically, two types of edges are defined in the graph: object edges linking the same object across the whole network and context edges linking different objects within a spatial-temporal proximity. We propose a manifold ranking method with a greedy diversification step to order the relevant items based on similarity as well as diversity within the database. Detailed experimental results using video data from a 10-camera network covering bike paths are presented.
C1 [Xu, Jiejun] Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
   [Jagadeesh, Vignesh; Ni, Zefeng; Sunderrajan, Santhoshkumar; Manjunath, B. S.] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
C3 University of California System; University of California Santa Barbara;
   University of California System; University of California Santa Barbara
RP Xu, JJ (corresponding author), Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
RI Manjunath, B S/AAM-8190-2020
OI Manjunath, B S/0000-0003-2804-3611
CR Agarwal S., 2006, P 23 INT C MACH LEAR, P25, DOI [10.1145/1143844.1143848, DOI 10.1145/1143844.1143848]
   Cabrera R. R., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P65, DOI 10.1109/CVPR.2011.5995735
   Cao L., 2008, Proc. ACM Multimedia, P121
   COLLINS R, 2003, P IEEE C COMP VIS PA
   Collins RT, 2001, P IEEE, V89, P1456, DOI 10.1109/5.959341
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Corvee E., 2012, P VISAPP INT C COMP
   De Leo C, 2011, LECT NOTES COMPUT SC, V6468, P94
   Du P, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P1239
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Fu YW, 2010, IEEE T MULTIMEDIA, V12, P717, DOI 10.1109/TMM.2010.2052025
   Gong Y., 2001, P ICME
   He J., P 12 ANN ACM INT C M, P9, DOI 10.1145/1027527.1027531
   He JR, 2006, IEEE T IMAGE PROCESS, V15, P3170, DOI 10.1109/TIP.2006.877491
   Hong K., 2012, A distributed system for supporting spatio-temporal analysis on large-scale camera networks
   Jagadeesh Vignesh, 2010, P 7 IND C COMP VIS G, P178, DOI [10.1145/1924559.1924583, DOI 10.1145/1924559.1924583]
   Javed O, 2005, PROC CVPR IEEE, P26
   Javed O, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P952
   Javed O., 2003, IEEE C MULTIMEDIA EX, P6
   JeongKyu Lee, 2005, 13th Annual ACM International Conference on Multimedia, P810
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Kettnaker V., 1999, IEEE C COMPUTER VISI, P252
   Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140
   Lovasz L, 1996, BOLYAI MATH STUD, V2, P353
   Mei T, 2009, VISUAL COMPUT, V25, P39, DOI 10.1007/s00371-008-0282-4
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Ni Zefeng., 2012, Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Confer- ence on, P7
   Ohbuchi R., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, MIR '08, P411
   Page L., 1999, 422 STANF INFOLAB ST
   Palmer CR, 2003, LECT NOTES ARTIF INT, V2637, P486
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Richter Fabian, 2010, P ICMR
   Tan H.-K., 2011, P 1 ACM INT C MULTIM, P1
   Wan XJ, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2903
   Xu B, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P525
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zhou D., 2003, P NIPS
   Zhu X., 2007, P HUM LANG TECHN C N, P97
   Zhu X., 2011, P WWW, P37
NR 39
TC 15
Z9 16
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2046
EP 2057
DI 10.1109/TMM.2013.2281019
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900026
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sardis, F
   Mapp, G
   Loo, J
   Aiash, M
   Vinel, A
AF Sardis, Fragkiskos
   Mapp, Glenford
   Loo, Jonathan
   Aiash, Mahdi
   Vinel, Alexey
TI On the Investigation of Cloud-Based Mobile Media Environments With
   Service-Populating and QoS-Aware Mechanisms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer network management; Communication system traffic control; Web
   services; Mobile computing
AB Recent advances in mobile devices and network technologies have set new trends in the way we use computers and access networks. Cloud Computing, where processing and storage resources are residing on the network is one of these trends. The other is Mobile Computing, where mobile devices such as smartphones and tablets are believed to replace personal computers by combining network connectivity, mobility, and software functionality. In the future, these devices are expected to seamlessly switch between different network providers using vertical handover mechanisms in order to maintain network connectivity at all times. This will enable mobile devices to access Cloud Services without interruption as users move around. Using current service delivery models, mobile devices moving from one geographical location to another will keep accessing those services from the local Cloud of their previous network, which might lead to moving a large volume of data over the Internet backbone over long distances. This scenario highlights the fact that user mobility will result in more congestion on the Internet. This will degrade the Quality of Service and by extension, the Quality of Experience offered by the services in the Cloud and especially multimedia services that have very tight temporal constraints in terms of bandwidth and jitter. We believe that a different approach is required to manage resources more efficiently, while improving the Quality of Service and Quality of Experience of mobile media services. This paper introduces a novel concept of Cloud-Based Mobile Media Service Delivery in which services run on localized public Clouds and are capable of populating other public Clouds in different geographical locations depending on service demands and network status. Using an analytical framework, this paper argues that as the demand for specific services increases in a location, it might be more efficient to move those services closer to that location. This will prevent the Internet backbone from experiencing high traffic loads due to multimedia streams and will offer service providers an automated resource allocation and management mechanism for their services.
C1 [Sardis, Fragkiskos; Mapp, Glenford; Loo, Jonathan; Aiash, Mahdi] Middlesex Univ, Sch Sci & Technol, London NW4 4BT, England.
   [Vinel, Alexey] Tampere Univ Technol, Dept Commun Engn, FIN-33101 Tampere, Finland.
   [Vinel, Alexey] Halmstad Univ, Halmstad, Sweden.
C3 Middlesex University; Tampere University; Halmstad University
RP Sardis, F (corresponding author), Middlesex Univ, Sch Sci & Technol, London NW4 4BT, England.
EM f.sardis@mdx.ac.uk; g.mapp@mdx.ac.uk; j.loo@mdx.ac.uk;
   m.aiash@mdx.ac.uk; vinel@ieee.org
RI Embrett, Mark G./H-4466-2014; Loo, Jonathan/E-6075-2019
OI Embrett, Mark G./0000-0002-3969-0219; Loo, Jonathan/0000-0002-2197-8126;
   Aiash, Mahdi/0000-0002-3984-6244
CR [Anonymous], P WORKSH HOT TOP CLO
   [Anonymous], P OSDI
   Brisko T., 1995, 1794 RFC IETF
   ETSI, 2011, MOB TECHN GSM
   Inamura H, 2003, 3481 RFC IETF
   Microsoft, 2012, CLOUD COMPUTING
   Motorola, 2012, LONG TERM EV LTE TEC
   Postel J., 1988, 948 ISI RFC IETF
   Sonnek J., 2010, P 39 INT C PAR PROC
   Thakker D. N., 2010, THESIS MIDDLESEX U L
   Waldspurger C. A., 2002, P OSDI
   Wood T., 2009, P 5 ACM INT C VIRT E
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 13
TC 36
Z9 38
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 769
EP 777
DI 10.1109/TMM.2013.2240286
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500006
DA 2024-07-18
ER

PT J
AU Mokhtarian, K
   Hefeeda, M
AF Mokhtarian, Kianoosh
   Hefeeda, Mohamed
TI Capacity Management of Seed Servers in Peer-to-Peer Streaming Systems
   With Scalable Video Streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Analytical models; peer-to-peer streaming; resource allocation; scalable
   video streaming
AB To improve rendered video quality and serve more receivers, peer-to-peer (P2P) video-on-demand streaming systems usually deploy seed servers. These servers complement the limited upload capacity offered by peers. In this paper, we are interested in optimally managing the capacity of seed servers, especially when scalable video streams are served to peers. Scalable video streams are encoded in multiple layers to support heterogeneous receivers. We show that the problem of optimally allocating the seeding capacity to serve scalable streams to peers is NP-complete. We then propose an approximation algorithm to solve it. Using the proposed allocation algorithm, we develop an analytical model to study the performance of P2P video-on-demand streaming systems and to manage their resources. The analysis also provides an upper bound on the maximum number of peers that can be admitted to the system in flash crowd scenarios. We validate our analysis by comparing its results to those obtained from simulations. Our analytical model can be used by administrators of P2P streaming systems to estimate the performance and video quality rendered to users under various network, peer, and video characteristics.
C1 [Mokhtarian, Kianoosh; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
C3 Simon Fraser University
RP Mokhtarian, K (corresponding author), Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.
EM kianoosh.mokhtarian@mail.utoronto.ca
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   British Columbia Innovation Council
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada and in part by the British Columbia
   Innovation Council. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Feng Wu.
CR Annapureddy S, 2007, IEEE INFOCOM SER, P2571, DOI 10.1109/INFCOM.2007.323
   [Anonymous], ACM T MULTIMEDIA COM
   Cui Y., 2003, Proceedings of the 13th international workshop on Network and operating systems support for digital audio and video, NOSSDAV '03, P162
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Hillested O., 2007, P PACK VID NOV, P26
   Hu H, 2010, NOSSDAV 2010: PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P27
   Kellerer H., 2004, Knapsack Problems. Springer Nature Book Archives Millennium, P317
   Kumar R, 2007, IEEE INFOCOM SER, P919, DOI 10.1109/INFCOM.2007.112
   Lan X., 2007, P ACM MULTIMEDIA, P783
   Li B, 2003, IEEE NETWORK, V17, P24
   Liu S, 2008, PERF E R SI, V36, P313, DOI 10.1145/1384529.1375493
   Mokhtarian K., 2010, Proceedings of ACM Multimedia Systems (MMSys), P133
   Mokhtarian K., 2009, P IEEE INT WORKSH QU, P1
   Mokhtarian K., 2009, THESIS S FRASER U SU
   Parvez KN, 2008, PERF E R SI, V36, P301, DOI 10.1145/1384529.1375492
   Rejaie R., 2003, Proceedings of ACM International Workshop on Network and Operating Systems Support for Digital Audio and Video (NOSSDAV), P153
   Schierl T, 2007, IEEE T CIRC SYST VID, V17, P1204, DOI 10.1109/TCSVT.2007.905528
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Small T., 2006, Proc. ACM Multimedia, Santa Barbara, P539
   Tu Y.C., 2005, ACM TOMCCAP, V1, P354
   Xu DY, 2006, MULTIMEDIA SYST, V11, P383, DOI 10.1007/s00530-006-0015-3
   Xu DY, 2002, INT CON DISTR COMP S, P363, DOI 10.1109/ICDCS.2002.1022274
   Xu XF, 2008, PROCEEDINGS OF 2008 CONFERENCE ON REGIONAL ECONOMY AND SUSTAINABLE DEVELOPMENT, P785
   Yin H., 2009, Proceedings of ACM International Conference on Multimedia, P25
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
   Zhou YP, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, P226, DOI 10.1109/ICNP.2007.4375853
NR 26
TC 12
Z9 13
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 181
EP 194
DI 10.1109/TMM.2012.2225042
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600015
DA 2024-07-18
ER

PT J
AU Mansour, MF
AF Mansour, Mohamed F.
TI A Transcoding System for Audio Standards
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio standards; Dolby digital; MPEG; transcoding
AB We describe an efficient implementation for converting MPEG-2/MPEG-4 Advanced Audio Coding (AAC) encoded data to Dolby Digital AC-3. We describe many techniques to exploit the information in the AAC bitstream to simplify the AC-3 encoder. These techniques can be straightforwardly used in other transcoding schemes between different multimedia standards.
C1 Texas Instruments Inc, Syst & Applicat R&D Ctr, Dallas, TX 75243 USA.
C3 Texas Instruments
RP Mansour, MF (corresponding author), Texas Instruments Inc, Syst & Applicat R&D Ctr, Dallas, TX 75243 USA.
EM mfmansour@ti.com
CR *ADV TEL SYST COMM, 2005, DIG AUD COMPR STAND
   [Anonymous], SOUND QUAL ASS MAT R
   Bang K., 2006, P 120 CONV AUD ENG S
   Bulmer M. G., 1979, Principles of Statistics
   Cheng MH, 2003, IEEE T SIGNAL PROCES, V51, P221, DOI 10.1109/TSP.2002.806566
   *ISO IEC, 1999, 144963 ISOIEC
   *ITU R, 1998, 1387 ITUR BS
   ITU-R, 2003, BS15341 ITUR
   Jayant N.C., 1984, Digital Coding of Waveforms: Principles and Applications to Speech and Video
   Johnston J. D., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P569, DOI 10.1109/ICASSP.1992.225993
   Ju FS, 2006, IEEE INT SYM MULTIM, P750
   LERCH A, EAQUAL EVALUATION AU
   MALVAR HS, 1990, IEEE T ACOUST SPEECH, V38, P969, DOI 10.1109/29.56057
   Mansour MF, 2010, INT CONF ACOUST SPEE, P1562, DOI 10.1109/ICASSP.2010.5495523
   Mansour MF, 2009, INT CONF ACOUST SPEE, P157, DOI 10.1109/ICASSP.2009.4959544
   Moore B. C. J., 1997, INTRO PSYCHOL HEARIN
   Takagi K, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P132, DOI 10.1109/MMSP.2006.285283
NR 17
TC 1
Z9 2
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2012
VL 14
IS 5
BP 1381
EP 1389
DI 10.1109/TMM.2012.2197191
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 008XT
UT WOS:000308990600001
DA 2024-07-18
ER

PT J
AU Hassan, E
   Chaudhury, S
   Gopal, M
AF Hassan, Ehtesham
   Chaudhury, Santanu
   Gopal, M.
TI Feature Combination in Kernel Space for Distance Based Image Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature combination; image indexing; multiple kernel learning
ID CONTENT-BASED RETRIEVAL; NEAREST-NEIGHBOR; SHAPE; SEARCH; FUSION; COLOR;
   TEXT
AB The paper presents a novel feature based indexing scheme for image collections. The scheme presents the extension of distance based hashing to kernel space for generating the indexing structure based on similarity in kernel space. The objective of the scheme is to incorporate multiple features for defining the image indexing space using the concept of multiple kernel learning. However, the indexing problems are defined with unique learning objective; therefore, a novel application of genetic algorithm is presented for the optimization task. The extensive evaluation of the proposed concept is performed for developing word based document indexing application of Devanagari, Bengali, and English scripts. In addition, the efficacy of the proposed concept is shown by experimental evaluations on handwritten digits and natural image collection.
C1 [Hassan, Ehtesham; Chaudhury, Santanu] Indian Inst Technol Delhi, Dept Elect Engn, New Delhi, India.
   [Gopal, M.] Shiv Nadar Univ, Sch Engn, Gautam Buddha Nagar, UP, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Delhi; Shiv Nadar University
RP Hassan, E (corresponding author), Indian Inst Technol Delhi, Dept Elect Engn, New Delhi, India.
EM hassan.ehtesham@gmail.com; santanuc@ee.iitd.ac.in; mgopal@snu.edu.in
OI Hassan, Ehtesham/0000-0002-9039-0561
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2007, P 24 INT C MACH LEAR, DOI DOI 10.1145/1273496.1273594
   [Anonymous], 2009, INTRO INFORM RETRIEV
   [Anonymous], STOC 2002
   Arya D., 2011, P 2011 JOINT WORKSH, P1
   Athitsos V, 2008, PROC INT CONF DATA, P327, DOI 10.1109/ICDE.2008.4497441
   Bajaj R, 1997, PATTERN RECOGN, V30, P1, DOI 10.1016/S0031-3203(96)00059-3
   Basak J, 2006, IEEE T IMAGE PROCESS, V15, P3773, DOI 10.1109/TIP.2006.881946
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bhardwaj A, 2009, LECT NOTES ARTIF INT, V5402, P403
   Böhm C, 2001, ACM COMPUT SURV, V33, P322, DOI 10.1145/502807.502809
   Chaudhury S, 2003, PROC INT CONF DOC, P885
   Chun YD, 2003, IEEE T CIRC SYST VID, V13, P951, DOI 10.1109/TCSVT.2003.816507
   Chun YD, 2008, IEEE T MULTIMEDIA, V10, P1073, DOI 10.1109/TMM.2008.2001357
   Czarn A, 2004, IEEE T EVOLUT COMPUT, V8, P405, DOI 10.1109/TEVC.2004.831262
   Dasigi V, 2001, PATTERN RECOGN, V34, P2413, DOI 10.1016/S0031-3203(00)00171-0
   Deb K., 1998, Foundations of Genetic Algorithms 5, P265
   Dorairaj R, 2004, CONF REC ASILOMAR C, P387
   Dutta R., 2008, ACM COMPUT SURV, V40, P262
   Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226
   Faloutsos C., 1995, SIGMOD Record, V24, P163, DOI 10.1145/568271.223812
   Fergus R., 2009, P NIPS
   Gagaudakis G, 2002, PATTERN RECOGN, V35, P81, DOI 10.1016/S0031-3203(01)00043-7
   Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169
   Goldberg D.E., 2000, GENETIC ALGORITHMS S
   Google Inc, 2007, BOOK SEARCH DAT
   Gosselin PH, 2008, COMPUT VIS IMAGE UND, V110, P403, DOI 10.1016/j.cviu.2007.09.018
   Govindaraju V, 2009, ADV PATTERN RECOGNIT, P1
   Haghani Parisa., 2009, PROC 12 INT C EXTEND, P744
   Haiying Shen, 2008, 2008 3rd International Conference on Digital Telecommunications (ICDT), P47, DOI 10.1109/ICDT.2008.12
   Hassan Ehtesham, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P206, DOI 10.1109/ICDAR.2009.63
   Hassan E., 2010, P 7 IND C COMP VIS G, P358
   Indyk P., 1998, P 13 ANN ACM S THEOR
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Kumar A, 2007, LECT NOTES COMPUT SC, V4843, P586
   Lacoste C, 2007, IEEE T CIRC SYST VID, V17, P889, DOI 10.1109/TCSVT.2007.897114
   Lanckriet GRG, 2004, BIOINFORMATICS, V20, P2626, DOI 10.1093/bioinformatics/bth294
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin YQ, 2005, IEEE T SYST MAN CY B, V35, P538, DOI 10.1109/TSMCB.2005.846656
   Lin YQ, 2005, IEEE T SYST MAN CY C, V35, P156, DOI 10.1109/TSMCC.2004.841912
   Liu Y, 2007, PATTERN RECOGN, V40, P262, DOI 10.1016/j.patcog.2006.04.045
   Lladós J, 2007, PROC INT CONF DOC, P362
   Lu SJ, 2008, IEEE T PATTERN ANAL, V30, P1913, DOI 10.1109/TPAMI.2008.89
   Marinai S, 2006, IEEE T PATTERN ANAL, V28, P1187, DOI 10.1109/TPAMI.2006.162
   Matei B, 2006, IEEE T PATTERN ANAL, V28, P1111, DOI 10.1109/TPAMI.2006.148
   Meshesha M, 2008, INT J DOC ANAL RECOG, V11, P29, DOI 10.1007/s10032-008-0067-3
   Mu YD, 2010, PROC CVPR IEEE, P3344, DOI 10.1109/CVPR.2010.5540024
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Qi XJ, 2005, PATTERN RECOGN, V38, P2449, DOI 10.1016/j.patcog.2005.04.005
   Rath R.M.T.M., 2003, Proceedings 2003 Symposium on Document Image Understanding Technology, P77
   Rath TM, 2003, PROC INT CONF DOC, P218
   Ren W, 2009, PATTERN RECOGN, V42, P267, DOI 10.1016/j.patcog.2008.08.033
   Sankar PK, 2006, LECT NOTES COMPUT SC, V4338, P837
   Saykol E, 2004, IEEE T IMAGE PROCESS, V13, P314, DOI 10.1109/TIP.2003.821114
   Scholkopf B., 2006, LEARNING KERNELS SUP
   Shen JL, 2006, IEEE T MULTIMEDIA, V8, P1179, DOI 10.1109/TMM.2006.884618
   Shuyong Bai, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P331, DOI 10.1109/ICDAR.2009.54
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang WH, 2009, PROCEEDINGS OF THE 2009 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND NATURAL COMPUTING, VOL I, P151, DOI 10.1109/CINC.2009.124
   Zhu GY, 2009, IEEE T PATTERN ANAL, V31, P2015, DOI 10.1109/TPAMI.2008.237
NR 64
TC 20
Z9 21
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1179
EP 1195
DI 10.1109/TMM.2012.2190388
PN 2
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400006
DA 2024-07-18
ER

PT J
AU Sang, JT
   Xu, CS
AF Sang, Jitao
   Xu, Changsheng
TI Robust Face-Name Graph Matching for Movie Character Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 11-15, 2011
CL Univ Ramon Llull, La Salle, Barcelona, SPAIN
SP IEEE, IEEE Signal Proc Soc, IEEE Circuits & Syst Soc, IEEE Comp Soc, IEEE Commun Soc
HO Univ Ramon Llull, La Salle
DE Character identification; graph edit; graph matching; graph partition;
   sensitivity analysis
ID SENSITIVITY-ANALYSIS
AB Automatic face identification of characters in movies has drawn significant research interests and led to many interesting applications. It is a challenging problem due to the huge variation in the appearance of each character. Although existing methods demonstrate promising results in clean environment, the performances are limited in complex movie scenes due to the noises generated during the face tracking and face clustering process. In this paper we present two schemes of global face-name matching based framework for robust character identification. The contributions of this work include the following. 1) A noise insensitive character relationship representation is incorporated. 2) We introduce an edit operation based graph matching algorithm. 3) Complex character changes are handled by simultaneously graph partition and graph matching. 4) Beyond existing character identification approaches, we further perform an in-depth sensitivity analysis by introducing two types of simulated noises. The proposed schemes demonstrate state-of-the-art performance on movie character identification in various genres of movies.
C1 [Sang, Jitao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Sang, Jitao; Xu, Changsheng] China Singapore Inst Digital Media, Singapore 119613, Singapore.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Sang, JT (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM jtsang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
CR [Anonymous], 2006, P COMP VIS PATT REC
   [Anonymous], 2000, ONLINE TXB SUPPORTIN
   [Anonymous], 2010, P 18 ACM INT C MULTI
   [Anonymous], THESIS ECOLE NATL SU
   Berg TL, 2004, PROC CVPR IEEE, P848
   Bini E, 2008, REAL-TIME SYST, V39, P5, DOI 10.1007/s11241-006-9010-1
   Bunke H, 1997, PATTERN RECOGN LETT, V18, P689, DOI 10.1016/S0167-8655(97)00060-3
   Chao Liang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3377, DOI 10.1109/CVPR.2011.5995681
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Cinbis RG, 2011, IEEE I CONF COMP VIS, P1559, DOI 10.1109/ICCV.2011.6126415
   Cour T, 2008, LECT NOTES COMPUT SC, V5305, P158, DOI 10.1007/978-3-540-88693-8_12
   Cour T, 2010, PROC CVPR IEEE, P1014, DOI 10.1109/CVPR.2010.5540106
   Cour T, 2009, PROC CVPR IEEE, P919, DOI 10.1109/CVPRW.2009.5206667
   Everingham M, 2005, IEEE I CONF COMP VIS, P1103
   Everingham M., 2006, BMVC, V2, P6
   Everingham M, 2009, IMAGE VISION COMPUT, V27, P545, DOI 10.1016/j.imavis.2008.04.018
   Fitzgibbon A, 2002, LECT NOTES COMPUT SC, V2352, P304
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Hong R., 2010, Proceedings of the 18th ACM international conference on Multimedia, P421
   Li Y, 2006, LECT NOTES COMPUT SC, V3979, P29
   Lin LA, 2010, IEEE T PATTERN ANAL, V32, P1426, DOI 10.1109/TPAMI.2009.150
   Ramanan D., 2007, PROC INT C COMPUT VI, P1
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   Saltelli A, 2005, CHEM REV, V105, P2811, DOI 10.1021/cr040659d
   SANFELIU A, 1983, IEEE T SYST MAN CYB, V13, P353, DOI 10.1109/TSMC.1983.6313175
   Sang Jitao., 2011, Multimedia and Expo (ICME), 2011 IEEE International Conference on, P1
   Satoh S, 1997, PROC CVPR IEEE, P368, DOI 10.1109/CVPR.1997.609351
   Sivic J, 2009, PROC CVPR IEEE, P1145, DOI 10.1109/CVPRW.2009.5206513
   Stallkamp J., 2007, PROC INT C COMPUT VI, P1
   Xu M., 2010, ACM MULTIMEDIA, P831
   Yang J, 2005, Proceedings of the 2005 International Conference on Active Media Technology (AMT 2005), P28
   Zhang YF, 2009, IEEE INT CON MULTI, P278, DOI 10.1109/ICME.2009.5202489
   Zhang YF, 2009, IEEE T MULTIMEDIA, V11, P1276, DOI 10.1109/TMM.2009.2030629
NR 33
TC 28
Z9 32
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 586
EP 596
DI 10.1109/TMM.2012.2188784
PN 1
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 943ZG
UT WOS:000304166300010
DA 2024-07-18
ER

PT J
AU Wang, M
   Hong, RC
   Yuan, XT
   Yan, SC
   Chua, TS
AF Wang, Meng
   Hong, Richang
   Yuan, Xiao-Tong
   Yan, Shuicheng
   Chua, Tat-Seng
TI Movie2Comics: Towards a Lively Video Content Presentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cartoonization; comics; descriptive picture extraction
AB As a type of artwork, comics is prevalent and popular around the world. However, despite the availability of assistive software and tools, the creation of comics is still a labor-intensive and time-consuming process. This paper proposes a scheme that is able to automatically turn a movie clip to comics. Two principles are followed in the scheme: 1) optimizing the information preservation of the movie; and 2) generating outputs following the rules and the styles of comics. The scheme mainly contains three components: script-face mapping, descriptive picture extraction, and cartoonization. The script-face mapping utilizes face tracking and recognition techniques to accomplish the mapping between characters' faces and their scripts. The descriptive picture extraction then generates a sequence of frames for presentation. Finally, the cartoonization is accomplished via three steps: panel scaling, stylization, and comics layout design. Experiments are conducted on a set of movie clips and the results have demonstrated the usefulness and the effectiveness of the scheme.
C1 [Wang, Meng; Hong, Richang] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
   [Yuan, Xiao-Tong] Rutgers State Univ, New Brunswick, NJ 08901 USA.
   [Yan, Shuicheng; Chua, Tat-Seng] Natl Univ Singapore, Singapore 117417, Singapore.
C3 Hefei University of Technology; Rutgers University System; Rutgers
   University New Brunswick; National University of Singapore
RP Wang, M (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
EM hongrc.hfut@gmail.com
RI Yan, Shuicheng/HCI-1431-2022; Wang, Meng/ITR-8699-2023
FU National Science Foundation of China [61172164]; NExT Research Center;
   MDA, Singapore, [WBS: R-252-300-001-490]
FX This work was supported by the National Science Foundation of China
   (61172164) and the "NExT Research Center" funded by MDA, Singapore,
   under the research grant: WBS: R-252-300-001-490. A four-page short
   version has been published in ACM Multimedia in 2010 [14]. Compared with
   the conference version, in this paper, we have enhancements in the
   following aspects: 1) we performed a more comprehensive survey of
   related work; 2) we conducted more experiments; and 3) more technical
   details, discussions, and analyses are provided. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Alan Hanjalic.
CR Agarwala A., 2004, P SIGGRAPH
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2001, P IEEE INT C COMP VI
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Arandjelovic O., 2005, P IEEE INT C COMP VI
   Bennett E., 2007, P SIGGRAPH
   Boreczky J., 2000, P SIGCHI
   Bouthemy P, 1999, IEEE T CIRC SYST VID, V9, P1030, DOI 10.1109/76.795057
   Calic J, 2007, IEEE T CIRC SYST VID, V17, P931, DOI 10.1109/TCSVT.2007.897466
   Chiu P., 2004, P IEEE INT C MULT EX
   Chun B.-K., 2006, P INT S VIS COMP
   Correa C., 2010, P SIGGRAPH
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Everingham M., 2006, P BRIT MACH VIS C, P899, DOI [DOI 10.5244/C.20.92, 10.5244/C.20.92]
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Hong R., 2010, P ACM MULT
   Hwang W., 2006, P INT C COMP GRAPH T
   Irani M, 1998, P IEEE, V86, P905, DOI 10.1109/5.664279
   Kim B, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P32
   Kim C, 2002, IEEE T CIRC SYST VID, V12, P1128, DOI 10.1109/TCSVT.2002.806813
   Kim J., 2000, P IEEE INT C MULT EX
   Konrad J., 1998, IMPROVED GLOBAL MOTI
   Kurlander D., 1996, P SIGGRAPH
   Lee J., 2005, P ACM MULT
   Luo ZQ, 2007, SIAM J OPTIMIZ, V18, P1, DOI 10.1137/050642691
   Magnussen A., 2004, HANS CHRISTIAN CHRIS
   Marr D., 1980, P ROY SOC LONDON, V207
   Marzotto R., 2004, P IEEE INT C COMP VI
   McCloud S., 2000, PERENNIAL
   Mei T, 2009, VISUAL COMPUT, V25, P39, DOI 10.1007/s00371-008-0282-4
   Obozinski G., 2009, STAT COMPUT, P1
   Preu J., 2007, P SIGGRAPH
   Scharcanski J., 2006, P INT C IM PROC
   Shamir A, 2006, IEEE COMPUT GRAPH, V26, P53, DOI 10.1109/MCG.2006.58
   Tang L., 2009, P ACM MULT
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Uchihashi S., 1999, P ACM MULT
   Wang J, 2004, ACM T GRAPHIC, V23, P574, DOI 10.1145/1015706.1015763
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, COMPUT VIS IMAGE UND, V113, P384, DOI 10.1016/j.cviu.2008.08.003
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Winnermoller H., 2006, P SIGGRAPH
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
NR 43
TC 93
Z9 97
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 858
EP 870
DI 10.1109/TMM.2012.2187181
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700017
DA 2024-07-18
ER

PT J
AU Hu, XT
   Li, KM
   Han, JW
   Hua, XS
   Guo, L
   Liu, TM
AF Hu, Xintao
   Li, Kaiming
   Han, Junwei
   Hua, Xiansheng
   Guo, Lei
   Liu, Tianming
TI Bridging the Semantic Gap via Functional Brain Imaging
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Brain-computer interface; brain imaging; high-level features; low-level
   features; semantic gap
ID RETRIEVAL; MODELS; MEMORY; SETS; TOOL
AB The multimedia content analysis community has made significant efforts to bridge the gaps between low-level features and high-level semantics perceived by humans. Recent advances in brain imaging and neuroscience in exploring the human brain's responses during multimedia comprehension demonstrated the possibility of leveraging cognitive neuroscience knowledge to bridge the semantic gaps. This paper presents our initial effort in this direction by using functional magnetic resonance imaging (fMRI). Specifically, task-based fMRI (T-fMRI) was performed to accurately localize the brain regions involved in video comprehension. Then, natural stimulus fMRI (N-fMRI) data were acquired when subjects watched the multimedia clips selected from the TRECVID datasets. The responses in the localized brain regions were measured and used to extract high-level features as the representation of the brain's comprehension of semantics in the videos. A novel computational framework was developed to learn the most relevant low-level feature sets that best correlate the fMRI-derived semantic features based on the training videos with fMRI scans, and then the learned model was applied to larger scale TRECVID video datasets without fMRI scans for category classification. Our experimental results demonstrate: 1) there are meaningful couplings between brain's fMRI-derived responses and video stimuli, suggesting the validity of linking semantics and low-level features via fMRI and 2) the computationally learned low-level features can significantly (p < 0.01) improve video classification in comparison with original low-level features and extracted low-level features resulted from well-known feature projection algorithms.
C1 [Hu, Xintao; Li, Kaiming; Han, Junwei; Guo, Lei] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
   [Hua, Xiansheng] Microsoft Res Asia, Beijing 10080, Peoples R China.
   [Liu, Tianming] Univ Georgia, Dept Comp Sci, Athens, GA 30602 USA.
   [Liu, Tianming] Univ Georgia, Bioimaging Res Ctr, Athens, GA 30602 USA.
C3 Northwestern Polytechnical University; Microsoft; Microsoft Research
   Asia; University System of Georgia; University of Georgia; University
   System of Georgia; University of Georgia
RP Hu, XT (corresponding author), Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
EM xhu@nwpu.edu.cn; likaiming@gmail.com; jhan@nwpu.edu.cn;
   xshua@microsoft.com; lguo@nwpu.edu.cn; tliu@cs.uga.edu
RI Hu, Xiaoyu/AAO-7197-2020; Li, Chun/KBC-9591-2024; Liu,
   Tianming/AAA-4602-2022; Li, Kun/JLL-6505-2023
OI Li, Kun/0000-0002-3638-2974; li, kaiming/0000-0003-4142-4435; Hu,
   Xintao/0000-0001-5633-3806
FU National Institutes of Health [EB 006878, R01HL087923-03S2]; University
   of Georgia; National Natural Science Foundation of China (NSFC)
   [61103061]; Postdoctoral Foundation of China [20110490174]; NSCF
   [61005018]; Northwestern Polytechnical University Foundation
   [NPU-FFR-JC201041]
FX Manuscript received March 15, 2011; revised July 17, 2011; accepted
   September 30, 2011. Date of publication October 17, 2011; date of
   current version March 21, 2012. This work was supported by the National
   Institutes of Health under Grant EB 006878 and Grant R01HL087923-03S2.
   The work of T. Liu was supported in part by The University of Georgia.
   The work of X. Hu was supported by the National Natural Science
   Foundation of China (NSFC) under Grant 61103061 and the Postdoctoral
   Foundation of China under Grant 20110490174. The work of J. Han was
   supported in part by the NSCF under Grant 61005018 and the Northwestern
   Polytechnical University Foundation for Fundamental Research under
   Grant. NPU-FFR-JC201041. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Charles D.
   Creusere.
CR ALBRIGHT TD, 1995, P NATL ACAD SCI USA, V92, P2433, DOI 10.1073/pnas.92.7.2433
   Amir A., 2005, P IBM RES TRECVID 20, P1
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2009, P 17 ACM INT C MULT, DOI DOI 10.1145/1631272.1631463
   [Anonymous], 2010, P 18 ACM INT C MULTI, DOI 10.1145/1873951.1874016
   Arndt R., 2009, HDB ONTOLOGIES
   BARNETT TP, 1987, MON WEATHER REV, V115, P1825, DOI 10.1175/1520-0493(1987)115<1825:OALOMA>2.0.CO;2
   Bigdely-Shamlo N, 2008, IEEE T NEUR SYS REH, V16, P432, DOI 10.1109/TNSRE.2008.2003381
   Bressler SL, 2010, TRENDS COGN SCI, V14, P277, DOI 10.1016/j.tics.2010.04.004
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Dudai Y, 2008, PROJECTIONS, V2, P21, DOI 10.3167/proj.2008.020203
   FARACO CC, 2009, NEUROIMAGE S1, V47, pS105
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Fernández G, 2001, NEUROIMAGE, V14, P585, DOI 10.1006/nimg.2001.0854
   Fotouhi F, 2009, IEEE MULTIMEDIA, V16, P16, DOI 10.1109/MMUL.2009.27
   Friman O, 2001, MAGNET RESON MED, V45, P323, DOI 10.1002/1522-2594(200102)45:2<323::AID-MRM1041>3.0.CO;2-#
   Friston KJ, 2009, SCIENCE, V326, P399, DOI 10.1126/science.1174521
   Friston KJ, 2003, NEUROIMAGE, V19, P1273, DOI 10.1016/S1053-8119(03)00202-7
   Gerson AD, 2006, IEEE T NEUR SYS REH, V14, P174, DOI 10.1109/TNSRE.2006.875550
   Han JW, 2005, IEEE T IMAGE PROCESS, V14, P511, DOI 10.1109/TIP.2004.841205
   Haskell B.G., 1997, DIGITAL VIDEO INTRO
   Hasson U, 2004, SCIENCE, V303, P1634, DOI 10.1126/science.1089506
   Hasson U, 2010, TRENDS COGN SCI, V14, P40, DOI 10.1016/j.tics.2009.10.011
   Hauptmann A., 2006, P 6 ACM INT C IM VID, P627
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Hornak J. P., 2002, BASIC MRI
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huang J., 2007, P IEEE COMP SOC C CO, P762
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   JAYANT N, 1993, P IEEE, V81, P1385, DOI 10.1109/5.241504
   Kapoor A, 2008, PROC CVPR IEEE, P2150
   Leller J. M., 1985, IEEE T SYST MAN CYB, VSMC-15, P580
   Liu T., 2002, P IEEE INT S CIRC SY
   Liu Y, 2007, PATTERN RECOGN, V40, P262, DOI 10.1016/j.patcog.2006.04.045
   Logothetis NK, 2008, NATURE, V453, P869, DOI 10.1038/nature06976
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lu SF, 2010, COGN SYST RES, V11, P74, DOI 10.1016/j.cogsys.2008.08.009
   Martin A, 2001, CURR OPIN NEUROBIOL, V11, P194, DOI 10.1016/S0959-4388(00)00196-3
   Martìnez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Mitchell TM, 2008, SCIENCE, V320, P1191, DOI 10.1126/science.1152876
   Pasemann F, 2002, NETWORK-COMP NEURAL, V13, P195, DOI 10.1088/0954-898X/13/2/303
   Redcay E, 2010, NEUROIMAGE, V50, P1639, DOI 10.1016/j.neuroimage.2010.01.052
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Schippers MB, 2010, P NATL ACAD SCI USA, V107, P9388, DOI 10.1073/pnas.1001791107
   Sewards TV, 2011, NEUROPSYCHOLOGIA, V49, P277, DOI 10.1016/j.neuropsychologia.2010.11.018
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith JR, 1996, INT CONF ACOUST SPEE, P2239, DOI 10.1109/ICASSP.1996.545867
   Smith SM, 2004, NEUROIMAGE, V23, pS208, DOI 10.1016/j.neuroimage.2004.07.051
   Sun QS, 2005, PATTERN RECOGN, V38, P2437, DOI 10.1016/j.patcog.2004.12.013
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Walther DB, 2009, J NEUROSCI, V29, P10573, DOI 10.1523/JNEUROSCI.0559-09.2009
   Wandell BA, 2005, PHILOS T R SOC B, V360, P693, DOI 10.1098/rstb.2005.1628
   Yeh CH, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P429, DOI 10.1109/IIH-MSP.2008.13
NR 55
TC 36
Z9 36
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 314
EP 325
DI 10.1109/TMM.2011.2172201
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500007
DA 2024-07-18
ER

PT J
AU Lee, J
   Park, E
AF Lee, Joonwhoan
   Park, EunJong
TI Fuzzy Similarity-Based Emotional Classification of Color Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Case-based reasoning; emotional evaluation of color images; fuzzy
   similarity relation; inter-and intra-similarities; MPEG-7 visual
   descriptors; rough approximation
ID OBJECTIVE EVALUATION; SYSTEM
AB This paper proposes a novel scheme for evaluating an emotional response to color images. The proposed scheme uses case-based reasoning in which the prototypical color images for each emotion are stored as cases and are compared with the images to be evaluated. In the comparison, the similarities in terms of image descriptors play an important role, and their combination is crucial for the construction of a proper similarity measure. In the training phase of the proposed scheme, the weights that represent the unequal importance of each descriptor is determined in order to obtain a similarity measure that can be used to evaluate and classify a color image with respect to a pair of emotions. Prior to classification, the representative color images are chosen for each emotion by human subjects and are stored as cases. The stored images are compared with an image to be classified using the constructed similarity measure to determine which emotion is appropriate between a pair of emotions. In this study, we used color and texture descriptors recommended by MPEG-7, represented as high-dimensional vectors. In the training, we proposed a method based on the rough approximation and the fuzzy inter- and intra-similarities to determine the weights that represent the unequal importance of the complex MPEG-7 descriptors. Experimental results show a promising performance for the proposed scheme, and better performance could be achieved by including more prototypical images as cases.
C1 [Lee, Joonwhoan] Univ Chonbuk, Dept Comp Engn, 664-14 1GA Deokjin Dong, Jeonju, South Korea.
   [Park, EunJong] Chonbuk Natl Univ, Jeonju, South Korea.
C3 Jeonbuk National University
RP Lee, J (corresponding author), Univ Chonbuk, Dept Comp Engn, 664-14 1GA Deokjin Dong, Jeonju, South Korea.
EM chlee@chonbuk.ac.kr; for511@etri.re.kr
RI Wendling, Adrienne/G-5239-2011
FU Brain Korea 21 Project; Postal Technology RAMP;D program of Korea Post.
   [2006-X-001-02]; National Research Foundation of Korea [과C6B1618]
   Funding Source: Korea Institute of Science & Technology Information
   (KISTI), National Science & Technology Information Service (NTIS)
FX Manuscript received August 15, 2010; revised February 07, 2011; accepted
   May 21, 2011. Date of publication June 02, 2011; date of current version
   September 16, 2011. This work was supported in part by the second stage
   of Brain Korea 21 Project in 2011 and in part by the Postal Technology
   R&D program of Korea Post. [2006-X-001-02, Development of Real-time
   Postal Logistics System]. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Nadia
   Magnenat-Thalmann.
CR Bianchi-Berthouze N, 2003, IEEE MULTIMEDIA, V10, P103, DOI 10.1109/MMUL.2003.1218262
   Cho SB, 2002, IEEE T SYST MAN CY A, V32, P452, DOI 10.1109/TSMCA.2002.802812
   CIEPLINSKI L, 2001, ISOIECJTC1SC29WG11N4
   Colombo C, 1999, IEEE MULTIMEDIA, V6, P38, DOI 10.1109/93.790610
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   Hayashi T, 1998, IEEE T CONSUM ELECTR, V44, P347, DOI 10.1109/30.681949
   Jiang YJ, 2006, INT J MACH TOOL MANU, V46, P107, DOI 10.1016/j.ijmachtools.2005.05.003
   KAWAMOTO N, 1993, COLOR RES APPL, V18, P260, DOI 10.1002/col.5080180409
   KIM SH, 2005, KOREAN J SCI EMOTION, V8, P355
   Klir G., 1995, Fuzzy sets and fuzzy logic, V4
   KOBAYASHI S, 1981, COLOR RES APPL, V6, P93, DOI 10.1002/col.5080060210
   LEE J, 2007, P 3 ICNC, V1, P140
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   MITCHELL T, 1989, ANNU REV COMPUT SCI, V4, P417
   PARK E, 2008, THESIS JEONBUK NATL
   Pawlak Z., 1991, ROUGH SETS THEORETIC
   SOEN T, 1987, COLOR RES APPL, V12, P187, DOI 10.1002/col.5080120406
   STEPANIUK J, 1998, P 1 INT C ROUGH SETS, V1424, P290
   Um J, 2002, COLOR RES APPL, V27, P208, DOI 10.1002/col.10052
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Wang WN, 2008, IEEE IMAGE PROC, P117, DOI 10.1109/ICIP.2008.4711705
   Wang WN, 2005, Proceedings of 2005 International Conference on Machine Learning and Cybernetics, Vols 1-9, P4571
   Wei-Ning W, 2006, IEEE SYS MAN CYBERN, P3534, DOI 10.1109/ICSMC.2006.384667
   WU Q, 2005, AFFECT COMPUT INTELL, V3782, P239
   YAMADA A, 2001, ISOIECJTC1SC29WG11N4
   Yanulevskaya V, 2008, P IEEE INT C IM PROC
   Yeung DS, 2002, IEEE T PATTERN ANAL, V24, P556, DOI 10.1109/34.993562
   Yoo HW, 2006, J INF SCI ENG, V22, P1205
NR 29
TC 25
Z9 27
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1031
EP 1039
DI 10.1109/TMM.2011.2158530
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300016
DA 2024-07-18
ER

PT J
AU Firouzmanesh, A
   Cheng, I
   Basu, A
AF Firouzmanesh, A.
   Cheng, I.
   Basu, A.
TI Perceptually Guided Fast Compression of 3-D Motion Capture Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compression; motion capture; online 3-D environments; perception of
   animation
AB A time efficient compression technique, incorporating attention stimulating factors, for motion capture data is proposed. Compression ratios of 25:1 to 30:1 can be achieved with very little noticeable degradation in perceptual quality of animation. Experimental analysis shows that the proposed algorithm is much faster than comparable approaches using wavelets, thereby making our approach feasible for motion capture, transmission, and real-time synthesis on mobile devices, where processing power and memory capacity are limited.
C1 [Firouzmanesh, A.] Univ Alberta, Multimedia Res Ctr, Dept Comp Sci, Edmonton, AB T8N 3N1, Canada.
C3 University of Alberta
RP Firouzmanesh, A (corresponding author), Univ Alberta, Multimedia Res Ctr, Dept Comp Sci, Edmonton, AB T8N 3N1, Canada.
EM firouzma@ualberta.ca; locheng@ual-berta.ca; basu@ualberta.ca
CR [Anonymous], SPEC INT GROUP HUM C
   Arikan O, 2006, ACM T GRAPHIC, V25, P890, DOI 10.1145/1141911.1141971
   Beaudoin Philippe, 2007, Proceedings Graphics Interface 2007, P313, DOI 10.1145/1268517.1268568
   CMU Graphics Lab, CMU graphics lab motion capture database
   GONZALEZ R, 2008, DIGITAL IMAGE PROCES, P604
   Harrison J, 2004, ACM T GRAPHIC, V23, P569, DOI 10.1145/1015706.1015761
   Ikemoto Leslie, 2006, P 2006 S INT 3D GRAP, P49
   *ISO IEC JTC1 SC29, 2004, AN FRAM EXTENSION AF
   Li ZN, 2004, FUNDAMENTALS MULTIME
   Önder O, 2008, 3DTV CONF, P293, DOI 10.1109/3DTV.2008.4547866
   Pan YX, 2005, IEEE T MULTIMEDIA, V7, P269, DOI 10.1109/TMM.2005.843364
   Pollick F., 2003, Dans Proc. Intl. Workshop on Epigenetic Robotics, P107
   Preda M, 2007, WEB3D 2007 - 12TH INTERNATIONAL CONFERENCE ON 3D WEB TECHNOLOGY, PROCEEDINGS, P181
   Ramos MG, 2000, IEEE IMAGE PROC, P645, DOI 10.1109/ICIP.2000.901041
   REITSMA PSA, 2003, P ACM SIGGRAPH 2003, P537
   Ren L, 2005, ACM T GRAPHIC, V24, P1090, DOI 10.1145/1073204.1073316
   Tournier M, 2009, COMPUT GRAPH FORUM, V28, P355, DOI 10.1111/j.1467-8659.2009.01375.x
   *WIK, MOT CAPT
NR 18
TC 24
Z9 27
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 829
EP +
DI 10.1109/TMM.2011.2129497
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kakar, P
   Sudha, N
   Ser, W
AF Kakar, Pravin
   Sudha, N.
   Ser, Wee
TI Exposing Digital Image Forgeries by Detecting Discrepancies in Motion
   Blur
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image cepstrum; image forgery detection; motion blur estimation
AB The widespread availability of photo manipulation software has made it unprecedentedly easy to manipulate images for malicious purposes. Image splicing is one such form of tampering. In recent years, researchers have proposed various methods for detecting such splicing. In this paper, we present a novel method of detecting splicing in images, using discrepancies in motion blur. We use motion blur estimation through image gradients in order to detect inconsistencies between the spliced region and the rest of the image. We also develop a new measure to assist in inconsistent region segmentation in images that contain small amounts of motion blur. Experimental results show that our technique provides good segmentation of regions with inconsistent motion blur. We also provide quantitative comparisons with other existing blur-based techniques over a database of images. It is seen that our technique gives significantly better detection results.
C1 [Kakar, Pravin; Sudha, N.; Ser, Wee] Nanyang Technol Univ, Inst Media Innovat, Singapore 637553, Singapore.
C3 Nanyang Technological University
RP Kakar, P (corresponding author), Nanyang Technol Univ, Inst Media Innovat, Singapore 637553, Singapore.
EM pkakar@pmail.ntu.edu.sg; sudha@ntu.edu.sg; ewser@ntu.edu.sg
RI Ser, Wee/A-5085-2011
OI Ser, Wee/0000-0002-5548-8889
FU Institute for Media Innovation
FX This work was supported by a Ph. D. scholarship awarded to P. Kakar by
   the Institute for Media Innovation. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Oscar C. Au.
CR [Anonymous], 1999, AIM1657 MIT
   [Anonymous], AUTOMATICA
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P IEEE INT S CONS EL
   [Anonymous], LECT NOTES COMPUT SC
   [Anonymous], VISION INTERFACE
   [Anonymous], P IEEE INT C AC SPEE
   [Anonymous], J INF HIDING MULTIME
   [Anonymous], P 4 IASTED INT C VIS
   [Anonymous], 2006, TR2006579
   Bayram S, 2009, INT CONF ACOUST SPEE, P1053, DOI 10.1109/ICASSP.2009.4959768
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   CANNON M, 1976, IEEE T ACOUST SPEECH, V24, P58, DOI 10.1109/TASSP.1976.1162770
   Dai S., 2008, P IEEE C COMPUTER VI, P1
   DUBUISSON MP, 1994, INT C PATT RECOG, P566, DOI 10.1109/ICPR.1994.576361
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   GENNERY DB, 1973, J OPT SOC AM, V63, P1571, DOI 10.1364/JOSA.63.001571
   Hsiao DY, 2005, INT WORK SYS APPR D, P264, DOI 10.1109/SADFE.2005.8
   HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073
   Ji Hui., 2008, 2008_IEEE Conference_on_Computer_Vision_and_Pattern_Recognition, P1, DOI DOI 10.1109/CVPR.2008.4587537
   Johnson M.K., 2005, Proceedings of the 7th Workshop on Multimedia and Security, P1
   Johnson MK, 2007, IEEE T INF FOREN SEC, V2, P450, DOI 10.1109/TIFS.2007.903848
   Johnson Micah K, 2006, ACM WORKSHOP MULTIME, P48
   Levin A., 2007, PROC IEEE C COMPUTER, P1
   Mahdian B, 2007, FORENSIC SCI INT, V171, P180, DOI 10.1016/j.forsciint.2006.11.002
   Mahdian B, 2010, SIGNAL PROCESS-IMAGE, V25, P389, DOI 10.1016/j.image.2010.05.003
   Marziliano P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P57, DOI 10.1109/ICIP.2002.1038902
   Popescu AC, 2005, IEEE T SIGNAL PROCES, V53, P758, DOI 10.1109/TSP.2004.839932
   Rav-Acha A, 2005, PATTERN RECOGN LETT, V26, P311, DOI 10.1016/j.patrec.2004.10.017
   Zhang S, 2009, IONIC LIQUIDS: PHYSICOCHEMICAL PROPERTIES, P1
NR 30
TC 49
Z9 60
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 443
EP 452
DI 10.1109/TMM.2011.2121056
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700004
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Hu, MC
   Chang, MH
   Wu, JL
   Chi, L
AF Hu, Min-Chun
   Chang, Ming-Hsiu
   Wu, Ja-Ling
   Chi, Lin
TI Robust Camera Calibration and Player Tracking in Broadcast Basketball
   Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Broadcast basketball video; camera calibration; CamShift algorithm;
   highlight extraction; player tracking
AB With the growth of fandom population, a considerable amount of broadcast sports videos have been recorded, and a lot of research has focused on automatically detecting semantic events in the recorded video to develop an efficient video browsing tool for a general viewer. However, a professional sportsman or coach wonders about high level semantics in a different perspective, such as the offensive or defensive strategy performed by the players. Analyzing tactics is much more challenging in a broadcast basketball video than in other kinds of sports videos due to its complicated scenes and varied camera movements. In this paper, by developing a quadrangle candidate generation algorithm and refining the model fitting score, we ameliorate the court-based camera calibration technique to be applicable to broadcast basketball videos. Player trajectories are extracted from the video by a CamShift-based tracking method and mapped to the real-world court coordinates according to the calibrated results. The player position/trajectory information in the court coordinates can be further analyzed for professional-oriented applications such as detecting wide open event, retrieving target video clips based on trajectories, and inferring implicit/explicit tactics. Experimental results show the robustness of the proposed calibration and tracking algorithms, and three practicable applications are introduced to address the applicability of our system.
C1 [Chang, Ming-Hsiu; Wu, Ja-Ling] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Hu, Min-Chun; Chang, Ming-Hsiu] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 106, Taiwan.
   [Chi, Lin] Ta Hwa Inst Technol, Phys Educ Ctr, Hsinchu 307, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Hu, MC (corresponding author), Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 106, Taiwan.
EM trimy@cmlab.csie.ntu.edu.tw; cmhsiu@cmlab.csie.ntu.edu.tw;
   wjl@cmlab.csie.ntu.edu.tw; chilin1215@hotmail.com
RI Hu, Min-Chun/AAX-1721-2020
OI Hu, Min-Chun/0000-0003-1917-2155; WU, JA-LING/0000-0002-3631-1551
CR ALLEN JG, 2003, P PAN SYDN AR WORKSH, P3
   Assfalg J, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P825, DOI 10.1109/ICME.2002.1035909
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   BRADSKI GR, 2001, OPEN SOURCE COMPUTER
   Chen HT, 2009, J VIS COMMUN IMAGE R, V20, P204, DOI 10.1016/j.jvcir.2008.11.008
   Chen L., 2005, P 2005 ACM SIGMOD IN, P491
   Chu WT, 2008, MULTIMED TOOLS APPL, V38, P27, DOI 10.1007/s11042-007-0145-4
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Farin D, 2004, PROC SPIE, V5307, P80
   FARIN D, 2005, P IEEE INT C MULT EX, P80
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   GONG Y, 1995, IEEE INT C MULT COMP, P167
   Guéziec A, 2002, COMPUTER, V35, P38, DOI 10.1109/2.989928
   Hanjalic A, 2002, IEEE T CIRC SYST VID, V12, P90, DOI 10.1109/76.988656
   Intille SS, 2001, COMPUT VIS IMAGE UND, V81, P414, DOI 10.1006/cviu.2000.0896
   Li BX, 2002, P SOC PHOTO-OPT INS, V4676, P202
   Li Zhang, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563097
   Liu S, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/32135
   Liu Y, 2005, INT CONF ACOUST SPEE, P421
   Pingali GS, 1998, PROC CVPR IEEE, P260, DOI 10.1109/CVPR.1998.698618
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Saur DD, 1997, P SOC PHOTO-OPT INS, V3022, P176, DOI 10.1117/12.263406
   Tovinkere Vasanth., 2001, Proceedings of the 2001 IEEE International Conference on Multimedia and Expo (ICME'Ol), P1040
   Xie LX, 2002, INT CONF ACOUST SPEE, P4096
   Xu C, 2008, IEEE T MULTIMEDIA, V10, P325
   Xu Peng., 2001, Proceedings of the 2001 IEEE International Conference on Multimedia and Expo (ICME), P721, DOI [10.1109/ICME.2001.1237822, DOI 10.1109/ICME.2001.1237822]
   Zhang D., 2002, ACM Multimedia, P315
   Zhang YF, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P2190
   Zhou W., 2000, ACM Workshops on Multimedia, P213
   Zhu GY, 2009, IEEE T MULTIMEDIA, V11, P49, DOI 10.1109/TMM.2008.2008918
NR 30
TC 49
Z9 57
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 266
EP 279
DI 10.1109/TMM.2010.2100373
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800009
DA 2024-07-18
ER

PT J
AU Theodoropoulos, D
   Kuzmanov, G
   Gaydadjiev, G
AF Theodoropoulos, Dimitris
   Kuzmanov, Georgi
   Gaydadjiev, Georgi
TI Multi-Core Platforms for Beamforming and Wave Field Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio systems; digital signal processors; reconfigurable architectures
ID IMPLEMENTATION
AB Immersive-Audio technologies are widely used to build experimental and commercial audio systems. However, most of them are based on standard PCs, which introduce performance limitations and excessive power consumption. To address these drawbacks, we explore the implementation prospectives of two Immersive-Audio technologies: the beamforming (BF) and the wave field synthesis (WFS). We target two popular multi-core platforms, namely graphic processor units (GPUs) and field programmable gate arrays (FPGAs). We identify the most computationally intensive parts of both applications and employ the CUDA environment to map them onto a Quadro FX1700, a GeForce 8600GT, a GTX275, and a GTX460 GPU. Furthermore, we design our custom multi-core hardware accelerators for both algorithms and map them onto Virtex6 FPGAs. Both GPU and FPGA implementations are compared against OpenMP-annotated software running on a Core2 Duo at 3.0 GHz. Experimental results suggest that middle-range GPUs process data equally well as the Core2 Duo for the BF, and approximately two times faster for the WFS. However, high-end GPU and FPGA solutions provide an order of magnitude better performance for BF, and approximately two orders of magnitude better performance for WFS than the Core2 Duo. Ultimately, single-chip GPU and FPGA implementations can provide more power-effective solutions, since they can drive more complex microphone and loudspeaker setups than PC-based approaches.
C1 [Theodoropoulos, Dimitris; Kuzmanov, Georgi; Gaydadjiev, Georgi] Delft Univ Technol, Comp Engn Lab, Fac Elect Engn Math & Comp Sci, NL-2600 GA Delft, Netherlands.
C3 Delft University of Technology
RP Theodoropoulos, D (corresponding author), Delft Univ Technol, Comp Engn Lab, Fac Elect Engn Math & Comp Sci, NL-2600 GA Delft, Netherlands.
EM D.Theodor-opoulos@tudelft.nl; G.K.Kuzmanov@tudelft.nl;
   g.n.gaydadjiev@tudelft.nl
RI Gaydadjiev, Georgi N/F-1488-2010; Gaydadjiev, Georgi/AAY-3859-2020
OI Gaydadjiev, Georgi N/0000-0002-3678-7007; Gaydadjiev,
   Georgi/0000-0002-3678-7007
FU Artemisia iFEST project [100203]; FP7 Reflect [248976]; FP6 hArtes
   [IST-035143]; Dutch Technology Foundation STW [DCS.7533]
FX This work was supported in part by Artemisia iFEST project (grant
   100203), FP7 Reflect (grant 248976), FP6 hArtes (IST-035143), and the
   Dutch Technology Foundation STW, applied science division of NWO
   (project DCS.7533). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Ketan Mayer-Patel.
CR ANALOG DEVICES INC, 2004, SHARC PROC ADSP 2126
   [Anonymous], 1993, THESIS DELFT U TECHN
   Beracoechea JA, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/40960
   BERKHOUT AJ, 1993, J ACOUST SOC AM, V93, P2764, DOI 10.1121/1.405852
   DANIEL J, 2003, P 114 CONV AUD ENG S, P58
   Fiala M, 2004, 3RD IEEE INTERNATIONAL WORKSHOP ON HAPTIC, AUDIO AND VISUAL ENVIRONMENTS AND THEIR APPLICATIONS - HAVE 2004, P47, DOI 10.1109/HAVE.2004.1391880
   GERZON MA, 1973, J AUDIO ENG SOC, V21, P2
   Kapralos B, 2003, INT J IMAG SYST TECH, V13, P95, DOI 10.1002/ima.10045
   Kyriakakis C, 1998, P IEEE, V86, P941, DOI 10.1109/5.664281
   LATTANZI A, 2010, P AUD ENG SOC MAY
   LYONS R.G., 2001, UNDERSTANDING DIGITA
   McGraw-Herdeg Michael P., 2007, P HIGH PERF EMB COMP
   MENZEL D, 2005, P INT TONM S OCT
   Nilsen CIC, 2009, INT CONF ACOUST SPEE, P609, DOI 10.1109/ICASSP.2009.4959657
   NVIDIA Corporation, 2010, CUDA PROGR GUID VERS
   SCHUITMAN JV, 2007, P I DEGA S WAV FIELD
   SCHUITMAN JV, 2007, RAYLEIGH 2 5D OPERAT
   TASHEV I, 2008, P INT C AC ECH NOIS
   Teutsch H, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P67, DOI 10.1109/ASPAA.2003.1285821
   Texas Instruments Inc, 2002, TMS320C62XC67X POW C
   THEILE G, 2004, P AC SCI TECHN, P393
   THEODOROPOULOS D, 2009, P IEEE REC ARCH WORK, P1
   THEODOROPOULOS D, 2010, P ACM SIGDA INT S FI, P107
   Theodoropoulos D, 2009, CF'09: CONFERENCE ON COMPUTING FRONTIERS & WORKSHOPS, P127
   Theodoropoulos D, 2009, 2009 IEEE 7TH SYMPOSIUM ON APPLICATION SPECIFIC PROCESSORS (SASP 2009), P80, DOI 10.1109/SASP.2009.5226341
   Van Veen B. D., 1988, IEEE ASSP Magazine, V5, P4, DOI 10.1109/53.665
   Wall K, 2005, ULTRASON, P1400
   Weinstein E, 2004, MITLCSTM642
   *XIL INC, 2010, ML605 HARDW US GUID
   Yermeche Z, 2007, IEEE INT SYMP CIRC S, P353, DOI 10.1109/ISCAS.2007.378462
   Yiu KFC, 2008, IEEE INT CONF ASAP, P203, DOI 10.1109/ASAP.2008.4580179
   2007, XCELL J, P36
NR 32
TC 20
Z9 21
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 235
EP 245
DI 10.1109/TMM.2010.2098397
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800006
DA 2024-07-18
ER

PT J
AU Liao, YT
   Gibson, JD
AF Liao, Yiting
   Gibson, Jerry D.
TI Routing-Aware Multiple Description Video Coding Over Mobile Ad-Hoc
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Error resilience; mobile ad-hoc networks; multipath routing; multiple
   description coding; video communications
AB Supporting video transmission over error-prone mobile ad-hoc networks is becoming increasingly important as these networks become more widely deployed. We propose a routing-aware multiple description video coding approach to support video transmission over mobile ad-hoc networks with multiple path transport. We build a statistical model to estimate the packet loss probability of each packet transmitted over the network based on the standard ad-hoc routing messages and network parameters. We then estimate the frame loss probability and dynamically select reference frames in order to alleviate error propagation caused by the packet losses. We conduct experiments using the QualNet simulator that accounts for node mobility, channel properties, MAC operation, multipath routing, and traffic type. The results demonstrate that our proposed method provides 0.7-2.3 dB gains in PSNR for different video sequences under different network settings and guarantees better video quality for a selectably high number of users of the network. Furthermore, we examine the estimation accuracy of our proposed estimation model and show that our model works effectively under various network settings.
C1 [Liao, Yiting; Gibson, Jerry D.] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
C3 University of California System; University of California Santa Barbara
RP Liao, YT (corresponding author), Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
EM yiting@ece.ucsb.edu; gibson@ece.ucsb.edu
FU UC Discovery Grant Program; Applied Signal Technology, Inc.
FX Manuscript received May 27, 2010; revised October 05, 2010; accepted
   October 15, 2010. Date of publication October 21, 2010; date of current
   version January 19, 2011. This work was supported in part by the UC
   Discovery Grant Program and in part by Applied Signal Technology, Inc.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. James E. Fowler.
CR [Anonymous], P 1 MULT COMM WORKSH
   [Anonymous], P IEEE ICC
   [Anonymous], 2004, PROBABILITY STOCHAST
   [Anonymous], P INT C MULT EXP JUL
   [Anonymous], P MIL COMM C
   [Anonymous], P INT C IM PROC
   [Anonymous], P IEEE INT C COMM MA
   [Anonymous], P 22 ANN JOINT C IEE
   [Anonymous], QUALNET 4 5 1 PROGR
   [Anonymous], P INT C IM PROC
   [Anonymous], P INT C IM PROC ICIP
   [Anonymous], 2012, 802112007 IEEE
   Apostolopoulos JG, 2001, PROC SPIE, V4310, P392
   Broch J., 1998, MobiCom'98. Proceedings of Fourth Annual ACM/IEEE International Conference on Mobile Computing and Networking, P85, DOI 10.1145/288235.288256
   Gogate N, 2002, IEEE T CIRC SYST VID, V12, P777, DOI 10.1109/TCSVT.2002.803229
   Heng BA, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/32592
   Hu J, 2008, IEEE T MULTIMEDIA, V10, P1465, DOI 10.1109/TMM.2008.2007329
   Johnson D., 1996, Mobile Computing, V353, P153, DOI DOI 10.1007/978-0-585-29603-6_5
   Kompella S., 2006, PROC MILITARY COMMUN, P1
   Lee C, 2006, OPT ENG, V45, DOI 10.1117/1.2160515
   Liang YJ, 2008, IEEE T CIRC SYST VID, V18, P861, DOI 10.1109/TCSVT.2008.923139
   Liang YJ, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P420
   Liao YT, 2008, CONF REC ASILOMAR C, P2243, DOI 10.1109/ACSSC.2008.5074835
   Lin S., 2001, Proceedings of IEEE International Conference on Multimedia and Expo, P96
   Mao SW, 2006, IEEE T MULTIMEDIA, V8, P1063, DOI 10.1109/TMM.2006.879845
   Mao SW, 2003, IEEE J SEL AREA COMM, V21, P1721, DOI 10.1109/JSAC.2003.815965
   Marina MK, 2001, NETWORK PROTOCOLS, P14, DOI 10.1109/ICNP.2001.992756
   Mueller S, 2004, LECT NOTES COMPUT SC, V2965, P209
   Nasipuri A., 1999, Proceedings Eight International Conference on Computer Communications and Networks (Cat. No.99EX370), P64, DOI 10.1109/ICCCN.1999.805497
   Wang Y, 2005, P IEEE, V93, P57, DOI 10.1109/JPROC.2004.839618
NR 30
TC 15
Z9 17
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 132
EP 142
DI 10.1109/TMM.2010.2089504
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900014
DA 2024-07-18
ER

PT J
AU Zhang, YS
   Zhang, YF
   Sun, SX
   Qin, SY
   He, ZH
AF Zhang, Yunsheng
   Zhang, Yongfei
   Sun, Shixin
   Qin, Shiyin
   He, Zhihai
TI Multihop Packet Delay Bound Violation Modeling for Resource Allocation
   in Video Streaming Over Mesh Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multihop mesh networks; networks optimization; resource allocation;
   video streaming
ID DISTORTION ANALYSIS; CHANNEL RATE; TRANSMISSION; SELECTION
AB Resource allocation plays a critical role in multisession video streaming over mesh networks to maximize the overall video presentation quality under transmission delay and network resource constraints. A critical component in efficient resource allocation is to analyze and model the multihop queuing behavior along the transmission path, estimate the packet loss ratio due to delay bound violation, and predict the amount of video quality degradation after multihop video transmission. In this work, we develop a multihop packet delay bound violation model to predict the packet loss probability and end-to-end distortion for video streaming over multihop networks. To this end, we extract salient features to characterize the input source and network conditions of links along the transmission path and construct a learning-based model using artificial neural network (ANN). Based on this model, we then formulate the resource allocation into a nonconvex optimization problem which aims to minimize the overall video distortion while maintaining fairness between sessions. We solve this optimization problem using Lagrangian duality methods. Extensive experimental results demonstrate that, with this widely-used offline-training-online-estimation mechanism, the proposed model is potentially applicable to almost all network conditions and can provide fairly accurate estimation results as compared with other models with a given sample data set. The proposed optimization algorithm achieves more efficient resource allocation than existing schemes.
C1 [Zhang, Yunsheng; Sun, Shixin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
   [Zhang, Yongfei; Qin, Shiyin] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
   [He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
C3 University of Electronic Science & Technology of China; Beihang
   University; University of Missouri System; University of Missouri
   Columbia
RP Zhang, YS (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM zhgyunsheng@gmail.com; zhangyf.ac@gmail.com; sxsun@uestc.edu.cn;
   qsy@buaa.edu.cn; HeZhi@missouri.edu
RI Zhang, Yongfei/A-1505-2010; qin, shi/JNY-1785-2023; He,
   Zhihai/A-5885-2019
OI Zhang, Yongfei/0000-0002-5080-1733
CR Abate J., 1994, STOCH MODELS, V10, P99
   [Anonymous], J INF SCI ENG
   Aubin J. P., 1976, Mathematics of Operations Research, V1, P225, DOI 10.1287/moor.1.3.225
   Bertsekas D. P., 1992, Data Networks, V2nd
   BOTVICH DD, 1995, QUEUEING SYST, V20, P293, DOI 10.1007/BF01245322
   BREASLU L, 2000, IEEE COMPUT, V33, P59
   BRIAN LM, 1998, IEEE ACM T NETWORK, V6, P811
   Chakareski J, 2006, IEEE T MULTIMEDIA, V8, P207, DOI 10.1109/TMM.2005.864284
   CHANG CS, 1994, IEEE T AUTOMAT CONTR, V39, P913, DOI 10.1109/9.284868
   CORMEN TH, 2001, BELLMANFORD ALGORITH, P588
   DANI JU, 2005, P IEEE GLOBECOM 2005
   Fall K., 2011, NS MANUAL FORMERLY N
   Fernandez JC, 2009, IEEE T MULTIMEDIA, V11, P1082, DOI 10.1109/TMM.2009.2026086
   Fu FW, 2007, IEEE J-STSP, V1, P264, DOI 10.1109/JSTSP.2007.901519
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   He ZH, 2006, IEEE T CIRC SYST VID, V16, P1051, DOI 10.1109/TCSVT.2006.881198
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Hsu CY, 1997, IEEE J SEL AREA COMM, V15, P1016, DOI 10.1109/49.611156
   JUNG YH, 2008, SCI DIRECT, V36, P702
   Jurca D, 2008, SIGNAL PROCESS-IMAGE, V23, P754, DOI 10.1016/j.image.2008.09.002
   Kelly F.P., 1996, Stochastic Networks: Theory and Applications, P141
   Klaue J, 2003, LECT NOTES COMPUT SC, V2794, P255, DOI 10.1007/978-3-540-45232-4_16
   KOLMOGOROV AN, 1957, DOKL AKAD NAUK SSSR+, V114, P953
   KUMER A, 2005, COMMUNICATION NETWOR
   Kumwilaisak W, 2003, IEEE J SEL AREA COMM, V21, P1685, DOI 10.1109/JSAC.2003.816445
   Li Y, 2009, IEEE T MULTIMEDIA, V11, P1182, DOI 10.1109/TMM.2009.2026102
   Nielsen RH, 1987, P IJCNN 87, VIII, P11
   NIELSON RH, 1989, P INT JOINT C NEUR N
   Pahalawatta PV, 2007, WIREL COMMUN MOB COM, V7, P131, DOI 10.1002/wcm.469
   Setton E, 2004, IEEE IMAGE PROC, P1751
   Shiang HP, 2007, IEEE J SEL AREA COMM, V25, P770, DOI 10.1109/JSAC.2007.070513
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Walrand J., 2000, HIGH PERFORMANCE COM, V2nd
   WU D, 2007, P SPIE BELL WA
   Wu M, 2001, IEEE T MULTIMEDIA, V3, P186, DOI 10.1109/6046.923818
   YU CY, 2007, P FUT GEN COMM NETW, V1, P90
   Yu W, 2006, IEEE T COMMUN, V54, P1310, DOI 10.1109/TCOMM.2006.877962
NR 37
TC 8
Z9 8
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2010
VL 12
IS 8
BP 886
EP 900
DI 10.1109/TMM.2010.2065217
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 681ZW
UT WOS:000284365100009
DA 2024-07-18
ER

PT J
AU Alías, F
   Munné, N
AF Alias, Francesc
   Munne, Natalia
TI Reliable Pitch Marking of Affective Speech at Peaks or Valleys Using
   Restricted Dynamic Programming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective speech; dynamic programming; pitch marking; speech analysis;
   unit-selection text-to-speech synthesis
AB The affective communication channel plays a key role in multimodal human-computer interaction. In this context, the generation of realistic talking-heads expressing emotions both in appearance and speech is of great interest. The synthetic speech of talking-heads is generally obtained from a text-to-speech (TTS) synthesizer. One of the dominant techniques for achieving high-quality synthetic speech is unit-selection TTS (US-TTS) synthesis. Affective US-TTS systems are driven by affective annotated speech databases. Since affective speech involves higher acoustic variability than neutral speech, achieving trustworthy speech labeling is a more challenging task. To that effect, this paper introduces a methodology for achieving reliable pitch marking on affective speech. The proposal adjusts the pitch marks at the signal peaks or valleys after applying a three-stage restricted dynamic programming algorithm. The methodology can be applied as a post-processing of any pitch determination and pitch marking algorithm (with any local criterion for locating pitch marks), or their merging. The experiments show that the proposed methodology significantly improves the results of the input state-of-the-art markers on affective speech.
C1 [Alias, Francesc; Munne, Natalia] La Salle Univ Ramon Llull, Grp Recerca Tecnol Media, Barcelona 08022, Spain.
C3 Universitat Ramon Llull
RP Alías, F (corresponding author), La Salle Univ Ramon Llull, Grp Recerca Tecnol Media, Barcelona 08022, Spain.
EM falias@salle.url.edu; st16874@salle.url.edu
RI Alías-Pujol, Francesc/L-1088-2014
OI Alías-Pujol, Francesc/0000-0002-1921-2375
CR Alías F, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P1698
   ASHOURI K, 2004, P EUSIPCO VIENN SEPT, P995
   Barner KE, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P2243, DOI 10.1109/ICSLP.1996.607252
   BARRACHICOTE R, 2008, P LREC MARR MOR
   Black A. W, 2003, P EUROPEAN C SPEECH, P1649
   Black AW, 2007, INT CONF ACOUST SPEE, P1229
   BONAFONTE A, 2006, P TC STAR WORKSH SPE, P199
   Cabral JP, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P1798
   Chen WR, 2001, PROC SPIE, V4257, P1
   CLARK RAJ, 2005, P INT LISB PORT, P101
   COLOTTE V, 2002, P 11 EUR SIGN PROC C, V1, P419
   Cosatto E, 2000, IEEE T MULTIMEDIA, V2, P152, DOI 10.1109/6046.865480
   de Cheveigné A, 2002, J ACOUST SOC AM, V111, P1917, DOI 10.1121/1.1458024
   DIKSHIT P, 2005, P ICASSP 2005 PHIL U, V1, P233
   FERENCZ A, 2004, P ICSLP JEJ ISL S KO, P2437
   Goncharoff V., 1998, P IASTED INT C SIGN, P281
   HARBECK S, 1995, P EUROSPEECH MADR, V2, P1337
   HOGE H, 2006, P ITG FACHT SPRACH K
   HOSOM JP, 2005, P INT LISB PORT, P317
   Hunt AJ, 1996, INT CONF ACOUST SPEE, P373, DOI 10.1109/ICASSP.1996.541110
   Hussein H, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P135
   HUSSEIN H, 2007, P INTERSPEECH ANTW B, P1653
   Iriondo I, 2009, SPEECH COMMUN, V51, P744, DOI 10.1016/j.specom.2008.12.001
   Jaimes A, 2007, COMPUT VIS IMAGE UND, V108, P116, DOI 10.1016/j.cviu.2006.10.019
   Kominek J., 2003, CMULTI03177
   Kotnik B, 2009, SIGNAL PROCESS, V89, P2555, DOI 10.1016/j.sigpro.2009.04.017
   Kounoudes A, 2002, INT CONF ACOUST SPEE, P349
   Legát M, 2007, LECT NOTES ARTIF INT, V4629, P502
   Legat M, 2007, INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION, VOLS 1-4, P1641
   LI X, 2004, P ICSLP JEJ ISL S KO, P1101
   LIN CY, 2004, P ICSLP JEJ ISL S KO, P1189
   MARTIN J, 2007, P AUD VIS SPEECH PRO
   Matousek J, 2008, SIXTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, LREC 2008, P1296
   Melenchón J, 2009, IEEE T AUDIO SPEECH, V17, P459, DOI 10.1109/TASL.2008.2010213
   Murty KSR, 2008, IEEE T AUDIO SPEECH, V16, P1602, DOI 10.1109/TASL.2008.2004526
   Naylor PA, 2007, IEEE T AUDIO SPEECH, V15, P34, DOI 10.1109/TASL.2006.876878
   NGOC TV, 1999, P EUR 1999 BUD HUNG, P2805
   SCHRODER S, 2009, P BLIZZ CHALL ED UK
   Steidl S, 2008, LECT NOTES ARTIF INT, V5246, P525, DOI 10.1007/978-3-540-87391-4_67
   SUN X, 2000, 6 INT C SPOK LANG PR, V4, P676
   Sun XJ, 2002, INT CONF ACOUST SPEE, P333
   Talkin D., 1995, Speech coding and synthesis, V495, P518
   Tang H, 2008, IEEE T MULTIMEDIA, V10, P969, DOI 10.1109/TMM.2008.2001355
   Thomas MRP, 2009, IEEE T AUDIO SPEECH, V17, P1557, DOI 10.1109/TASL.2009.2022430
   Ververidis D, 2006, SPEECH COMMUN, V48, P1162, DOI 10.1016/j.specom.2006.04.003
NR 45
TC 1
Z9 2
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2010
VL 12
IS 6
BP 481
EP 489
DI 10.1109/TMM.2010.2051873
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 668TB
UT WOS:000283291900002
DA 2024-07-18
ER

PT J
AU Biswas, S
   Aggarwal, G
   Chellappa, R
AF Biswas, Soma
   Aggarwal, Gaurav
   Chellappa, Rama
TI An Efficient and Robust Algorithm for Shape Indexing and Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fast retrieval; indexing; shape matching
ID NONRIGID SHAPES; RECOGNITION; SIMILARITY; REPRESENTATION;
   CLASSIFICATION; DISTANCE
AB Many shape matching methods are either fast but too simplistic to give the desired performance or promising as far as performance is concerned but computationally demanding. In this paper, we present a very simple and efficient approach that not only performs almost as good as many state-of-the-art techniques but also scales up to large databases. In the proposed approach, each shape is indexed based on a variety of simple and easily computable features which are invariant to articulations, rigid transformations, etc. The features characterize pairwise geometric relationships between interest points on the shape. The fact that each shape is represented using a number of distributed features instead of a single global feature that captures the shape in its entirety provides robustness to the approach. Shapes in the database are ordered according to their similarity with the query shape and similar shapes are retrieved using an efficient scheme which does not involve costly operations like shape-wise alignment or establishing correspondences. Depending on the application, the approach can be used directly for matching or as a first step for obtaining a short list of candidate shapes for more rigorous matching. We show that the features proposed to perform shape indexing can be used to perform the rigorous matching as well, to further improve the retrieval performance.
   To illustrate the computational and performance advantages of the proposed approach, extensive experiments have been performed on several challenging problems that involve matching shapes. We also highlight the effectiveness of the approach to perform robust and efficient shape matching in real images and videos for different applications like human pose estimation and activity classification.
C1 [Biswas, Soma; Aggarwal, Gaurav; Chellappa, Rama] Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA.
   [Biswas, Soma; Chellappa, Rama] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   [Aggarwal, Gaurav] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park;
   University System of Maryland; University of Maryland College Park;
   University System of Maryland; University of Maryland College Park
RP Biswas, S (corresponding author), Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
EM sbiswas@nd.edu; gaggarwa@nd.com; rama@cfar.umd.edu
RI Chellappa, Rama/B-6573-2012; Chellappa, Rama/AAV-8690-2020
FU UNISYS; NSF-ITR [03-25119]; ONR MURI [N00014-08-1-0638]
FX Manuscript received September 13, 2009; revised February 11, 2010;
   accepted April 14, 2010. Date of publication May 20, 2010; date of
   current version July 16, 2010. This work was supported in part by
   UNISYS, in part by NSF-ITR Grant 03-25119, and in part by ONR MURI Grant
   N00014-08-1-0638. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. James Z. Wang.
CR Adamek T, 2004, IEEE T CIRC SYST VID, V14, P742, DOI 10.1109/TCSVT.2004.826776
   Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21
   Alajlan N, 2008, IEEE T PATTERN ANAL, V30, P1003, DOI 10.1109/TPAMI.2008.37
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], 1997, Image Databases and Multi-Media Search, DOI DOI 10.1142/9789812797988_
   Attalla E, 2005, PATTERN RECOGN, V38, P2229, DOI 10.1016/j.patcog.2005.02.009
   BEIS J, 1997, P IEEE C COMP VIS PA, P984
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Ben Hamza A, 2003, LECT NOTES COMPUT SC, V2886, P378
   Berretti S, 2000, IEEE T MULTIMEDIA, V2, P225, DOI 10.1109/6046.890058
   BISWAS S, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383292
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   CARLSSON S, 2001, P IEEE COMP SOC WORK
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   Daliri MR, 2008, PATTERN RECOGN, V41, P1782, DOI 10.1016/j.patcog.2007.10.020
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   FELZENSZWALB P, 2007, P IEEE C COMP VIS PA, P1
   Fudos I, 2002, PATTERN RECOGN LETT, V23, P731, DOI 10.1016/S0167-8655(01)00148-9
   Gdalyahu Y, 1999, IEEE T PATTERN ANAL, V21, P1312, DOI 10.1109/34.817410
   Germain RS, 1997, IEEE COMPUT SCI ENG, V4, P42, DOI 10.1109/99.641608
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Grigorescu C, 2003, IEEE T IMAGE PROCESS, V12, P729, DOI 10.1109/TIP.2003.814250
   Hjaltason GR, 2003, IEEE T PATTERN ANAL, V25, P530, DOI 10.1109/TPAMI.2003.1195989
   Ip CY, 2002, P 7 ACM S SOL MOD AP, P273, DOI 10.1145/566282.566322
   Lamdan Y., 1988, PROC 2 INT C COMPUTE, P238
   Latecki LJ, 2000, IEEE T PATTERN ANAL, V22, P1185, DOI 10.1109/34.879802
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   LEIBE B, 2003, P IEEE C COMP VIS PA
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   LIU Y, 2002, P INT C SHAP MOD APP
   McNeill G., 2006, P IEEE C COMP VIS PA, P885
   Mori G, 2005, IEEE T PATTERN ANAL, V27, P1832, DOI 10.1109/TPAMI.2005.220
   Mori G, 2003, PROC CVPR IEEE, P134
   Mori G, 2001, PROC CVPR IEEE, P723
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   Ohbuchi R, 2003, THEORY AND PRACTICE OF COMPUTER GRAPHICS, PROCEEDINGS, P97, DOI 10.1109/TPCG.2003.1206936
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Peter Adrian., 2008, Proc. IEEE Conf. Computer Vision and Pattern Recognition, P1
   Petrakis EGM, 2002, IEEE T PATTERN ANAL, V24, P1501, DOI 10.1109/TPAMI.2002.1046166
   Rafiei D, 2002, VLDB J, V11, P17, DOI 10.1007/s007780100059
   Rao C, 2002, INT J COMPUT VISION, V50, P203, DOI 10.1023/A:1020350100748
   Sebastian TB, 2003, IEEE T PATTERN ANAL, V25, P116, DOI 10.1109/TPAMI.2003.1159951
   Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924
   Sharvit D, 1998, J VIS COMMUN IMAGE R, V9, P366, DOI 10.1006/jvci.1998.0396
   Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703
   Super BJ, 2006, INT J PATTERN RECOGN, V20, P1117, DOI 10.1142/S0218001406005174
   Thayananthan A, 2003, PROC CVPR IEEE, P127
   TRESADERN P, 2007, P BRIT MACH VIS C
   Tu ZW, 2004, LECT NOTES COMPUT SC, V3023, P195
   WANG J, 1999, P IEEE INT C MULT CO
   Xie J, 2008, PATTERN RECOGN, V41, P1756, DOI 10.1016/j.patcog.2007.11.005
   Yang XW, 2009, PROC CVPR IEEE, P357, DOI 10.1109/CVPRW.2009.5206844
   Yilmaz A, 2005, PROC CVPR IEEE, P984
NR 54
TC 25
Z9 30
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2010
VL 12
IS 5
BP 372
EP 385
DI 10.1109/TMM.2010.2050735
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 656DN
UT WOS:000282306500003
DA 2024-07-18
ER

PT J
AU Huang, Z
   Shen, HT
   Shao, J
   Cui, B
   Zhou, XF
AF Huang, Zi
   Shen, Heng Tao
   Shao, Jie
   Cui, Bin
   Zhou, Xiaofang
TI Practical Online Near-Duplicate Subsequence Detection for Continuous
   Video Streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Continuous detection; near-duplicate subsequence; online detection;
   subsequence match; video stream
ID QUICK SEARCH METHOD; AUDIO
AB Online video content is surging to an unprecedented level. Massive video publishing and sharing impose heavy demands on online near-duplicate detection for many novel video applications. This paper presents an accurate and practical system for online near-duplicate subsequence detection over continuous video streams. We propose to transform a video stream into a one-dimensional video distance trajectory (VDT) monitoring the continuous changes of consecutive frames with respect to a reference point, which is further segmented and represented by a sequence of compact signatures called linear smoothing functions (LSFs). LSFs of each subsequence of the incoming video stream are continuously generated and temporally stored in a buffer for comparison with query LSFs. LSF adopts compound probability to combine three independent video factors for effective segment similarity measure, which is then utilized to compute sequence similarity for near-duplicate detection. To avoid unnecessary sequence similarity computations, an efficient sequence skipping strategy is also embedded. Experimental results on detecting diverse near-duplicates of TV commercials in real video streams show the superior performance of our system on both effectiveness and efficiency over existing methods.
C1 [Huang, Zi; Shen, Heng Tao; Zhou, Xiaofang] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
   [Shao, Jie] Univ Melbourne, Dept Comp Sci & Software Engn, Parkville, Vic 3010, Australia.
   [Cui, Bin] Peking Univ, Dept Comp Sci, Beijing 100871, Peoples R China.
C3 University of Queensland; University of Melbourne; Peking University
RP Huang, Z (corresponding author), Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
EM huang@itee.uq.edu.au; shenht@itee.uq.edu.au; jsh@csse.unimelb.edu.au;
   zxf@itee.uq.edu.au
RI Zhou, Xiaofang/C-6169-2013; Zhou, Xiangfeng/KDO-8724-2024; Cui,
   Bin/A-4554-2012; Shen, Heng Tao/ABD-5331-2021; zhou, xin-fu/F-4119-2013
OI Zhou, Xiaofang/0000-0001-6343-1455; HUANG, ZI/0000-0002-9738-4949; CUI,
   Bin/0000-0003-1681-4677
CR [Anonymous], P 14 ACM INT C MULT
   [Anonymous], P ACMMM
   [Anonymous], 2009, P ACM INT C MULT, DOI DOI 10.1145/1631272.1631295
   [Anonymous], 2007, Proceedings of the 33rd International Conference on Very Large Data Bases. VLDB'07
   [Anonymous], P 16 INT C DAT ENG S
   [Anonymous], 2003, KDD
   Chakrabarti K, 2002, ACM T DATABASE SYST, V27, P188, DOI 10.1145/568518.568520
   Chang HS, 1999, IEEE T CIRC SYST VID, V9, P1269, DOI 10.1109/76.809161
   Chen L., 2005, P 2005 ACM SIGMOD IN, P491
   CHERUBINI M, 2009, P ACM MULT, P35
   Cheung SCS, 2003, IEEE T CIRC SYST VID, V13, P59, DOI 10.1109/TCSVT.2002.808080
   CHIU CY, 2006, P ICPR 06, P228
   Hoad TC, 2006, ACM T INFORM SYST, V24, P1, DOI 10.1145/1125857.1125858
   Joly A, 2003, LECT NOTES COMPUT SC, V2728, P414
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Kashino K, 2003, IEEE T MULTIMEDIA, V5, P348, DOI 10.1109/TMM.2003.813281
   KE Y, 2004, P ACM MULT
   Kimura A, 2008, IEEE T AUDIO SPEECH, V16, P396, DOI 10.1109/TASL.2007.912362
   Law-To J., 2006, P ACM INT C MULTIMED, P835
   Law-To Julien., 2007, P 6 ACM INT C IMAGE, P371
   Shen H.T., 2005, P ACM SIGMOD INT C M, P730
   Shen H.T., 2007, VLDB, P1374
   Wu A. G., 2007, P ACM MM, P218
   Wu H., 2004, P INT C MANAGEMENT D, P23, DOI DOI 10.1145/1007568.1007574
   YAN Y, 2008, P ICDE
   Yi B K., 2000, Proceedings of the 26st International Conference on Very Large Databases, P385
NR 26
TC 35
Z9 39
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2010
VL 12
IS 5
BP 386
EP 398
DI 10.1109/TMM.2010.2050737
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 656DN
UT WOS:000282306500004
DA 2024-07-18
ER

PT J
AU Abdollahian, G
   Taskiran, CM
   Pizlo, Z
   Delp, EJ
AF Abdollahian, Golnaz
   Taskiran, Cuneyt M.
   Pizlo, Zygmunt
   Delp, Edward J.
TI Camera Motion-Based Analysis of User Generated Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-based video analysis; eye tracking; home video; motion-based
   analysis; regions of interest; saliency maps; user generated video;
   video summarization
ID COMPRESSED VIDEO; VISUAL-ATTENTION; MODEL
AB In this paper we propose a system for the analysis of user generated video (UGV). UGV often has a rich camera motion structure that is generated at the time the video is recorded by the person taking the video, i.e., the "camera person." We exploit this structure by defining a new concept known as camera view for temporal segmentation of UGV. The segmentation provides a video summary with unique properties that is useful in applications such as video annotation. Camera motion is also a powerful feature for identification of keyframes and regions of interest (ROIs) since it is an indicator of the camera person's interests in the scene and can also attract the viewers' attention. We propose a new location-based saliency map which is generated based on camera motion parameters. This map is combined with other saliency maps generated using features such as color contrast, object motion and face detection to determine the ROIs. In order to evaluate our methods we conducted several user studies. A subjective evaluation indicated that our system produces results that is consistent with viewers' preferences. We also examined the effect of camera motion on human visual attention through an eye tracking experiment. The results showed a high dependency between the distribution of fixation points of the viewers and the direction of camera movement which is consistent with our location-based saliency map.
C1 [Abdollahian, Golnaz; Delp, Edward J.] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
   [Taskiran, Cuneyt M.] Motorola Inc, Applicat Res & Technol Ctr, Schaumburg, IL 60196 USA.
   [Pizlo, Zygmunt] Purdue Univ, Dept Psychol Sci, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University; Purdue University System;
   Purdue University
RP Abdollahian, G (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM gabdollla@ecn.purdue.edu; cuneyt.taskiran@motorola.com;
   pizlo@psych.purdue.edu; ace@ecn.purdue.edu
RI Delp, Edward J/C-3616-2013
OI Delp, Edward/0000-0002-2909-7323
FU Motorola Partnerships
FX This work was supported by a Motorola Partnerships in Research Grant.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Qibin Sun.
CR ABDOLLAHIAN G, 2009, P IEEE INT C MULT EX
   Abdollahian G, 2007, PROC SPIE, V6506, DOI 10.1117/12.705728
   ALBIOL A, 2003, P IEEE INT C AC SPEE
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2005, P 18 ANN ACM S US IN
   [Anonymous], 2003, P 11 ACM INT C MULTI, DOI DOI 10.1145/957013.957094
   [Anonymous], 2002, P 10 ACM INT C MULT
   BABAGUCHI N, 2000, P ACM MULT 2000 WORK, P205
   BOX GEP, 1949, BIOMETRIKA, V36, P317, DOI 10.2307/2332671
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Choi JW, 2000, PROCEEDINGS OF THE 43RD IEEE MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS I-III, P758, DOI 10.1109/MWSCAS.2000.952867
   Chong-Wah N., 2001, INT J IMAGE GRAPH, V1, P445
   COUDERT F, 1998, P SPIE C MULT STOR A, V3527
   Dagli CK, 2005, Third International Conference on Information Technology and Applications, Vol 1, Proceedings, P173
   Dony RD, 2005, IEE P-VIS IMAGE SIGN, V152, P425, DOI 10.1049/ip-vis:20045109
   DUAN LY, 2004, P ACM MULTIMEDIA NEW, P328
   Gatica-Perez D, 2003, IEEE T CIRC SYST VID, V13, P539, DOI 10.1109/TCSVT.2003.813428
   GATICAPEREZ D, 2002, P IEEE INT C MULT EX, V2, P525
   GIRGENSOHN A, 2000, P ACM S US INT SOFTW, V2, P81
   Gray R. M., 1984, IEEE ASSP Magazine, V1, P4, DOI 10.1109/MASSP.1984.1162229
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   HU Y, 2004, P ACM MULT OCT
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L., 2004, NEUROMORPHIC ENG, V1, P10
   KENDER JR, 2000, P AS C COMP VIS TAIP
   KRAAIJ W, 2005, TREC VIDEO LOW LEVEL
   LAN D, 2003, P IEEE INT C MULT EX
   Ma YF, 2002, IEEE IMAGE PROC, P129
   MEUR OL, 2005, P INT C IM PROC, V3, P1188
   Nesvadba J, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P788, DOI 10.1109/ICME.2005.1521541
   Osberger W, 2001, PROC SPIE, V4299, P361, DOI 10.1117/12.429506
   OVER P, 2008, TRECVID RUSH SUMM WO
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Salvucci D.D., 2000, P 2000 S EYE TRACK R, P71, DOI [DOI 10.1145/355017.355028, 10.1145/355017.355028]
   Sauer K, 1996, IEEE T CIRC SYST VID, V6, P513, DOI 10.1109/76.538933
   SAWHNEY HS, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P583
   Tan YP, 2000, IEEE T CIRC SYST VID, V10, P133, DOI 10.1109/76.825867
   Taskiran C, 2004, IEEE T MULTIMEDIA, V6, P103, DOI 10.1109/TMM.2003.819783
   Taskiran CM, 2006, IEEE T MULTIMEDIA, V8, P775, DOI 10.1109/TMM.2006.876282
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WU P, 2004, P ICASSP, V5, P957
   Wu S, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P217
   Xie X, 2006, IEEE T MULTIMEDIA, V8, P707, DOI 10.1109/TMM.2006.876294
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   Yeung M, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P296, DOI 10.1109/MMCS.1996.534991
   Yeung MM, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pA338
   PITTSBURGH PATTERN R
NR 49
TC 42
Z9 52
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2010
VL 12
IS 1
BP 28
EP 41
DI 10.1109/TMM.2009.2036286
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 533SJ
UT WOS:000272844800003
DA 2024-07-18
ER

PT J
AU Naccari, M
   Tagliasacchi, M
   Tubaro, S
AF Naccari, Matteo
   Tagliasacchi, Marco
   Tubaro, Stefano
TI No-Reference Video Quality Monitoring for H.264/AVC Coded Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video coding; video quality monitoring
ID MODE SELECTION; DISTORTION; TRANSMISSION; VISIBILITY; CHANNEL
AB When video is transmitted over a packet-switched network, the sequence reconstructed at the receiver side might suffer from impairments introduced by packet losses, which can only be partially healed by the action of error concealment techniques. In this context we propose NORM (NO-Reference video quality Monitoring), an algorithm to assess the quality degradation of H. 264/AVC video affected by channel errors. NORM works at the receiver side where both the original and the uncorrupted video content is unavailable. We explicitly account for distortion introduced by spatial and temporal error concealment together with the effect of temporal motion-compensation. NORM provides an estimate of the mean square error distortion at the macroblock level, showing good linear correlation (correlation coefficient greater than 0.80) with the distortion computed in full-reference mode. In addition, the estimate at the macroblock level can be successfully exploited by forward quality monitoring systems that compute quality objective metrics to predict mean opinion score (MOS) values. As a proof of concept, we feed the output of NORM to a reduced-reference quality monitoring system that computes an estimate of the structural similarity metric (SSIM) score, which is known to be well correlated with perceptual quality.
C1 [Naccari, Matteo; Tagliasacchi, Marco; Tubaro, Stefano] Politecn Milan, Dipartimento Elettron & Informaz, I-20133 Milan, Italy.
C3 Polytechnic University of Milan
RP Naccari, M (corresponding author), Politecn Milan, Dipartimento Elettron & Informaz, I-20133 Milan, Italy.
EM matteo.naccari@polimi.it; marco.tagliasacchi@polimi.it;
   stefano.tubaro@polimi.it
FU European Commission
FX This work was presented in part in [ 1] and has been developed within
   VISNET II, a European Network of Excellence (http://www.visnet-noe.org),
   funded under the European Commission IST FP6 programme.
CR Agrafiotis D, 2006, IEEE T CIRC SYST VID, V16, P960, DOI 10.1109/TCSVT.2006.879988
   [Anonymous], P INT C AC SPEECH SI
   [Anonymous], H 264 AVC REF SOFTW
   BRANDAO T, 2007, P PICT COD S LISB PO
   Chang YC, 1998, P SOC PHOTO-OPT INS, V3299, P173, DOI 10.1117/12.320108
   Chua TK, 2006, IEEE NETWORK, V20, P14, DOI 10.1109/MNET.2006.273116
   CLAYPOOL M, 1999, P ACM MULT ORL FL NO
   Engelke U, 2008, INT CONF ACOUST SPEE, P869, DOI 10.1109/ICASSP.2008.4517748
   FARBER N, 1999, P IEEE INT C IM PROC
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   Ichigaya A, 2006, IEEE T CIRC SYST VID, V16, P251, DOI 10.1109/TCSVT.2005.858745
   *ISO IEC FDIS, 2003, 1449610 ISOIEC FDIS
   Kanumuri S, 2006, IEEE T MULTIMEDIA, V8, P341, DOI 10.1109/TMM.2005.864343
   KANUMURI S, 2006, P INT C IM PROC ATL
   Lee C, 2006, OPT ENG, V45, DOI 10.1117/1.2160515
   List P, 2003, IEEE T CIRC SYST VID, V13, P614, DOI 10.1109/TCSVT.2003.815175
   NACCARI M, 2008, P INT C IM PROC SAN
   Reibman AR, 2004, IEEE T MULTIMEDIA, V6, P327, DOI 10.1109/TMM.2003.822785
   Richardson I.E. G., 2002, VIDEO CODEC DESIGN
   Secker A, 2004, IEEE T IMAGE PROCESS, V13, P1029, DOI 10.1109/TIP.2004.826089
   SESHADRINATHAN K, 2007, P INT C AC SPEECH SI
   SHU T, 2005, P INT WORKSH NETW OP
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   SULLIVAN GJ, 2003, JVT1049
   Wang Y, 2006, IEEE T CIRC SYST VID, V16, P716, DOI 10.1109/TCSVT.2006.875203
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2001, IEEE T IMAGE PROCESS, V10, P1397, DOI 10.1109/83.951527
   Wenger S, 2003, IEEE T CIRC SYST VID, V13, P645, DOI 10.1109/TCSVT.2003.814966
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   WINKLER S, 2007, P EUR SIGN PROC C PO
   WINKLER S, IEEE T BROA IN PRESS
   YAMADA T, 2007, IEEE PACKET VIDEO LA
   Yang H, 2007, IEEE T CIRC SYST VID, V17, P845, DOI 10.1109/TCSVT.2007.897116
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
NR 36
TC 46
Z9 57
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 932
EP 946
DI 10.1109/TMM.2009.2021785
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300012
DA 2024-07-18
ER

PT J
AU Wan, T
   Canagarajah, N
   Achim, A
AF Wan, Tao
   Canagarajah, Nishan
   Achim, Alin
TI Segmentation-Driven Image Fusion Based on Alpha-Stable Modeling of
   Wavelet Coefficients
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bivariate alpha-stable distributions; image segmentation;
   Kullback-Leibler distance; region-based image fusion; wavelet
   decomposition
AB A novel region-based image fusion framework based on multiscale image segmentation and statistical feature extraction is proposed. A dual-tree complex wavelet transform (DT-CWT) and a statistical region merging algorithm are used to produce a region map of the source images. The input images are partitioned into meaningful regions containing salient information via symmetric alpha-stable (S alpha S) distributions. The region features are then modeled using bivariate alpha-stable (B alpha S) distributions, and the statistical measure of similarity between corresponding regions of the source images is calculated as the Kullback-Leibler distance (KLD) between the estimated B a S models. Finally, a segmentation-driven approach is used to fuse the images, region by region, in the complex wavelet domain. A novel decision method is introduced by considering the local statistical properties within the regions, which significantly improves the reliability of the feature selection and fusion processes. Simulation results demonstrate that the bivariate alpha-stable model outperforms the univariate alpha-stable and generalized Gaussian densities by not only capturing the heavy-tailed behavior of the subband marginal distribution, but also the strong statistical dependencies between wavelet coefficients at different scales. The experiments show that our algorithm achieves better performance in comparison with previously proposed pixel and region-level fusion approaches in both subjective and objective evaluation tests.
C1 [Wan, Tao; Canagarajah, Nishan; Achim, Alin] Univ Bristol, Ctr Commun Res, Dept Elect & Elect Engn, Bristol BS8 1UB, Avon, England.
C3 University of Bristol
RP Wan, T (corresponding author), Univ Bristol, Ctr Commun Res, Dept Elect & Elect Engn, Bristol BS8 1UB, Avon, England.
EM Tao.Wan@bristol.ac.uk; Nishan.Canagarajah@bristol.ac.uk;
   Alin.Achim@bristol.ac.uk
RI Achim, Alin/A-3358-2009
OI Achim, Alin/0000-0002-0982-7798
CR Abidi M.A., 1992, Data fusion in robotics and machine intelligence
   Achim A, 2005, IEEE SIGNAL PROC LET, V12, P17, DOI 10.1109/LSP.2004.839692
   Achim A, 2003, IEEE T GEOSCI REMOTE, V41, P1773, DOI 10.1109/TGRS.2003.813488
   Achim A, 2001, IEEE T MED IMAGING, V20, P772, DOI 10.1109/42.938245
   ACHIM AM, 2005, P INT C INF FUS JUL, V1, P515
   [Anonymous], 1994, Stable Non-Gaussian Random Processes: Stochastic Models with Infinite Variance
   BERGEN JR, 1991, VISION VISUAL DYSFUN, V10
   Chen JQ, 2005, IEEE T IMAGE PROCESS, V14, P1524, DOI 10.1109/TIP.2005.852204
   Crouse MS, 1998, IEEE T SIGNAL PROCES, V46, P886, DOI 10.1109/78.668544
   Delignon Y, 1997, IEEE T IMAGE PROCESS, V6, P1364, DOI 10.1109/83.624951
   Deng Y., 1999, P IEEE INT S CIRC SY, V4, P21, DOI DOI 10.1109/ISCAS.1999.779933
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Do MN, 2002, IEEE T MULTIMEDIA, V4, P517, DOI 10.1109/TMM.2002.802019
   Gersho A., 1993, VECTOR QUANTIZATION
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Kingsbury N, 2001, APPL COMPUT HARMON A, V10, P234, DOI 10.1006/acha.2000.0343
   LEWIS JJ, 2002, INF FUS, V8, P119
   LI H, 1995, GRAPH MODEL IM PROC, V57, P235, DOI 10.1006/gmip.1995.1022
   Ma XY, 1995, IEEE T SIGNAL PROCES, V43, P2884, DOI 10.1109/78.476432
   MALLAT S, 1992, IEEE T PATTERN ANAL, V14, P710, DOI 10.1109/34.142909
   Nikias C. L., 1995, SIGNAL PROCESSING AL
   Nolan JP, 1998, MULTIVARIATE STABLE
   NOLAN JP, 2005, P DTSCH BUND 2005 AN
   O'Callaghan RJ, 2005, IEEE T IMAGE PROCESS, V14, P49, DOI 10.1109/TIP.2004.838695
   PANJWANI DK, 1995, IEEE T PATTERN ANAL, V17, P939, DOI 10.1109/34.464559
   PETROVIC V, 2000, P 3 INT C IM FUS, V2, P14
   Piella G., 2003, Information Fusion, V4, P259, DOI 10.1016/S1566-2535(03)00046-0
   Piella G, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P173
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Simoncelli EP, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL I, P379
   TZAGKARAKIS G, 2004, P 5 INT WORKSH IM AN
   WAN T, 2007, P INT WORKSH NONL SI
   WAN T, 2007, P IEEE INT C IM PROC, P357
   Wan T, 2007, INT CONF ACOUST SPEE, P1213
   Wang JP, 1998, IEEE T PATTERN ANAL, V20, P619, DOI 10.1109/34.683775
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Yang JZ, 2006, 2006 40TH ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-4, P468, DOI 10.1109/CISS.2006.286513
NR 37
TC 73
Z9 82
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 624
EP 633
DI 10.1109/TMM.2009.2017640
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900005
OA Green Published
DA 2024-07-18
ER

PT J
AU Wu, L
   Hu, Y
   Li, MJ
   Yu, NH
   Hua, XS
AF Wu, Lei
   Hu, Yang
   Li, Mingjing
   Yu, Nenghai
   Hua, Xian-Sheng
TI Scale-Invariant Visual Language Modeling for Object Categorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; content-based image retrieval; image classification;
   visual language model
AB In recent years, "bag-of-words" models, which treat an image as a collection of unordered visual words, have bee widely applied in the multimedia and computer vision fields. However, their ignorance of the spatial structure among visual words makes them indiscriminative for objects with similar word frequencies but different word spatial distributions. In this paper, we propose a visual language modeling method (VLM), which incorporates the spatial context of the local appearance features into the statistical language model. To represent the object categories, models with different orders of statistical dependencies have been exploited. In addition, the multilayer extension to the VLM makes it more resistant to scale variations of objects. The model is effective and applicable to large scale image categorization. We train scale invariant visual language models based on the images which are grouped by Flickr tags, and use these models for object categorization. Experimental results show they achieve better performance than single layer visual language models and "bag-of-words" models. The), also achieve comparable performance with 2-D MHMM and SVM-based methods, while costing much less computational time.
C1 [Wu, Lei; Hu, Yang; Li, Mingjing; Yu, Nenghai] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, MOE Microsoft Key Lab Multimedia Comp & Commun, Hefei 230026, Peoples R China.
   [Hua, Xian-Sheng] Microsoft Res Asia, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft Research Asia; Microsoft
RP Wu, L (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, MOE Microsoft Key Lab Multimedia Comp & Commun, Hefei 230026, Peoples R China.
EM leiwu@live.com; yanghu@ustc.edu; limingjing@ustc.edu; ynh@ustc.edu.cn;
   xshua@microsoft.com
CR AGARWAL A, 2006, P IEEE C EUR C COMP
   [Anonymous], P EUR C COMP VIS ECC
   [Anonymous], 2001, COMP SCI W
   [Anonymous], P EUR C COMP VIS ECC
   [Anonymous], 2005, THESIS J GUTENBERG U
   Bi JB, 2005, PROC CVPR IEEE, P1121
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cao L., 2007, ICCV
   CLARKSON P, 1997, P EUR 97 RHOD GREEC, P2707
   COHEN FS, 1991, IEEE T PATTERN ANAL, V13, P192, DOI 10.1109/34.67648
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Fei-Fei L., 2004, CVPR WORKSH GEN MOD, V106, P178, DOI DOI 10.1109/CVPR.2004.383
   Fergus R, 2005, IEEE I CONF COMP VIS, P1816
   He X., 2004, MULTISCALE CONDITION
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   KATZ S, 1997, IEEE T ACOUST SPEECH, V35, P400
   Lafferty John, 2001, INT C MACH LEARN ICM
   LARLUS D, 2008, P IEEE INT C COMP VI
   Lazebnik S, 2005, IEEE I CONF COMP VIS, P832, DOI 10.1109/ICCV.2005.10
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Marée R, 2005, PROC CVPR IEEE, P34
   Mikolajczyk K., 2002, P INT C COMPUTER VIS, P128
   Murphy K., 2005, Hidden Markov model (HMM) toolbox for Matlab
   OTLUMAN H, 2000, P IEEE C INT S COMP
   Quelhas P, 2005, IEEE I CONF COMP VIS, P883
   Russell B.C., 2006, Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, V2, P1605, DOI DOI 10.1109/CVPR.2006.326
   Savarese S., 2006, P 2006 IEEE COMPUTER, V2, P2033, DOI DOI 10.1109/CVPR.2006.102
   Sivic J, 2005, IEEE I CONF COMP VIS, P370
   Wang B., 2006, P IEEE INT C MULT EX
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Yuan J, 2007, PROC CVPR IEEE, P1930, DOI 10.1109/CVPR.2007.383222
NR 32
TC 34
Z9 42
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 286
EP 294
DI 10.1109/TMM.2008.2009692
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800009
DA 2024-07-18
ER

PT J
AU Wu, X
   Ngo, CW
   Hauptmann, AG
   Tan, HK
AF Wu, Xiao
   Ngo, Chong-Wah
   Hauptmann, Alexander G.
   Tan, Hung-Khoon
TI Real-Time Near-Duplicate Elimination for Web Video Search With Content
   and Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content; context; copy detection; filtering; near-duplicates; novelty
   and redundancy detection; similarity measure; web video
ID RETRIEVAL; QUERY
AB With the exponential growth of social media, there exist huge numbers of near-duplicate web videos, ranging from simple formatting to complex mixture of different editing effects. In addition to the abundant video content, the social web provides rich sets of context information associated with web videos, such as thumbnail image, time duration and so on. At the same time, the popularity of Web 2.0 demands for timely response to user queries. To balance the speed and accuracy aspects, in this paper, we combine the contextual information from time duration, number of views, and thumbnail images with the content analysis derived from color and local points to achieve real-time near-duplicate elimination. The results of 24 popular queries retrieved from You Tube show that the proposed approach integrating content and context can reach real-time novelty re-ranking of web videos with extremely high efficiency, where the majority of duplicates can be rapidly detected and removed from the top rankings. The speedup of the proposed approach can reach 164 times faster than the effective hierarchical method proposed in [31], with just a slight loss of performance.
C1 [Wu, Xiao; Ngo, Chong-Wah; Tan, Hung-Khoon] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Hauptmann, Alexander G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
C3 City University of Hong Kong; Carnegie Mellon University
RP Wu, X (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM wuxiao@cs.cityu.edu.hk; cwngo@cs.cityu.edu.hk; alex@cs.cmu.edu;
   hktan@cs.cityu.edu.hk
OI Ngo, Chong Wah/0000-0003-4182-8261; Wu, Xiao/0000-0002-8322-8558
CR Adjeroh DA, 1999, COMPUT VIS IMAGE UND, V75, P25, DOI 10.1006/cviu.1999.0764
   Allan J., 2002, INTRO TOPIC DETECTIO
   [Anonymous], P 14 ACM INT C MULT
   [Anonymous], P MIR NOV
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   CHANG SF, 2005, P TRECVID WASH DC
   Cheung SCS, 2005, IEEE T MULTIMEDIA, V7, P524, DOI 10.1109/TMM.2005.846906
   Cheung SCS, 2003, IEEE T CIRC SYST VID, V13, P59, DOI 10.1109/TCSVT.2002.808080
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   HAMPAPUR A, 2002, P STOR RETR MED DAT
   Hsu W. H., 2006, MULTIMEDIA '06, P35
   Hsu WinstonH., 2007, ACM MM
   JAIMES A, 2003, THESIS COLUMBIA U NE
   Jain AK, 1999, MULTIMEDIA SYST, V7, P369, DOI 10.1007/s005300050139
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Ke Y., 2004, ACM MULTIMEDIA 04, P869
   Kennedy Lyndon., 2007, P 15 INT C MULTIMEDI, P631
   Law-To J., 2006, P ACM INT C MULTIMED, P835
   Lienhart R, 2000, MULTIMED TOOLS APPL, V10, P47, DOI 10.1023/A:1009663921899
   Liu Jingjing., 2007, MULTIMEDIA 07, P208
   LIU L, 2007, P MULT MOD C JAN
   Liu Xiaoming., 1999, P ACM INT C MULTIMED, P41, DOI [10.1145/319878.319889, DOI 10.1145/319878.319889]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mikolajczyk K, 2003, PROC CVPR IEEE, P257
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Shevade B, 2007, ACM-IEEE J CONF DIG, P127, DOI 10.1145/1255175.1255200
   Wu A. G., 2007, P ACM MM, P218
   Wu X, 2006, IEEE SIGNAL PROC MAG, V23, P59
   Wu X., 2007, ACM MULTIMEDIA 07, P168
   Wu XB, 2007, PROCEEDINGS OF THE 2007 INTERNATIONAL CONFERENCE ON AGRICULTURE ENGINEERING, P162
   YANG Y, 2002, P SIGKDD
   Yuan JS, 2004, LECT NOTES COMPUT SC, V3332, P479
   ZHAI C.X., 2003, Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in informaion Retrieval (Toronto, Canada, July 28 - August 01, 2003), P10, DOI [10.1145/860435.860440, DOI 10.1145/860435.860440]
   ZHANG B, 2005, P 28 ANN INT ACM SIG, P504
   ZHANG D.-Q., 2004, PROC ACM INT C MULTI, P877, DOI DOI 10.1145/1027527.1027730
   ZHANG Y, 2002, P 25 ANN INT ACM SIG, P81
NR 37
TC 94
Z9 111
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 196
EP 207
DI 10.1109/TMM.2008.2009673
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800002
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Hayat, K
   Puech, W
   Gesquière, G
AF Hayat, Khizar
   Puech, William
   Gesquiere, Gilles
TI Scalable 3-D Terrain Visualization Through Reversible JPEG2000-Based
   Blind Data Hiding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D visualization; data hiding; data synchronization; Digital Elevation
   Model (DEM); Discrete Wavelet Transform (DWT); Geographic Information
   System (GIS); JPEG2000
ID SCHEME; IMAGES
AB In this paper a new method is presented for 3-D terrain visualization via reversible JPEC2000-based blind data hiding with special focus on data synchronization and scalability. Online real-time 3-D terrain visualization involves considerable amount of data. The process is essentially the mapping of the aerial photograph, called texture, onto its corresponding digital elevation model (DEM) implying at least two distinct data inputs. The presence of large disparate data necessitates a compression strategy on one hand and the integration of the DEM and texture into one unit on the other. Whilst the compression must accommodate the scalability requirement originated by the diversity of clients, the unification of data ought to be synchronous. For scalability this paper relies on the multiresolution nature of the DWT-based JPEG2000 standard whereas the synchronized unification of DEM with the texture is realized by the application of a perceptually transparent data hiding strategy in the DWT domain. The proposed method is blind in the sense that only a secret key, if any, and the size of the original DEM are needed to extract the data from the texture image. We believe that this is one of the pioneering methods to propose scalable embedding of DEM in the texture image. The method is cost effective, in terms of memory and bandwidths, which is an advantage, especially, in real-time environments when quicker transfer of data is required. The results of a 3-D visualization simulation effected with our method were encouraging and gave a useful insight to the effectiveness of our method in various bandwidth scenarios.
C1 [Hayat, Khizar; Puech, William] Univ Montpellier 2, Lab LIRMM, CNRS, UMR 5506, F-34392 Montpellier 05, France.
   [Gesquiere, Gilles] Univ Aix Marseille, CNRS, UMR 6168, LSIS,IUT, F-13200 Arles, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; Aix-Marseille Universite; Centre National de la Recherche
   Scientifique (CNRS)
RP Hayat, K (corresponding author), Univ Montpellier 2, Lab LIRMM, CNRS, UMR 5506, F-34392 Montpellier 05, France.
EM khizar.hayat@lirmm.fr; william.puech@lirmm.fr; gilles.gesquiere@lsis.org
RI Hayat, Khizar/P-8954-2019; GESQUIERE, Gilles/L-2345-2018
OI Hayat, Khizar/0000-0001-5216-6019; Puech, William/0000-0001-9383-2401;
   GESQUIERE, Gilles/0000-0001-7088-1067
CR [Anonymous], 2008, A wavelet tour of signal processing: The sparse way
   [Anonymous], 2004, 154441 ISOIEC
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   Bender W, 1996, IBM SYST J, V35, P313, DOI 10.1147/sj.353.0313
   Cox I. J., 2002, Digital Watermarking
   DAUBECHIES I, 1998, FOURIER ANAL APPL, V4
   FLORIANI LD, 1995, ACM T GRAPHIC, V14, P363
   FOWLER RJ, 1979, P SIGGRAPH 79, P199
   GERLEK MP, 2004, GEOTIFF BOX SPECIFIC
   HAYAT K, 2007, P SPIE ELECT IMAGING, V6495
   HAYAT K, 2007, P 15 EUR SIGN PROC C, P2519
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Kong XW, 2004, IMAGE VISION COMPUT, V22, P583, DOI 10.1016/j.imavis.2003.09.016
   Kundur D, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P544, DOI 10.1109/ICIP.1997.647970
   Kundur D, 1998, INT CONF ACOUST SPEE, P2969, DOI 10.1109/ICASSP.1998.678149
   KUNDUR D, 1999, P ACM WORKSH MULT SE, P53
   LAKE R, 2005, 05047R2 OGC
   Liu JL, 2006, COMPUT STAND INTER, V28, P356, DOI 10.1016/j.csi.2005.07.001
   Losasso F, 2004, ACM T GRAPHIC, V23, P769, DOI 10.1145/1015706.1015799
   MARTIN A, 2006, P SPIE ELECT IMAGING, V6060
   Meerwald P, 2001, PROC SPIE, V4314, P505, DOI 10.1117/12.435434
   Meerwald P, 2001, INT FED INFO PROC, V64, P69
   Noore A, 2007, FORENSIC SCI INT, V169, P188, DOI 10.1016/j.forsciint.2006.08.019
   Pajarola R, 2007, VISUAL COMPUT, V23, P583, DOI [10.1007/s00371-007-0163-2, 10.1007/S00371-007-0163-2]
   Pennebaker W. B., 1992, JPEG STILL IMAGE DAT
   Piva A, 2005, INT J IMAGE GRAPH, V5, P149, DOI 10.1142/S0219467805001707
   Su PC, 2001, J VLSI SIG PROC SYST, V27, P35, DOI 10.1023/A:1008111228727
   SWELDENS W, 1995, P SOC PHOTO-OPT INS, V2569, P68, DOI 10.1117/12.217619
   WANG HJ, 1998, P IEEE INT C AC SPEE, P3271
   Xia XG, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P548, DOI 10.1109/ICIP.1997.647971
NR 30
TC 7
Z9 7
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1261
EP 1276
DI 10.1109/TMM.2008.2004905
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700004
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Boutemedjet, S
   Ziou, D
AF Boutemedjet, Sabri
   Ziou, Djemel
TI A graphical model for context-aware visual content recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE collaborative filtering; content-based image retrieval; content-based
   image suggestion; context-aware retrieval; diversity ranking; image
   summarizing; mixture models; recommender systems
ID OF-THE-ART; DIRICHLET DISTRIBUTION; IMAGE RETRIEVAL; COLOR
AB Existing recommender systems provide an elegant solution to the information overload in current digital libraries such as the Internet archive. Nowadays, the sensors that capture the user's contextual information such as the location and time are become available and have raised a need to personalize recommendations for each user according to his/her changing needs in different contexts. In addition, visual documents have richer textual and visual information that was not exploited by existing recommender systems. In this paper, we propose a new framework for context-aware recommendation of visual documents by modeling the user needs, the context and also the visual document collection together in a unified model. We address also the user's need for diversified recommendations. Our pilot study showed the merits of our approach in content based image retrieval.
C1 [Boutemedjet, Sabri; Ziou, Djemel] Univ Sherbrooke, Dept Informat, Sherbrooke, PQ J1K 2R1, Canada.
C3 University of Sherbrooke
RP Boutemedjet, S (corresponding author), Univ Sherbrooke, Dept Informat, Sherbrooke, PQ J1K 2R1, Canada.
EM sabri.boutemedjet@usherbrooke.ca; Djemel.ziou@usherbrooke.ca
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   [Anonymous], 1995, Journal of Retailing and Consumer Services, DOI [DOI 10.1016/0969-6989(95)00038-0, 10.1016/0969-6989(95)00038-0]
   BELK RW, 1975, J CONSUM RES, V2, P157, DOI 10.1086/208627
   Billsus D., 1998, Proceedings of the Fifteenth International Conference on Machine Learning', ICML'98, P46
   Bouguila N, 2004, IEEE T IMAGE PROCESS, V13, P1533, DOI 10.1109/TIP.2004.834664
   Bouguila N, 2003, LECT NOTES ARTIF INT, V2734, P172
   Breese J., 1998, P 14 C UNC ART INT, P43
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   CASCIA ML, 1998, P IEEE WORKSH CONT B
   CHEN H, 2006, P 29 INT ACM C RES D
   Cheung KW, 2004, IEEE T SYST MAN CY A, V34, P143, DOI 10.1109/TSMCA.2003.818877
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dai W., 2005, CIKM, P752
   Datcu M, 2003, IEEE T GEOSCI REMOTE, V41, P2923, DOI 10.1109/TGRS.2003.817197
   Graeff TR, 1997, PSYCHOL MARKET, V14, P49, DOI 10.1002/(SICI)1520-6793(199701)14:1<49::AID-MAR4>3.3.CO;2-D
   HIRSCHMAN EC, 1980, J CONSUM RES, V7, P283, DOI 10.1086/208816
   Hofmann T, 2004, ACM T INFORM SYST, V22, P89, DOI 10.1145/963770.963774
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Jain AK, 1996, PATTERN RECOGN, V29, P1233, DOI 10.1016/0031-3203(95)00160-3
   Jin R, 2006, INFORM RETRIEVAL, V9, P357, DOI 10.1007/s10791-006-4651-1
   Jones GJF, 2005, INTERNATIONAL WORKSHOP ON UBIQUITOUS DATA MANAGEMENT, PROCEEDINGS, P53, DOI 10.1109/UDM.2005.5
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kherfi ML, 2006, IEEE T IMAGE PROCESS, V15, P1017, DOI 10.1109/TIP.2005.863969
   Kherfi ML, 2004, INT C PATT RECOG, P961, DOI 10.1109/ICPR.2004.1334418
   Kherfi ML, 2004, ACM COMPUT SURV, V36, P35, DOI 10.1145/1013208.1013210
   Kim CY, 2004, IEEE INTELL SYST, V19, P32
   McLachlan G. J., 1997, The EM Algorithm and Extensions, V473, P486
   Melville P, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P187
   Messaris P., 1997, Visual Persuasion: The Role of Images in Advertising
   Mooney R.J., 2000, Proceedings of the fifth ACM conference on Digital libraries', DL'00, P195
   Oard DW, 1997, USER MODEL USER-ADAP, V7, P141, DOI 10.1023/A:1008287121180
   Pearl J., 2000, MODELS REASONING INF
   POPESCUL A., 2001, P 17 C UNCERTAINTY A, P437
   Resnick P., 1994, P ACM C COMP SUPP CO, P175
   Robertson S, 2000, J DOC, V56, P312, DOI 10.1108/EUM0000000007118
   Si L., 2004, Proc. of the 13th ACM Int. Conf. on Information and Knowledge Management, P156
   Si L., 2003, PROC 20 INT C MACHIN, P704
   Yu K, 2004, IEEE T KNOWL DATA EN, V16, P56, DOI 10.1109/TKDE.2004.1264822
   ZHAI C.X., 2003, Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in informaion Retrieval (Toronto, Canada, July 28 - August 01, 2003), P10, DOI [10.1145/860435.860440, DOI 10.1145/860435.860440]
   ZHANG J, 2004, P ADV NEUR INF PROC, V17, P1617
   ZHANG Y, 2002, P 25 ANN INT ACM SIG, P81
NR 41
TC 35
Z9 38
U1 2
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 52
EP 62
DI 10.1109/TMM.2007.911226
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200006
DA 2024-07-18
ER

PT J
AU Gao, S
   Sun, QB
AF Gao, Sheng
   Sun, Qibin
TI Improving semantic concept detection through optimizing ranking function
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE area under ROC; information retrieval; multimedia database; ROC curve;
   semantic concept detection
ID ROC CURVE
AB In this paper, a kernel-based learning algorithm, KernelRank, is presented for improving the performance of semantic concept detection. By designing a classifier optimizing the receiver operating characteristic (ROC) curve using KernelRank, we provide a generic framework to optimize any differentiable ranking function using effective smoothing functions. KernelRank directly maximizes a 1-D quality measure of ROC, i.e., AUC (Area under the ROC). It exploits the kernel density estimation to model the ranking score distributions and approximate the correct ranking count. The ranking metric is then derived and the learnable parameters are naturally embedded. To address the issues of computation and memory in learning, an efficient implementation is developed based on the gradient descent algorithm. We apply KernelRank with two types of kernel density functions to train the linear discriminant function and the Gaussian mixture model classifiers. From our experiments carried out on the development set for TREC Video Retrieval 2005, we conclude that 1) KernelRank is capable of training any differentiable classifier with various kernels; and 2) the learned ranking function performs better than traditional maximization likelihood or classification error minimization based algorithms in terms of AUC and average precision (AP).
C1 Inst Infocomm Res, Singapore 119613, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R)
RP Gao, S (corresponding author), Inst Infocomm Res, Singapore 119613, Singapore.
EM gaosheng@i2r.a-star.edu.sg; qibin@i2r.a-star.edu.sg
RI Sun, Qibin/Q-5360-2017
OI Sun, Qibin/0000-0001-5715-0497
CR [Anonymous], C M
   [Anonymous], 2003, Journal of machine learning research
   [Anonymous], 1975, SIGNAL DETECTION THE
   [Anonymous], 2003, HP INVENT
   Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2
   BURGES C, P 22 INT C MACH LEAR, P89
   Cortes C, 2004, ADV NEUR IN, V16, P313
   Drummond C., 2004, Work. ROC Anal. AI ROCAI, P19
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Flach P. A., 2003, P 20 INT C MACH LEAR, P194
   Gao S., 2003, P ACM SIGIR, P174
   Gao S, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1489, DOI 10.1109/ICME.2006.262824
   Gao S, 2006, INT C PATT RECOG, P679
   Gaosheng ShengGao., 2004, ICML 04, P329
   HERSCHTAL A, 2004, P 21 INT C MACH LEAR, P385
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Joachims T., 2002, LEARNING CLASSIFY TE
   Joachims Thorsten, 2005, P 22 INT C MACH LEAR, P377, DOI DOI 10.1145/1102351.1102399
   Lehmann EL., 1959, TESTING STAT HYPOTHE
   LING X, 2003, P 20 INT C MACH LEAR, P480
   Mozer MC, 2002, ADV NEUR IN, V14, P1409
   Prati R., 2004, Proccedings of the ECML/PKDD Workshop on Advances in Inductive Rule Learning, P144
   Rakotomamonjy A., 2004, SUPPORT VECTOR MACHI
   RUDIN C, 2006, P 19 ANN C LEARN THE, P589
   Rudin C., 2005, P 18 ANN C LEARN THE, P63
   Yan L., 2003, P INT C MACHINE LEAR, P848
   Yang YM, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P42, DOI 10.1145/312624.312647
NR 27
TC 7
Z9 7
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1430
EP 1442
DI 10.1109/TMM.2007.906597
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400009
DA 2024-07-18
ER

PT J
AU Nguyen, GP
   Worring, M
   Smeulders, AWM
AF Nguyen, Giang P.
   Worring, Marcel
   Smeulders, Arnold W. M.
TI Interactive search by direct manipulation of dissimilarity space
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE active learning; dissimilarity learning; interactive; image search;
   visualization
ID IMAGE RETRIEVAL; RELEVANCE FEEDBACK; DISCOVERY; DISTANCE
AB In this paper, we argue to learn dissimilarity for interactive search in content based image retrieval. In literature, dissimilarity is often learned via the feature space by feature selection, feature weighting or by adjusting the parameters of a function of the features. Other than existing techniques, we use feedback to adjust the dissimilarity space independent of feature space. This has the great advantage that it manipulates dissimilarity directly. To create a dissimilarity space, we use the method proposed by Pekalska and Duin, selecting a set of images called prototypes and computing distances to those prototypes for all images in the collection. After the user gives feedback, we apply active learning with a one-class support vector machine to decide the movement of images such that relevant images stay close together while irrelevant ones are pushed away (the work of Guo et al). The dissimilarity space is then adjusted accordingly. Results on a Corel dataset of 10000 images and a TrecVid collection of 43907 keyframes show that our proposed approach is not only intuitive, it also significantly improves the retrieval performance.
C1 Univ Amsterdam, Inst Informat, Integrated Syst Lab, NL-1098 SJ Amsterdam, Netherlands.
   Aalborg Univ, Media Technol Grp, Aalborg, Denmark.
C3 University of Amsterdam; Aalborg University
RP Nguyen, GP (corresponding author), Univ Amsterdam, Inst Informat, Integrated Syst Lab, NL-1098 SJ Amsterdam, Netherlands.
EM giangnp@science.uva.nl; worring@science.uva.nl; smeulders@science.uva.nl
RI Nguyen, Giang/HZI-3726-2023; Worring, Marcel/JRW-7059-2023
OI Worring, Marcel/0000-0003-4097-4136
CR Bhanu B, 1998, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES - PROCEEDINGS, P14, DOI 10.1109/IVL.1998.694471
   BRUNO E, 2005, P 3 WORKSH AD MULT R
   CARKACIOGLU A, 2002, P INT C IM PROC
   Chen YQ, 2001, IEEE IMAGE PROC, P34, DOI 10.1109/ICIP.2001.958946
   Cox K., 1994, SIGDOC '94. 1994 ACM 12th Annual International Conference on Systems Documentation, P176, DOI 10.1145/192506.192560
   Duin RPW, 1997, PATTERN RECOGN LETT, V18, P1159, DOI 10.1016/S0167-8655(97)00138-4
   El-Naqa I, 2004, IEEE T MED IMAGING, V23, P1233, DOI 10.1109/TMI.2004.834601
   Guo GD, 2002, IEEE T NEURAL NETWOR, V13, P811, DOI 10.1109/TNN.2002.1021882
   He XF, 2003, IEEE T CIRC SYST VID, V13, P39, DOI 10.1109/TCSVT.2002.808087
   Heesch D, 2005, LECT NOTES COMPUT SC, V3568, P609
   Li BT, 2003, MULTIMEDIA SYST, V8, P512, DOI 10.1007/s00530-002-0069-9
   Manevitz LM, 2002, J MACH LEARN RES, V2, P139, DOI 10.1162/15324430260185574
   MOGHADDAM B, 2000, INT J COMPUT VISION, V56, P109
   NGUYEN GP, 2006, ACM T MULTIMEDIA COM
   NGUYEN GP, 2006, J VIS LANG COMPUT
   Nguyen Hieu T., 2004, INT C MACHINE LEARNI, P79
   Pekalska E, 2006, PATTERN RECOGN, V39, P189, DOI 10.1016/j.patcog.2005.06.012
   Pekalska E, 2002, PATTERN RECOGN LETT, V23, P943, DOI 10.1016/S0167-8655(02)00024-7
   RODDEN K, 2001, P SIGCHI C HUM FACT, P190, DOI DOI 10.1145/365024.365097
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   RUMELHART DE, 1985, COGNITIVE SCI, V9, P75, DOI 10.1207/s15516709cog0901_5
   Santini S, 2000, IEEE MULTIMEDIA, V7, P26, DOI 10.1109/93.879766
   Santini S, 1999, IEEE T PATTERN ANAL, V21, P871, DOI 10.1109/34.790428
   Santini S, 2001, IEEE T KNOWL DATA EN, V13, P337, DOI 10.1109/69.929893
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snoek CG, 2005, P ACM MULT
   Squire DM, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P88, DOI 10.1109/ACV.1998.732863
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   VANGEMERT J, 2006, CVPR WORKSH SEM LEAR
   YE H, 2003, P 5 INT C COMP INT M, P131
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
   [No title captured]
   [No title captured]
NR 34
TC 11
Z9 12
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1404
EP 1415
DI 10.1109/TMM.2007.906586
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chakrabarti, A
   Rajagopalan, AN
   Chellappa, R
AF Chakrabarti, Ayan
   Rajagopalan, A. N.
   Chellappa, Rama
TI Super-resolution of face images using kernel PCA-based prior
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE face; higher-order statistics; kernel PCA; principal component analysis
   (PCA); super-resolution
ID HIGH-RESOLUTION IMAGE; COMPONENT ANALYSIS; RECONSTRUCTION; RECOGNITION;
   MODELS; LIMITS; VIDEO
AB We present a learning-based method to super-resolve face images using a kernel principal component analysis-based prior model. A prior probability is formulated based on the energy lying outside the span of principal components identified in a higher-dimensional feature space. This is used to regularize the reconstruction of the high-resolution image. We demonstrate with experiments that including higher-order correlations results in significant improvements.
C1 Indian Inst Technol, Dept Elect Engn, Madras 600036, Tamil Nadu, India.
   Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Madras; University System of Maryland; University of
   Maryland College Park; University System of Maryland; University of
   Maryland College Park
RP Chakrabarti, A (corresponding author), Indian Inst Technol, Dept Elect Engn, Madras 600036, Tamil Nadu, India.
EM ayan@ee.iitm.ac.in; raju@ee.iitm.ac.in; rama@umiacs.umd.edu
RI Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/B-6573-2012; Chakraborty,
   Ayan/HJP-0835-2023
OI Chakrabarti, Ayan/0000-0002-4843-740X; Ambasamudram,
   Rajagopalan/0000-0002-0006-6961
CR Altunbasak Y, 2002, IEEE T CIRC SYST VID, V12, P217, DOI 10.1109/76.999200
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Gunturk BK, 2003, IEEE T IMAGE PROCESS, V12, P597, DOI 10.1109/TIP.2003.811513
   Hardie RC, 1997, IEEE T IMAGE PROCESS, V6, P1621, DOI 10.1109/83.650116
   Kim KI, 2005, IEEE T PATTERN ANAL, V27, P1351, DOI 10.1109/TPAMI.2005.181
   KIM SP, 1990, IEEE T ACOUST SPEECH, V38, P1013, DOI 10.1109/29.56062
   Lin ZC, 2004, IEEE T PATTERN ANAL, V26, P83, DOI 10.1109/TPAMI.2004.1261081
   Liu C, 2001, PROC CVPR IEEE, P192
   Liu CJ, 2004, IEEE T PATTERN ANAL, V26, P572, DOI 10.1109/TPAMI.2004.1273927
   Martinkauppi B, 2002, CGIV'2002: FIRST EUROPEAN CONFERENCE ON COLOUR IN GRAPHICS, IMAGING, AND VISION, CONFERENCE PROCEEDINGS, P380
   NGUYEN N, 2001, IEEE T IMAGE PROCESS, V10, P1187
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   PICKUP LC, 2003, ADV NEURAL INFO P SY
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Schultz RR, 1996, IEEE T IMAGE PROCESS, V5, P996, DOI 10.1109/83.503915
   SEZER OG, 2006, P SOC PHOTO-OPT INS, P52
   Sun J, 2003, PROC CVPR IEEE, P729
NR 20
TC 134
Z9 150
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 888
EP 892
DI 10.1109/TMM.2007.893346
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200018
OA Green Published
DA 2024-07-18
ER

PT J
AU Karande, SS
   Radha, H
AF Karande, Shirish S.
   Radha, Hayder
TI Hybrid erasure-error protocols for wireless video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Communications (ICC 2005)
CY MAY 16-20, 2005
CL Seoul, SOUTH KOREA
SP IEEE Commun Soc, IEEE Comp Soc, TCPP, KT, SAMSUNG, LG Elect, SK Telecom, Cisco
DE channel capacity; channel coding; wireless networks
ID CAPACITY
AB Many recently proposed cross-layer protocols for wireless video, have advocated the relay of corrupted packet to higher layers. Such protocols lead to both errors and erasures at the compressed video application layer. We generically refer to such schemes as hybrid erasure-error protocols (HEEPs). I this paper, we analyze the utility of HEEPs for efficient transmission of video over wireless channels. In order to maintain the generic nature of the deductions in this paper, we base our analysis on two (rather abstract) communication schemes for wireless video: hybrid error-erasure cross-layer design (CLD) and hybrid error-erasure cross-layer design with side-information (CLDS). We make a comparative analysis of the channel capacities of these schemes over single and multi-hop wireless channels to identify the conditions under which the HEEPs can provide improved performance over conventional (CON) protocols. In addition, we employ Reed Solomon (RS) and low-density parity check (LDPC)-code-based forward-error correction (FEC) schemes to illustrate that the improvement in capacity can easily enable an FEC scheme employed in conjunction with a HEEP to provide improved throughput. Finally we compare the performance of CON, CLD, and CLDS in terms of video quality using the H.264 video standard. The simulation results show a significant advantage for the HEEPs.
C1 Michigan State Univ, Dept Elect & Comp Engn ECE, E Lansing, MI 48824 USA.
C3 Michigan State University
RP Karande, SS (corresponding author), Michigan State Univ, Dept Elect & Comp Engn ECE, E Lansing, MI 48824 USA.
EM karandes@egr.msu.edu; radha@egr.msu.edu
CR [Anonymous], 2003, 51 ALL C COMM CONTR
   Bolot JC, 1999, IEEE INFOCOM SER, P1453, DOI 10.1109/INFCOM.1999.752166
   Cover T. M., 1991, SER WILEY SERIES TEL
   El-Khamy M, 2006, IEEE J SEL AREA COMM, V24, P481, DOI 10.1109/JSAC.2005.862399
   Goldsmith AJ, 1996, IEEE T INFORM THEORY, V42, P868, DOI 10.1109/18.490551
   Hu XY, 2005, IEEE T INFORM THEORY, V51, P386, DOI 10.1109/TIT.2004.839541
   *ISO IEC JTC, 2003, 1SC29WG11 ISO IEC JT
   KARANDE SS, 2003, P IEEE INT C MULT EX
   Khayam SA, 2003, SIGNAL PROCESS-IMAGE, V18, P575, DOI 10.1016/S0923-5965(03)00053-5
   LARZON LA, 1999, P IEEE INT C COMM IC
   MEROLOSZARAGOZA RH, 2006, ART ERROR CORRECTING
   MUSHKIN M, 1989, IEEE T INFORM THEORY, V35, P1277, DOI 10.1109/18.45284
   RIZZO L, 1997, COMPUT COMMUN REV, V24, P24
   RYAN W., 2004, CRC HDB CODING SIGNA
   SINGH A, 2001, P NOSSDAV
   Vanstone S.A., INTRO ERROR CORRECTI
   WU M, 2005, SIGNAL PROCESS IMAGE, V20
   Zheng HT, 2001, IEEE T MULTIMEDIA, V3, P356, DOI 10.1109/6046.944478
   2001, IEEE T INFORM THEORY, V47
NR 19
TC 10
Z9 12
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 307
EP 319
DI 10.1109/TMM.2006.886280
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900010
DA 2024-07-18
ER

PT J
AU Wang, PC
   Chan, CT
   Lee, CL
   Chang, HY
AF Wang, Pi-Chung
   Chan, Chia-Tai
   Lee, Chun-Liang
   Chang, Hung-Yi
TI Scalable packet classification for enabling Internet differentiated
   services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE best matching prefix; multicast; multidimensional range lookup; packet
   classification
ID LOOKUP
AB Nowadays, IP networks are rapidly evolving toward a QoS-enabled infrastructure. The need for packet classification is increasing in accordance with emerging differentiated services. While the new differentiated services could significantly increase the number of rules, it has been demonstrated that performing packet classification on a potentially large number of rules is difficult and has poor worst-case performance. In this work, we present an enhanced tuple pruning search algorithm called "Tuple Pruning Plus" (TPP) for packet classification, which outperforms the existing schemes on the scalability. Our main idea is to simplify the lookup procedure and to avoid unnecessary tuple probing by maintaining the least-cost property of rule through precomputation and the proposed Information Marker. With extra rules added for In,formation Marker, only one tuple access is required in each packet classification. In our experiments, 70 MB DRAM is used to achieve 50 million packets per second (MPPS) for a 1 M-rule set, showing a performance improvement by a factor of 50. We also present a heuristic to further reduce the required storage to about 20 MB. These results demonstrate the effectiveness of the TPP scheme to achieve high speed packet classification.
C1 Natl Chung Hsing Univ, Dept Comp Sci, Taichung 40227, Taiwan.
   Natl Yang Ming Univ, Inst Biomed Engn, Taipei 112, Taiwan.
   Chang Gung Univ, Dept Comp Sci & Informat Engn, Tao Yuan 333, Taiwan.
   Natl Kaohsiung First Univ Sci & Technol, Dept Informat Management, Kaohsiung 811, Taiwan.
C3 National Chung Hsing University; National Yang Ming Chiao Tung
   University; Chang Gung University; National Kaohsiung University of
   Science & Technology
RP Chan, CT (corresponding author), Natl Chung Hsing Univ, Dept Comp Sci, Taichung 40227, Taiwan.
EM pcwang@cs.nchu.edu.tw; ctchan@ym.edu.tw; leecl@csie.nctu.edu.tw;
   leorean@ccms.nkfust.edu.tw
RI Wang, Pi-Chung/AAQ-3600-2020
OI Wang, Pi-Chung/0000-0002-4220-2853
CR [Anonymous], 2001, 3031 RFC
   [Anonymous], 1998, 2430 RFC
   [Anonymous], 1998, An Architecture for Differentiated Services, RFC 2475 Informational
   Baboescu F, 2005, IEEE ACM T NETWORK, V13, P2, DOI 10.1109/TNET.2004.842232
   Baboescu F, 2003, IEEE INFOCOM SER, P53
   BOYLE J, 1997, RSVP EXTENSIONS CIDR
   BUDDHIKOT M, 2000, IFIP 6 INT WORKSH HI, P25
   Estrin D, 1997, 2117 RFC
   Feldman A., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1193, DOI 10.1109/INFCOM.2000.832493
   Gallardo JR, 2001, IEEE T MULTIMEDIA, V3, P177, DOI 10.1109/6046.923817
   Gupta P, 2001, IEEE NETWORK, V15, P2
   Gupta P, 1999, COMP COMM R, V29, P147, DOI 10.1145/316194.316217
   Gupta S. K., 2000, Agricultural Science Digest, V20, P1
   Kumar VP, 1998, IEEE COMMUN MAG, V36, P152, DOI 10.1109/35.668286
   Lakshman T. V., 1998, Computer Communication Review, V28, P203, DOI 10.1145/285243.285283
   Lin YD, 2001, COMPUT COMMUN, V24, P667, DOI 10.1016/S0140-3664(00)00305-4
   *MER NETW INC, IMPA PROJ
   *NAT LAB APPL NETW, NLANR PROJ
   Singh S., 2003, ACM SIGCOMM COMP COM, P213, DOI DOI 10.1145/863955.863980
   Srinivasan V, 1999, ACM T COMPUT SYST, V17, P1, DOI 10.1145/296502.296503
   Srinivasan V, 1999, COMP COMM R, V29, P135, DOI 10.1145/316194.316216
   SRINIVASAN V, 1998, ACM SIGCOMM, P191
   WAITZMAN D, 1993, 1075 RFC
   Wang PC, 2004, IEEE T MULTIMEDIA, V6, P925, DOI 10.1109/TMM.2004.837263
   Wang PC, 2001, IEEE COMMUN LETT, V5, P125, DOI 10.1109/4234.913161
   Woo T. Y. C., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1213, DOI 10.1109/INFCOM.2000.832499
   [No title captured]
NR 27
TC 9
Z9 9
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1239
EP 1249
DI 10.1109/TMM.2006.884610
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700013
DA 2024-07-18
ER

PT J
AU Chen, MJ
   Li, GL
   Chiang, YY
   Hsu, CT
AF Chen, Mei-Juan
   Li, Gwo-Long
   Chiang, Yi-Yen
   Hsu, Ching-Ting
TI Fast multiframe motion estimation algorithms by motion vector
   composition for the MPEG-4/AVC/H.264 standard
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE h.264; motion estimation; motion vector; multiple reference frame; video
   coding
AB The MPEG-4/AVC/H.264 video coding standard adopts various coding schemes such as multiple reference frames and variable block sizes for motion estimation. Hence, MPEG-4/AVC/H.264 provides gains in compression efficiency of up to 50% over a wide range of bit rates and video resolutions compared to previous standards. However, these features result in a considerable increase in encoder complexity, mainly regarding to mode decision and motion estimation. The proposed algorithms use the stored motion vectors to compose the motion vector without performing the full search in each reference frame. Therefore, the proposed algorithms can obtain an average speed up ratio of four for encoding, thus benefiting from the prediction of the motion vector for the reference frames in advance and maintaining good performance. Any fast search algorithm can be utilized to further largely reduce the computational load.
C1 Natl Dong Hwa Univ, Dept Elect Engn, Hualien 974, Taiwan.
C3 National Dong Hwa University
RP Chen, MJ (corresponding author), Natl Dong Hwa Univ, Dept Elect Engn, Hualien 974, Taiwan.
EM cmj@mail.ndhu.edu.tw; m9323004@em93.ndhu.edu.tw;
   iy_chiang@ms92.url.com.tw; d9223001@em92.ndhu.edu.tw
OI Chen, Mei-Juan/0000-0003-3382-8296
CR Al-Mualla M.E., 2002, VIDEO CODING MOBILE
   Chang A, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P704
   Chen MJ, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 3, PROCEEDINGS, P737
   Chen MJ, 2002, IEEE T CIRC SYST VID, V12, P269, DOI 10.1109/76.999204
   Hsu CT, 2004, IEICE T COMMUN, VE87B, P3827
   HUANG YW, 2003, IEEE INT C AC SPEECH, V3, P145
   *JVT ISO IEC MPEG, 2003, H26411ISOIEC ITUT
   Li X, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P369
   Lin SF, 2005, IEEE INT SYMP CIRC S, P1505
   Sadka AbdulH., 2002, COMPRESSED VIDEO COM
   Tamhankar A, 2003, PROCEEDINGS EC-VIP-MC 2003, VOLS 1 AND 2, P1
   Ting CW, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS & SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P1258
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   YOUN J, 1999, P IEEE INT S CIRC SY, V4, P243
   Youngs Gillian., 1999, International Feminist Journal of Politics, V1, P1
NR 15
TC 39
Z9 43
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 478
EP 487
DI 10.1109/TMM.2006.870739
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000005
DA 2024-07-18
ER

PT J
AU Mao, SW
   Bushmitch, D
   Narayanan, S
   Panwar, SS
AF Mao, SW
   Bushmitch, D
   Narayanan, S
   Panwar, SS
TI MRTP: A multiflow real-time transport protocol for ad hoc networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE ad hoc networks; multipath transport; real-time transport protocol;
   traffic partitioning; video communications
ID ERROR CONTROL; VIDEO
AB Real-time multimedia transport has stringent quality of service requirements, which are generally not supported by current network architectures. In emerging mobile ad hoc networks, frequent topology changes and link failures cause severe packet losses, which degrade the quality of received media. However, in such mesh networks, there usually exist multiple paths between any source and destination nodes. Such path diversity has been demonstrated to be effective in combating congestion and link failures for improved media quality. In this paper, we present a new protocol to facilitate multipath transport of real-time multimedia data. The proposed protocol, the multiflow real-time transport protocol (MRTP), provides a convenient vehicle for real-time applications to partition and transmit data using multiple flows. We demonstrate through analysis that data partitioning, which is an essential function of MRTP, can effectively reduce the short-range dependence of multimedia data, thus improving its queueing performance in underlying networks. Furthermore, we show that a few flows are sufficient for MRTP to exploit most of the benefits of multipath transport. Finally, we present a comprehensive simulation study on the performance of MRTP under a mobile ad hoc network. We show that with one additional path, MRTP outperformed single-flow RTP by a significant margin.
C1 Virginia Polytech Inst & State Univ, Dept Elect & Comp Engn, Blacksburg, VA 24061 USA.
   Panason Technol Inc, PDNL, Princeton, NJ 08540 USA.
   Polytech Univ, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
C3 Virginia Polytechnic Institute & State University; New York University
RP Virginia Polytech Inst & State Univ, Dept Elect & Comp Engn, Blacksburg, VA 24061 USA.
EM smao@ieee.org; db@research.panasonic.com; sathya@research.panasonic.com;
   panwar@catt.poly.edu
RI Panwar, Shivendra/K-6473-2019; Mao, Shiwen/AAY-4471-2020; Panwar,
   Shivendra S/A-6884-2016
OI Panwar, Shivendra S/0000-0002-9822-6838
CR [Anonymous], 2001, P 9 ACM INT C MULTIM, DOI DOI 10.1145/500141.500205
   [Anonymous], 2000, 2960 RFC, DOI DOI 10.17487/RFC2960
   Apostolopoulos J, 2002, IEEE INFOCOM SER, P1736, DOI 10.1109/INFCOM.2002.1019427
   Banerjea A., 1996, Computer Communication Review, V26, P194, DOI 10.1145/248157.248174
   Basagni Stefano., 1998, Proceedings of the 4th annual ACM/IEEE international conference on Mobile computing and networking, MobiCom '98, P76
   Begen AC, 2005, SIGNAL PROCESS-IMAGE, V20, P39, DOI 10.1016/j.image.2004.09.002
   BERAN J, 1995, IEEE T COMMUN, V43, P1566, DOI 10.1109/26.380206
   Beran J., 1994, STAT LONG MEMORY PRO
   Broch J., 1998, MobiCom'98. Proceedings of Fourth Annual ACM/IEEE International Conference on Mobile Computing and Networking, P85, DOI 10.1145/288235.288256
   BUSHMITCH D, 2004, THESIS POLYTECHNIC U
   CHAKARESKI J, 2003, P 11 ACM INT C MULT, P422
   CHANDRAMOULI R, 2003, IEEE J SEL AREAS COM, V21
   Choe J, 1998, IEEE ACM T NETWORK, V6, P659, DOI 10.1109/90.731205
   Erramilli A, 1996, IEEE ACM T NETWORK, V4, P209, DOI 10.1109/90.491008
   Girod B, 1999, P IEEE, V87, P1707, DOI 10.1109/5.790632
   GIROD B, 2005, IEEE WIRELESS CO JUN
   Gogate N, 2002, IEEE T CIRC SYST VID, V12, P777, DOI 10.1109/TCSVT.2002.803229
   GOGATE N, 1994, IEEE INFOCOM SER, P40, DOI 10.1109/INFCOM.1994.337634
   GOGATE N, 1999, P INT C COMM JUN, P1701
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Gustafsson E, 1997, IEEE NETWORK, V11, P28, DOI 10.1109/65.580915
   Hsieh H.-Y., 2003, MOBICOM 03, P1
   Hsieh H.-Y., 2002, Proceedings of the 8th annual international conference on Mobile computing and networking, P83, DOI [DOI 10.1145/570645.570656, 10.1145/570645.570656]
   Huitema C., 1998, IPv6: The New Internet Protocol
   Johnson D.B., 2003, The Dynamic Source Routing Protocol for Mobile Ad Hoc Networks (DSR)
   KOENEN R, 2000, 14496 ISOIEC
   Lee JYB, 2002, IEEE T MULTIMEDIA, V4, P423, DOI 10.1109/TMM.2002.806533
   Li J., 2001, P 7 ANN INT C MOB CO, P61, DOI DOI 10.1145/381677.381684
   LI Y, 2005, P 2005 IEEE INT C CO
   Liebeherr J, 2003, LECT NOTES COMPUT SC, V2816, P242
   Ma Z, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1330
   MAO S, 2005, P IEEE ICC SEOUL KOR
   Mao SW, 2006, MOBILE NETW APPL, V11, P63, DOI 10.1007/s11036-005-4461-5
   Mao SW, 2005, IEEE INFOCOM SER, P2325
   Mao SW, 2005, IEEE INFOCOM SER, P740
   Mao SW, 2003, IEEE J SEL AREA COMM, V21, P1721, DOI 10.1109/JSAC.2003.815965
   Martinez JM, 2003, JTC1SC29WG11 ISOIEC
   MAXEMCHUK NF, 1993, COMPUT NETWORKS ISDN, V25, P645, DOI 10.1016/0169-7552(93)90059-D
   MIU A, 2003, P IEEE ICME BALT MD
   Montgomery M, 1996, IEEE INFOCOM SER, P513, DOI 10.1109/INFCOM.1996.493343
   NARAYANAN S, 2004, UNPUB MRTP MULTIFLOW
   Nasipuri A., 1999, Proceedings Eight International Conference on Computer Communications and Networks (Cat. No.99EX370), P64, DOI 10.1109/ICCCN.1999.805497
   Nebat Y, 2002, IEEE INFOCOM SER, P1326, DOI 10.1109/INFCOM.2002.1019383
   Nguyen T, 2004, IEEE T MULTIMEDIA, V6, P315, DOI 10.1109/TMM.2003.822790
   Nguyen T, 2003, IEEE INFOCOM SER, P663
   Padmanabhan V.N., 2002, P 12 INT WORKSHOP NE, P177
   Park VD, 1997, IEEE INFOCOM SER, P1405, DOI 10.1109/INFCOM.1997.631180
   Phatak DS, 2002, IEEE INFOCOM SER, P773, DOI 10.1109/INFCOM.2002.1019323
   REISSLEIN M, 1999, P IEEE INT C MULT CO, P7
   Rivest R., 1992, Tech. Rep.
   ROSENBERG J, 2002, SIP SESS INT PROT JU
   Schulzrinne H., 2003, RTP TRANSPORT PROTOC
   Stevens W, 1994, TCP/ IP Illustrated, V1
   Wang Y, 2002, IEEE T CIRC SYST VID, V12, P438, DOI 10.1109/TCSVT.2002.800320
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Xu XF, 2004, IEEE IMAGE PROC, P1759
   ZHU W, 2005, P IEEE SPEC ISS ADV, V93
NR 57
TC 45
Z9 56
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 356
EP 369
DI 10.1109/TMM.2005.864347
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300015
DA 2024-07-18
ER

PT J
AU Elsayed, KMF
AF Elsayed, KMF
TI A framework for end-to-end deterministic-delay service provisioning in
   multiservice packet networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE call admission control; deterministic delay bounds; quality of service
   routing; rate-controlled earliest-deadline first scheduling; resource
   reservation
ID ADMISSION CONTROL; QUALITY
AB The problem of providing end-to-end delay guarantees for deterministic-delay services in multiservice packet networks is addressed through a combination of dynamic resource reservation and routing. Our model is based on using rate-controlled earliest-deadline-first (RC-EDF) for providing hard bounds on end-to-end delays. With RC-EDF, a certain delay bound has to be allocated for a connection at each node in the selected path. The most commonly used resource reservation policy is uniform allocation which is based on dividing the end-to-end delay bound equally among the nodes in the selected path. This simple allocation policy could lead to nonuniform resource loading and subsequently lead to high blocking rates. Moreover, the most commonly used routing method is shortest-path first routing which is known to lead to network hotspots. We propose a set of dynamic nonuniform resource reservation policies and dynamic routing methods. One of the routing methods is the well-known widest-shortest path method and the other is a dynamic routing method that adaptively adjusts link costs and uses a similar algorithm to shortest-path routing (e.g., Dijkstra's algorithm). We show that for both uniform and nonuniform traffic loading of some example network topologies that the combination of the proposed resource reservation policies and dynamic routing can lead to significant reduction in the connection blocking ratio in all loading conditions except for excessively high loads.
C1 Cairo Univ, Dept Elect & Commun Engn, Cairo 12613, Egypt.
C3 Egyptian Knowledge Bank (EKB); Cairo University
RP Elsayed, KMF (corresponding author), Cairo Univ, Dept Elect & Commun Engn, Cairo 12613, Egypt.
EM khaled@ieee.org
RI Elsayed, Khaled/ABD-3460-2021
OI Elsayed, Khaled/0000-0002-3942-8042
CR ANDREWS M, 2000, P IEEE INFOCOM 2000
   [Anonymous], 1997, 2205 RFC
   [Anonymous], 1992, Data networks
   Ayad AS, 2001, IEEE SYMP COMP COMMU, P249, DOI 10.1109/ISCC.2001.935383
   Blake S., 1998, 2475 IETF RFC
   Braden R., 1994, INTEGRATED SERVICES
   Chen SG, 1998, IEEE NETWORK, V12, P64, DOI 10.1109/65.752646
   CHEN TM, 1989, IEEE J SEL AREA COMM, V7, P632, DOI 10.1109/49.32327
   Elsayed KMF, 2002, COMPUT COMMUN, V25, P1513, DOI 10.1016/S0140-3664(02)00058-0
   FERRARI D, 1990, IEEE J SEL AREA COMM, V8, P368, DOI 10.1109/49.53013
   Firoiu V, 1996, IEEE INFOCOM SER, P94, DOI 10.1109/INFCOM.1996.497882
   Firoiu V, 1998, IEEE ACM T NETWORK, V6, P558, DOI 10.1109/90.731190
   Fishwick P., SIMPACK TOOLKIT
   Fortz B, 2002, IEEE J SEL AREA COMM, V20, P756, DOI 10.1109/JSAC.2002.1003042
   Georgiadis L, 1996, IEEE ACM T NETWORK, V4, P482, DOI 10.1109/90.532860
   GOLESTANI SJ, 1991, IEEE J SEL AREA COMM, V9, P1064, DOI 10.1109/49.103553
   Li CZ, 2002, IEEE ACM T NETWORK, V10, P776, DOI 10.1109/TNET.2002.805024
   Liebeherr J, 1996, IEEE ACM T NETWORK, V4, P885, DOI 10.1109/90.556345
   Lorenz DH, 2002, IEEE ACM T NETWORK, V10, P102, DOI 10.1109/90.986559
   MA Q, 1997, P 4 INT IFIP WORKHSH
   NAGARAJAN R, 1993, P IFIP WORKSH PERF A
   PAREKH AK, 1994, IEEE ACM T NETWORK, V2, P137, DOI 10.1109/90.298432
   Stoica Ion, 1999, P ACM SIGCOMM 99 CAM
   Wang Z, 1996, IEEE J SEL AREA COMM, V14, P1228, DOI 10.1109/49.536364
NR 24
TC 14
Z9 14
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 563
EP 571
DI 10.1109/TMM.2005.846779
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200018
DA 2024-07-18
ER

PT J
AU Belfiore, S
   Grangetto, M
   Magli, E
   Olmo, G
AF Belfiore, S
   Grangetto, M
   Magli, E
   Olmo, G
TI Concealment of whole-frame losses for wireless low bit-rate video based
   on multiframe optical flow estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE optical flow estimation; packet-based video transmission; video error
   concealment; whole-frame losses; wireless communications
ID ERROR-CONCEALMENT; MODE SELECTION; TRANSMISSION; NETWORKS;
   INTERPOLATION; ALGORITHM; SEQUENCES; IMAGES
AB In low bit-rate packet-based video communications, video frames may have very small size, so that each frame fills the payload of a single network packet; thus, packet losses correspond to whole-frame losses, to which the existing error concealment algorithms are badly suited and generally not applicable. In this paper, we deal with the problem of concealment of whole frame-losses, and propose a novel technique which is capable of handling this very critical case. The proposed technique presents other two major innovations with respect to the state-of-the-art: i) it is based on optical How estimation applied to error concealment and ii) it performs multiframe estimation, thus optimally exploiting the multiple reference frame buffer featured by the most modern video coders such as H.263+ and H.264. If data partitioning is employed, by e.g., sending headers, motion vectors, and coding modes in prioritized packets as can be done in the DiffServ network model, the algorithm is capable of exploiting the motion vectors to improve the error concealment results. The algorithm has been embedded in the H.264 test model software, and tested under both independent and correlated packet loss models with parameters typical of the wireless environment. Results show that the proposed algorithm significantly outperforms other techniques by several dBs in peak signal-to-noise ratio (PSNR), provides good visual quality, and has a rather low complexity, which makes it possible to perform real-time operation with reasonable computational resources.
C1 Politecn Torino, Dipartimento Elettr, CERCOM, Ctr Multimedia Radio Commun, I-10129 Turin, Italy.
C3 Polytechnic University of Turin
RP Politecn Torino, Dipartimento Elettr, CERCOM, Ctr Multimedia Radio Commun, I-10129 Turin, Italy.
EM belfiore@mail.tlc.polito.it; grangetto@polito.it; magli@polito.it;
   olmo@polito.it
RI Grangetto, Marco/AFM-8024-2022; Olmo, Gabriella/AAB-4987-2021
OI Grangetto, Marco/0000-0002-2709-7864; OLMO,
   Gabriella/0000-0002-3670-9412
CR Alkachouh Z, 2000, IEEE T IMAGE PROCESS, V9, P729, DOI 10.1109/83.841948
   [Anonymous], 1981, P 7 INT JOINT C ART
   *ANSI IEEE, 802111999 ANSI IEEE
   APOSTOLOPOULOS JG, 2000, P ICIP IEEE INT C IM
   Atzori L, 2001, IEEE T MULTIMEDIA, V3, P326, DOI 10.1109/6046.944476
   Atzori L, 1999, SIGNAL PROCESS-IMAGE, V15, P57, DOI 10.1016/S0923-5965(99)00024-7
   Belfiore S, 2003, SIGNAL PROCESS-IMAGE, V18, P907, DOI 10.1016/j.image.2003.08.008
   BELFIORE S, 2002, P ICASSP IEEE INT C
   BELFIORE S, 2003, P ICIP IEEE INT C IM
   BELFIORE S, 2002, P MMSP IEEE INT WORK
   Carpenter BE, 2002, P IEEE, V90, P1479, DOI 10.1109/JPROC.2002.802000
   Crow BP, 1997, IEEE COMMUN MAG, V35, P116, DOI 10.1109/35.620533
   GNAVI S, 2003, P ICME IEEE INT C MU
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   HORN BKP, 1984, ARTIF INTELL, V23, P309
   HUANG TS, 1979, IEEE T ACOUST SPEECH, V27, P13, DOI 10.1109/TASSP.1979.1163188
   *IEEE STD, 80211B1999 IEEE STD
   *ISO IEC MPEG ITU, 2002, JOINT COMM DRAFT JCD
   *ITU T VCEG M77, 2001, VCEG SG16 Q6 14 M SA
   KALMAN M, 2002, P ICME IEEE INT C MU
   LAM WM, 1995, IEEE T IMAGE PROCESS, V4, P533, DOI 10.1109/83.382489
   Lee YC, 2002, IEEE T IMAGE PROCESS, V11, P1314, DOI 10.1109/TIR2002.804275
   Li HZ, 2001, IEEE T CIRC SYST VID, V11, P1183, DOI 10.1109/76.964785
   Li X, 2002, IEEE T CIRC SYST VID, V12, P857, DOI 10.1109/TCSVT.2002.804882
   LU J, 2000, P SPIE IM VID COMM P
   Majumdar A, 2002, IEEE T CIRC SYST VID, V12, P524, DOI 10.1109/TCSVT.2002.800315
   NAGEL HH, 1986, IEEE T PATTERN ANAL, V8, P565, DOI 10.1109/TPAMI.1986.4767833
   Pitas I., 1990, NONLINEAR DIGITAL FI
   Salama P, 2000, IEEE J SEL AREA COMM, V18, P1129, DOI 10.1109/49.848263
   Schultz RR, 1996, IEEE T IMAGE PROCESS, V5, P996, DOI 10.1109/83.503915
   Shirani S, 2000, IEEE J SEL AREA COMM, V18, P1122, DOI 10.1109/49.848261
   SUN HF, 1995, IEEE T IMAGE PROCESS, V4, P470, DOI 10.1109/83.370675
   Tekalp A.M., 1995, DIGITAL VIDEO PROCES
   Turaga DS, 2002, IEEE T CIRC SYST VID, V12, P483, DOI 10.1109/TCSVT.2002.800318
   Wiegand T, 2000, IEEE J SEL AREA COMM, V18, P1050, DOI 10.1109/49.848255
   Wu XL, 2001, IEEE T MULTIMEDIA, V3, P132, DOI 10.1109/6046.909600
   Yu GS, 1998, IEEE T CIRC SYST VID, V8, P422, DOI 10.1109/76.709409
NR 37
TC 63
Z9 87
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 316
EP 329
DI 10.1109/TMM.2005.843347
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400013
DA 2024-07-18
ER

PT J
AU Kuzmanov, G
   Vassiliadis, S
   van Eijndhoven, JTJ
AF Kuzmanov, G
   Vassiliadis, S
   van Eijndhoven, JTJ
TI Hardwired MPEG-4 repetitive padding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE arithmetic-logical-unit (ALU) augmentation; field-programmable gate
   array (FPGA); hardwired repetitive padding; MPEG-4; systolic structure
AB We consider two hardwired solutions for repetitive padding, a performance restricting algorithm for real time MPEG-4 execution. The first solution regards application specific implementations, the second regards general purpose processing. For the application specific implementations we propose a systolic array structure. To determine the chip area and speed, we have synthesized its VHDL models for two field-programmable gate array families-Xilinx and Altera. Depending on the implemented configuration, the unit can process between 77 K and 950 K macroblocks per second (MB/s) when mapped on FPGA chips containing less than 10 K logical gates and frequency capabilities below 100 MHz. The second approach regards an augmentation of a general-purpose arithmetic logical units with an extra functionality added to perform repetitive padding. At trivial hardware costs of a few hundred 2 x 2 AND-OR logic gates, we achieve an order of magnitude speed-up compared to nonaugmented general purpose processor padding. The proposed hardware solutions meet the requirements of all MPEG-4 visual profile levels. Both approaches have been proven to be scalable and fit into different architectural concepts and operand widths.
C1 Delft Univ Technol, EEMCS, Comp Engn Lab, NL-2600 GA Delft, Netherlands.
   PHILIPS Res, Dept Informat & Software Technol, Eindhoven, Netherlands.
C3 Delft University of Technology; Philips; Philips Research
RP Kuzmanov, G (corresponding author), Delft Univ Technol, EEMCS, Comp Engn Lab, NL-2600 GA Delft, Netherlands.
EM Kuzmanov@EWI.TUDelft.NL; S.Vassiliadis@EWI.TUDelft.NL;
   jos.van.eijndhoven@philips.com
CR Berekovic M, 1999, J VLSI SIG PROC SYST, V23, P27, DOI 10.1023/A:1008188618930
   Chang HC, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL II, P449, DOI 10.1109/ISCAS.2000.856361
   CHANG HK, 1994, GENE THER, V1, P208
   Edirisinghe EA, 2000, IEEE T CONSUM ELECTR, V46, P514, DOI 10.1109/30.883404
   HEER C, 1999, P IS T SPIE C MED PR, V3655, P113
   *ISO IEC, 1999, 144962 ISOIEC 2
   *ISO IEC JTC11 SC2, 2001, NEW MPEG 4 PROF CONS
   *ISO IEC JTC11 SC2, N3312 ISOIEC JTC11SC
   *ISO IEC JTC11 SC2, 2001, N4030 ISOIEC JTC11SC
   Kneip J, 1998, 1998 IEEE WORKSHOP ON SIGNAL PROCESSING SYSTEMS-SIPS 98, P43, DOI 10.1109/SIPS.1998.715767
   Moon JH, 1999, IEEE T CIRC SYST VID, V9, P35, DOI 10.1109/76.744273
   PUTRINO M, 1989, INT J ELECTRON, V66, P163, DOI 10.1080/00207218908925372
   PUTRINO M, 1988, INT J ELECTRON, V65, P139, DOI 10.1080/00207218808945212
   Shi Y.Q., 2000, IMAGE PROC SER
   VASSILIADIS S, 2001, P 11 INT C FIELD PRO
   VASSILIADIS S, 2001, P 1K INT C SAER 2001
NR 16
TC 0
Z9 0
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 261
EP 268
DI 10.1109/TMM.2005.843365
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400008
OA Green Published
DA 2024-07-18
ER

PT J
AU Almeida, JM
   Eager, DL
   Vernon, MK
   Wright, SJ
AF Almeida, JM
   Eager, DL
   Vernon, MK
   Wright, SJ
TI Minimizing delivery cost in scalable streaming content distribution
   systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content distribution systems; modeling; streaming media
ID COMPLEXITY
AB Recent scalable multicast streaming protocols for on-demand delivery of media content offer the promise of greatly reduced server and network bandwidth. However, a key unresolved issue is how to design scalable content distribution systems that place replica servers closer to various client populations and route client requests and response streams so as to minimize the total server and network delivery cost. This issue is significantly more complex than the design of distribution systems for traditional Web files or unicast on-demand streaming, for two reasons. First, closest server and shortest path routing does not minimize network bandwidth usage; instead, the optimal routing of client requests and server multicasts is complex and interdependent. Second, the server bandwidth usage increases with the number of replicas. Nevertheless, this paper shows that the complex replica placement and routing optimization problem, in its essential form, can be expressed fairly simply, and can be solved for example client populations and realistic network topologies. The solutions show that the optimal scalable system can differ significantly from the optimal system for conventional delivery. Furthermore, simple canonical networks are analyzed to develop insights into effective heuristics for near-optimal placement and routing. The proposed new heuristics can be used for designing large and heterogeneous systems that are of practical interest. For a number of example networks, the best heuristics produce systems with total delivery cost that is within 16% of optimality.
C1 Univ Fed Minas Gerais, BR-30160010 Belo Horizonte, MG, Brazil.
   Univ Saskatchewan, Saskatoon, SK S7N 5A9, Canada.
   Univ Wisconsin, Madison, WI 53706 USA.
C3 Universidade Federal de Minas Gerais; University of Saskatchewan;
   University of Wisconsin System; University of Wisconsin Madison
RP Univ Fed Minas Gerais, BR-30160010 Belo Horizonte, MG, Brazil.
EM jussara@dcc.ufmg.br; eager@cs.usask.ca; vernon@cs.wisc.edu;
   swright@cs.wisc.edu
RI Almeida, Jussara M/AAD-4947-2022
CR ALMEIDA J, 2001, P NOSSDAV PORT JEFF
   ALMEIDA JM, 2002, P INFOCOM NEW YORK J
   ALMEIDA JM, 2003, THESIS U WISCONSIN M
   [Anonymous], 1979, COMPUT INTRACTABILIT
   APOSTOLOPOULOS J, 2002, P INFOCOM NEW YORK J
   Barbehenn M, 1998, IEEE T COMPUT, V47, P263, DOI 10.1109/12.663776
   BARNOY A, 2001, P ACM SIAM S DISCR A
   BROIDO A, 2001, P SPIE INT S CONV IT
   Brooke A., 1988, GAMS: A User's Guide
   Eager D, 2001, IEEE T KNOWL DATA EN, V13, P742, DOI 10.1109/69.956098
   EAGER DL, 2000, PERF EVAL SPECIAL IS, V42
   Fei ZM, 2002, IEEE J SEL AREA COMM, V20, P1399, DOI 10.1109/JSAC.2002.802069
   HUA KA, 1998, P ACM MULT BRIST UK
   HUA KA, 1997, P ACM SIGCOMM CANN F
   JAMIN S, 2001, P INFOCOM ANCH AK AP
   Krishnan P, 2000, IEEE ACM T NETWORK, V8, P568, DOI 10.1109/90.879344
   Mahanti A., 2001, P ACM SIGCOMM SAN DI
   QIU L, 2001, P INFOCOM ANCH AK AP
   Radoslavov P, 2002, COMPUT COMMUN, V25, P384, DOI 10.1016/S0140-3664(01)00410-8
   TAN H, 2002, P PERF ROM IT SEPT
   Verscheure O, 2002, COMPUT COMMUN, V25, P413, DOI 10.1016/S0140-3664(01)00413-3
   WANG B, 2002, P INFOCOM NEW YORK J
   ZHANG S, UNPUB FURTHER ANAL E
   ZHAO Y, 2002, P INFOCOM NEW YORK N
   [No title captured]
NR 25
TC 45
Z9 52
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 356
EP 365
DI 10.1109/TMM.2003.822796
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400014
OA Green Published
DA 2024-07-18
ER

PT J
AU Kang, SY
   Yeom, HY
AF Kang, SY
   Yeom, HY
TI Storing continuous media objects to multizone recording disks using
   multirate smoothing technique
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE block placement; multimedia storage system; MZR disk; smoothing; VBR
   data
ID VIDEO; TRANSMISSION
AB Since multizone recording disks have different bandwidths and capacities depending on the zone in use, data placement schemes for traditional constant angular density disks are not suitable for multizone recording disks. In this paper, we propose a new block placement algorithm for multizone recording disks used for continuous media servers. The proposed scheme exploits the bandwidth-saving effect of smoothing variable bit rate data before storing them. The diversity of zone bandwidths in multizone recording disks enables it possible to achieve large smoothing effect using relatively small buffer space. Variable bit rate data blocks of an object are smoothed using multiple smoothing rates which are bandwidths of zones multiplied by the service time assigned to the object and are stored into the corresponding zones. This multirate smoothing technique decreases the buffer space required to provide deterministic service to clients. Simulation results show that a proper restructuring of blocks according to the smoothing algorithm results in dramatic performance enhancement in continuous media servers.
C1 Seoul Natl Univ, Sch Comp Sci & Engn, Seoul 151742, South Korea.
C3 Seoul National University (SNU)
RP Kang, SY (corresponding author), Hanyang Univ, Dept Comp Sci Educ, Seoul 133791, South Korea.
CR [Anonymous], **NON-TRADITIONAL**
   Birk Y., 1995, Proceedings of the International Conference on Multimedia Computing and Systems (Cat. No.95TH8066), P248, DOI 10.1109/MMCS.1995.484930
   GHANDEHARIZADEH S, 1996, MULTIMEDIA INFORMATI
   KANG J, 1999, P INT WORKSH NETW OP
   Kang SY, 1999, COMPUT COMMUN, V22, P173, DOI 10.1016/S0140-3664(98)00250-3
   KIM JW, 1997, P 4 IEEE INT C MULT, P29
   LEE K, 1998, P 4 INT S MULT INF S
   McManus JM, 1996, IEEE J SEL AREA COMM, V14, P1087, DOI 10.1109/49.508280
   Mitoma M.F., 1993, US Patent, Patent No. [5,202,799, 5202799]
   Paek S, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P108, DOI 10.1109/MMCS.1996.535893
   SALEHI J, 1996, P ACM SIGMETRICS 96, P222
   SHROFF N, 1994, P IEEE INFOCOM, P342
   SKELLY P, 1993, IEEE ACM T NETWORKIN, V1
   WANG YC, 1997, P MULT COMP NETW SAN
   Zhang H, 1997, MULTIMEDIA SYST, V5, P164, DOI 10.1007/s005300050053
   ZHANG J, 1997, P IEEE INT C MULT CO, P3
NR 16
TC 2
Z9 3
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 473
EP 482
DI 10.1109/TMM.2003.814724
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500018
DA 2024-07-18
ER

PT J
AU Chen, F
   Yang, YL
   He, HJ
   Yuan, Y
AF Chen, Fan
   Yang, Yaolin
   He, Hongjie
   Yuan, Yuan
TI Adaptive Coding and Ordered-Index Extended Scrambling Based RDH in
   Encrypted Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Reversible data hiding; image encryption; adaptive coding; extended
   scrambling; chosen plaintext attack (CPA)
ID SECURITY
AB Reversible data hiding in encrypted images (RDHEI) technique can be used to realize privacy protection and management in the image outsourcing scenario. Most existing RDHEI schemes focus on increasing the maximum embedding rate (Max-ER), but not paying much attention to the security improvement under various attacks. In this paper, a RDHEI method based on the adaptive bit-plane (ABP) coding is proposed to improve the Max-ER. The order-index extended scrambling (OIES) encryption scheme is also developed to strengthen the RDHEI's ability of thwarting various attacks. The effectiveness of ABP coding is achieved by proper selections of the threshold. The OIES enables the design of a novel scramble-key (SK) generation method to greatly reduce the probability of generating the same SK by the same user-key. This significantly improves the ability of resisting various attacks in that the attack on the scrambling encryption is mainly via the SK rather than the user-key estimation. Analysis shows that the probability of OIES obtaining the same SK is reduced from 1.0 to 0.01 for different images and to 1/2(a) for the same image. Simulation results demonstrate that the proposed ABP coding and OIES schemes outperform the state-of-the-art RDHEI algorithms in terms of the Max-ER and ability against various attacks.
C1 [Chen, Fan] Southwest Jiaotong Univ, Sch Comp & Artificial Intelligence, Chengdu 611756, Peoples R China.
   [Yang, Yaolin; He, Hongjie; Yuan, Yuan] Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 611756, Peoples R China.
C3 Southwest Jiaotong University; Southwest Jiaotong University
RP He, HJ (corresponding author), Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 611756, Peoples R China.
EM 27660022@qq.com; 245401677@qq.com; hjhe@swjtu.edu.cn; ytuanyuan@qq.com
FU National Natural Science Foundation of China (NSFC) [U1936113, 61872303]
FX & nbsp;This work was supported by the National Natural Science
   Foundation of China (NSFC) under Grants 61872303 and U1936113.& nbsp;&
   nbsp;
CR Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Chen F., 2018, IWDW DIGIT FORENSICS, P216
   Chen F, 2021, IEEE T CIRC SYST VID, V31, P905, DOI 10.1109/TCSVT.2020.2992817
   Chen Y., 2018, Chinese Patent, Patent No. 201810174576
   Chen YY, 2018, CMC-COMPUT MATER CON, V56, P299, DOI 10.3970/cmc.2018.03179
   Fu YJ, 2019, INFORM SCIENCES, V494, P21, DOI 10.1016/j.ins.2019.04.043
   He HJ, 2009, SIGNAL PROCESS, V89, P1557, DOI 10.1016/j.sigpro.2009.02.009
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Hu X., 2019, HDB APPL CRYPTOGRAPH
   Huang FJ, 2016, IEEE T INF FOREN SEC, V11, P2777, DOI 10.1109/TIFS.2016.2598528
   Ismail S., 2020, SIGNAL PROCESS, V167, P1
   Jolfaei A, 2016, IEEE T INF FOREN SEC, V11, P235, DOI 10.1109/TIFS.2015.2489178
   Khelifi F, 2018, SIGNAL PROCESS, V143, P336, DOI 10.1016/j.sigpro.2017.09.020
   Koppanati Rama Krishna, 2021, International Conference on Deep Learning, Artificial Intelligence and Robotics ICDLAIR 2019. Proceedings. Lecture Notes in Networks and Systems (LNNS 175), P246, DOI 10.1007/978-3-030-67187-7_26
   Koppanati RK, 2021, IEEE CONSUM ELECTR M, V10, P41, DOI 10.1109/MCE.2020.3003127
   Li SJ, 2008, SIGNAL PROCESS-IMAGE, V23, P212, DOI 10.1016/j.image.2008.01.003
   Liao X, 2015, J VIS COMMUN IMAGE R, V28, P21, DOI 10.1016/j.jvcir.2014.12.007
   Liu ZL, 2018, INFORM SCIENCES, V433, P188, DOI 10.1016/j.ins.2017.12.044
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Purohit Kaustubh, 2020, Soft Computing: Theories and Applications. Proceedings of SoCTA 2019. Advances in Intelligent Systems and Computing (AISC 1154), P135, DOI 10.1007/978-981-15-4032-5_14
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Qian ZX, 2016, J VIS COMMUN IMAGE R, V40, P732, DOI 10.1016/j.jvcir.2016.08.020
   Qian ZX, 2016, IEEE T CIRC SYST VID, V26, P636, DOI 10.1109/TCSVT.2015.2418611
   Qin C, 2018, SIGNAL PROCESS, V153, P109, DOI 10.1016/j.sigpro.2018.07.008
   Qin C, 2018, INFORM SCIENCES, V465, P285, DOI 10.1016/j.ins.2018.07.021
   Qu LF, 2022, IEEE T MULTIMEDIA, V24, P2924, DOI 10.1109/TMM.2021.3090588
   Qu LF, 2022, IEEE T CIRC SYST VID, V32, P920, DOI 10.1109/TCSVT.2021.3069811
   Qu LF, 2020, MULTIMED TOOLS APPL, V79, P29451, DOI 10.1007/s11042-020-09379-3
   Sheidani S, 2021, IEEE T INF FOREN SEC, V16, P3647, DOI 10.1109/TIFS.2021.3080497
   Shi YQ, 2016, IEEE ACCESS, V4, P3210, DOI 10.1109/ACCESS.2016.2573308
   Wu XT, 2014, SIGNAL PROCESS, V104, P387, DOI 10.1016/j.sigpro.2014.04.032
   Wu YQ, 2020, IEEE T MULTIMEDIA, V22, P1929, DOI 10.1109/TMM.2019.2952979
   Xu DW, 2016, SIGNAL PROCESS, V123, P9, DOI 10.1016/j.sigpro.2015.12.012
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Yi S, 2017, SIGNAL PROCESS, V133, P40, DOI 10.1016/j.sigpro.2016.10.017
   Yin ZX, 2020, IEEE T MULTIMEDIA, V22, P874, DOI 10.1109/TMM.2019.2936314
   Yin ZX, 2017, MULTIMED TOOLS APPL, V76, P3899, DOI 10.1007/s11042-016-4049-z
   Zhang LY, 2018, INFORM SCIENCES, V430, P228, DOI 10.1016/j.ins.2017.11.021
   Zhang QC, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2886779
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
   Zhou JT, 2016, IEEE T CIRC SYST VID, V26, P441, DOI 10.1109/TCSVT.2015.2416591
NR 42
TC 2
Z9 2
U1 9
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2864
EP 2875
DI 10.1109/TMM.2021.3132168
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600032
DA 2024-07-18
ER

PT J
AU Chen, J
   Chen, LL
   Zeng, HQ
   Hsia, CH
   Wang, TL
   Ma, KK
AF Chen, Jing
   Chen, Linlin
   Zeng, Huanqiang
   Hsia, Chih-Hsien
   Wang, Tianlei
   Ma, Kai-Kuang
TI 3D-Gradient Guided Rate Control Model for Screen Content Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HEVC-SCC; rate control; spatial-temporal features; 3D-gradient
ID OPTIMAL BIT ALLOCATION; LEVEL RATE CONTROL; SIMILARITY
AB Compared with natural videos, screen content videos (SCVs) have particular features, such as fruitful sharper edges, lots of computer-generated graphics and texts, a large amount of flat areas. New tools are adopted to HEVC extensions on Screen Content Coding (HEVC-SCC), the traditional video rate control methods for natural videos are not effective for SCVs. For that, a 3D-gradient guided rate control model for SCV coding, named 3DG-RC, is proposed to allocate bitrate more efficiently serving for SCVs. By considering the particular spatial-temporal characteristics of SCVs, the spatial and temporal feature extraction scheme is developed by using 3D-gradient filter and performed on the SCV to extract the spatial and temporal features simultaneously for guiding the bit allocation. The spatial-temporal feature similarity between three original reference SCV frames and their reconstructed ones is used to estimate the encoding parameters of the current block and frame. Experimental results demonstrate that compared with the classical and state-of-the-art rate control methods for HEVC-SCC, the proposed 3DG-RC algorithm achieves significant bitrate mismatch reduction and coding efficiency improvement for HEVC-SCC. In specific, the proposed 3DG-RC model outperforms the rate control model in SCM-8.8 with over 41.33% and 37.95% BD-BR savings on average, for low delay B (LDB) and random access (RA) coding structure, respectively.
C1 [Chen, Jing; Chen, Linlin] Huaqiao Univ, Sch Informat Sci & Engn, Quanzhou 362021, Fujian, Peoples R China.
   [Zeng, Huanqiang] Huaqiao Univ, Sch Engn, Quanzhou 362021, Fujian, Peoples R China.
   [Zeng, Huanqiang] Huaqiao Univ, Sch Informat Sci & Engn, Quanzhou 362021, Fujian, Peoples R China.
   [Hsia, Chih-Hsien] Ilan Univ, Dept Comp Sci & Informat Engn, Yilan 260, Taiwan.
   [Wang, Tianlei] Hangzhou Dianzi Univ, Machine Learning & I Hlth Int Co operat Base Zhej, Hangzhou 310005, Zhejiang, Peoples R China.
   [Wang, Tianlei] Hangzhou Dianzi Univ, Artificial Intelligence Inst, Hangzhou 310005, Zhejiang, Peoples R China.
   [Ma, Kai-Kuang] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Huaqiao University; Huaqiao University; Huaqiao University; National
   Ilan University; Hangzhou Dianzi University; Hangzhou Dianzi University;
   Nanyang Technological University
RP Zeng, HQ (corresponding author), Huaqiao Univ, Sch Engn, Quanzhou 362021, Fujian, Peoples R China.; Zeng, HQ (corresponding author), Huaqiao Univ, Sch Informat Sci & Engn, Quanzhou 362021, Fujian, Peoples R China.
EM chenjing8005@hqu.edu.cn; chenlinlin_117@163.com; zeng0043@hqu.edu.cn;
   hsiach@niu.edu.tw; tianleiwang@hdu.edu.cn; ekkma@ntu.edu.sg
RI Zeng, Huanqiang/U-2017-2018; Ma, Kai-Kuang/KBA-9411-2024
OI Chen, Jing/0000-0002-5596-4013; Wang, Tianlei/0000-0002-4498-4326
FU National Key R&D Program of China [2021YFE0205400]; National Natural
   Science Foundation of China [61871434, 61976098]; Natural Science
   Foundation for Outstanding Young Scholars of Fujian Province
   [2022J06023]; Natural Science Foundation of Fujian Province
   [2022J01294]; Collaborative Innovation Platform Project of
   Fuzhou-Xiamen-Quanzhou National Independent Innovation Demonstration
   Zone [2021FX03]
FX This work was supported in part by the National Key R&D Program of
   China, under Grant 2021YFE0205400, in part by the National Natural
   Science Foundation of China under Grants 61871434 and 61976098, in part
   by the Natural Science Foundation for Outstanding Young Scholars of
   Fujian Province under Grant 2022J06023, in part by the Natural Science
   Foundation of Fujian Province under Grant 2022J01294, and in part by
   Collaborative Innovation Platform Project of Fuzhou-Xiamen-Quanzhou
   National Independent Innovation Demonstration Zone under Grant 2021FX03.
CR Bjontegaard G., 2001, CALCULATION AVERAGE
   Cheng S, 2020, IEEE T IMAGE PROCESS, V29, P8636, DOI 10.1109/TIP.2020.3018256
   Choi H., 2012, JCT-VC, document JCTVC-H0213
   Choi H, 2013, IEEE J-STSP, V7, P1112, DOI 10.1109/JSTSP.2013.2272241
   Guo HW, 2019, IEEE T BROADCAST, V65, P270, DOI 10.1109/TBC.2018.2847445
   Guo YY, 2015, IEEE INT SYMP CIRC S, P1118, DOI 10.1109/ISCAS.2015.7168834
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P928, DOI 10.1109/76.937431
   Jia CC, 2015, LECT NOTES COMPUT SC, V9242, P493, DOI 10.1007/978-3-319-23989-7_50
   Kuang W, 2020, IEEE T CIRC SYST VID, V30, P1917, DOI 10.1109/TCSVT.2019.2929317
   Li B, 2013, JCTVCM0036
   Li B., 2012, JCT-VC, document JCTVC-I0426
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Li B, 2013, IEEE INT SYMP CIRC S, P477, DOI 10.1109/ISCAS.2013.6571884
   Li B, 2018, IEEE MULTIMEDIA, V25, P79, DOI 10.1109/MMUL.2018.112142602
   Li SX, 2017, IEEE T CIRC SYST VID, V27, P2409, DOI 10.1109/TCSVT.2016.2589878
   Li YW, 2014, IEEE T CIRC SYST VID, V24, P842, DOI 10.1109/TCSVT.2013.2283648
   Liu M, 2010, IEEE IMAGE PROC, P1277, DOI 10.1109/ICIP.2010.5653340
   Liu S, 2015, APSIPA TRANS SIGNAL, V4, DOI 10.1017/ATSIP.2015.11
   Liu XY, 2019, IEEE ACCESS, V7, P154959, DOI 10.1109/ACCESS.2019.2948709
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   Ma Z, 2014, IEEE T IMAGE PROCESS, V23, P4399, DOI 10.1109/TIP.2014.2346995
   Maloney RT, 2015, NEUROIMAGE, V119, P129, DOI 10.1016/j.neuroimage.2015.06.034
   Ni ZK, 2017, IEEE T IMAGE PROCESS, V26, P4818, DOI 10.1109/TIP.2017.2718185
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tan YH, 2012, IEEE T CIRC SYST VID, V22, P1236, DOI 10.1109/TCSVT.2012.2198132
   Wang S., 2013, JCT-VC, document JCTVC-L0317
   Wang SS, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351596
   Wu JJ, 2015, IEEE T IMAGE PROCESS, V24, P4602, DOI 10.1109/TIP.2015.2460467
   Xiao JS, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang H, 2020, IEEE T BROADCAST, V66, P333, DOI 10.1109/TBC.2019.2954062
   Yingya Guo, 2015, 2015 IEEE 34th International Performance Computing and Communications Conference (IPCCC), P1, DOI 10.1109/PCCC.2015.7410320
   Yu H., 2013, document JCTVC-L0301
   Yu H., 2014, ISO/IEC JTC 1/SC 29/WG 11, document MPEG014/N14174
   Yu H., 2014, JCT-VC, document JCTVC-Q1015
NR 36
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7930
EP 7942
DI 10.1109/TMM.2022.3232020
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400025
DA 2024-07-18
ER

PT J
AU Deng, X
   Wang, H
   Xu, M
   Li, L
   Wang, ZL
AF Deng, Xin
   Wang, Hao
   Xu, Mai
   Li, Li
   Wang, Zulin
TI Omnidirectional Image Super-Resolution via Latitude Adaptive Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Omnidirectional image; super-resolution
AB Omnidirectional images (ODIs) have recently attracted extensive attention from both academia and industry. However, due to storage and transmission limitations, ODIs are usually at extremely low resolution. Thus, it is necessary to restore a high-resolution ODI from a low-resolution ODI, i.e., omnidirectional image super-resolution (ODI-SR). Towards ODI-SR, we propose in this paper a novel latitude-aware upscaling network, namely LAU-Net+, which fully considers the above characteristics of ODIs. In our network, different latitude bands can learn to adopt distinct upscaling factors, which significantly saves the computational resources and improves the SR efficiency. Specifically, a Laplacian multilevel pyramid network is introduced in which the upscaling factor is gradually increased with the number of levels. Each level is composed of a feature enhancement module (FEM), a drop-band decision module (DDM) and a high-latitude enhancement module (HEM). The FEM module serves to enhance the high-level features extracted from the input ODI, while the role of DDM is to dynamically drop the unnecessary high latitude bands and send the remained bands to the next level. The HEM is adopted to further enhance high-level features of dropped latitude bands with a lightweight architecture. In DDM, we develop a reinforcement learning scheme with a latitude adaptive reward to determine which band should be dropped. To the best of our knowledge, our method is the first work which considers the latitude characteristics for ODI-SR task. Extensive experimental results demonstrate that our LAU-Net+ achieves state-of-the-art results on ODI-SR both quantitatively and qualitatively on various ODI datasets.
C1 [Deng, Xin] Beihang Univ, Sch Cyber Sci & Technol, Beijing 100083, Peoples R China.
   [Wang, Hao; Xu, Mai; Li, Li; Wang, Zulin] Beihang Univ, Dept Elect Informat Engn, Beijing 100083, Peoples R China.
C3 Beihang University; Beihang University
RP Xu, M (corresponding author), Beihang Univ, Dept Elect Informat Engn, Beijing 100083, Peoples R China.
EM wang_hao@buaa.edu.cn; maixu@buaa.edu.cn; lili2005@buaa.edu.cn;
   wzulin@buaa.edu.cn
FU NSFC [62001016, 61876013, 61922009]; Beijing Natural Science Foundation
   [JQ20020]; Alibaba Innovative Research
FX This work was supported by NSFC under Grants 62001016, 61876013, and
   61922009, in part by the Beijing Natural Science Foundation under Grant
   JQ20020, and in part by the Alibaba Innovative Research.
CR [Anonymous], 2019, Tech. Rep.
   Arican Z, 2011, IEEE T IMAGE PROCESS, V20, P3151, DOI 10.1109/TIP.2011.2144609
   Bagnato L, 2010, IEEE IMAGE PROC, P2829, DOI 10.1109/ICIP.2010.5652095
   Choi JS, 2017, IEEE COMPUT SOC CONF, P1150, DOI 10.1109/CVPRW.2017.153
   Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32
   Deng X, 2021, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR46437.2021.00907
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Elbamby MS, 2018, IEEE NETWORK, V32, P78, DOI 10.1109/MNET.2018.1700268
   F. VR, About us
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D, 2014, C LEARNING REPRESENT, P12
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Nagahara H, 2000, IEEE IND ELEC, P2559, DOI 10.1109/IECON.2000.972401
   Ozcinar C, 2019, IEEE INT WORKSH MULT, DOI 10.1109/mmsp.2019.8901764
   Qiu YJ, 2019, IEEE I CONF COMP VIS, P4179, DOI 10.1109/ICCV.2019.00428
   Shang TZ, 2020, IEEE COMPUT SOC CONF, P1778, DOI 10.1109/CVPRW50498.2020.00228
   Sun YL, 2017, IEEE SIGNAL PROC LET, V24, P1408, DOI 10.1109/LSP.2017.2720693
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Umer RM, 2020, IEEE COMPUT SOC CONF, P1769, DOI 10.1109/CVPRW50498.2020.00227
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang X, 2018, LECT NOTES COMPUT SC, V11217, P420, DOI 10.1007/978-3-030-01261-8_25
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xiao JX, 2012, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2012.6247991
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yu K, 2018, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2018.00259
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang ZD, 2017, IEEE COMPUT SOC CONF, P1015, DOI 10.1109/CVPRW.2017.138
   Zhou YF, 2018, INT CONF SIGN PROCES, P54, DOI 10.1109/ICSP.2018.8652269
NR 42
TC 2
Z9 2
U1 2
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4108
EP 4120
DI 10.1109/TMM.2022.3171401
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200004
DA 2024-07-18
ER

PT J
AU Gan, MG
   Zhang, Y
AF Gan, Ming-Gang
   Zhang, Yan
TI Temporal Attention-Pyramid Pooling for Temporal Action Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Feature extraction; Task analysis; Convolution; Computational
   modeling; Context modeling; Three-dimensional displays; Action proposal
   representation; temporal action proposal generation; temporal action
   detection; untrimmed video analysis
ID ACTION RECOGNITION
AB Temporal action detection is a challenging task in video understanding, which is usually divided into two stages: proposal generation and classification. Learning proposal features is a crucial step for both stages. However, most methods ignore temporal information of proposals and consider background and action frames in proposals equally, leading to poor proposal features. In this paper, we propose a novel Temporal Attention-Pyramid Pooling (TAPP) method to learn proposal features of arbitrary length action proposals. The TAPP method exploits the attention mechanism to focus on the discriminative part of proposals, suppressing background influence on proposal features. It constructs a temporal pyramid structure to convert arbitrary length proposal feature sequences to multiple fixed-length sequences while retaining the temporal information. In the TAPP method, we design a multi-scale temporal function and apply it to the temporal pyramid to generate final proposal features. Based on the TAPP method, we construct a temporal action proposal generation model and an action proposal classification model, and then we perform extensive experiments on two mainstream temporal action detection datasets for the temporal action proposal and temporal action detection tasks to verify our models. On the THUMOS'14 dataset, our models based on the TAPP significantly outperform the previous state-of-the-art methods for both tasks.
C1 [Gan, Ming-Gang; Zhang, Yan] Beijing Inst Technol, Sch Automat, State Key Lab Intelligent Control & Decis Complex, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Zhang, Y (corresponding author), Beijing Inst Technol, Sch Automat, State Key Lab Intelligent Control & Decis Complex, Beijing 100081, Peoples R China.
EM aganbit@126.com; zhangyanfxtx_2016@163.com
OI Zhang, Yan/0000-0002-9125-3630
FU National Key RAMP;D Program of China [2020YFB1708500]
FX This work was supported by the National Key R&D Program of China under
   Grant 2020YFB1708500.
CR Alwassel H, 2018, LECT NOTES COMPUT SC, V11213, P253, DOI 10.1007/978-3-030-01240-3_16
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   [Anonymous], 2016, CUHK & ETHZ & SIAT submission to ActivityNet challenge 2016
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   DUETAL Y, 2018, P EUR C COMPUT VIS, P373
   GAO J, 2017, P BRIT MACH VIS C
   Gao JL, 2020, AAAI CONF ARTIF INTE, V34, P10810
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao LL, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107477
   Gao ZN, 2019, AAAI CONF ARTIF INTE, P8328
   Gong GH, 2021, PREP BIOCHEM BIOTECH, V51, P183, DOI 10.1080/10826068.2020.1805755
   Heilbron FC, 2017, PROC CVPR IEEE, P3175, DOI 10.1109/CVPR.2017.338
   HU Y, 2019, CMSN CONTINUOUS MULT
   Huang YP, 2019, IEEE INT CON MULTI, P1288, DOI 10.1109/ICME.2019.00224
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Kalfaoglu M. Esat, 2020, Proceedings of the 16th European Conference on Computer Vision (ECCV 2020) Workshops. Lecture Notes in Computer Science (LNCS 12539), P731, DOI 10.1007/978-3-030-68238-5_48
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, ARXIV170506950
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li J, 2020, AAAI CONF ARTIF INTE, V34, P4626
   Li X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4004, DOI 10.1145/3394171.3413860
   Lin CM, 2021, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR46437.2021.00333
   Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu QY, 2020, AAAI CONF ARTIF INTE, V34, P11612
   Liu YF, 2019, PROC CVPR IEEE, P7099, DOI [10.1109/CVPR.2019.00726, 10.1109/CVPR.2019.00372]
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Majd M, 2020, NEUROCOMPUTING, V396, P224, DOI 10.1016/j.neucom.2018.10.095
   Meng LL, 2019, IEEE INT CONF COMP V, P1513, DOI 10.1109/ICCVW.2019.00189
   Neimark D, 2021, IEEE INT CONF COMP V, P3156, DOI [arXiv:2102.00719, 10.1109/ICCVW54120.2021.00355]
   Peisen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P539, DOI 10.1007/978-3-030-58598-3_32
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Shou Zheng, 2017, P IEEE C COMP VIS PA, P5734, DOI DOI 10.48550/ARXIV.1703.01515
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58
   Song LF, 2018, IEEE IMAGE PROC, P808, DOI 10.1109/ICIP.2018.8451662
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   TANG Y, 2019, AFO TAD ANCHORFREE O
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, P 2016 INT C SUP ICS, P1
   Wang P, 2017, IEEE T CIRC SYST VID, V27, P2613, DOI 10.1109/TCSVT.2016.2576761
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang X, 2021, PROC CVPR IEEE, P1905, DOI 10.1109/CVPR46437.2021.00194
   XIONG Y, 2017, A PURSUIT TEMPORAL A
   Xu M., 2020, CVPR, P10156
   Yueran Bai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P121, DOI 10.1007/978-3-030-58604-1_8
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
NR 60
TC 9
Z9 9
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3799
EP 3810
DI 10.1109/TMM.2022.3166025
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500020
DA 2024-07-18
ER

PT J
AU Jin, HY
   Lai, SQ
   Tang, Q
   Zhu, TY
   Qian, XM
AF Jin, Hanyang
   Lai, Shenqi
   Tang, Qi
   Zhu, Tianyu
   Qian, Xueming
TI MPPM: A Mobile-Efficient Part Model for Object re-ID
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Decoding; Hardware; Neural networks; Eye protection;
   Electronic mail; Costs; Mobile-efficient; neural processing units;
   object re-ID; part model
ID PERSON REIDENTIFICATION; NETWORK; VEHICLE
AB Object re-identification (re-ID) is one of the core technologies in Multi-Object Tracking (MOT) that requires realtime decision-making. A Neural Processing Unit (NPU) is a lowpower device that is dedicated to deploying neural network-based algorithms and has become one of the most important devices in today'smobile onboard systems. However, the currentmainstream re-ID methods rarely consider the NPU characteristics, which makes it difficult for these methods to achieve both high onboard frame rates and high accuracies on an NPU. To address this problem, this paper focuses on designing a re-ID algorithm suitable for NPU deployment. The model of the object re-ID can be divided into two parts: the encoder (backbone) and the decoder. In this article, a Mobile-efficient Pure Part Model (MPPM) is presented for re-ID task. First, for the backbone of re-ID, we propose an efficient structure GogglesNet, which is composed of traditional convolutions. GogglesNet performs well on the re-ID task and can be comparable to lightweight networks on ImageNet with regard to accuracy and is faster on NPU. We then revisit the architectures of Pure Part Model (PPM) in person re-ID, including PCB and MGN, and propose a mobile-efficient decoder Dual Pattern Network (DPN) for re-ID. The proposed MPPM achieves comparable performance with MGN on five reID datasets Market-1501, DukeMTMC-reID, MSMT17, VeRi-776, andVehicleID, while the proposed parameter amount is only 10.2% of it, and the speed on NPU is more than eight times higher.
C1 [Jin, Hanyang; Lai, Shenqi; Qian, Xueming] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, SMILES LAB, Xian 710049, Shaanxi, Peoples R China.
   [Tang, Qi] AXERA Semicond Shanghai Co Ltd, Shanghai 200000, Peoples R China.
   [Zhu, Tianyu] Hebei Zete Jiyuan Technol Co Ltd, Shijiazhuang 050000, Hebei, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Minist Educ Key Lab Intelligent Networks & Networ, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, SMILES LAB, Xian 710049, Shaanxi, Peoples R China.
EM jhy0606@stu.xjtu.edu.cn; laishenqi@stu.xjtu.edu.cn;
   tangqi@axera-tech.com; 2910670457xty@gmail.com; qianxm@mail.xjtu.edu.cn
RI Zhu, Tianyu/AAP-4299-2020
FU National Key R&D Program of China [2018AAA0101501]; NSFC [61772407,
   61732008]; Science and Technology Program of Xi'an, China [21RGZN0017];
   Xi'an Science and Technology Planning Project [20YXYJ0005-3]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0101501, in part by NSFC under Grants 61772407 and
   61732008, in part by the Science and Technology Program of Xi'an, China
   under Grant 21RGZN0017, and in part by the Xi'an Science and Technology
   Planning Project under Grant 20YXYJ0005-3. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Guoying Zhao. (Hanyang Jin and Shenqi Lai
   contributed equally to this work.)
CR Barman A, 2021, IEEE T PATTERN ANAL, V43, P753, DOI 10.1109/TPAMI.2019.2944597
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen XS, 2020, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR42600.2020.00336
   Fan BY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P655, DOI 10.1145/3394171.3414038
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gholami A, 2018, IEEE COMPUT SOC CONF, P1719, DOI 10.1109/CVPRW.2018.00215
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, Arxiv, DOI arXiv:2102.04378
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hou RB, 2021, IEEE T NEUR NET LEAR, V32, P4460, DOI 10.1109/TNNLS.2020.3017939
   Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291
   Huang W, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11052010
   Jin HY, 2022, IEEE T CIRC SYST VID, V32, P2170, DOI 10.1109/TCSVT.2021.3088446
   Jin X, 2020, AAAI CONF ARTIF INTE, V34, P11165
   Khorramshahi Pirazh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P369, DOI 10.1007/978-3-030-58568-6_22
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lavin A, 2016, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2016.435
   Li HJ, 2021, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR46437.2021.00666
   Li K, 2022, IEEE T NEUR NET LEAR, V33, P826, DOI 10.1109/TNNLS.2020.3029299
   Li X, 2022, IEEE T CIRC SYST VID, V32, P1792, DOI 10.1109/TCSVT.2021.3082635
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liu CX, 2021, IEEE T CIRC SYST VID, V31, P2480, DOI 10.1109/TCSVT.2020.3020079
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Liu YH, 2021, IEEE T IMAGE PROCESS, V30, P2060, DOI 10.1109/TIP.2021.3050839
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Luo H, 2019, PATTERN RECOGN, V94, P53, DOI 10.1016/j.patcog.2019.05.028
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Meng DC, 2020, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR42600.2020.00713
   Iandola FN, 2016, Arxiv, DOI arXiv:1602.07360
   Qian JJ, 2020, MEAS SCI TECHNOL, V31, DOI 10.1088/1361-6501/ab8b81
   Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385
   Radosavovic Ilija, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10425, DOI 10.1109/CVPR42600.2020.01044
   Ren XA, 2023, IEEE T MULTIMEDIA, V25, P4387, DOI 10.1109/TMM.2022.3174768
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shizhen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P647, DOI 10.1007/978-3-030-58539-6_39
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun ZR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3523, DOI 10.1145/3394171.3413541
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tsai-Shien Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P330, DOI 10.1007/978-3-030-58536-5_20
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang J., 2018, PROC BRIT MACH VIS C
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P3416, DOI 10.1109/TIP.2019.2959923
   Wang L, 2020, P AS C COMP VIS, P341
   Wang RJ, 2018, 32 C NEURAL INFORM P
   Wang X, 2022, IEEE T MULTIMEDIA, V24, P2804, DOI 10.1109/TMM.2021.3088639
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P3362, DOI 10.1109/TMM.2020.3024822
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xie GT, 2018, PROC CVPR IEEE, P8847, DOI 10.1109/CVPR.2018.00922
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang XY, 2022, IEEE T INTELL TRANSP, V23, P3048, DOI 10.1109/TITS.2020.3030301
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong YJ, 2021, IEEE T IMAGE PROCESS, V30, P8384, DOI 10.1109/TIP.2021.3113183
   Zhou KY, 2022, IEEE T PATTERN ANAL, V44, P5056, DOI 10.1109/TPAMI.2021.3069237
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou Q, 2021, IEEE T IMAGE PROCESS, V30, P1623, DOI 10.1109/TIP.2019.2914575
NR 67
TC 2
Z9 2
U1 5
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6356
EP 6370
DI 10.1109/TMM.2022.3207895
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500050
DA 2024-07-18
ER

PT J
AU Li, H
   Qin, JH
   Yang, ZJ
   Wei, PX
   Pan, JS
   Lin, L
   Shi, YK
AF Li, Hao
   Qin, Jinghui
   Yang, Zhijing
   Wei, Pengxu
   Pan, Jinshan
   Lin, Liang
   Shi, Yukai
TI Real-World Image Super-Resolution by Exclusionary Dual-Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep representation learning; dual learning; efficient training;
   real-world image restoration
ID QUALITY ASSESSMENT; NETWORK
AB Real-world image super-resolution is a practical image restoration problem that aims to obtain high-quality images from in-the-wild input, has recently received considerable attention with regard to its tremendous application potentials. Although deep learning-based methods have achieved promising restoration quality on real-world image super-resolution datasets, they ignore the relationship between L1- and perceptual- minimization and roughly adopt auxiliary large-scale datasets for pre-training. In this paper, we discuss the image types within a corrupted image and the property of perceptual- and Euclidean- based evaluation protocols. Then we propose a method, Real-World image Super-Resolution by Exclusionary Dual-Learning (RWSR-EDL) to address the feature diversity in perceptual- and L1- based cooperative learning. Moreover, a noise-guidance data collection strategy is developed to address the training time consumption in multiple datasets optimization. When an auxiliary dataset is incorporated, RWSR-EDL achieves promising results and repulses any training time increment by adopting the noise-guidance data collection strategy. Extensive experiments show that RWSR-EDL achieves competitive performance over state-of-the-art methods on four in-the-wild image super-resolution datasets.
C1 [Li, Hao; Yang, Zhijing; Shi, Yukai] Guangdong Univ Technol, Sch Informat Technol, Guangzhou 510006, Peoples R China.
   [Pan, Jinshan] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Qin, Jinghui; Wei, Pengxu; Lin, Liang] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Peoples R China.
C3 Guangdong University of Technology; Nanjing University of Science &
   Technology; Sun Yat Sen University
RP Shi, YK (corresponding author), Guangdong Univ Technol, Sch Informat Technol, Guangzhou 510006, Peoples R China.
EM lihao9605@gmail.com; qinjingh@mail2.sysu.edu.cn; yzhj@gdut.edu.cn;
   weipx3@mail.sysu.edu.cn; sdluran@gmail.com; linliang@ieee.org;
   ykshi@gdut.edu.cn
RI Lin, Liang/IQR-8601-2023; Pan, Jinshan/S-3658-2019
OI Lin, Liang/0000-0003-2248-3755; wei, pengxu/0000-0002-2190-0767; Shi,
   Yukai/0000-0002-9413-6528; Pan, Jinshan/0000-0003-0304-9507
FU National Natural Science Foundation of China (NSFC) [62002069]; Science
   and Technology Project of Guangdong Province [2021A1515011341,
   2022A1515011835]; China Postdoctoral Science Foundation [2021M703687];
   Guangzhou Science and Technology Plan Project [202002030386, 102020369]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 62002069, in part by the Science
   and Technology Project of Guangdong Province under Grants
   2021A1515011341 and 2022A1515011835, in part by the China Postdoctoral
   Science Foundation funded project under Grant 2021M703687, and in part
   by Guangzhou Science and Technology Plan Project under Grants
   202002030386 and 102020369.
CR Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325
   Bell-Kligler S, 2019, ADV NEUR IN, V32
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318
   Chen C, 2019, PROC CVPR IEEE, P1652, DOI 10.1109/CVPR.2019.00175
   Chen JW, 2018, PROC CVPR IEEE, P3155, DOI 10.1109/CVPR.2018.00333
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Fritsche M, 2019, IEEE INT CONF COMP V, P3599, DOI 10.1109/ICCVW.2019.00445
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu XC, 2021, IEEE T NEUR NET LEAR, V32, P4151, DOI 10.1109/TNNLS.2020.3016974
   Ji XZ, 2020, IEEE COMPUT SOC CONF, P1914, DOI 10.1109/CVPRW50498.2020.00241
   Jiang K, 2022, IEEE T NEUR NET LEAR, V33, P378, DOI 10.1109/TNNLS.2020.3027849
   Jiang K, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107475
   Kingma D. P., 2014, arXiv
   Lebrun M, 2013, SIAM J IMAGING SCI, V6, P1665, DOI 10.1137/120874989
   Lebrun M, 2015, IEEE T IMAGE PROCESS, V24, P3149, DOI 10.1109/TIP.2015.2439041
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li MY, 2021, IEEE T MULTIMEDIA, V23, P468, DOI 10.1109/TMM.2020.2984092
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Lucic M, 2018, ADV NEUR IN, V31
   Lugmayr A, 2020, IEEE COMPUT SOC CONF, P2058, DOI 10.1109/CVPRW50498.2020.00255
   Lugmayr A, 2019, IEEE INT CONF COMP V, P3575, DOI 10.1109/ICCVW.2019.00442
   Luo ZW, 2021, IEEE COMPUT SOC CONF, P471, DOI 10.1109/CVPRW53098.2021.00058
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Mei YQ, 2020, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR42600.2020.00573
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei P., 2020, P EUR C COMP VIS, P101
   Wei YX, 2021, PROC CVPR IEEE, P13380, DOI 10.1109/CVPR46437.2021.01318
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yan YT, 2022, IEEE T MULTIMEDIA, V24, P1473, DOI 10.1109/TMM.2021.3065731
   Yingxue Pang, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P468, DOI 10.1007/978-3-030-67070-2_28
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 46
TC 5
Z9 5
U1 6
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4752
EP 4763
DI 10.1109/TMM.2022.3181457
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, L
   Li, Z
   Liu, S
   Li, HQ
AF Li, Li
   Li, Zhu
   Liu, Shan
   Li, Houqiang
TI Plenoptic Point Cloud Compression Using Multiview Extension of High
   Efficiency Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiview extension of high efficiency video coding; plenoptic point
   cloud; point cloud compression; rate distortion optimization;
   video-based point cloud compression
ID STANDARDS; TRANSFORM
AB Plenoptic point clouds are more complete representations of three-dimensional (3-D) objects than single-color point clouds, as they can have multiple colors per spatial point, representing colors of each point as seen from different view angles. They are more realistic but also involve a larger volume of data in need of compression. Therefore, in this paper, a multiview-video-based framework is proposed to exploit the correlations in color across different viewpoints to compress plenoptic point clouds efficiently. To the best of the authors' knowledge, this is the first work to exploit correlations in color across different viewpoints using a multiview-video-based framework. In addition, it is observed that some unoccupied pixels, which do not have corresponding points in plenoptic point clouds and are of no use to the quality of the reconstructed plenoptic point cloud colors, may cost many bits. To address this problem, a block-based group smoothing and a combined occupancy-map-based rate distortion optimization and four-neighbor average residual padding are further proposed to reduce the bit cost of unoccupied color pixels. The proposed algorithms are implemented in the moving pictures experts group (MPEG) video-based point cloud compression (V-PCC) and multiview extension of High Efficiency Video Coding (MV-HEVC) reference software. Compared with the V-PCC independently applied to each view direction, the proposed algorithms can provide a BD-rate reduction of over 70%.
C1 [Li, Li; Li, Houqiang] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
   [Li, Zhu] Univ Missouri Kansas City, Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
   [Liu, Shan] Tencent Amer, Palo Alto, CA 94301 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Missouri System; University of Missouri Kansas
   City
RP Li, L (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
EM lil1@ustc.edu.cn; lizhu@umkc.edu; shanl@tencent.com; lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013; huang, shan/JVN-1240-2024; tian,
   ye/KGL-6485-2024
OI , Shan/0000-0002-1442-1207; Li, Zhu/0000-0002-8246-177X
FU Natural Science Foundation of China [62171429, 62021001]; Natural
   Science Foundation [1747751]; USTC Research Funds of the Double
   First-Class Initiative [YD3490002001]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62171429 and 62021001, in part by the Natural Science
   Foundation under Grant 1747751, and in part by USTC Research Funds of
   the Double First-Class Initiative under Grant YD3490002001.
CR [Anonymous], 2022, POINT CLOUD COMPR 2
   [Anonymous], 2022, MPEG PC ERR SOFTW PC
   [Anonymous], 2022, HTM163
   Bjontegaard G., 2001, ITU SG16 VCEG-M33
   Bruder G, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P161, DOI 10.1109/3DUI.2014.6798870
   Budagavi M., 2017, JTC1SC29WG11M41808 I
   Champel M.-L., 2017, P SOFTW PROC IMPR EX, V10396, P183
   Chou PA, 2020, IEEE T IMAGE PROCESS, V29, P2203, DOI 10.1109/TIP.2019.2908095
   Cohen RA, 2016, IEEE DATA COMPR CONF, P141, DOI 10.1109/DCC.2016.67
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3507, DOI 10.1109/TIP.2017.2699922
   de Queiroz RL, 2016, IEEE T IMAGE PROCESS, V25, P3947, DOI 10.1109/TIP.2016.2575005
   Gonçalves M, 2019, IEEE IMAGE PROC, P3726, DOI [10.1109/ICIP.2019.8803524, 10.1109/icip.2019.8803524]
   Graziosi D., 2019, JTC1SC29WG11M46212 I
   Graziosi D., 2018, JTC1SC29WG11M43681 I
   Gu S, 2020, IEEE T IMAGE PROCESS, V29, P796, DOI 10.1109/TIP.2019.2936738
   Hannuksela MM, 2015, IEEE IMAGE PROC, P2154
   He LY, 2017, ASIA-PAC CONF COMMUN, P345
   Kathariya B, 2018, IEEE INT CON MULTI
   Krivokuca M., 2018, ISOIECTC1SC29WG11M42
   Krivokuca M, 2020, INT CONF ACOUST SPEE, P1978, DOI [10.1109/icassp40776.2020.9053862, 10.1109/ICASSP40776.2020.9053862]
   Krivokuca M, 2020, IEEE T IMAGE PROCESS, V29, P2217, DOI 10.1109/TIP.2019.2957853
   Lasserre S., 2017, JTC1SC29WG11M41822 I
   Li L, 2021, IEEE T MULTIMEDIA, V23, P2806, DOI 10.1109/TMM.2020.3016894
   Li L, 2021, IEEE T CIRC SYST VID, V31, P326, DOI 10.1109/TCSVT.2020.2966118
   Li L, 2019, IEEE IMAGE PROC, P3167, DOI [10.1109/icip.2019.8803233, 10.1109/ICIP.2019.8803233]
   Liu H, 2020, IEEE T BROADCAST, V66, P701, DOI 10.1109/TBC.2019.2957652
   Mammou K., 2017, document ISO/IEC JTC1/SC29/WG11 m41649
   Mammou K., 2019, JTC1SC29WG11M46188 I
   Mammou K., 2018, JTC1SC29WG11M4264 IS
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Naik D., 2019, JTC1SC29WG11 ISOIEC
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Park SB, 2009, IEEE T MULTIMEDIA, V11, P177, DOI 10.1109/TMM.2008.2008868
   Hong PN, 2020, IEEE ACCESS, V8, P73804, DOI 10.1109/ACCESS.2020.2987875
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Rhyu S., 2018, JTC1SC29WG11M4366 IS
   Sandri G, 2018, IEEE IMAGE PROC, P1153, DOI 10.1109/ICIP.2018.8451367
   Sandri G, 2019, IEEE T IMAGE PROCESS, V28, P1419, DOI 10.1109/TIP.2018.2877486
   Schnabel R., 2006, P S POINT BAS GRAPH, V6, P111, DOI DOI 10.2312/SPBG/SPBG06/111-120
   Schwarz H, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1929, DOI 10.1109/ICME.2006.262934
   Schwarz S., 2017, JTC1SC29WG11M41779 I
   Schwarz S., 2018, ISO/IEC JTC1/SC29/WG11
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Shao YT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1199, DOI 10.1145/3240508.3240696
   Shao YT, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Stankovic RS, 2003, COMPUT ELECTR ENG, V29, P25, DOI 10.1016/S0045-7906(01)00011-8
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tulvan C., 2016, ISOIECJTC1SC29WG11
   Zakharchenko V., 2019, JTC1SC29G11M45966 IS
   Zhang C, 2014, IEEE IMAGE PROC, P2066, DOI 10.1109/ICIP.2014.7025414
   Zhang X, 2019, IEEE J EM SEL TOP C, V9, P163, DOI 10.1109/JETCAS.2018.2883479
NR 51
TC 4
Z9 4
U1 2
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2007
EP 2021
DI 10.1109/TMM.2022.3142528
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100033
DA 2024-07-18
ER

PT J
AU Lin, SD
   Tang, F
   Dong, WM
   Pan, XJ
   Xu, CS
AF Lin, Shideng
   Tang, Fan
   Dong, Weiming
   Pan, Xingjia
   Xu, Changsheng
TI SMNet: Synchronous Multi-Scale Low Light Enhancement Network With Local
   and Global Concern
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Low-light image enhancement; multi-scale feature learning; deep-learning
ID ADAPTIVE HISTOGRAM EQUALIZATION; IMAGE QUALITY ASSESSMENT; ALGORITHM
AB Limited by objectively poor lighting conditions and hardware devices, low-light images with low visual quality and low visibility are inevitable in the real world. Accurate local details and reasonable global information play their essential and distinct roles in low-light image enhancement: local details contribute to fine textures, while global information is critical for a proper understanding of the global brightness level. In this article, we focus on integrating local and global aspects to achieve high-quality low-light image enhancement by proposing the synchronous multi-scale low-light enhancement network (SMNet). A synchronous multi-scale representation learning structure and a global feature recalibration module are adopted in SMNet. Different from the traditional multi-scale feature learning architecture, SMNet carries out the multi-scale representation learning in a synchronous way: we first calculate the rough contextual representations in a top-down manner and then learn multi-scale representations in a bottom-up way to generate representations with rich local details. To acquire global brightness information, a global feature recalibration module (GFRM) is applied after the synchronous multi-scale representations to perceive and exploit proper global information by global pooling and projection to recalibrate channel weights globally. The synchronous multi-scale representation and GFRM compose the basic local-and-global block. Experimental results on mainstream real-world dataset LOL and synthetic dataset MIT-Adobe FiveK show that the proposed SMNet not only leads the way on objective metrics (0.41/2.31 improvement of PSNR on two datasets) but is also superior in subjective comparisons compared with typical SoTA methods.
C1 [Lin, Shideng; Dong, Weiming; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
   [Lin, Shideng; Dong, Weiming; Xu, Changsheng] Chinese Acad Sci, Inst Automat, NLPR, Beijing 101408, Peoples R China.
   [Tang, Fan] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Pan, Xingjia] Tencent, Youtu Lab, Shanghai 200001, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Institute of Computing Technology, CAS; Tencent
RP Tang, F (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
EM linshideng2019@ia.ac.cn; tangfan@ict.ac.cn; weiming.dong@ia.ac.cn;
   xjia.pan@gmail.com; csxu@nlpr.ia.ac.cn
RI DONG, Weiming/AAG-7678-2020; Tang, Fan/O-3923-2018; xu, cj/HJZ-3488-2023
OI DONG, Weiming/0000-0001-6502-145X; Tang, Fan/0000-0002-3975-2483; xu,
   chang sheng/0000-0001-8343-9665
FU Beijing Natural Science Foundation
FX No Statement Available
CR Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Huang T, 2021, PROC CVPR IEEE, P14776, DOI 10.1109/CVPR46437.2021.01454
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Kumar K, 2018, MULTIMED TOOLS APPL, V77, P26635, DOI 10.1007/s11042-018-5882-z
   Lee CH, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P43, DOI 10.1109/SITIS.2013.19
   Lee H, 2019, IEEE I CONF COMP VIS, P1854, DOI 10.1109/ICCV.2019.00194
   Liang JX, 2020, Arxiv, DOI arXiv:2007.02018
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Mao XJ, 2016, ADV NEUR IN, V29
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Paszke Adam, 2017, NIPS W
   Pisano ED, 1998, J DIGIT IMAGING, V11, P193, DOI 10.1007/BF03178082
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Simonyan K., 2014, CORR
   Singh M, 2019, IET COMPUT VIS, V13, P578, DOI 10.1049/iet-cvi.2018.5814
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wei C., 2018, P 29 BRIT MACH VIS C, P1
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu RK, 2021, IEEE COMPUT SOC CONF, P414, DOI 10.1109/CVPRW53098.2021.00052
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Yang MJ, 2022, IEEE T NEUR NET LEAR, V33, P6402, DOI 10.1109/TNNLS.2021.3079627
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Ying ZQ, 2017, IEEE INT CONF COMP V, P3015, DOI 10.1109/ICCVW.2017.356
   Yun-Chun Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P442, DOI 10.1007/978-3-030-58523-5_26
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zhang AR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3356019
   Zhang DY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3398685
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang L, 2012, IEEE IMAGE PROC, P1477, DOI 10.1109/ICIP.2012.6467150
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang Q, 2019, COMPUT GRAPH FORUM, V38, P243, DOI 10.1111/cgf.13833
   Zhang Y., 2020, arXiv
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhou D., 2020, P COMP VIS ECCV 2020, P680, DOI DOI 10.1007/978-3-030-58580-8_40
   Zhu MF, 2020, AAAI CONF ARTIF INTE, V34, P13106
NR 75
TC 3
Z9 3
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9506
EP 9517
DI 10.1109/TMM.2023.3254141
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200019
DA 2024-07-18
ER

PT J
AU Liu, K
   Xue, F
   Guo, D
   Sun, PJ
   Qian, SS
   Hong, RC
AF Liu, Kang
   Xue, Feng
   Guo, Dan
   Sun, Peijie
   Qian, Shengsheng
   Hong, Richang
TI Multimodal Graph Contrastive Learning for Multimedia-Based
   Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaboration; Visualization; Contamination; Convolution; Neural
   networks; Feature extraction; Electronic mail; Recommender system;
   collaborative filtering; multimodal user preference; graph convolution
   network; contrastive learning
ID NETWORK
AB Multimedia-based recommendation is a challenging task that requires not only learning collaborative signals from user-item interaction, but also capturing modality-specific user interest clues from complex multimedia content. Though significant progress on this challenge has been made, we argue that current solutions remain limited by multimodal noise contamination. Specifically, a considerable proportion of multimedia content is irrelevant to the user preference, such as the background, overall layout, and brightness of images; the word order and semantic-free words in titles; etc. We take this irrelevant information as noise contamination to discover user preferences. Moreover, most recent research has been conducted by graph learning. This means that noise is diffused into the user and item representations with the message propagation; the contamination influence is further amplified. To tackle this problem, we develop a novel framework named Multimodal Graph Contrastive Learning (MGCL), which captures collaborative signals from interactions and uses visual and textual modalities to respectively extract modality-specific user preference clues. The key idea of MGCL involves two aspects: First, to alleviate noise contamination during graph learning, we construct three parallel graph convolution networks to independently generate three types of user and item representations, containing collaborative signals, visual preference clues, and textual preference clues. Second, to eliminate as much preference-independent noisy information as possible from the generated representations, we incorporate sufficient self-supervised signals into the model optimization with the help of contrastive learning, thus enhancing the expressiveness of the user and item representations. Extensive experiments validate the effectiveness and scalability of MGCL at https://github.com/hfutmars/MGCL.
C1 [Liu, Kang; Guo, Dan; Hong, Richang] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230601, Peoples R China.
   [Xue, Feng] Minist Educ, Key Lab Knowledge Engn Big Data, Hefei, Peoples R China.
   [Xue, Feng] Hefei Univ ofTechnol, Intelligent Interconnected SystemsLaboratory Anhui, Hefei 230601, Peoples R China.
   [Xue, Feng] Hefei Univ Technol, Sch Software, Hefei 230601, Peoples R China.
   [Sun, Peijie] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Qian, Shengsheng] Chinese Acad Sci, Inst Automation, Beijing 100040, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology; Hefei
   University of Technology; Tsinghua University; Chinese Academy of
   Sciences; Institute of Automation, CAS
RP Xue, F (corresponding author), Minist Educ, Key Lab Knowledge Engn Big Data, Hefei, Peoples R China.; Xue, F (corresponding author), Hefei Univ ofTechnol, Intelligent Interconnected SystemsLaboratory Anhui, Hefei 230601, Peoples R China.; Xue, F (corresponding author), Hefei Univ Technol, Sch Software, Hefei 230601, Peoples R China.
EM kangliu1225@gmail.com; feng.xue@hfut.edu.cn; guodan@hfut.edu.cn;
   sun.hfut@gmail.com; shengsheng.qian@nlpr.ia.ac.cn; hongrc.hfut@gmail.com
RI Sun, Peijie/AAY-8158-2020
OI Sun, Peijie/0000-0001-9733-0521; Xue, Feng/0000-0003-4962-9734; liu,
   kang/0000-0001-6789-1811; Guo, Dan/0000-0003-2594-254X
FU National Natural Science Foundation of China
FX No Statement Available
CR Cai DS, 2022, IEEE T MULTIMEDIA, V24, P805, DOI 10.1109/TMM.2021.3059508
   Chen C, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1583, DOI 10.1145/3178876.3186070
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XS, 2022, IEEE T MULTIMEDIA, V24, P506, DOI 10.1109/TMM.2021.3054525
   Du Xiaoyu, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P619, DOI 10.1145/3503161.3548405
   Gao XY, 2020, IEEE T MULTIMEDIA, V22, P1647, DOI 10.1109/TMM.2019.2945180
   Geng X, 2015, IEEE I CONF COMP VIS, P4274, DOI 10.1109/ICCV.2015.486
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   He XN, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P639, DOI 10.1145/3397271.3401063
   He XN, 2018, ACM/SIGIR PROCEEDINGS 2018, P355, DOI 10.1145/3209978.3209981
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Kang WC, 2017, IEEE DATA MINING, P207, DOI 10.1109/ICDM.2017.30
   Kim D, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P233, DOI 10.1145/2959100.2959165
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Konstan J., 2001, P 10 INT C WORLD WID, P285
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lei CY, 2016, PROC CVPR IEEE, P2545, DOI 10.1109/CVPR.2016.279
   Liu DH, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P344, DOI 10.1145/3292500.3330906
   Liu Fan, 2023, IEEE Transactions on Multimedia, P7149, DOI 10.1109/TMM.2022.3217449
   Liu HT, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P1233, DOI 10.1145/3331184.3331371
   Liu K, 2023, ACM T INFORM SYST, V41, DOI 10.1145/3544106
   Liu K, 2022, INTELL DATA ANAL, V26, P427, DOI 10.3233/IDA-205725
   Liu K, 2023, IEEE T COMPUT SOC SY, V10, P72, DOI 10.1109/TCSS.2022.3151822
   Mao KL, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P1253, DOI 10.1145/3459637.3482291
   Min WQ, 2020, IEEE T MULTIMEDIA, V22, P2659, DOI 10.1109/TMM.2019.2958761
   Ni JM, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P188
   Niu W, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P423, DOI 10.1145/3159652.3159728
   Rendle S., 2009, P 25 C UNC ART INT, P452, DOI DOI 10.5555/1795114.1795167
   Salakhutdinov Ruslan, 2008, ADV NEURAL INFORM PR, P1257, DOI DOI 10.5555/2981562.2981720
   Shuai J, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P1283, DOI 10.1145/3477495.3531927
   Tao ZL, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2020.102277
   Truong QT, 2021, 15TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS 2021), P834, DOI 10.1145/3460231.3473324
   Wang S., 2021, P 30 INT JOINT C ART, P4644, DOI 10.24963/ijcai.2021/630
   Wang SH, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P391, DOI 10.1145/3038912.3052638
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wei YW, 2022, IEEE T MULTIMEDIA, V24, P2701, DOI 10.1109/TMM.2021.3088307
   Wei YW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3541, DOI 10.1145/3394171.3413556
   Wei YW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5382, DOI 10.1145/3474085.3475665
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Wu CH, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4884
   Wu F, 2019, PR MACH LEARN RES, V97
   Wu JC, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P726, DOI 10.1145/3404835.3462862
   Wu L, 2023, IEEE T KNOWL DATA EN, V35, P4425, DOI 10.1109/TKDE.2022.3145690
   Wu L, 2020, IEEE T KNOWL DATA EN, V32, P1854, DOI 10.1109/TKDE.2019.2913394
   Xue F, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3314578
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yang YH, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P71, DOI 10.1145/3404835.3462928
   Yao TS, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P4321, DOI 10.1145/3459637.3481952
   Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890
   Zhang FZ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P353, DOI 10.1145/2939672.2939673
   Zheng L, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P425, DOI 10.1145/3018661.3018665
   Zhu YQ, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2069, DOI 10.1145/3442381.3449802
NR 55
TC 1
Z9 1
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9343
EP 9355
DI 10.1109/TMM.2023.3251108
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300014
DA 2024-07-18
ER

PT J
AU Nie, J
   Zhao, Z
   Huang, L
   Nie, WZ
   Wei, ZQ
AF Nie, Jie
   Zhao, Zian
   Huang, Lei
   Nie, Weizhi
   Wei, Zhiqiang
TI Cross-Domain Recommendation Via User-Clustering and Multidimensional
   Information Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaboration; Fuses; Graph neural networks; Robustness; Representation
   learning; Deep learning; Codes; Attention mechanism; cross-domain
   recommendation; user-group modeling
AB Recently, recommendation systems have been widely usedin online business scenarios, which can improve the online experience by learning the user or item characteristics to predict the user's future behavior and to realize precision marketing. However, data sparsity and cold-start problems limit the performance of recommendation systems in some emerging fields. Thus, cross-domain recommendation has been proposed to handle the abovementioned problems. Nonetheless, many cross-domain recommendations only consider modeling a single user's representation and ignore user-group information (this group has similar behavior and interests). Additionally, most studies are based on matrix factorization for generating embeddings, which results in a weak generalization ability of user latent features. In this paper, we propose a novel cross-domain recommendation model via User-Clustering and Multidimensional information Fusion (UCMF) that attempts to enhance user representation learning in a data sparsity scenario for accurate recommendation. In addition, we consider a user's individual information and cross-domain feature information. A novel multidimensional information fusion is proposed to guarantee the robustness of the user features. In particular, we apply a graph neural network to learn the user-group features, which can effectively save the correlation among users' information and guarantee feature performance. In other words, the Wasserstein autoencoder is utilized to learn the cross-domain user features, which can guarantee the consistency of user features from different domains. Experiments conducted on real-world datasets empirically demonstrate that our proposed method outperforms the state-of-the-art methods in cross-domain recommendation.
C1 [Nie, Jie; Zhao, Zian; Huang, Lei; Wei, Zhiqiang] Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
   [Nie, Weizhi] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Ocean University of China; Tianjin University
RP Zhao, Z (corresponding author), Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.; Nie, WZ (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM niejie@ouc.edu.cn; zza@stu.ouc.edu.cn; huangl@ouc.edu.cn;
   weizhinie@tju.edu.cn; weizhiqiang@ouc.edu.cn
RI Nie, Jie/ABG-9228-2021; wei, zhiqiang/M-8868-2013
OI Nie, Jie/0000-0003-4952-7666; nie, weizhi/0000-0002-0578-8138; Huang,
   Lei/0000-0003-4087-3677
FU National Natural Science Foundation of China [61702471, 61872326]; Key
   Research and Development Program of Qingdao Science and Technology Plan
   [21-1-2-18-xx]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61702471 and 61872326 and in part by
   the Key Research and Development Program of Qingdao Science and
   Technology Plan (21-1-2-18-xx).
CR [Anonymous], 2013, P INT C MACH LEARN R
   Bach Francis R., 2005, PROBABILISTIC INTERP
   Bobadilla J, 2012, INFORM SCIENCES, V185, P1, DOI 10.1016/j.ins.2011.09.014
   Bouchard G., 2013, P 16 INT C ARTIFICIA, P144
   Chen W, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P892
   Chen X, 2022, Arxiv, DOI arXiv:2009.06884
   Chen ZH, 2020, NEURAL COMPUT APPL, V32, P7489, DOI 10.1007/s00521-019-04262-1
   Deng ZH, 2019, AAAI CONF ARTIF INTE, P61
   Fang Y., 2011, P 2 INT WORKSH INF H, P65, DOI DOI 10.1145/2039320.2039330
   Gao C, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P491, DOI 10.1145/3308558.3313538
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   He XN, 2018, Arxiv, DOI arXiv:1808.03912
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Hu GN, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P667, DOI 10.1145/3269206.3271684
   Hu Liang, 2013, P 22 INT C WORLD WID, P595, DOI DOI 10.1145/2488388.2488441
   Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22
   Huang SR, 2017, IEEE T MULTIMEDIA, V19, P1314, DOI 10.1109/TMM.2017.2652074
   Jin BW, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P659, DOI 10.1145/3397271.3401072
   Khan MM, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3073565
   Kingma D. P., 2014, arXiv
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Koren Y, 2011, RECOMMENDER SYSTEMS HANDBOOK, P145, DOI 10.1007/978-0-387-85820-3_5
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Li B., 2011, P 22 INT JOINT C ART, P2293
   Li P, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P331, DOI 10.1145/3336191.3371793
   Lian JX, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P817, DOI 10.1145/3041021.3054207
   Liang DW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P689, DOI 10.1145/3178876.3186150
   Man T, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2464
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Paszkeet A., 2017, AUTOMATIC DIFFERENTI
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Rendle S, 2012, Arxiv, DOI arXiv:1205.2618
   Salakhutdinov R., 2008, ICML, P880, DOI DOI 10.1145/1390156.1390267
   Salakhutdinov Ruslan, 2008, ADV NEURAL INFORM PR, P1257, DOI DOI 10.5555/2981562.2981720
   Shi C, 2021, IEEE T KNOWL DATA EN, V33, P1413, DOI 10.1109/TKDE.2019.2941938
   Singh A. P., 2008, P 14 ACM SIGKDD INT, P650
   Sun Z, 2019, ELECTRON COMMER R A, V37, DOI 10.1016/j.elerap.2019.100879
   Sutskever I, 2014, ADV NEUR IN, V27
   Tang Jie, 2012, P 18 ACM SIGKDD INT, P1285, DOI DOI 10.1145/2339530.2339730
   Tolstikhin I., 2017, arXiv
   Vaswani A, 2023, Arxiv, DOI [arXiv:1706.03762, DOI 10.48550/ARXIV.1706.03762, 10.48550/arXiv.1706.03762]
   Villani C., 2003, TOPICS OPTIMAL TRANS, DOI DOI 10.1090/GSM/058
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wang X, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P950, DOI 10.1145/3292500.3330989
   Wu Y, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P153, DOI 10.1145/2835776.2835837
   Xin X, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1827
   Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890
   Yuan F, 2019, Arxiv, DOI arXiv:1905.10760
   Zhao C, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P2165, DOI 10.1145/3357384.3358166
   Zhao F., 2016, P 30 INT JOINT C ART, P2385
   Zhao GS, 2019, IEEE T MULTIMEDIA, V21, P771, DOI 10.1109/TMM.2018.2863598
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zhong ST, 2022, IEEE T CYBERNETICS, V52, P5229, DOI 10.1109/TCYB.2020.3029002
   Zhu F, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3001
   Zhu F, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1533, DOI 10.1145/3357384.3357992
NR 55
TC 1
Z9 1
U1 6
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 868
EP 880
DI 10.1109/TMM.2021.3134161
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900015
DA 2024-07-18
ER

PT J
AU Piao, YR
   Wu, W
   Zhang, M
   Jiang, YY
   Lu, HC
AF Piao, Yongri
   Wu, Wei
   Zhang, Miao
   Jiang, Yongyao
   Lu, Huchuan
TI Noise-Sensitive Adversarial Learning for Weakly Supervised Salient
   Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adversarial learning; salient object detection; weakly supervised
   learning
ID NETWORK
AB Weakly supervised salient object detection (WSOD) aims at training saliency detection models with weak supervision. Normally, the WSOD methods use pseudo labels converted from image-level classification labels to train the saliency network. However, the converted pseudo labels always contain noise information compared to ground truth. Previous methods are directly affected by pseudo label noise to generate error-prone predictions. To mitigate this problem, we design a noise-robust adversarial learning framework and propose a noise-sensitive training strategy for the framework. The framework consists of a saliency network and a noise-robust discriminator network. With the guidance of noise-robust discriminator network, our saliency network is robust to noise information in pseudo labels. The proposed noise-sensitive training strategy can make good use of both superior and inferior samples in the pseudo label dataset. With the noise-sensitive training strategy, our framework can further balance the learning of saliency information and the robustness of noise information. Comprehensive experiments on five public datasets demonstrate that our method outperforms the existing image-level classification label based WSOD methods.
C1 [Piao, Yongri; Wu, Wei; Jiang, Yongyao; Lu, Huchuan] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
   [Zhang, Miao] Dalian Univ Technol, DUT RU Int Sch Informat Science& Engn, Key Lab Ubiquitous Network & Serv Software Liaonin, Dalian 116024, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Zhang, M (corresponding author), Dalian Univ Technol, DUT RU Int Sch Informat Science& Engn, Key Lab Ubiquitous Network & Serv Software Liaonin, Dalian 116024, Peoples R China.
EM yrpiao@dlut.edu.cn; wwei@mail.dlut.edu.cn; miaozhang@dlut.edu.cn;
   jiangyy@mail.dlut.edu.cn; lhchuan@dlut.edu.cn
RI LU, Jia-Hong/X-1395-2019
OI LU, Jia-Hong/0000-0002-1147-125X; Piao, Yongri/0000-0002-0860-252X
FU National Natural Science Foundation of China [62172070, 61976035];
   Central Guidance on Local Science and Technology Development Foundation
   of Liaoning [2022JH6/100100028]; Natural Science Foundation of Liaoning
   [2021-MS-123]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172070 and 61976035, and the Central
   Guidance on Local Science and Technology Development Foundation of
   Liaoning under Grant 2022JH6/100100028, and in part by the Natural
   Science Foundation of Liaoning under Grant 2021-MS-123.& nbsp;
CR Achanta R., 2010, SLIC SUPERPIXELSTECH
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   [Anonymous], 2016, NIPS Workshop on Adversarial Training
   Borji A., 2012, 2012 IEEE COMPUTER S, P23, DOI 10.1109/CVPRW.2012.6239191
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P742, DOI 10.1109/TIP.2014.2383320
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hendrycks D., 2019, PROC C WORKSHOP NEUR, P10456
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hung WC, 2018, ARXIV PREPRINT ARXIV
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Li GB, 2018, AAAI CONF ARTIF INTE, P7024
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin X, 2019, IEEE T MULTIMEDIA, V21, P1646, DOI 10.1109/TMM.2018.2884474
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Lu P, 2021, IEEE T MULTIMEDIA, V23, P3618, DOI 10.1109/TMM.2020.3029882
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Ma F, 2022, IEEE T NEUR NET LEAR, V33, P6275, DOI 10.1109/TNNLS.2021.3073248
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Natarajan N., 2013, ADV NEURAL INFORM PR, P1196, DOI DOI 10.5555/2999611.2999745
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Radford A., 2016, ICLR
   Radford A., 2015, COMPUTER SCI
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Tang YB, 2019, IEEE T MULTIMEDIA, V21, P2237, DOI 10.1109/TMM.2019.2900908
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu SY, 2021, AAAI CONF ARTIF INTE, V35, P3234
   Zeng Y, 2019, PROC CVPR IEEE, P6067, DOI 10.1109/CVPR.2019.00623
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   Zhang Jing, 2020, P IEEECVF C COMPUTER, P12546, DOI DOI 10.1109/CVPR42600.2020.01256
   Zhang QJ, 2021, IEEE T IMAGE PROCESS, V30, P1305, DOI 10.1109/TIP.2020.3042084
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhang Y., 2020, IEEE T MULTIMEDIA, V23, P4232
   Zhou H., 2020, P IEEE C COMP VIS PA, P9141, DOI 10.1109/CVPR42600.2020.00916
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
   Zhu Y, 2019, PROC CVPR IEEE, P3111, DOI 10.1109/CVPR.2019.00323
   Zhuge YZ, 2018, IEEE SIGNAL PROC LET, V25, P1800, DOI 10.1109/LSP.2018.2875586
NR 65
TC 13
Z9 13
U1 4
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2888
EP 2897
DI 10.1109/TMM.2022.3152567
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600034
DA 2024-07-18
ER

PT J
AU Qian, JJ
   Zhu, SM
   Zhao, CY
   Yang, J
   Wong, WK
AF Qian, Jianjun
   Zhu, Shumin
   Zhao, Chaoyu
   Yang, Jian
   Wong, Wai Keung
TI OTFace: Hard Samples Guided Optimal Transport Loss for Deep Face
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Training; Task analysis; Neural networks; Facial
   features; Representation learning; Generators; Face representation;
   margin based softmax; Optimal transport
ID RECOGNITION
AB Face representation in the wild is extremely hard due to the large scale face variations. Some deep convolutional neural networks (CNNs) have been developed to learn discriminative feature by designing properly margin-based losses, which perform well on easy samples but fail on hard samples. Although some methods mainly adjust the weights of hard samples in training stage to improve the feature discrimination, they overlook the distribution property of feature. It is worth noting that the miss-classified hard samples may be corrected from the feature distribution view. To overcome this problem, this paper proposes the hard samples guided optimal transport (OT) loss for deep face representation, OTFace in short. OTFace aims to enhance the performance of hard samples by introducing the feature distribution discrepancy while maintaining the performance on easy samples. Specifically, we embrace triplet scheme to indicate hard sample groups in one mini-batch during training. OT is then used to characterize the distribution differences of features from the high level convolutional layer. Finally, we integrate the margin-based-softmax (e.g. ArcFace or AM-Softmax) and OT together to guide deep CNN learning. Extensive experiments were conducted on several benchmark databases. The quantitative results demonstrate the advantages of the proposed OTFace over state-of-the-art methods.
C1 [Qian, Jianjun; Zhao, Chaoyu; Yang, Jian] Syst High Dimens Informat Minist Educ, PCA Lab, Key Lab Intelligent Percept, Nanjing, Jiangsu, Peoples R China.
   [Qian, Jianjun; Zhao, Chaoyu; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Social, Nanjing 210094, Jiangsu, Peoples R China.
   [Zhu, Shumin; Wong, Wai Keung] Hong Kong Polytech Univ, Sch Fash & Text, Hong Kong, Peoples R China.
   [Wong, Wai Keung] Lab Artificial Intelligence Design, Hong Kong, Peoples R China.
C3 Nanjing University of Science & Technology; Hong Kong Polytechnic
   University
RP Wong, WK (corresponding author), Hong Kong Polytech Univ, Sch Fash & Text, Hong Kong, Peoples R China.; Wong, WK (corresponding author), Lab Artificial Intelligence Design, Hong Kong, Peoples R China.
EM csjqian@njust.edu.cn; 21122693r@connect.polyu.hk; cyzhao@njust.edu.cn;
   csjyang@njust.edu.cn; calvin.wong@polyu.edu.hk
RI li, xiaomin/KCX-9845-2024
OI Zhu, Shumin/0000-0003-4050-9592; Lai, Zhihui/0000-0002-4388-3080; Wong,
   Wai Keung/0000-0002-5214-7114; Qian, Jianjun/0000-0002-0968-8556
FU National Science Fund of China [62176124, 61876083]
FX This work was supported by the National Science Fund of China under
   Grants 62176124 and 61876083. The Guest Editor coordinating the review
   of this manuscript and approving it for publication was Dr. Cairong
   Zhao.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chen BH, 2017, PROC CVPR IEEE, P4021, DOI 10.1109/CVPR.2017.428
   Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46
   Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
   Cuturi M., 2013, ADV NEURAL INFORM PR, P2292, DOI DOI 10.48550/ARXIV.1306.0895
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2017, IEEE COMPUT SOC CONF, P2006, DOI 10.1109/CVPRW.2017.251
   Deng WH, 2019, IEEE T PATTERN ANAL, V41, P758, DOI 10.1109/TPAMI.2018.2800008
   Duan QY, 2022, IEEE T CIRC SYST VID, V32, P3761, DOI 10.1109/TCSVT.2021.3111648
   Duan QY, 2021, IEEE T NEUR NET LEAR, V32, P214, DOI 10.1109/TNNLS.2020.2978127
   Flamary R, 2018, MACH LEARN, V107, P1923, DOI 10.1007/s10994-018-5717-1
   Genevay A, 2018, PR MACH LEARN RES, V84
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He M., 2022, IEEECVF C COMPUTER V, P4062
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Huang YG, 2020, PROC CVPR IEEE, P5900, DOI 10.1109/CVPR42600.2020.00594
   Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527
   Khalid SS, 2023, IEEE T PATTERN ANAL, V45, P15249, DOI 10.1109/TPAMI.2022.3162705
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lei Z, 2014, IEEE T PATTERN ANAL, V36, P289, DOI 10.1109/TPAMI.2013.112
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Liu H, 2019, PROC CVPR IEEE, P11939, DOI 10.1109/CVPR.2019.01222
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu WY, 2016, PR MACH LEARN RES, V48
   Lu JW, 2015, IEEE T PATTERN ANAL, V37, P2041, DOI 10.1109/TPAMI.2015.2408359
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   Meng Q, 2021, PROC CVPR IEEE, P14220, DOI 10.1109/CVPR46437.2021.01400
   Mengxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13933, DOI 10.1109/CVPR42600.2020.01395
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Phan H., 2022, IEEE C COMP VIS PATT, P20227
   Qian JJ, 2022, IEEE T CYBERNETICS, V52, P1553, DOI 10.1109/TCYB.2020.2991219
   Ren C.-X., 2022, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2022.319064, DOI 10.1109/TPAMI.2022.319064]
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sengupta S., 2016, 2016 IEEE WINT C APP, P1, DOI [DOI 10.1109/WACV.2016.7477558, 10.1109/WACV.2016.7477558]
   Shi YC, 2019, IEEE I CONF COMP VIS, P6901, DOI 10.1109/ICCV.2019.00700
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun JN, 2020, IEEE T MULTIMEDIA, V22, P2833, DOI 10.1109/TMM.2020.2966863
   Sun Y, 2015, Arxiv, DOI arXiv:1502.00873
   Sun Y, 2014, ADV NEUR IN, V27
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang XB, 2020, AAAI CONF ARTIF INTE, V34, P12241
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Whitelam C, 2017, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2017.87
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xie SF, 2010, IEEE T IMAGE PROCESS, V19, P1349, DOI 10.1109/TIP.2010.2041397
   Yang J, 2007, IEEE T PATTERN ANAL, V29, P650, DOI 10.1109/TPAMI.2007.1008
   Yang J, 2017, IEEE T PATTERN ANAL, V39, P156, DOI 10.1109/TPAMI.2016.2535218
   Yang J, 2013, IEEE T NEUR NET LEAR, V24, P1023, DOI 10.1109/TNNLS.2013.2249088
   Zhang L, 2021, IEEE T CYBERNETICS, V51, P5883, DOI 10.1109/TCYB.2019.2959403
   Zhang N., 2007, Tech. Rep. 07-49, P7
   Zhang X, 2019, PROC CVPR IEEE, P10815, DOI 10.1109/CVPR.2019.01108
   Zhao CR, 2022, IEEE T CIRC SYST VID, V32, P7047, DOI 10.1109/TCSVT.2022.3179441
   Zhao CR, 2021, IEEE T IMAGE PROCESS, V30, P4212, DOI 10.1109/TIP.2021.3070182
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zhao WL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9867, DOI 10.1109/ICCV48922.2021.00974
   Zheng T., 2018, Tech. Rep., V5
   Zheng TY, 2017, Arxiv, DOI arXiv:1708.08197
   Zhong YY, 2022, IEEE T MULTIMEDIA, V24, P1186, DOI 10.1109/TMM.2021.3123478
NR 69
TC 3
Z9 3
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1427
EP 1438
DI 10.1109/TMM.2022.3230331
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Rahmati, M
   Qi, ZR
   Pompili, D
AF Rahmati, Mehdi
   Qi, Zhuoran
   Pompili, Dario
TI Underwater Adaptive Video Transmissions Using MIMO-Based
   Software-Defined Acoustic Modems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Static VAr compensators; Video recording; Scalability;
   Quality assessment; MIMO communication; Channel coding; Scalable video
   coding; software-defined modem; underwater acoustic MIMO communications;
   video transmissions
AB Achieving reliable acoustic wireless video transmissionsin the extreme and uncertain underwater environment is a challenge due to the limited bandwidth and the error-prone nature of the channel. Aiming at optimizing the received video quality and the user's experience, an adaptive solution for underwater video transmissions is proposed that is specifically designed for Multi-Input Multi-Output (MIMO)-based Software-Defined Acoustic Modems (SDAMs). To keep the video distortion under an acceptable threshold and to keep the Physical-Layer Throughput (PLT) high, cross-layer techniques utilizing diversity-spatial multiplexing and Unequal Error Protection (UEP) are presented along with the scalable video compression at the application layer. Specifically, the scalability of the utilized SDAM with high processing capabilities is exploited in the proposed structure along with the temporal, spatial, and quality scalability of the Scalable Video Coding (SVC) H.264/MPEG-4 AVC compression standard. The transmitter broadcasts one video stream and realizes multicasting at different users. Experimental results at the Sonny Werblin Recreation Center, Rutgers University-NJ, are presented. Several scenarios for unknown channels at the transmitter are experimentally considered when the hydrophones are placed in different locations in the pool to achieve the required SVC-based video Quality of Service (QoS) and Quality of Experience (QoE) given the channel state information and the robustness of different SVC scalability. The video quality level is determined by the best communication link while the transmission scheme is decided based on the worst communication link, which guarantees that each user is able to receive the video with appropriate quality.
C1 [Rahmati, Mehdi] Cleveland State Univ, Dept Elect Engn & Comp Sci, Cleveland 44106, OH USA.
   [Rahmati, Mehdi; Qi, Zhuoran; Pompili, Dario] Rutgers Univ New Brunswick, Dept Elect & Comp Engn ECE, New Brunswick 08854, NJ USA.
C3 University System of Ohio; Cleveland State University; Rutgers
   University System; Rutgers University New Brunswick
RP Rahmati, M (corresponding author), Cleveland State Univ, Dept Elect Engn & Comp Sci, Cleveland 44106, OH USA.; Rahmati, M (corresponding author), Rutgers Univ New Brunswick, Dept Elect & Comp Engn ECE, New Brunswick 08854, NJ USA.
EM mehdi_rahmati@cac.rutgers.edu; zhuoran.qi@rutgers.edu;
   pompili@cac.rutgers.edu
RI Qi, Zhuoran/ADX-6317-2022
OI Qi, Zhuoran/0000-0002-2992-9687
FU NSF NeTS [CNS-1763964]
FX This work was supported by NSF NeTS Award No. CNS-1763964.
CR Ahamed Imtiaz, 2017, 2017 2nd International Conference on Communication and Electronics Systems (ICCES). Proceedings, P47, DOI 10.1109/CESYS.2017.8321136
   Amdouni N., 2011, Proceedings of the 2011 Mediterranean Microwave Symposium (MMS), P169, DOI 10.1109/MMS.2011.6068554
   Babich F., 2015, Proceedings of Vehicular Technology Conference (VTC Spring), P1
   bluerobotics, BLUE ROB
   Chen Baozhi., 2010, Sensor Mesh and Ad Hoc Communications and Networks (SECON), 2010 7th Annual IEEE Communications Society Conference on, P1
   Chen X, 2017, IEEE T CIRC SYST VID, V27, P366, DOI 10.1109/TCSVT.2015.2511815
   Chen YG, 2015, IEEE J OCEANIC ENG, V40, P159, DOI 10.1109/JOE.2014.2304254
   Chong JH, 2014, HUM-CENTRIC COMPUT I, V4, DOI 10.1186/s13673-014-0002-1
   Demirors E., 2016, PROC ACM INT C UNDER, P12
   Demirors E, 2015, IEEE COMMUN MAG, V53, P64, DOI 10.1109/MCOM.2015.7321973
   Diamant R, 2017, IEEE T WIREL COMMUN, V16, P8037, DOI 10.1109/TWC.2017.2756055
   ettus, USRP XSERIES
   Guo Y, 2009, IEEE T CIRC SYST VID, V19, P781, DOI 10.1109/TCSVT.2009.2017311
   Jubran M. K., 2006, PROC MILCOM IEEE MIL, P1
   Jubran MK, 2008, IEEE T MULTIMEDIA, V10, P1698, DOI 10.1109/TMM.2008.2007317
   Kalachikov Alexander A., 2018, 2018 XIV International Scientific-Technical Conference on Actual Problems of Electronics Instrument Engineering (APEIE). Proceedings, P180, DOI 10.1109/APEIE.2018.8545229
   Khalek AA, 2012, IEEE J SEL AREA COMM, V30, P1157, DOI 10.1109/JSAC.2012.120802
   Hue LDT, 2018, 2018 IEEE 12TH INTERNATIONAL SYMPOSIUM ON EMBEDDED MULTICORE/MANY-CORE SYSTEMS-ON-CHIP (MCSOC 2018), P69, DOI 10.1109/MCSoC2018.2018.00023
   Lee JS, 2011, IEEE T MULTIMEDIA, V13, P882, DOI 10.1109/TMM.2011.2157333
   live.ece, LAB IM VID ENG U TEX
   Narasimhan R, 2006, IEEE T INFORM THEORY, V52, P3965, DOI 10.1109/TIT.2006.880057
   Pérez J, 2005, IEEE COMMUN LETT, V9, P961, DOI 10.1109/LCOMM.2005.11001
   Pompili D, 2010, IEEE T WIREL COMMUN, V9, P2924, DOI 10.1109/TWC.2010.062910.100137
   Rahmati M., 2015, Proceedings of ACM International Conference on Underwater Networks and Systems (WUWNET), P1
   Rahmati M., 2018, Proceedings of OCEANS Conference, P1, DOI DOI 10.1109/OCEANS.2018.8604782
   Rahmati M., 2019, Proceedings of International Conference on Sensing, Communication, and Networking (SECON), P1
   Rahmati M, 2019, IEEE J OCEANIC ENG, V44, P881, DOI 10.1109/JOE.2019.2910940
   Rahmati M, 2018, IEEE J OCEANIC ENG, V43, P777, DOI 10.1109/JOE.2017.2731061
   Rahmati M, 2017, IEEE INT CONF MOB, P180, DOI 10.1109/MASS.2017.71
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shahhosseini S., 2010, PROC INT C COMPUT AP, P537
   Singh KD, 2011, IEEE ICC
   Song D, 2007, IEEE T CIRC SYST VID, V17, P1218, DOI 10.1109/TCSVT.2007.905531
   Stojanovic M, 2007, P 1 WORKSH UND NETW, P41, DOI [10.1145/1161039.1161049, DOI 10.1145/1161039.1161049, 10.1145/1347364.1347373]
   Stojanovic M, 2009, IEEE COMMUN MAG, V47, P84, DOI 10.1109/MCOM.2009.4752682
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   teledynemarine, RESON TC4013 HYDR PR
   Torres D, 2015, AD HOC NETW, V34, P252, DOI 10.1016/j.adhoc.2015.01.010
   Zhou C, 2015, IEEE T CIRC SYST VID, V25, P1002, DOI 10.1109/TCSVT.2014.2364418
   Zhou S, 2014, OFDM FOR UNDERWATER ACOUSTIC COMMUNICATIONS, P1
NR 40
TC 4
Z9 4
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 473
EP 485
DI 10.1109/TMM.2021.3127454
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800011
DA 2024-07-18
ER

PT J
AU Si, TZ
   He, FZ
   Zhang, Z
   Duan, YS
AF Si, Tongzhen
   He, Fazhi
   Zhang, Zhong
   Duan, Yansong
TI Hybrid Contrastive Learning for Unsupervised Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrastive learning; feature distribution; unsupervised person
   re-identification
ID ENHANCEMENT
AB Unsupervised person re-identification (Re-ID) aims to learn discriminative features without human-annotated labels. Recently, contrastive learning has provided a new prospect for unsupervised person Re-ID, and existing methods primarily constrain the feature similarity among easy sample pairs. However, the feature similarity among hard sample pairs is neglected, which yields suboptimal performance in unsupervised person Re-ID. In this paper, we propose a novel Hybrid Contrastive Model (HCM) to perform the identity-level contrastive learning and the image-level contrastive learning for unsupervised person Re-ID, which adequately explores feature similarities among hard sample pairs. Specifically, for the identity-level contrastive learning, an identity-based memory is constructed to store pedestrian features. Accordingly, we define the dynamic contrast loss to identify identity information with dynamic factor for distinguishing hard/easy samples. As for the image-level contrastive learning, an image-based memory is established to store each image feature. We design the sample constraint loss to explore the similarity relationship between hard positive and negative sample pairs. Furthermore, we optimize the two contrastive learning processes in one unified framework to make use of their own advantages as so to constrain the feature distribution for extracting potential information. Extensive experiments demonstrate that the proposed HCM distinctly outperforms existing methods.
C1 [Si, Tongzhen; He, Fazhi] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Zhang, Zhong] Tianjin Normal Univ, Tianjin Key Lab Wireless Mobile Commun & Power Tra, Tianjin 300387, Peoples R China.
   [Duan, Yansong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
C3 Wuhan University; Tianjin Normal University; Wuhan University
RP He, FZ (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM tongzhensi@whu.edu.cn; fzhe@whu.edu.cn; zhong.zhang8848@gmail.com;
   ysduan@whu.edu.cn
OI Si, Tongzhen/0000-0002-1141-9718; duan, yan song/0000-0002-8037-7638
FU National Natural Science Foundation of China [62072348]; National Key
   R&D Program of China [2019YFC1509604]; Science and Technology Major
   Project of Hubei Province (Next -Generation AI Technologies)
   [2019AEA170]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62072348, in part by the National Key
   R&D Program of China under Grant 2019YFC1509604, and in part by the
   Science and Technology Major Project of Hubei Province (Next -Generation
   AI Technologies) under Grant 2019AEA170.
CR [Anonymous], 2021, P IEEE C COMP VIS PA, DOI DOI 10.1109/EMBC46164.2021.9630080
   [Anonymous], 2021, P INT C COMP VIS, DOI DOI 10.1109/ICCV48922.2021.00364
   Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Bai ZC, 2021, PROC CVPR IEEE, P12909, DOI 10.1109/CVPR46437.2021.01272
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen H, 2023, CURR PSYCHOL, V42, P3134, DOI [10.1007/s12144-021-01658-y, 10.3969/j.issn.1004-132X.2021.01.001, 10.1145/3411764.3445308]
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Ge Y., 2020, P NIPS, V33, P11309
   Ge Yixiao, 2020, ARXIV200101526
   Grill Jean-Bastien, 2020, ADV NEURAL INFORM PR
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Ji Z, 2020, EUR C COMP VIS, P20, DOI DOI 10.1007/978-3-030-58604-1_2
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Lai SQ, 2021, IEEE INT CONF COMP V, P4133, DOI 10.1109/ICCVW54120.2021.00461
   Lee KS, 2021, IEEE WINT CONF APPL, P3941, DOI 10.1109/WACV48630.2021.00399
   Li HJ, 2021, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR46437.2021.00666
   Li YY, 2022, IEEE T MULTIMEDIA, V24, P415, DOI 10.1109/TMM.2021.3052354
   Li YY, 2021, IEEE T IMAGE PROCESS, V30, P7952, DOI 10.1109/TIP.2021.3112039
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin Shan, 2018, 2018 IEEE 36 VLSI TE
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2020, IEEE T IMAGE PROCESS, V29, P5481, DOI 10.1109/TIP.2020.2982826
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Pan YT, 2020, WORLD WIDE WEB, V23, P2259, DOI 10.1007/s11280-020-00793-z
   Qi L, 2019, IEEE I CONF COMP VIS, P8079, DOI 10.1109/ICCV.2019.00817
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Shi QB, 2020, PROCEEDINGS OF 2020 IEEE 2ND INTERNATIONAL CONFERENCE ON CIVIL AVIATION SAFETY AND INFORMATION TECHNOLOGY (ICCASIT), P1048, DOI [10.1109/iccasit50869.2020.9368788, 10.1109/ICCASIT50869.2020.9368788]
   Si TZ, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108462
   Song YP, 2022, COMPUT AIDED DESIGN, V146, DOI 10.1016/j.cad.2022.103196
   Sun J, 2021, IEEE T IMAGE PROCESS, V30, P2935, DOI 10.1109/TIP.2021.3056889
   Tian JJ, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3454130
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu GL, 2020, AAAI CONF ARTIF INTE, V34, P12362
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Xin Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P735, DOI 10.1007/978-3-030-58571-6_43
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482
   Yang FX, 2020, AAAI CONF ARTIF INTE, V34, P12597
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yin J., 2021, ARXIV210400202
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Yu ZX, 2022, IEEE T MULTIMEDIA, V24, P4482, DOI 10.1109/TMM.2021.3119133
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhai Y, 2020, COMPUTER VISION ECCV, P594, DOI DOI 10.1007/978-3-030-58571-6_35
   Zhan FN, 2021, INT C PATT RECOG, P6889, DOI 10.1109/ICPR48806.2021.9412395465
   Zhang H, 2021, IEEE T IMAGE PROCESS, V30, P5287, DOI 10.1109/TIP.2021.3082298
   Zhang MY, 2021, AAAI CONF ARTIF INTE, V35, P3360
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang Z, 2021, PROC CVPR IEEE, P12131, DOI 10.1109/CVPR46437.2021.01196
   Zhang Z, 2020, IEEE T FUZZY SYST, V28, P1356, DOI 10.1109/TFUZZ.2019.2914626
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8351, DOI 10.1109/ICCV48922.2021.00826
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhu J, 2021, IEEE T MULTIMEDIA, V23, P2614, DOI 10.1109/TMM.2020.3013531
NR 65
TC 57
Z9 57
U1 52
U2 60
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4323
EP 4334
DI 10.1109/TMM.2022.3174414
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200018
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Song, PP
   Guo, D
   Cheng, J
   Wang, M
AF Song, Peipei
   Guo, Dan
   Cheng, Jun
   Wang, Meng
TI Contextual Attention Network for Emotional Video Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotional video captioning; context; attention
ID EXPRESSION
AB This paper investigates an emerging and challenging task-emotional video captioning. Formally, given a video, the task aims to not only describe the factual content of the video, but also discover the emotional clues in the video. We propose a novel Contextual Attention Network (CANet), which recognizes and describes the fact and emotion in the video by semantic-rich context learning. To be specific, at each time step, we first extract visual and textual features from both input video and previously generated words. Then, we apply the attention mechanism to these features to capture informative contexts for captioning. We train the CANet model with the joint optimization of cross-entropy loss L-CE and contrastive loss L-CL, where L-CE constrains the semantics of the generated sentence to be close to human annotation and L-CL encourages discriminative representation learning from positive and negative pairs of video and caption. Experiments on two emotional video captioning datasets (i.e., EmVidCap and EmVidCap-S) demonstrate the superiority of CANet compared to the state-of-the-art approaches.
C1 [Song, Peipei; Guo, Dan; Wang, Meng] Hefei Univ Technol, Key Lab Knowledge Engn Big Data HFUT, Minist Educ, Hefei 230601, Anhui, Peoples R China.
   [Song, Peipei; Guo, Dan; Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230601, Anhui, Peoples R China.
   [Cheng, Jun] Chinese Acad Sci, Shenzhen Inst Adv Technol, CAS Key Lab Human Machine Intelligence Synergy Sys, Beijing 100045, Peoples R China.
   [Cheng, Jun] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology; Chinese
   Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS;
   Chinese University of Hong Kong
RP Guo, D; Wang, M (corresponding author), Hefei Univ Technol, Key Lab Knowledge Engn Big Data HFUT, Minist Educ, Hefei 230601, Anhui, Peoples R China.; Guo, D; Wang, M (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230601, Anhui, Peoples R China.
EM beta.songpp@gmail.com; guodan@hfut.edu.cn; jun.cheng@siat.ac.cn;
   eric.mengwang@gmail.com
RI Song, Peipei/KEH-0796-2024
OI Guo, Dan/0000-0003-2594-254X
FU National Natural Science Foundation of China [61725203, 62020106007,
   61876058]; Fundamental Research Funds for the Central Universities
   [JZ2020HGTB0020]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61725203, 62020106007, and 61876058 and
   in part by the Fundamental Research Funds for the Central Universities
   under Grant JZ2020HGTB0020. The guest editor coordinating the review of
   this manuscript and approving it for publication was Prof. Junwei Han.
CR [Anonymous], 2015, P 3 INT C LEARN REPR
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen JW, 2019, AAAI CONF ARTIF INTE, P8167
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Chen TL, 2018, LECT NOTES COMPUT SC, V11214, P527, DOI 10.1007/978-3-030-01249-6_32
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Fan CY, 2019, PROC CVPR IEEE, P1999, DOI 10.1109/CVPR.2019.00210
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Guo D, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P920
   Guo LT, 2019, PROC CVPR IEEE, P4199, DOI 10.1109/CVPR.2019.00433
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Irie G, 2010, IEEE T MULTIMEDIA, V12, P523, DOI 10.1109/TMM.2010.2051871
   Jiang YG, 2014, AAAI CONF ARTIF INTE, P73
   Kahou SE, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P543, DOI 10.1145/2522848.2531745
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Mathews A, 2016, AAAI CONF ARTIF INTE, P3574
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Mittal T., 2021, PROC IEEECVF C COMPU, P5661
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Rohrbach A, 2014, LECT NOTES COMPUT SC, V8753, P184, DOI 10.1007/978-3-319-11752-2_15
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Ryu H, 2021, AAAI CONF ARTIF INTE, V35, P2514
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2015, P ANN C N AM CHAPT A, P1494, DOI 10.3115/v1/N15-1173
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang HL, 2022, IEEE T MULTIMEDIA, V24, P715, DOI 10.1109/TMM.2021.3058555
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Yang JY, 2021, PROC CVPR IEEE, P4235, DOI 10.1109/CVPR46437.2021.00422
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   You Q., 2018, Image Captioning at Will: A Versatile Scheme for Effectively Injecting Sentiments into Image Descriptions
   Yuan YT, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1085, DOI 10.1145/3394171.3413908
   Zhao B, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1177
   Zhao SC, 2020, AAAI CONF ARTIF INTE, V34, P303
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
   Zijun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13103, DOI 10.1109/CVPR42600.2020.01312
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 53
TC 5
Z9 5
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1858
EP 1867
DI 10.1109/TMM.2022.3183402
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100021
DA 2024-07-18
ER

PT J
AU Wang, YM
   Chang, DX
   Fu, ZQ
   Zhao, Y
AF Wang, Yiming
   Chang, Dongxia
   Fu, Zhiqiang
   Zhao, Yao
TI Consistent Multiple Graph Embedding for Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mutual information; Clustering methods; Clustering algorithms; Data
   models; Task analysis; Laplace equations; Fuses; Graph neural networks;
   multi-view clustering; mutual information; representation learning
AB Graph-based multi-view clustering aiming to obtain a partition of data across multiple views, has received considerable attention in recent years. Although great efforts have been made for graph-based multi-view clustering, it is still challenging to fuse characteristics from various views to learn a common representation for clustering. In this paper, we propose a novel Consistent Multiple Graph Embedding Clustering framework (CMGEC). Specifically, a multiple graph auto-encoder (M-GAE) is designed to flexibly encode the complementary information of multi-view data using a multi-graph attention fusion encoder. To guide the learned common representation maintaining the similarity of the neighboring characteristics in each view, a Multi-view Mutual Information Maximization module (MMIM) is introduced. Furthermore, a graph fusion network (GFN) is devised to explore the relationship among graphs from different views and provide a common consensus graph needed in M-GAE. By jointly training these models, the common representation can be obtained, which encodes more complementary information from multiple views and depicts data more comprehensively. Experiments on three types of multi-view datasets demonstrate CMGEC outperforms the state-of-the-art clustering methods.
C1 [Wang, Yiming; Chang, Dongxia; Fu, Zhiqiang; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Wang, Yiming; Chang, Dongxia; Fu, Zhiqiang; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Chang, DX (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM wangym@bjtu.edu.cn; dxchang@bjtu.edu.cn; fuzhiqiang1230@outlook.com;
   yzhao@bjtu.edu.cn
OI Zhao, Yao/0000-0002-8581-9554; Yiming, Wang/0000-0002-8765-7640
FU Fundamen-tal Research Funds for the Central Universities under
   [2021YJS027]; National Key Research and Development of China under
   [2018AAA0102100]
FX This work was supported in part by the Fundamen-tal Research Funds for
   the Central Universities under Grant 2021YJS027 and in part by the
   National Key Research and Development of China under Grant
   2018AAA0102100. The Associate Editor coordinating the review of
   thismanuscript and approving it for publication was Prof. Xin Geng.
CR Bachman P, 2019, ADV NEUR IN, V32
   Belghazi MI, 2018, PR MACH LEARN RES, V80
   Bo DY, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1400, DOI 10.1145/3366423.3380214
   Chao Guoqing, 2021, IEEE Trans Artif Intell, V2, P146, DOI 10.1109/tai.2021.3065894
   Chao GQ, 2019, INFORM SCIENCES, V494, P278, DOI 10.1016/j.ins.2019.04.039
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Chikhi NF, 2016, INFORM PROCESS MANAG, V52, P618, DOI 10.1016/j.ipm.2015.12.007
   Fan SH, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P3070, DOI 10.1145/3366423.3380079
   Gao XJ, 2018, PATTERN RECOGN, V75, P223, DOI 10.1016/j.patcog.2017.02.035
   Hjelm R. Devon, 2018, LEARNING DEEP REPRES
   JARVIS RA, 1973, IEEE T COMPUT, VC-22, P1025, DOI 10.1109/T-C.1973.223640
   Jiang WH, 2018, PATTERN RECOGN, V81, P484, DOI 10.1016/j.patcog.2018.04.018
   Kang Z, 2020, KNOWL-BASED SYST, V189, DOI 10.1016/j.knosys.2019.105102
   Kang Z, 2020, NEURAL NETWORKS, V122, P279, DOI 10.1016/j.neunet.2019.10.010
   Kipf T. N., 2016, ARXIV161107308, V1050, P21
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Krause A., 2010, ADV NEURAL INFORM PR, V23, P775
   Li CT, 2018, IEEE T COMPUT SOC SY, V5, P33, DOI 10.1109/TCSS.2017.2763973
   Li RH, 2019, IEEE I CONF COMP VIS, P8171, DOI 10.1109/ICCV.2019.00826
   Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108
   Mao YQ, 2021, AAAI CONF ARTIF INTE, V35, P8893
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Riemenschneider H, 2014, LECT NOTES COMPUT SC, V8693, P516, DOI 10.1007/978-3-319-10602-1_34
   Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38
   Sculley D., 2010, P INT C WORLD WID WE, V19, P1177, DOI [DOI 10.1145/1772690.1772862, 10.1145/1772690.1772862]
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Son JW, 2017, AAAI CONF ARTIF INTE, P2548
   Sun M., 2021, IEEE T MULTIMEDI JUN
   Tao ZQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3562
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3670
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang SW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3778
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wang ZD, 2019, PROC CVPR IEEE, P1117, DOI 10.1109/CVPR.2019.00121
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Wen J, 2019, IEEE T CYBERNETICS, V49, P1279, DOI 10.1109/TCYB.2018.2799862
   Yin M, 2019, IEEE T NEUR NET LEAR, V30, P851, DOI 10.1109/TNNLS.2018.2851444
   Yu Zheng, 2015, IEEE Transactions on Big Data, V1, P16, DOI 10.1109/TBDATA.2015.2465959
   Yun S, 2019, ADV NEUR IN, V32
   Zhan K, 2019, IEEE T KNOWL DATA EN, V31, P1984, DOI 10.1109/TKDE.2018.2872061
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang C, 2019, PROC CVPR IEEE, P9444, DOI 10.1109/CVPR.2019.00968
NR 45
TC 17
Z9 17
U1 8
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1008
EP 1018
DI 10.1109/TMM.2021.3136098
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900025
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, YX
   Liao, LZ
   Zhang, GY
   Lei, WQ
   Zhao, GS
   Qian, XM
   Chua, TS
AF Wu, Yuxia
   Liao, Lizi
   Zhang, Gangyi
   Lei, Wenqiang
   Zhao, Guoshuai
   Qian, Xueming
   Chua, Tat-Seng
TI State Graph Reasoning for Multimodal Conversational Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Index Terms-Conversation; knowledge graph; recommend-ation systems
AB Conversational recommendation system (CRS) attracts increasing attention in various application domains such as retail and travel. It offers an effective way to capture users' dynamic preferences with multi-turn conversations. However, most current studies center on the recommendation aspect while over-simplifying the conversation process. The negligence of complexity in data structure and conversation flow hinders their practicality and utility. In reality, there exist various relationships among slots and values, while users' requirements may dynamically adjust or change. Moreover, the conversation often involves visual modality to facilitate the conversation. These actually call for a more advanced internal state representation of the dialogue and a proper reasoning scheme to guide the decision making process. In this paper, we explore multiple facets of multimodal conversational recommendation and try to address the above mentioned challenges. In particular, we represent the structured back-end database as a multimodal knowledge graph which captures the various relations and evidence in different modalities. The user preferences expressed via conversation utterances will then be gradually updated to the state graph with clear polarity. Based on these, we train an end-to-end State Graph-based Reasoning model (SGR) to perform reasoning over the whole state graph. The prediction of our proposed model benefits from the structure of the graph. It not only allows for zero-shot reasoning for items unseen in training conversations, but also provides a natural way to explain the policies. Extensive experiments show that our model achieves better performance compared with existing methods.
C1 [Wu, Yuxia; Zhao, Guoshuai; Qian, Xueming] Xi An Jiao Tong Univ, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Shaanxi, Peoples R China.
   [Liao, Lizi] Singapore Management Univ, Singapore 188065, Singapore.
   [Zhang, Gangyi] Univ Sci & Technol China, Hefei 101127, Hebei, Peoples R China.
   [Lei, Wenqiang; Chua, Tat-Seng] Natl Univ Singapore, Singapore 119077, Singapore.
C3 Xi'an Jiaotong University; Singapore Management University; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS;
   National University of Singapore
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Shaanxi, Peoples R China.; Liao, LZ (corresponding author), Singapore Management Univ, Singapore 188065, Singapore.
EM wuyuxia@stu.xjtu.edu.cn; liaolizi.llz@gmail.com;
   gangyi.zhang@outlook.com; wenqianglei@gmail.com;
   guoshuai.zhao@xjtu.edu.cn; qianxm@mail.xjtu.edu.cn; dcscts@nus.edu.sg
RI liu, xingwang/KCY-1277-2024; he, xi/JXN-3817-2024; WANG,
   SHIHAO/KHC-8263-2024; Lin, Yi/KEH-1784-2024; Zhang,
   Lijuan/KAM-0174-2024; chen, huan/KEC-2019-2024
OI , Yuxia Wu/0000-0003-3873-3982; Liao, Lizi/0000-0002-9973-3305
FU China Scholarship Council [202006280325]; NSFC, China [61902309,
   61701391, 61772407]; ShaanXi Province [2018JM6092]; Fundamental Research
   Funds for the Central Universities, China [xxj022019003]; China
   Postdoctoral Science Foundation [2020M683496]; National Postdoctoral
   Innovative Talents Support Program, China [BX20190273]; Science and
   Technology Program of Xi'an, China [21RGZN0017]
FX & nbsp;This work was supported in part by the scholarship from China
   Scholarship Council under Grant 202006280325, in part by NSFC, China,
   under Grants 61902309, 61701391, and 61772407, in part by ShaanXi
   Province under Grant 2018JM6092, in part by the Fundamental Research
   Funds for the Central Universities, China, under Grant xxj022019003, in
   part by the China Postdoctoral Science Foundation under Grant
   2020M683496, in part by the National Postdoctoral Innovative Talents
   Support Program, China, under Grant BX20190273, and in part by the
   Science and Technology Program of Xi'an, China, under Grant 21RGZN0017.&
   nbsp;
CR Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663
   Bordes A., 2013, P 26 INT C NEURAL IN, P2787
   Chapelle O., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710
   Chen QB, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1803
   Christakopoulou K, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P139, DOI 10.1145/3219819.3219894
   Christakopoulou K, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P815, DOI 10.1145/2939672.2939746
   Deng Y, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1431, DOI 10.1145/3404835.3462913
   Derr T, 2018, IEEE DATA MINING, P929, DOI 10.1109/ICDM.2018.00113
   Firdaus M, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3430752
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XN, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P639, DOI 10.1145/3397271.3401063
   Hosseini-Asl Ehsan, 2020, ARXIV200500796, V33, P20179
   Lei WQ, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P2425, DOI 10.1145/3397271.3401419
   Lei WQ, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2073, DOI 10.1145/3394486.3403258
   Lei WQ, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P304, DOI 10.1145/3336191.3371769
   Li R, 2018, ADV NEUR IN, V31
   Liao LZ, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P675, DOI 10.1145/3404835.3462970
   Liao LZ, 2020, LECT NOTES COMPUT SC, V11961, P405, DOI 10.1007/978-3-030-37731-1_33
   Liao LZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P801, DOI 10.1145/3240508.3240605
   Liu Y, 2019, LECT NOTES COMPUT SC, V11503, P459, DOI 10.1007/978-3-030-21348-0_30
   Liu ZM, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P1036
   Moon S, 2020, P 28 INT C COMP LING, P1103
   Moon S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P845
   Mousselly-Sergieh H., 2018, P 7 JOINT C LEX COMP, P225, DOI DOI 10.18653/V1/S18-2027
   Nie LQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1098, DOI 10.1145/3343031.3350923
   Pezeshkpour P, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P3208
   Radford A., 2019, LANGUAGE MODELS ARE
   Saha A, 2018, AAAI CONF ARTIF INTE, P696
   Sanh, 2019, P 5 WORKSH EN EFF MA
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715
   Sun R, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1405, DOI 10.1145/3340531.3411947
   Sun YM, 2018, ACM/SIGIR PROCEEDINGS 2018, P235, DOI 10.1145/3209978.3210002
   Wang M, 2020, BIG DATA RES, V22, DOI 10.1016/j.bdr.2020.100159
   Wolf T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P38
   Xie RB, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3140
   Xu H., 2020, P 28 INT C COMP LING
   Youze Wang, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P540, DOI 10.1145/3372278.3390713
   Zhang YC, 2020, AAAI CONF ARTIF INTE, V34, P9604
   Zhang YF, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P177, DOI 10.1145/3269206.3271776
   Zhou K, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1006, DOI 10.1145/3394486.3403143
   Zhou Kun, 2020, P 28 INT C COMP LING, P4128
   Zou J, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P881, DOI 10.1145/3397271.3401180
NR 42
TC 10
Z9 10
U1 5
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3113
EP 3124
DI 10.1109/TMM.2022.3155900
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200013
OA Green Published
DA 2024-07-18
ER

PT J
AU Xu, SH
   Rao, HC
   Hu, XP
   Cheng, J
   Hu, B
AF Xu, Shihao
   Rao, Haocong
   Hu, Xiping
   Cheng, Jun
   Hu, Bin
TI Prototypical Contrast and Reverse Prediction: Unsupervised Skeleton
   Based Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Prototypes; Encoding; Task analysis; Semantics; Decoding;
   Prediction algorithms; Prototypical contrast; unsupervised learning;
   skeleton based action recognition
AB We focus on unsupervised representation learning for skeleton based action recognition. Existing unsupervised approaches usually learn action representations by motion prediction but they lack the ability to fully learn inherent semantic similarity. In this paper, we propose a novel framework named Prototypical Contrast and Reverse Prediction (PCRP) to address this challenge. Different from plain motion prediction, PCRP performs reverse motion prediction based on encoder-decoder structure to extract more discriminative temporal pattern, and derives action prototypes by clustering to explore the inherent action similarity within the action encoding. Specifically, we regard action prototypes as latent variables and formulate PCRP as an expectation-maximization (EM) task. PCRP iteratively runs (1) E-step as to determine the distribution of action prototypes by clustering action encoding from the encoder while estimating concentration around prototypes, and (2) M-step as optimizing the model by minimizing the proposed ProtoMAE loss, which helps simultaneously pull the action encoding closer to its assigned prototype by contrastive learning and perform reverse motion prediction task. Besides, the sorting can also serve as a temporal task similar as reverse prediction in the proposed framework. Extensive experiments on N-UCLA, NTU 60, and NTU 120 dataset present that PCRP outperforms main stream unsupervised methods and even achieves superior performance over many supervised methods.
C1 [Xu, Shihao; Hu, Xiping; Hu, Bin] Lanzhou Univ, Sch Informat Sci & Engn, Gansu Prov Key Lab Wearable Comp, Lanzhou 730000, Peoples R China.
   [Xu, Shihao; Rao, Haocong; Cheng, Jun] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Rao, Haocong] Nanyang Technol Univ, Joint NTU UBC Res Ctr Excellence Act Living Elderl, Singapore 639798, Singapore.
   [Rao, Haocong] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Hu, Xiping] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Shenzhen 518107, Peoples R China.
   [Hu, Bin] Beijing Inst Technol, Inst Engn Med, Beijing 100000, Peoples R China.
C3 Lanzhou University; Chinese Academy of Sciences; Shenzhen Institute of
   Advanced Technology, CAS; Nanyang Technological University; Nanyang
   Technological University; Sun Yat Sen University; Beijing Institute of
   Technology
RP Hu, XP; Hu, B (corresponding author), Lanzhou Univ, Sch Informat Sci & Engn, Gansu Prov Key Lab Wearable Comp, Lanzhou 730000, Peoples R China.; Cheng, J (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.; Hu, XP (corresponding author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Shenzhen 518107, Peoples R China.; Hu, B (corresponding author), Beijing Inst Technol, Inst Engn Med, Beijing 100000, Peoples R China.
EM xushh16@lzu.edu.cn; haocongrao@gmail.com; huxp@lzu.edu.cn;
   jun.cheng@siat.ac.cn; bh@lzu.edu.cn
RI Hu, Bin/ACD-0145-2022; Xu, Shihao/B-5422-2018; Rao,
   Haocong/JPL-4767-2023; Hu, Xiping/GSE-5065-2022; Rao,
   Haocong/ABF-7884-2021
OI Hu, Bin/0000-0003-3514-5413; Hu, Xiping/0000-0002-4952-699X; Rao,
   Haocong/0000-0002-9576-2379; Xu, Shihao/0000-0002-1522-730X
FU National Key Research and Development Program of China [2019YFA0706200];
   National Natural Science Foundation of China [61632014, 61627808]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2019YFA0706200 and in part by
   the National Natural Science Foundation of China under Grants 61632014
   and 61627808. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Pradeep K Atrey.
CR [Anonymous], 2006, Book Annosearch: Image auto-annotation by search, DOI DOI 10.1109/CVPR.2006.58
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bhatnagar BL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1447
   Caron M., 2020, Advances in neural information processing systems, V33, P9912
   Chen T, 2020, PR MACH LEARN RES, V119
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Cheng Y.B., 2021, IEEE INT C MULTIMEDI, P1, DOI [DOI 10.1109/ICME51207.2021.9428459, 10.1109/ICME51207.2021.9428459]
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Devillers L, 2005, NEURAL NETWORKS, V18, P407, DOI 10.1016/j.neunet.2005.03.007
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Gutmann Michael, 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hu GY, 2020, IEEE T MULTIMEDIA, V22, P2207, DOI 10.1109/TMM.2019.2953325
   Hu JF, 2019, IEEE T PATTERN ANAL, V41, P2568, DOI 10.1109/TPAMI.2018.2863279
   Johnson J, 2021, IEEE T BIG DATA, V7, P535, DOI 10.1109/TBDATA.2019.2921572
   Jones S, 2014, PROC CVPR IEEE, P604, DOI 10.1109/CVPR.2014.84
   Kulkarni T, 2019, ADV NEUR IN, V32
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Lee I, 2021, IEEE T MULTIMEDIA, V23, P415, DOI 10.1109/TMM.2020.2978637
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li J., 2021, PROC INT C LEARN REP
   Li J., 2018, P ADV NEUR INF PROC, V31, P1254
   Liang DH, 2019, IEEE COMPUT SOC CONF, P934, DOI 10.1109/CVPRW.2019.00123
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Luo ZL, 2017, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR.2017.751
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Ng W, 2022, IEEE T MULTIMEDIA, V24, P1678, DOI 10.1109/TMM.2021.3070127
   Peng B, 2020, IEEE T IND INFORM, V16, P555, DOI 10.1109/TII.2019.2937514
   Perez M, 2022, IEEE T MULTIMEDIA, V24, P366, DOI 10.1109/TMM.2021.3050642
   Rao HC, 2022, IEEE T PATTERN ANAL, V44, P6649, DOI 10.1109/TPAMI.2021.3092833
   Rao HC, 2021, INFORM SCIENCES, V569, P90, DOI 10.1016/j.ins.2021.04.023
   Rao HC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P898
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Snell J, 2017, ADV NEUR IN, V30
   Sutskever I, 2014, ADV NEUR IN, V27
   Nguyen-Phuoc T, 2019, IEEE I CONF COMP VIS, P7587, DOI 10.1109/ICCV.2019.00768
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xu SA, 2021, IEEE INTERNET THINGS, V8, P15990, DOI 10.1109/JIOT.2020.3042986
   Yang JY, 2021, IEEE T MULTIMEDIA, V23, P883, DOI 10.1109/TMM.2020.2990082
   Zhang PF, 2020, IEEE T IMAGE PROCESS, V29, P1061, DOI 10.1109/TIP.2019.2937724
   Zhang T, 2020, IEEE T MULTIMEDIA, V22, P2926, DOI 10.1109/TMM.2020.2966878
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
   Zhuang CX, 2019, IEEE I CONF COMP VIS, P6001, DOI 10.1109/ICCV.2019.00610
NR 52
TC 15
Z9 15
U1 4
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 624
EP 634
DI 10.1109/TMM.2021.3129616
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800022
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, GL
   Fini, E
   Xu, D
   Rota, P
   Ding, ML
   Tang, H
   Alameda-Pineda, X
   Ricci, E
AF Yang, Guanglei
   Fini, Enrico
   Xu, Dan
   Rota, Paolo
   Ding, Mingli
   Tang, Hao
   Alameda-Pineda, Xavier
   Ricci, Elisa
TI Continual Attentive Fusion for Incremental Learning in Semantic
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Semantics; Image segmentation; Tensors; Feature
   extraction; Deep learning; Training; Incremental learning; knowledge
   distillation; semantic segmentation
AB Over the past years, semantic segmentation, similar to many other tasks in computer vision, has benefited from the progress in deep neural networks, resulting in significantly improved performance. However, deep architectures trained with gradient-based techniques suffer from catastrophic forgetting, which is the tendency to forget previously learned knowledge while learning new tasks. Aiming at devising strategies to counteract this effect, incremental learning approaches have gained popularity over the past years. However, the first incremental learning methods for semantic segmentation appeared only recently. While effective, these approaches do not account for a crucial aspect in pixel-level dense prediction problems, i.e., the role of attention mechanisms. To fill this gap, in this paper, we introduce a novel attentive feature distillation approach to mitigate catastrophic forgetting while accounting for semantic spatial- and channellevel dependencies. Furthermore, we propose a continual attentive fusion structure, which takes advantage of the attention learned from the new and the old tasks while learning features for the new task. Finally, we also introduce a novel strategy to account for the background class in the distillation loss, thus preventing biased predictions. We demonstrate the effectiveness of our approach with an extensive evaluation on Pascal-VOC 2012 and ADE20 K, setting a new state of the art.
C1 [Yang, Guanglei; Ding, Mingli] Harbin Inst Technol HIT, Sch Instrument Sci & Engn, Harbin 150001, Peoples R China.
   [Fini, Enrico; Rota, Paolo] Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Povo, Italy.
   [Xu, Dan] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong 999077, Peoples R China.
   [Tang, Hao] Swiss Fed Inst Technol, Dept Informat Technol & Elect Engn, CH-8092 Zurich, Switzerland.
   [Alameda-Pineda, Xavier] INRIA, RobotLearn Grp, F-38330 Montbonnot St Martin, France.
   [Ricci, Elisa] Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Povo, Italy.
   [Ricci, Elisa] Fdn Bruno Kessler, Deep Visual Learning Grp, I-38123 Trento, Italy.
C3 Harbin Institute of Technology; University of Trento; Hong Kong
   University of Science & Technology; Swiss Federal Institutes of
   Technology Domain; ETH Zurich; Inria; University of Trento; Fondazione
   Bruno Kessler
RP Ding, ML (corresponding author), Harbin Inst Technol HIT, Sch Instrument Sci & Engn, Harbin 150001, Peoples R China.; Ricci, E (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Povo, Italy.; Ricci, E (corresponding author), Fdn Bruno Kessler, Deep Visual Learning Grp, I-38123 Trento, Italy.
EM yangguanglei@hit.edu.cn; enrico.fini@unitn.it; danxu@cse.ust.hk;
   paolo.rota@unitn.it; dingml@hit.edu.cn; hao.tang@vision.ee.ethz.ch;
   xavier.alameda-pineda@inria.fr; eliricci@fbk.eu
RI Xu, Dan/KPA-7396-2024; Yang, Guanglei/HME-0024-2023; Rota,
   Paolo/AAG-1352-2020; Ricci, Elisa/IYS-6532-2023
OI Xu, Dan/0000-0003-4602-3550; Yang, Guanglei/0000-0002-5324-3642; Rota,
   Paolo/0000-0003-0663-5659; Fini, Enrico/0000-0002-1671-875X;
   Alameda-Pineda, Xavier/0000-0002-5354-1084; Ricci,
   Elisa/0000-0002-0228-1147; Tang, Hao/0000-0002-2077-1246
FU China Scholarship Council (CSC); H2020 EU project SPRING; H2020 EU
   project AI4Media; Caritro Foundation; Deep Learning Lab of the ProM
   Facility; European Commission [871245]; European Institute of Innovation
   AMP; Technology (EIT); ARN under Grant ML3RI [ANR-19-CE33-0008-01]
FX This work was supported in part by the China Scholarship Council (CSC),
   in part by the H2020 EU projects SPRING and AI4Media, in part by the
   Caritro Foundation, in part by the Deep Learning Lab of the ProM
   Facility, and in part by the European Commission under Grant GA 871245.
   The work of Guanglei Yang was supported by China Scholarship Council
   (CSC) during a visit of University of Trento. The work of Enrico Fini
   was supported by the European Institute of Innovation & Technology
   (EIT). The work of Xavier Alameda-Pinedawas supported by the ARN under
   Grant ML3RI (ANR-19-CE33-0008-01).
CR Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9
   Bulo S. Rota, 2018, PROC CVPR IEEE, P5639, DOI DOI 10.1109/CVPR.2018.00591
   CASTRO FM, 2018, P EUR C COMPUT VIS, P320
   Cermelli F., 2020, P IEEE CVF C COMP VI, P9230, DOI DOI 10.1109/CVPR42600.2020.00925
   Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33
   Chen L.Z, 2017, CORR ABS170605587
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen W, 2022, IEEE T MULTIMEDIA, V24, P1844, DOI 10.1109/TMM.2021.3073279
   Chorowski Jan K, 2015, ADV NEURAL INFORM PR, DOI [DOI 10.1016/0167-739X(94)90007-8, 10.1016/j.asr.2015.02.035, DOI 10.1016/J.ASR.2015.02.035]
   DELANGE M, 2019, CONTINUAL LEARNING A
   Dhar P, 2019, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR.2019.00528
   Ding L, 2021, IEEE T GEOSCI REMOTE, V59, P426, DOI 10.1109/TGRS.2020.2994150
   Douillard Arthur, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P86, DOI 10.1007/978-3-030-58565-5_6
   Douillard A, 2021, PROC CVPR IEEE, P4039, DOI 10.1109/CVPR46437.2021.00403
   Duan B, 2021, IEEE WINT CONF APPL, P4012, DOI 10.1109/WACV48630.2021.00406
   Duan B, 2021, INT C PATT RECOG, P1336, DOI 10.1109/ICPR48806.2021.9412890
   EVERINGHAM M, 2007, THE PASCAL VISUAL OB
   Fini Enrico, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P720, DOI 10.1007/978-3-030-58604-1_43
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fujii K, 2021, IEEE T MULTIMEDIA, V23, P3892, DOI 10.1109/TMM.2020.3033125
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HINTON G, 2015, NEURIPS WORKSHOP
   Hou SH, 2019, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2019.00092
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Iscen Ahmet, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P699, DOI 10.1007/978-3-030-58517-4_41
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   LI Y, 2022, P 16TH AAAI C ARTIF, P4283
   Li ZZ, 2016, LECT NOTES COMPUT SC, V9908, P614, DOI 10.1007/978-3-319-46493-0_37
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   LIU X, 2020, MULTI TASK INCREMENT
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mallya A, 2018, LECT NOTES COMPUT SC, V11208, P72, DOI 10.1007/978-3-030-01225-0_5
   Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810
   McCloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P109, DOI [10.1016/S0079-7421(08)60536-8, DOI 10.1016/S0079-7421(08)60536-8]
   Michieli U, 2021, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR46437.2021.00117
   Michieli U, 2019, IEEE INT CONF COMP V, P3205, DOI 10.1109/ICCVW.2019.00400
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Ostapenko O, 2019, PROC CVPR IEEE, P11313, DOI 10.1109/CVPR.2019.01158
   Ozdemir F, 2019, INT J COMPUT ASS RAD, V14, P1187, DOI 10.1007/s11548-019-01984-4
   Ozdemir F, 2018, LECT NOTES COMPUT SC, V11073, P361, DOI 10.1007/978-3-030-00937-3_42
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Shin H, 2017, ADV NEUR IN, V30
   Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368
   Tasar O, 2019, IEEE J-STARS, V12, P3524, DOI 10.1109/JSTARS.2019.2925416
   Thuseethan Selvarajah, 2021, IEEE T MULTIMEDIA, P1
   Tian X, 2021, IEEE T MULTIMEDIA, V23, P1210, DOI 10.1109/TMM.2020.2994509
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wu Chenshen, 2018, NEURIPS
   Wu Y, 2019, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.2019.00046
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   XU D, 2017, NEURIPS
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan SP, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3052, DOI 10.1145/3474085.3475443
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zenke F, 2017, PR MACH LEARN RES, V70
   Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043
   Zhang HM, 2021, IEEE T MULTIMEDIA, V23, P2033, DOI 10.1109/TMM.2020.3007352
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang ZL, 2018, LECT NOTES COMPUT SC, V11214, P273, DOI 10.1007/978-3-030-01249-6_17
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhong Z., 2020, P IEEE CVF C COMP VI, P13065, DOI DOI 10.1109/CVPR42600.2020.01308
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 65
TC 10
Z9 10
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3841
EP 3854
DI 10.1109/TMM.2022.3167555
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500023
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, LR
   Li, HL
   Wu, QB
   Meng, FM
   Qiu, HQ
   Xu, LF
AF Yang, Longrong
   Li, Hongliang
   Wu, Qingbo
   Meng, Fanman
   Qiu, Heqian
   Xu, Linfeng
TI Bias-Correction Feature Learner for Semi-Supervised Instance
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semi-supervised learning; instance segmentation; contrastive learning;
   low-confident proposals
AB Instance segmentation is heavily reliant on large-scale annotated datasets to yield an ideal accuracy. However, annotated data are difficult to collect. To expand the annotated data, a straightforward idea is to introduce semi-supervised learning, which uses a trained model to obtain initial proposals on unlabeled images and then use initial proposals to generate pseudo labels. However, existing methods inevitably introduce the bias for the model learning, i.e., the foreground in initial low-confident proposals (low-confident foreground) is arbitrarily assigned as background. This bias makes the foreground and background closer in the feature space, which degenerates the model accuracy. To address this issue, this paper discards incorrect supervision and designs a bias-correction feature learner. Specifically, on the one hand, low-confident foreground does not participate in supervised learning. On the other hand, we extract possible foreground regions from all initial proposals to construct high-quality positive pairs which depict objects of the same category in contrastive learning. Then, positive pairs are pulled closer in the feature space. This helps models extract closely clustered foreground features. Experimental results demonstrate the effectiveness of our method on the public datasets (i.e., COCO, Cityscapes and Pascal VOC).
C1 [Yang, Longrong; Li, Hongliang; Wu, Qingbo; Meng, Fanman; Qiu, Heqian; Xu, Linfeng] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Li, HL; Xu, LF (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM yanglr@std.uestc.edu.cn; hlli@uestc.edu.cn; qbwu@uestc.edu.cn;
   fmmeng@uestc.edu.cn; hqqiu@std.uestc.edu.cn; lfxu@uestc.edu.cn
RI Xu, Linfeng/HME-1913-2023; Wu, Qingbo/M-5065-2015
OI Xu, Linfeng/0000-0002-9934-0958; Qiu, Heqian/0000-0002-0963-0311; Wu,
   Qingbo/0000-0003-2936-6340
FU National Key R&D Program of China [2021ZD0112001]; National Natural
   Science Foundation of China [61831005, 61971095, 61871087, 62071086]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021ZD0112001 and in part by the National Natural Science
   Foundation of China under Grants 61831005, 61971095, 61871087, and
   62071086.
CR Bachman P, 2014, ADV NEUR IN, V27
   Bellver M., 2019, IEEE C COMP VIS PATT, P93
   Berthelot D, 2019, ADV NEUR IN, V32
   Brabandere B. D., 2017, P IEEE CVF C COMP VI
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chen C, 2022, IEEE T MULTIMEDIA, V24, P3679, DOI 10.1109/TMM.2021.3105807
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen Ting, 2020, ADV NEURAL INFORM PR, V33, P22243
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32
   Dai P, 2019, IEEE T MULTIMEDIA, V21, P1709, DOI 10.1109/TMM.2018.2885922
   Deng WJ, 2021, IEEE T CIRC SYST VID, V31, P29, DOI 10.1109/TCSVT.2020.2968484
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dosovitskiy A, 2014, ADV NEUR IN, V27
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   French G., 2020, P BRIT MACH VIS C, P420
   Gadde R, 2016, LECT NOTES COMPUT SC, V9905, P597, DOI 10.1007/978-3-319-46448-0_36
   Ghosh S, 2019, NATL CONF COMMUN, DOI 10.1109/ncc.2019.8732250
   Ghosh S, 2020, IEEE T CIRC SYST VID, V30, P2015, DOI 10.1109/TCSVT.2019.2916589
   Ghosh S, 2018, IEEE GLOB CONF SIG, P26, DOI 10.1109/GlobalSIP.2018.8646671
   Ghosh S, 2016, INT CO SIG PROC COMM
   Grill Jean-Bastien, 2020, ADV NEURAL INFORM PR
   Gutmann Michael, 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Jeong J, 2019, ADV NEUR IN, V32
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kalluri T, 2019, IEEE I CONF COMP VIS, P5258, DOI 10.1109/ICCV.2019.00536
   Khosla P., 2020, C NEUR INF PROC SYST
   Kim Donghyun, 2021, ICCV
   Kirillov A., 2020, P IEEECVF C COMPUTER, P9799, DOI DOI 10.48550/ARXIV.1912.08193
   Kuo WC, 2019, IEEE I CONF COMP VIS, P9206, DOI 10.1109/ICCV.2019.00930
   Li C.-L., 2020, Advances in neural information processing systems, V33, P596
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li LD, 2021, IEEE T MULTIMEDIA, V23, P2757, DOI 10.1109/TMM.2020.3016124
   Li Y, 2021, IEEE T MULTIMEDIA, V23, P1354, DOI 10.1109/TMM.2020.2997185
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Lin SH, 2022, IEEE T MULTIMEDIA, V24, P728, DOI 10.1109/TMM.2021.3058546
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y.-C., 2021, P INT C LEARN REPR
   Liu YD, 2018, LECT NOTES COMPUT SC, V11207, P708, DOI 10.1007/978-3-030-01219-9_42
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Min SB, 2021, IEEE T MULTIMEDIA, V23, P899, DOI 10.1109/TMM.2020.2990063
   Ouali Yassine, 2020, P IEEECVF C COMPUTER, DOI DOI 10.1109/CVPR42600.2020.01269
   Purushwalkam S., 2020, ADV NEURAL INFORM PR, P3407, DOI 10.5555/3495724.3496011
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Radosavovic I, 2018, PROC CVPR IEEE, P4119, DOI 10.1109/CVPR.2018.00433
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi HC, 2021, IEEE T MULTIMEDIA, V23, P995, DOI 10.1109/TMM.2020.2991504
   Sohn K, 2020, Arxiv, DOI [arXiv:2005.04757, DOI 10.48550/ARXIV.2005.04757]
   Souly N, 2017, IEEE I CONF COMP VIS, P5689, DOI 10.1109/ICCV.2017.606
   Tarvainen A, 2017, ADV NEUR IN, V30
   Tian Y., 2019, Contrastive multiview coding
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Wu S, 2018, IEEE T MULTIMEDIA, V20, P851, DOI 10.1109/TMM.2017.2758522
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Xu MD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3040, DOI 10.1109/ICCV48922.2021.00305
   Zhan K, 2021, FUTURE GENER COMP SY, V115, P837, DOI 10.1016/j.future.2020.10.016
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang FY, 2022, AAAI CONF ARTIF INTE, P3252
   Zhanghan Ke, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P429, DOI 10.1007/978-3-030-58601-0_26
   Zhong YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7253, DOI 10.1109/ICCV48922.2021.00718
   Zhou Q, 2021, PROC CVPR IEEE, P4079, DOI 10.1109/CVPR46437.2021.00407
   Zhou Yanzhao, 2020, PROC IEEE C COMPUTER, P10307
NR 72
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5852
EP 5863
DI 10.1109/TMM.2022.3199922
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500015
DA 2024-07-18
ER

PT J
AU Zhang, KY
   Hua, ZY
   Li, YM
   Chen, YY
   Zhou, YC
AF Zhang, Kuiyuan
   Hua, Zhongyun
   Li, Yuanman
   Chen, Yongyong
   Zhou, Yicong
TI AMS-Net: Adaptive Multi-Scale Network for Image Compressive Sensing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compressive sensing; convolutional neural networks; discrete wavelet
   transform; block compressive sampling
ID RECONSTRUCTION
AB Recently, deep convolutional neural networks have been applied to image compressive sensing (CS) to improve reconstruction quality while reducing computation cost. Existing deep learning-based CS methods can be divided into two classes: sampling image at single scale and sampling image across multiple scales. However, these existing methods treat the image low-frequency and high-frequency components equally, which is an obstruction to get a high reconstruction quality. This paper proposes an adaptive multi-scale image CS network in wavelet domain called AMS-Net, which fully exploits the different importance of image low-frequency and high-frequency components. First, the discrete wavelet transform is used to decompose an image into four sub-bands, namely the low-low (LL), low-high (LH), high-low (HL), and high-high (HH) sub-bands. Considering that the LL sub-band is more important to the final reconstruction quality, the AMS-Net allocates it a larger sampling ratio, while allocating the other three sub-bands a smaller one. Since different blocks in each sub-band have different sparsity, the sampling ratio is further allocated block-by-block within the four sub-bands. Then a dual-channel scalable sampling model is developed to adaptively sample the LL and the other three sub-bands at arbitrary sampling ratios. Finally, by unfolding the iterative reconstruction process of the traditional multi-scale block CS algorithm, we construct a multi-stage reconstruction model to utilize multi-scale features for further improving the reconstruction quality. Experimental results demonstrate that the proposed model outperforms both the traditional and state-of-the-art deep learning-based methods.
C1 [Zhang, Kuiyuan; Hua, Zhongyun; Chen, Yongyong] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Li, Yuanman] Shenzhen Univ, Coll Elect & Informat Engn, Shenzhen 518060, Peoples R China.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 Harbin Institute of Technology; Shenzhen University; University of Macau
RP Hua, ZY (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM zkyhitsz@gmail.com; huazyum@gmail.com; yuanmanli@szu.edu.cn;
   yongyongchen.cn@gmail.com; yicongzhou@um.edu.mo
RI Zhou, Yicong/A-8017-2009; Hua, Zhongyun/F-1887-2016; Chen,
   yongyong/P-3801-2016
OI Zhou, Yicong/0000-0002-4487-6384; Zhang, Kuiyuan/0000-0001-6320-9096;
   Chen, yongyong/0000-0003-1970-1993
FU National Natural Science Foundation of China [62071142, 62106063];
   Guangdong Basic and Applied Basic Research Foundation [2021A1515011406,
   2022A1515010645, 2022A1515010819]; Shenzhen College Stability Support
   Plan [GXWD20201230155427003-20200824210638001,
   GXWD20201230155427003-20200824113231001]; Foundation for Science and
   Technology Innovation of Shenzhen [RCBS20210609103708014]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62071142 and 62106063, in part by
   Guangdong Basic and Applied Basic Research Foundation under Grants
   2021A1515011406, 2022A1515010645, and 2022A1515010819, in part by
   Shenzhen College Stability Support Plan under Grants
   GXWD20201230155427003-20200824210638001 and
   GXWD20201230155427003-20200824113231001, and in part by the Foundation
   for Science and Technology Innovation of Shenzhen under Grant
   RCBS20210609103708014. The Associate Editor coordinating the review of
   this manuscript and approving it for publication was Dr. Vladan
   Velisavljevic. (Corresponding author: Zhongyun Hua.)
CR [Anonymous], 2016, PRUNING CONVOLUTIONA
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Canh TN, 2021, IEEE T COMPUT IMAG, V7, P86, DOI 10.1109/TCI.2020.3034433
   Chen C, 2011, CONF REC ASILOMAR C, P1193, DOI 10.1109/ACSSC.2011.6190204
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fowler JE, 2011, EUR SIGNAL PR CONF, P564
   Fu XY, 2022, IEEE T NEUR NET LEAR, V33, P6802, DOI 10.1109/TNNLS.2021.3083504
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Jiang QR, 2020, IEEE T MULTIMEDIA, V22, P594, DOI 10.1109/TMM.2019.2931400
   Kingma D.P., 2014, ARXIV14126980
   Korhonen J, 2012, INT WORK QUAL MULTIM, P37, DOI 10.1109/QoMEX.2012.6263880
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Li C., 2009, CAAM Rep, V20, P1
   Liu RH, 2019, IEEE IMAGE PROC, P2070, DOI [10.1109/ICIP.2019.8803190, 10.1109/icip.2019.8803190]
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Mou C, 2022, IEEE T MULTIMEDIA, V24, P1366, DOI 10.1109/TMM.2021.3063916
   Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163
   Ran MS, 2021, IEEE T RADIAT PLASMA, V5, P120, DOI 10.1109/TRPMS.2020.2991877
   Shi WZ, 2019, PROC CVPR IEEE, P12282, DOI 10.1109/CVPR.2019.01257
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Shi WZ, 2017, IEEE INT CON MULTI, P877, DOI 10.1109/ICME.2017.8019428
   Canh TN, 2019, IEEE IMAGE PROC, P2105, DOI [10.1109/ICIP.2019.8803165, 10.1109/icip.2019.8803165]
   Canh TN, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Unde AS, 2017, J VIS COMMUN IMAGE R, V44, P187, DOI 10.1016/j.jvcir.2017.01.028
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu K, 2018, LECT NOTES COMPUT SC, V11214, P491, DOI 10.1007/978-3-030-01249-6_30
   You D., 2021, 2021 IEEE INT C MULT, P1
   Yu Y, 2010, IEEE SIGNAL PROC LET, V17, P973, DOI 10.1109/LSP.2010.2080673
   Yuan XF, 2021, IEEE T NEUR NET LEAR, V32, P3296, DOI 10.1109/TNNLS.2019.2951708
   Yuan X, 2021, IEEE SIGNAL PROC MAG, V38, P65, DOI 10.1109/MSP.2020.3023869
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang B, 2021, IEEE T MULTIMEDIA, V23, P2656, DOI 10.1109/TMM.2020.3014489
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang YS, 2020, IEEE T IND INFORM, V16, P6641, DOI 10.1109/TII.2020.2966511
   Zhang ZH, 2021, IEEE T IMAGE PROCESS, V30, P1487, DOI 10.1109/TIP.2020.3044472
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zheng S, 2021, IEEE T MULTIMEDIA, V23, P3577, DOI 10.1109/TMM.2020.3028479
NR 41
TC 6
Z9 6
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5676
EP 5689
DI 10.1109/TMM.2022.3198323
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500003
DA 2024-07-18
ER

PT J
AU Zhang, M
   Yao, SY
   Hu, BQ
   Piao, YR
   Ji, W
AF Zhang, Miao
   Yao, Shunyu
   Hu, Beiqi
   Piao, Yongri
   Ji, Wei
TI C<SUP>2</SUP>DFNet: Criss-Cross Dynamic Filter Network for RGB-D Salient
   Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic filter; fusion network; RGB-D salient object detection
AB The ability to deal with intra and inter-modality features has been critical to the development of RGB-D salient object detection. While many works have advanced in leaps and bounds in this field, most existing methods have not taken their way down into the inherent differences between the RGB and depth data due to widely adopted conventional convolution in which fixed parameter kernels are applied during inference. To promote intra and inter-modality interaction conditioned on various scenarios, as RGB and depth data are processed independently and later fused interactively, we develop a new insight and a better model. In this paper, we introduce a criss-cross dynamic filter network by decoupling dynamic convolution. First, we propose a Model-specific Dynamic Enhanced Module (MDEM) that dynamically enhances the intra-modality features with global context guidance. Second, we propose a Scene-aware Dynamic Fusion Module (SDFM) to realize dynamic feature selection between two modalities. As a result, our model achieves accurate predictions of salient objects. Extensive experiments demonstrate that our method achieves competitive performance over 28 state-of-the-art RGB-D methods on 7 public datasets.
C1 [Zhang, Miao] Dalian Univ Technol, DUT RU Int Sch Informat Sci & Software Engn, Key Lab Ubiquitous Network & Serv Software Liaonin, Dalian 116024, Peoples R China.
   [Yao, Shunyu; Hu, Beiqi] Dalian Univ Technol, Sch Software Technol, Dalian 116024, Peoples R China.
   [Piao, Yongri] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
   [Ji, Wei] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2R3, Canada.
C3 Dalian University of Technology; Dalian University of Technology; Dalian
   University of Technology; University of Alberta
RP Piao, YR (corresponding author), Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
EM miaozhang@dlut.edu.cn; ysyfeverfew@mail.dlut.edu.cn;
   hbq1211@mail.dlut.edu.cn; yrpiao@dlut.edu.cn; wji3@ualberta.ca
OI Piao, Yongri/0000-0002-0860-252X; Ji, Wei/0000-0003-4059-5902
FU National Natural Science Foundation of China [62172070, 61976035];
   Central Government Guided Local Science and Technology Development Funds
   of Liaoning Province [2022JH6/100100028]; Natural Science Foundation of
   Liaoning Province [2021-MS-123]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172070 and 61976035, in part by the
   Central Government Guided Local Science and Technology Development Funds
   of Liaoning Province under Grant 2022JH6/100100028, and in part by the
   Natural Science Foundation of Liaoning Province under Grant 2021-MS-123.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Boulch A, 2018, COMPUT GRAPH-UK, V71, P189, DOI 10.1016/j.cag.2017.11.010
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P2350, DOI 10.1109/TIP.2021.3052069
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen S., 2020, ECCV, P520, DOI DOI 10.1007/978-3-030-58598-3_31
   Cheng Y, 2014, IEEE INT CON MULTI
   Cohen JR, 2016, J NEUROSCI, V36, P12083, DOI 10.1523/JNEUROSCI.2965-15.2016
   De Brabandere B, 2016, ADV NEUR IN, V29
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu JF, 2018, LECT NOTES COMPUT SC, V11211, P346, DOI 10.1007/978-3-030-01234-2_21
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang NAC, 2022, IEEE T MULTIMEDIA, V24, P1651, DOI 10.1109/TMM.2021.3069297
   Huang NAC, 2021, IEEE T MULTIMEDIA, V23, P2428, DOI 10.1109/TMM.2020.3011327
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Jin WD, 2021, IEEE T IMAGE PROCESS, V30, P3376, DOI 10.1109/TIP.2021.3060167
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Liu D, 2021, IEEE T MULTIMEDIA, V23, P967, DOI 10.1109/TMM.2020.2991523
   Lord LD, 2017, PHILOS T R SOC A, V375, DOI 10.1098/rsta.2016.0283
   Miao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P374, DOI 10.1007/978-3-030-58604-1_23
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31
   Nie XC, 2019, IEEE I CONF COMP VIS, P6941, DOI 10.1109/ICCV.2019.00704
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Pandey R, 2019, PROC CVPR IEEE, P9701, DOI 10.1109/CVPR.2019.00994
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tao Zhang, 2022, IEEE Transactions on Knowledge and Data Engineering, V34, P1763, DOI [10.1109/LGRS.2020.3033988, 10.1109/TKDE.2020.3002567]
   Wang PC, 2017, IEEE INT CONF COMP V, P1005, DOI 10.1109/ICCVW.2017.123
   Wang R, 2021, P NATL ACAD SCI USA, V118, DOI 10.1073/pnas.2022288118
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang WY, 2018, LECT NOTES COMPUT SC, V11215, P144, DOI 10.1007/978-3-030-01252-6_9
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wu YH, 2022, IEEE T PATTERN ANAL, V44, P10261, DOI 10.1109/TPAMI.2021.3134684
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Zhang C, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2094, DOI 10.1145/3474085.3475364
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang M, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4107, DOI [10.1145/3394171.3413969, 10.1145/33941713413969]
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang TW, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13142771
   Zhang WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P731, DOI 10.1145/3474085.3475240
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao X., 2020, P EUR C COMP VIS, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zhu CB, 2020, CAAI T INTELL TECHNO, V5, P1, DOI 10.1049/trit.2019.0034
   Zhu CB, 2019, IEEE INT CONF COMP V, P2541, DOI 10.1109/ICCVW.2019.00311
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
   Zhu CB, 2017, IEEE INT CONF COMP V, P1509, DOI 10.1109/ICCVW.2017.178
   Zhu CB, 2017, IEEE INT CONF COMP V, P3008, DOI 10.1109/ICCVW.2017.355
   Zimmermann C, 2018, IEEE INT CONF ROBOT, P1986, DOI 10.1109/ICRA.2018.8462833
NR 83
TC 24
Z9 25
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5142
EP 5154
DI 10.1109/TMM.2022.3187856
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300038
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zhang, YH
   Zhang, XP
   Li, J
   Qiu, RC
   Xu, HH
   Tian, Q
AF Zhang, Yuhang
   Zhang, Xiaopeng
   Li, Jie
   Qiu, Robert C.
   Xu, Haohang
   Tian, Qi
TI Semi-Supervised Contrastive Learning With Similarity Co-Calibration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semi-supervised learning; contrastive learning; similarity
   co-calibration
AB Semi-supervised learning acts as an effective way to leverage massive unlabeled data. In this paper, we propose a novel training strategy, termed as Semi-supervised Contrastive Learning (SsCL), which combines the well-known contrastive loss in self-supervised learning with the cross entropy loss in semi-supervised learning, and jointly optimizes the two objectives in an end-to-end way. The highlight is that different from self-training based semi-supervised learning that conducts prediction and retraining over the same model weights, SsCL interchanges the predictions over the unlabeled data between the two branches, and thus formulates a co-calibration procedure, which we find is beneficial for better prediction and avoids being trapped in local minimum. Towards this goal, the contrastive loss branch models pairwise similarities among samples, using the pseudo labels generated from the cross entropy branch, and in turn calibrates the prediction distribution of the cross entropy branch with the contrastive similarity. We showthatSsCLproducesmore discriminative representation and is beneficial to semi-supervised learning. Notably, on ImageNet with ResNet50 as the backbone, SsCL achieves 60.2% and 72.1% top-1 accuracy with 1% and 10% labeled samples respectively, which significantly outperforms the baseline, and is better than previous semi-supervised and self-supervised methods.
C1 [Zhang, Yuhang; Li, Jie; Qiu, Robert C.] Shanghai Jiao Tong Univ, Dept Comp Sci, Shanghai 200240, Peoples R China.
   [Xu, Haohang] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Zhang, Xiaopeng; Tian, Qi] Huawei Inc, Shenzhen 518129, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Huawei
   Technologies
RP Qiu, RC (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci, Shanghai 200240, Peoples R China.
EM hang_universe@sjtu.edu.cn; zxphistory@gmail.com; lijiecs@sjtu.edu.cn;
   caiming@hust.edu.cn; xuhaohang@sjtu.edu.cn; tian.qi1@huawei.com
RI Li, jie/GXG-4583-2022
OI Li, Jie/0000-0002-4974-6116
FU National Key R & D Program of China [2018YFF0214705]; NSFC [61932014,
   12141107]; National Key R&D Program of China [2020YFB1806700,
   BE2020026]; Key R&D Program of Jiangsu, China
FX This work was supported in part by the National Key R & D Program of
   China under Grant 2018YFF0214705., NSFC Grant 61932014, NSFC Grant
   12141107, National Key R&D Program of China No.2020YFB1806700,Project
   BE2020026, and in part by the Key R&D Program of Jiangsu, China.
CR [Anonymous], 2020, IJCNN
   Berthelot D., 2019, Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring
   Caron M., 2020, Advances in neural information processing systems, V33, P9912
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chen T, 2020, Arxiv, DOI [arXiv:2002.05709, DOI 10.48550/ARXIV.2002.05709]
   Chen T, 2020, Arxiv, DOI arXiv:2006.10029
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Hjelm RD, 2019, Arxiv, DOI arXiv:1808.06670
   Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Gan C, 2016, LECT NOTES COMPUT SC, V9907, P849, DOI 10.1007/978-3-319-46487-9_52
   Grill Jean-Bastien, 2020, ADV NEURAL INFORM PR
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   He K., 2017, IEEE T PATTERN ANAL, V42, P387
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jawed S, 2020, LECT NOTES ARTIF INT, V12084, P499, DOI 10.1007/978-3-030-47426-3_39
   Jian M, 2016, IEEE T MULTIMEDIA, V18, P458, DOI 10.1109/TMM.2016.2515367
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kingma D. P., 2014, Advances in neural information processing systems, P3581
   Krause J., 2013, 2 WORKSH FIN GRAIN V
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lee D.-H., 2013, PSEUDOLABEL SIMPLE E, V3, P896
   Li J., 2021, PROC INT C LEARN REP
   Li ZX, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3426974
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Marussy K, 2013, LECT NOTES ARTIF INT, V7894, P437, DOI 10.1007/978-3-642-38658-9_39
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Oliver A, 2018, ADV NEUR IN, V31
   Rasmus A, 2015, ADV NEUR IN, V28
   Sohn K, 2020, Arxiv, DOI [arXiv:2001.07685, DOI 10.48550/ARXIV.2001.07685]
   Sukhbaatar S, 2015, Arxiv, DOI [arXiv:1406.2080, 10.48550/arXiv.1406.2080]
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P2837, DOI 10.1109/TMM.2019.2909860
   Tarvainen A, 2017, ADV NEUR IN, V30
   Tian YL, 2020, Arxiv, DOI arXiv:1906.05849
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Wu S, 2018, IEEE T MULTIMEDIA, V20, P851, DOI 10.1109/TMM.2017.2758522
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xie JH, 2021, Arxiv, DOI arXiv:2008.11702
   Xie QZ, 2020, Arxiv, DOI arXiv:1904.12848
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zou FH, 2019, WORLD WIDE WEB, V22, P825, DOI 10.1007/s11280-018-0581-2
NR 51
TC 9
Z9 9
U1 8
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1749
EP 1759
DI 10.1109/TMM.2022.3158069
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, F
   Zhao, WD
   Lu, HM
   Liu, Y
   Yao, LB
   Liu, Y
AF Zhao, Fan
   Zhao, Wenda
   Lu, Huimin
   Liu, Yong
   Yao, Libo
   Liu, Yu
TI Depth-Distilled Multi-Focus Image Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Feature extraction; Lenses; Task analysis; Testing;
   Cameras; Adaptation models; Depth distillation; multi-focus image
   fusion; multi-level decision map fusion
ID ENHANCEMENT; TRANSFORM; FRAMEWORK; NETWORK
AB Homogeneous regions, which are smooth areas that lack blur clues to discriminate if they are focused or non-focused. Therefore, they bring a great challenge to achieve high accurate multi-focus image fusion (MFIF). Fortunately, we observe that depth maps are highly related to focus and defocus, containing a preponderance of discriminative power to locate homogeneous regions. This offers the potential to provide additional depth cues to assist MFIF task. Taking depth cues into consideration, in this paper, we propose a new depth-distilled multi-focus image fusion framework, namely D2MFIF. In D2MFIF, depth-distilled model (DDM) is designed for adaptively transferring the depth knowledge into MFIF task, gradually improving MFIF performance. Moreover, multi-level fusion mechanism is designed to integrate multi-level decision maps from intermediate outputs for improving the final prediction. Visually and quantitatively experimental results demonstrate the superiority of our method over several state-of-the-art methods.
C1 [Zhao, Fan] Liaoning Normal Univ, Sch Phys & Elect Technol, Dalian 116029, Peoples R China.
   [Zhao, Wenda] Dalian Univ Technol, Key Lab Intelligent Control & Optimizat Ind Equipm, Minist Educ, Dalian 116024, Peoples R China.
   [Lu, Huimin] Kyushu Inst Technol, Kyushu, Japan.
   [Liu, Yong] Acad Mil Sci, Natl Innovat Inst Def Technol, Beijing 100052, Peoples R China.
   [Yao, Libo; Liu, Yu] Naval Aviat Univ, Res Inst informat Fus, Yantai 264001, Peoples R China.
C3 Liaoning Normal University; Dalian University of Technology; Kyushu
   Institute of Technology
RP Zhao, WD (corresponding author), Dalian Univ Technol, Key Lab Intelligent Control & Optimizat Ind Equipm, Minist Educ, Dalian 116024, Peoples R China.
EM fan_zhao20@163.com; zhaowenda@dlut.edu.cn; luhuimin@ieee.org;
   xhliuyong@sina.com; b_rs@126.com; liuyu77360132@126.com
RI Yao, Libo/AHA-0939-2022; Lu, Huimin/A-6540-2012
OI Yao, Libo/0000-0001-7183-529X; Lu, Huimin/0000-0001-9794-3221; Zhao,
   Fan/0000-0002-4826-2769
FU National Natural Science Foundation of China [62001450, 62176038,
   61801077, 61901504]
FX Manuscript received 21 July 2021; revised 1 November 2021; accepted 28
   November 2021. Date of publication 13 December 2021; date of current
   version 9 March 2023. This work was supported by the National Natural
   Science Foundation of China under Grants 62001450, 62176038, 61801077,
   and 61901504. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Mai Xu.
   (Corresponding author: Wenda Zhao.)
CR Aymaz S, 2019, INFORM FUSION, V45, P113, DOI 10.1016/j.inffus.2018.01.015
   Bouzos O, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2922097
   Carvalho M., 2018, 2018 IEEE 20th International Conference on e-Health Networking, Applications and Services (Healthcom), P1
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P655, DOI 10.1109/TMM.2021.3057493
   Chen YB, 2018, IEEE T IMAGE PROCESS, V27, P1526, DOI 10.1109/TIP.2017.2779274
   Chen YJ, 2021, IEEE T IMAGE PROCESS, V30, P4008, DOI 10.1109/TIP.2021.3068645
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   He KJ, 2019, SOFT COMPUT, V23, P4685, DOI 10.1007/s00500-018-3118-9
   He KJ, 2018, NEUROCOMPUTING, V320, P157, DOI 10.1016/j.neucom.2018.09.018
   Hu XW, 2019, IEEE I CONF COMP VIS, P2472, DOI 10.1109/ICCV.2019.00256
   Kausar N, 2016, COMPUT ELECTR ENG, V54, P393, DOI 10.1016/j.compeleceng.2016.01.013
   Kingma D. P., 2014, arXiv
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Ma B., 2020, END TO END LEARNING
   Ma BY, 2021, NEURAL COMPUT APPL, V33, P5793, DOI 10.1007/s00521-020-05358-9
   Nava R., 2007, Proc. SPIE, V34, P94
   Ni BB, 2013, IEEE T CYBERNETICS, V43, P1383, DOI 10.1109/TCYB.2013.2276433
   Paul S, 2016, J CIRCUIT SYST COMP, V25, DOI 10.1142/S0218126616501231
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qiang Z., 2020, PATTERN RECOGN, V104
   Qiang Zhang, 2021, Pattern Recognition, V113, DOI 10.1016/j.patcog.2020.107752
   Qiu XH, 2019, SIGNAL PROCESS-IMAGE, V72, P35, DOI 10.1016/j.image.2018.12.004
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Singh R, 2008, PATTERN RECOGN, V41, P880, DOI 10.1016/j.patcog.2007.06.022
   Song G, 2018, IEEE IMAGE PROC, P1563, DOI 10.1109/ICIP.2018.8451201
   Sujatha K, 2018, MULTIMED TOOLS APPL, V77, P1735, DOI 10.1007/s11042-016-4312-3
   Vishwakarma A, 2019, IEEE T INSTRUM MEAS, V68, P3367, DOI 10.1109/TIM.2018.2877285
   Wang Y., 2021, IEEE T NEUR NET LEAR
   Wei YX, 2021, PROC CVPR IEEE, P13380, DOI 10.1109/CVPR46437.2021.01318
   Xiao B, 2020, IEEE T MULTIMEDIA, V22, P285, DOI 10.1109/TMM.2019.2928516
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Y. L. A. S. L. A. and Z. W. A. B., 2015, INFORM FUSION, V23, P139
   Yang Y, 2019, IEEE T COMPUT IMAG, V5, P262, DOI 10.1109/TCI.2018.2889959
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Zhang LX, 2019, INT J PATTERN RECOGN, V33, DOI 10.1142/S0218001419540107
   Zhang Q, 2018, PATTERN RECOGN, V83, P299, DOI 10.1016/j.patcog.2018.06.003
   Zhang Q, 2016, IEEE T IMAGE PROCESS, V25, P2045, DOI 10.1109/TIP.2016.2524212
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhao F, 2022, IEEE T CIRC SYST VID, V32, P2719, DOI 10.1109/TCSVT.2021.3095347
   Zhao F, 2021, INFORM FUSION, V76, P189, DOI 10.1016/j.inffus.2021.06.002
   Zhao F, 2021, IEEE T MULTIMEDIA, V23, P2745, DOI 10.1109/TMM.2020.3016123
   Zhao WD, 2019, IEEE T CIRC SYST VID, V29, P1102, DOI 10.1109/TCSVT.2018.2821177
   Zhao WD, 2020, IEEE T PATTERN ANAL, V42, P1884, DOI 10.1109/TPAMI.2019.2906588
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zheng YF, 2007, INFORM FUSION, V8, P177, DOI 10.1016/j.inffus.2005.04.003
NR 48
TC 19
Z9 19
U1 20
U2 56
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 966
EP 978
DI 10.1109/TMM.2021.3134565
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900022
DA 2024-07-18
ER

PT J
AU Zhao, YZ
   Po, LM
   Wang, XH
   Yan, Q
   Shen, W
   Zhang, YJ
   Liu, W
   Wong, CK
   Pang, CS
   Ou, WF
   Yu, WY
   Liu, BH
AF Zhao, Yuzhi
   Po, Lai-Man
   Wang, Xuehui
   Yan, Qiong
   Shen, Wei
   Zhang, Yujia
   Liu, Wei
   Wong, Chun-Kit
   Pang, Chiu-Sing
   Ou, Weifeng
   Yu, Wing-Yin
   Liu, Buhua
TI ChildPredictor: A Child Face Prediction Framework With Disentangled
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Faces; Genetics; Generative adversarial networks;
   Training; Glass; Skin; Child face prediction; disentangled learning;
   generative adversarial network; image-to-image translation
ID GENERATIVE ADVERSARIAL NETWORKS
AB The appearances of children are inherited from their parents, whichmakes it feasible to predict them. Predicting realistic children's faces may help settle many social problems, such as ageinvariant face recognition, kinship verification, and missing child identification. It can be regarded as an image-to-image translation task. Existing approaches usually assume domain information in the image-to-image translation can be interpreted by "style", i.e., the separation of image content and style. However, such separation is improper for the child face prediction, because the facial contours between children and parents are not the same. To address this issue, we propose a new disentangled learning strategy for children's face prediction. We assume that children's faces are determined by genetic factors (compact family features, e.g., face contour), external factors (facial attributes irrelevant to prediction, such as moustaches and glasses), and variety factors (individual properties for each child). On this basis, we formulate predictions as a mapping from parents' genetic factors to children's genetic factors, and disentangle them from external and variety factors. In order to obtain accurate genetic factors and perform the mapping, we propose a ChildPredictor framework. It transfers human faces to genetic factors by encoders and back by generators. Then, it learns the relationship between the genetic factors of parents and children through a mapping function. To ensure the generated faces are realistic, we collect a large Family Face Database to train ChildPredictor and evaluate it on the FF-Database validation set. Experimental results demonstrate that ChildPredictor is superior to other well-known image-to-image translation methods in predicting realistic and diverse child faces. Implementation codes can be found at https:// github.com/ zhaoyuzhi/ChildPredictor.
C1 [Zhao, Yuzhi; Po, Lai-Man; Zhang, Yujia; Wong, Chun-Kit; Pang, Chiu-Sing; Ou, Weifeng; Yu, Wing-Yin] City Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
   [Wang, Xuehui; Shen, Wei] Shanghai Jiao Tong Univ, Artificial Intelligence Inst, Shanghai 201100, Peoples R China.
   [Yan, Qiong] SenseTime Res & Tetras AI, Hong Kong, Peoples R China.
   [Liu, Wei] ByteDance Ltd, Beijing 100080, Peoples R China.
   [Liu, Buhua] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China.
C3 City University of Hong Kong; Shanghai Jiao Tong University; Hong Kong
   Baptist University
RP Zhao, YZ (corresponding author), City Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
EM yzzhao2-c@my.cityu.edu.hk; eelmpo@cityu.edu.hk; wangxuehui@sjtu.edu.cn;
   sophie.yanqiong@gmail.com; wei.shen@sjtu.edu.cn;
   yzhang2383-c@my.cityu.edu.hk; liujikun63@gmail.com;
   ckwong535-c@my.cityu.edu.hk; chiuspang2-c@my.cityu.edu.hk;
   weifengou2-c@my.cityu.edu.hk; wingyinyu8-c@my.cityu.edu.hk;
   csbhliu@comp.hkbu.edu.hk
RI /ADN-5973-2022
OI /0000-0002-8908-3863; Zhao, Yuzhi/0000-0001-8561-2206; YU, Wing
   Yin/0000-0002-9559-1055
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Ali SS, 2019, PALGR STUD ISLAM BAN, P1, DOI 10.1007/978-3-030-12793-0_1
   [Anonymous], 2018, 2018 IEEE INT C COMM
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bau D., 2019, 7 INT C LEARNING REP
   Bhattacharjee D, 2020, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR42600.2020.00484
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Chandran PS, 2018, 2018 IEEE RECENT ADVANCES IN INTELLIGENT COMPUTATIONAL SYSTEMS (RAICS), P113, DOI 10.1109/RAICS.2018.8635054
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen D., 2020, Advances in Neural Information Processing Systems, V33, P12673
   Chen HT, 2020, AAAI CONF ARTIF INTE, V34, P3585
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dibeklioglu H, 2013, IEEE I CONF COMP VIS, P1497, DOI 10.1109/ICCV.2013.189
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fang RG, 2013, IEEE IMAGE PROC, P2983, DOI 10.1109/ICIP.2013.6738614
   Gabbay A., 2019, STYLE GENERATOR INVE
   GAO P, 2021, ASME INT MECH ENG C, V2, P1, DOI DOI 10.1115/IMECE2021-73392
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Goetschalckx L, 2019, IEEE I CONF COMP VIS, P5743, DOI 10.1109/ICCV.2019.00584
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu SH, 2019, IEEE I CONF COMP VIS, P2511, DOI 10.1109/ICCV.2019.00260
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   HE Z, 2020, PA GAN PROGRESSIVE A
   He Z., 2021, P IEEE CVF INT C COM, P14408
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Heusel M., 2017, Advances in Neural Information Processing Systems, P6627, DOI [DOI 10.48550/ARXIV.1706.08500, 10.48550/arXiv.1706.08500]
   Hua M, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION SYSTEMS (ICCS 2018), P1, DOI 10.1109/ICCS.2018.8689226
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Huang XW, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS, TECHNOLOGIES AND APPLICATIONS (ICTA 2018), P172, DOI 10.1109/CICTA.2018.8706048
   Huang ZZ, 2021, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR46437.2021.00720
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kim T., 2017, P 34 INT C MACH LEAR, P1857, DOI DOI 10.1109/WPT.2017.7953894
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618
   Li WH, 2021, PROC CVPR IEEE, P16130, DOI 10.1109/CVPR46437.2021.01587
   Li XY, 2021, PROC CVPR IEEE, P8635, DOI 10.1109/CVPR46437.2021.00853
   Lin J., 2020, EUROPEAN C COMPUTER, P18
   Lin Z., 2021, INT C ARTIFICIAL INT, V130, P1522
   Liu T., 2017, P INT C ADV NEUR INF, V30, P700, DOI DOI 10.48550/ARXIV.1703.00848
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu YH, 2021, PROC CVPR IEEE, P10780, DOI 10.1109/CVPR46437.2021.01064
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Nitzan Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417826
   Odena A, 2017, PR MACH LEARN RES, V70
   Park U, 2010, IEEE T PATTERN ANAL, V32, P947, DOI 10.1109/TPAMI.2010.14
   Perarnau G., 2016, NIPS WORKSH ADV TRAI
   Pizzati F, 2021, PROC CVPR IEEE, P14283, DOI 10.1109/CVPR46437.2021.01406
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   RAMEEN A, 2020, P IEEE CVF C COMPUT, P8293
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Robinson JP, 2018, IEEE T PATTERN ANAL, V40, P2624, DOI 10.1109/TPAMI.2018.2826549
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Song LX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P627, DOI 10.1145/3240508.3240612
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Ulyanov Dmitry, 2016, arXiv
   Viazovetskyi Yuri, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P170, DOI 10.1007/978-3-030-58542-6_11
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang O., 2020, PROC IEEECVF C COMPU, V7, P8695
   Wang XZ, 2018, PLATELETS, V29, P48, DOI 10.1080/09537104.2017.1293807
   Wei Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P613, DOI 10.1007/978-3-030-58542-6_37
   XIA S, 2011, P INT JOINT C ART IN, P2539
   XIAO T, 2020, P AAAI C ARTIF INTEL, P12434
   Xiao TH, 2018, LECT NOTES COMPUT SC, V11214, P172, DOI 10.1007/978-3-030-01249-6_11
   Xie L., 2018, DIFFERENTIALLY PRIVA
   Xinqi Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P684, DOI 10.1007/978-3-030-58607-2_40
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang G, 2018, LECT NOTES COMPUT SC, V11210, P422, DOI 10.1007/978-3-030-01231-1_26
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhao J, 2022, IEEE T PATTERN ANAL, V44, P474, DOI 10.1109/TPAMI.2020.3011426
   ZHOU S, 2017, P BRIT MACH VIS C
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   ZHU X, 2021, P IEEECVF C COMPUTER, P5861
NR 94
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3737
EP 3752
DI 10.1109/TMM.2022.3164785
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, C
   Jiang, M
   Kong, J
AF Zhou, Chen
   Jiang, Min
   Kong, Jun
TI BGTracker: Cross-Task Bidirectional Guidance Strategy for Multiple
   Object Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Object tracking; Feature extraction; Target tracking;
   Optimization; Trajectory; Transformers; Multiple object tracking;
   joint-detection-and-embedding; real-time tracking; bidirectional
   guidance; feature decoupling; Transformer encoder
ID SIAMESE NETWORKS; ONLINE TRACKING
AB Recent works have shown that the joint-detection-and-embedding (JDE) paradigm has significantly enhanced the performance of multiple object tracking by simultaneously learning detection and re-identification features. These methods always utilize a weight-shared backbone network and two non-interactive branches for different tasks. This non-interactive multi-task learning strategy cannot make full use of geometric and semantic information between detection and re-identification tasks. And in the JDE paradigm, there exists a feature misalignment between detection and re-identification due to their different optimization directions. In this article, BGTracker is proposed as a novel online tracking framework with a cross-task bidirectional guidance strategy between detection and re-identification. Firstly, we propose a Channel-based Decoupling module and Cross-direction Transformer to alleviate feature misalignment, which can obtain task-aligned embeddings and discriminative representations at the feature level. Then, we propose the bidirectional guidance strategy to link the two tasks by the prediction map's statistical information. In this strategy, two designed feature transformations are employed to utilize the advantages of each task for complementing each other at the task level. Finally, extensive experiments demonstrate that the proposed BGTracker outperforms various existing methods on the MOTChallenge benchmarks.
C1 [Zhou, Chen; Jiang, Min] Jiangnan Univ, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Peoples R China.
   [Kong, Jun] Jiangnan Univ, Key Lab Adv Proc Control Light Ind, Minist Educ, Wuxi 214122, Peoples R China.
C3 Jiangnan University; Jiangnan University
RP Jiang, M (corresponding author), Jiangnan Univ, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Peoples R China.
EM 6201910042@stu.jiangnan.edu.cn; minjiang@jiangnan.edu.cn;
   kongjun@jiangnan.edu.cn
RI JIANG, MIN/KSM-4856-2024
OI Tao, Xuefeng/0000-0003-1142-619X; Kong, Jun/0000-0003-2551-4748
FU Fundamental Research Funds for the Central Universities
FX No Statement Available
CR Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Brasó G, 2020, PROC CVPR IEEE, P6246, DOI 10.1109/CVPR42600.2020.00628
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen L, 2018, IEEE INT CON MULTI, DOI 10.1097/PEC.0000000000001553
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chenkai Yu, 2020, arXiv
   Dai P, 2019, IEEE T MULTIMEDIA, V21, P1709, DOI 10.1109/TMM.2018.2885922
   Dendorfer P., 2020, arXiv
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Ess A, 2008, PROC CVPR IEEE, P1857
   Fan H., 2020, 2020 EUR C COMP VIS, P713
   Feichtenhofer C, 2017, IEEE I CONF COMP VIS, P3057, DOI 10.1109/ICCV.2017.330
   Gao TZ, 2022, IEEE T MULTIMEDIA, V24, P995, DOI 10.1109/TMM.2021.3062489
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo S, 2021, PROC CVPR IEEE, P8132, DOI 10.1109/CVPR46437.2021.00804
   Henschel R, 2019, IEEE COMPUT SOC CONF, P770, DOI 10.1109/CVPRW.2019.00105
   Janai J, 2020, FOUND TRENDS COMPUT, V12, P1, DOI 10.1561/0600000079
   Jiang M, 2021, IEEE T CIRC SYST VID, V31, P3154, DOI 10.1109/TCSVT.2020.3037947
   Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kingma D, 2014, ICLR P, V2014, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Leal-Taix‚ L, 2015, Arxiv, DOI arXiv:1504.01942
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li X, 2021, PROC CVPR IEEE, P11627, DOI 10.1109/CVPR46437.2021.01146
   Liang C, 2022, IEEE T IMAGE PROCESS, V31, P3182, DOI 10.1109/TIP.2022.3165376
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Oh SM, 2011, PROC CVPR IEEE
   Pang B, 2020, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR42600.2020.00634
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shao S, 2018, Arxiv, DOI [arXiv:1805.00123, DOI 10.48550/ARXIV.1805.00123]
   Shen JB, 2018, IEEE T INTELL TRANSP, V19, P162, DOI 10.1109/TITS.2017.2750082
   Sheng H, 2019, IEEE T CIRC SYST VID, V29, P3269, DOI 10.1109/TCSVT.2018.2882192
   Shuai B, 2021, PROC CVPR IEEE, P12367, DOI 10.1109/CVPR46437.2021.01219
   Stadler D, 2021, PROC CVPR IEEE, P10953, DOI 10.1109/CVPR46437.2021.01081
   Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252
   Sun PZ, 2021, Arxiv, DOI arXiv:2012.15460
   Sun ZH, 2021, IEEE T CIRC SYST VID, V31, P1819, DOI 10.1109/TCSVT.2020.3009717
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Tokmakov P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10840, DOI 10.1109/ICCV48922.2021.01068
   Vaswani A, 2017, ADV NEUR IN, V30
   Voigtlaender P, 2019, PROC CVPR IEEE, P7934, DOI 10.1109/CVPR.2019.00813
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang Q, 2021, PROC CVPR IEEE, P3875, DOI 10.1109/CVPR46437.2021.00387
   Wang YX, 2021, IEEE INT CONF ROBOT, P13708, DOI 10.1109/ICRA48506.2021.9561110
   Wen LY, 2014, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2014.167
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu JL, 2021, PROC CVPR IEEE, P12347, DOI 10.1109/CVPR46437.2021.01217
   Xiang J, 2021, IEEE T CIRC SYST VID, V31, P275, DOI 10.1109/TCSVT.2020.2975842
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680
   Yu E, 2023, IEEE T MULTIMEDIA, V25, P2686, DOI 10.1109/TMM.2022.3150169
   Yu FW, 2016, LECT NOTES COMPUT SC, V9914, P36, DOI 10.1007/978-3-319-48881-3_3
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Yu RX, 2018, IEEE T CIRC SYST VID, V28, P1623, DOI 10.1109/TCSVT.2017.2668278
   Zeng FG, 2022, LECT NOTES COMPUT SC, V13687, P659, DOI 10.1007/978-3-031-19812-0_38
   Zhang Y, 2020, IEEE INTERNET THINGS, V7, P7892, DOI 10.1109/JIOT.2020.2996609
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zheng Liang, 2017, P IEEE C COMP VIS PA, P3346, DOI DOI 10.1109/CVPR.2017.357
   Zhichao Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14656, DOI 10.1109/CVPR42600.2020.01468
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7
   Zhou QQ, 2019, IEEE T MULTIMEDIA, V21, P1183, DOI 10.1109/TMM.2018.2875360
   Zhou XY, 2022, PROC CVPR IEEE, P8761, DOI 10.1109/CVPR52688.2022.00857
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
   Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23
NR 77
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8132
EP 8144
DI 10.1109/TMM.2023.3256761
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000053
DA 2024-07-18
ER

PT J
AU Chen, W
   Liu, Y
   Pu, N
   Wang, WP
   Liu, L
   Lew, MS
AF Chen, Wei
   Liu, Yu
   Pu, Nan
   Wang, Weiping
   Liu, Li
   Lew, Michael S.
TI Feature Estimations Based Correlation Distillation for Incremental Image
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Correlation; Data models; Modeling; Training; Context
   modeling; Image retrieval; Incremental learning; fine-grained image
   retrieval; correlations distillation; feature estimation
ID KNOWLEDGE
AB Deep learning for fine-grained image retrieval in an incremental context is less investigated. In this paper, we explore this task to realize the model's continuous retrieval ability. That means, the model enables to perform well on new incoming data and reduce forgetting of the knowledge learned on preceding old tasks. For this purpose, we distill semantic correlations knowledge among the representations extracted from the new data only so as to regularize the parameters updates using the teacher-student framework. In particular, for the case of learning multiple tasks sequentially, aside from the correlations distilled from the penultimate model, we estimate the representations for all prior models and further their semantic correlations by using the representations extracted from the new data. To this end, the estimated correlations are used as an additional regularization and further prevent catastrophic forgetting over all previous tasks, and it is unnecessary to save the stream of models trained on these tasks. Extensive experiments demonstrate that the proposed method performs favorably for retaining performance on the already-trained old tasks and achieving good accuracy on the current task when new data are added at once or sequentially.
C1 [Chen, Wei; Pu, Nan; Lew, Michael S.] Leiden Univ, Leiden Inst Adv Comp Sci, NL-2311 EZ Leiden, Netherlands.
   [Liu, Yu] Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Dalian 116024, Peoples R China.
   [Wang, Weiping; Liu, Li] NUDT, Coll Syst Engn, Changsha 410073, Peoples R China.
   [Liu, Li] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland.
C3 Leiden University - Excl LUMC; Leiden University; Dalian University of
   Technology; National University of Defense Technology - China;
   University of Oulu
RP Chen, W (corresponding author), Leiden Univ, Leiden Inst Adv Comp Sci, NL-2311 EZ Leiden, Netherlands.
EM w.chen@liacs.leidenuniv.nl; liuyu8824@dlut.edu.cn;
   n.pu@liacs.leidenuniv.nl; wangwp@nudt.edu.cn; li.liu@oulu.fi;
   m.s.k.lew@liacs.leidenuniv.nl
RI Pu, Nan/JMQ-1195-2023
OI Pu, Nan/0000-0002-2179-8301; Liu, li/0000-0002-2011-2873; CHEN,
   Wei/0000-0001-7875-4548
FU National Natural Science Foundation of China [71701205, 62022091]; China
   Scholarship Council (CSC) [201703170183]; LIACS Media Laboratory at
   Leiden University
FX This work was supported in part by LIACS Media Laboratory at Leiden
   University, China Scholarship Council (CSC No. 201703170183), and in
   part by the National Natural Science Foundation of China under Grants
   71701205 and 62022091.
CR Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753
   Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33
   Chebotar Y, 2016, INTERSPEECH, P3439, DOI 10.21437/Interspeech.2016-1190
   Chen W.., 2020, PROC BRIT MACH VIS C, P1
   Dean J., 2015, NIPS DEEP LEARNING R
   Del Chiaro R., 2020, Advances in Neural Information Processing Systems, V33, P16736
   Fu CM, 2019, IEEE I CONF COMP VIS, P6320, DOI 10.1109/ICCV.2019.00641
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Hou SH, 2018, LECT NOTES COMPUT SC, V11207, P452, DOI 10.1007/978-3-030-01219-9_27
   Huang X, 2019, IEEE T MULTIMEDIA, V21, P2850, DOI 10.1109/TMM.2019.2911456
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jung H, 2018, AAAI CONF ARTIF INTE, P3358
   Khosla A., 2011, P IEEE C COMP VIS PA, V2
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Lee K, 2019, IEEE I CONF COMP VIS, P312, DOI 10.1109/ICCV.2019.00040
   Li H, 2019, ADV ENERGY MATER, V9, DOI 10.1002/aenm.201802930
   Li W., 2018, ARXIV180505510
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Mandal D., 2018, P AS C COMP VIS, P305
   Mandal D., 2020, ARXIV200200677
   McCloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P109, DOI [10.1016/S0079-7421(08)60536-8, DOI 10.1016/S0079-7421(08)60536-8]
   Michieli U, 2019, IEEE INT CONF COMP V, P3205, DOI 10.1109/ICCVW.2019.00400
   Ng WWY, 2019, IEEE T CYBERNETICS, V49, P3844, DOI 10.1109/TCYB.2018.2846760
   Park D, 2019, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2019.00343
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Parshotam K.., 2020, P IEEE C COMP VIS PA, P224
   Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511
   Peng YX, 2019, IEEE T MULTIMEDIA, V21, P1538, DOI 10.1109/TMM.2018.2877885
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Perez-Rua Juan-Manuel, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13843, DOI 10.1109/CVPR42600.2020.01386
   Qi JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P528, DOI 10.1145/3240508.3240558
   Shin H, 2017, ADV NEUR IN, V30
   Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Tian X., 2020, IEEE T MULTIMEDIA
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Upchurch P, 2017, PROC CVPR IEEE, P6090, DOI 10.1109/CVPR.2017.645
   van de Ven G. M., 2018, ARXIV180910635
   Wah Catherine, 2011, Technical report
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wang Y, 2017, AAAI CONF ARTIF INTE, P2739
   Wu C., 2018, NEURIPS, P5962
   Wu DY, 2019, PROC CVPR IEEE, P9061, DOI 10.1109/CVPR.2019.00928
   Wu Y, 2019, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.2019.00046
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Yao X, 2019, NEURAL COMPUT, V31, P2266, DOI 10.1162/neco_a_01232
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yu L, 2019, PROC CVPR IEEE, P2902, DOI 10.1109/CVPR.2019.00302
   Yu Z., 2020, CVPR, P12856
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhai MY, 2019, IEEE I CONF COMP VIS, P2759, DOI 10.1109/ICCV.2019.00285
   Zhang B, 2019, IEEE-ACM T AUDIO SPE, V27, P2278, DOI 10.1109/TASLP.2019.2946480
   Zhou P.., 2020, PROC BRIT MACH VIS C, P1
NR 57
TC 11
Z9 11
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1844
EP 1856
DI 10.1109/TMM.2021.3073279
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0I7PA
UT WOS:000779607000003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Chen, YY
   Wang, SQ
   Xiao, XL
   Liu, YF
   Hua, ZY
   Zhou, YC
AF Chen, Yongyong
   Wang, Shuqin
   Xiao, Xiaolin
   Liu, Youfa
   Hua, Zhongyun
   Zhou, Yicong
TI Self-Paced Enhanced Low-Rank Tensor Kernelized Multi-View Subspace
   Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tensors; Kernel; Streaming media; Feature extraction; Videos;
   Reliability; Clustering methods; Multi-view clustering; low-rank tensor
   representation; kernel; enhanced low-rank representation; self-paced
   learning
ID FUSION; GRAPH
AB This paper addresses the multi-view subspace clustering problem and proposes the self-paced enhanced low-rank tensor kernelized multi-view subspace clustering (SETKMC) method, which is based on two motivations: (1) singular values of the representations and multiple instances should be treated differently. The reasons are that larger singular values of the representations usually quantify the major information and should be less penalized; samples with different degrees of noise may have various reliability for clustering. (2) many existing methods may cause the degraded performance when multi-view features reside in different nonlinear subspaces. This is because they usually assumed that multiple features lie within the union of several linear subspaces. SETKMC integrates the nonconvex tensor norm, self-paced learning, and kernel trick into a unified model for multi-view subspace clustering. The nonconvex tensor norm imposes different weights on different singular values. The self-paced learning gradually involves instances from more reliable to less reliable ones while the kernel trick aims to handle the multi-view data in nonlinear subspaces. One iterative algorithm is proposed based on the alternating direction method of multipliers. Extensive results on seven real-world datasets show the effectiveness of the proposed SETKMC compared to fifteen state-of-the-art multi-view clustering methods.
C1 [Chen, Yongyong; Hua, Zhongyun] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Chen, Yongyong; Hua, Zhongyun] Harbin Inst Technol, Biocomp Res Ctr, Shenzhen 518055, Peoples R China.
   [Chen, Yongyong; Hua, Zhongyun] Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Wang, Shuqin] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Xiao, Xiaolin] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Liu, Youfa] Huazhong Agr Univ, Coll Informat, Wuhan 430070, Peoples R China.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 Harbin Institute of Technology; Harbin Institute of Technology; Beijing
   Jiaotong University; South China University of Technology; Huazhong
   Agricultural University; University of Macau
RP Hua, ZY (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.; Hua, ZY (corresponding author), Harbin Inst Technol, Biocomp Res Ctr, Shenzhen 518055, Peoples R China.; Liu, YF (corresponding author), Huazhong Agr Univ, Coll Informat, Wuhan 430070, Peoples R China.
EM YongyongChen.cn@gmail.com; ShuqinWang.cn@hotmail.com;
   shellyxiaolin@gmail.com; liuyoufa@mail.hzau.edu.cn; huazyum@gmail.com;
   yicongzhou@una.edu.mo
RI Zhou, Yicong/A-8017-2009; Chen, yongyong/U-8997-2019; Hua,
   Zhongyun/F-1887-2016
OI Zhou, Yicong/0000-0002-4487-6384; Chen, yongyong/0000-0003-1970-1993;
   Hua, Zhongyun/0000-0002-3529-0541; Liu, Youfa/0000-0002-3540-5775
FU National Natural Science Foundation of China [62106063, 62106081,
   62071142]; Shenzhen College Stability Support Plan
   [GXWD20201230155427003-20200824113231001,
   GXWD20201230155427003-20200824210638001]; Fundamental Research Funds for
   the Central Universities [2662020XXQD002]; University of Macau
   [MYRG2018-00136-FST]; Cultivation Project [2662021JC008]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62106063, 62106081, and 62071142, in
   part by Shenzhen College Stability Support Plan under Grants
   GXWD20201230155427003-20200824113231001 and
   GXWD20201230155427003-20200824210638001, in part by the Fundamental
   Research Funds for the Central Universities under Grant 2662020XXQD002,
   in part by Cultivation Project under Grant 2662021JC008, and in part by
   the University of Macau (File no. MYRG2018-00136-FST).
CR Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Brbic M, 2020, IEEE T CYBERNETICS, V50, P1711, DOI 10.1109/TCYB.2018.2883566
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Chao Guoqing, 2021, IEEE Trans Artif Intell, V2, P146, DOI 10.1109/tai.2021.3065894
   Chen YY, 2022, IEEE T CIRC SYST VID, V32, P92, DOI 10.1109/TCSVT.2021.3055625
   Chen YY, 2021, IEEE T IMAGE PROCESS, V30, P4022, DOI 10.1109/TIP.2021.3068646
   Chen YY, 2022, IEEE T NEUR NET LEAR, V33, P4712, DOI 10.1109/TNNLS.2021.3059874
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Chen YY, 2020, IEEE T IMAGE PROCESS, V29, P1426, DOI 10.1109/TIP.2019.2941319
   Chen YY, 2019, IEEE INT CON MULTI, P1348, DOI 10.1109/ICME.2019.00234
   Chen YY, 2017, IEEE T GEOSCI REMOTE, V55, P5366, DOI 10.1109/TGRS.2017.2706326
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Gao QX, 2021, IEEE T PATTERN ANAL, V43, P2133, DOI 10.1109/TPAMI.2020.3017672
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Huang SD, 2019, PATTERN RECOGN, V88, P174, DOI 10.1016/j.patcog.2018.11.007
   Ikizler N., 2008, ICPR, P1, DOI DOI 10.1109/ICPR.2008.4761663
   Jia YH, 2021, IEEE T CIRC SYST VID, V31, P4784, DOI 10.1109/TCSVT.2021.3055039
   Jiang L, 2014, ADV NEUR IN, V27
   Li RH, 2019, IEEE I CONF COMP VIS, P8171, DOI 10.1109/ICCV.2019.00826
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu YF, 2019, IEEE INT CON MULTI, P350, DOI 10.1109/ICME.2019.00068
   Lu CY, 2019, IEEE T PATTERN ANAL, V41, P487, DOI 10.1109/TPAMI.2018.2794348
   Lu CY, 2015, AAAI CONF ARTIF INTE, P1805
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Pawan Kumar M., 2010, NIPS
   Shen JL, 2021, INFORM SCIENCES, V569, P469, DOI 10.1016/j.ins.2020.11.026
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tzortzis G, 2012, IEEE DATA MINING, P675, DOI 10.1109/ICDM.2012.43
   Wang BY, 2021, IEEE T NEUR NET LEAR, V32, P3484, DOI 10.1109/TNNLS.2020.3011717
   Wang QQ, 2021, IEEE T MULTIMEDIA, V23, P3483, DOI 10.1109/TMM.2020.3025666
   Wu JL, 2020, AAAI CONF ARTIF INTE, V34, P6388
   Wu JL, 2019, IEEE T IMAGE PROCESS, V28, P5910, DOI 10.1109/TIP.2019.2916740
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Xia W, 2021, IEEE T MULTIMEDIA, V24, P3182, DOI 10.1109/TMM.2021.3094296
   Xiao SJ, 2016, IEEE T NEUR NET LEAR, V27, P2268, DOI 10.1109/TNNLS.2015.2472284
   Xiao XL, 2021, IEEE T MULTIMEDIA, V23, P4555, DOI 10.1109/TMM.2020.3045259
   Xie L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3133
   Xie XY, 2018, IEEE T IMAGE PROCESS, V27, P477, DOI 10.1109/TIP.2017.2764262
   Xie Y, 2021, IEEE T NEUR NET LEAR, V32, P868, DOI 10.1109/TNNLS.2020.2979685
   Xie Y, 2020, IEEE T CYBERNETICS, V50, P572, DOI 10.1109/TCYB.2018.2869789
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Xu C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3974
   Xu HL, 2020, NEURAL NETWORKS, V132, P245, DOI 10.1016/j.neunet.2020.08.019
   Yang Y, 2018, BIG DATA MIN ANAL, V1, P83, DOI 10.26599/BDMA.2018.9020003
   Yin M, 2016, IEEE T PATTERN ANAL, V38, P504, DOI 10.1109/TPAMI.2015.2462360
   Yu S, 2012, IEEE T PATTERN ANAL, V34, P1031, DOI 10.1109/TPAMI.2011.255
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhao Q, 2015, AAAI CONF ARTIF INTE, P3196
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
   Zhou P, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2133
   Zhou P, 2021, IEEE T NEUR NET LEAR, V32, P1497, DOI 10.1109/TNNLS.2020.2984814
   Zhou T, 2020, IEEE T CYBERNETICS, V50, P3517, DOI 10.1109/TCYB.2019.2918495
NR 60
TC 22
Z9 22
U1 6
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4054
EP 4066
DI 10.1109/TMM.2021.3112230
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400027
DA 2024-07-18
ER

PT J
AU Ding, YJ
   Ma, YS
   Wong, WK
   Chua, TS
AF Ding, Yujuan
   Ma, Yunshan
   Wong, Wai Keung
   Chua, Tat-Seng
TI Modeling Instant User Intent and Content-Level Transition for Sequential
   Fashion Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Markov processes; Color; Aggregates; Recommender systems;
   Visualization; Semantics; Fashion recommendation; Instant intent
   modeling; Translation method
AB Fashion recommendation, aiming to explore specific user preference in fashion, has become an important research topic for its practical significance to the fashion business sector. However, little work has been done on an important sub-task called sequential fashion recommendation, which aims to capture additional short-term fashion interest of users by modeling the item-to-item transitions. In this paper, we propose a novel Attentional Content-level Translation-based Recommender (ACTR) framework, which simultaneously models the instant user intent of each transition and the intent-specific transition probability. Specifically, we define instant intent with the relationships between adjacent items that the users interacted, which are the three fundamental domain-specific relationships of: match, substitute and others. To further exploit the characteristics of fashion domain and alleviate the item transition sparsity problem, we augment the item-level transition modeling with multiple sub-transitions using various content-level attributes. An attention mechanism is further devised to effectively aggregate multiple content-level transitions. To the best of our knowledge, this is the first work that specifies the implicit user actions in online fashion shopping with explicit instant intent, which enhances the connectivity of fashion items and boosts the recommendation performance. Extensive experiments on two real-world fashion E-commerce datasets demonstrate the effectiveness of the proposed method in sequential fashion recommendation.
C1 [Ding, Yujuan] Hong Kong Polytech Univ, Inst Texitiles & Clothing, Kowloon, Hong Kong, Peoples R China.
   [Wong, Wai Keung] Hong Kong Polytech Univ, Kowloon, Hong Kong, Peoples R China.
   [Wong, Wai Keung] Lab Artificial Intelligence Design, Kowloon, Hong Kong, Peoples R China.
   [Ma, Yunshan; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117416, Singapore.
C3 Hong Kong Polytechnic University; Hong Kong Polytechnic University;
   National University of Singapore
RP Wong, WK (corresponding author), Hong Kong Polytech Univ, Kowloon, Hong Kong, Peoples R China.
EM dingyujuan385@gmail.com; yunshan.ma@u.nus.edu; calvin.wong@polyu.edu.hk;
   dcscts@nus.edu.sg
OI Wong, Wai Keung/0000-0002-5214-7114; Lai, Zhihui/0000-0002-4388-3080
FU National Research Foundation, Singapore under its International Research
   Centres in Singapore Funding Initiative; Laboratory for Artificial
   Intelligence in Design, Hong Kong [RP3-1]
FX This work was in part supported by the Laboratory for Artificial
   Intelligence in Design (Project Code: RP3-1), Hong Kong, in part
   supported by the National Research Foundation, Singapore under its
   International Research Centres in Singapore Funding Initiative. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Zhu Liu.
CR [Anonymous], 2016, AAAI CONF ARTIF INTE
   Bordes A., 2013, P 26 INT C NEURAL IN, P2787
   Cardoso A, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P80, DOI 10.1145/3219819.3219888
   Chen T, 2019, PROC INT CONF DATA, P304, DOI 10.1109/ICDE.2019.00035
   Chen W., ARXIV190501866, V2019
   Ding Y., 2021, ARXIV210507585
   Elkahky A, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P278, DOI 10.1145/2736277.2741667
   Fang H., 2019, ARXIV190501997
   Feng SS, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2069
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Graves A., 2013, GENERATING SEQUENCES
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   He RN, 2017, PROCEEDINGS OF THE ELEVENTH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'17), P161, DOI 10.1145/3109859.3109882
   He RN, 2016, IEEE DATA MINING, P191, DOI [10.1109/ICDM.2016.88, 10.1109/ICDM.2016.0030]
   He XN, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P355, DOI 10.1145/3077136.3080777
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Hidasi B., 2015, SESSION BASED RECOMM
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   Jing PG, 2020, IEEE T MULTIMEDIA, V22, P1555, DOI 10.1109/TMM.2019.2944749
   Kang WC, 2019, PROC CVPR IEEE, P10524, DOI 10.1109/CVPR.2019.01078
   Kang WC, 2018, IEEE DATA MINING, P197, DOI 10.1109/ICDM.2018.00035
   Kang WC, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1143, DOI 10.1145/3269206.3271792
   Kang WC, 2017, IEEE DATA MINING, P207, DOI 10.1109/ICDM.2017.30
   Krichene W, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1748, DOI 10.1145/3394486.340226
   Li J, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1419, DOI 10.1145/3132847.3132926
   Li X, 2020, ARXIV200512566
   Li Y., 2016, P 4 INT C LEARNING R
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Lin Y., 2015, PROC AAAI C ARTIF IN
   Lin YS, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P77, DOI 10.1145/3366423.3380096
   Liu Q, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1831, DOI 10.1145/3219819.3219950
   Liu X., 2020, IEEE T SYST MAN CY-S, P1, DOI DOI 10.1109/TSMC.2020.3035612
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Quadrana M, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3190616
   Quadrana M, 2017, PROCEEDINGS OF THE ELEVENTH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'17), P130, DOI 10.1145/3109859.3109896
   Rendle S., 2009, P 25 C UNC ART INT, P452, DOI DOI 10.5555/1795114.1795167
   Rendle S., 2010, P 19 INT C WORLD WID, P811, DOI DOI 10.1145/1772690.1772773
   Rendle S, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P635
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Tan Y. K., 2016, Proceedings of the 1st workshop on deep learning for recommender systems, P17
   Tang JX, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P565, DOI 10.1145/3159652.3159656
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Velickovic Petar, 2017, ARXIV171010903, DOI DOI 10.48550/ARXIV.1710.10903
   Wang PF, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P403, DOI 10.1145/2766462.2767694
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wang ZY, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P169, DOI 10.1145/3397271.3401142
   Wu S, 2016, PROC INT CONF DATA, P1218, DOI 10.1109/ICDE.2016.7498326
   Wu S, 2019, AAAI CONF ARTIF INTE, P346
   Yang X, 2019, AAAI CONF ARTIF INTE, P403
   Yu WH, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P649, DOI 10.1145/3178876.3186146
   Zhang FZ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P353, DOI 10.1145/2939672.2939673
   Zhang SA, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3285029
   Zheng H., 2020, ARXIV200512439
NR 53
TC 8
Z9 8
U1 2
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2687
EP 2700
DI 10.1109/TMM.2021.3088281
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600032
DA 2024-07-18
ER

PT J
AU Li, PD
   Xie, HT
   Min, SB
   Zha, ZJ
   Zhang, YD
AF Li, Pandeng
   Xie, Hongtao
   Min, Shaobo
   Zha, Zheng-Jun
   Zhang, Yongdong
TI Online Residual Quantization Via Streaming Data Correlation Preserving
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Balanced affinity matrix; online retrieval; residual quantization
   module; streaming data
ID PRODUCT QUANTIZATION; IMAGE RETRIEVAL; DATA SET; OBJECT
AB Recently, the online retrieval task has been receiving widespread attention, which is closely related to many real-world applications. However, existing online retrieval methods based on hashing suffer from two main problems: a) the models tend to be biased towards the current streaming data due to unavailable history streaming data; h) when new streaming data comes in and the hashing functions have been updated, all history binary codes should he recomputed, which takes much computation burden. To address the above two issues, we propose a novel Online Residual Quantization (ORQ) method that can achieve efficient streaming data quantization via the small-scale residual quantization codebooks. For the first problem, we design a residual quantization module by learning multiple residual codebooks to quantize the float streaming data, which effectively reduces the quantization error and enables the binary codes to be easily reconstructed back to original float data. Then, with the reconstructed history data, a balanced affinity matrix is developed to model the semantic relationship, e.g., similarity and difference, between the history and current data distributions, which can prevent the model from being biased towards the current data distribution. For the second problem, when inputting current streaming data, only the residual codehooks should he updated, instead of the whole history binary codes in hashing-based methods, which significantly reduces the computation burden. Comprehensive experiments on six benchmarks demonstrate that ORQ yields significant improvements (i.e., 1.2% similar to 4.9% in average mAP) compared to the state-of-the-art methods.
C1 [Li, Pandeng; Xie, Hongtao; Min, Shaobo; Zha, Zheng-Jun; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Xie, HT; Zhang, YD (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
EM lpd@mail.ustc.edu.cn; htxie@ustc.edu.cn; mbobo@mail.ustc.edu.cn;
   zhazj@ustc.edu.cn; zhyd73@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020
OI Li, Pandeng/0000-0002-0717-8659
FU National Key Research and Development Program of China [2017YFC0820600];
   National Nature Science Foundation of China [62022076, U1936210]; Youth
   Innovation Promotion Association Chinese Academy of Sciences [2017209]
FX Thisworkwas supported in part by theNationalKey Research and Development
   Program of China under Grant 2017YFC0820600, in part by the National
   Nature Science Foundation of China under Grants 62022076 and U1936210,
   and in part by the Youth Innovation Promotion Association Chinese
   Academy of Sciences under Grant 2017209.
CR [Anonymous], 2012, 20 ACM INT C MULT
   Babenko A, 2015, PROC CVPR IEEE, P4240, DOI 10.1109/CVPR.2015.7299052
   Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124
   Bjorck A., 1996, NUMERICAL METHODS LE
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Cakir F, 2017, IEEE I CONF COMP VIS, P437, DOI 10.1109/ICCV.2017.55
   Cakir F, 2017, COMPUT VIS IMAGE UND, V156, P162, DOI 10.1016/j.cviu.2016.10.009
   Cakir F, 2015, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2015.125
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chen X., 2017, UDORN DESIGN FRAMEWO, P1
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P1996, DOI 10.1109/TMM.2017.2705918
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Clarkson KL, 2009, ACM S THEORY COMPUT, P205
   Dietterich TG, 1994, J ARTIF INTELL RES, V2, P263
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Dmochowski JP, 2010, J MACH LEARN RES, V11, P3313
   Eghbali S, 2019, PROC CVPR IEEE, P11682, DOI 10.1109/CVPR.2019.01196
   Ercoli S, 2017, IEEE T MULTIMEDIA, V19, P2521, DOI 10.1109/TMM.2017.2697824
   Fu X, 2020, IEEE T MULTIMEDIA, V22, P2354, DOI 10.1109/TMM.2019.2957948
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Ge TZ, 2013, PROC CVPR IEEE, P2946, DOI 10.1109/CVPR.2013.379
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Huang Long-Kai., 2013, IJCAI, P1422
   Huang ZX, 1998, DATA MIN KNOWL DISC, V2, P283, DOI 10.1023/A:1009769707641
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Kang C, 2019, IEEE T MULTIMEDIA, V21, P1563, DOI 10.1109/TMM.2018.2883868
   Klein B., 2017, ARXIV171108589, V2, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lang K., 1995, P 12 INT C MACHINE L, P331
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leng C, 2015, PROC CVPR IEEE, P2503, DOI 10.1109/CVPR.2015.7298865
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li WJ, 2016, IJCAI, P1711
   Li XR, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1325
   Lin MB, 2019, AAAI CONF ARTIF INTE, P8722
   Lin MB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1635, DOI 10.1145/3240508.3240519
   Liu B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P755, DOI 10.1145/3240508.3240543
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Luo X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2518
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Martinez J., 2014, ARXIV14112173, P1
   Matsui Y, 2018, ITE TRANS MEDIA TECH, V6, P2, DOI 10.3169/mta.6.2
   Pang SM, 2019, IEEE T MULTIMEDIA, V21, P1513, DOI 10.1109/TMM.2018.2876833
   Le Q, 2014, PR MACH LEARN RES, V32, P1188
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JK, 2020, INT J COMPUT VISION, V128, P2243, DOI 10.1007/s11263-020-01305-2
   Song JK, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P912
   Sun CC, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P725, DOI 10.1145/3331184.3331229
   Tibshirani R, 2001, J ROY STAT SOC B, V63, P411, DOI 10.1111/1467-9868.00293
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Xie H.-X., 2020, P 28 ACM INT C MULT, P2871
   Xie HT, 2019, IEEE T MULTIMEDIA, V21, P1248, DOI 10.1109/TMM.2018.2872898
   Xu DN, 2018, IEEE T KNOWL DATA EN, V30, P2185, DOI 10.1109/TKDE.2018.2817526
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P284, DOI 10.1109/TITS.2017.2749965
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yu T, 2018, LECT NOTES COMPUT SC, V11205, P191, DOI 10.1007/978-3-030-01246-5_12
   Zhang HW, 2014, IEEE T IMAGE PROCESS, V23, P2996, DOI 10.1109/TIP.2014.2325784
   Zhang YD, 2014, IEEE T MULTIMEDIA, V16, P1127, DOI 10.1109/TMM.2014.2306392
   Zhang ZZ, 2019, IEEE T MULTIMEDIA, V21, P2878, DOI 10.1109/TMM.2019.2915036
   Zheng YT, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168770
   Zhou BL, 2014, ADV NEUR IN, V27
NR 67
TC 10
Z9 10
U1 3
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 981
EP 994
DI 10.1109/TMM.2021.3062480
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100036
DA 2024-07-18
ER

PT J
AU Ma, RJ
   Li, SY
   Zhang, B
   Li, ZM
AF Ma, Ruijun
   Li, Shuyi
   Zhang, Bob
   Li, Zhengming
TI Towards Fast and Robust Real Image Denoising With Attentive Neural
   Network and PID Controller
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise reduction; Noise measurement; Image denoising; Feature extraction;
   Adaptation models; Process control; Deep learning; Image denoising;
   Real-world Noisy Image; LSTM; PID Controller
AB With the development of deep learning technologies, recent research on real-world noisy image denoising has achieved a considerable improvement in performance. However, a common limitation for existing approaches is the imbalanced trade-off between denoising accuracy and efficiency. To address this problem, we propose a robust and efficient denoiser, called a hierarchical-based PID-attention denoising network (HPDNet), to flexibly deal with the sophisticated noise. The core of our algorithm is the PID-attentive recurrent network (PAR-Net) whose framework mainly consists of the LSTM network and PID controller. PAR-Net inherits the advantages of both the attentive recurrent network and control action, which can encourage more discriminatory feature representations. This learning procedure is implemented within a feedback control system, allowing a faster and more robust means to enhance feature discriminability. Furthermore, by decomposing the noisy image and stacking the PAR-Nets, our PAR-Net can work on a progressively hierarchical framework, and hence obtain multi-scale features and manageable successive refinements. On several widely used datasets, the proposed HPDNet demonstrates high efficiency, while delivering a better perceptually appealing image quality over state-of-the-art image denoising methods.
C1 [Ma, Ruijun; Li, Shuyi; Zhang, Bob] Univ Macau, Dept Comp & Informat Sci, PAMI Res Grp, Macau, Peoples R China.
   [Ma, Ruijun; Li, Zhengming] Guangdong Polytech Normal Univ, Guangdong Ind Training Ctr, Guangzhou 510665, Peoples R China.
C3 University of Macau; Guangdong Polytechnic Normal University
RP Zhang, B (corresponding author), Univ Macau, Dept Comp & Informat Sci, PAMI Res Grp, Macau, Peoples R China.
EM yb97442@um.edu.mo; yb97443@um.edu.mo; bobzhang@um.edu.mo;
   gslzm@gpnu.edu.cn
RI Zhang, Bob/ABD-5926-2021; Zhang, Bob/HIR-3656-2022
OI Zhang, Bob/0000-0003-2497-9519; Zhang, Bob/0000-0001-6512-0474; Li,
   Shuyi/0000-0001-6264-9006
FU University of Macau [MYRG2019-00006-FST]; National Natural Science
   Foundation of China [61602540]; Youth Innovation Project of the
   Department of Education of Guangdong Province [2020KQNCX040]
FX This work was supported in part by the University of Macau under Grant
   MYRG2019-00006-FST, in part by the National Natural Science Foundation
   of China under Grant 61602540, and in part by the Youth Innovation
   Project of the Department of Education of Guangdong Province under Grant
   2020KQNCX040.
CR Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Agustsson Eirikur, 2017, IEEE CVF C COMP VIS, P126, DOI DOI 10.1109/CVPRW.2017.150
   An WP, 2018, PROC CVPR IEEE, P8522, DOI 10.1109/CVPR.2018.00889
   Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325
   Avola D, 2020, IEEE T MULTIMEDIA, V22, P2481, DOI 10.1109/TMM.2019.2960588
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Chen JW, 2018, PROC CVPR IEEE, P3155, DOI 10.1109/CVPR.2018.00333
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Cho SI, 2018, IEEE T MULTIMEDIA, V20, P1738, DOI 10.1109/TMM.2017.2781371
   Dabov K, 2007, IEEE IMAGE PROC, P313, DOI 10.1109/icip.2007.4378954
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dave A, 2017, IEEE IMAGE PROC, P1702, DOI 10.1109/ICIP.2017.8296572
   Dwarampudi M., 2019, ARXIV190307288V1
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Haque K.N., 2018, ARXIV180105141
   HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126
   Jakhetiya V, 2017, IEEE T MULTIMEDIA, V19, P93, DOI 10.1109/TMM.2016.2609419
   Kim SJ, 2012, IEEE T PATTERN ANAL, V34, P2289, DOI 10.1109/TPAMI.2012.58
   Kim Y, 2020, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR42600.2020.00354
   Kingma D. P., 2014, arXiv
   Lebrun M., SIAM J IMAG SCI, V6, P1665
   Lebrun M, 2015, IEEE T IMAGE PROCESS, V24, P3149, DOI 10.1109/TIP.2015.2439041
   Ma RJ, 2022, IEEE T NEUR NET LEAR, V33, P3010, DOI 10.1109/TNNLS.2020.3048031
   Ma RJ, 2020, IEEE T IMAGE PROCESS, V29, P3927, DOI 10.1109/TIP.2020.2965294
   Martin D., 2001, P ICCV, P416, DOI [DOI 10.1109/ICCV.2001.937655, 10.1109/ICCV.2001.937655]
   Mei Y., ARXIV200413824
   Nam S, 2016, PROC CVPR IEEE, P1683, DOI 10.1109/CVPR.2016.186
   Odena A., 2018, ABS180508318 CORR
   Plötz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294
   Rajeev R., J MED SYST, P243
   Roth S, 2005, PROC CVPR IEEE, P860
   Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tian CW, 2020, NEURAL NETWORKS, V121, P461, DOI 10.1016/j.neunet.2019.08.022
   Torres WL, 2017, IEEE LAT AM T, V15, P1250, DOI 10.1109/TLA.2017.7959343
   Tsin Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P480, DOI 10.1109/ICCV.2001.937555
   Wang HY, 2017, IEEE T MULTIMEDIA, V19, P969, DOI 10.1109/TMM.2016.2638624
   Xie J, 2015, IEEE T MULTIMEDIA, V17, P1525, DOI 10.1109/TMM.2015.2457678
   Xu JL, 2018, CAMB CHINA LIBR, P1
   Xu J, 2018, LECT NOTES COMPUT SC, V11212, P21, DOI 10.1007/978-3-030-01237-3_2
   Xu J, 2018, IEEE T IMAGE PROCESS, V27, P2996, DOI 10.1109/TIP.2018.2811546
   Xu J, 2017, IEEE I CONF COMP VIS, P1105, DOI 10.1109/ICCV.2017.125
   Yakkundi Shashidhar V., 2020, 2020 4th International Conference on Trends in Electronics and Informatics (ICOEI). Proceedings, P1011, DOI 10.1109/ICOEI48184.2020.9142982
   Yu K, 2018, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2018.00259
   Yu Ke, 2019, ARXIV190410343
   Yue ZS, 2019, ADV NEUR IN, V32
   Zamir S.W., 2020, P EUR C COMP VIS
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   [张黎明 ZHANG Liming], 2011, [生态环境学报, Ecology and Environmental Sciences], V20, P1
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
   Zhao Y., 2019, 2019 IEEE Visual Communications and Image Processing (VCIP), P1, DOI [DOI 10.1109/VCIP47243.2019.8965754, 10.1109/vcip47243.2019.8965754]
   Zhu FY, 2017, IEEE T PATTERN ANAL, V39, P1518, DOI 10.1109/TPAMI.2016.2604816
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 63
TC 25
Z9 25
U1 4
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2366
EP 2377
DI 10.1109/TMM.2021.3079697
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600011
DA 2024-07-18
ER

PT J
AU Wu, ZQ
   Zhuang, CQ
   Shi, J
   Guo, JW
   Xiao, J
   Zhang, XP
   Yan, DM
AF Wu, Zhongqi
   Zhuang, Chuanqing
   Shi, Jian
   Guo, Jianwei
   Xiao, Jun
   Zhang, Xiaopeng
   Yan, Dong-Ming
TI Single-Image Specular Highlight Removal via Real-World Dataset
   Construction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lighting; Image color analysis; Training; Task analysis; Light sources;
   Color; Estimation; Specular highlight removal; PSD-Dataset; deep
   learning
ID REFLECTION COMPONENTS; COLOR CONSTANCY; SEPARATION; DIFFUSE;
   DECOMPOSITION; RECOVERY
AB Specular reflections pose great challenges on various multimedia and computer vision tasks, e.g., image segmentation, detection and matching. In this paper, we build a large-scale Paired Specular-Diffuse (PSD) image dataset, where the images are carefully captured by using real-world objects and the ground-truth specular-free diffuse images are provided. To the best of our knowledge, this is the first real-world benchmark dataset for specular highlight removal task, which is useful for evaluating and encouraging new deep learning-based approaches. Given this dataset, we present a novel Generative Adversarial Network (GAN) for specular highlight removal from a single image by introducing the detection of specular reflection information as a guidance. Our network also makes full use of the attention mechanism and is able to directly model the mapping relation between the diffuse area and the specular highlight area without any explicit estimation of the illumination. Experimental results demonstrate that the proposed network is more effective to remove specular reflection components with the guidance of specular highlight detection than recent state-of-the-art methods.
C1 [Wu, Zhongqi; Shi, Jian; Guo, Jianwei; Zhang, Xiaopeng; Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Wu, Zhongqi; Zhuang, Chuanqing; Shi, Jian; Guo, Jianwei; Xiao, Jun; Zhang, Xiaopeng; Yan, Dong-Ming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Guo, JW (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.; Guo, JW; Xiao, J (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM wuzhongqi2019@ia.ac.cn; zhuangchuanqing19@mails.ucas.ac.cn;
   jian.shi@ia.ac.cn; jianwei.guo@nlpr.ia.ac.cn; xiaojun@ucas.ac.cn;
   xiaopeng.zhang@ia.ac.cn; yandongming@gmail.com
RI Wu, zhongqi/GQP-4928-2022
OI Xiao, Jun/0000-0002-1799-3948; shi, jian/0000-0003-1679-4063; Zhuang,
   Chuanqing/0000-0002-2102-5499; Guo, Jianwei/0000-0002-3376-1725; Yan,
   Dong-Ming/0000-0003-2209-2404
FU National Natural Science Foundation of China [62172416, 62172415,
   U2003109, 62002358, 61972459]; National Key R&D Program of China
   [2019YFB2204104]; Strategic Priority Research Program of theChinese
   Academy of Sciences [XDA23090304]; Key Research Program of Frontier
   Sciences CAS [QYZDY-SSW-SYS004]; Youth Innovation Promotion Association
   of the Chinese Academy of Sciences [Y201935]; Fundamental Research Funds
   for the Central Universities; Alibaba Group through Alibaba Innovative
   Research Program
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172416, 62172415, U2003109, 62002358,
   and 61972459, in part by the National Key R&D Program of China under
   Grant 2019YFB2204104, in part by the Strategic Priority Research Program
   of theChinese Academy of Sciences under Grant XDA23090304, in part by
   the Key Research Program of Frontier Sciences CAS under Grant
   QYZDY-SSW-SYS004, in part by the Youth Innovation Promotion Association
   of the Chinese Academy of Sciences under Grant Y201935, in part by the
   Fundamental Research Funds for the Central Universities, and in part by
   Alibaba Group through Alibaba Innovative Research Program.
CR Akashi Y, 2015, LECT NOTES COMPUT SC, V9007, P611, DOI 10.1007/978-3-319-16814-2_40
   Alperovich A, 2017, LECT NOTES COMPUT SC, V10113, P66, DOI 10.1007/978-3-319-54187-7_5
   [Anonymous], 2015, C TRACK P 3 INT C LE
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Artusi A, 2011, COMPUT GRAPH FORUM, V30, P2208, DOI 10.1111/j.1467-8659.2011.01971.x
   Atkinson GA, 2006, IEEE T IMAGE PROCESS, V15, P1653, DOI 10.1109/TIP.2006.871114
   Bajcsy R, 1996, INT J COMPUT VISION, V17, P241, DOI 10.1007/BF00128233
   Beigpour S, 2015, IEEE I CONF COMP VIS, P172, DOI 10.1109/ICCV.2015.28
   Born M., 2013, PRINCIPLES OPTICS EL, DOI 10.1017/CBO9781139644181
   Brainard DH, 1997, J OPT SOC AM A, V14, P1393, DOI 10.1364/JOSAA.14.001393
   Chang Angel X., 2015, arXiv
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   Du B, 2017, IEEE T MULTIMEDIA, V19, P67, DOI 10.1109/TMM.2016.2608780
   Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351
   Finayson GD, 2001, IEEE T PATTERN ANAL, V23, P1209, DOI 10.1109/34.969113
   FORSYTH DA, 1990, INT J COMPUT VISION, V5, P5, DOI 10.1007/BF00056770
   Fu G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1873, DOI 10.1145/3394171.3413586
   Fu G, 2019, COMPUT GRAPH FORUM, V38, P253, DOI 10.1111/cgf.13834
   Funke I, 2018, PROC SPIE, V10576, DOI 10.1117/12.2293755
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Gevers T, 2003, IEEE T MULTIMEDIA, V5, P237, DOI 10.1109/TMM.2003.811620
   Guo XJ, 2014, PROC CVPR IEEE, P2195, DOI 10.1109/CVPR.2014.281
   Hansen T, 2006, NAT NEUROSCI, V9, P1367, DOI 10.1038/nn1794
   Imai Y, 2011, LECT NOTES COMPUT SC, V6626, P85, DOI 10.1007/978-3-642-20404-3_7
   Jachnik J, 2012, INT SYM MIX AUGMENT, P91, DOI 10.1109/ISMAR.2012.6402544
   Jolicoeur-Martineau A., 2019, P INT C LEARN REPR
   Joze HRV, 2014, IEEE T PATTERN ANAL, V36, P860, DOI 10.1109/TPAMI.2013.169
   KLINKER GJ, 1988, INT J COMPUT VISION, V2, P7, DOI 10.1007/BF00836279
   LEE SW, 1992, IMAGE VISION COMPUT, V10, P643, DOI 10.1016/0262-8856(92)90009-R
   Lei CY, 2020, PROC CVPR IEEE, P1747, DOI 10.1109/CVPR42600.2020.00182
   Lin J, 2019, LECT NOTES COMPUT SC, V11482, P3, DOI 10.1007/978-3-030-20205-7_1
   Lin S, 2002, LECT NOTES COMPUT SC, V2352, P210
   Lin S, 2001, PROC CVPR IEEE, P341
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lyu Y., 2019, Advances in neural information processing systems, V32, P14532
   Münzer B, 2018, MULTIMED TOOLS APPL, V77, P1323, DOI 10.1007/s11042-016-4219-z
   Muhammad S, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.11.001
   Nayar SK, 1997, INT J COMPUT VISION, V21, P163, DOI 10.1023/A:1007937815113
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sapiro G, 1999, IEEE T PATTERN ANAL, V21, P1210, DOI 10.1109/34.809114
   SATO Y, 1994, J OPT SOC AM A, V11, P2990, DOI 10.1364/JOSAA.11.002990
   SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409
   Shen HL, 2008, PATTERN RECOGN, V41, P2461, DOI 10.1016/j.patcog.2008.01.026
   Shen HL, 2013, APPL OPTICS, V52, P4483, DOI 10.1364/AO.52.004483
   Shen HL, 2009, APPL OPTICS, V48, P2711, DOI 10.1364/AO.48.002711
   Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619
   Tan P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P164, DOI 10.1109/ICCV.2003.1238333
   Tan RT, 2004, J OPT SOC AM A, V21, P321, DOI 10.1364/JOSAA.21.000321
   Tan RT, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P870
   Tan RT, 2003, PROC CVPR IEEE, P673
   Tao MW, 2016, IEEE T PATTERN ANAL, V38, P1155, DOI 10.1109/TPAMI.2015.2477811
   Wan RJ, 2020, IEEE T PATTERN ANAL, V42, P2969, DOI 10.1109/TPAMI.2019.2921574
   Wan RJ, 2018, PROC CVPR IEEE, P4777, DOI 10.1109/CVPR.2018.00502
   Wan RJ, 2017, IEEE I CONF COMP VIS, P3942, DOI 10.1109/ICCV.2017.423
   Wei KX, 2019, PROC CVPR IEEE, P8170, DOI 10.1109/CVPR.2019.00837
   Wu Z., 2020, P SIGGRAPH AS POST, P1
   Xia WY, 2019, IEEE ACCESS, V7, P125976, DOI 10.1109/ACCESS.2019.2939229
   Xue ML, 2021, IEEE T MULTIMEDIA, V23, P2706, DOI 10.1109/TMM.2020.3015037
   Yamamoto T, 2017, IEEE IMAGE PROC, P4222, DOI 10.1109/ICIP.2017.8297078
   Yang JW, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P891, DOI 10.1109/ICCVW.2013.122
   Yang QX, 2015, IEEE T PATTERN ANAL, V37, P1304, DOI 10.1109/TPAMI.2014.2360402
   Yi RJ, 2020, AAAI CONF ARTIF INTE, V34, P12685
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Zhang L, 2017, IEEE T IMAGE PROCESS, V26, P4114, DOI 10.1109/TIP.2017.2712283
   Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503
   Zhu D., 2019, CVPR, P7586
NR 67
TC 17
Z9 19
U1 5
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3782
EP 3793
DI 10.1109/TMM.2021.3107688
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400009
DA 2024-07-18
ER

PT J
AU Zhang, CY
   Wang, Q
   Xie, GS
   Wu, Q
   Shen, FM
   Tang, ZN
AF Zhang, Chuanyi
   Wang, Qiong
   Xie, Guosen
   Wu, Qi
   Shen, Fumin
   Tang, Zhenmin
TI Robust Learning From Noisy Web Images Via Data Purification for
   Fine-Grained Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Training; Task analysis; Visualization; Noise
   robustness; Labeling; Annotations; Fine-grained; label noise; web images
ID CATEGORY
AB Manually labeling fine-grained datasetsis laborious and typically requires domain-specific expert knowledge. Conversely, a vast amount of web data is relatively easy to obtain with nearly no human effort. Therefore, learning from noisy web data for fine-grained tasks is attracting increasing attention in recent years. However, the presence of noise in web images is a huge obstacle for training robust fine-grained recognition models. To this end, we propose a novel approach to identify noisy images as well as specifically distinguish in- and out-of-distribution samples. It can purify the noisy web training set by discarding out-of-distribution noise and relabeling in-distribution noisy samples. Then we can train the model on the purified dataset to alleviate the harmful effects of noise and make the most of web images to achieve better performance. Extensive experiments on three commonly used fine-grained datasets demonstrate that our approach is far superior to current state-of-the-art web-supervised methods. The data and source code of this work have been made publicly available at: https://github.com/NUST-Machine-Intelligence-Laboratory/Dataset-Purification.
C1 [Zhang, Chuanyi; Wang, Qiong; Tang, Zhenmin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Xie, Guosen] Incept Inst Artificial Intelligence, Dhabi, U Arab Emirates.
   [Wu, Qi] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
   [Shen, Fumin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610056, Peoples R China.
C3 Nanjing University of Science & Technology; University of Adelaide;
   University of Electronic Science & Technology of China
RP Wang, Q (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM zhangchuanyi@njust.edu.cn; wangq@njust.edu.cn; gsxiehm@gmail.com;
   qi.wu01@adelaide.edu.au; fumin.shen@gmail.com; tzm.cs@njust.edu.cn
RI Xie, Guo-Sen/AAL-6674-2020; Wu, Qi/ABD-6304-2021; Shen,
   Fumin/R-2121-2016
OI Xie, Guo-Sen/0000-0002-5487-9845; Wu, Qi/0000-0003-3631-256X; Wang,
   Qiong/0000-0003-4193-0960
FU National Natural Science Foundation of China [62102182, 61976116];
   Natural Science Foundation of Jiangsu Province [BK20210327]; Fundamental
   Research Funds for the Central Universities [30920021135]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62102182 and 61976116, in part by the
   Natural Science Foundation of Jiangsu Province under Grant BK20210327,
   and in part by the Fundamental Research Funds for the Central
   Universities under Grant 30920021135. The Associate Editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Shin'ichi Satoh.
CR Abu-El-Haija Sami, 2016, arXiv
   [Anonymous], 2013, Tech. rep.
   [Anonymous], 2018, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00749
   Arazo E, 2019, PR MACH LEARN RES, V97
   Branson S., 2014, BIRD SPECIES CATEGOR, V1, P7
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47
   Chen G, 2015, IEEE WINT CONF APPL, P860, DOI 10.1109/WACV.2015.119
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Cui Y, 2016, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2016.130
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gavves E, 2015, INT J COMPUT VISION, V111, P191, DOI 10.1007/s11263-014-0741-5
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Gidaris S., 2018, P 6 INT C LEARNING R
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XT, 2017, PROC CVPR IEEE, P7332, DOI 10.1109/CVPR.2017.775
   Hendrycks D, 2019, ADV NEUR IN, V32
   Hendrycks D, 2018, ADV NEUR IN, V31
   Huang L., 2020, P ADV NEUR INF PROC, P1
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jiankang Deng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P741, DOI 10.1007/978-3-030-58621-8_43
   Kanan C, 2014, IEEE WINT CONF APPL, P23, DOI 10.1109/WACV.2014.6836122
   Korsch D, 2019, LECT NOTES COMPUT SC, V11824, P62, DOI 10.1007/978-3-030-33676-9_5
   Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   Li YC, 2017, IEEE I CONF COMP VIS, P1928, DOI 10.1109/ICCV.2017.211
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu HF, 2022, IEEE T MULTIMEDIA, V24, P546, DOI 10.1109/TMM.2021.3055024
   Loshchilov I, 2016, ARXIV
   Malach E, 2017, ADV NEUR IN, V30
   Niu L, 2015, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2015.7298894
   Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Song H, 2019, PR MACH LEARN RES, V97
   Sun Z., P IEEE INT C COMP VI, V2021, P10602
   Sun ZR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P92, DOI 10.1145/3394171.3413978
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wan H, 2018, IEEE T PATTERN ANAL, V40, P409, DOI 10.1109/TPAMI.2017.2672557
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wang YS, 2018, PROC CVPR IEEE, P8688, DOI 10.1109/CVPR.2018.00906
   Wei Hongxin, 2020, P IEEE C COMP VIS PA
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xie SN, 2015, PROC CVPR IEEE, P2645, DOI 10.1109/CVPR.2015.7298880
   Xu Z, 2018, IEEE T PATTERN ANAL, V40, P1100, DOI 10.1109/TPAMI.2016.2637331
   Yang S., 2012, Advances in Neural Information Processing Systems, P3122
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   Yao Y., 2020, ACM MM, P1735
   Yao Y., 2016, P 24 ACM INT C MULT, P212
   Yao YZ, 2021, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR46437.2021.00515
   Yao YZ, 2020, IEEE T NEUR NET LEAR, V31, P2348, DOI 10.1109/TNNLS.2020.2966644
   Yao YZ, 2020, IEEE T KNOWL DATA EN, V32, P1199, DOI 10.1109/TKDE.2019.2903036
   Yao YZ, 2019, IEEE T MULTIMEDIA, V21, P184, DOI 10.1109/TMM.2018.2847248
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yao Yazhou, 2021, CVPR
   Yu Xingrui, 2019, PROC INT C MACH LEAR, P7164
   Zeng HQ, 2017, PROC INT CONF RECON
   Zhang C., 2020, ACM MM, P2372
   Zhang C., P ACM INT C MULT, V2021, P4063
   Zhang CY, 2020, AAAI CONF ARTIF INTE, V34, P12781
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zhang XP, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P287, DOI 10.1145/2647868.2654937
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhu ML, 2006, IEEE T PATTERN ANAL, V28, P1274, DOI 10.1109/TPAMI.2006.172
NR 79
TC 7
Z9 7
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1198
EP 1209
DI 10.1109/TMM.2021.3134156
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800016
DA 2024-07-18
ER

PT J
AU Zhang, XQ
   Tan, Z
   Sun, HJ
   Wang, ZG
   Qin, MW
AF Zhang, Xiaoqian
   Tan, Zhen
   Sun, Huaijiang
   Wang, Zungang
   Qin, Mingwei
TI Orthogonal Low-Rank Projection Learning for Robust Image Feature
   Extraction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Robustness; Data models; Principal component
   analysis; Dimensionality reduction; Data mining; Task analysis; Feature
   extraction; WTSN; low-rank representation; projection learning
ID SCHATTEN-P-NORM; FACE RECOGNITION; EIGENFACES; GRAPH; LDA
AB Projecting the original data into a low-dimensional target space for feature extraction is a common method. Recently, presentation-based approaches have been widely concerned and many feature extraction algorithms based on this have been proposed. However, in the process of acquiring real data, the pollution of complex noise cannot always be avoided, which will greatly increase the difficulty of feature extraction and even lead to failed feature extraction results. Thus, a robust image feature extraction model based on Orthogonal Low-Rank Projection Learning (OLRPL) is proposed, in which the introduction of orthogonal matrix can encourage the preservation of the main components of the sample. Particularly, the row sparsity constraint introduced on the projection matrix can encourage the features to be more compact, discriminative and interpretable. In particular, the Weighted Truncated Schatten p-norm (WTSN) is proposed to better solve the optimization problem of low-rank constraints. At the same time, the correntropy is applied in OLRPL to suppress the complex noise in the data and thus improve the robustness of the model. Finally, we specially design a robust classification loss function so that our model can be fitted the supervised scene effectively. Experiments on five general databases have proved that OLRPL has better effectiveness and robustness than existing advanced methods.
C1 [Zhang, Xiaoqian; Tan, Zhen; Qin, Mingwei] Southwest Univ Sci & Technol, Sch Informat Engn, Mianyang 621010, Sichuan, Peoples R China.
   [Zhang, Xiaoqian; Tan, Zhen; Sun, Huaijiang; Qin, Mingwei] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 230026, Peoples R China.
   [Wang, Zungang] State Key Lab NBC Protect Civilian, Beijing 102205, Peoples R China.
C3 Southwest University of Science & Technology - China; Nanjing University
   of Science & Technology
RP Sun, HJ (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 230026, Peoples R China.; Wang, ZG (corresponding author), State Key Lab NBC Protect Civilian, Beijing 102205, Peoples R China.
EM zhxq0528@163.com; jdc70197493pozh@163.com; sunhuaijiang@njust.edu.cn;
   zhigang7991@163.com; qmw_qyq@163.com
OI Huaijiang, Sun/0000-0002-8795-6413; Zhang, Xiaoqian/0000-0002-4007-4501
FU National Natural Science Foundation of China [62102331, 62176125,
   61772272]; Sichuan Science and Technology Program [2020YJ0432,
   2019YJ0309]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62102331, 62176125, and 61772272 and in
   part by the Sichuan Science and Technology Program under Grants
   2020YJ0432 and 2019YJ0309.
CR Bao BK, 2012, IEEE T IMAGE PROCESS, V21, P3794, DOI 10.1109/TIP.2012.2192742
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Dán G, 2015, IEEE T MULTIMEDIA, V17, P591, DOI 10.1109/TMM.2015.2406574
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Du HS, 2015, NEUROCOMPUTING, V164, P220, DOI 10.1016/j.neucom.2015.02.067
   Fang XZ, 2018, IEEE T NEUR NET LEAR, V29, P5228, DOI 10.1109/TNNLS.2018.2796133
   Feng L, 2016, SIGNAL PROCESS-IMAGE, V47, P28, DOI 10.1016/j.image.2016.05.012
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Hong MY, 2016, SIAM J OPTIMIZ, V26, P337, DOI 10.1137/140990309
   Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Li J, 2016, IEEE T IMAGE PROCESS, V25, P4803, DOI 10.1109/TIP.2016.2598654
   Li S, 2015, IEEE T KNOWL DATA EN, V27, P1274, DOI 10.1109/TKDE.2014.2365793
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Liu L, 2014, J COMPUT APPL MATH, V267, P218, DOI 10.1016/j.cam.2014.02.015
   Liu WF, 2007, IEEE T SIGNAL PROCES, V55, P5286, DOI 10.1109/TSP.2007.896065
   Lu YW, 2016, IEEE T CYBERNETICS, V46, P1900, DOI 10.1109/TCYB.2015.2457611
   Martinez A., 1998, AR FACE DATABASE
   Meng M, 2020, IEEE T IMAGE PROCESS, V29, P186, DOI 10.1109/TIP.2019.2926774
   Mohan K, 2010, P AMER CONTR CONF, P2953
   Naseem I, 2010, IEEE T PATTERN ANAL, V32, P2106, DOI 10.1109/TPAMI.2010.128
   Nene S. A., 1996, COLUMBIA OBJECT IMAG
   Pang YW, 2014, IEEE T NEUR NET LEAR, V25, P2191, DOI 10.1109/TNNLS.2014.2306844
   PARK H, 1991, PARALLEL COMPUT, V17, P913, DOI 10.1016/S0167-8191(05)80075-4
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Song FX, 2007, IEEE T SYST MAN CY B, V37, P1599, DOI 10.1109/TSMCB.2007.906579
   Tang KW, 2014, IEEE T NEUR NET LEAR, V25, P2167, DOI 10.1109/TNNLS.2014.2306063
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Wong WK, 2017, IEEE T IMAGE PROCESS, V26, P2905, DOI 10.1109/TIP.2017.2691543
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xia GY, 2018, IEEE T IMAGE PROCESS, V27, P135, DOI 10.1109/TIP.2017.2738562
   Xie Y, 2016, IEEE T IMAGE PROCESS, V25, P4842, DOI 10.1109/TIP.2016.2599290
   Xue XQ, 2020, INFORM SCIENCES, V513, P190, DOI 10.1016/j.ins.2019.10.058
   Ye MX, 2020, IEEE T MULTIMEDIA, V22, P1113, DOI 10.1109/TMM.2019.2942479
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang XQ, 2021, INFORM SCIENCES, V551, P324, DOI 10.1016/j.ins.2020.10.059
   Zhang XQ, 2020, IEEE T KNOWL DATA EN, V32, P2426, DOI 10.1109/TKDE.2019.2922637
   Zhang XQ, 2019, INFORM SCIENCES, V477, P430, DOI 10.1016/j.ins.2018.10.049
   Zhang YMZ, 2013, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2013.93
   Zhuang LS, 2015, IEEE T IMAGE PROCESS, V24, P3717, DOI 10.1109/TIP.2015.2441632
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
   Zuo WM, 2006, IEEE T SYST MAN CY B, V36, P946, DOI 10.1109/TSMCB.2005.863377
NR 46
TC 5
Z9 5
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3882
EP 3895
DI 10.1109/TMM.2021.3109442
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400016
DA 2024-07-18
ER

PT J
AU Bao, Q
   Liu, W
   Cheng, YH
   Zhou, BY
   Mei, T
AF Bao, Qian
   Liu, Wu
   Cheng, Yuhao
   Zhou, Boyan
   Mei, Tao
TI Pose-Guided Tracking-by-Detection: Robust Multi-Person Pose Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Task analysis; Pose estimation; Robustness; Detectors; Object
   tracking; Pose tracking; pose estimation; graph convo-lutional networks;
   human association
ID NETWORK
AB Multi-person pose tracking task aims to estimate and track person keypoints in videos. Most of the previous methods follow the general track-by-detection strategy that ignores the consistent pose information during the whole framework. Thus, they often suffer from missing detections or inaccurate human association in challenging scenes with motion blur or person occlusion. To handle those problems, we propose a pose-guided tracking-by-detection framework that fuses pose information into both video human detection and human association procedures. In the video human detection stage, we adopt the pose-guided person location prediction exploiting the temporal information to make up missing detections. Technically, pose heatmaps are utilized to cope with the person-specific intra-class distractors. Furthermore, in the human association stage, we propose an appearance discriminative model based on the hierarchical pose-guided graph convolutional networks (PoseGCN). The PoseGCN-based model exploits human structural relations to boost person representation. Extensive experiments show the superiority of our method on the challenging pose tracking benchmark. Our proposed method ranks first on the PoseTrack leaderboard. (1) (1) http://posetrack.net/leaderboard.php till the submission date (22-Aug-2019) of this paper. Our code has been publicly available at https://github.com/human-centric982/PGPT.
C1 [Bao, Qian; Liu, Wu; Cheng, Yuhao; Zhou, Boyan; Mei, Tao] JD AI Res, Beijing 100101, Peoples R China.
RP Liu, W (corresponding author), JD AI Res, Beijing 100101, Peoples R China.
EM baoqian@jd.com; liuwu@live.cn; chengyuhao@jd.com; zhouboyan94@gmail.cn;
   tmei@live.com
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307
CR Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   [Anonymous], 2016, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-46484-8_29
   [Anonymous], 2017, AUTOMATIC DIFFERENTI
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.90
   [Anonymous], 2018, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00742
   [Anonymous], 2017, P 31 INT C NEUR INF
   [Anonymous], 2014, P ADV NEUR INF PROC
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chu Q, 2017, IEEE I CONF COMP VIS, P4846, DOI 10.1109/ICCV.2017.518
   Doering A., 2018, P BRIT MACH VIS C, P1
   Fabbri M, 2018, LECT NOTES COMPUT SC, V11208, P450, DOI 10.1007/978-3-030-01225-0_27
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Fu ZY, 2019, IEEE T MULTIMEDIA, V21, P2277, DOI 10.1109/TMM.2019.2902480
   Gao MF, 2017, 2017 13TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (ICNC-FSKD), P1077, DOI 10.1109/FSKD.2017.8392913
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong XW, 2019, IEEE ACCESS, V7, P131374, DOI 10.1109/ACCESS.2019.2935116
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Insafutdinov E, 2017, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2017.142
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Iqbal U, 2017, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2017.495
   Jin Sheng, 2017, ICCV PoseTrack Workshop
   Kipf T. N., 2016, ARXIV161107308, V1050, P21
   Kuanar S, 2019, IEEE IMAGE PROC, P1351, DOI [10.1109/ICIP.2019.8803037, 10.1109/icip.2019.8803037]
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Li JN, 2022, IEEE T PATTERN ANAL, V44, P622, DOI 10.1109/TPAMI.2019.2929036
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu K, 2018, AAAI CONF ARTIF INTE, P7138
   Liu W, 2018, NEUROINFORMATICS, V16, P457, DOI 10.1007/s12021-018-9362-4
   Liu W, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1618, DOI 10.1145/3123266.3123422
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Marcheggiani Diego, 2017, EMNLP, DOI DOI 10.18653/V1/D17-1159
   Milan A, 2015, PROC CVPR IEEE, P5397, DOI 10.1109/CVPR.2015.7299178
   Ning G., 2019, ARXIV190502822
   Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Park B, 2019, IEEE INT CONF COMP V, P3494, DOI 10.1109/ICCVW.2019.00433
   Park G, 2019, 2019 7TH INTERNATIONAL CONFERENCE ON ROBOT INTELLIGENCE TECHNOLOGY AND APPLICATIONS (RITA), P1, DOI 10.1109/RITAPP.2019.8932779
   Park S, 2018, IEEE T PATTERN ANAL, V40, P1555, DOI 10.1109/TPAMI.2017.2731842
   Raaj Y, 2019, PROC CVPR IEEE, P4615, DOI 10.1109/CVPR.2019.00475
   Revaud J, 2016, INT J COMPUT VISION, V120, P300, DOI 10.1007/s11263-016-0908-3
   Ruan WJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P284, DOI 10.1145/3343031.3350984
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sarfraz M. S., 2017, ARXIV171110378, P420
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xiu Y., 2018, P BRIT MACH VIS C, P53
   Zaitseva N. V, 2017, Meditsina Truda i Promyshlennaya Ekologiya, P1
   Zhang YH, 2018, LECT NOTES COMPUT SC, V11213, P355, DOI 10.1007/978-3-030-01240-3_22
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zhou QQ, 2019, IEEE T MULTIMEDIA, V21, P1183, DOI 10.1109/TMM.2018.2875360
   Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 64
TC 40
Z9 42
U1 4
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 161
EP 175
DI 10.1109/TMM.2020.2980194
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600013
DA 2024-07-18
ER

PT J
AU Chao, FY
   Zhang, L
   Hamidouche, W
   Déforges, O
AF Chao, Fang-Yi
   Zhang, Lu
   Hamidouche, Wassim
   Deforges, Olivier
TI A Multi-FoV Viewport-Based Visual Saliency Model Using Adaptive
   Weighting Losses for 360° Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human eye fixation; saliency; omnidirectional image; convolutional
   neural network; deep learning
ID PREDICTION
AB 360 degrees media allows observers to explore the scene in all directions. The consequence is that the human visual attention is guided by not only the perceived area in the viewport but also the overall content in 360 degrees. In this paper, we propose a method to estimate the 360 degrees saliency map which extracts salient features from the entire 360 degrees image in each viewport in three different Field of Views (FoVs). Our model is first pretrained with a large-scale 2D image dataset to enable the interpretation of semantic contents, then fine-tuned with a relative small 360 degrees image dataset. Anovel weighting loss function attached with stretch weightedmaps is introduced to adaptively weight the losses of three evaluation metrics and attenuate the impact of stretched regions in equirectangular projection during training process. Experimental results demonstrate that our model achieves better performance with the integration of three FoVs and its diverse viewport images. Results also show that the adaptive weighting losses and stretch weighted maps effectively enhance the evaluation scores compared to the fixed weighting losses solutions. Comparing to other state of the art models, our method surpasses them on three different datasets and ranks the top using 5 performance evaluation metrics on the Salient360! benchmark set. The code is available at https: //github.com/FannyChao/MV-SalGAN360.
C1 [Chao, Fang-Yi; Zhang, Lu; Hamidouche, Wassim; Deforges, Olivier] Natl Inst Appl Sci, Inst Elect & Telecommun Rennes IETR, UMR CNRS 6164, F-35700 Rennes, France.
C3 Centre National de la Recherche Scientifique (CNRS); CNRS - Institute
   for Engineering & Systems Sciences (INSIS); Universite de Rennes
RP Zhang, L (corresponding author), Natl Inst Appl Sci, Inst Elect & Telecommun Rennes IETR, UMR CNRS 6164, F-35700 Rennes, France.
EM fang-yi.chao@insa-rennes.fr; lu.ge@insa-rennes.fr;
   wassim.hamidouche@insa-rennes.fr; olivier.deforges@insa-rennes.fr
OI zhang, lu/0000-0002-8859-5453
CR Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   [Anonymous], 2018, P 2018 10 INT C QUAL
   [Anonymous], 2018, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-030-01234-2_30
   [Anonymous], IEEE T PATTERN ANAL
   Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Battisti F, 2018, SIGNAL PROCESS-IMAGE, V69, P53, DOI 10.1016/j.image.2018.03.008
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bylinskii Z, 2019, Mit saliency benchmark
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Carlson C., How I Made Wine Glasses from Sunflowers
   Chao FY, 2018, IEEE INT CONF MULTI
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   De Abreu A., 2017, PROC IEEE 9 INT C QU, P1, DOI 10.1109/QoMEX.2017.7965634
   Djemai I., 2020, PROC IEEE INT S CIRC
   Eder Marc, 2019, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, P1
   Fan CL, 2020, IEEE T MULTIMEDIA, V22, P744, DOI 10.1109/TMM.2019.2931807
   Fang YM, 2018, SIGNAL PROCESS-IMAGE, V69, P1, DOI 10.1016/j.image.2018.07.009
   Fu CW, 2009, IEEE T MULTIMEDIA, V11, P634, DOI 10.1109/TMM.2009.2017626
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Gutiérrez J, 2018, SIGNAL PROCESS-IMAGE, V69, P35, DOI 10.1016/j.image.2018.05.003
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Jia S, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103887
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Lebreton P, 2018, SIGNAL PROCESS-IMAGE, V69, P69, DOI 10.1016/j.image.2018.03.006
   Li C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P932, DOI 10.1145/3240508.3240581
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Li N, 2018, IEEE T MULTIMEDIA, V20, P1365, DOI 10.1109/TMM.2017.2771566
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Ling J, 2018, SIGNAL PROCESS-IMAGE, V69, P60, DOI 10.1016/j.image.2018.03.007
   Ling SY, 2018, IEEE INT CON MULTI
   Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047
   Liu YW, 2019, IEEE T MULTIMEDIA, V21, P1302, DOI 10.1109/TMM.2018.2876044
   Maugey T., 2017, 2017 IEEE 19th International Workshop on Multimedia Signal Processing, P1
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Ozcinar C, 2018, INT WORK QUAL MULTIM, P1
   Ozcinar C, 2019, IEEE J EM SEL TOP C, V9, P217, DOI 10.1109/JETCAS.2019.2895096
   Pan J., 2017, PROC IEEE CVPR SUNW
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Rai Y., 2017, 2017 9 INT C QUAL MU, P1, DOI 10.1109/QoMEX.2017.7965659
   Rai Y, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P205, DOI 10.1145/3083187.3083218
   Shum HY, 2005, IEEE T MULTIMEDIA, V7, P85, DOI 10.1109/TMM.2004.840591
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Smolic A., 2002, ISOIEC JTC1SC29WG11
   Startsev M, 2018, SIGNAL PROCESS-IMAGE, V69, P43, DOI 10.1016/j.image.2018.03.013
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Su YC, 2018, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2018.00816
   Su YC, 2017, LECT NOTES COMPUT SC, V10114, P154, DOI 10.1007/978-3-319-54190-7_10
   Tang MH, 2019, IEEE T MULTIMEDIA, V21, P957, DOI 10.1109/TMM.2018.2867266
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Tseng PH, 2009, J VISION, V9, DOI 10.1167/9.7.4
   University of Nantes, 2017, PROC IEEE INT C MULT, P35
   Upenik E, 2016, PICT COD SYMP
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Wang X, 2019, SIGNAL PROCESS-IMAGE, V75, P158, DOI 10.1016/j.image.2019.03.012
   Xu M, 2019, IEEE T CIRC SYST VID, V29, P3516, DOI [10.1109/TCSVT.2018.2886277, 10.1080/17445302.2018.1558727]
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang K, 2019, IEEE T CIRC SYST VID, V29, P3544, DOI 10.1109/TCSVT.2018.2883305
   Zhu Y., SIGNAL PROCESS-IMAGE, V69, P15
NR 66
TC 16
Z9 18
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1811
EP 1826
DI 10.1109/TMM.2020.3003642
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, H
   Lu, M
   Ma, Z
   Zhang, X
   Xu, YL
   Shen, Q
   Zhang, WJ
AF Chen, Hao
   Lu, Ming
   Ma, Zhan
   Zhang, Xu
   Xu, Yiling
   Shen, Qiu
   Zhang, Wenjun
TI Learned Resolution Scaling Powered Gaming-as-a-Service at Scale
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaming-as-a-Service; compressed video strea-ming; learned resolution
   scaling
ID VIDEO; SUPERRESOLUTION
AB nsive real-life exper Built on the explosive advancement of cloud and telecommunication technologies, Gaming-as-a-Service (GaaS) or cloud gaming system is expected to revolutionize the traditional multi-billion video game market in the near future. This wave is analogous to the rise of live-video-streaming-based-Netflix to replace conventional DVD rental business for movies and TVs. In practice, a successful GaaS platform need to operate in a transparent mode without requiring substantial efforts from both content providers and end users, and offer the pristine quality of experience (QoE) at an affordable cost. Our analysis suggests that GaaS provisioning cost can be reduced significantly by enforcing the game video rendering and streaming at a lower resolution (so as to increase the user concurrency in the cloud and reduce the streaming bandwidth over the network). However, streaming video at a lower resolution may deteriorate the QoE. To maintain the client QoE at the level using the default-native resolution for streaming or even enhance it, we introduce the learned resolution scaling (LRS), which leverages the computational capabilities at clients/edges to restore/improve the reconstructed image/video quality via stacked deep neural networks (DNN). We integrate this LRS into a commercialized GaaS platform - AnyGame, to study its efficiency and complexity quantitatively. Exteiments have shown that LRS-powered AnyGame offers the state-of-the-art performance, and the lower operational cost, paving the road for a potential success of GaaS over the Internet. Additionally, we dive into proposed LRS via ablation studies to further demonstrate its consistent performance, including the discussions on trade-off between efficiency and complexity, alternative training sets, etc.
C1 [Chen, Hao] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
   [Xu, Yiling] Shanghai Jiao Tong Univ, Sch Elect Informat & Elecron Engn, Shanghai 200240, Peoples R China.
   [Zhang, Wenjun] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Lu, Ming] Nanjing Univ, Vis Lab, Nanjing 210093, Jiangsu, Peoples R China.
   [Ma, Zhan; Shen, Qiu] Nanjing Univ, Fac Elect Sci & Engn Sch, Nanjing 210093, Peoples R China.
   [Zhang, Xu] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210093, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai
   Jiao Tong University; Nanjing University; Nanjing University; Nanjing
   University
RP Xu, YL (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elecron Engn, Shanghai 200240, Peoples R China.; Ma, Z (corresponding author), Nanjing Univ, Fac Elect Sci & Engn Sch, Nanjing 210093, Peoples R China.
EM chenhao1210@sjtu.edu.cn; luming@smail.nju.edu.cn; mazhan@nju.edu.cn;
   xzhang17@nju.edu.cn; yl.xu@sjtu.edu.cn; shenqiu@nju.edu.cn;
   zhangwenjun@sjtu.edu.cn
RI Chen, Hao/AGI-0052-2022; Sui, Yanwei/AAH-9928-2021; Ma,
   Zhan/HKW-2859-2023; Zhang, Wenjun/GNH-2095-2022; Lu, Ming/IWM-1207-2023
OI Chen, Hao/0000-0002-1179-8199; Ma, Zhan/0000-0003-3686-4057; Zhang,
   Wenjun/0000-0002-5282-3725; Lu, Ming/0000-0002-5044-8802; Zhang,
   Xu/0000-0002-1882-736X
FU National Natural Science Foundation of China [61971282, 61902178,
   U1936202]; National Key Research and Development Project of China
   Science and Technology Exchange Center [2018YFE0206700]; Scientific
   Research Plan of the Science and Technology Commission of Shanghai
   Municipality [18511105402]; Shanghai Key Laboratory of Digital Media
   Processing and Transmission [STCSM18DZ2270700]; Natural Science
   Foundation of Jiangsu [BK20190295]; Leading Technology of Jiangsu Basic
   Research Plan [BK20192003]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61971282, 61902178, and U1936202, in
   part by the National Key Research and Development Project of China
   Science and Technology Exchange Center (2018YFE0206700), in part by
   Scientific Research Plan of the Science and Technology Commission of
   Shanghai Municipality under Grant 18511105402, in part by the Shanghai
   Key Laboratory of Digital Media Processing and Transmission under Grant
   STCSM18DZ2270700, in part by the Natural Science Foundation of Jiangsu
   under Grant BK20190295, and in part by the Leading Technology of Jiangsu
   Basic Research Plan under Grant BK20192003. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sanjeev Mehrotra.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   [Anonymous], 2019, LATENCY ANAL GOOGLE
   Anwar S., 2019, ABS190407523 ARXIV
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bjontegaard G., 2001, ITUTVCEGM33, P1
   Bokani A, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P344, DOI 10.1145/2910017.2910618
   Bruckstein AM, 2003, IEEE T IMAGE PROCESS, V12, P1132, DOI 10.1109/TIP.2003.816023
   Cai W, 2016, IEEE ACCESS, V4, P7605, DOI 10.1109/ACCESS.2016.2590500
   Chen KT, 2014, IEEE T MULTIMEDIA, V16, P480, DOI 10.1109/TMM.2013.2291532
   Cheng M, 2017, DES AUT CON, DOI 10.1145/3061639.3062326
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Dai Y, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.10.007
   Han B, 2019, IEEE COMMUN MAG, V57, P112, DOI 10.1109/MCOM.001.1800876
   Han  S., 2015, ARXIV151000149
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Huang Chun-Ying, 2013, P 4 ACM MULT SYST C, P36, DOI DOI 10.1145/2483977.2483981
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   Kingma D. P., 2014, arXiv
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Liao XF, 2016, IEEE ACM T NETWORK, V24, P2128, DOI 10.1109/TNET.2015.2450254
   Lu X., 2018, ARXIV180110342
   Ma Z, 2017, IEEE T MULTIMEDIA, V19, P2322, DOI 10.1109/TMM.2017.2737944
   Ma Z, 2013, IEEE INT CONF MULTI
   Ma Z, 2012, IEEE T CIRC SYST VID, V22, P671, DOI 10.1109/TCSVT.2011.2177143
   Newzoo, 2017 GLOB GAM MARK R
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Shea R, 2015, IEEE T CIRC SYST VID, V25, P2026, DOI 10.1109/TCSVT.2015.2450172
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Tian Yapeng, 2020, CVPR
   Wang SX, 2013, IEEE T MULTIMEDIA, V15, P870, DOI 10.1109/TMM.2013.2240674
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xie Junyuan, 2012, ADV NEURAL INFORM PR, P341, DOI [DOI 10.5555/2999134.2999173, DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605]
   Xu YL, 2018, IEEE NETWORK, V32, P42, DOI 10.1109/MNET.2018.1700153
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Zhang Ya-Qin, 2002, Video processing and communications, V1
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 45
TC 9
Z9 9
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 584
EP 596
DI 10.1109/TMM.2020.2985538
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200006
DA 2024-07-18
ER

PT J
AU Gao, JY
   Yang, XS
   Zhang, YY
   Xu, CS
AF Gao, Junyu
   Yang, Xiaoshan
   Zhang, Yingying
   Xu, Changsheng
TI Unsupervised Video Summarization via Relation-Aware Assignment Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Training; Optimization; Semantics; Recurrent neural
   networks; Task analysis; Graph neural network; unsupervised learning;
   video summarization
ID ACTION RECOGNITION; DEEP
AB We address the problem of unsupervised video summarization that automatically selects key video clips. Most state-of-the-art approaches suffer from two issues: (1) they model video clips without explicitly exploiting their relations, and (2) they learn soft importance scores over all the video clips to generate the summary representation. However, a meaningful video summary should be inferred by taking the relation-aware context of the original video into consideration, and directly selecting a subset of clips with a hard assignment. In this paper, we propose to exploit clip-clip relations to learn relation-aware hard assignments for selecting key clips in an unsupervised manner. First, we consider the clips as graph nodes to construct an assignment-learning graph. Then, we utilize the magnitude of the node features to generate hard assignments as the summary selection. Finally, we optimize the whole framework via a proposed multi-task loss including a reconstruction constraint, and a contrastive constraint. Extensive experimental results on three popular benchmarks demonstrate the favourable performance of our approach.
C1 [Gao, Junyu; Yang, Xiaoshan; Zhang, Yingying; Xu, Changsheng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Gao, Junyu; Yang, Xiaoshan; Zhang, Yingying; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Gao, Junyu; Yang, Xiaoshan; Zhang, Yingying; Xu, Changsheng] PengCheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM gaojunyu2015@ia.ac.cn; xiaoshan.yang@nlpr.ia.ac.cn;
   zhangyingying2017@ia.ac.cn; csxu@nlpr.ia.ac.cn
RI Gao, Junyu/HDO-5516-2022; Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023
OI xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2018AAA0102200];
   National Natural Science Foundation of China [61720106006, 61721004,
   61832002, 61702511, 61751211, 61532009, U1836220, U1705262, 61872424,
   61936005]; Key Research Program of Frontier Sciences of CAS
   [QYZDJSSWJSC039]; Research Program of National Laboratory of Pattern
   Recognition [Z-2018007]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2018AAA0102200, in part by the National
   Natural Science Foundation of China under Grants 61720106006, 61721004,
   61832002, 61702511, 61751211, 61532009, U1836220, U1705262, 61872424,
   and 61936005, in part by the Key Research Program of Frontier Sciences
   of CAS under Grant QYZDJSSWJSC039, and in part by the Research Program
   of National Laboratory of Pattern Recognition under Grant Z-2018007. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Mei-Ling Shyu.
CR [Anonymous], 2018, PROC EAR C COMPUT VI
   Apostolidis E., 2019, P 1 INT WORKSHOP SMA, P17
   Battaglia, 2018, ARXIV180601261
   Cai SJ, 2018, LECT NOTES COMPUT SC, V11218, P193, DOI 10.1007/978-3-030-01264-9_12
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen F, 2014, IEEE T MULTIMEDIA, V16, P455, DOI 10.1109/TMM.2013.2291967
   Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981
   Defferrard M, 2016, ADV NEUR IN, V29
   Gao J, 2020, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Gao JY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P690, DOI 10.1145/3240508.3240566
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Gao JY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P127, DOI 10.1145/3123266.3123433
   Gao JY, 2018, IEEE T IMAGE PROCESS, V27, P3074, DOI 10.1109/TIP.2018.2813166
   Gao JY, 2017, IEEE T IMAGE PROCESS, V26, P1845, DOI 10.1109/TIP.2017.2656628
   Gong BQ, 2014, ADV NEUR IN, V27
   Gygli M, 2016, PROC CVPR IEEE, P1001, DOI 10.1109/CVPR.2016.114
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Hu J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1157, DOI 10.1145/3343031.3350966
   Jiao YF, 2018, IEEE T MULTIMEDIA, V20, P2693, DOI 10.1109/TMM.2018.2815998
   Jung Y, 2019, AAAI CONF ARTIF INTE, P8537
   Kanehira A, 2018, PROC CVPR IEEE, P7435, DOI 10.1109/CVPR.2018.00776
   Kipf T. N., 2017, 8 INT C LEARN REPR, P1
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, IEEE INT CONF MULTI
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Liu YJ, 2016, IEEE T MULTIMEDIA, V18, P1269, DOI 10.1109/TMM.2016.2557061
   Maddison Chris J, 2016, ARXIV161100712
   Mahasseni B, 2017, PROC CVPR IEEE, P2982, DOI 10.1109/CVPR.2017.318
   Malinowski M, 2018, LECT NOTES COMPUT SC, V11210, P3, DOI 10.1007/978-3-030-01231-1_1
   Mayu O., 2019, P CVPR
   Mnih V, 2014, ADV NEUR IN, V27
   Olah C., 2018, Distill, V3, P10
   Panda R, 2017, IEEE I CONF COMP VIS, P3677, DOI 10.1109/ICCV.2017.395
   Panda R, 2017, PROC CVPR IEEE, P4274, DOI 10.1109/CVPR.2017.455
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Rochan M, 2019, PROC CVPR IEEE, P7894, DOI 10.1109/CVPR.2019.00809
   Rochan M, 2018, LECT NOTES COMPUT SC, V11216, P358, DOI 10.1007/978-3-030-01258-8_22
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Sharghi A, 2017, PROC CVPR IEEE, P2127, DOI 10.1109/CVPR.2017.229
   Shemer Yair., 2019, ILS SUMM ITERATED LO
   Shen T, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4345
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Sun C, 2019, PROC CVPR IEEE, P273, DOI 10.1109/CVPR.2019.00036
   Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tejero-de-Pablos A, 2018, IEEE T MULTIMEDIA, V20, P2000, DOI 10.1109/TMM.2018.2794265
   Tsai YHH, 2019, PROC CVPR IEEE, P10416, DOI 10.1109/CVPR.2019.01067
   Vaswani A, 2017, ADV NEUR IN, V30
   Vivekraj VK, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3347712
   Wang M, 2019, P INT C LEARN REPR W
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wu JC, 2019, PROC CVPR IEEE, P9956, DOI 10.1109/CVPR.2019.01020
   Wu JX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P827, DOI 10.1145/3343031.3350938
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xiong B, 2019, PROC CVPR IEEE, P1258, DOI 10.1109/CVPR.2019.00135
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang H, 2015, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2015.526
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Yuan L, 2019, AAAI CONF ARTIF INTE, P9143
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang K, 2016, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2016.120
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhang Y, 2016, IEEE T MULTIMEDIA, V18, P418, DOI 10.1109/TMM.2016.2520827
   Zhang Yujia, 2018, BRIT MACH VIS C BMVC
   Zhang Zechen., 2018, ACM Trans. Graph, P11
   Zhao B, 2018, PROC CVPR IEEE, P7405, DOI 10.1109/CVPR.2018.00773
   Zhou J, 2018, ARXIV181208434
   Zhou KY, 2018, AAAI CONF ARTIF INTE, P7582
NR 74
TC 17
Z9 17
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3203
EP 3214
DI 10.1109/TMM.2020.3021980
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000020
DA 2024-07-18
ER

PT J
AU Kim, N
   Kang, JW
AF Kim, Nayoung
   Kang, Je-Won
TI Dynamic Motion Estimation and Evolution Video Prediction Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Dynamics; Convolution; Streaming media; Motion estimation;
   Adaptation models; Spatiotemporal phenomena; Long-term video generation
   and prediction; video understanding and analysis; deep learning;
   Convolutional Neural Network; Long Short-term Memory
AB Future video prediction provides valuable information that helps a computer machine understand the surrounding environment and make critical decisions in real-time. However, long-term video prediction remains a challenging problem due to the complicated spatiotemporal dynamics in a video. In this paper, we propose a dynamic motion estimation and evolution (DMEE) network model to generate unseen future videos from the observed videos in the past. Our primary contribution is to use trained kernels in convolutional neural network (CNN) and long short-term memory (LSTM) architectures, adapted to each time step and sample position, to efficiently manage spatiotemporal dynamics. DMEE uses the motion estimation (ME) and motion update (MU) kernels to predict the future video using an end-to-end prediction-update process. In the prediction, the ME kernel estimates the temporal changes. In the update step, the MU kernel combines the estimates with the previously generated frames as reference frames using a weighted average. The kernels are not only used for a current frame, but also are evolved to generate successive frames to enable temporally specific filtering. We perform qualitative performance analysis and quantitative performance analysis based on the peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and video classification score developed for examining the visual quality of the generated video. It is demonstrated with experiments that our algorithm provides better qualitative and quantitative performance superior to the current state-of-the-art algorithms. Our source codes are available in https://github.com/Nayoung-Kim-ICP/Video-Generation.
C1 [Kim, Nayoung; Kang, Je-Won] Ewha Womans Univ, Dept Elect & Elect Engn, Seoul 03760, South Korea.
   [Kang, Je-Won] Ewha Womans Univ, Smart Factory Multidisciplinary Program, Seoul 03760, South Korea.
C3 Ewha Womans University; Ewha Womans University
RP Kang, JW (corresponding author), Ewha Womans Univ, Dept Elect & Elect Engn, Seoul 03760, South Korea.; Kang, JW (corresponding author), Ewha Womans Univ, Smart Factory Multidisciplinary Program, Seoul 03760, South Korea.
EM 1210513skdud@ewhain.net; jewonk@ewha.ac.kr
RI Kim, Nayoung/J-5387-2012
OI Kang, Jewon/0000-0002-1637-9479
FU National Research Foundation of Korea (NRF) - Korea Government(MSIT)
   [NRF-2019R1C1C1010249]; Institute of Information and communications
   Technology Planning and Evaluation (IITP) - Korea government (MSIT)
   [2018-0-00765]
FX This work was supported in part by the National Research Foundation of
   Korea (NRF) grant funded by the Korea
   Government(MSIT)(NRF-2019R1C1C1010249) and in part by the Institute of
   Information and communications Technology Planning and Evaluation (IITP)
   grant funded by the Korea government (MSIT) (2018-0-00765 Development of
   Compression and Transmission Technologies for Ultra High Quality
   Immersive Videos Supporting 6DoF).
CR Abu-El-Haija Sami, 2016, arXiv
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bao WB, 2018, IEEE T IMAGE PROCESS, V27, P3813, DOI 10.1109/TIP.2018.2825100
   Barratt Shane, 2018, ARXIV180101973
   Chen XT, 2020, IEEE T MULTIMEDIA, V22, P1591, DOI 10.1109/TMM.2019.2946475
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Choi S, 2015, IEEE T IMAGE PROCESS, V24, P3652, DOI 10.1109/TIP.2015.2449078
   Cui RP, 2020, IEEE T MULTIMEDIA, V22, P2551, DOI 10.1109/TMM.2019.2960700
   Denton E. L., 2017, ADV NEURAL INFORM PR, P4414
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Finn C, 2016, ADV NEUR IN, V29
   Girdhar R, 2017, PROC CVPR IEEE, P3165, DOI 10.1109/CVPR.2017.337
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hecht-Nielsen R., 1992, Neural Networks for Perception, P65, DOI DOI 10.1016/B978-0-12-741252-8.50010-8
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Ioffe S., 2015, arXiv: Learning
   Jia X., 2016, Advances in Neural Information Processing Systems, V29, P667
   Kim N, 2018, IEEE IMAGE PROC, P3578, DOI 10.1109/ICIP.2018.8451079
   King DB, 2015, ACS SYM SER, V1214, P1
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194
   Liu WQ, 2018, LECT NOTES COMPUT SC, V11216, P175, DOI 10.1007/978-3-030-01258-8_11
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Lotter W., 2016, Deep predictive coding networks for video prediction and unsupervised learning
   Lu CC, 2017, PROC CVPR IEEE, P2137, DOI 10.1109/CVPR.2017.230
   Mathieu M., 2015, PROC INT C LEARN REP
   Miech A., 2017, ARXIV PREPRINT ARXIV
   Mottaghi R, 2016, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2016.383
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Odena A., 2016, DISTILL, V1, P3, DOI [10.23915/distill.00003., DOI 10.23915/DISTILL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Oliu M., 2017, ARXIV171200311
   Qi MS, 2020, IEEE T IMAGE PROCESS, V29, P5420, DOI 10.1109/TIP.2020.2983567
   Radwan Nisreen I., 2012, P ICCSEA, P765
   Reda FA, 2018, LECT NOTES COMPUT SC, V11211, P747, DOI 10.1007/978-3-030-01234-2_44
   Shen XT, 2012, J AM STAT ASSOC, V107, P223, DOI 10.1080/01621459.2011.645783
   Shi XJ, 2015, ADV NEUR IN, V28
   Shmelkov K, 2018, LECT NOTES COMPUT SC, V11206, P218, DOI 10.1007/978-3-030-01216-8_14
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soomro K., 2012, ARXIV12120402CS
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Villegas R., 2017, International Conference on Machine Learning, P3560
   Villegas Ruben, 2017, ICLR
   Wichers N., 2018, ICML
   Xu JW, 2018, PROC CVPR IEEE, P1460, DOI 10.1109/CVPR.2018.00158
   Yoon SJ, 2018, IEEE T IMAGE PROCESS, V27, P5918, DOI 10.1109/TIP.2018.2861567
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhu H, 2018, PROC CVPR IEEE, P4450, DOI 10.1109/CVPR.2018.00468
NR 51
TC 8
Z9 8
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3986
EP 3998
DI 10.1109/TMM.2020.3035281
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900006
DA 2024-07-18
ER

PT J
AU Li, ZY
   Deng, C
   Yang, EK
   Tao, DC
AF Li, Zeyu
   Deng, Cheng
   Yang, Erkun
   Tao, Dacheng
TI Staged Sketch-to-Image Synthesis via Semi-supervised Generative
   Adversarial Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gallium nitride; Generative adversarial networks; Image generation;
   Training; Image edge detection; Task analysis; Image retrieval; Gan;
   image generation; sketch
AB Sketch-based image synthesis is a challenging problem in computer graphics and vision. Existing approaches either require exact edge maps or rely on the retrieval of existing photographs, which limits their applications in real-world scenarios. Accordingly in this work, we propose a staged semi-supervised generative adversarial networks based method for sketch-to-image synthesis, which can directly generate realistic images from novice sketches. More specifically, we first adopt a conditional generative adversarial network (CGAN) to extract class-wise representations from unpaired images. These class-wise representations are then exploited and incorporated with another CGAN, which are used to generate realistic images from sketches. By incorporating the class-wise representations, our method can leverage both the general class information from unpaired images and the targeted object information from input sketches. Additionally, this network architecture also enables us to take full advantage of widely available unpaired images and learn more accurate class representations. Extensive experiments demonstrate, compared with state-of-the-art image translation methods, our approach can achieve more promising results and synthesize images with significantly better Inception Scores and Frechet Inception Distance.
C1 [Li, Zeyu; Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Yang, Erkun] Univ N Carolina, Chapel Hill, NC 27599 USA.
   [Tao, Dacheng] Univ Sydney, Fac Engn & Informat Technol, UBTECH Sydney Artificial Intelligence Ctr, Darlington, NSW 2008, Australia.
   [Tao, Dacheng] Univ Sydney, Fac Engn & Informat Technol, Sch Informat Technol, Darlington, NSW 2008, Australia.
C3 Xidian University; University of North Carolina; University of North
   Carolina Chapel Hill; University of Sydney; University of Sydney
RP Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM zeyuli1990@gmail.com; chdeng.xd@gmail.com; ekyang@med.unc.edu;
   dacheng.tao@sydney.edu.au
RI li, zy/HZM-1892-2023; Li, Zeyu/GXM-4336-2022; Tao, Dacheng/A-5449-2012
OI Tao, Dacheng/0000-0001-7225-5449; Deng, Cheng/0000-0003-2620-3247
FU National Key R&D Program of China [2017YFE0104100, 2016YFE0200400]
FX This work was supported in part by the National Key R&D Program of China
   under Grants 2017YFE0104100 and 2016YFE0200400.
CR Bodla N, 2018, LECT NOTES COMPUT SC, V11209, P689, DOI 10.1007/978-3-030-01228-1_41
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Chen WL, 2018, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR.2018.00981
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Forsyth D.A., 2003, COMPUTER VISION MODE, V17, P21
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Ha David, 2018, INT C LEARN REPR
   Heusel M., 2017, ADV NEURAL INFORM PR, P6626
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Koenderink J.J., 1991, Applied Optics, V30, P714
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Liu Y., 2019, IEEE T MULTIMEDIA
   Lu Yongyi, 2018, P EUR C COMP VIS, P205
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Miyato T., 2018, Proceedings of the 6th International Conference on Learning Representations, P1
   Parui S, 2014, LECT NOTES COMPUT SC, V8694, P398, DOI 10.1007/978-3-319-10599-4_26
   Qin J, 2017, PROC CVPR IEEE, P6728, DOI 10.1109/CVPR.2017.712
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saavedra J. M., 2015, BRIT MACH VIS C, P7
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Shen FL, 2018, PROC CVPR IEEE, P8061, DOI 10.1109/CVPR.2018.00841
   Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345
   Shen FM, 2015, IEEE I CONF COMP VIS, P4148, DOI 10.1109/ICCV.2015.472
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang ZY, 2019, IEEE T IMAGE PROCESS, V28, P2530, DOI 10.1109/TIP.2018.2887017
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xinyu Gong, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11209), P56, DOI 10.1007/978-3-030-01228-1_4
   Yang M., 2020, PROGR DOMAIN INDEPEN
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Zhang JY, 2018, LECT NOTES COMPUT SC, V11206, P304, DOI 10.1007/978-3-030-01216-8_19
   Zhou YZ, 2019, PROC CVPR IEEE, P4041, DOI 10.1109/CVPR.2019.00417
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 48
TC 21
Z9 21
U1 3
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2694
EP 2705
DI 10.1109/TMM.2020.3015015
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600012
DA 2024-07-18
ER

PT J
AU Min, SB
   Chen, XJ
   Xie, HT
   Zha, ZJ
   Zhang, YD
AF Min, Shaobo
   Chen, Xuejin
   Xie, Hongtao
   Zha, Zheng-Jun
   Zhang, Yongdong
TI A Mutually Attentive Co-Training Framework for Semi-Supervised
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Data models; Training; Reliability; Task analysis;
   Predictive models; Image segmentation; Self-training; mutual attention;
   noisy labels; recognition
ID SEGMENTATION; CLASSIFICATION; NETWORK; GRAPH
AB Self-training plays an important role in practical recognition applications where sufficient clean labels are unavailable. Existing methods focus on generating reliable pseudo labels to retrain a model, while ignoring the importance of improving model reliability to those inevitably mislabeled data. In this paper, we propose a novel Mutually Attentive Co-training Framework (MACF) that can effectively alleviate the negative impacts of incorrect labels on model retraining by exploring deep model disagreements. Specifically, MACF trains two symmetrical sub-networks that have the same input and are connected by several attention modules at different layers. Each attention module analyzes the inferred features from two sub-networks for the same input and feedback attention maps for them to indicate noisy gradients. This is realized by exploring the back-propagation process of incorrect labels at different layers to design attention modules. By multi-layer interception, the noisy gradients caused by incorrect labels can be effectively reduced for both sub-networks, leading to robust training to potential incorrect labels. In addition, a hierarchical distillation strategy is developed to improve the pseudo labels by aggregating the predictions from multi-models and data transformations. The experiments on six general benchmarks, including classification and biomedical segmentation, demonstrate that MACF is much robust to noisy labels than previous methods.
C1 [Min, Shaobo; Chen, Xuejin; Xie, Hongtao; Zha, Zheng-Jun; Zhang, Yongdong] Univ Sci & Technol China, Natl Engn Lab Brain Inspired Intelligence Technol, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Chen, XJ; Xie, HT (corresponding author), Univ Sci & Technol China, Natl Engn Lab Brain Inspired Intelligence Technol, Hefei 230026, Peoples R China.
EM mbobo@mail.ustc.edu.cn; xjchen99@ustc.edu.cn; htxie@ustc.edu.cn;
   zhazj@ustc.edu.cn; zhyd73@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020
OI Chen, Xuejin/0000-0003-0478-7018
FU National Key Research and Development Program of China [2017YFC0820600];
   National Nature Science Foundation of China [61525206, 61632006,
   U1936210]; Youth Innovation Promotion Association Chinese Academy of
   Sciences [2017209]; Fundamental Research Funds for the Central
   Universities [WK3490000003, WK2100100030]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFC0820600, in part by the
   National Nature Science Foundation of China under Grant 61525206, Grant
   61632006, and Grant U1936210, in part by the Youth Innovation Promotion
   Association Chinese Academy of Sciences under Grant 2017209, and in part
   by the Fundamental Research Funds for the Central Universities under
   Grant WK3490000003 and Grant WK2100100030.
CR [Anonymous], 2016, Reconstruction, Segmentation, and Analysis of Medical Images
   [Anonymous], 2007, PROCESSDINGS AAAI
   Atlas L.E., 1990, NIPS, V2, P566
   Barandela R, 2000, LECT NOTES COMPUT SC, V1876, P621
   Charniak, 2008, HLT SHORT, V08, P101104
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Cheng K.T., IEEE J BIOMED HLTH I, VPP, P1
   cicek Ozgtin, 2016, INT C MED IM COMP CO, P424, DOI DOI 10.1007/978-3-319-46723-8_49
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DIDACI L, 2006, JOINT IAPR INT WORKS, P522
   Dópido I, 2013, IEEE T GEOSCI REMOTE, V51, P4032, DOI 10.1109/TGRS.2012.2228275
   Dou Q, 2017, MED IMAGE ANAL, V41, P40, DOI 10.1016/j.media.2017.05.001
   Gold JR, 2017, PLAN HIST ENVIRON SE, P1
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   Hajmohammadi MS, 2015, INFORM SCIENCES, V317, P67, DOI 10.1016/j.ins.2015.04.003
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Han X, 2018, AAAI CONF ARTIF INTE, P4832
   HANSEN LK, 1990, IEEE T PATTERN ANAL, V12, P993, DOI 10.1109/34.58871
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kistler M, 2013, J MED INTERNET RES, V15, DOI 10.2196/jmir.2930
   Laine Samuli, 2016, PROC INT C LEARN REP
   Lequan Yu, 2017, Medical Image Computing and Computer-Assisted Intervention, MICCAI 2017. 20th International Conference. Proceedings: LNCS 10434, P287, DOI 10.1007/978-3-319-66185-8_33
   Li F, 2018, IEEE T GEOSCI REMOTE, V56, P3, DOI 10.1109/TGRS.2017.2713123
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Madani A, 2018, I S BIOMED IMAGING, P1038, DOI 10.1109/ISBI.2018.8363749
   Makropoulos A, 2018, NEUROIMAGE, V170, P231, DOI 10.1016/j.neuroimage.2017.06.074
   Malach E., 2017, ADV NEURAL INFORM PR, P961971
   Min SB, 2020, IEEE T IMAGE PROCESS, V29, P4996, DOI 10.1109/TIP.2020.2977457
   Mnih V., 2012, P 29 INT C MACH LEAR, P567
   Mohammadreza S., 2017, INT MICCAI BRAINL WO, P279
   Müller H, 2017, IEEE T MULTIMEDIA, V19, P2093, DOI 10.1109/TMM.2017.2729400
   Mukhopadhyay A., 2016, RECONSTRUCTION SEGME, P165
   Nigam K., 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P86, DOI 10.1145/354756.354805
   Pace DF, 2015, LECT NOTES COMPUT SC, V9351, P80, DOI 10.1007/978-3-319-24574-4_10
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Qiao YG, 2019, IEEE T MULTIMEDIA, V21, P1, DOI 10.1109/TMM.2018.2845699
   Radosavovic I, 2018, PROC CVPR IEEE, P4119, DOI 10.1109/CVPR.2018.00433
   Reedman SE, 2016, PHYS OCCUP THER PEDI, V36, P292, DOI 10.3109/01942638.2015.1040576
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rooyen B. V., 2015, P ADV NEUR INF PROC, P10
   Saha S, 2016, EXPERT SYST APPL, V52, P50, DOI 10.1016/j.eswa.2016.01.005
   Shahzad R, 2017, LECT NOTES COMPUT SC, V10129, P147, DOI 10.1007/978-3-319-52280-7_15
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Tanha J, 2017, INT J MACH LEARN CYB, V8, P355, DOI 10.1007/s13042-015-0328-7
   Tseng KL, 2017, PROC CVPR IEEE, P3739, DOI 10.1109/CVPR.2017.398
   Tziritas G, 2017, LECT NOTES COMPUT SC, V10129, P129, DOI 10.1007/978-3-319-52280-7_13
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang GT, 2018, IEEE T MED IMAGING, V37, P1562, DOI 10.1109/TMI.2018.2791721
   Wang L, 2017, IEEE T MULTIMEDIA, V19, P646, DOI 10.1109/TMM.2016.2617079
   Wang M, 2015, IEEE T KNOWL DATA EN, V27, P2564, DOI 10.1109/TKDE.2015.2415497
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P975, DOI 10.1109/TMM.2012.2185041
   Wu D, 2018, NEUROCOMPUTING, V275, P180, DOI 10.1016/j.neucom.2017.05.072
   Xu H, 2019, LECT NOTES COMPUT SC, V11766, P420, DOI 10.1007/978-3-030-32248-9_47
   YANG X, 2017, ACMTRANS MULTIMEDIA, V13, P1
   Yu L., 2016, Reconstruction, Segmentation, and Analysis of Medical Images, P103
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 61
TC 10
Z9 10
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 899
EP 910
DI 10.1109/TMM.2020.2990063
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300006
DA 2024-07-18
ER

PT J
AU Qian, XM
   Wu, YX
   Li, MD
   Ren, YY
   Jiang, SH
   Li, ZT
AF Qian, Xueming
   Wu, Yuxia
   Li, Mingdi
   Ren, Yayun
   Jiang, Shuhui
   Li, Zhetao
TI LAST: Location-Appearance-Semantic-Temporal Clustering Based POI
   Summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Social network services; Clustering methods; Semantics;
   Planning; Data mining; Internet; Clustering; feature extraction;
   multimedia; POI summarization; social media
ID SOCIAL MEDIA; RECOMMENDATION
AB When planning a trip, users tend to browse Place-of-Interest (POI) information on the Internet and then depart. Many works aimed at summarizing POIs by visual and textual analysis, while many of them ignored the inter-relationship between different views offered by the community-contributed information. In this paper, we propose a City-POI-LOI (CPL) summarization method to automatically mine POIs from the city-level landmark images. And a Location-Appearance-Semantic-Temporal (LAST) clustering method is proposed to mine the popular viewpoints termed Location-Of-Interest (LOI) in each POI by taking location, appearance, semantic, and temporal feature into consideration. We perform text and image summarization for each LOI, and we further summarize the POIs based on season. We conduct a series of experiments based on DIV400 and ATCF Dataset. Experimental results show the effectiveness of the proposed POI summarization approach.
C1 [Qian, Xueming] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, SMILES LAB, Xian 710049, Peoples R China.
   [Wu, Yuxia; Li, Mingdi; Ren, Yayun] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
   [Jiang, Shuhui] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.
   [Li, Zhetao] Xiangtan Univ, Coll Informat Engn, Xiangtan 411105, Hunan, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong
   University; Northeastern University; Xiangtan University
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, SMILES LAB, Xian 710049, Peoples R China.
EM qianxm@mail.xjtu.edu.cn; wuyuxia@stu.xjtu.edu.cn;
   limingdi@stu.xjtu.edu.cn; renyy@stu.xjtu.edu.cn; shjiang@ece.neu.edu;
   liztchina@gmail.com
RI Jiang, Shuhui/W-6907-2019; Li, Zhetao/H-1293-2017; Ren,
   Yayun/KHD-0116-2024
OI Ren, Yayun/0000-0001-8630-7262
FU NSFC [61732008, 61772407]; Guangdong Provincial Science and Technology
   Plan [2016A010101005]
FX This work was supported in part by the NSFC under Grants 61732008 and
   61772407 and in part by Guangdong Provincial Science and Technology Plan
   under Grant 2016A010101005.
CR [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], 2014, MULT SYST C 2014
   Bickel S, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P19, DOI 10.1109/ICDM.2004.10095
   CAO XC, 2015, PROC CVPR IEEE, P586, DOI DOI 10.1109/CVPR.2015.7298657
   Chen TY, 2013, IEEE INT CONF COMMUN, P11, DOI 10.1109/ICCChinaW.2013.6670558
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1283, DOI 10.1109/TMM.2013.2265077
   Cheng SL, 2016, INTERDISCIP SCI, V8, P65, DOI 10.1007/s12539-015-0106-y
   Chum O, 2010, IEEE T PATTERN ANAL, V32, P371, DOI 10.1109/TPAMI.2009.166
   FAYYAD UM, 1993, IJCAI-93, VOLS 1 AND 2, P1022
   Fengjiao Wang, 2016, 2016 17th IEEE International Conference on Mobile Data Management (MDM), P142, DOI 10.1109/MDM.2016.31
   Goel S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1434, DOI 10.1145/3123266.3123407
   Guan ZY, 2015, IEEE T KNOWL DATA EN, V27, P3016, DOI 10.1109/TKDE.2015.2448542
   Hao Qiang., 2009, Proceedings of the ACM International Conference on Multimedia, P801
   Huang C, 2016, IEEE INT CONF DISTR, P67, DOI 10.1109/DCOSS.2016.12
   Huang J, 2015, 2015 IEEE 6TH INTERNATIONAL SYMPOSIUM ON MICROWAVE, ANTENNA, PROPAGATION, AND EMC TECHNOLOGIES (MAPE), P4, DOI 10.1109/MAPE.2015.7510254
   Ionescu B, 2014, IEEE IMAGE PROC, P3072, DOI 10.1109/ICIP.2014.7025621
   Jeffries Adrianne., 2013, The man behind flickr on making the service 'awesome again'
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang M, 2014, IEEE T KNOWL DATA EN, V26, P2789, DOI 10.1109/TKDE.2014.2300487
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Kaur S, 2017, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING, DATA SCIENCE AND ENGINEERING (CONFLUENCE 2017), P210, DOI 10.1109/CONFLUENCE.2017.7943151
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Li K, 2021, IEEE T MULTIMEDIA, V23, P257, DOI 10.1109/TMM.2020.2981237
   Li MZ, 2016, INT CONF BIG DATA, P289, DOI 10.1109/BIGCOMP.2016.7425930
   Liu Q, 2014, IEEE T KNOWL DATA EN, V26, P278, DOI 10.1109/TKDE.2012.233
   Liu YC, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1015, DOI 10.1145/2939672.2939773
   Luo SR, 2018, AAAI CONF ARTIF INTE, P3730
   McKenzie G, 2017, LECT NOTES GEOINF CA, P237, DOI 10.1007/978-3-319-56759-4_14
   Mikolov T., 2013, P 26 INT C NEURAL IN, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Nie WZ, 2017, J VIS COMMUN IMAGE R, V48, P375, DOI 10.1016/j.jvcir.2016.11.015
   Pang Y., 2010, NEUROCOMPUTING, V115, P352
   Papadopoulos S, 2011, IEEE MULTIMEDIA, V18, P52, DOI 10.1109/MMUL.2010.68
   Psyllidis A, 2018, IEEE ACCESS, V6, P34334, DOI 10.1109/ACCESS.2018.2850062
   Qian XM, 2019, KNOWL-BASED SYST, V164, P107, DOI 10.1016/j.knosys.2018.10.028
   Qian XM, 2018, IEEE T IMAGE PROCESS, V27, P1178, DOI 10.1109/TIP.2017.2769454
   Qian XM, 2017, P IEEE, V105, P1937, DOI 10.1109/JPROC.2017.2731600
   Qian XM, 2016, IEEE T CIRC SYST VID, V26, P1746, DOI 10.1109/TCSVT.2015.2475815
   Qian XM, 2015, IEEE T CIRC SYST VID, V25, P1857, DOI 10.1109/TCSVT.2014.2369731
   Quack Till., 2008, P 2008 INT C CONTENT, P47
   Shuhui Jiang, 2016, IEEE Transactions on Big Data, V2, P43, DOI 10.1109/TBDATA.2016.2541160
   Simon I, 2007, IEEE I CONF COMP VIS, P274
   Spyromitros-Xioufis E, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P323, DOI 10.1145/2671188.2749334
   Tzortzis G, 2012, IEEE DATA MINING, P675, DOI 10.1109/ICDM.2012.43
   Vinh Nguyen Xuan, 2009, P 26 ANN INT C MACH, P1073, DOI DOI 10.1145/1553374.1553511
   Wang CD, 2016, IEEE T KNOWL DATA EN, V28, P1007, DOI 10.1109/TKDE.2015.2503743
   Wang H, 2019, KNOWL-BASED SYST, V163, P1009, DOI 10.1016/j.knosys.2018.10.022
   Wang XY, 2015, IEEE T MULTIMEDIA, V17, P409, DOI 10.1109/TMM.2014.2385473
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Xiao Cai, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1977, DOI 10.1109/CVPR.2011.5995740
   Xu JL, 2017, IEEE T IMAGE PROCESS, V26, P3016, DOI 10.1109/TIP.2017.2665976
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang YL, 2014, PROCEEDINGS OF 2013 INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CLOUD COMPUTING COMPANION (ISCC-C), P1, DOI 10.1109/ISCC-C.2013.50
   Ye M, 2008, INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTATION TECHNOLOGY AND AUTOMATION, VOL 2, PROCEEDINGS, P423, DOI 10.1109/ICICTA.2008.189
   Ye Y, 2016, Proceedings NICOGRAPH International 2016, P137, DOI 10.1109/NicoInt.2016.30
   Zhang GY, 2016, PROCEEDINGS 2016 IEEE SECOND INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING SERVICE AND APPLICATIONS (BIGDATASERVICE 2016), P286, DOI 10.1109/BigDataService.2016.12
   Zhang XC, 2014, IEEE DATA MINING, P1103, DOI 10.1109/ICDM.2014.19
   Zhang XC, 2016, PROCEEDINGS OF 2016 IEEE INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND BIG DATA ANALYSIS (ICCCBDA 2016), P109, DOI 10.1109/ICCCBDA.2016.7529543
   Zhao GS, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3337967
   Zhao GS, 2019, IEEE T MULTIMEDIA, V21, P771, DOI 10.1109/TMM.2018.2863598
   Zhao KQ, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P417, DOI 10.1145/2983323.2983850
   Zhao YN, 2011, ADV MATER RES-SWITZ, V337, P392, DOI 10.4028/www.scientific.net/AMR.337.392
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
   Zhou P, 2016, IEEE T MULTIMEDIA, V18, P1217, DOI 10.1109/TMM.2016.2537216
NR 63
TC 14
Z9 14
U1 6
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 378
EP 390
DI 10.1109/TMM.2020.2977478
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600030
DA 2024-07-18
ER

PT J
AU Zhang, YC
   Min, WQ
   Nie, LQ
   Jiang, SQ
AF Zhang, Yanchao
   Min, Weiqing
   Nie, Liqiang
   Jiang, Shuqiang
TI Hybrid-Attention Enhanced Two-Stream Fusion Network for Video Venue
   Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Feature extraction; Convolution; Streaming media; Object
   oriented modeling; Three-dimensional displays; Neural networks; Feature
   extraction; knowledge representation; supervised learning; video signal
   processing
AB Video venue category prediction has been drawing more attention in the multimedia community for various applications such as personalized location recommendation and video verification. Most of existing works resort to the information from either multiple modalities or other platforms for strengthening video representations. However, noisy acoustic information, sparse textual descriptions and incompatible cross-platform data could limit the performance gain and reduce the universality of the model. Therefore, we focus on discriminative visual feature extraction from videos by introducing a hybrid-attention structure. Particularly, we propose a novel Global-Local Attention Module (GLAM), which can be inserted to neural networks to generate enhanced visual features from video content. In GLAM, the Global Attention (GA) is used to catch contextual scene-oriented information via assigning channels with various weights while the Local Attention (LA) is employed to learn salient object-oriented features via allocating different weights for spatial regions. Moreover, GLAM can be extended to ones with multiple GAs and LAs for further visual enhancement. These two types of features respectively captured by GAs and LAs are integrated via convolution layers, and then delivered into convolutional Long Short-Term Memory (convLSTM) to generate spatial-temporal representations, constituting the content stream. In addition, video motions are explored to learn long-term movement variations, which also contributes to video venue prediction. The content and motion stream constitute our proposed Hybrid-Attention Enhanced Two-Stream Fusion Network (HA-TSFN). HA-TSFN finally merges the features from two streams for comprehensive representations. Extensive experiments demonstrate that our method achieves the state-of-the-art performance in the large-scale dataset Vine. The visualization also shows that the proposed GLAM can capture complementary scene-oriented and object-oriented visual features from videos. Our code is available at: https://github.com/zhangyanchao1014/HA-TSFN.
C1 [Zhang, Yanchao; Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266000, Peoples R China.
   [Zhang, Yanchao; Min, Weiqing; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Min, Weiqing; Jiang, Shuqiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Shandong University; Chinese Academy of Sciences; Institute of Computing
   Technology, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Min, WQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM zycl641299934@gmail.com; minweiqing@ict.ac.cn; nieliqiang@gmail.com;
   sqjiang@ict.ac.cn
RI zhen, wang/KBA-3844-2024; yang, yijing/JWO-8234-2024; Zhang,
   Yanchao/JMB-7717-2023
FU Shandong Provincial Key Research and Development Program
   [2019JZZY010118]; National Natural Science Foundation of China
   [61972378, 61532018, U1936203, U19B2040]; Shandong Provincial Natural
   Science Foundation [ZR2019JQ23]; Innovation Teams in Colleges and
   Universities in Jinan [2018GXRC014]
FX This work was supported in part by Shandong Provincial Key Research and
   Development Program, 2019JZZY010118, in part by the National Natural
   Science Foundation of China under Grants 61972378, 61532018, U1936203,
   and U19B2040, in part by Shandong Provincial Natural Science Foundation
   under Grant ZR2019JQ23, and in part by the Innovation Teams in Colleges
   and Universities in Jinan under Grant 2018GXRC014.
CR [Anonymous], 2016, P 13 AAAI C ART INT
   [Anonymous], 2018, EUR C COMP VIS
   [Anonymous], 2016, P ADV NEUR INF PROC
   Ba J., 2015, P ICLR, P1, DOI DOI 10.1016/J.JCYT.2014.02.008
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cherian A, 2017, PROC CVPR IEEE, P1581, DOI 10.1109/CVPR.2017.172
   Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Friedland G., 2011, Proceedings of the 3rd ACM SIGMM International Workshop on Social Media, P23
   Hadji Isma, 2018, PROC EUROPEAN C COMP, P320
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, ADV NEUR IN, V9, P473
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang SQ, 2019, IEEE T MULTIMEDIA, V21, P1609, DOI 10.1109/TMM.2018.2876830
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Li XL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2208
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   LINSLEY D., 2019, INT C LEARNING REPRE
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Vo N, 2017, IEEE I CONF COMP VIS, P2640, DOI 10.1109/ICCV.2017.286
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Sarker MMK, 2019, IEEE ACCESS, V7, P39069, DOI 10.1109/ACCESS.2019.2902225
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, IEEE INT C ICLR
   Simonyan K, 2014, ADV NEUR IN, V27
   Wang J., 2018, ECCV, P685
   Wang J, 2018, PROC CVPR IEEE, P1149, DOI 10.1109/CVPR.2018.00126
   Wang L, 2019, IEEE I CONF COMP VIS, P8697, DOI 10.1109/ICCV.2019.00879
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Wei YW, 2020, IEEE T IMAGE PROCESS, V29, P1, DOI 10.1109/TIP.2019.2923608
   Weyand T, 2016, LECT NOTES COMPUT SC, V9912, P37, DOI 10.1007/978-3-319-46484-8_3
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Zhang J., 2016, P ACM INT C MULT, P1415
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu YY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P99, DOI 10.1145/3240508.3240525
NR 46
TC 2
Z9 2
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2917
EP 2929
DI 10.1109/TMM.2020.3019714
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600030
DA 2024-07-18
ER

PT J
AU Zhou, ZH
   Li, J
   Quan, YH
   Xu, RT
AF Zhou, Zihan
   Li, Jing
   Quan, Yuhui
   Xu, Ruotao
TI Image Quality Assessment Using Kernel Sparse Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Kernel; Dictionaries; Measurement; Encoding;
   Visualization; Mathematical model; Image quality assessment; sparse
   representation; kernel sparse coding; dictionary learning
ID SIMILARITY DEVIATION; DICTIONARY; REPRESENTATION
AB One key in image quality assessment (IQA) is the design of image representations that can capture the changes of image structures caused by distortions. Recent studies show that sparse coding has emerged as a promising approach to analyzing image structures for IQA. However, existing sparse-coding-based IQA approaches use linear coding models, which ignore the nonlinearities of manifolds of image patches and thus cannot analyze complex image structures well. To overcome such a weakness, in this paper, we introduce nonlinear sparse coding to IQA. A kernel dictionary construction scheme is proposed, which combines analytic dictionaries and learnable dictionaries to guarantee both the stability and effectiveness of kernel sparse coding in the context of IQA. Built upon the kernel dictionary construction, an effective full-reference IQA metric is developed. Benefiting from the considerations on nonlinearities during sparse coding, the proposed IQA metric not only characterizes image distortions better, but also achieves improvement on the consistency with subjective perception, when compared to the metrics built upon linear sparse coding. Such benefits are demonstrated with the experimental results on eight benchmark datasets in terms of common criteria.
C1 [Zhou, Zihan; Quan, Yuhui; Xu, Ruotao] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Zhou, Zihan; Quan, Yuhui; Xu, Ruotao] South China Univ Technol, Guangdong Prov Key Lab Computat Intelligence & Cy, Guangzhou 510006, Peoples R China.
   [Li, Jing] Alibaba Grp, Moku Lab, Beijing 100016, Peoples R China.
C3 South China University of Technology; South China University of
   Technology; Alibaba Group
RP Quan, YH (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM cszzh@mail.scut.edu.cn; lj225205@alibaba-inc.com; csyhquan@scut.edu.cn;
   xu.ruotao@mail.scut.edu.cn
RI Xu, Ruotao/HTR-9465-2023
OI Xu, Ruotao/0000-0002-5277-9859
FU National Natural Science Foundation of China [61872151, U1611461];
   Natural Science Foundation of Guangdong Province [2017A030313376,
   2020A1515011128]; Fundamental Research Funds for Central Universities of
   China [x2js-D2181690]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872151 and U1611461, in part by the
   Natural Science Foundation of Guangdong Province under Grants
   2017A030313376 and 2020A1515011128, and in part by the Fundamental
   Research Funds for Central Universities of China underGrant
   x2js-D2181690. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xiaoqing Zhu.
CR Ahar A, 2018, IEEE T IMAGE PROCESS, V27, P879, DOI 10.1109/TIP.2017.2771412
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2008, A wavelet tour of signal processing: The sparse way
   [Anonymous], 2004, ADV MOD RADIOELECTRO
   Barlow H, 2001, NETWORK-COMP NEURAL, V12, P241, DOI 10.1088/0954-898X/12/3/301
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chang HW, 2013, IEEE T IMAGE PROCESS, V22, P4007, DOI 10.1109/TIP.2013.2266579
   Di Z., 2007, P IEEE INT C IM PROC, P2953
   Dong WS, 2011, PROC CVPR IEEE, P457, DOI 10.1109/CVPR.2011.5995478
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Golts A, 2016, IEEE J-STSP, V10, P726, DOI 10.1109/JSTSP.2016.2555241
   Gong D., 2010, J MACH LEARN RES, V2010, P265
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Guha T, 2014, SIGNAL PROCESS-IMAGE, V29, P1138, DOI 10.1016/j.image.2014.09.010
   Harandi M, 2015, PROC CVPR IEEE, P3926, DOI 10.1109/CVPR.2015.7299018
   He L, 2013, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2013.51
   He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795
   Nguyen HV, 2013, IEEE T IMAGE PROCESS, V22, P5123, DOI 10.1109/TIP.2013.2282078
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jin MY, 2018, CMC-COMPUT MATER CON, V56, P501, DOI 10.3970/cmc.2018.02371
   Lai YK, 2000, J VIS COMMUN IMAGE R, V11, P17, DOI 10.1006/jvci.1999.0433
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Le Callet P., 2005, Subjective quality assessment irccyn/ivc database
   LeCallet P., 2005, 1 INT WORKSH VID PRO
   Li LD, 2016, IEEE T IMAGE PROCESS, V25, P3775, DOI 10.1109/TIP.2016.2577891
   Li LD, 2016, J VIS COMMUN IMAGE R, V38, P550, DOI 10.1016/j.jvcir.2016.04.006
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu BD, 2013, PATTERN RECOGN, V46, P1879, DOI 10.1016/j.patcog.2012.11.018
   Liu XM, 2017, IEEE T IMAGE PROCESS, V26, P782, DOI 10.1109/TIP.2016.2623481
   Liu YT, 2020, IEEE T CIRC SYST VID, V30, P929, DOI 10.1109/TCSVT.2019.2900472
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Liu YT, 2016, IEEE INT SYMP CIRC S, P1586, DOI 10.1109/ISCAS.2016.7538867
   Ma KD, 2020, IEEE T PATTERN ANAL, V42, P851, DOI 10.1109/TPAMI.2018.2889948
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   OBrien C., 2016, P IMA INT C MATH SIG
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Peyré G, 2009, COMPUT VIS IMAGE UND, V113, P249, DOI 10.1016/j.cviu.2008.09.003
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Quan YH, 2016, PROC CVPR IEEE, P308, DOI 10.1109/CVPR.2016.40
   Samee MK, 2011, INT SYMP IMAGE SIG, P425
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shi JP, 2015, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2015.7298665
   Shi ZF, 2018, SIGNAL PROCESS, V145, P99, DOI 10.1016/j.sigpro.2017.11.015
   Soundararajan R, 2012, IEEE T IMAGE PROCESS, V21, P517, DOI 10.1109/TIP.2011.2166082
   Sun W, 2017, PATTERN RECOGN, V61, P153, DOI 10.1016/j.patcog.2016.07.033
   Wang SQ, 2012, IEEE T CIRC SYST VID, V22, P516, DOI 10.1109/TCSVT.2011.2168269
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZJ, 2017, IEEE T IMAGE PROCESS, V26, P4578, DOI 10.1109/TIP.2017.2718187
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Xu Y, 2015, IEEE T IMAGE PROCESS, V24, P2098, DOI 10.1109/TIP.2015.2413298
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zheng M, 2011, IEEE T IMAGE PROCESS, V20, P1327, DOI 10.1109/TIP.2010.2090535
   Zhu R, 2018, IEEE SIGNAL PROC MAG, V35, P133, DOI 10.1109/MSP.2018.2829209
NR 62
TC 10
Z9 10
U1 2
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1592
EP 1604
DI 10.1109/TMM.2020.3001472
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300009
DA 2024-07-18
ER

PT J
AU Luo, H
   Jiang, W
   Gu, YZ
   Liu, FX
   Liao, XY
   Lai, SQ
   Gu, JY
AF Luo, Hao
   Jiang, Wei
   Gu, Youzhi
   Liu, Fuxu
   Liao, Xingyu
   Lai, Shenqi
   Gu, Jianyang
TI A Strong Baseline and Batch Normalization Neck for Deep Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Person ReID; baseline; tricks; BNNeck; deep learning
ID ATTENTION
AB This study proposes a simple but strong baseline for deep person re-identification (ReID). Deep person ReID has achieved great progress and high performance in recent years. However, many state-of-the-art methods design complex network structures and concatenate multi-branch features. In the literature, some effective training tricks briefly appear in several papers or source codes. The present study collects and evaluates these effective training tricks in person ReID. By combining these tricks, the model achieves 94.5% rank-1 and 85.9% mean average precision on Market1501 with only using the global features of ResNet50. The performance surpasses all existing global- and part-based baselines in person ReID. We propose a novel neck structure named as batch normalization neck (BNNeck). BNNeck adds a batch normalization layer after global pooling layer to separate metric and classification losses into two different feature spaces because we observe they are inconsistent in one embedding space. Extended experiments show that BNNeck can boost the baseline, and our baseline can improve the performance of existing state-of-the-art methods.
C1 [Luo, Hao; Jiang, Wei; Gu, Youzhi; Gu, Jianyang] Zhejiang Univ, Coll Control Sci & Enginneering, State Key Lab Ind Control Technol, Hangzhou 310027, Peoples R China.
   [Liu, Fuxu] Ping An Technol, Shenzhen 518000, Peoples R China.
   [Liao, Xingyu] Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Lai, Shenqi] Xi An Jiao Tong Univ, Xian 710049, Peoples R China.
C3 Zhejiang University; Chinese Academy of Sciences; Xi'an Jiaotong
   University
RP Jiang, W (corresponding author), Zhejiang Univ, Coll Control Sci & Enginneering, State Key Lab Ind Control Technol, Hangzhou 310027, Peoples R China.
EM haoluocsc@zju.edu.cn; jiangwei_zju@zju.edu.cn; gu_youzhi@zju.edu.cn;
   LIUFUXU641@pingan.com.cn; randall@mail.ustc.edu.cn;
   laishenqi@stu.xjtu.edu.cn; 3150102234@zju.edu.cn
RI jiang, wei/J-6317-2018; liao, xingyu/KHE-4272-2024; Luo,
   Hao/AAG-2570-2020; Gu, Jianyang/JCE-9689-2023
OI jiang, wei/0000-0002-9240-5851; Luo, Hao/0000-0002-6405-4011; Gu,
   Jianyang/0000-0002-4060-7427
FU National Natural Science Foundation of China [61633019]; Science
   Foundation of Chinese Aerospace Industry [JCKY2018204B053]; Autonomous
   Research Project of the State Key Laboratory of Industrial Control
   Technology, China [ICT1917]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61633019, in part by the Science
   Foundation of Chinese Aerospace Industry under Grant JCKY2018204B053 and
   in part by the Autonomous Research Project of the State Key Laboratory
   of Industrial Control Technology, China under Grant ICT1917.
CR [Anonymous], 2017, ARXIV
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], 2018, ARXIV181107130
   Bai S, 2019, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2019.00083
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan X, 2019, LECT NOTES COMPUT SC, V11362, P19, DOI 10.1007/978-3-030-20890-5_2
   Fan X, 2019, J VIS COMMUN IMAGE R, V60, P51, DOI 10.1016/j.jvcir.2019.01.010
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jian MW, 2019, APPL SOFT COMPUT, V80, P425, DOI 10.1016/j.asoc.2019.04.025
   Jian MW, 2018, J VIS COMMUN IMAGE R, V53, P31, DOI 10.1016/j.jvcir.2018.03.008
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Leibe B., 2017, ARXIV170307737CS
   Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Luo H, 2019, PATTERN RECOGN, V94, P53, DOI 10.1016/j.patcog.2019.05.028
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Qi L., 2018, ARXIV180403864
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Ristani E, 2018, PROC CVPR IEEE, P6036, DOI 10.1109/CVPR.2018.00632
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen YT, 2018, PROC CVPR IEEE, P2265, DOI 10.1109/CVPR.2018.00241
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xiong F, 2019, PROC SPIE, V11069, DOI 10.1117/12.2524386
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Zhang X, 2017, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2017.578
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao YR, 2018, LECT NOTES COMPUT SC, V11213, P508, DOI 10.1007/978-3-030-01240-3_31
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
NR 54
TC 337
Z9 364
U1 12
U2 92
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2597
EP 2609
DI 10.1109/TMM.2019.2958756
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000007
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Maniotis, P
   Bourtsoulatze, E
   Thomos, N
AF Maniotis, Pantelis
   Bourtsoulatze, Eirina
   Thomos, Nikolaos
TI Tile-Based Joint Caching and Delivery of 360° Videos in Heterogeneous
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaborative caching; 360 degrees video; tile encoding; layered video;
   distortion optimization
ID EDGE; PERFORMANCE
AB The recent surge of applications involving the use of 360 degrees video challenges mobile networks infrastructure, as 360 degrees video files are of significant size, and current delivery and edge caching architectures are unable to guarantee their timely delivery. In this paper, we investigate the problem of joint collaborative content-aware caching and delivery of 360 degrees videos in a video on demand setting. The proposed scheme takes advantage of 360 degrees video encoding in multiple tiles and layers to make fine-grained decisions regarding which tiles to cache in each Small Base Station (SBS), and where to deliver them from to the end users, as users may reside in the coverage area of multiple SBSs. This permits to cache the most popular tiles in the SBSs, while the remaining tiles may be obtained through the backhaul. In addition, we explicitly consider the time delivery constraints to ensure continuous video playback. To reduce the computational complexity of the optimization problem, we simplify it by introducing a fairness constraint. This allows us to split the original problem into subproblems corresponding to Groups of Pictures (GOP). Each of the subproblems is then solved with the method of Lagrange partial relaxation. Finally, we evaluate the performance of the proposed method for various system parameters and compare it with schemes that do not consider 360 degrees video encoding into multiple tiles and quality layers, as well as with two variants of the proposed method: one that considers layered encoding and SBSs collaboration and another that uses tiles encoding but with no SBSs collaboration. The results showcase the benefits coming from caching and delivery decisions on per tile basis and the importance of exploiting SBSs collaboration.
C1 [Maniotis, Pantelis; Thomos, Nikolaos] Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
   [Bourtsoulatze, Eirina] UCL, Dept Elect & Elect Engn, London WC1E 6BT, England.
C3 University of Essex; University of London; University College London
RP Thomos, N (corresponding author), Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
EM p.maniotis@essex.ac.uk; e.bourtsoulatze@ucl.ac.uk; nthomos@essex.ac.uk
RI Thomos, Nikolaos/AAU-2328-2020; Bourtsoulatze, Eirina/ABG-5003-2021
OI Thomos, Nikolaos/0000-0001-7266-2642
FU European Union Horizon 2020 Research and Innovation Programme under the
   Marie Sklodowska-Curie Grant [750254]; Marie Curie Actions (MSCA)
   [750254] Funding Source: Marie Curie Actions (MSCA)
FX This work was supported by the European Union Horizon 2020 Research and
   Innovation Programme under the Marie Sklodowska-Curie Grant 750254. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Chuan Wu
CR Alexander L., 1998, TEACHING MATH APPL, V17, P155
   [Anonymous], 2017, IEEE T VEH TECHNOL, DOI DOI 10.1109/TVT.2017.2724547
   [Anonymous], 2006, COMPUT OPER RES, DOI DOI 10.1016/J.COR.2004.09.016
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Cheung G., 2017, P IEEE INT C IM PROC
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   George J., 2016, P INT C EM TECHN TRE
   Ghosh A., 2018, ARXIV181200816CS
   Ghosh A., 2017, ARXIV170408215CS
   Gong QS, 2015, IEEE INT SYM MULTIM, P101, DOI 10.1109/ISM.2015.81
   Grois D., 2016, P PICT COD S NUR GER
   Hosseini M, 2017, P IEEE VIRT REAL ANN, P423, DOI 10.1109/VR.2017.7892357
   Jiang X., 2017, ARXIV171010724
   Kellerer H., 2004, KNAPSACK PROBLEMS
   Khreishah A, 2015, IEEE CONF COMPUT, P257, DOI 10.1109/INFCOMW.2015.7179394
   Koutsopoulos I., 2016, P IEEE INT C COMP CO
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Le Feuvre J, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P329, DOI 10.1145/2910017.2910641
   Liu D, 2016, IEEE COMMUN MAG, V54, P22, DOI 10.1109/MCOM.2016.7565183
   Lo WC, 2017, ASIA-PAC NETW OPER M, P205, DOI 10.1109/APNOMS.2017.8094203
   Ma G, 2017, IEEE J SEL AREA COMM, V35, P1076, DOI 10.1109/JSAC.2017.2680958
   Maniotis P., 2019, P IEEE MULT SIGN PRO
   Nguyen D.T., 2017, 2017 TRON S TRONSH
   Ostovar P, 2016, IEEE INT CONF MOB, P136, DOI [10.1109/MASS.2016.027, 10.1109/MASS.2016.22]
   Perabathini B, 2015, IEEE INT CONF COMM, P2830, DOI 10.1109/ICCW.2015.7247608
   Poderys J, 2018, IEEE ACCESS, V6, P8630, DOI 10.1109/ACCESS.2018.2809490
   Poularaki K, 2019, IEEE T MOBILE COMPUT, V18, P757, DOI 10.1109/TMC.2018.2850818
   Poularakis K, 2014, IEEE INFOCOM SER, P1087, DOI 10.1109/INFOCOM.2014.6848038
   Rossi S, 2017, P IEEE 19 INT WORKSH
   Sermpezis P, 2018, IEEE J SEL AREA COMM, V36, P1300, DOI 10.1109/JSAC.2018.2844983
   Skupin R, 2016, IEEE INT SYM MULTIM, P399, DOI [10.1109/ISM.2016.0089, 10.1109/ISM.2016.137]
   Su Z, 2017, IEEE T MULTIMEDIA, V19, P2210, DOI 10.1109/TMM.2017.2733338
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tran H. T. T., 2017, P IEEE INT WORKSH MU
   Tran HTT, 2017, INT CONF UBIQ FUTUR, P7
   Vu TX, 2018, IEEE T WIREL COMMUN, V17, P2827, DOI 10.1109/TWC.2018.2803816
   Wang Y, 2016, IEEE ACCESS, V4, P8625, DOI 10.1109/ACCESS.2016.2633488
   Zare A., 2016, P 24 ACM INT C MULT, P601
   Zhan C, 2017, IEEE COMMUN LETT, V21, P2714, DOI 10.1109/LCOMM.2017.2756033
   Zhang B, 2016, IEEE T MULTIMEDIA, V18, P521, DOI 10.1109/TMM.2016.2518485
   Zink M, 2009, COMPUT NETW, V53, P501, DOI 10.1016/j.comnet.2008.09.022
NR 41
TC 21
Z9 22
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2382
EP 2395
DI 10.1109/TMM.2019.2957993
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200013
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, HB
   Ma, X
   Li, YB
AF Wu, Hanbo
   Ma, Xin
   Li, Yibin
TI Convolutional Networks With Channel and STIPs Attention Model for Action
   Recognition in Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Dynamics; Feature extraction; Three-dimensional displays; Two
   dimensional displays; Image sequences; Convolution; Action recognition;
   dynamic image sequence; channel attention; spatial-temporal interest
   points (STIPs) attention; deep networks
ID EFFICIENT FALL DETECTION
AB With the help of convolutional neural networks (CNNs), video-based human action recognition has made significant progress. CNN features that are spatial and channel-wise can provide rich information for powerful image description. However, CNNs lack the ability to process the long-term temporal dependency of an entire video and further cannot well focus on the informative motion regions of actions. Aiming at the two problems, we propose a novel video-based action recognition framework in this paper. We first represent videos with dynamic image sequences (DISs), which effectively describe videos by modeling the local spatial-temporal dynamics and dependencies. Then a channel and spatial-temporal interest points (STIPs) attention model (CSAM) based on CNNs is proposed to focus on the discriminative channels in networks and the informative spatial motion regions of human actions. Specifically, channel attention (CA) is implemented by automatically learning channel-wise convolutional features and assigning different weights for different channels. STIPs attention (SA) is encoded by projecting the detected STIPs on frames of dynamic image sequences into the corresponding convolutional feature map space. The proposed CSAM is embedded after CNN convolutional layers to refine the feature maps, followed by global average pooling to produce effective feature representations for videos. Finally frame-level video representations are fed into an LSTM to capture the temporal dependencies and make classification. Experiments on three challenging RGB-D datasets show that our method has better performance and outperforms the state-of-the-art approaches with only depth data.
C1 [Wu, Hanbo; Ma, Xin; Li, Yibin] Shandong Univ, Ctr Robot, Sch Control Sci & Engn, Jinan 250000, Peoples R China.
C3 Shandong University
RP Ma, X (corresponding author), Shandong Univ, Ctr Robot, Sch Control Sci & Engn, Jinan 250000, Peoples R China.
EM whbo@mail.sdu.edu.cn; maxin@sdu.edu.cn; liyb@sdu.edu.cn
FU National Key Research and Development Plan of China [2018YFB1305803];
   National Natural Science Foundation of China [61673245]; National
   Programs for High Technology Research and Development of China
   [2015AA042307]
FX This work was supported in part by the National Key Research and
   Development Plan of China under Grant 2018YFB1305803, in part by the
   National Natural Science Foundation of China under Grant 61673245, in
   part by the National Programs for High Technology Research, and in part
   by the Development of China under Grant 2015AA042307.
CR Akagündüz E, 2017, IEEE J BIOMED HEALTH, V21, P756, DOI 10.1109/JBHI.2016.2570300
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.331
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2018, COMPUT VIS IMAGE UND, DOI DOI 10.1016/J.CVIU.2017.10.011
   [Anonymous], 2005, PROC CVPR IEEE
   [Anonymous], 2018, MULTIMED TOOLS APPL, DOI DOI 10.1007/S11042-017-4825-4
   [Anonymous], 2014, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2014.108
   Aslan M, 2015, APPL SOFT COMPUT, V37, P1023, DOI 10.1016/j.asoc.2014.12.035
   Aydin R, 2014, IN C IND ENG ENG MAN, P1, DOI 10.1109/IEEM.2014.7058588
   Baradel F, 2018, PROC CVPR IEEE, P469, DOI 10.1109/CVPR.2018.00056
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chakraborty B, 2012, COMPUT VIS IMAGE UND, V116, P396, DOI 10.1016/j.cviu.2011.09.010
   Chen J., 2018, P C PATT REC COMP VI
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
   Evangelidis G, 2014, INT C PATT RECOG, P4513, DOI 10.1109/ICPR.2014.772
   Fernando B, 2017, IEEE T PATTERN ANAL, V39, P773, DOI 10.1109/TPAMI.2016.2558148
   Hou YH, 2018, IEEE ACCESS, V6, P2206, DOI 10.1109/ACCESS.2017.2782258
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Imran J, 2016, 2016 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P144, DOI 10.1109/ICACCI.2016.7732038
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Ke Q., 2018, PROC ASIAN C COMPUT, P729
   Ke QH, 2018, IEEE T IMAGE PROCESS, V27, P2842, DOI 10.1109/TIP.2018.2812099
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li WB, 2015, IEEE I CONF COMP VIS, P4444, DOI 10.1109/ICCV.2015.505
   Lin M., 2014, DES AUT CON
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P1453, DOI 10.1109/TPAMI.2019.2898954
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Luong T., 2015, P 2015 C EMP METH NA, DOI [DOI 10.18653/V1/D15-1166, 10.18653/v1/D15-1166]
   Ma X, 2014, IEEE J BIOMED HEALTH, V18, P1915, DOI 10.1109/JBHI.2014.2304357
   Mnih V, 2014, ADV NEUR IN, V27
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Sharma S., 2016, P INT C LEARN REPR W, P1
   Simonyan K., 2014, 14091556 ARXIV
   Simonyan K, 2014, ADV NEUR IN, V27
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Sun D., 2018, P PAC RIM C MULT HEF, P854
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang PC, 2018, COMPUT VIS IMAGE UND, V171, P118, DOI 10.1016/j.cviu.2018.04.007
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Wang PC, 2017, IEEE INT CONF COMP V, P1005, DOI 10.1109/ICCVW.2017.123
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yun K., 2012, P IEEE COMP SOC C CO, P28
   Zang JL, 2018, IFIP ADV INF COMM TE, V519, P97, DOI 10.1007/978-3-319-92007-8_9
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
NR 62
TC 17
Z9 17
U1 5
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2293
EP 2306
DI 10.1109/TMM.2019.2953814
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200007
DA 2024-07-18
ER

PT J
AU Yan, HL
   Li, ZT
   Wang, QL
   Li, PH
   Xu, Y
   Zuo, WM
AF Yan, Hongliang
   Li, Zhetao
   Wang, Qilong
   Li, Peihua
   Xu, Yong
   Zuo, Wangmeng
TI Weighted and Class-Specific Maximum Mean Discrepancy for Unsupervised
   Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Adaptation models; Airplanes; Gallium nitride; Task
   analysis; Generative adversarial networks; Degradation; Image
   recognition; unsupervised domain adaption; convolutional neural network;
   expectation-maximization algorithms
ID KERNEL; TEXT
AB Although maximum mean discrepancy (MMD) has achieved great success in unsupervised domain adaptation (UDA), most of existing UDA methods ignore the issue of class weight bias across domains, which is ubiquitous and evidently gives rise to the degradation of UDA performance. In this work, we propose two improved MMD metrics, i.e., weighted MMD (WMMD) and class-specific MMD (CMMD), to alleviate the adverse effect caused by the changes of class prior distributions between source and target domains. In WMMD, class-specific auxiliary weights are deployed to reweigh the source samples. In CMMD, we calculate the MMD for each class of source and target samples. Since the class labels of target samples are unknown for UDA problem, we present a classification expectation-maximization algorithm to estimate the pseudo-labels of target samples on the fly and update the model parameters using estimated labels. The proposed methods can be flexibly incorporated into deep convolutional neural networks to form WMMD and CMMD based domain adaptation networks, which we called WDAN and CDAN, respectively. By combining WMMD with CMMD, we present a CWMMD based domain adaptation network (CWDAN) to further improve classification performance. Experiments show that, both WMMD and CMMD benefit the classification accuracy, and our CWDAN can achieve compelling UDA performance in comparison with MMD and the state-of-the-art UDA methods.
C1 [Yan, Hongliang; Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Li, Zhetao] Xiangtan Univ, Key Lab Hunan Prov Internet Things & Informat Sec, Xiangtan 411105, Hunan, Peoples R China.
   [Li, Zhetao] Xiangtan Univ, Coll Informat Engn, Xiangtan 411105, Hunan, Peoples R China.
   [Wang, Qilong] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Li, Peihua] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
   [Xu, Yong] Harbin Inst Technol, Biocomp Res Ctr, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Xiangtan University; Xiangtan
   University; Tianjin University; Dalian University of Technology; Harbin
   Institute of Technology
RP Zuo, WM (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
EM yhldhit@gmail.com; liztchina@gmail.com; qlwang@tju.edu.cn;
   peihuali@dlut.edu.cn; yongxu@ymail.com; cswmzuo@gmail.com
RI Zuo, Wangmeng/B-3701-2008; Li, Zhetao/H-1293-2017; Wang,
   qilong/ITV-3230-2023
OI Zuo, Wangmeng/0000-0002-3330-783X; yan, hongliang/0000-0001-9125-5671
FU National Natural Scientific Foundation of China (NSFC) [61671182,
   61806140, 61971086]; National Key R&D Program of China [2017YFC0113000,
   2018YFB1003702]; Hunan Provincial Natural Science Foundation of China
   for Distinguished Young Scholars [2018JJ1025]; ShenzhenMunicipal Science
   and Technology Innovation Council [GJHZ20180419190732022]
FX This work was supported in part by theNational Natural Scientific
   Foundation of China (NSFC) under Grants 61671182, 61806140, and
   61971086, in part by the National Key R&D Program of China under Grants
   2017YFC0113000 and 2018YFB1003702, in part by the Hunan Provincial
   Natural Science Foundation of China for Distinguished Young Scholars
   under Grant 2018JJ1025, and in part by ShenzhenMunicipal Science and
   Technology Innovation Council under Grant GJHZ20180419190732022. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Mohammed Daoudi.
CR [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.18
   [Anonymous], 2013, INT CONF COMPUTAT
   [Anonymous], 2018, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2018.2791803
   [Anonymous], 2002, FR ART INT
   Baktashmotlagh M, 2016, J MACH LEARN RES, V17
   Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100
   Bergamo L. T. A., 2010, P ADV NEURAL INF DEC
   Bousmalis K., 2016, P ADV NEUR INF PROC, P343
   Bousmalis K, 2018, IEEE INT CONF ROBOT, P4243
   Cao ZJ, 2018, PROC CVPR IEEE, P2724, DOI 10.1109/CVPR.2018.00288
   Caputo B., 2014, PROC CLEF, P192
   CELEUX G, 1992, COMPUT STAT DATA AN, V14, P315, DOI 10.1016/0167-9473(92)90042-E
   Chen QC, 2018, PROC CVPR IEEE, P7976, DOI 10.1109/CVPR.2018.00832
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4026
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gretton A., 2006, P ADV NEURAL INF PRO, P513
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Griffin G., 2007, CNSTR2007001 CALTECH
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu TMH, 2015, IEEE I CONF COMP VIS, P4121, DOI 10.1109/ICCV.2015.469
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang J., 2007, P M ASS COMP LING, P264
   Jing PG, 2017, IEEE T MULTIMEDIA, V19, P1050, DOI 10.1109/TMM.2016.2644866
   Kouw WM, 2016, J MACH LEARN RES, V17
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Li YH, 2018, PATTERN RECOGN, V80, P109, DOI 10.1016/j.patcog.2018.03.005
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long M., 2016, Advances in neural information processing systems, V29
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Mansour Y., 2009, P ADV NEURAL INF PRO, P1041
   Meagher M, 2007, IEEE INT CONF INF VI, P601
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Rozantsev A, 2019, IEEE T PATTERN ANAL, V41, P801, DOI 10.1109/TPAMI.2018.2814042
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Simonyan K., 2014, PREPRINT
   Sogaard A., 2013, SYNTHESIS LECT HUMAN, V6, P1
   Sohn K., 2017, P IEEE C COMP VIS PA, P3210
   Sohn Kihyuk, 2019, P INT C LEARN REPR
   Song H, 2018, IEEE T MULTIMEDIA, V20, P1088, DOI 10.1109/TMM.2017.2763322
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2017, P INT C LEARN REPR
   Tobin J, 2017, IEEE INT C INT ROBOT, P23
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Wu S, 2018, IEEE T MULTIMEDIA, V20, P851, DOI 10.1109/TMM.2017.2758522
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Yang BY, 2018, PATTERN RECOGN, V81, P615, DOI 10.1016/j.patcog.2018.04.027
   Yang E, 2018, IEEE T NEUR NET LEAR, V29, P5292, DOI 10.1109/TNNLS.2018.2793863
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuo JB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P261, DOI 10.1145/3123266.3123292
NR 80
TC 53
Z9 54
U1 11
U2 52
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2420
EP 2433
DI 10.1109/TMM.2019.2953375
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200016
DA 2024-07-18
ER

PT J
AU Zha, ZJ
   Liu, JW
   Chen, D
   Wu, F
AF Zha, Zheng-Jun
   Liu, Jiawei
   Chen, Di
   Wu, Feng
TI Adversarial Attribute-Text Embedding for Person Search With Natural
   Language Query
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Natural languages; Feature extraction; Cameras;
   Semantics; Task analysis; Robustness; Person search; natural language;
   adversarial learning; visual attributes; graph convolution network
ID REIDENTIFICATION
AB The newly emerging task of person search with natural language query aims at retrieving the target pedestrian by a text description of the pedestrian. It is more applicable compared to person search with image/video query, i.e., person re-identification. In this paper, we propose a novel Adversarial Attribute-Text Embedding (AATE) network for person search with text query. In particular, a cross-modal adversarial learning module is proposed to learn discriminative and modality-invariant visual-textual features. It consists of a cross-modal learner and a modality discriminator, playing a min-max game in an adversarial learning way. The former is to improve intra-modality discrimination and inter-modality invariance towards confusing the modality discriminator. The latter is to distinguish the features from different modalities and boost the learning of modality-invariant features. Moreover, a visual attribute graph convolutional network is proposed to learn visual attributes of pedestrians, which possess better descriptiveness, interpretability and robustness compared to pedestrian appearance features. A hierarchical text embedding network, consisting of multi-stacked bidirectional LSTMs and a textual attention block, is developed to extract effective textual features from text descriptions of pedestrians. Extensive experimental results on two challenging benchmarks, have demonstrated the effectiveness of the proposed approach.
C1 [Zha, Zheng-Jun; Liu, Jiawei; Chen, Di; Wu, Feng] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zha, ZJ (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
EM zhazj@ustc.edu.cn; ljw368@mail.ustc.edu.cn; cdrom000@mail.ustc.edu.cn;
   fengwu@ustc.edu.cn
RI Wu, Feng/KCY-3017-2024; Zha, Zheng-Jun/AAF-8667-2020
FU National Key RAMP;D Program of China [2017YFB1300201]; National Natural
   Science Foundation of China (NSFC) [61622211, U19B2038, 61620106009];
   Fundamental Research Funds for the Central Universities [WK2100100030]
FX This work was supported in part by the National Key R&D Program of China
   underGrant 2017YFB1300201, in part by the National Natural Science
   Foundation of China (NSFC) under Grants 61622211, U19B2038 and
   61620106009, and in part by the Fundamental Research Funds for the
   Central Universities under Grant WK2100100030.
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   [Anonymous], 2017, ICCV
   [Anonymous], 2019, P 2019 IEEE C COMP V
   [Anonymous], 2017, P CVPR
   [Anonymous], P ACM INT C MULT
   [Anonymous], IEEE T CIRC SYST VID
   [Anonymous], 2017, IEEE DECIS CONTR P
   [Anonymous], 2020, PROC 14 EUR C ANTENN
   [Anonymous], 2017, P CVPR
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], P HUM LANG TECHN C N
   Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128
   Chen L, 2018, 2018 11TH INTERNATIONAL WORKSHOP ON HUMAN FRIENDLY ROBOTICS (HFR), P54, DOI 10.1109/HFR.2018.8633487
   Chen TL, 2018, IEEE WINT CONF APPL, P1879, DOI 10.1109/WACV.2018.00208
   Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756
   Chung D, 2017, IEEE I CONF COMP VIS, P1992, DOI 10.1109/ICCV.2017.218
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Dorfer M, 2018, INT J MULTIMED INF R, V7, P117, DOI 10.1007/s13735-018-0151-5
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He L, 2017, IEEE INT CON MULTI, P1153, DOI 10.1109/ICME.2017.8019549
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kviatkovsky Igor, 2013, IEEE Trans Pattern Anal Mach Intell, V35, P1622, DOI 10.1109/TPAMI.2012.246
   Lee KF, 2018, 2018 IEEE PHOTONICS SOCIETY SUMMER TOPICAL MEETING SERIES (SUM), P201, DOI 10.1109/PHOSST.2018.8456773
   Li SW, 2017, COMPLEXITY, DOI 10.1155/2017/9781890
   Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046
   Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194
   Liu J, 2019, AUTOPHAGY, V15, P2033, DOI 10.1080/15548627.2019.1659623
   Liu JW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P665, DOI 10.1145/3343031.3350991
   Liu JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P737, DOI 10.1145/3240508.3240585
   Liu Jiawei, 2016, P 24 ACM INT C MULT, P192
   Liu ZM, 2017, IEEE I CONF COMP VIS, P2448, DOI 10.1109/ICCV.2017.266
   Ma BP, 2012, LECT NOTES COMPUT SC, V7583, P413, DOI 10.1007/978-3-642-33863-2_41
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Meng Z., 2018, P EUROPEAN C COMPUTE, P552
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Ouyang ZC, 2017, IEEE INT SYMP PARAL, P1107, DOI 10.1109/ISPA/IUCC.2017.00167
   Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426
   Prosser B., 2010, PERSON RE IDENTIFICA, V2, P1
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Shen Y, 2018, P EUR C COMP VIS ECC, P486
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Vaswani A., 2017, P ADV NEUR INF PROC, V2017, P5998
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang TQ, 2016, IEEE T PATTERN ANAL, V38, P2501, DOI 10.1109/TPAMI.2016.2522418
   Wang YC, 2018, PROC CVPR IEEE, P1470, DOI 10.1109/CVPR.2018.00159
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yamaguchi M, 2017, IEEE I CONF COMP VIS, P1462, DOI 10.1109/ICCV.2017.162
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   You JJ, 2016, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2016.150
   Zhang HW, 2014, IEEE T IMAGE PROCESS, V23, P2996, DOI 10.1109/TIP.2014.2325784
   Zhang Y., 2018, P EUR C COMP VIS ECC, P686
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zhou TT, 2019, INT GEOSCI REMOTE SE, P202, DOI [10.1109/IGARSS.2019.8900438, 10.1109/igarss.2019.8900438]
   Zhou Z., 2017, P IEEE C COMP VIS PA, P4747
   Zhu XK, 2018, IEEE T IMAGE PROCESS, V27, P5683, DOI 10.1109/TIP.2018.2861366
NR 72
TC 35
Z9 38
U1 3
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1836
EP 1846
DI 10.1109/TMM.2020.2972168
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500015
DA 2024-07-18
ER

PT J
AU Zhang, WQ
   Tang, SL
   Cao, YP
   Pu, SL
   Wu, F
   Zhuang, YT
AF Zhang, Wenqiao
   Tang, Siliang
   Cao, Yanpeng
   Pu, Shiliang
   Wu, Fei
   Zhuang, Yueting
TI Frame Augmented Alternating Attention Network for Video Question
   Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Knowledge discovery; Task analysis;
   Data mining; Neural networks; Semantics; Video QA; alternating
   attention; augmented features; neural network
AB Vision and language understanding is one of the most fundamental and challenging problems in Multimedia Intelligence. Simultaneously understanding video actions with a related natural language question, and further produces accurate answer is even more challenging since it requires joint modeling information across modality. In the past few years, some studies begin to attack this problem by utilizing attention enhanced deep neural networks. However, simple attention mechanisms such as unidirectional attention fail to yield a better mapping between different modalities. Moreover, none of these Video QA models explore high-level semantics in augmented video-frame level. In this paper, we augmented each frame representation with its context information by a novel feature extractor that combines the advantages of Resnet and a variant of C3D. In addition, we proposed a novel alternating attention network which can alternately attend frame regions, video frames and words in the question in multi-turns. This yields better joint representations of video and question, further help the deep model to discover the deeper relationship between two modalities. Our method outperforms the state-of-the-art Video QA models on two existing video question answering datasets. Further ablation studies proved that our feature extractor and the alternating attention mechanism can improve the performance jointly.
C1 [Zhang, Wenqiao; Tang, Siliang; Wu, Fei; Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Cao, Yanpeng] Zhejiang Univ, Sch Mech Engn, Hangzhou 310027, Peoples R China.
   [Pu, Shiliang] Hikvis Res Inst, Hangzhou 310012, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Tang, SL (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.; Cao, YP (corresponding author), Zhejiang Univ, Sch Mech Engn, Hangzhou 310027, Peoples R China.
EM wenqiaozhang@zju.edu.cn; siliang@zju.edu.cn; caoyp@zju.edu.cn;
   pushiliang@hikvision.com; wufei@zju.edu.cn; yzhuang@cs.zju.edu.cn
RI zhang, yimeng/JLL-7337-2023
OI Tang, Siliang/0000-0002-7356-9711
FU National Key Research and Development Program of China
   [SQ2018AAA010010]; National Natural Science Foundation of China
   [61751209, U1611461, 51605428]; Hikvision-Zhejiang University Joint
   Research Center; Zhejiang University-Tongdun Technology Joint Laboratory
   of Artificial Intelligence; Chinese Knowledge Center of Engineering
   Science and Technology (CKCEST); Engineering Research Center of Digital
   Library; Ministry of Education; Zhejiang University iFLYTEK Joint
   Research Center
FX This work was supported in part by National Key Research and Development
   Program of China under Grant SQ2018AAA010010, in part by National
   Natural Science Foundation of China under Grants 61751209, U1611461, and
   51605428, in part by Hikvision-Zhejiang University Joint Research
   Center, in part by Zhejiang University-Tongdun Technology Joint
   Laboratory of Artificial Intelligence, in part by Zhejiang University
   iFLYTEK Joint Research Center, in part by Chinese Knowledge Center of
   Engineering Science and Technology (CKCEST), in part by Engineering
   Research Center of Digital Library, and in part byMinistry of Education.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2018, IEEE T NEUR NET LEAR, DOI DOI 10.1109/TNNLS.2018.2817340
   [Anonymous], 2017, P IEEE INT C COMP VI
   [Anonymous], 2009, TECH REP
   [Anonymous], ARXIV160508140
   [Anonymous], P INT JOINT C ART IN
   [Anonymous], 2017, ARXIV170301515
   [Anonymous], 2012, P 13 ANN C INT SPEEC
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Carlsson N, 2017, IEEE T MULTIMEDIA, V19, P1637, DOI 10.1109/TMM.2017.2673412
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Han TT, 2017, IEEE T MULTIMEDIA, V19, P712, DOI 10.1109/TMM.2016.2631881
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Liang JW, 2018, PROC CVPR IEEE, P6135, DOI 10.1109/CVPR.2018.00642
   Liu JW, 2017, IEEE T SERV COMPUT, V10, P286, DOI 10.1109/TSC.2015.2446991
   Lu JS, 2016, ADV NEUR IN, V29
   Lu P, 2017, ARXIV PREPRINT ARXIV
   Nie LQ, 2013, IEEE T MULTIMEDIA, V15, P426, DOI 10.1109/TMM.2012.2229971
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Ren M., 2015, NEURIPS, P2953
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sukhbaatar S, 2015, ADV NEURAL INFORM PR, V28, P2440
   Wang M, 2017, IEEE T KNOWL DATA EN, V29, P1101, DOI 10.1109/TKDE.2017.2654445
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Ye YN, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P829, DOI 10.1145/3077136.3080655
   Yu Zhou, 2019, P AAAI C ART INT
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhao Z, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1050, DOI 10.1145/3123266.3123364
   Zhu WW, 2015, IEEE MULTIMEDIA, V22, P96, DOI 10.1109/MMUL.2015.66
NR 41
TC 30
Z9 33
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 1032
EP 1041
DI 10.1109/TMM.2019.2935678
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400017
DA 2024-07-18
ER

PT J
AU Shi, C
   Pun, CM
AF Shi, Cheng
   Pun, Chi-Man
TI Multiscale Superpixel-Based Hyperspectral Image Classification Using
   Recurrent Neural Networks With Stacked Autoencoders
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Recurrent neural networks; Correlation; Feature extraction; Neurons;
   Hyperspectral imaging; Principal component analysis; Kernel;
   Hyperspectral image classification; Local and nonlocal similarities;
   Recurrent neural networks; Stacked autoencoders
ID SPECTRAL-SPATIAL CLASSIFICATION; SPARSE REPRESENTATION; ALGORITHM
AB This paper develops a novel hyperspectral image (HSI) classification framework by exploiting the spectral-spatial features of multiscale superpixels via recurrent neural networks with stacked autoencoders. The superpixels can be used to segment an HSI into shape-adaptive regions, and multiscale superpixels can capture the object information more accurately. Therefore, the superpixel-based classification methods have been studied by many researchers. In this paper, we propose a multiscale superpixel-based classification method. In contrast to current research, the proposed method not only captures the features of each scale but also considers the correlation among different scales via recurrent neural networks. In this way, the spectral-spatial information within a superpixel is more efficiently exploited. In this paper, we first segment the HSI from coarse to fine scales using the superpixels. Then, the spatial features within each superpixel and among superpixels are sufficiently exploited by the local and nonlocal similarity measure. Finally, recurrent neural networks with stacked autoencoders are proposed to learn the high-level multiscale spectral-spatial features. Experiments are conducted on real HSI datasets. The results demonstrate the superiority of the proposed method over several well-known methods in both visual appearance and classification accuracy.
C1 [Shi, Cheng; Pun, Chi-Man] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 University of Macau
RP Pun, CM (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
EM chengc_s@163.com; cmpun@umac.mo
RI Pun, Chi Man/GRJ-3703-2022
OI Pun, Chi-Man/0000-0003-1788-3746
FU Research Committee of the University of Macau [MYRG2015-00011-FST,
   MYRG2018-00035-FST]; Science and Technology Development Fund of Macau
   SAR [041/2017/A1, 093-2014-A2]
FX This work was supported in part by the Research Committee of the
   University of Macau under Grant MYRG2015-00011-FST and Grant
   MYRG2018-00035-FST and in part by the Science and Technology Development
   Fund of Macau SAR under Grant 041/2017/A1 and Grant 093-2014-A2. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Xilin Chen.
CR Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Benediktsson JA, 2005, IEEE T GEOSCI REMOTE, V43, P480, DOI 10.1109/TGRS.2004.842478
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Bernard K, 2012, IEEE T IMAGE PROCESS, V21, P2008, DOI 10.1109/TIP.2011.2175741
   Camps-Valls G, 2006, IEEE GEOSCI REMOTE S, V3, P93, DOI 10.1109/LGRS.2005.857031
   Chen HG, 2016, SIGNAL PROCESS-IMAGE, V43, P68, DOI 10.1016/j.image.2016.01.007
   Chen HT, 2005, PROC CVPR IEEE, P846
   Chen YS, 2016, IEEE T GEOSCI REMOTE, V54, P6232, DOI 10.1109/TGRS.2016.2584107
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Fang LY, 2015, IEEE T GEOSCI REMOTE, V53, P6663, DOI 10.1109/TGRS.2015.2445767
   Fang LY, 2015, IEEE T GEOSCI REMOTE, V53, P4186, DOI 10.1109/TGRS.2015.2392755
   Fu W, 2015, INT GEOSCI REMOTE SE, P4971, DOI 10.1109/IGARSS.2015.7326948
   Guo YH, 2018, PROCEDIA COMPUT SCI, V129, P219, DOI 10.1016/j.procs.2018.03.048
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lei Y, 2017, IEEE T MULTIMEDIA, V19, P740, DOI 10.1109/TMM.2016.2638204
   Li JY, 2015, IEEE T GEOSCI REMOTE, V53, P5338, DOI 10.1109/TGRS.2015.2421638
   Li J, 2013, IEEE T GEOSCI REMOTE, V51, P4816, DOI 10.1109/TGRS.2012.2230268
   Li SS, 2016, INT J REMOTE SENS, V37, P4905, DOI 10.1080/01431161.2016.1225175
   Li ST, 2016, IEEE T GEOSCI REMOTE, V54, P7416, DOI 10.1109/TGRS.2016.2603190
   Li W, 2017, IEEE T GEOSCI REMOTE, V55, P844, DOI 10.1109/TGRS.2016.2616355
   Li W, 2012, IEEE T GEOSCI REMOTE, V50, P1185, DOI 10.1109/TGRS.2011.2165957
   Li YS, 2017, PATTERN RECOGN, V63, P371, DOI 10.1016/j.patcog.2016.10.019
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liu JJ, 2013, IEEE J-STARS, V6, P2462, DOI 10.1109/JSTARS.2013.2252150
   Ma XR, 2016, IEEE J-STARS, V9, P4073, DOI 10.1109/JSTARS.2016.2517204
   Makantasis K, 2015, INT GEOSCI REMOTE SE, P4959, DOI 10.1109/IGARSS.2015.7326945
   Marpu PR, 2012, INT J IMAGE DATA FUS, V3, P269, DOI 10.1080/19479832.2012.702687
   Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323
   Mou LC, 2017, IEEE T GEOSCI REMOTE, V55, P3639, DOI 10.1109/TGRS.2016.2636241
   Nogueira K, 2017, PATTERN RECOGN, V61, P539, DOI 10.1016/j.patcog.2016.07.001
   Prasad S, 2008, IEEE GEOSCI REMOTE S, V5, P625, DOI 10.1109/LGRS.2008.2001282
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Rodarmel C., 2002, Surveying and Land Information Science, V62, P115, DOI DOI 10.1109/IGARSS.2001.976068
   Salakhutdinov R., 2009, AISTATS
   Shi C, 2018, NEUROCOMPUTING, V294, P82, DOI 10.1016/j.neucom.2018.03.012
   Sugiyama M, 2007, J MACH LEARN RES, V8, P1027
   Tarabalka Y, 2010, PATTERN RECOGN, V43, P2367, DOI 10.1016/j.patcog.2010.01.016
   Tarabalka Y, 2010, INT GEOSCI REMOTE SE, P1410, DOI 10.1109/IGARSS.2010.5649222
   Tarabalka Y, 2010, IEEE GEOSCI REMOTE S, V7, P736, DOI 10.1109/LGRS.2010.2047711
   Villa A, 2011, IEEE T GEOSCI REMOTE, V49, P4865, DOI 10.1109/TGRS.2011.2153861
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655
   Wang Q, 2016, FRONT PLANT SCI, V07, P1
   Xanthopoulos P., 2013, LINEAR DISCRIMINANT, P237
   Yue J, 2015, REMOTE SENS LETT, V6, P468, DOI 10.1080/2150704X.2015.1047045
   Zhang XR, 2017, IEEE GEOSCI REMOTE S, V14, P1928, DOI 10.1109/LGRS.2017.2737823
   Zhang XY, 2016, REMOTE SENS ENVIRON, V178, P172, DOI 10.1016/j.rse.2016.03.015
   Zhao WZ, 2017, ISPRS J PHOTOGRAMM, V132, P48, DOI 10.1016/j.isprsjprs.2017.08.011
   Zhao WZ, 2017, IEEE J-STARS, V10, P3386, DOI 10.1109/JSTARS.2017.2680324
   Zhao WZ, 2016, IEEE T GEOSCI REMOTE, V54, P4544, DOI 10.1109/TGRS.2016.2543748
   Zhao WZ, 2016, ISPRS J PHOTOGRAMM, V113, P155, DOI 10.1016/j.isprsjprs.2016.01.004
   Zhao WZ, 2015, INT J REMOTE SENS, V36, P3368, DOI 10.1080/2150704X.2015.1062157
   Zhou YC, 2016, IEEE T CYBERNETICS, V46, P1667, DOI 10.1109/TCYB.2015.2453359
NR 55
TC 56
Z9 57
U1 4
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 487
EP 501
DI 10.1109/TMM.2019.2928491
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300016
DA 2024-07-18
ER

PT J
AU Liu, LX
   Wang, TS
   Huang, H
AF Liu, Lixiong
   Wang, Tianshu
   Huang, Hua
TI Pre-Attention and Spatial Dependency Driven No-Reference Image Quality
   Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pre-attention; no-reference (NR); image quality assessment (IQA);
   spatial dependency; chromatic data
ID FREE-ENERGY PRINCIPLE; PERCEPTUAL IMAGE; COLOR; STATISTICS; ATTENTION
AB The excessive emulation of the human visual system and the lack of connection between chromatic data and distortion have been the major bottlenecks in developing image quality assessment. To address this issue, we develop a new no-reference (NR) image quality assessment (IQA) metric that accounts for the impact of pre-attention and spatial dependency on the perceived quality of distorted images. The resulting model, dubbed the Pre-attention and Spatial-dependency driven Quality Assessment (PSQA) predictor, introduces the pre-attention theory to emulate early phase visual perception by refining luminance-channel data. Chromatic data are also processed concurrently by transforming images from RGB to the perceptually optimized SCIELAB color space. Considering that the gray-tone spatial dependency matrix conveys important texture properties that are closely related to visual quality, this matrix, as a mathematical solution for subsequent visual process emulation, is calculated along with its statistical features on both gray and color channels. To clarify the influence of different regression procedures on model output, support vector regression and AdaBoosting Back Propagation (BP) neural networks are adopted separately to train the prediction models. We thoroughly evaluated PSQA on four public image quality databases: LIVE, TID2013, CSIQ, and VCL. The experimental results show that PSQA delivers highly competitive performance compared with top-rank NR and full-reference IQA metrics.
C1 [Liu, Lixiong; Wang, Tianshu; Huang, Hua] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Huang, H (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM lxliu@bit.edu.cn; wangts@bit.edu.cn; huahuang@bit.edu.cn
RI Huang, Hua/M-9684-2013
OI Huang, Hua/0000-0003-2587-1702
FU National Natural Science Foundation of China [61672095, 61425013]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61672095 and 61425013. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Hantao Liu.
CR [Anonymous], 1955, PRINCIPLES GESTALT P
   Barghout-Stein L., 2003, GLOBAL PERCEPTUAL CO
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Bovik AC, 2013, P IEEE, V101, P2008, DOI 10.1109/JPROC.2013.2257632
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Chang HW, 2015, NEUROCOMPUTING, V151, P1142, DOI 10.1016/j.neucom.2014.04.081
   Corrales J. A., 2006, IPCV, P114
   Ding L, 2017, IEEE T IMAGE PROCESS, V26, P1799, DOI 10.1109/TIP.2017.2665972
   Fine I, 2003, J OPT SOC AM A, V20, P1283, DOI 10.1364/JOSAA.20.001283
   Gao XB, 2013, IEEE T NEUR NET LEAR, V24, P2013, DOI 10.1109/TNNLS.2013.2271356
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, IEEE T BROADCAST, V62, P446, DOI 10.1109/TBC.2015.2511624
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Han Z, 2017, IEEE T IMAGE PROCESS, V26, P1031, DOI 10.1109/TIP.2016.2642788
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Joshi P, 2017, MULTIMED TOOLS APPL, V76, P18871, DOI 10.1007/s11042-017-4418-2
   Krüger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lee D, 2016, IEEE T IMAGE PROCESS, V25, P3875, DOI 10.1109/TIP.2016.2579308
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Lindeberg T., 1994, Journal of AppliedStatistics, V21, P225
   Lindeberg T, 2013, BIOL CYBERN, V107, P589, DOI 10.1007/s00422-013-0569-z
   Liu DL, 2016, IEEE ACCESS, V4, P4478, DOI 10.1109/ACCESS.2016.2598289
   Liu HT, 2016, IEEE T IMAGE PROCESS, V25, P3087, DOI 10.1109/TIP.2016.2561406
   Liu HT, 2011, IEEE T CIRC SYST VID, V21, P971, DOI 10.1109/TCSVT.2011.2133770
   Liu HT, 2010, IEEE T CIRC SYST VID, V20, P529, DOI 10.1109/TCSVT.2009.2035848
   Liu LX, 2017, SIGNAL PROCESS-IMAGE, V58, P287, DOI 10.1016/j.image.2017.08.011
   Liu LX, 2016, SIGNAL PROCESS-IMAGE, V40, P1, DOI 10.1016/j.image.2015.10.005
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Liu ZG, 2017, IET IMAGE PROCESS, V11, P854, DOI 10.1049/iet-ipr.2016.1053
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Oliva Aude, 2005, P251, DOI 10.1016/B978-012375731-9/50045-8
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Omari M, 2015, MULTIMED TOOLS APPL, V74, P8685, DOI 10.1007/s11042-014-2353-z
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   POTTER MC, 1976, J EXP PSYCHOL-HUM L, V2, P509, DOI 10.1037/0278-7393.2.5.509
   Roelfsema PR, 1998, NATURE, V395, P376, DOI 10.1038/26475
   Saha A, 2015, IEEE T IMAGE PROCESS, V24, P1879, DOI 10.1109/TIP.2015.2411436
   Sazzad ZMP, 2008, SIGNAL PROCESS-IMAGE, V23, P257, DOI 10.1016/j.image.2008.03.005
   Sheikh H.R., 2006, LIVE image quality assessment database release 2
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Simone F. D., 2009, P 4 INT WORKSH VID P
   Temel D, 2016, SIGNAL PROCESS-IMAGE, V48, P92, DOI 10.1016/j.image.2016.08.008
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Wang Q, 2016, NEUROCOMPUTING, V173, P1798, DOI 10.1016/j.neucom.2015.09.057
   Wang SQ, 2015, IEEE SIGNAL PROC LET, V22, P2387, DOI 10.1109/LSP.2015.2487369
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wolfe J., 1998, ATTENTION
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   Ye P, 2012, IEEE T IMAGE PROCESS, V21, P3129, DOI 10.1109/TIP.2012.2190086
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zaric A, 2012, AUTOMATIKA, V53, P344, DOI 10.7305/automatika.53-4.241
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang W, 2018, IEEE T CIRC SYST VID, V28, P1462, DOI 10.1109/TCSVT.2017.2650910
   Zhang W, 2017, NEUROCOMPUTING, V247, P183, DOI 10.1016/j.neucom.2017.03.054
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P2424, DOI 10.1109/TIP.2017.2681424
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P1275, DOI 10.1109/TIP.2017.2651410
   Zhang X., 1997, Journal of the Society for Information Display, V5, P61, DOI 10.1889/1.1985127
NR 71
TC 26
Z9 26
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2305
EP 2318
DI 10.1109/TMM.2019.2900941
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200012
DA 2024-07-18
ER

PT J
AU Ma, XH
   Zhang, TZ
   Xu, CS
AF Ma, Xinhong
   Zhang, Tianzhu
   Xu, Changsheng
TI Deep Multi-Modality Adversarial Networks for Unsupervised Domain
   Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Unsupervised domain adaptation; triplet loss; stacked attention;
   multi-modality; social event recognition
ID REPRESENTATION; KERNEL; SPARSE
AB Unsupervised domain adaptation aims to transfer domain knowledge from existing well-defined tasks to new ones where labels are unavailable. In the real-world applications, domain discrepancy is usually uncontrollable especially for multi-modality data. Therefore, it is significantly motivated to deal with a multi-modality domain adaptation task. As labels are unavailable in a target domain, how to learn semantic multi-modality representations and successfully adapt the classifier from a source to the target domain remain open challenges in a multi-modality domain adaptation task. To deal with these issues, we propose a multi-modality adversarial network (MMAN), which applies stacked attention to learn semantic multi-modality representations and reduces domain discrepancy via adversarial training. Unlike the previous domain adaptation methods, which cannot make full use of source domain categories information, multi-channel constraint is employed to capture fine-grained categories of knowledge that could enhance the discrimination of target samples and boost target performance on single-modality and multi-modality domain adaptation problems. We apply the proposed MMAN to two applications including cross-domain object recognition and cross-domain social event recognition. The extensive experimental evaluations demonstrate the effectiveness of the proposed model for unsupervised domain adaptation.
C1 [Ma, Xinhong; Zhang, Tianzhu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Ma, Xinhong; Zhang, Tianzhu; Xu, Changsheng] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Ma, Xinhong; Zhang, Tianzhu; Xu, Changsheng] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
EM xinhong.ma@nlpr.ia.ac.cn; tzzhang10@gmail.com; csxu@nlpr.ia.ac.cn
RI Xinhong, Ma/ACH-5160-2022; Zhang, Tianzhu/AGY-9389-2022; xu,
   cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022
OI Zhang, Tianzhu/0000-0003-0764-6106; Ma, Xinhong/0000-0003-1200-2268; xu,
   chang sheng/0000-0001-8343-9665; zhang, tian zhu/0000-0003-1856-9564
FU National Natural Science Foundation of China [61432019, 61572498,
   61532009, 61728210, 61721004, 61751211, 61772244, 61472379, 61720106006,
   U1705262]; Key Research Program of Frontier Sciences, Chinese Academy of
   Sciences [QYZDJ-SSW-JSC039]; Beijing Natural Science Foundation
   [4172062]; Youth Innovation Promotion Association Chinese Academy of
   Sciences [2018166]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61432019, 61572498, 61532009, 61728210,
   61721004, 61751211, 61772244, 61472379, 61720106006, and U1705262, in
   part by the Key Research Program of Frontier Sciences, Chinese Academy
   of Sciences under Grant QYZDJ-SSW-JSC039, in part by the Beijing Natural
   Science Foundation under Grant 4172062, and in part by the Youth
   Innovation Promotion Association Chinese Academy of Sciences (2018166).
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Abdulmotaleb El Saddik.
CR Agrawal A, 2017, INT J COMPUT VISION, V123, P4, DOI 10.1007/s11263-016-0966-6
   [Anonymous], 2011, P ICML
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2009, PROC 22 ANN C NEURAL
   [Anonymous], 2014, ABS14123474 CORR
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 1986, CUCS32186
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2014, P ADV NEUR INF PROC
   [Anonymous], J MACHINE LEARNING R
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Cao Z., 2017, P IEEE C COMP VIS PA, P7291
   Csurka G, 2017, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-319-58347-1
   Daras P, 2012, IEEE T MULTIMEDIA, V14, P734, DOI 10.1109/TMM.2011.2181343
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gholami B, 2017, IEEE I CONF COMP VIS, P3601, DOI 10.1109/ICCV.2017.387
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gopalan R, 2014, IEEE T PATTERN ANAL, V36, P2288, DOI 10.1109/TPAMI.2013.249
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Gretton A, 2009, NEURAL INF PROCESS S, P131
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hoffman Judy, 2018, ICML, P1994
   Hou CA, 2016, IEEE T IMAGE PROCESS, V25, P5552, DOI 10.1109/TIP.2016.2609820
   Hu LQ, 2018, PROC CVPR IEEE, P1498, DOI 10.1109/CVPR.2018.00162
   Kazemi V., 2017, ARXIV170403162
   Kim T, 2017, PR MACH LEARN RES, V70
   King DB, 2015, ACS SYM SER, V1214, P1
   Leibe B., 2017, ARXIV170307737CS
   Li J., IEEE T CYBERN
   Liu Ming Yu, 2016, ADV NEURAL INF PROCE, P469
   Liu YC, 2018, PROC CVPR IEEE, P8867, DOI 10.1109/CVPR.2018.00924
   Long M., 2017, Proc Mach Learn Res, V70, P2208
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MH, 2015, 2015 2ND INTERNATIONAL CONFERENCE ON SOCIAL SCIENCE (ICSS 2015), P92
   Long MS, 2013, PROC CVPR IEEE, P407, DOI 10.1109/CVPR.2013.59
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Qian SS, 2018, IEEE T MULTIMEDIA, V20, P2733, DOI 10.1109/TMM.2018.2815785
   Qian SS, 2018, IEEE T MULTIMEDIA, V20, P2086, DOI 10.1109/TMM.2017.2785227
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Reuter Timo., 2013, P MEDIAEVAL 2013 MUL, P1
   Shen J, 2018, AAAI CONF ARTIF INTE, P4058
   Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548
   Simonyan K., 2014, 14091556 ARXIV
   Srivastava Neelam., 2012, The Postcolonial Gramsci, P1
   Sugiyama Masashi, 2008, Advances in Neural Information Processing Systems, P1433
   Sun B, 2016, IEEE IND ELEC, P2052, DOI 10.1109/IECON.2016.7793405
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   THEIL H, 1988, STAT PROBABIL LETT, V6, P137, DOI 10.1016/0167-7152(88)90107-1
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang YY, 2018, RAILWAY DEVELOPMENT, OPERATIONS, AND MAINTENANCE, P36
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Xie CC, 2013, INT CONF COMPUTAT, P222, DOI 10.1109/ICCPS.2013.6893597
   Xie SL, 2018, I C CONT AUTOMAT ROB, P419, DOI 10.1109/ICARCV.2018.8581163
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu K., 2015, COMPUTER SCI, P2048
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Zadrozny B., 2004, P 21 INT C MACH LEAR, P114, DOI [10.1145/1015330.1015425, DOI 10.1145/1015330.1015425]
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
NR 77
TC 56
Z9 57
U1 10
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2419
EP 2431
DI 10.1109/TMM.2019.2902100
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200021
DA 2024-07-18
ER

PT J
AU Wang, Z
   Jiang, JJ
   Yu, Y
   Satoh, S
AF Wang, Zheng
   Jiang, Junjun
   Yu, Yi
   Satoh, Shin'ichi
TI Incremental Re-Identification by Cross-Direction and Cross-Ranking
   Adaption
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Person re-identification; log; cross-direction; cross-ranking
ID PERSON REIDENTIFICATION; NETWORK
AB Person re-identification is widely applied in video surveillance and criminal investigation applications. To achieve better performance, an additional re-ranking step is often exploited. Related methods attempt to optimize the result according to every single query independently. However, in a practical scene, as the investigation process goes on, the other queries, in particular, the gradually accumulated logs, can be used to guide or regularize the current query. In this paper, we propose to optimize the result according to not only the current query itself but also the other queries and historical logs. We respectively investigate the cross-direction and the cross-ranking constraints among different queries. Based on the investigations, we propose a reciprocal optimization method to refine multiple ranking lists reciprocally. Experiments on the VIPeR, new-protocol CUHK03, and Market-1501 datasets confirm the effectiveness of our method. In particular, on the Market-1501 dataset, with full utilization of the other queries, the method achieves an accuracy rate of 94.66% at rank-1 and a very high mAP of 75.12%, and significantly outperforms the state-of-the-art methods.
C1 [Wang, Zheng; Yu, Yi; Satoh, Shin'ichi] Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.
   [Jiang, Junjun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Jiang, Junjun] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; Harbin Institute of Technology;
   Peng Cheng Laboratory
RP Jiang, JJ (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM wangz@nii.ac.jp; jiangjunjun@hit.edu.cn; yiyu@nii.ac.jp; satoh@nii.ac.jp
RI Jiang, Junjun/L-7087-2019; Wang, Zheng/ABC-6029-2020
OI Jiang, Junjun/0000-0002-5694-505X; Wang, Zheng/0000-0003-3846-9157
FU JST CREST [JPMJCR1686, 18F18378]; National Natural Science Foundation of
   China [61801335]; Microsoft Research Asia
FX This work was supported in part by JST CREST under Grant JPMJCR1686, in
   part by the Grant-in-Aid for JSPS Fellows under Grant 18F18378, in part
   by the National Natural Science Foundation of China under Grant
   61801335, and in part by Microsoft Research Asia. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Jianfei Cai.
CR Ali S., 2010, ACM Multimedia Conference (ACM MM), P895
   An L., 2013, PROC 7 INT C DISTRIB, P1
   [Anonymous], 2016, 2016 11 INT C ECOLOG, DOI DOI 10.1109/EVER.2016.7491875
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2016, ARXIV
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], P INT TECH M I NAVIG
   Arandjelovic R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.92
   Bai S, 2016, IEEE T IMAGE PROCESS, V25, P1056, DOI 10.1109/TIP.2016.2514498
   Carpineto C, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2071389.2071390
   García J, 2015, IEEE I CONF COMP VIS, P1305, DOI 10.1109/ICCV.2015.154
   Gray Douglas, 2007, P IEEE INT WORKSH PE, V3, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Hoi SCH, 2006, IEEE T KNOWL DATA EN, V18, P509, DOI 10.1109/TKDE.2006.1599389
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Leng QM, 2015, MULTIMED TOOLS APPL, V74, P6989, DOI 10.1007/s11042-014-1949-7
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li XF, 2017, ACTA POLYM SIN, P1609, DOI 10.11777/j.issn1000-3304.2017.17157
   Li Z, 2013, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2013.463
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62
   Matsukawa T, 2016, INT C PATT RECOG, P2428, DOI 10.1109/ICPR.2016.7900000
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Rashedi E, 2015, MULTIMED TOOLS APPL, V74, P3799, DOI 10.1007/s11042-013-1800-6
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Syed MA, 2016, IEEE IMAGE PROC, P784, DOI 10.1109/ICIP.2016.7532464
   Wang HH, 2018, EVID-BASED COMPL ALT, V2018, DOI 10.1155/2018/4983891
   Wang J, 2017, IEEE T CIRC SYST VID, V27, P513, DOI 10.1109/TCSVT.2016.2586851
   Wang XQ, 2007, INT J THERM SCI, V46, P1, DOI 10.1016/j.ijthermalsci.2006.06.010
   Wang Z, 2018, IEEE T CYBERNETICS, V48, P3006, DOI 10.1109/TCYB.2017.2755044
   Wang Z, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1267, DOI 10.1145/2733373.2806400
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wang ZH, 2014, METHOD ENZYMOL, V548, P1, DOI 10.1016/B978-0-12-397918-6.00001-X
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Ye M., 2018, IJCAI, P1092
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Ye Mang, 2017, IEEE INTCONF COMPUT, P5142
   Ye M, 2015, COMPUTING, CONTROL, INFORMATION AND EDUCATION ENGINEERING, P1005
   Yu R., 2017, DIVIDE FUSE RERANKIN
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhu FQ, 2018, MULTIMEDIA SYST, V24, P477, DOI 10.1007/s00530-017-0571-8
NR 55
TC 45
Z9 46
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2376
EP 2386
DI 10.1109/TMM.2019.2898753
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200017
DA 2024-07-18
ER

PT J
AU Xu, ML
   Su, H
   Li, YF
   Li, X
   Liao, J
   Niu, JW
   Lv, P
   Zhou, B
AF Xu, Mingliang
   Su, Hao
   Li, Yafei
   Li, Xi
   Liao, Jing
   Niu, Jianwei
   Lv, Pei
   Zhou, Bing
TI Stylized Aesthetic QR Code
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE QR code; style-oriented; visual aesthetics; robust
AB With the continued proliferation of smart mobile devices, the Quick Response (QR) code has become one of the most-used types of two-dimensional code in the world. Aiming at beautifying the visual-unpleasant appearance of QR codes, existing works have developed a series of techniques. However, these works still leave much to be desired, such as personalization, artistry, and robustness. To address these issues, in this paper, we propose a novel type of aesthetic QR codes, Stylized a EsthEtic (SEE) QR code, and a three-stage approach to automatically produce such robust style-oriented codes. Specifically, in the first stage, we propose a method to generate an optimized baseline aesthetic QR code, which reduces the visual contrast between the noise-like black/white modules and the blended image. In the second stage, to obtain an art style QR code, we tailor an appropriate neural style transformation network to endow the baseline aesthetic QR code with artistic elements. In the third stage, we design a module-based robustness-optimization mechanism to ensure the performance robust by balancing two competing terms: visual quality and readability. Extensive experiments demonstrate that the SEE QR code has high quality in terms of both visual appearance and robustness and also offers a greater variety of personalized choices to users.
C1 [Xu, Mingliang; Su, Hao; Li, Yafei; Lv, Pei; Zhou, Bing] Zhengzhou Univ, Zhengzhou 450066, Henan, Peoples R China.
   [Li, Xi] Zhejiang Univ, Hangzhou 310027, Zhejiang, Peoples R China.
   [Liao, Jing] Microsoft Res Lab Asia, Beijing 100080, Peoples R China.
   [Niu, Jianwei] Beihang Univ, Beijing 100083, Peoples R China.
C3 Zhengzhou University; Zhejiang University; Beihang University
RP Li, YF (corresponding author), Zhengzhou Univ, Zhengzhou 450066, Henan, Peoples R China.
EM iexumingliang@zzu.edu.cn; suhao@gs.zzu.edu.cn; ieyfli@zzu.edu.cn;
   xilizju@zju.edu.cn; jliao@microsoft.com; niujianwei@buaa.edu.cn;
   ielvpei@zzu.edu.cn; iebzhou@zzu.edu.cn
RI Li, Xi/L-1234-2013
OI Li, Xi/0000-0003-3023-1662; Li, Yafei/0000-0001-9651-6092; Niu,
   Jianwei/0000-0003-3946-5107; , Pei/0000-0002-2654-0561; LIAO,
   Jing/0000-0001-7014-5377
FU National Natural Science Foundation of China [61822701, 61602420,
   61672469, 61472370, 61772474, 61872324]; National Key R&D Program of
   China [2017YFC0804401]; China Postdoctoral Science Foundation
   [2018M630836]; Supporting Plan for Scientific and Technological
   Innovative Talents in Universities of Henan Province [18HASTIT020];
   Training Plan of Young Key Teachers in Colleges and Universities of
   Henan Province [2016GGJS-001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Projects 61822701, 61602420, 61672469,
   61472370, 61772474, and 61872324, in part by the National Key R&D
   Program of China under Project 2017YFC0804401, in part by the China
   Postdoctoral Science Foundation through Project 2018M630836, in part by
   the Supporting Plan for Scientific and Technological Innovative Talents
   in Universities of Henan Province under Project 18HASTIT020, and in part
   by the Training Plan of Young Key Teachers in Colleges and Universities
   of Henan Province under Project 2016GGJS-001. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Marco Bertini. (Corresponding author: Yafei Li.)
CR Aliva N., 2012, Visualead
   [Anonymous], 2013, ZXING
   [Anonymous], 2000, 18004 ISOIEC
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chengfang Fang, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P278, DOI 10.1007/978-3-319-04114-8_24
   Chu HK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508408
   Cox R., 2012, Qartcodes
   FALCON A, 2013, 40 GORGEOUS QR CODE
   Garateguy GJ, 2014, IEEE T IMAGE PROCESS, V23, P2842, DOI 10.1109/TIP.2014.2321501
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kuribayashi M, 2017, IEICE T INF SYST, VE100D, P42, DOI 10.1587/transinf.2016MUP0002
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Liao J., 2017, ACM Trans. Graph.
   Lin SS, 2015, IEEE T MULTIMEDIA, V17, P1515, DOI 10.1109/TMM.2015.2437711
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin YS, 2013, COMPUT GRAPH FORUM, V32, P137, DOI 10.1111/cgf.12221
   Lin YH, 2013, IEEE T MULTIMEDIA, V15, P2198, DOI 10.1109/TMM.2013.2271745
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Samretwit D., 2011, Proceedings of the 2011 Third International Conference on Intelligent Networking and Collaborative Systems (INCoS 2011), P552, DOI 10.1109/INCoS.2011.117
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Ulyanov D., 2016, P 33 INT C INT C MAC, V48, P1349
   Wakahara T., 2011, Proceedings of the 2011 14th International Conference on Network-Based Information Systems (NBiS 2011), P484, DOI 10.1109/NBiS.2011.80
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu M, 2018, ARXIV180302280
   Zhang YJ, 2015, ADV SOC SCI EDUC HUM, V17, P183
NR 26
TC 46
Z9 51
U1 0
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 1960
EP 1970
DI 10.1109/TMM.2019.2891420
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700006
DA 2024-07-18
ER

PT J
AU Feng, YA
   Zhou, P
   Xu, J
   Ji, SL
   Wu, DP
AF Feng, Yinan
   Zhou, Pan
   Xu, Jie
   Ji, Shouling
   Wu, Dapeng
TI Video Big Data Retrieval Over Media Cloud: A Context-Aware Online
   Learning Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Big data; video retrieval; online learning; media cloud; contextual
   bandit; online learning
ID RELEVANCE-FEEDBACK; IMAGE RETRIEVAL; RECOMMENDATION; CODES
AB Online video sharing (e.g., via YouTube or YouKu) has emerged as one of the most important services in the current Internet, where billions of videos on the cloud are awaiting exploration. Hence, a personalized video retrieval system is needed to help users find interesting videos from big data content. Two of the main challenges are to process the increasing amount of video big data and resolve the accompanying "cold start" issue efficiently. Another challenge is to satisfy the users' need for personalized retrieval results, of which the accuracy is unknown. In this paper, we formulate the personalized video big data retrieval problem as an interaction between the user and the system via a stochastic process, not just a similarity matching, accuracy (feedback) model of the retrieval; introduce users' real-time context into the retrieval system; and propose a general framework for this problem. By using a novel contextual multiarmed bandit-based algorithm to balance the accuracy and efficiency, we propose a context-based online big-data-oriented personalized video retrieval system. This system can support datasets that are dynamically increasing in size and has the property of cross-modal retrieval. Our approach provides accurate retrieval results with sublinear regret and linear storage complexity and significantly improves the learning speed. Furthermore, by learning for a cluster of similar contexts simultaneously, we can realize sublinear storage complexity with the same regret but slightly poorer performance on the "cold start" issue compared to the previous approach. We validate our theoretical results experimentally on a tremendously large dataset; the results demonstrate that the proposed algorithms outperform existing bandit-based online learning methods in terms of accuracy and efficiency and the adaptation from the bandit framework offers additional benefits.
C1 [Feng, Yinan; Zhou, Pan] Huazhong Univ Sci & Technol, Wuhan 430072, Peoples R China.
   [Xu, Jie] Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33146 USA.
   [Ji, Shouling] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Wu, Dapeng] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
C3 Huazhong University of Science & Technology; University of Miami;
   Zhejiang University; State University System of Florida; University of
   Florida
RP Zhou, P (corresponding author), Huazhong Univ Sci & Technol, Wuhan 430072, Peoples R China.
EM yinanfenghust@gmail.com; panzhou@hust.edu.cn; jiexu@miami.edu;
   sji@gatech.edu; wu@ece.ufl.edu
OI Wu, Dapeng/0000-0003-1755-0183; Xu, Jie/0000-0002-0515-1647
FU National Science Foundation of China [61401169]
FX This work was supported by the National Science Foundation of China
   under Grant 61401169.
CR Anjulan A, 2009, IEEE T CIRC SYST VID, V19, P63, DOI 10.1109/TCSVT.2008.2005801
   [Anonymous], 2016, P 24 ACM INT C MULT
   [Anonymous], P ACM MULT
   Araujo A, 2015, IEEE IMAGE PROC, P1518
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Azar MG, 2014, PR MACH LEARN RES, V32, P1557
   Barjasteh I, 2016, IEEE T KNOWL DATA EN, V28, P1462, DOI 10.1109/TKDE.2016.2522422
   Bashir FI, 2007, IEEE T MULTIMEDIA, V9, P58, DOI 10.1109/TMM.2006.886346
   Benini S, 2011, IEEE T MULTIMEDIA, V13, P1356, DOI 10.1109/TMM.2011.2163058
   Bouneffouf D, 2012, LECT NOTES COMPUT SC, V7665, P324, DOI 10.1007/978-3-642-34487-9_40
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Bubeck S, 2011, J MACH LEARN RES, V12, P1655
   Canini L, 2013, IEEE T CIRC SYST VID, V23, P636, DOI 10.1109/TCSVT.2012.2211935
   Chang HS, 1999, IEEE T CIRC SYST VID, V9, P1269, DOI 10.1109/76.809161
   Davidson James, 2010, P 4 ACM C REC SYST, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708]
   Demir B, 2015, IEEE T GEOSCI REMOTE, V53, P2323, DOI 10.1109/TGRS.2014.2358804
   Doulamis AD, 2004, IEEE T CIRC SYST VID, V14, P656, DOI 10.1109/TCSVT.2004.826752
   Doulamis N, 2006, SIGNAL PROCESS-IMAGE, V21, P334, DOI 10.1016/j.image.2005.11.006
   Feng Y., 2018, SUPPLEMENTARY VIDEO
   Ghosh H., 2007, P INT MULTIMEDIA C A, P39
   Guo GB, 2014, KNOWL-BASED SYST, V57, P57, DOI 10.1016/j.knosys.2013.12.007
   He J, 2010, NINTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS, VOLS I-III, P1061
   Hu WM, 2007, IEEE T IMAGE PROCESS, V16, P1168, DOI 10.1109/TIP.2006.891352
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Leng B, 2014, EPL-EUROPHYS LETT, V105, DOI 10.1209/0295-5075/105/58004
   Lika B, 2014, EXPERT SYST APPL, V41, P2065, DOI 10.1016/j.eswa.2013.09.005
   Meng JJ, 2016, IEEE T MULTIMEDIA, V18, P116, DOI 10.1109/TMM.2015.2500734
   Min HS, 2012, IEEE T CIRC SYST VID, V22, P1174, DOI 10.1109/TCSVT.2012.2197080
   Mironica I, 2016, COMPUT VIS IMAGE UND, V143, P38, DOI 10.1016/j.cviu.2015.10.005
   Mukherjee DP, 2007, IEEE T CIRC SYST VID, V17, P612, DOI 10.1109/TCSVT.2007.895353
   Patel S, 2012, 3 BIOTECH, V2, P1, DOI 10.1007/s13205-011-0036-2
   Qian XM, 2016, IEEE T IMAGE PROCESS, V25, P195, DOI 10.1109/TIP.2015.2497145
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Saveski M, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P89, DOI 10.1145/2645710.2645751
   Shang L., 2010, Proceedings of the International Conference on Multimedia, P531
   Shao L, 2014, IEEE T CIRC SYST VID, V24, P504, DOI 10.1109/TCSVT.2013.2276700
   Simsek O, 2012, COMPUTER AND INFORMATION SCIENCES II, P157, DOI 10.1007/978-1-4471-2155-8_19
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Song LQ, 2016, IEEE T SERV COMPUT, V9, P433, DOI 10.1109/TSC.2014.2365795
   Su CW, 2007, IEEE T MULTIMEDIA, V9, P1193, DOI 10.1109/TMM.2007.902875
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   Tekin C, 2015, IEEE T IMAGE PROCESS, V24, P3666, DOI 10.1109/TIP.2015.2446936
   Tekin C, 2013, ANN ALLERTON CONF, P1435, DOI 10.1109/Allerton.2013.6736696
   Tseng KC, 2013, 2013 1ST INTERNATIONAL FUTURE ENERGY ELECTRONICS CONFERENCE (IFEEC 2013), P283, DOI 10.1109/IFEEC.2013.6687517
   Vallet D., 2015, IEEE T CIRCUITS SYST, V17, P336
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Xu J, 2015, IEEE J-STSP, V9, P702, DOI 10.1109/JSTSP.2015.2389196
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Ye GN, 2013, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2013.282
   Zhang M, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P73, DOI 10.1145/2600428.2609599
   Zhang SF, 2018, IEEE T CIRC SYST VID, V28, P2716, DOI 10.1109/TCSVT.2017.2710345
   Zhao WNX, 2016, IEEE T KNOWL DATA EN, V28, P1147, DOI 10.1109/TKDE.2015.2508816
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
   Zhu M, 2014, IEEE INT SYMP INFO, P2177, DOI 10.1109/ISIT.2014.6875219
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 57
TC 21
Z9 22
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1762
EP 1777
DI 10.1109/TMM.2018.2885237
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700012
DA 2024-07-18
ER

PT J
AU Roy, D
   Murty, KSR
   Mohan, CK
AF Roy, Debaditya
   Murty, Kodukula Sri Rama
   Mohan, Chalavadi Krishna
TI Unsupervised Universal Attribute Modeling for Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; universal attribute model; unsupervised feature
   extraction; MAP adaptation; factor analysis; Gaussian mixture model
AB A fixed dimensional representation for action clips of varying lengths has been proposed in the literature using aggregation models like bag-of-words and Fisher vector. These representations are high dimensional and require classification techniques for action recognition. In this paper, we propose a framework for unsupervised extraction of a discriminative low-dimensional representation called action-vector. To start with, local spatio-temporal features are utilized to capture the action attributes implicitly in a large Gaussian mixture model called the universal attribute model (UAM). To enhance the contribution of the significant attributes in each action clip, a maximum aposteriori adaptation of the UAM means is performed for each clip. This results in a concatenated mean vector called super action vector (SAV) for each action clip. However, the SAV is still high dimensional because of the presence of redundant attributes. Hence, we employ factor analysis to represent every SAV only in terms of the few important attributes contributing to the action clip. This leads to a low-dimensional representation called action-vector. This entire procedure requires no class labels and produces action-vectors that are distinct representations of each action irrespective of the inter-actor variability encountered in unconstrained videos. An evaluation on trimmed action datasets UCF101 and HMDB51 demonstrates the efficacy of action-vectors for action classification over state-of-the-art techniques. Moreover, we also show that action-vectors can adequately represent untrimmed videos from the THUMOS14 dataset and produce classification results comparable to existing techniques.
C1 [Roy, Debaditya; Mohan, Chalavadi Krishna] Indian Inst Technol Hyderabad, Dept Comp Sci & Engn, Visual Learning & Intelligence Grp, Hyderabad 502205, India.
   [Murty, Kodukula Sri Rama] Indian Inst Technol Hyderabad, Dept Elect Engn, Hyderabad 502205, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Hyderabad; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Hyderabad
RP Roy, D (corresponding author), Indian Inst Technol Hyderabad, Dept Comp Sci & Engn, Visual Learning & Intelligence Grp, Hyderabad 502205, India.
EM cs13p1001@iith.ac.in; ksrm@iith.ac.in; ckm@iith.ac.in
RI Roy, Debaditya/W-6315-2019
OI Roy, Debaditya/0000-0002-8779-1241; Kodukula, Sri Rama
   Murty/0000-0002-6355-5287; IIT Hyderabad, EE
   Department/0000-0002-5880-4023; Chalavadi, Krishna
   Mohan/0000-0002-7316-0836
CR [Anonymous], ECCV WORKSH
   Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   Diba A, 2017, PROC CVPR IEEE, P1541, DOI 10.1109/CVPR.2017.168
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65
   HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Inoue N, 2012, IEEE T MULTIMEDIA, V14, P1196, DOI 10.1109/TMM.2012.2191395
   Ioffe S, 2006, LECT NOTES COMPUT SC, V3954, P531
   Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599
   Kenny P, 2005, IEEE T SPEECH AUDI P, V13, P345, DOI 10.1109/TSA.2004.840940
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616
   Li K, 2012, LECT NOTES COMPUT SC, V7572, P286, DOI 10.1007/978-3-642-33718-5_21
   Li Y, 2016, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2016.215
   Peng YX, 2019, IEEE T CIRC SYST VID, V29, P773, DOI 10.1109/TCSVT.2018.2808685
   Perronnin F, 2008, IEEE T PATTERN ANAL, V30, P1243, DOI 10.1109/TPAMI.2007.70755
   Prince SJD, 2007, IEEE I CONF COMP VIS, P1751
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   de Souza CR, 2016, LECT NOTES COMPUT SC, V9911, P697, DOI 10.1007/978-3-319-46478-7_43
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Wang H., 2013, ICCV workshop on action recognition with a large number of classes, P1
   Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, Temporal segment networks: Towards good practices for deep action recognition, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang L, 2014, IEEE INT CONF VLSI
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Wu JZ, 2014, IEEE T MULTIMEDIA, V16, P147, DOI 10.1109/TMM.2013.2283846
   Yi Zhu, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P668, DOI 10.1007/978-3-319-46604-0_47
   Zhao YZ, 2017, IEEE INT CON MULTI, P847, DOI 10.1109/ICME.2017.8019343
   Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
NR 39
TC 26
Z9 26
U1 2
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1672
EP 1680
DI 10.1109/TMM.2018.2887021
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700005
DA 2024-07-18
ER

PT J
AU Wang, QR
   Yuan, C
   Liu, Y
AF Wang, Qiurui
   Yuan, Chun
   Liu, Yan
TI Learning Deep Conditional Neural Network for Image Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Segmentation; object parsing; convolutional neural networks; conditional
   Boltzmann machines
AB Combining Convolutional Neural Networks (CNNs) with Conditional Random Fields (CRFs) achieves great success among recent object segmentation methods. There are two advantages by such usage. First, CNNs can extract low-level features, which are very similar to the extracted features in primates' primary visual cortex (V1). Second, CRFs can set up the relationship between input features and output labels in a direct way. In this paper, we extend the first advantage by using CNNs for low-level feature extraction and a Structured Random Forest (SRF)-based border ownership detector for high-level feature extraction, which are similar to the outputs of primates secondary visual cortex (V2). Compared to the CRF model, an improved Conditional Boltzmann Machine (CBM), which has a multi-channel visible layer, is proposed to model the relationship between predicted labels, local and global contexts of objects with multi-scale and multilevel features. Besides, our proposed CBM model is extended for object parsing by using multivisible branches instead of a single visible layer of CBM, which cannot only segment the whole body but also the parts of the body under. These visible branches use each branch for the segmentation of the whole body or one of the body parts. All branches share the same hidden layers of CBM and train the branches under an iterative way. By exploiting object parsing, the whole body segmentation performance of object is improved. To refine the segmentation output, two kinds of optimization algorithms are proposed. The superpixel-based algorithm can re-label the overlapped regions of multiple kinds of objects. The other curve correction algorithm corrects the edges of segmented object parts by using smooth edges under a curve similarity criterion. Experiments demonstrate that our models yield competitive results for object segmentation on the PASCAL VOC 2012 dataset and for object parsing on the PennFudan Pedestrian Parsing dataset, Pedestrian Parsing Surveillance Scenes dataset, Horse-Cow parsing dataset, and PASCAL Quadrupeds dataset.
C1 [Wang, Qiurui; Yuan, Chun] Tsinghua Univ, Dept Comp Sci, Beijing 12442, Peoples R China.
   [Liu, Yan] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
C3 Tsinghua University; Hong Kong Polytechnic University
RP Yuan, C (corresponding author), Tsinghua Univ, Dept Comp Sci, Beijing 12442, Peoples R China.
EM wangqr12@mails.tsinghua.edu.cn; yuanc@sz.tsinghua.edu.cn;
   csyliu@comp.polyu.edu.hk
RI liu, yan/HGV-1365-2022
OI Wang, Qiurui/0000-0003-0508-3418
FU NSFC [U1833101]; Shenzhen Science and Technologies project
   [JCYJ20160428182137473]; Joint Research Center of Tencent and Tsinghua
FX This work is supported by NSFC project Grant No. U1833101, Shenzhen
   Science and Technologies project under Grant No. JCYJ20160428182137473
   and the Joint Research Center of Tencent and Tsinghua
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.348
   [Anonymous], 2015, CVPR, DOI DOI 10.1109/CVPR.2015.7298642
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Cao XC, 2013, IEEE I CONF COMP VIS, P313, DOI 10.1109/ICCV.2013.46
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Eslami C. W. S. M. Ali, 2012, ADV NEURAL INFORM PR, P272
   Eslami S. M. A., 2012, P IEEE C COMP VIS PA, P2648
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu KR, 2017, IEEE T MULTIMEDIA, V19, P1531, DOI 10.1109/TMM.2017.2679898
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Kae A, 2013, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2013.263
   Kim S, 2014, IEEE T PATTERN ANAL, V36, P1761, DOI 10.1109/TPAMI.2014.2303095
   Krahenbuhl P., 2013, P 30 INT C MACH LEAR, P513
   Krüger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Lafferty John, 2001, INT C MACH LEARN ICM
   Liang X, 2017, INT J ADV ROBOT SYST, V14, DOI 10.1177/1729881417724179
   Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   LIU S, 2015, PROC CVPR IEEE, P1419, DOI DOI 10.1109/CVPR.2015.7298748
   Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo P, 2013, IEEE I CONF COMP VIS, P2648, DOI 10.1109/ICCV.2013.329
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Pont-Tuset J, 2016, IEEE T PATTERN ANAL, V38, DOI [10.1109/TPAMI.2015.2481406, 10.1109/TPAMI.2016.2537320]
   Rauschert I., 2012, P EUR C COMP VIS, P553
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sigal L., 2006, P IEEE INT C COMP VI, P1264
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szummer M, 2008, LECT NOTES COMPUT SC, V5303, P582, DOI 10.1007/978-3-540-88688-4_43
   Teo CL, 2015, PROC CVPR IEEE, P5117, DOI 10.1109/CVPR.2015.7299147
   Wang HY, 2015, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2015.7298788
   Wang L., 2007, P AS C COMP VIS, P855
   Wang P, 2015, IEEE I CONF COMP VIS, P1573, DOI 10.1109/ICCV.2015.184
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yang JM, 2014, PROC CVPR IEEE, P320, DOI 10.1109/CVPR.2014.48
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Yang Z, 2015, INT GEOSCI REMOTE SE, P1785, DOI 10.1109/IGARSS.2015.7326136
   Yihang Bo, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2265, DOI 10.1109/CVPR.2011.5995609
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhao B, 2016, IEEE T MULTIMEDIA, V18, P1111, DOI 10.1109/TMM.2016.2537783
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhu HY, 2016, IEEE T MULTIMEDIA, V18, P1516, DOI 10.1109/TMM.2016.2571629
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 59
TC 13
Z9 13
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1839
EP 1852
DI 10.1109/TMM.2018.2890360
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700018
DA 2024-07-18
ER

PT J
AU Wu, L
   Wang, Y
   Gao, JB
   Li, X
AF Wu, Lin
   Wang, Yang
   Gao, Junbin
   Li, Xue
TI Where-and-When to Look: Deep Siamese Attention Networks for Video-Based
   Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video-based person re-identification; gated recurrent units; spatial
   correlations; visual attention
AB Video-based person re-identification (re-id) is a central application in surveillance systems with a significant concern in security. Matching persons across disjoint camera views in their video fragments are inherently challenging due to the large visual variations and uncontrolled frame rates. There are two steps crucial to person re-id, namely, discriminative feature learning and metric learning. However, existing approaches consider the two steps independently, and they do not make full use of the temporal and spatial information in the videos. In this paper, we propose a Siamese attention architecture that jointly learns spatiotemporal video representations and their similarity metrics. The network extracts local convolutional features from regions of each frame and enhances their discriminative capability by focusing on distinct regions when measuring the similarity with another pedestrian video. The attention mechanism is embedded into spatial gated recurrent units to selectively propagate relevant features and memorize their spatial dependencies through the network. The model essentially learns which parts (where) from which frames (when) are relevant and distinctive for matching persons and attaches higher importance therein. The proposed Siamese model is end-to-end trainable to jointly learn comparable hidden representations for paired pedestrian videos and their similarity value. Extensive experiments on three benchmark datasets show the effectiveness of each component of the proposed deep network while outperforming state-of-the-art methods.
C1 [Wu, Lin] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230000, Anhui, Peoples R China.
   [Wu, Lin; Li, Xue] Univ Queensland, St Lucia, Qld 4072, Australia.
   [Wang, Yang] Dalian Univ Technol, Dalian 116024, Peoples R China.
   [Gao, Junbin] Univ Sydney, Discipline Business Analyt, Sch Business, Sydney, NSW 2006, Australia.
C3 Hefei University of Technology; University of Queensland; Dalian
   University of Technology; University of Sydney
RP Wang, Y (corresponding author), Dalian Univ Technol, Dalian 116024, Peoples R China.
EM lin.wu@uq.edu.au; yang.wang@dlut.edu.cn; junbin.gao@sydney.edu.au;
   xueli@itee.uq.edu.au
RI Gao, Junbin/C-6566-2008; Wu, Lin Yuanbo/HME-1691-2023; Gao,
   Junbin/A-1766-2009
OI Wu, Lin Yuanbo/0000-0001-6119-058X; Gao, Junbin/0000-0001-9803-0256; LI,
   Xue/0000-0002-4515-6792
FU Australian Research Council (ARC) [DP 160104075]; NSFC [61806035]; ARC
   [DP140102270]; University of Sydney Business School ARC Bridging Fund
FX This work was supported in part by Australian Research Council (ARC)
   Discovery Project DP 160104075. The work of Y. Wang was supported by
   NSFC under Grant 61806035. The work of J. Gao was supported in part by
   the ARC Discovery Projects funding scheme through Project DP140102270
   and in part by the University of Sydney Business School ARC Bridging
   Fund. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Elisa Ricci.
CR [Anonymous], ARXIV160601609
   [Anonymous], 2016, P INT C LEARN REPR W
   [Anonymous], 2016, arXiv preprint arXiv:1611.05244
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247939
   Ballas N., 2016, ICLR, P1
   Bastien F., 2012, P DEEP LEARN UNS LEA
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349
   Chen SZ, 2016, IEEE T IMAGE PROCESS, V25, P2353, DOI 10.1109/TIP.2016.2545929
   Cho K., 2014, ARXIV14061078
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Cho YJ, 2016, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2016.151
   Chung J., 2014, ARXIV14213555
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Das A, 2017, COMPUT VIS IMAGE UND, V156, P66, DOI 10.1016/j.cviu.2016.10.012
   DEHGHAN A, 2015, PROC CVPR IEEE, P4091, DOI DOI 10.1109/CVPR.2015
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   García J, 2017, IEEE T IMAGE PROCESS, V26, P1650, DOI 10.1109/TIP.2017.2652725
   Gheissari N., 2006, 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06), V2, P1528
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hsieh JW, 2008, IEEE T MULTIMEDIA, V10, P372, DOI 10.1109/TMM.2008.917403
   Khan FM, 2017, IEEE WINT CONF APPL, P605, DOI 10.1109/WACV.2017.73
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Li Z, 2013, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2013.463
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62
   Liu K, 2015, IEEE I CONF COMP VIS, P3810, DOI 10.1109/ICCV.2015.434
   MA B, 2012, P EUR C COMPUT VIS, P413, DOI DOI 10.1007/978-3-642-33863-241
   Ma XL, 2017, PATTERN RECOGN, V65, P197, DOI 10.1016/j.patcog.2016.11.018
   Martinel N, 2017, 11TH INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS (ICDSC 2017), P151, DOI 10.1145/3131885.3131923
   Martinel N, 2015, IEEE T IMAGE PROCESS, V24, P5645, DOI 10.1109/TIP.2015.2487048
   McLaughlin A, 2015, INT C PAR DISTRIB SY, P9, DOI 10.1109/ICPADS.2015.10
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426
   Prosser B., 2010, P BRIT MACH VIS C BM, DOI DOI 10.5244/C.24.21
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Shi XS, 2015, 2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P802, DOI 10.1109/IAEAC.2015.7428667
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tieleman T., 2012, P COURS COURS NEUR N
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wang Y, 2017, IEEE T IMAGE PROCESS, V26, P1393, DOI 10.1109/TIP.2017.2655449
   Wang Y, 2017, IEEE T NEUR NET LEAR, V28, P57, DOI 10.1109/TNNLS.2015.2498149
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wu L., 2016, ARXIV160107255
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu L, 2018, COMPUT VIS IMAGE UND, V167, P63, DOI 10.1016/j.cviu.2017.11.009
   Wu L, 2018, PATTERN RECOGN, V76, P727, DOI 10.1016/j.patcog.2017.10.004
   Wu L, 2018, PATTERN RECOGN, V73, P275, DOI 10.1016/j.patcog.2017.08.029
   Wu L, 2017, IEEE T CYBERNETICS, V47, P4497, DOI 10.1109/TCYB.2016.2612686
   Wu L, 2017, PATTERN RECOGN, V65, P238, DOI 10.1016/j.patcog.2016.12.022
   Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan YC, 2016, LECT NOTES COMPUT SC, V9910, P701, DOI 10.1007/978-3-319-46466-4_42
   You JJ, 2016, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2016.150
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhu Xiaoke., 2016, Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, P3552
NR 73
TC 268
Z9 286
U1 3
U2 116
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1412
EP 1424
DI 10.1109/TMM.2018.2877886
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400006
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Tang, WJ
   Ren, J
   Zhang, YX
AF Tang, Wenjuan
   Ren, Ju
   Zhang, Yaoxue
TI Enabling Trusted and Privacy-Preserving Healthcare Services in Social
   Media Health Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social media healthcare networks; trust; privacy preservation; bloom
   filter; collaborative filtering; sybil attack
ID SYBIL ATTACK; PROTECTION; MANAGEMENT; SECURITY; SYSTEMS; EDGE
AB Social Media Health Networks provide a promising paradigm to attract patients to share and communicate their personal health status with other online patients, and consult healthcare services from online caregivers with social networks. Social Media Health Networks transform healthcare services from time-consuming offline hospital-centered paradigm to the convenient and efficient online paradigm through Internet, which can expand the traditional healthcare services and shorten the information gap between patients and caregivers. However, how to build the trust between patients and caregivers raises a challenging issue due to the openness of the social networks; meanwhile, the personal privacy may be disclosed when sharing personal health information with other patients and caregivers. In this paper, we propose a personalized and trusted healthcare service approach to enable trusted and privacy-preserving healthcare services in social media health networks, which can improve the trustiness between patients and caregivers through authentic ratings toward caregivers and guarantee the patients' privacy. Specifically, we employ the collaborative filtering model to seek appropriate personalized caregivers, bloom filter to extract and map the personal healthcare symptoms, and inner product to compute the similarity between patients for finding patients with similar health symptoms in a privacy-preserving way. Meanwhile, to guarantee authentic ratings and reviews toward caregivers, we develop a sybil attack detection scheme to find patients' fake ratings and reviews using different pseudonyms. Security analysis shows that our proposed approach can preserve the privacy of patients and prevent sybil attacks. Performance evaluation demonstrates that our approach can achieve prominent performance improvement, in terms of personalized caregivers finding and sybil attack resistance.
C1 [Tang, Wenjuan; Ren, Ju; Zhang, Yaoxue] Cent S Univ, Sch Informat Sci & Engn, Changsha 410083, Hunan, Peoples R China.
C3 Central South University
RP Ren, J (corresponding author), Cent S Univ, Sch Informat Sci & Engn, Changsha 410083, Hunan, Peoples R China.
EM wenjuantang@csu.edu.cn; renju@csu.edu.cn; zyx@csu.edu.cn
RI Ren, Ju/ABD-5251-2021; Ren, Ju/ABD-5213-2021
OI Ren, Ju/0000-0003-2782-183X; tang, wenjaun/0000-0002-9607-8974
FU National Natural Science Foundation Of China [61702562, 61702561]; 111
   project [B18059]; International Science & Technology Cooperation Program
   of China [2013DFB10070]; China Hunan Provincial Science & Technology
   Program [2012GK4106]
FX This work was supported in part by the National Natural Science
   Foundation Of China under Grants 61702562 and 61702561, in part by 111
   project B18059, in part by the International Science & Technology
   Cooperation Program of China under Grant 2013DFB10070, and in part by
   the China Hunan Provincial Science & Technology Program under Grant
   2012GK4106. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhou Su.
   (Corresponding author: Ju Ren.)
CR Al-Qurishi M, 2018, IEEE T IND INFORM, V14, P799, DOI 10.1109/TII.2017.2753202
   [Anonymous], IEEE T DEPENDABLE SE
   Awuor FM, 2018, IEEE ACCESS, V6, P28339, DOI 10.1109/ACCESS.2018.2834359
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   Brozovsky L, 2007, ARXIVCS0703042
   Ferrag MA, 2017, IEEE COMMUN SURV TUT, V19, P3015, DOI 10.1109/COMST.2017.2718178
   Govindan K, 2012, IEEE COMMUN SURV TUT, V14, P279, DOI 10.1109/SURV.2011.042711.00083
   He DJ, 2012, IEEE T INF TECHNOL B, V16, P623, DOI 10.1109/TITB.2012.2194788
   Jabeen F, 2018, IEEE ACCESS, V6, P17246, DOI 10.1109/ACCESS.2018.2810337
   Jiang JF, 2017, IEEE T IND INFORM, V13, P342, DOI 10.1109/TII.2015.2510226
   Jiang WJ, 2016, IEEE T COMPUT, V65, P1211, DOI 10.1109/TC.2015.2444842
   Jiang WJ, 2016, IEEE T COMPUT, V65, P952, DOI 10.1109/TC.2015.2435785
   Kuan Zhang, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P271, DOI 10.1109/INFOCOM.2015.7218391
   Li JX, 2016, IEEE T KNOWL DATA EN, V28, P1820, DOI 10.1109/TKDE.2016.2542804
   Liang XH, 2014, IEEE T PARALL DISTR, V25, P310, DOI 10.1109/TPDS.2013.37
   Lin XD, 2013, IEEE J SEL AREA COMM, V31, P237, DOI 10.1109/JSAC.2013.SUP.0513021
   Liu ZW, 2016, PLOS ONE, V3, P11
   Meng WZ, 2018, IEEE T NETW SERV MAN, V15, P761, DOI 10.1109/TNSM.2018.2815280
   Moradi P, 2015, EXPERT SYST APPL, V42, P7386, DOI 10.1016/j.eswa.2015.05.027
   Truong NB, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17061346
   Peng XH, 2018, IEEE INTERNET THINGS, V5, P1816, DOI 10.1109/JIOT.2018.2812803
   PwC, HRI SURV
   Ren J, 2017, IEEE NETWORK, V31, P96, DOI 10.1109/MNET.2017.1700030
   Ren J, 2016, IEEE T WIREL COMMUN, V15, P3143, DOI 10.1109/TWC.2016.2517618
   Rubenstein J, 2014, CURR UROL REP, V15, DOI 10.1007/s11934-014-0449-7
   Sánchez-Guerrero R, 2017, IEEE J BIOMED HEALTH, V21, P1741, DOI 10.1109/JBHI.2017.2655419
   Shen  S., 2017, CHIN MED ETHICS, V30, P1098
   Su Z, 2020, IEEE T NETW SCI ENG, V7, P343, DOI 10.1109/TNSE.2018.2879674
   Su Z, 2018, IEEE J SEL AREA COMM, V36, P2175, DOI 10.1109/JSAC.2018.2869948
   Su Z, 2017, IEEE T MULTIMEDIA, V19, P2210, DOI 10.1109/TMM.2017.2733338
   Tang W., 2017, IEEE Signal Process. Lett, V66, P1, DOI DOI 10.1109/ICOCN.2017.8121297
   Tang WJ, 2019, IEEE T MOBILE COMPUT, V18, P845, DOI 10.1109/TMC.2018.2848644
   Wang B., 2017, P IEEE INT C COMP CO, P1, DOI DOI 10.1109/INFOCOM.2017.8057178
   Wang GJ, 2015, IEEE T PARALL DISTR, V26, P824, DOI 10.1109/TPDS.2014.2312932
   Wang SG, 2015, IEEE T SERV COMPUT, V8, P755, DOI 10.1109/TSC.2014.2320262
   Wu J, 2018, INFORM FUSION, V41, P232, DOI 10.1016/j.inffus.2017.09.012
   Wu TL, 2018, J MED INTERNET RES, V20, DOI 10.2196/jmir.9003
   Xu CG, 2017, IEEE T INF FOREN SEC, V12, P3081, DOI 10.1109/TIFS.2017.2737966
   Xu QC, 2022, IEEE T BIG DATA, V8, P113, DOI 10.1109/TBDATA.2017.2764925
   Xu QC, 2019, IEEE INTERNET THINGS, V6, P4536, DOI 10.1109/JIOT.2018.2876417
   Xu QC, 2018, IEEE T IND INFORM, V14, P2550, DOI 10.1109/TII.2017.2787201
   Yang Z, 2016, IEEE T DEPEND SECURE, V13, P488, DOI 10.1109/TDSC.2015.2410792
   Yao Y, 2019, IEEE T MOBILE COMPUT, V18, P362, DOI 10.1109/TMC.2018.2833849
   Yuan XL, 2016, IEEE T MULTIMEDIA, V18, P2002, DOI 10.1109/TMM.2016.2602758
   Zhang K, 2015, IEEE WIREL COMMUN, V22, P104, DOI 10.1109/MWC.2015.7224734
   Zhang LP, 2018, IEEE T IND ELECTRON, V65, P2795, DOI 10.1109/TIE.2017.2739683
NR 46
TC 45
Z9 46
U1 4
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 579
EP 590
DI 10.1109/TMM.2018.2889934
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800005
DA 2024-07-18
ER

PT J
AU Zhou, P
   Wang, KH
   Xu, J
   Wu, DP
AF Zhou, Pan
   Wang, Kehao
   Xu, Jie
   Wu, Dapeng
TI Differentially-Private and Trustworthy Online Social Multimedia Big Data
   Retrieval in Edge Computing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Edge computing; differential privacy; online learning; big data;
   trustworthiness; social mutlimedia
ID NETWORKS; INTERNET; THINGS; SYSTEMS; CLOUD
AB The explosive growth of multimedia contents (MCs) in today's mobile social networks has pushed edge computing to face severe security and online big data-processing problems. On the one hand, the edge nodes (ENs) should help mobile users find, cache, and share MCs in the presence of an ever-increasing scale of multimedia big data. On the other hand, how to provide secure MC retrieval schemes to exclude dishonest-and-malicious untrusted ENs and to prevent privacy breaches from honest-but-curious ENs and users is a challenging issue. To tackle these problems, we study the privacy-preserving and trustworthy MCs retrieval system to make personalized MC recommendations from ENs to users with big data support. In our framework, each EN is modeled as a distributed context-aware online learner. ENs collaborate to learn users' preferences based on their contexts and previous behaviors and social intimacy. To support big data analytics, we establish an MC-cluster tree from top to the bottom to handle the dynamically varying cached MC datasets. A differentially private algorithm is proposed to preserve the data privacy among honest-but-curious ENs and users. To guarantee trustworthy edge computing, a trust evaluation mechanism is designed to evaluate the reliability of ENs. We further consider the structure of edge networks to improve the performance of our algorithm. Experimental results validate that our new framework can support increasing multimedia big datasets while striking a balance among privacy-preserving level, Trustworthy level, and caching MC prediction accuracy.
C1 [Zhou, Pan] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun Engn, Wuhan 430074, Hubei, Peoples R China.
   [Wang, Kehao] MIT, Informat & Decis Syst Lab, Cambridge, MA 02139 USA.
   [Wang, Kehao] Wuhan Univ Technol, Dept Informat Engn, Wuhan 430074, Hubei, Peoples R China.
   [Xu, Jie] Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33146 USA.
   [Wu, Dapeng] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
C3 Huazhong University of Science & Technology; Massachusetts Institute of
   Technology (MIT); Wuhan University of Technology; University of Miami;
   State University System of Florida; University of Florida
RP Wang, KH (corresponding author), MIT, Informat & Decis Syst Lab, Cambridge, MA 02139 USA.; Wang, KH (corresponding author), Wuhan Univ Technol, Dept Informat Engn, Wuhan 430074, Hubei, Peoples R China.
EM panzhou@hust.edu.cn; kehao.wang@whut.edu.cn; jiexu@miami.edu;
   wu@ece.ufl.edu
OI Wu, Dapeng/0000-0003-1755-0183; Xu, Jie/0000-0002-0515-1647
FU National Science Foundation of China [61672395, 61401169]
FX This work was supported in part by the National Science Foundation of
   China under Grants 61672395, 61401169. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Zhou Su. (Corresponding author: Kehao Wang.)
CR Abbas N, 2018, IEEE INTERNET THINGS, V5, P450, DOI 10.1109/JIOT.2017.2750180
   Adomavicius G, 2011, RECOMMENDER SYSTEMS HANDBOOK, P217, DOI 10.1007/978-0-387-85820-3_7
   [Anonymous], 2015, ARXIV150301817
   [Anonymous], 2011, ACM T INFORM SYSTEM
   [Anonymous], 37 MIND BLOWING YOUT
   [Anonymous], DIFFERENTIALLY PRIVA
   [Anonymous], 2018, IEEE T BIG DATA
   [Anonymous], IEEE COMMUN SURV TUT
   [Anonymous], 2012, QUANTA MAG
   Araniti G, 2017, IEEE INTERNET THINGS, V4, P427, DOI 10.1109/JIOT.2016.2561839
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Azar MG, 2014, PR MACH LEARN RES, V32, P1557
   Bachi G, 2012, PROCEEDINGS OF 2012 ASE/IEEE INTERNATIONAL CONFERENCE ON PRIVACY, SECURITY, RISK AND TRUST AND 2012 ASE/IEEE INTERNATIONAL CONFERENCE ON SOCIAL COMPUTING (SOCIALCOM/PASSAT 2012), P552, DOI 10.1109/SocialCom-PASSAT.2012.115
   Brickell Justin, 2008, P 14 ACM SIGKDD INT, P70, DOI [10.1145/1401890.1401904, DOI 10.1145/1401890.1401904]
   Buccapatnam S, 2013, IEEE DECIS CONTR P, P7309, DOI 10.1109/CDC.2013.6761049
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Chang Z, 2018, IEEE WIREL COMMUN, V25, P28, DOI 10.1109/MWC.2018.1700317
   Cicirelli F, 2018, IEEE INTERNET THINGS, V5, P2557, DOI 10.1109/JIOT.2017.2775739
   Dautov R, 2018, IEEE ACCESS, V6, P29822, DOI 10.1109/ACCESS.2018.2839915
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork C, 2010, ACM S THEORY COMPUT, P715
   He Y, 2018, IEEE WIREL COMMUN, V25, P103, DOI 10.1109/MWC.2018.1700274
   Huang XM, 2017, IEEE ACCESS, V5, P25408, DOI 10.1109/ACCESS.2017.2769878
   Jorgensen Zach., 2014, INT C EXTENDING DATA, P571, DOI 10.5441/002/edbt.2014.51
   Kameda T, 1997, J PERS SOC PSYCHOL, V73, P296, DOI 10.1037/0022-3514.73.2.296
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Miao Y, 2018, IEEE Trans Serv Comput, V2018, P1
   Narayanan Arvind., 2006, CORR
   Ortiz AM, 2014, IEEE INTERNET THINGS, V1, P206, DOI 10.1109/JIOT.2014.2318835
   Pinto S, 2017, IEEE INTERNET COMPUT, V21, P40, DOI 10.1109/MIC.2017.17
   Portelli K, 2017, 2017 5TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD WORKSHOPS (FICLOUDW) 2017, P164, DOI 10.1109/FiCloudW.2017.72
   Preuveneers D, 2008, WIREL NETW MOB COMMU, P287
   Pu L., 2018, IEEE INTERNET THINGS, P1
   Shang S, 2014, P 29 ANN ACM S APPL, P266, DOI DOI 10.1145/2554850.2554924
   Slivkins Aleksandrs, 2011, P 24 ANN C LEARN THE, P679
   Song LQ, 2016, IEEE T SERV COMPUT, V9, P433, DOI 10.1109/TSC.2014.2365795
   Su Z, 2020, IEEE T NETW SCI ENG, V7, P343, DOI 10.1109/TNSE.2018.2879674
   Su Z, 2018, IEEE J SEL AREA COMM, V36, P2175, DOI 10.1109/JSAC.2018.2869948
   Tang JL, 2015, IEEE T KNOWL DATA EN, V27, P1724, DOI 10.1109/TKDE.2014.2382576
   Tekin C, 2014, IEEE J-STSP, V8, P638, DOI 10.1109/JSTSP.2014.2299517
   Tekin C, 2013, ANN ALLERTON CONF, P1435, DOI 10.1109/Allerton.2013.6736696
   Wang XK, 2017, IEEE COMMUN MAG, V55, P80, DOI 10.1109/MCOM.2017.1700360
   Wanigasekara N, 2016, P 6 INT C INT THINGS, ppp121
   Xu QC, 2019, IEEE INTERNET THINGS, V6, P4536, DOI 10.1109/JIOT.2018.2876417
   Xu QC, 2018, IEEE CONF COMPUT, P808
   Yang MM, 2018, IEEE ACCESS, V6, P17119, DOI 10.1109/ACCESS.2018.2817523
   Yuan J, 2018, IEEE CONF COMPUT, P819, DOI 10.1109/INFCOMW.2018.8406900
   Zeydan E, 2016, IEEE COMMUN MAG, V54, P36, DOI 10.1109/MCOM.2016.7565185
   Zhang K, 2014, IEEE COMMUN MAG, V52, P58, DOI 10.1109/MCOM.2014.6766086
NR 50
TC 33
Z9 33
U1 0
U2 46
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 539
EP 554
DI 10.1109/TMM.2018.2885509
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800002
DA 2024-07-18
ER

PT J
AU Gao, YY
   Hu, HM
   Li, B
   Guo, Q
   Pu, SL
AF Gao, Yuanyuan
   Hu, Hai-Miao
   Li, Bo
   Guo, Qiang
   Pu, Shiliang
TI Detail Preserved Single Image Dehazing Algorithm Based on Airlight
   Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dehazing; airlight consistency; image sharpening; haze removal
ID CONTRAST ENHANCEMENT; VISION
AB Single-image haze removal is important for many practical applications (e.g., surveillance). However, dehazed results of existing algorithms tend to be oversmoothed with missing fine image details. This drawback is caused by two factors: inaccurate airlight estimations and disregarding multiple scattering. In this paper, we propose a detail-preserving image dehazing algorithm based on two key priors, namely, the depth-edge aware prior and the airlight impact regularity prior. The proposed algorithm makes contributions in both the haze removal step and the postprocessing step. First, based on the depth-edge aware prior, an airlight refinement algorithm is proposed. The gradient strength of the minimum channel is employed to calculate punishment weights to smooth the dark channel. Second, based on the airlight impact regularity prior, an adaptive sharpening model that considers the refined airlight to determine the sharpening strength value is established to enhance levels of detail. Experimental results demonstrate that the proposed algorithm cannot only effectively remove haze but can also enhance levels of detail to thus outperform the state of the art on a wide variety of images.
C1 [Gao, Yuanyuan] Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
   [Gao, Yuanyuan] China Elect Technol Grp Corp, Informat Sci Acad, Internet Things Technol Res Inst, Beijing 100086, Peoples R China.
   [Hu, Hai-Miao; Li, Bo; Guo, Qiang] Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
   [Pu, Shiliang] Hangzhou Hikvis Digital Technol Co Ltd, Hangzhou 310000, Zhejiang, Peoples R China.
C3 Beihang University; China Electronics Technology Group; Beihang
   University; Hangzhou Hikvision Digital Technology Co., Ltd.
RP Hu, HM (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
EM gyy002005@163.com; frank0139@163.com; boli@buaa.edu.cn;
   1263836618@qq.com; pushiliang@hikvision.com
RI Li, Bo/AAA-8968-2020; Li, bo/IWL-9318-2023
OI Li, Bo/0000-0002-7294-6888; 
FU National Key Research and Development Program [2016YFC0801003]; National
   Natural Science Foundation of China [61772058, 61421003]; Fundamental
   Research Funds for the Central Universities
FX This work was supported in part by the National Key Research and
   Development Program under Grant 2016YFC0801003, in part by the National
   Natural Science Foundation of China under Grant 61772058 and Grant
   61421003, and in part by the Fundamental Research Funds for the Central
   Universities. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Marco Grangetto.
CR Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   [Anonymous], P ACM SIGGRAPH
   [Anonymous], P ACM SIGGRAPH
   [Anonymous], 2002, PROC IEEE C COMPUT V
   [Anonymous], P ACM SIGGRAPH
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Deng G, 2011, IEEE T IMAGE PROCESS, V20, P1249, DOI 10.1109/TIP.2010.2092441
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Gao YY, 2014, SIGNAL PROCESS, V103, P380, DOI 10.1016/j.sigpro.2014.02.016
   Gibson KB, 2014, IEEE T IMAGE PROCESS, V23, P3179, DOI 10.1109/TIP.2014.2328180
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He LY, 2017, IEEE T IMAGE PROCESS, V26, P1063, DOI 10.1109/TIP.2016.2644267
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Kratz L, 2009, IEEE I CONF COMP VIS, P1701, DOI 10.1109/ICCV.2009.5459382
   Lai YH, 2015, IEEE T CIRC SYST VID, V25, P1, DOI 10.1109/TCSVT.2014.2329381
   Levi L., 1974, Computer Graph- ics and Image Processing, V3, P163, DOI 10.1016/S0146-664X(74)80005-5.
   Li JF, 2015, NEUROCOMPUTING, V156, P1, DOI 10.1016/j.neucom.2015.01.026
   Ma KD, 2015, IEEE IMAGE PROC, P3600, DOI 10.1109/ICIP.2015.7351475
   Narasimhan S. G., 2003, Interactive (de) weathering of an image using physical models, V6, P1
   Narasimhan SG, 2003, PROC CVPR IEEE, P665
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Nishino K, 2012, INT J COMPUT VISION, V98, P263, DOI 10.1007/s11263-011-0508-1
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Schechner YY, 2003, APPL OPTICS, V42, P511, DOI 10.1364/AO.42.000511
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Tan R. T., 2008, P IEEE C COMP VIS PA, P1
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang SQ, 2016, IEEE T MULTIMEDIA, V18, P219, DOI 10.1109/TMM.2015.2510326
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Yang QX, 2012, LECT NOTES COMPUT SC, V7572, P399, DOI 10.1007/978-3-642-33718-5_29
   Zhengmao Ye, 2007, 16th IEEE International Conference on Control Applications. Part of IEEE Multi-conference on Systems and Control, P313
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zuiderveld K., 1994, Graphics Gems, P474, DOI 10.1016/B978-0-12-336156-1.50061-6
NR 43
TC 29
Z9 31
U1 1
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 351
EP 362
DI 10.1109/TMM.2018.2856095
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400007
DA 2024-07-18
ER

PT J
AU Ma, CS
   Yan, ZS
   Chen, CW
AF Ma, Changsha
   Yan, Zhisheng
   Chen, Chang Wen
TI Scalable Access Control For Privacy-Aware Media Sharing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social media sharing; privacy; access control; SCP-ABE; scalable media
   format
ID CLOUD; SECURITY
AB The prevalence of social networks has made it easier than ever for users to share their photos, videos, and other media content with anybody from anywhere. However, the easy access of user-generated media content also brings about privacy concerns. Traditional access control mechanisms, where a single access policy is made for a specific piece of content, cannot satisfy the user privacy requirements in large-scale media sharing systems. Instead, configuring multiple levels of access privileges for the shared media content is desired. On one hand, it conforms to the principle of social networks in information propagation. On the other hand, it accords with the diverse and complex social relationship among social network users. In this paper, we propose a scalable media access control (SMAC) system to enable such a configuration in a secure and efficient manner. The proposed SMAC system is empowered by the scalable ciphertext policy attribute-based encryption algorithm as well as a comprehensive key-management scheme. We provide formal security proof to prove the security of the proposed SMAC system. In addition, we conduct extensive experiments on mobile devices to demonstrate its efficiency.
C1 [Ma, Changsha; Chen, Chang Wen] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
   [Yan, Zhisheng] Georgia State Univ, Dept Comp Sci, Atlanta, GA 30303 USA.
   [Chen, Chang Wen] Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen, Peoples R China.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Buffalo; University System of Georgia; Georgia State University;
   The Chinese University of Hong Kong, Shenzhen
RP Yan, ZS (corresponding author), Georgia State Univ, Dept Comp Sci, Atlanta, GA 30303 USA.
EM changsha@buffalo.edu; zyan@gsu.edu; chencw@cuhk.edu.cn
OI Chen, Chang Wen/0000-0002-6720-234X
FU NSF [ECCS-1405594]
FX This work is supported by NSF Grant ECCS-1405594.
CR Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318
   Algin GB, 2011, J VIS COMMUN IMAGE R, V22, P353, DOI 10.1016/j.jvcir.2011.02.005
   Ali M, 2017, IEEE SYST J, V11, P395, DOI 10.1109/JSYST.2014.2379646
   [Anonymous], ENCY CRYPTOGRAPHY SE
   [Anonymous], 2016, CoRR, DOI DOI 10.1109/QOMEX.2016.7498955
   Barnes S. B., 2006, 1 MONDAY, V11, DOI DOI 10.5210/FM.V11I9.1394
   Bethencourt J, 2007, P IEEE S SECUR PRIV, P321, DOI 10.1109/sp.2007.11
   Crampton J, 2010, LECT NOTES COMPUT SC, V6166, P130, DOI 10.1007/978-3-642-13739-6_9
   Dufaux F, 2008, IEEE T CIRC SYST VID, V18, P1168, DOI 10.1109/TCSVT.2008.928225
   Dumbere DM, 2014, I C CURR TRENDS ENG, P332, DOI 10.1109/ICCTET.2014.6966311
   Fire M, 2014, IEEE COMMUN SURV TUT, V16, P2019, DOI 10.1109/COMST.2014.2321628
   Fu Z., 2016, IEEE Transactions on Services Computing, P1
   Goyal V., 2006, P 13 ACM C COMP COMM, P89, DOI DOI 10.1145/1180405.1180418
   Hankerson Darrel, 2006, Guide to Elliptic Curve Cryptography
   Karahan S., 2016, 2016 Int Conf Biom Spec Interest Group BIOSIG, P1, DOI [10.1109/BIOSIG.2016.7736924, DOI 10.1109/BIOSIG.2016.7736924]
   Lian SG, 2010, TELECOMMUN SYST, V45, P21, DOI 10.1007/s11235-009-9233-2
   Ma CS, 2016, IEEE INT CON MULTI, DOI 10.1109/ICME.2016.7552906
   Ma C, 2014, INT CONF SERVICE SCI, P1, DOI 10.1109/ICSS.2014.36
   Marcellin M. W., 2000, Proceedings DCC 2000. Data Compression Conference, P523, DOI 10.1109/DCC.2000.838192
   Maurer Ueli M., 1994, Advances in Cryptology - Proceedings of CRYPTO 94, Lecture Notes in Computer Science, P271
   Mick C. K., 2008, U. S. Patent, Patent No. [20080247543 A1, 0247543]
   Paterson KG, 2002, ELECTRON LETT, V38, P1025, DOI 10.1049/el:20026682
   PRENEEL B, 1994, EUR T TELECOMMUN, V5, P431
   Rahimi MR, 2014, MOBILE NETW APPL, V19, P133, DOI 10.1007/s11036-013-0477-4
   Samuel A, 2015, IEEE T MULTIMEDIA, V17, P1484, DOI 10.1109/TMM.2015.2458299
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shokri R, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1310, DOI 10.1145/2810103.2813687
   Wang C, 2011, IEEE INFOCOM SER, P820, DOI 10.1109/INFCOM.2011.5935305
   Wei LF, 2014, INFORM SCIENCES, V258, P371, DOI 10.1016/j.ins.2013.04.028
   Wu YD, 2013, IEEE T MULTIMEDIA, V15, P778, DOI 10.1109/TMM.2013.2238910
   Xinglei Zhu, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2769, DOI 10.1109/ICIP.2011.6116244
   Yan Z, 2017, IEEE T CLOUD COMPUT, V5, P485, DOI 10.1109/TCC.2015.2469662
   Yan Z, 2016, FUTURE GENER COMP SY, V62, P175, DOI 10.1016/j.future.2015.11.006
   Yang K, 2016, IEEE T MULTIMEDIA, V18, P940, DOI 10.1109/TMM.2016.2535728
   Yu SC, 2010, IEEE INFOCOM SER, DOI 10.1109/INFCOM.2010.5462174
   Yuan L, 2017, IET SIGNAL PROCESS, V11, P1031, DOI 10.1049/iet-spr.2016.0756
   Yuan L, 2015, IEEE CONF COMPUT, P185, DOI 10.1109/INFCOMW.2015.7179382
   Zhang C, 2010, IEEE NETWORK, V24, P13, DOI 10.1109/MNET.2010.5510913
   Zhou J, 2015, INFORM SCIENCES, V314, P255, DOI 10.1016/j.ins.2014.09.003
   Zhou J, 2015, IEEE T INF FOREN SEC, V10, P1299, DOI 10.1109/TIFS.2015.2407326
   Zhu BB, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P443, DOI 10.1109/ICME.2004.1394224
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 42
TC 16
Z9 16
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 173
EP 183
DI 10.1109/TMM.2018.2851446
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700015
DA 2024-07-18
ER

PT J
AU Lokoc, J
   Bailer, W
   Schoeffmann, K
   Muenzer, B
   Awad, G
AF Lokoc, Jakub
   Bailer, Werner
   Schoeffmann, Klaus
   Muenzer, Bernd
   Awad, George
TI On Influential Trends in Interactive Video Retrieval: Video Browser
   Showdown 2015-2017
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interactive video retrieval; video browsing; content-based methods;
   evaluation metrics
ID SEARCH
AB The last decade has seen innovations that make video recording, manipulation, storage, and sharing easier than ever before, thus impacting many areas of life. New video retrieval scenarios emerged as well, which challenge the state-of-the-art video retrieval approaches. Despite recent advances in content analysis, video retrieval can still benefit from involving the human user in the loop. We present our experience with a class of interactive video retrieval scenarios and our methodology to stimulate the evolution of new interactive video retrieval approaches. More specifically, the video browser showdown evaluation campaign is thoroughly analyzed, focusing on the years 2015-2017. Evaluation scenarios, objectives, and metrics are presented, complemented by the results of the annual evaluations. The results reveal promising interactive video retrieval techniques adopted by the most successful tools and confirm assumptions about the different complexity of various types of interactive retrieval scenarios. A comparison of the interactive retrieval tools with automatic approaches (including fully automatic and manual query formulation) participating in the TRECVID 2016 ad hoc video search task is discussed. Finally, based on the results of data analysis, a substantial revision of the evaluation methodology for the following years of the video browser showdown is provided.
C1 [Lokoc, Jakub] Charles Univ Prague, Fac Math & Phys, Dept Software Engn, CR-11636 Prague, Czech Republic.
   [Bailer, Werner] JOANNEUM RES, DIGITAL, A-8010 Graz, Austria.
   [Schoeffmann, Klaus; Muenzer, Bernd] Alpen Adria Univ Klagenfurt, A-9020 Klagenfurt, Austria.
   [Awad, George] NIST, Informat Access Div, Informat Technol Lab, Gaithersburg, MD 20899 USA.
C3 Charles University Prague; University of Klagenfurt; National Institute
   of Standards & Technology (NIST) - USA
RP Lokoc, J (corresponding author), Charles Univ Prague, Fac Math & Phys, Dept Software Engn, CR-11636 Prague, Czech Republic.
EM lokoc@ksi.mff.cuni.cz; werner.bailer@joanneum.at; ks@itec.aau.at;
   bernd@itec.aau.at; george.awad@nist.gov
RI Lokoč, Jakub/P-1216-2017
OI Bailer, Werner/0000-0003-2442-4900
FU Czech Science Foundation (GACR) [17-22224S]; Universitat Klagenfurt;
   Lakeside Labs GmbH, Klagenfurt, Austria; European Regional Development
   Fund; Carinthian Economic Promotion Fund [KWF-20214 U. 3520/26336/38165]
FX This work was supported in part by the Czech Science Foundation (GACR)
   Project 17-22224S, in part by the Universitat Klagenfurt and Lakeside
   Labs GmbH, Klagenfurt, Austria, and in part by the European Regional
   Development Fund and the Carinthian Economic Promotion Fund (KWF) under
   Grant KWF-20214 U. 3520/26336/38165. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Tao Mei. (Corresponding author: Jakub Lokoc.)
CR Adams B, 2012, IEEE INT CONF MULTI, P127, DOI 10.1109/ICMEW.2012.29
   [Anonymous], IAUTOMOTION AUTONOMO
   [Anonymous], P INT C MULT RETR
   [Anonymous], IMOTION CONTENT BASE
   [Anonymous], 2017, ACM TURING 50 CELEBR
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], NIST TRECVID 2003
   [Anonymous], COLLABORATIVE VIDEO
   [Anonymous], VERGE MULTIMODAL INT
   [Anonymous], SEMANTIC EXTRACTION
   [Anonymous], 2015, NII UIT BROWSER MULT, DOI DOI 10.1007/978-3-319-14442-9_28
   [Anonymous], IMOTION SEARCHING VI
   [Anonymous], 2010, P ACM C MULTIMEDIA S
   [Anonymous], 2007, CIVR '07
   [Anonymous], SELECTING USER GENER
   [Anonymous], COLLABORATIVE FEATUR
   [Anonymous], P 17 ACM INT C MULT
   [Anonymous], P TREC VID RETR EV
   [Anonymous], CONCEPT BASED INTERA
   [Anonymous], P MEDIAEVAL
   [Anonymous], P 6 ACM INT C IM VID
   [Anonymous], ENHANCED RETRIEVAL B
   [Anonymous], VERGE MULTIMODAL INT
   [Anonymous], ENHANCED SIGNATURE B
   [Anonymous], 2003, P 5 ACM SIGMM INT WO, DOI DOI 10.1145/973264.973269
   [Anonymous], 2013, P 3 ACM C INT C MULT
   [Anonymous], GRAPH BASED BROWSING
   [Anonymous], MULTISKETCH SEMANTIC
   [Anonymous], P MEDIAEVAL WORKSH
   [Anonymous], MENTAL VISUAL BROWSI
   [Anonymous], 2016, MULTIMEDIA MODELING, DOI DOI 10.1007/978
   [Anonymous], P 6 ACM INT C IM VID
   [Anonymous], VIDEO HUNTER VBS 201
   [Anonymous], STORYBOARD BASED INT
   Awad G., 2016, ITE Transactions on Media Technology and Applications, V4, P187, DOI DOI 10.3169/MTA.4.187
   Awad G, 2017, INT J MULTIMED INF R, V6, P1, DOI 10.1007/s13735-017-0121-3
   Beecks C, 2014, MULTIMED TOOLS APPL, V71, P349, DOI 10.1007/s11042-012-1334-3
   Christel MG, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P1032
   de Rooij O, 2010, IEEE COMPUT GRAPH, V30, P42, DOI 10.1109/MCG.2010.66
   de Rooij O, 2010, IEEE T MULTIMEDIA, V12, P121, DOI 10.1109/TMM.2009.2037388
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ferecatu M, 2009, IEEE T PATTERN ANAL, V31, P1087, DOI 10.1109/TPAMI.2008.259
   Francis D., 2017, PROC 21 INT WORKSHOP, P1
   Geetha P., 2008, Journal of Computer Sciences, V4, P474, DOI 10.3844/jcssp.2008.474.486
   Giangreco Ivan, 2016, Datenbank-Spektrum, V16, P17, DOI 10.1007/s13222-015-0209-y
   Hauptmann A.G., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, P385, DOI DOI 10.1145/1180639.1180721
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Hürst W, 2017, LECT NOTES COMPUT SC, V10133, P480, DOI 10.1007/978-3-319-51814-5_45
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang YG, 2016, IEEE T MULTIMEDIA, V18, P2161, DOI 10.1109/TMM.2016.2614233
   Keim DA, 2002, IEEE T VIS COMPUT GR, V8, P1, DOI 10.1109/2945.981847
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larson M, 2017, IEEE MULTIMEDIA, V24, P93, DOI 10.1109/MMUL.2017.9
   Lokoc Jakub, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8326, P415, DOI 10.1007/978-3-319-04117-9_49
   Lokoc J, 2017, LECT NOTES ARTIF INT, V10604, P754, DOI 10.1007/978-3-319-69179-4_53
   Luan HB, 2011, INFORM SCIENCES, V181, P4197, DOI 10.1016/j.ins.2011.05.018
   Moumtzidou A, 2017, LECT NOTES COMPUT SC, V10133, P486, DOI 10.1007/978-3-319-51814-5_46
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sacha D, 2017, NEUROCOMPUTING, V268, P164, DOI 10.1016/j.neucom.2017.01.105
   Sacha D, 2014, IEEE T VIS COMPUT GR, V20, P1604, DOI 10.1109/TVCG.2014.2346481
   Schoeffmann K, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1321, DOI 10.1145/2733373.2807417
   Schoeffmann K, 2015, ACM COMPUT SURV, V48, DOI 10.1145/2808796
   Schoeffmann K, 2014, INT J MULTIMED INF R, V3, P113, DOI 10.1007/s13735-013-0050-8
   Schoeffmann K, 2014, IEEE MULTIMEDIA, V21, P8, DOI 10.1109/MMUL.2014.56
   Schoeffmann K, 2014, IEEE T MULTIMEDIA, V16, P1942, DOI 10.1109/TMM.2014.2333666
   Schonert-Reichl KA, 2012, SCH MENT HEALTH, V4, P1, DOI 10.1007/s12310-011-9064-7
   Smeaton AF, 2008, INT J IMAG SYST TECH, V18, P195, DOI 10.1002/ima.20150
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Snoek CGM, 2008, IEEE MULTIMEDIA, V15, P86, DOI 10.1109/MMUL.2008.21
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P280, DOI 10.1109/TMM.2006.886275
   Ueki Kazuya., 2017, Waseda meisei at trecvid 2017: Ad-hoc video search
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Worring M, 2012, IEEE MULTIMEDIA, V19, P6, DOI 10.1109/MMUL.2012.53
   Yilmaz E, 2008, KNOWL INF SYST, V16, P173, DOI 10.1007/s10115-007-0101-7
   Yuan J, 2011, IEEE T MULTIMEDIA, V13, P1343, DOI 10.1109/TMM.2011.2168813
NR 75
TC 62
Z9 64
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3361
EP 3376
DI 10.1109/TMM.2018.2830110
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600015
DA 2024-07-18
ER

PT J
AU Liu, D
   Wu, J
   Cui, H
   Zhang, DD
   Luo, C
   Wu, F
AF Liu, Dian
   Wu, Jun
   Cui, Hao
   Zhang, Dongdong
   Luo, Chong
   Wu, Feng
TI Cost-Distortion Optimization and Resource Control in Pseudo-Analog
   Visual Communications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bandwidth-power-distortion model; pseudo-analog transmission
ID VIDEO; TRANSMISSION; DESIGN; MODEL
AB The rate-distortion in conventional digital systems is replaced by cost distortion in pseudo-analog systems where the cost consists of power and bandwidth. In this paper, we formulate the cost-distortion optimization problem in terms of a power-bandwidth pair versus distortion to bring an insight to pseudo-analog transmission. Using a divide-and-conquer strategy, the 3-D optimization problem of a power-bandwidth pair versus distortion is decomposed into two subproblems: power distortion and bandwidth distortion optimization. To solve the integer nonlinear optimization problem, we propose two prediction models that transform the partial summation of variances and the square roots of variances into continuous functions. The proposed models are used to derive the closed-form solutions for both optimization subproblems, and a tradeoff between power and bandwidth is discussed. Accordingly, the resource control algorithm is designed to allocate the fewest resources required to obtain specific video quality; power can be traded for bandwidth and vice versa. Our experimental results show that the proposed optimization models achieve stable perceptual video quality comparable to that of Groups of Pictures and use resources more efficiently than do SoftCast.
C1 [Liu, Dian; Cui, Hao; Zhang, Dongdong] Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Wu, Jun] Tongji Univ, Key Lab, Minist Educ Embedded Syst & Serv Comp, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Luo, Chong] Microsoft Res Asia, Beijing 10080, Peoples R China.
   [Wu, Feng] Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China.
C3 Tongji University; Tongji University; Microsoft; Microsoft Research
   Asia; Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Wu, J (corresponding author), Tongji Univ, Key Lab, Minist Educ Embedded Syst & Serv Comp, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
EM 1310518@tongji.edu.cn; wujun@tongji.edu.cn; hao.cui@live.com;
   ddzhang@tongji.edu.cn; chong.luo@microsoft.com; fengwu@ustc.edu.cn
RI Wu, Feng/KCY-3017-2024
FU National Science Foundation China [61390513, 61631017, 61502341,
   61571329]; National Science and Technology Support Plan [2012BAH15F03]
FX This work was supported in part by the National Science Foundation China
   under Grants 61390513, 61631017, 61502341, and 61571329, and in part by
   the National Science and Technology Support Plan under Grant
   2012BAH15F03.
CR [Anonymous], 2011, P 45 ANN C INF SCI S
   [Anonymous], 36101 3GPP TS
   Chen LL, 2006, PROC SPIE, V6077, DOI 10.1117/12.657435
   Chernov N, 2004, J COMPLEXITY, V20, P484, DOI 10.1016/j.jco.2004.01.004
   Cui H, 2014, IEEE INFOCOM SER, P73, DOI 10.1109/INFOCOM.2014.6847926
   Cui Hao, 2013, P ACM INT C MOD AN S, P273
   Duel-Hallen A, 2000, IEEE SIGNAL PROC MAG, V17, P62, DOI 10.1109/79.841729
   Fan XP, 2012, IEEE DATA COMPR CONF, P199, DOI 10.1109/DCC.2012.27
   Fan XM, 2012, IEEE NUCL SCI CONF R, P1
   Fujihashi T, 2018, IEEE T MULTIMEDIA, V20, P473, DOI 10.1109/TMM.2017.2743984
   Gastpar M, 2003, IEEE T INFORM THEORY, V49, P1147, DOI 10.1109/TIT.2003.810631
   GOBLICK TJ, 1965, IEEE T INFORM THEORY, V11, P558, DOI 10.1109/TIT.1965.1053821
   Gong MX, 2005, IEEE ICC, P3401
   He DL, 2017, IEEE T MULTIMEDIA, V19, P1894, DOI 10.1109/TMM.2017.2686703
   He DL, 2015, IEEE T MULTIMEDIA, V17, P1658, DOI 10.1109/TMM.2015.2451956
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P970, DOI 10.1109/TCSVT.2002.805511
   He ZF, 2014, MOB COMPUT COMMUN RE, V18, P14, DOI 10.1145/2508478.2508485
   Huang YS, 2009, IEEE T MULTIMEDIA, V11, P1072, DOI 10.1109/TMM.2009.2026085
   Jakubczak S., 2011, PROC MOBICOM, P289
   LEE KH, 1976, IEEE T COMMUN, V24, P1283
   Li CL, 2014, IEEE T CIRC SYST VID, V24, P1170, DOI 10.1109/TCSVT.2014.2302517
   Li X, 2009, IEEE T CIRC SYST VID, V19, P193, DOI 10.1109/TCSVT.2008.2009255
   Liu XL, 2014, IEEE T MULTIMEDIA, V16, P2038, DOI 10.1109/TMM.2014.2331616
   Pudlewski S, 2013, IEEE T MULTIMEDIA, V15, P2072, DOI 10.1109/TMM.2013.2280245
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Wiegand T, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P542, DOI 10.1109/ICIP.2001.958171
   Zhang Y, 2007, IEEE T MULTIMEDIA, V9, P445, DOI 10.1109/TMM.2006.887989
NR 30
TC 9
Z9 10
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3097
EP 3110
DI 10.1109/TMM.2018.2823903
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800018
DA 2024-07-18
ER

PT J
AU Guo, GJ
   Wang, HZ
   Shen, CH
   Yan, Y
   Liao, HYM
AF Guo, Guanjun
   Wang, Hanzi
   Shen, Chunhua
   Yan, Yan
   Liao, Hong-Yuan Mark
TI Automatic Image Cropping for Visual Aesthetic Enhancement Using Deep
   Neural Networks and Cascaded Regression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image cropping; cascaded cropping regression; convolutional neural
   network; random-ferns regressor
AB Despite recent progress, computational visual aesthetic is still challenging. Image cropping, which refers to the removal of unwanted scene areas, is an important step to improve the aesthetic quality of an image. However, it is challenging to evaluate whether cropping leads to aesthetically pleasing results because the assessment is typically subjective. In this paper, we propose a novel cascaded cropping regression (CCR) method to perform image cropping by learning the knowledge from professional photographers. The proposed CCR method improves the convergence speed of the cascaded method, which directly uses random-ferns regressors. In addition, a two-step learning strategy is proposed and used in the CCR method to address the problem of lacking labelled cropping data. Specifically, a deep convolutional neural network (CNN) classifier is first trained on large-scale visual aesthetic datasets. The deep CNN model is then designed to extract features from several image cropping datasets, upon which the cropping bounding boxes are predicted by the proposed CCR method. Experimental results on public image cropping datasets demonstrate that the proposed method significantly outperforms several state-of-the-art image cropping methods.
C1 [Guo, Guanjun; Wang, Hanzi; Yan, Yan] Xiamen Univ, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Peoples R China.
   [Guo, Guanjun; Wang, Hanzi; Yan, Yan] Xiamen Univ, Sch Informat Sci & Engn, Xiamen 361005, Peoples R China.
   [Shen, Chunhua] Univ Adelaide, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia.
   [Shen, Chunhua] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
   [Liao, Hong-Yuan Mark] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
C3 Xiamen University; Xiamen University; University of Adelaide; University
   of Adelaide; Academia Sinica - Taiwan
RP Wang, HZ (corresponding author), Xiamen Univ, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Peoples R China.; Wang, HZ (corresponding author), Xiamen Univ, Sch Informat Sci & Engn, Xiamen 361005, Peoples R China.
EM gjguo@stu.xmu.edu.cn; hanzi.wang@xmu.edu.cn;
   chunhua.shen@adelaide.edu.au; yanyan@xmu.edu.cn; liao@iis.sinica.edu.tw
RI wang, handong/HLH-5739-2023; wang, hao/HSE-7975-2023; Wang,
   Han/GPW-9809-2022; Liao, Hong-Yuan Mark/AAQ-5514-2021
OI Shen, Chunhua/0000-0002-8648-8718
FU National Natural Science Foundation of China [U1605252, 61472334,
   61571379]; Natural Science Foundation of Fujian Province of China
   [2017J01127]
FX This work was supported by the National Natural Science Foundation of
   China under Grants U1605252, 61472334 and 61571379, and by the Natural
   Science Foundation of Fujian Province of China under Grant 2017J01127.
CR [Anonymous], 2017, INT J ADV MANUF TECH
   [Anonymous], P INT C CONS EL
   [Anonymous], 2010, ACM MULTIMEDIA
   Breiman L, 1998, BIOMETRICS, DOI [10.1201/9781315139470, DOI 10.2307/2530946]
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Collobert R, 2011, BIGLEARN NIPS WORKSH, P1
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Duffy N, 2002, MACH LEARN, V47, P153, DOI 10.1023/A:1013685603443
   Fang C, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1105, DOI 10.1145/2647868.2654979
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Gao F, 2015, IEEE T NEUR NET LEAR, V26, P2275, DOI 10.1109/TNNLS.2014.2377181
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Koffka Kurt, 2013, PRINCIPLES GESTALT P
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laurentini A, 2014, COMPUT VIS IMAGE UND, V125, P184, DOI 10.1016/j.cviu.2014.04.006
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Özuysal M, 2010, IEEE T PATTERN ANAL, V32, P448, DOI 10.1109/TPAMI.2009.23
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Stentiford F., 2007, Proceedings of the International Conference on Computer Vision Systems, P1
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Vikram TN, 2012, PATTERN RECOGN, V45, P3114, DOI 10.1016/j.patcog.2012.02.009
   Wang KN, 2014, KNOWL-BASED SYST, V71, P290, DOI 10.1016/j.knosys.2014.08.003
   Yan JZ, 2015, INT J COMPUT VISION, V114, P74, DOI 10.1007/s11263-015-0801-5
   Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   Yu J, 2017, IEEE T CYBERNETICS, V47, P4014, DOI 10.1109/TCYB.2016.2591583
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
NR 41
TC 46
Z9 51
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2073
EP 2085
DI 10.1109/TMM.2018.2794262
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lu, Z
   Ramakrishnan, S
   Zhu, XQ
AF Lu, Zheng
   Ramakrishnan, Sangeeta
   Zhu, Xiaoqing
TI Exploiting Video Quality Information With Lightweight Network
   Coordination for HTTP-Based Adaptive Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HTTP-based adaptive video streaming; rate adaptation algorithm;
   quality-aware rate adaptation; combinatorial optimization; lightweight
   network coordination
ID OPTIMIZATION; ADAPTATION
AB This paper investigates how video-quality information can he exploited by HTTP-based adaptive streaming clients in their rate adaptation schemes. We also seek to answer the question: How much network coordination is required to achieve quality fairness across multiple competing clients? To that end, we envision a loosely coupled architecture where a lightweight centralized coordinator complements multiple federated quality-aware clients. Instead of actively programming network bandwidth allocation to each client, in our proposed architecture, the central coordinator simply collects quality and buffer level information from individual clients and publishes the aggregate quality statistics as reference. Such global reference statistics help to guide each federated client to self-tune its aggressiveness in its quality-aware adaptation scheme. Testhed-based evaluations show that such a lightweight approach is effective in reaping most of the performance gains previously achieved by a centralized joint optimization scheme. Compared to both state-of-the-art rate-based clients and independent quality-aware clients, our proposed federated quality-aware scheme can save up to 50% of total bandwidth while maintaining comparable aggregate video quality across all clients.
C1 [Lu, Zheng; Ramakrishnan, Sangeeta; Zhu, Xiaoqing] Cisco Syst, Chief Technol & Architecture Off, San Jose, CA 95134 USA.
C3 Cisco Systems Inc
RP Zhu, XQ (corresponding author), Cisco Syst, Chief Technol & Architecture Off, San Jose, CA 95134 USA.
EM zhelu3@cisco.com; rsangeet@cisco.com; xiaoqzhu@cisco.com
CR [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], 2016, PROC 7 INT C MULTIME
   [Anonymous], 2015, P IFIP NETW C
   [Anonymous], 2013, P IEEE 20 INT PACK V
   [Anonymous], EURASIP J SIGNAL PRO
   [Anonymous], P INT C WIR COMM MOB
   Aronco SD, 2016, IEEE INT SYM MULTIM, P113, DOI [10.1109/ISM.2016.0030, 10.1109/ISM.2016.107]
   Bokani A, 2015, IEEE T MULTIMEDIA, V17, P2297, DOI 10.1109/TMM.2015.2494458
   Bouten N, 2014, IEEE T MULTIMEDIA, V16, P2281, DOI 10.1109/TMM.2014.2362856
   Chen C, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2337277
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   De Cicco L., 2011, P 2 ANN ACM C MULTIM, P145
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   El Essaili A., 2013, 2013 IEEE International Conference on Communications (ICC), P2480, DOI 10.1109/ICC.2013.6654905
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   Houdaille R., 2012, P 3 MULTIMEDIA SYSTE, P1
   Hu SH, 2014, IEEE GLOB COMM CONF, P1336, DOI 10.1109/GLOCOM.2014.7036993
   Huang T.Y., 2013, P 2013 ACM SIGCOMM W, P9, DOI [DOI 10.1145/2491172, DOI 10.1145/2491172.2491179]
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Joseph V, 2014, IEEE INFOCOM SER, P82, DOI 10.1109/INFOCOM.2014.6847927
   Keimel C, 2010, INT CONF ACOUST SPEE, P2442, DOI 10.1109/ICASSP.2010.5496299
   Kun Ma, 2011, 2011 7th International Conference on Next Generation Web Services Practices, P1, DOI 10.1109/NWeSP.2011.6088144
   Li Z., 2014, Proceedings of the 5th ACM Multimedia Systems Conference, MMSys '14, P248
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Lu Z, 2016, IEEE INT SYM MULTIM, P99, DOI [10.1109/ISM.2016.78, 10.1109/ISM.2016.0027]
   Miller K, 2015, IEEE T MULTIMEDIA, V17, P1309, DOI 10.1109/TMM.2015.2441002
   Ramakrishnan S, 2016, INT J MULTIMED DATA, V7, P22, DOI 10.4018/IJMDEM.2016100102
   Ramakrishnan S, 2015, IEEE INT SYM MULTIM, P120, DOI 10.1109/ISM.2015.53
   Sani Y, 2015, IEEE INT SYM MULTIM, P89, DOI 10.1109/ISM.2015.65
   Schulzrinne H., 1998, 2326 RFC
   Soundararajan R, 2013, IEEE T CIRC SYST VID, V23, P684, DOI 10.1109/TCSVT.2012.2214933
   Tian G., 2012, P 8 INT C EM NETW EX, P109
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zhu XQ, 2013, IEEE INT WORKSH MULT, P230, DOI 10.1109/MMSP.2013.6659293
NR 36
TC 9
Z9 9
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1848
EP 1863
DI 10.1109/TMM.2017.2772802
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100020
DA 2024-07-18
ER

PT J
AU Bahirat, K
   Raghuraman, S
   Prabhakaran, B
AF Bahirat, Kanchan
   Raghuraman, Suraj
   Prabhakaran, Balakrishnan
TI Real-Time, Curvature-Sensitive Surface Simplification Using Depth Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D tele-presence; 3D tele-immersion; visual quality; meshing; GPU;
   shader; virtual reality
ID MESH; TRIANGULATION; ALGORITHM; ERROR
AB With the rising popularity of handheld virtual reality (VR) devices and depth sensing RGB-D cameras, a variety of VR applications merging these two technologies has been suggested. However, immersive quality of experience in such VR applications is constrained mainly by the large data size and the hardware limitations to handle it. The depth data captured by RGB-D cameras provide a dense sampling of the surface, resulting in a high-poly mesh, which is difficult to be rendered on handheld VR devices due to their limited processing power. To improve the immersive VR experience, a sparse approximation of the depth data is needed. Traditional mesh and point cloud simplification methods are iterative and so are unsuitable for real-time applications. In this paper, we introduce a depth-image-based approach that is capable of generating a good quality sparse mesh for visualization in real time. We propose a curvature-sensitive surface simplification-CS3 operator that assigns an importance measure to each point in the depth image, based on the local curvature. Further, it applies an importance-order-based restrictive sampling to generate a sparse representation that retains the overall shape as well as the finer features of the object. We also modify the 2-D sweep-line-based constrained Delaunay triangulation to generate 3-D meshes from the sparse point sampling obtained using CS3. In addition, the proposed approach preserves key surface properties, such as texture coordinates and materials. We used three different datasets containing dense 3-D models with and without texture, which are scanned using various sensors to validate and compare the robustness, real-time performance, and accuracy of the proposed method over existing approaches. Based on the experimental results, we show that the proposed CS3 operator and modified 2-D sweep-line-based triangulation generate sparse meshes from depth image in real time, performing significantly faster than current state-of-the-art methods while maintaining similar visual quality.
C1 [Bahirat, Kanchan; Raghuraman, Suraj; Prabhakaran, Balakrishnan] Univ Texas Dallas, Dept Comp Sci & Engn, Richardson, TX 75080 USA.
C3 University of Texas System; University of Texas Dallas
RP Bahirat, K (corresponding author), Univ Texas Dallas, Dept Comp Sci & Engn, Richardson, TX 75080 USA.
EM kab131130@utdallas.edu; suraj@utdallas.edu; bprabhakaran@utdallas.edu
FU National Science Foundation [1012975]; U.S. Army Research Office
   [W911NF-17-1-0299]
FX This work was supported in part by the National Science Foundation under
   Grant 1012975 and in part by the U.S. Army Research Office under Grant
   W911NF-17-1-0299.
CR Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   Arkworks, FLIGHT NEW DIM VR OC
   BESL PJ, 1986, COMPUT VISION GRAPH, V33, P33, DOI 10.1016/0734-189X(86)90220-3
   Cacciola F., 2016, CGAL User and Reference Manual, V4
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen Y, 2016, IEEE T MULTIMEDIA, V18, P576, DOI 10.1109/TMM.2016.2525010
   Cheng I, 2006, IEEE T MULTIMEDIA, V8, P550, DOI 10.1109/TMM.2006.870722
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cignoni P, 1998, COMPUT GRAPH-UK, V22, P37, DOI 10.1016/S0097-8493(97)00082-4
   Cignoni P., 2008, EUR IT C
   Curless B., STANFORD 3D SCANNING
   Domiter V, 2008, INT J GEOGR INF SCI, V22, P449, DOI 10.1080/13658810701492241
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312
   Gautier J, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P81, DOI 10.1109/PCS.2012.6213291
   Heckbert PS, 1999, COMP GEOM-THEOR APPL, V14, P49, DOI 10.1016/S0925-7721(99)00030-9
   Hoppe H., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P59, DOI 10.1109/VISUAL.1999.809869
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hou WG, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0120151
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Lindstrom P, 1998, VISUALIZATION '98, PROCEEDINGS, P279, DOI 10.1109/VISUAL.1998.745314
   Linsen L., 2001, POINT CLOUD REPRESEN
   Luebke DP, 2001, IEEE COMPUT GRAPH, V21, P24, DOI 10.1109/38.920624
   Mekuria R, 2014, IEEE T MULTIMEDIA, V16, P1809, DOI 10.1109/TMM.2014.2331919
   Moenning C., 2003, Proc. Int. Conf. on Visualization, P1027
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Pruett C., SQUEEZING PERFORMANC
   Raghuraman S., 2013, ACM MULTIMEDIA, P721
   Romanoni A., 2016, P 2016 IEEE WINT C A, P1
   Rossignac J., 1993, Multi-Resolution 3D Approximations for Rendering Complex Scenes
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   SIMPSON RB, 1994, APPL NUMER MATH, V14, P183, DOI 10.1016/0168-9274(94)90025-6
   Sung-Yeol Kim, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P117
   Tech G., 2012, ITU T SG INT TELECOM, V16
   Unity3D, VERT LIM UNITY3D
   Wang K, 2012, COMPUT GRAPH-UK, V36, P808, DOI 10.1016/j.cag.2012.06.004
   Wasenmuller O., 2016, APPL COMP VIS WACV 2, P1, DOI DOI 10.1109/WACV.2016.7477636
   Wu W., 2011, P 19 ACM INT C MULTI, P13
NR 39
TC 3
Z9 4
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1489
EP 1498
DI 10.1109/TMM.2017.2769447
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400016
OA hybrid
DA 2024-07-18
ER

PT J
AU Chen, QB
   Sun, HB
   Zheng, NN
AF Chen, Qiubo
   Sun, Hongbin
   Zheng, Nanning
TI Worst Case Driven Display Frame Compression for Energy-Efficient
   Ultra-HD Display Processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Display frame compression; ultra-high definition (HD) video; energy
   efficiency; memory bandwidth; very large scale integration (VLSI)
   architecture
ID RECOMPRESSION ALGORITHM; VIDEO
AB Display frame compression is an effective technique to address the challenge of external memory access in ultrahigh definition video display system. Nevertheless, previously proposed display frame compression designs are inadequate in terms of either energy efficiency or throughput. This paper aims to exploit the algorithm and very large scale integration (VLSI) architecture of a worst case driven display frame compression. By using a prediction-and-compression framework and a semi-fixed length coding scheme, the proposed design can achieve the much better balance between compression efficiency and throughput, and substantially reduce the bandwidth requirement and energy consumption of external memory system in the meanwhile. Extensive experiments demonstrate that the proposed display frame compression achieves 5.7-dB peak signal-to-noise ratio improvement, 3.1% compression ratio reduction, 3 x throughput, and 66.4% hardware cost saving, compared with the best previous work. In addition, the proposed VLSI design can support the throughput of 4 K x 2 K@60 Hz and reduce at least 17.6% energy consumption of external memory system by exploiting dynamic voltage and frequency scaling, compared with conventional display frame compression works.
C1 [Chen, Qiubo; Sun, Hongbin; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University
RP Sun, HB (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
EM chenqiubo@stu.xjtu.edu.cn; hsun@mail.xjtu.edu.cn;
   nnzheng@mail.xjtu.edu.cn
FU National Science and Technology Major Project of China
   [2013ZX01033001-001]; National Natural Science Foundation of China
   [61774125]; Joint Foundation of Ministry of Education of China
   [6141A02033303]
FX This work was supported jointly by the National Science and Technology
   Major Project of China (No. 2013ZX01033001-001), the National Natural
   Science Foundation of China (No. 61774125), and the Joint Foundation of
   Ministry of Education of China (No. 6141A02033303).
CR [Anonymous], 2007, TECHNICAL NOTE CALCU
   Caviedes JE, 2012, P IEEE, V100, P872, DOI 10.1109/JPROC.2011.2182072
   Chen QB, 2015, IEEE INT SYMP CIRC S, P2357, DOI 10.1109/ISCAS.2015.7169157
   Côté G, 1999, IEEE T IMAGE PROCESS, V8, P1451, DOI 10.1109/83.791971
   David H., 2011, Proceedings of the 8th International Conference on Autonomic Computing, ICAC 2011, Karlsruhe, Germany, June 14-18, 2011, P31
   Deng Q., 2011, P ASPLOS JUN, P255
   Guo L, 2014, IEEE T MULTIMEDIA, V16, P2323, DOI 10.1109/TMM.2014.2350256
   Hwang YT, 2015, IEEE T CIRC SYST VID, V25, P674, DOI 10.1109/TCSVT.2014.2355691
   Kim J, 2010, IEEE T CIRC SYST VID, V20, P848, DOI 10.1109/TCSVT.2010.2045923
   Kuo HC, 2012, IEEE T MULTIMEDIA, V14, P500, DOI 10.1109/TMM.2012.2191945
   Lee GG, 2014, IEEE J EM SEL TOP C, V4, P29, DOI 10.1109/JETCAS.2014.2298923
   Lee TY, 2003, IEEE T CIRC SYST VID, V13, P529, DOI 10.1109/TCSVT.2003.813425
   Mochizuki S, 2016, ISSCC DIG TECH PAP I, V59, P78, DOI 10.1109/ISSCC.2016.7417915
   Song L, 2013, INT WORK QUAL MULTIM, P34, DOI 10.1109/QoMEX.2013.6603201
   Tsai T.-H., IEEE T CIRCUITS SYST, V20, P1277
   Uchiyama M, 2009, IEEE ASIAN SOLID STA, P201, DOI 10.1109/ASSCC.2009.5357148
   Weinberger MJ, 2000, IEEE T IMAGE PROCESS, V9, P1309, DOI 10.1109/83.855427
   Weste N. H. E., 2011, CIRCUITS SYSTEMS PER
   Wu XL, 1997, IEEE T COMMUN, V45, P437, DOI 10.1109/26.585919
   Yang HT, 2009, 2009 IEEE/ACM/IFIP 7TH WORKSHOP ON EMBEDDED SYSTEMS FOR REAL-TIME MULTIMEDIA, P28, DOI 10.1109/ESTMED.2009.5336820
   Yng TLB, 2008, IEEE T CONSUM ELECTR, V54, P1453, DOI 10.1109/TCE.2008.4637640
   Zhang X, 2016, ADV SOC SCI EDUC HUM, V53, P1, DOI 10.1109/PSCC.2016.7540881
NR 22
TC 5
Z9 5
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1113
EP 1125
DI 10.1109/TMM.2017.2762004
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400008
DA 2024-07-18
ER

PT J
AU Wang, M
   Cheng, B
   Yuen, C
AF Wang, Ming
   Cheng, Bo
   Yuen, Chau
TI Joint Coding-Transmission Optimization for a Video Surveillance System
   With Multiple Cameras
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High definition; real time; surveillance video transmission system;
   throughput; video mosaicing; wide viewing angle
ID PARALLEL FRAMEWORK; INTERNET
AB Surveillance video has become an important multimedia application in recent years, with development trends toward wide viewing angles and high definition. At the same time, transmission of surveillance video over the Internet and storage in the cloud are becoming increasingly popular. However, it is extremely challenging to transfer surveillance video with both a wide viewing angle and a high resolution over the Internet because of 1) the tradeoff between a wide viewing angle and high-surveillance object resolution for a single camera, 2) the large throughput requirement, and 3) the fluctuating delay and bandwidth of the Internet. In this paper, we present a reference-frame-cache-based surveillance video transmission system (RSVTS), which delivers wide-viewing-angle and high-definition surveillance video over the Internet in real time using multiple rotatable cameras. First, we develop an efficient video mosaicing method for merging two (or more) surveillance videos together. Second, novel reference frame selection and update algorithms are proposed for the RSVTS encoder to reduce the amount of encoded data. Third, we implement a reference frame cache on both the sender and receiver sides to increase v ideo quality by increasing the probability of video decoding. We conduct a performance evaluation using a simulation system that integrates RSVTS and ns-3. Compared with the H.264/SVC and H.264/AVC schemes, RSVTS achieves appreciable improvements in enhancing the video peak signal-to-noise ratio and bandwidth conservation.
C1 [Wang, Ming; Cheng, Bo] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Yuen, Chau] Singapore Univ Technol & Design, Engn Prod Dev Pillar, Singapore 487372, Singapore.
C3 Beijing University of Posts & Telecommunications; Singapore University
   of Technology & Design
RP Cheng, B (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
EM wangming_bupt@bupt.edu.cn; chengbo@bupt.edu.cn; yuenchau@sutd.edu.sg
RI zhang, rui/HZI-0079-2023; Yuen, Chau/C-5493-2013
OI Yuen, Chau/0000-0002-9307-2120
FU National High-tech Research and Development Program of China (863
   Program) [2013AA102301]; National Research Foundation Singapore under
   its Interactive Digital Media (IDM) Strategic Research Programme
FX This research was supported in part by the National High-tech Research
   and Development Program of China (863 Program) under Grant 2013AA102301
   and in part by the National Research Foundation Singapore under its
   Interactive Digital Media (IDM) Strategic Research Programme.
CR [Anonymous], 2001, VCEGM33 ITUT SG16 Q
   [Anonymous], EVALVID 2 7
   [Anonymous], JOINT VIDEO TEAM JVT
   [Anonymous], ADV VID COD GEN AUD
   [Anonymous], IDC DIGITAL UNIVERSE
   [Anonymous], 2015, 2015 IEEE International Conference on Multimedia and Expo (ICME)
   [Anonymous], P INT C INT COMP EL
   Borgnat P, 2009, IEEE INFOCOM SER, P711, DOI 10.1109/INFCOM.2009.5061979
   Chen MJ, 2011, J REAL-TIME IMAGE PR, V6, P281, DOI 10.1007/s11554-010-0170-9
   Ghosh D, 2016, J VIS COMMUN IMAGE R, V34, P1, DOI 10.1016/j.jvcir.2015.10.014
   Kantarci A, 2010, MULTIMEDIA SYST, V16, P381, DOI 10.1007/s00530-010-0183-z
   Lee H, 2016, IEEE T MULTIMEDIA, V18, P2093, DOI 10.1109/TMM.2016.2595262
   Lin CH, 2013, IEEE T MULTIMEDIA, V15, P195, DOI 10.1109/TMM.2012.2225028
   Liu Y, 2016, IEEE T MULTIMEDIA, V18, P351, DOI 10.1109/TMM.2016.2514848
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo J., 2009, INT J IMAGE PROCESSI, V3, P143, DOI DOI 10.1007/S11270-006-2859-8
   Nguyen DV, 2013, 2013 INTERNATIONAL JOINT CONFERENCE ON AWARENESS SCIENCE AND TECHNOLOGY & UBI-MEDIA COMPUTING (ICAST-UMEDIA), P579, DOI 10.1109/ICAwST.2013.6765506
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Porikli F, 2013, IEEE SIGNAL PROC MAG, V30, P190, DOI 10.1109/MSP.2013.2241312
   Pudlewski S, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2342202
   Puri A, 2004, SIGNAL PROCESS-IMAGE, V19, P793, DOI 10.1016/j.image.2004.06.003
   Saini M, 2012, IEEE T MULTIMEDIA, V14, P555, DOI 10.1109/TMM.2012.2186957
   Salem F, 2013, IEEE T MULTIMEDIA, V15, P27, DOI 10.1109/TMM.2012.2225037
   Sanchez Y, 2012, SIGNAL PROCESS-IMAGE, V27, P329, DOI 10.1016/j.image.2011.10.002
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wang XG, 2013, PATTERN RECOGN LETT, V34, P3, DOI 10.1016/j.patrec.2012.07.005
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weigle MC, 2006, ACM SIGCOMM COMP COM, V36, P67, DOI 10.1145/1140086.1140094
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu DP, 2000, P IEEE, V88, P1855, DOI 10.1109/5.899055
   Wu JY, 2017, IEEE T MOBILE COMPUT, V16, P1090, DOI 10.1109/TMC.2016.2584049
   Wu JY, 2017, IEEE T CIRC SYST VID, V27, P32, DOI 10.1109/TCSVT.2016.2527398
   Wu JY, 2016, IEEE J SEL AREA COMM, V34, P2231, DOI 10.1109/JSAC.2016.2577178
   Wu JY, 2016, IEEE T MOBILE COMPUT, V15, P2345, DOI 10.1109/TMC.2015.2497238
   Wu JY, 2016, IEEE T COMMUN, V64, P2477, DOI 10.1109/TCOMM.2016.2553138
   Wu JY, 2015, IEEE T COMMUN, V63, P3584, DOI 10.1109/TCOMM.2015.2469296
   Xu C., 2006, PROC 14 ANN ACM INT, P221, DOI DOI 10.1145/1180639.1180699
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Yan CG, 2014, IEEE SIGNAL PROC LET, V21, P573, DOI 10.1109/LSP.2014.2310494
   Zatt B, 2010, IEEE IMAGE PROC, P3053, DOI 10.1109/ICIP.2010.5651700
   Zeng L, 2012, J COMPUT, V7, P218, DOI 10.4304/jcp.7.1.218-225
   Zhang S., 2012, Proceedings of the 2012 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC), P353, DOI 10.1109/ICSPCC.2012.6335739
   Zhang XG, 2014, IEEE T IMAGE PROCESS, V23, P769, DOI 10.1109/TIP.2013.2294549
   Zhang XG, 2013, IEEE T MULTIMEDIA, V15, P1769, DOI 10.1109/TMM.2013.2280117
NR 45
TC 24
Z9 24
U1 6
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 620
EP 633
DI 10.1109/TMM.2017.2748459
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500009
DA 2024-07-18
ER

PT J
AU Hwang, MJ
   Lee, J
   Lee, M
   Kang, HG
AF Hwang, Min-Jae
   Lee, JeeSok
   Lee, MiSuk
   Kang, Hong-Goo
TI SVD-Based Adaptive QIM Watermarking on Stereo Audio Signals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio watermarking; blind; data hiding; singular value decomposition
   (SVD); quantization index modulation (QIM); stereo
ID SPREAD-SPECTRUM WATERMARKING; SINGULAR-VALUE DECOMPOSITION; ROBUST
   WATERMARKING; MODULATION; MULTIMEDIA
AB This paper proposes a blind digital audio watermarking algorithm that utilizes the quantization index modulation (QIM) and the singular value decomposition (SVD) of stereo audio signals. Conventional SVD-based blind audio watermarking algorithms lack physical interpretation since the matrix construction method for the input matrix for SVD is heuristically defined. However, in the proposed approach, because the SVD is directly applied to the stereo input signals, the resulting decomposed elements convey a conceptually meaningful interpretation of the original audio signal. As the proposed approach effectively utilizes the ratio of singular values, the embedded watermark is highly imperceptible and robust against volumetric scaling attacks; most QIM-based watermarking schemes are weak to these types of attacks. Experimental results under well-known practical attacks, such as compressions, resampling, and various types of signal processing, confirm that the proposed algorithm performs well compared to conventional audio watermarking algorithms.
C1 [Hwang, Min-Jae; Lee, JeeSok; Kang, Hong-Goo] Yonsei Univ, Dept Elect & Elect, Seoul 120749, South Korea.
   [Lee, MiSuk] Elect & Telecommun Res Inst, Daejeon 305700, South Korea.
C3 Yonsei University; Electronics & Telecommunications Research Institute -
   Korea (ETRI)
RP Kang, HG (corresponding author), Yonsei Univ, Dept Elect & Elect, Seoul 120749, South Korea.
EM hmj234@dsp.yonsei.ac.kr; losoul85@dsp.yonsei.ac.kr; lms@etri.re.kr;
   hgkang@yonsei.ac.kr
RI Kang, Hong-Goo/G-8545-2012
FU Electronics and Telecommunications Research Institute Grant - Korean
   Government [15ZR1200]
FX This work was supported by the Electronics and Telecommunications
   Research Institute Grant funded by the Korean Government (15ZR1200, The
   Development of Active Audioprint 'technology to Enhance the Media
   Usage). The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Yi-Hsuan Yang.
CR Abd El-Samie FE, 2009, INT J SPEECH TECHNOL, V12, P27, DOI 10.1007/s10772-009-9056-2
   Al-Haj Ali, 2010, 2010 Fifth International Conference on Digital Information Management (ICDIM 2010), P525, DOI 10.1109/ICDIM.2010.5664651
   [Anonymous], P IEEE INT C AC SPEE
   [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], 2000, Digital Watermarking
   Bhat V, 2011, MULTIMED TOOLS APPL, V52, P369, DOI 10.1007/s11042-010-0515-1
   Bhat KV, 2010, DIGIT SIGNAL PROCESS, V20, P1547, DOI 10.1016/j.dsp.2010.02.006
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   CHENG S, 2002, ACOUST SPEECH SIG PR, P3728
   Cox I., 2001, Digital Watermarking
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Dhar PK, 2015, J INF SECUR APPL, V20, P74, DOI 10.1016/j.jisa.2014.10.007
   Elshazly AR, 2016, 2016 FOURTH INTERNATIONAL JAPAN-EGYPT CONFERENCE ON ELECTRONICS, COMMUNICATIONS AND COMPUTERS (JEC-ECC), P52, DOI 10.1109/JEC-ECC.2016.7518966
   Harner R. N., 1990, BRAIN TOPOGRAPHY, V3, P4347
   Hartung F, 1999, P IEEE, V87, P1079, DOI 10.1109/5.771066
   Hu H. T., 2014, EURASIP J ADV SIG PR, P316
   ITU, 2001, METH OBJ MEAS PERC A
   Kabal P., 2002, 2 MCGILL U DEP EL CO
   Kakkirala KR, 2014, 2014 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), VOLS 1-2, P180, DOI 10.1109/ICALIP.2014.7009782
   Kang XG, 2011, IEEE T MULTIMEDIA, V13, P181, DOI 10.1109/TMM.2010.2098850
   Katzenbeisser S., 2000, INFORM TECHNIQUES ST
   Kirovski Darko., 2001, LECT NOTES COMPUTER, V2137, P354
   KLEMA VC, 1980, IEEE T AUTOMAT CONTR, V25, P164, DOI 10.1109/TAC.1980.1102314
   Lalitha N.V., 2011, International Journal of Scientific Engineering Research, V2, P1
   Langelaar GC, 2000, IEEE SIGNAL PROC MAG, V17, P20, DOI 10.1109/79.879337
   Lei BY, 2012, SIGNAL PROCESS, V92, P1985, DOI 10.1016/j.sigpro.2011.12.021
   Malvar HS, 2003, IEEE T SIGNAL PROCES, V51, P898, DOI 10.1109/TSP.2003.809385
   Pérez-González F, 2005, IEEE T SIGNAL PROCES, V53, P3960, DOI 10.1109/TSP.2005.855407
   Potdar VA, 2005, 2005 3RD IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS (INDIN), P709
   Suresh G., 2012, 2012 International Conference on Devices, Circuits and Systems (ICDCS 2012), P177, DOI 10.1109/ICDCSyst.2012.6188699
   Wang J, 2011, SIGNAL PROCESS, V91, P1693, DOI 10.1016/j.sigpro.2011.01.014
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P311, DOI 10.1109/TMM.2013.2291658
   WEARE BC, 1976, J PHYS OCEANOGR, V6, P671, DOI 10.1175/1520-0485(1976)006<0671:EOAOPS>2.0.CO;2
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhao XB, 2011, CHINA BREW, V8, P1
   Zhou B., 2004, CHINESE J IMAGE GRAP, V9, P506
   Zhu XS, 2014, IEEE T MULTIMEDIA, V16, P1888, DOI 10.1109/TMM.2014.2340695
NR 37
TC 52
Z9 57
U1 1
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 45
EP 54
DI 10.1109/TMM.2017.2721642
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700004
DA 2024-07-18
ER

PT J
AU Fan, ZW
   Jiang, TT
   Huang, TJ
AF Fan, Zhiwei
   Jiang, Tingting
   Huang, Tiejun
TI Active Sampling Exploiting Reliable Informativeness for Subjective Image
   Quality Assessment Based on Pairwise Comparison
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active sampling; pairwise comparison; quality of experience; subjective
   image quality assessment (IQA)
ID BORDA COUNT; RANKING
AB Subjective image quality assessment (IQA) based on pairwise comparison (PC) overcome the shortcomings of IQA based on category rating, such as an ambiguous scale definition. However, the testing scale of PC tests can be very large, as the number of image pairs for comparison is a quadratic form of the number of images. To conduct PC tests on a large-scale image set with limited budget, an active sampling strategy to reduce testing scale is required. The conventional active sampling strategies usually select the most informative sample and assume that any image pair's correct label can be obtained from any subjects who are attentive. However, this is not true for IQA, because of human visual system's limitation. If two images are similar, their difference can be too subtle for some subjects to perceive. It means that it takes subjects more effort to obtain correct preference labels of two similar images, and that it is even impossible to obtain the correct preference labels of two images that are too similar. To address this issue, we study the reliability of preference labels. Based on the combination of reliability and informativeness, we design a new active sampling framework. It not only considers the informativeness, but also adjusts the effort spent on an image pair according to its ambiguity. Experiments show that this adjustment can effectively improve the performance of sampling strategies only based on informativeness. Besides, the proposed method is expected to be applied to more general subjective tests based on PC beyond IQA.
C1 [Fan, Zhiwei; Jiang, Tingting; Huang, Tiejun] Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China.
   [Fan, Zhiwei; Jiang, Tingting; Huang, Tiejun] Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
C3 Peking University
RP Jiang, TT (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China.
EM fanzw@pku.edu.cn; ttjiang@pku.edu.cn; tjhuang@pku.edu.cn
RI Huang, Tiejun/D-6161-2011
OI Jiang, Tingting/0000-0002-5372-0656
FU National Basic Research Program of China (973 Program) [2015CB351803];
   Natural Science Foundation of China [61572042, 61390514, 61421062,
   61210005, 61527084]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2015CB351803, and in part by the
   Natural Science Foundation of China under Grant 61572042, Grant
   61390514, Grant 61421062, Grant 61210005, and Grant 61527084. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Abdulmotaleb El Saddik.
   (Corresponding author: Zhiwei Fan.)
CR Ailon N, 2012, J MACH LEARN RES, V13, P137
   Alon N, 2006, SIAM J DISCRETE MATH, V20, P137, DOI 10.1137/050623905
   [Anonymous], 2006, P ACMSIGKDD INT C KN
   [Anonymous], SUB VID QUAL ASS MET
   [Anonymous], 1994, COMPUT STAT DATA AN, DOI DOI 10.1016/0167-9473(96)90015-8
   [Anonymous], 2009, P 3 ACM C REC SYST A, DOI DOI 10.1145/1639714.1639720
   [Anonymous], 2011, ADV NEURAL INFORM PR
   [Anonymous], P 5 IEEE INT C DAT M
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Brotherton MD, 2006, IEICE T FUND ELECTR, VE89A, P2920, DOI 10.1093/ietfec/e89-a.11.2920
   Burges C., 2005, ICML, P89
   Chapelle O, 2010, INFORM RETRIEVAL, V13, P201, DOI 10.1007/s10791-009-9109-9
   Chapelle O, 2007, NEURAL COMPUT, V19, P1155, DOI 10.1162/neco.2007.19.5.1155
   Chen KT, 2010, IEEE NETWORK, V24, P28, DOI 10.1109/MNET.2010.5430141
   Chen KT, 2009, MATH COMPUT SCI ENG, P491, DOI 10.1145/1631272.1631339
   Chen XF, 2013, KEY ENG MATER, V538, P193, DOI 10.4028/www.scientific.net/KEM.538.193
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   Hua G, 2013, IEEE I CONF COMP VIS, P1209, DOI 10.1109/ICCV.2013.153
   Jain P, 2009, PROC CVPR IEEE, P762, DOI 10.1109/CVPRW.2009.5206651
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x
   Joshi Ajay J., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2372, DOI 10.1109/CVPRW.2009.5206627
   Kapoor A, 2009, IEEE I CONF COMP VIS, P1058, DOI 10.1109/ICCV.2009.5459392
   Le Callet P., 2005, Subjective quality assessment irccyn/ivc database
   Lee JS, 2013, IEEE INT SYMP CIRC S, P1099, DOI 10.1109/ISCAS.2013.6572042
   Lee JS, 2014, IEEE T MULTIMEDIA, V16, P564, DOI 10.1109/TMM.2013.2292590
   Lee JS, 2011, IEEE T MULTIMEDIA, V13, P882, DOI 10.1109/TMM.2011.2157333
   Liang L, 2014, PROC CVPR IEEE, P208, DOI 10.1109/CVPR.2014.34
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Long CJ, 2013, IEEE I CONF COMP VIS, P3000, DOI 10.1109/ICCV.2013.373
   Negahban S., 2012, Advances in neural information processing systems, V25, P2474
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Reilly B, 2002, INT POLIT SCI REV, V23, P355, DOI 10.1177/0192512102023004002
   Ribeiro F, 2011, INT CONF ACOUST SPEE, P2416
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Shen YJ, 2016, IEEE IMAGE PROC, P2077, DOI 10.1109/ICIP.2016.7532724
   Siahaan E, 2016, IEEE T MULTIMEDIA, V18, P1338, DOI 10.1109/TMM.2016.2559942
   Stern M.K., 2010, The Corsini Encyclopedia of Psychology
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Vijayanarasimhan S, 2011, PROC CVPR IEEE, P1449, DOI 10.1109/CVPR.2011.5995430
   Xu Q., 2012, P 20 ACM INT C MULT, P359, DOI DOI 10.1145/2393347.2393400
   Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924
   Yan R, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P516
   Ye P, 2014, PROC CVPR IEEE, P4249, DOI 10.1109/CVPR.2014.541
   Zhang Y, 2014, IEEE T KNOWL DATA EN, V26, P83, DOI 10.1109/TKDE.2012.250
   Zhu JB, 2010, IEEE T AUDIO SPEECH, V18, P1323, DOI 10.1109/TASL.2009.2033421
NR 47
TC 6
Z9 6
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2720
EP 2735
DI 10.1109/TMM.2017.2711860
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200007
DA 2024-07-18
ER

PT J
AU Shi, Y
   Wang, KZ
   Chen, CY
   Xu, L
   Lin, L
AF Shi, Yukai
   Wang, Keze
   Chen, Chongyu
   Xu, Li
   Lin, Liang
TI Structure-Preserving Image Super-Resolution via Contextualized Multitask
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional network; context learning; multitask learning;
   structure-preserving image super-resolution (SR)
ID INTERPOLATION
AB Single-image super-resolution (SR), which refers to reconstructing a higher resolution image from the observed low-resolution (LR) image, has received substantial attention due to its tremendous application potentials. Despite the breakthroughs of recently proposed SR methods using convolutional neural networks, their generated results usually lack of preserving structural (high-frequency) details. In this paper, regarding global boundary context and residual context as complimentary information for enhancing structural details in image restoration, we develop a contextualized multitask learning framework to address the SR problem. Specifically, our method first extracts convolutional features from the input LR image and applies one deconvolutional module to interpolate the LR feature maps in a content-adaptive way. Then, the resulting feature maps are fed into two branched subnetworks. On several standard benchmarks (e.g., Set5, Set14, and BSD200), our extensive evaluations demonstrate the effectiveness of our SR method on achieving both higher restoration quality and computational efficiency compared with several state-of-the-art SR approaches.
C1 [Shi, Yukai; Wang, Keze; Chen, Chongyu; Lin, Liang] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510275, Guangdong, Peoples R China.
   [Wang, Keze] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
   [Xu, Li] Sense Time Grp Ltd, Beijing 100084, Peoples R China.
C3 Sun Yat Sen University; Hong Kong Polytechnic University
RP Chen, CY (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510275, Guangdong, Peoples R China.
EM shiyk3@mail2.sysu.edu.cn; kezewang@gmail.com;
   chenchy47@mail.sysu.edu.cn; xuli@sensetime.com; linliang@ieee.org
RI L, J/JEF-9564-2023; Lin, L/HKO-8213-2023; Wang, Keze/Z-3605-2019; LU,
   LU/JEZ-4760-2023; l, j/JVZ-8480-2024; Chen, Chongyu/IWM-0773-2023; Lin,
   Liang/IQR-8601-2023; zhang, cl/JDW-6549-2023; l, j/HNC-5728-2023
OI Lin, Liang/0000-0003-2248-3755; Wang, Keze/0000-0002-7817-8306
FU State Key Development Program [2016YFB1001004]; National Natural Science
   Foundation of China [61602533]; Fundamental Research Funds for the
   Central Universities; Guangdong Science and Technology Program
   [2015B010128009]
FX This work was supported in part by the State Key Development Program
   under Grant 2016YFB1001004, in part by the National Natural Science
   Foundation of China under Grant 61602533, in part by the Fundamental
   Research Funds for the Central Universities, and in part by Guangdong
   Science and Technology Program under Grant 2015B010128009. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Dong Xu. (Corresponding author: Chongyu Chen.)
CR [Anonymous], 2015, arXiv
   [Anonymous], 2008, ACM T GRAPHIC, DOI DOI 10.1145/1409060.1409106
   [Anonymous], ARXIV161002915
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cao QX, 2017, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2017.180
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chetlur S., 2014, ARXIV14100759
   Chu JY, 2008, INT CONF SIGN PROCES, P1027, DOI 10.1109/ICOSP.2008.4697303
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84
   Everingham M, 2012, PASCAL VISUAL OBJECT
   Gregor K, 2010, P 27 INT C INT C MAC, P399
   Gu SH, 2015, IEEE I CONF COMP VIS, P1823, DOI 10.1109/ICCV.2015.212
   He HL, 2016, IEEE T MED IMAGING, V35, P812, DOI 10.1109/TMI.2015.2497159
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu YT, 2016, IEEE T IMAGE PROCESS, V25, P4091, DOI 10.1109/TIP.2016.2580942
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Jain V., 2009, ADV NEURAL INFORM PR, P769, DOI DOI 10.5555/2981780.2981876
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kumar N, 2016, IEEE T MULTIMEDIA, V18, P1504, DOI 10.1109/TMM.2016.2571625
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163
   Liang YD, 2015, IEEE IMAGE PROC, P2110, DOI 10.1109/ICIP.2015.7351173
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Michaeli T, 2013, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2013.121
   Ren JS., 2015, ADV NEURAL INFORM PR, V1, P901
   Romano Y, 2017, IEEE T COMPUT IMAG, V3, P110, DOI 10.1109/TCI.2016.2629284
   Sajjadi Mehdi SM, 2016, ARXIV161207919
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi Y, 2016, 2016 IEEE INT C MULT, P1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   van der Walt S. J., 2012, ARXIV12103404
   Wang K., 2016, P ACM INT C MULT, P271
   Wang KZ, 2016, PROC CVPR IEEE, P2138, DOI 10.1109/CVPR.2016.235
   Wang KZ, 2015, IEEE T IMAGE PROCESS, V24, P3019, DOI 10.1109/TIP.2015.2432712
   Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Xie Junyuan, 2012, ADV NEURAL INFORM PR, P341, DOI [DOI 10.5555/2999134.2999173, DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605]
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang S H., 2011, 2011 International Workshop on Multi-Platform/Multi-Sensor Remote Sensing and Mapping, P1
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yu J, 2017, IEEE T INF FOREN SEC, V12, P1005, DOI 10.1109/TIFS.2016.2636090
   Zeng K, 2017, IEEE T CYBERNETICS, V47, P27, DOI 10.1109/TCYB.2015.2501373
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
NR 54
TC 43
Z9 44
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2804
EP 2815
DI 10.1109/TMM.2017.2711263
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU You, QZ
   Pang, R
   Cao, LL
   Luo, JB
AF You, Quanzeng
   Pang, Ran
   Cao, Liangliang
   Luo, Jiebo
TI Image-Based Appraisal of Real Estate Properties
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep neural networks; real estate; visual content analysis
AB Real estate appraisal, which is the process of estimating the price for real estate properties, is crucial for both buyers and sellers as the basis for negotiation and transaction. Traditionally, the repeat sales model has been widely adopted to estimate real estate prices. However, it depends on the design and calculation of a complex economic-related index, which is challenging to estimate accurately. Today, real estate brokers provide easy access to detailed online information on real estate properties to their clients. We are interested in estimating the real estate price from these large amounts of easily accessed data. In particular, we analyze the prediction power of online house pictures, which is one of the key factors for online users to make a potential visiting decision. The development of robust computer vision algorithms makes the analysis of visual content possible. In this paper, we employ a recurrent neural network to predict real estate prices using the state-of-the-art visual features. The experimental results indicate that our model outperforms several other state-of-the-art baseline algorithms in terms of both mean absolute error and mean absolute percentage error.
C1 [You, Quanzeng; Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14623 USA.
   [Pang, Ran] PayPaL, San Jose, CA 95131 USA.
   [Cao, Liangliang] Columbia Univ, Elect Engn & Comp Sci Dept, New York, NY 10013 USA.
   [Cao, Liangliang] customerserviceAI, New York, NY 10013 USA.
C3 University of Rochester; PayPal Holdings, Inc.; Columbia University
RP You, QZ (corresponding author), Univ Rochester, Dept Comp Sci, Rochester, NY 14623 USA.
EM qyou@cs.rochester.edu; pangrr89@gmail.com; liangliang.cao@gmail.com;
   jluo@cs.rochester.edu
RI You, Quanzeng/X-9917-2019; Luo, Jiebo/AAI-7549-2020
OI You, Quanzeng/0000-0003-3608-0607; Luo, Jiebo/0000-0002-4516-9729
CR Anderson ST, 2006, REG SCI URBAN ECON, V36, P773, DOI 10.1016/j.regsciurbeco.2006.03.007
   [Anonymous], 1995, The Journal of Real Estate Research, v
   [Anonymous], 2011, Public transit's impact on housing costs: a review of the literature
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2012, TECH REP
   [Anonymous], P 4 ANN PAC RIM REAL
   [Anonymous], P INT REAL EST SOC C
   [Anonymous], 2011, 22 INT JT C ART INT, DOI 10.5555/2283516.2283603
   [Anonymous], 2012, MOMENTUM
   [Anonymous], RUN COMP MARKET ANAL
   BAILEY MJ, 1963, J AM STAT ASSOC, V58, P933
   BEJA A, 1980, J FINANC, V35, P235, DOI 10.2307/2327380
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bengio Yoshua, 2014, P INT C LEARN REPR
   Di W, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P633, DOI 10.1145/2556195.2556226
   Fu YJ, 2014, IEEE DATA MINING, P120, DOI 10.1109/ICDM.2014.18
   Fu YJ, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1047, DOI 10.1145/2623330.2623675
   Gers F., 2001, NEURAL COMPUT
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jin Xin, 2010, P 18 ACM INT C MULT, P1235, DOI [10.1145/1873951.1874196, DOI 10.1145/1873951.1874196]
   Kempa O, 2011, LECT NOTES ARTIF INT, V6592, P323, DOI 10.1007/978-3-642-20042-7_33
   Kontrimas V, 2011, APPL SOFT COMPUT, V11, P443, DOI 10.1016/j.asoc.2009.12.003
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lasota T, 2011, LECT NOTES COMPUT SC, V6936, P17, DOI 10.1007/978-3-642-23878-9_3
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   MEESE R, 1991, AREUEA J, V19, P308
   Nagaraja CH, 2011, ANN APPL STAT, V5, P124, DOI 10.1214/10-AOAS380
   Nguyen N., 2001, J. Real. Estate Res, V22, P313, DOI DOI 10.1080/10835547.2001.12091068
   Pascanu R., 2013, INT C MACH LEARN, P1310
   Perozzi B., 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang FT, 1997, J HOUS ECON, V6, P93, DOI 10.1006/jhec.1997.0209
   You QZ, 2016, SIGNAL PROCESS, V124, P45, DOI 10.1016/j.sigpro.2015.10.032
   You QZ, 2015, IEEE T MULTIMEDIA, V17, P2271, DOI 10.1109/TMM.2015.2487863
   Zhou BL, 2014, ADV NEUR IN, V27
NR 43
TC 34
Z9 42
U1 2
U2 57
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2751
EP 2759
DI 10.1109/TMM.2017.2710804
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chadha, A
   Andreopoulos, Y
AF Chadha, Aaron
   Andreopoulos, Yiannis
TI Voronoi-Based Compact Image Descriptors: Efficient Region-of-Interest
   Retrieval With VLAD and Deep-Learning-Based Descriptors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks (CNN); deep learning; vector of locally
   aggregated descriptors; visual queries; Voronoi partitioning
ID PRODUCT QUANTIZATION
AB We investigate the problem of image retrieval based on visual queries when the latter comprise arbitrary regions-of-interest (ROI) rather than entire images. Our proposal is a compact image descriptor that combines the state-of-the-art in content-based descriptor extraction with a multilevel, Voronoi-based spatial partitioning of each dataset image. The proposed multilevel Voronoi-based encoding uses a spatial hierarchical K-means over interest-point locations, and computes a content-based descriptor over each cell. In order to reduce the matching complexity with minimal or no sacrifice in retrieval performance: 1) we utilize the tree structure of the spatial hierarchical K-means to perform top-to-bottom pruning for local similarity maxima; 2) we propose a new image similarity score that combines relevant information from all partition levels into a single measure for similarity; 3) we combine our proposal with a novel and efficient approach for optimal bit allocation within quantized descriptor representations. By deriving both a Voronoi-based VLAD descriptor (called Fast-VVLAD) and a Voronoi-based deep convolutional neural network (CNN) descriptor (called Fast-VDCNN), we demonstrate that our Voronoi-based framework is agnostic to the descriptor basis, and can easily be slotted into existing frameworks. Via a range of ROI queries in two standard datasets, it is shown that the Voronoi-based descriptors achieve comparable or higher mean average precision against conventional grid-based spatial search, while offering more than twofold reduction in complexity. Finally, beyond ROI queries, we show that Voronoi partitioning improves the geometric invariance of compact CNN descriptors, thereby resulting in competitive performance to the current state-of-the-art on whole image retrieval.
C1 [Chadha, Aaron; Andreopoulos, Yiannis] UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
C3 University of London; University College London
RP Chadha, A (corresponding author), UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
EM aaron.chadha.14@ucl.ac.uk; andreopoulos@ucl.ac.uk
OI Andreopoulos, Ioannis/0000-0002-2714-4800
FU Innovate UK [101932]; EPSRC [EP/M00113X/1]; EPSRC CASE award - BAFTA;
   Royal Commission from the Exhibition of 1851; EPSRC [EP/P02243X/1,
   EP/M00113X/1] Funding Source: UKRI; Innovate UK [101932] Funding Source:
   UKRI
FX This work was supported in part by Innovate UK (Project VideoClarity,
   101932) and EPSRC (Project EP/M00113X/1 and EPSRC CASE award
   co-sponsored by BAFTA). The work of A. Chadha was supported in part by a
   fellowship from the Royal Commission from the Exhibition of 1851. This
   work was presented in part at the International Conference on Computer
   Vision Systems, Copenhagen, Denmark, July 2015. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Qi Tian.
CR Andreopoulos Y, 2002, DSP 2002: 14TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING PROCEEDINGS, VOLS 1 AND 2, P501, DOI 10.1109/ICDSP.2002.1028137
   Andreopoulos Y, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P330, DOI 10.1109/ICIP.2001.958118
   Andreopoulos Y, 2008, IEEE T SIGNAL PROCES, V56, P140, DOI 10.1109/TSP.2007.906727
   [Anonymous], 2004, P 20 ACM S COMP
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2015, P INT C MACH LEARN
   [Anonymous], 2009, NEURIPS
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], P IEEE 4 INT WORKSH
   [Anonymous], PROC CVPR IEEE
   [Anonymous], NEAREST NEIGHBOR MET
   [Anonymous], 2012, VECTOR QUANTIZATION
   [Anonymous], 2014, P ADV NEUR INF PROC
   [Anonymous], ACOUST SPEECH SIG PR
   [Anonymous], 2013, ARXIV PREPRINT ARXIV, DOI [DOI 10.48550/ARXIV.1312.6229, 10.48550/arXiv.1312.6229]
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Azizpour H, 2016, IEEE T PATTERN ANAL, V38, P1790, DOI 10.1109/TPAMI.2015.2500224
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bai S, 2016, LECT NOTES COMPUT SC, V9906, P592, DOI 10.1007/978-3-319-46475-6_37
   Bishop C, 2007, RECOGNITION PATTERN
   Brandt J, 2010, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2010.5539852
   Chadha A, 2015, LECT NOTES COMPUT SC, V9163, P218, DOI 10.1007/978-3-319-20904-3_21
   Chandrasekhar V, 2016, SIGNAL PROCESS, V128, P426, DOI 10.1016/j.sigpro.2016.05.021
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Chum O., 2008, PROC IEEE INT C COMP, P1
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2010, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2010.5539997
   Cover Thomas M, 1999, Elements of information theory
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fergus R, 2003, PROC CVPR IEEE, P264
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   González-Díaz I, 2014, IEEE T MULTIMEDIA, V16, P169, DOI 10.1109/TMM.2013.2286083
   Gray R. M., 1984, IEEE ASSP Magazine, V1, P4, DOI 10.1109/MASSP.1984.1162229
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Ji TX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1005, DOI 10.1145/2647868.2655018
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kontorinis N, 2009, IEEE T CIRC SYST VID, V19, P1000, DOI 10.1109/TCSVT.2009.2020256
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li Y, 2015, IEEE I CONF COMP VIS, P3819, DOI 10.1109/ICCV.2015.435
   Liu JJ, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2501654.2501658
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu P, 2015, IEEE T MULTIMEDIA, V17, P1297, DOI 10.1109/TMM.2015.2441004
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9
   Paulin M, 2017, INT J COMPUT VISION, V121, P149, DOI 10.1007/s11263-016-0924-3
   Perronnin F., 2007, PROC IEEE INT C COMP, P1
   Perronnin F, 2015, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR.2015.7298998
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
   Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song DJ, 2015, IEEE I CONF COMP VIS, P1922, DOI 10.1109/ICCV.2015.223
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Torralba A., 2008, PROC IEEE INT C COMP, P1
   Xu H, 2011, IEEE I CONF COMP VIS, P1631, DOI 10.1109/ICCV.2011.6126424
   Xu X, 2015, INT J PROD RES, V53, P7005, DOI 10.1080/00207543.2014.937013
   Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng L, 2015, IEEE T MULTIMEDIA, V17, P648, DOI 10.1109/TMM.2015.2408563
   Zhong ZY, 2015, IEEE T MULTIMEDIA, V17, P1391, DOI 10.1109/TMM.2015.2446201
   Zhou RH, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P342, DOI 10.1109/VCIP.2014.7051576
NR 71
TC 23
Z9 25
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1596
EP 1608
DI 10.1109/TMM.2017.2673415
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Akbari, A
   Trocan, M
   Granado, B
AF Akbari, Ali
   Trocan, Maria
   Granado, Bertrand
TI Sparse Recovery-Based Error Concealment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Error concealment; sparse recovery; robust image transmission; packet
   loss
ID CODED IMAGES; TRANSMISSION
AB Image and video transmission over heterogeneous networks may encounter packet loss due to the channel impairments, leading to quality degradation of the received image. In this paper, a novel robust image transmission system is proposed by casting the error concealment challenge into a sparse recovery framework. To this purpose, a robust encoder is carefully designed in order to mitigate the negative effects of the packet loss. After wavelet decomposition, a quadtree structure of the wavelet coefficients is used to rearrange them into independent partitions. Random linear combinations of coefficients for each partition are then adopted to provide a high error recovery capability. This linear process coupled with a simple packetization process introduces more robustness and error resilience into the transmission system. At the receiver side, the tree-sparse structure of the wavelet coefficients is explicitly exploited in order to model the error recovery problem as a sparse recovery framework. This is achieved by adaptation of a wellknown iterative sparse reconstruction algorithm to the defined tree structure built in the wavelet domain. Compared with the state-of-the-art error concealment algorithms, experimental results show that the proposed method has better reconstruction performance in terms of objective and subjective evaluations over a range of packet loss rates, and ensure that a high-quality image can be recovered for the high packet loss scenarios.
C1 [Akbari, Ali; Trocan, Maria] Inst Super Elect Paris, Dept Signal Images & Telecommun, F-75006 Paris, France.
   [Granado, Bertrand] Lab Informat Paris 6, F-75005 Paris, France.
C3 Sorbonne Universite
RP Akbari, A (corresponding author), Inst Super Elect Paris, Dept Signal Images & Telecommun, F-75006 Paris, France.
EM ali.akbari@isep.fr; maria.trocan@isep.fr; Bertrand.Granado@lip6.fr
RI Granado, Bertrand/AAE-4923-2020
OI Granado, Bertrand/0000-0002-9667-9737
CR [Anonymous], 2015, INT WORKSHOP COMPUTA
   Antonini M, 1992, IEEE T IMAGE PROCESS, V1, P205, DOI 10.1109/83.136597
   Asheri H, 2012, IEEE T CONSUM ELECTR, V58, P880, DOI 10.1109/TCE.2012.6311331
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Blumensath T, 2008, J FOURIER ANAL APPL, V14, P629, DOI 10.1007/s00041-008-9035-z
   Candes E. J., 1992, IEEE T INFORM THEORY, V52, P5406
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candès EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Chen C, 2014, IEEE T SIGNAL PROCES, V62, P2803, DOI 10.1109/TSP.2014.2318138
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   DEVORE RA, 1992, IEEE T INFORM THEORY, V38, P719, DOI 10.1109/18.119733
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Kim J, 2003, IEEE T IMAGE PROCESS, V12, P121, DOI 10.1109/TIP.2003.809006
   Kim M, 2008, IEEE T CONSUM ELECTR, V54, P1811, DOI 10.1109/TCE.2008.4711239
   Kim T, 2001, IEEE T CIRC SYST VID, V11, P1022, DOI 10.1109/76.946519
   Koloda Jan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1976, DOI 10.1109/ICASSP.2014.6853944
   Koloda J, 2014, IEEE T MULTIMEDIA, V16, P1729, DOI 10.1109/TMM.2014.2330314
   Koloda J, 2013, IEEE T MULTIMEDIA, V15, P957, DOI 10.1109/TMM.2013.2238524
   Koloda J, 2013, CIRC SYST SIGNAL PR, V32, P815, DOI 10.1007/s00034-012-9504-0
   Laska JN, 2011, APPL COMPUT HARMON A, V31, P429, DOI 10.1016/j.acha.2011.02.002
   Le Gall D., 1988, ICASSP 88: 1988 International Conference on Acoustics, Speech, and Signal Processing (Cat. No.88CH2561-9), P761, DOI 10.1109/ICASSP.1988.196696
   Li X, 2002, IEEE T CIRC SYST VID, V12, P857, DOI 10.1109/TCSVT.2002.804882
   Liu J, 2015, IEEE T CIRC SYST VID, V25, P353, DOI 10.1109/TCSVT.2014.2359145
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Romberg J, 2008, IEEE SIGNAL PROC MAG, V25, P14, DOI 10.1109/MSP.2007.914729
   Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Shirani S, 1999, INT CONF ACOUST SPEE, P3117, DOI 10.1109/ICASSP.1999.757501
   Song D, 2008, J VIS COMMUN IMAGE R, V19, P311, DOI 10.1016/j.jvcir.2008.03.003
   SUN HF, 1995, IEEE T IMAGE PROCESS, V4, P470, DOI 10.1109/83.370675
   Unser M, 2003, IEEE T IMAGE PROCESS, V12, P1080, DOI 10.1109/TIP.2003.812329
   Wah BW, 2000, INTERNATIONAL SYMPOSIUM ON MULTIMEDIA SOFTWARE ENGINEERING, PROCEEDINGS, P17, DOI 10.1109/MMSE.2000.897185
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang SH, 2007, IEEE T CIRC SYST VID, V17, P558, DOI 10.1109/TCSVT.2007.895343
   Zhai GT, 2010, IEEE T CIRC SYST VID, V20, P1224, DOI 10.1109/TCSVT.2010.2057019
   Zhai GT, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P621, DOI 10.1109/ICME.2008.4607511
   Zhang RF, 2004, IEEE T CONSUM ELECTR, V50, P335, DOI 10.1109/TCE.2004.1277882
NR 37
TC 23
Z9 23
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1339
EP 1350
DI 10.1109/TMM.2017.2662203
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400018
DA 2024-07-18
ER

PT J
AU Lin, SH
   Pal, R
   Wang, BC
   Golubchik, L
AF Lin, Sung-Han
   Pal, Ranjan
   Wang, Bo-Chun
   Golubchik, Leana
TI On Market-Driven Hybrid-P2P Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Peer-to-peer (P2P); quality of service (QoS); market equilibrium; market
   efficiency
ID SERVICE DIFFERENTIATION; P2P; PERFORMANCE; MULTIMEDIA
AB Consistent (pause-free) quality of service is required in peer-to-peer (P2P) video streaming systems. In this paper, we aim to eliminate the problem of playback pauses in such systems via the use of positive incentives for peers to contribute high upload rates. We model our problem as a market, where the market stakeholders consist of multiple content providers, advertisement providers, and network peers; the positive incentives for peers in the market are reduced advertisement (ad) viewing durations. From a system design perspective, one of our primary goals is to compute the market equilibria that include appropriate ad viewing durations, offering sufficient incentives for network peers to continue contributing. Our simulation-based studies demonstrate that we mitigate the "playback pause" problem for peers by up to 80% as compared to existing approaches, generate sufficient utility for advertisers to be part of the market, and enable content providers to achieve their desired utility by providing sufficient incentives for all peers to stay in the system without violating ad provider agreements.
C1 [Lin, Sung-Han; Pal, Ranjan; Wang, Bo-Chun; Golubchik, Leana] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Lin, SH (corresponding author), Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
EM sunghan@usc.edu; rpal@usc.edu; bochunwa@usc.edu; leana@usc.edu
RI Lin, Sung-Han/ABC-9193-2021
OI Lin, Sung-Han/0000-0001-5666-9393
FU NSF [CNS-1423505]; Zumberge Research Award
FX This work was supported in part by the NSF under Grant CNS-1423505, and
   in part by the Zumberge Research Award. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Lingfen Sun.
CR [Anonymous], 2004, P 5 ACM C EL COMM
   Balachandran A., 2002, Performance Evaluation Review, V30, P195, DOI 10.1145/511399.511359
   Bocek T, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON PARALLEL & DISTRIBUTED PROCESSING, VOLS 1-8, P653
   Brown GW., 1951, Activity analysis of production and allocation, V13
   Chen Y, 2010, IEEE T CIRC SYST VID, V20, P1346, DOI 10.1109/TCSVT.2010.2077490
   Choe Y.R., 2007, MULTIMEDIA 07 P 15 I, P117
   Cobb C. W., 1928, American Economic Review, P139
   D'Acunto L., 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P89, DOI 10.1109/ISM.2010.22
   DASGUPTA P, 1986, REV ECON STUD, V53, P1, DOI 10.2307/2297588
   DEBREU G, 1952, P NATL ACAD SCI USA, V38, P886, DOI 10.1073/pnas.38.10.886
   FAN K, 1952, P NATL ACAD SCI USA, V38, P121, DOI 10.1073/pnas.38.2.121
   Fiandrotti A, 2014, IEEE T MULTIMEDIA, V16, P521, DOI 10.1109/TMM.2013.2285518
   Fudenberg D., 1991, GAME THEORY, V393, P1
   GLICKSBERG IL, 1952, P AM MATH SOC, V3, P170, DOI 10.2307/2032478
   Gurler CG, 2012, IEEE IMAGE PROC, P2253, DOI 10.1109/ICIP.2012.6467344
   Hamra A. A., 2007, CORR
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Heusse M, 2003, IEEE INFOCOM SER, P836
   Huang G., 2007, P ACM SIGCOMM WORKSH, P22
   Huang SL, 2016, IEEE T MULTIMEDIA, V18, P752, DOI 10.1109/TMM.2016.2530411
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Krishnan S. S., 2012, ACM C INT MEAS IMC, P2001
   Legout A, 2007, PERF E R SI, V35, P301
   Lin SH, 2015, 2015 IEEE 23RD INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS), P125, DOI 10.1109/IWQoS.2015.7404722
   Lin WS, 2009, IEEE T MULTIMEDIA, V11, P396, DOI 10.1109/TMM.2009.2012915
   Liu ZY, 2008, I C NETWORK PROTOCOL, P94, DOI 10.1109/ICNP.2008.4697028
   Liu Zhengye., 2007, Proceedings of the 2007 Workshop on Peer-to-Peer Streaming and IP-TV (P2P-TV), P311
   Ma RTB, 2006, IEEE ACM T NETWORK, V14, P978, DOI 10.1109/TNET.2006.882904
   Magli E, 2013, IEEE T MULTIMEDIA, V15, P1195, DOI 10.1109/TMM.2013.2241415
   Mas-Colell A., 1995, MICROECONOMIC THEORY, V1
   Mokhtarian K, 2013, IEEE T MULTIMEDIA, V15, P181, DOI 10.1109/TMM.2012.2225042
   Mol JJD, 2006, SIXTH IEEE INTERNATIONAL CONFERENCE ON PEER-TO-PEER COMPUTING, PROCEEDINGS, P275, DOI 10.1109/P2P.2006.45
   NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48
   ParandehGheibi A, 2011, IEEE J SEL AREA COMM, V29, P1064, DOI 10.1109/JSAC.2011.110516
   Park Hyunggon., 2010, Streaming Media Architectures, Techniques, and Applications: Recent Advances, P262
   Parvez KN, 2008, PERF E R SI, V36, P301, DOI 10.1145/1384529.1375492
   Poo Kuan Hoong, 2008, WSEAS Transactions on Communications, V7, P33
   Qiu Tianhao., 2007, Journal of Internet Engineering, V1, P61
   Rodrigues PL, 2012, P 7 IB C INF SYST TE, P1
   Rückert J, 2012, LECT NOTES COMPUT SC, V7290, P1, DOI 10.1007/978-3-642-30054-7_1
   Schmidt Wayne., How Much TV Commercial Length has Grown over the Years
   Tan G, 2008, IEEE T PARALL DISTR, V19, P940, DOI 10.1109/TPDS.2007.70778
   Vlavianos Aggelos., 2006, Proceedings IEEE INFOCOM 2006. 25TH IEEE International Conference on Computer Communications, P1
   Vu L., PPLIVE PROJECT
   Wang A., 2012, 3 MULTIMEDIA SYSTEMS, P77, DOI [10.1145/2155555.2155567[15]Z, DOI 10.1145/2155555.2155567[15]Z]
   Wu D, 2013, IEEE T CIRC SYST VID, V23, P1029, DOI 10.1109/TCSVT.2013.2249020
   Xu YD, 2012, LECT NOTES COMPUT SC, V7290, P343, DOI 10.1007/978-3-642-30054-7_27
   Xu YD, 2012, IEEE INFOCOM SER, P1826, DOI 10.1109/INFCOM.2012.6195557
   Yang Y., 2010, J NANOMATER, V2010, P1, DOI DOI 10.1371/J0URNAL.P0NE.0013427
   Yin H., 2009, Proceedings of ACM International Conference on Multimedia, P25
   Zhang G, 2015, IEEE T MULTIMEDIA, V17, P229, DOI 10.1109/TMM.2014.2383617
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
   Zhanikeev M, 2013, 2013 FIRST INTERNATIONAL SYMPOSIUM ON COMPUTING AND NETWORKING (CANDAR), P461, DOI 10.1109/CANDAR.2013.81
NR 53
TC 8
Z9 9
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 984
EP 998
DI 10.1109/TMM.2016.2644868
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000008
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, HL
   Tian, T
   Ma, M
   Wu, J
AF Wang, Hanli
   Tian, Tao
   Ma, Ming
   Wu, Jun
TI Joint Compression of Near-Duplicate Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Near-duplicate video (NDV); video coding; intervideo prediction;
   graph-based video grouping; joint compression
ID PHOTO ALBUM COMPRESSION; SCALE; RETRIEVAL; MAPREDUCE
AB The expanding social network and multimedia technologies encourage more and more people to store and transmit information in visual format, such as image and video. However, the cost of this convenience brings about a shock to traditional video severs and exposes them under the risk of overloading. In the huge volume of online videos, there are a large amount of near-duplicate videos (NDVs). Although quite a number of research work have been proposed to detect NDVs, little research effort is made to compress these NDVs in a more effective manner than independent video compression. In this study, we make an in-depth exploration of the data redundancy of NDVs and propose a video analysis and coding framework to jointly compress NDVs. In order to employ the proposed NDV analysis and coding framework, a graph-based similar video grouping method and a number of preprocessing functions are designed to explore the correlation of visual information among NDVs and thus suit the requirement of joint video coding. Experimental results verify that the proposed NDV analysis and coding framework is able to effectively compress NDVs and thus save video data storage.
C1 [Wang, Hanli] Tongji Univ, Minist Educ, Dept Comp Sci & Technol, Shanghai 200092, Peoples R China.
   Tongji Univ, Minist Educ, Key Lab Embedded Syst & Serv Comp, Shanghai 200092, Peoples R China.
C3 Tongji University; Tongji University
RP Wang, HL (corresponding author), Tongji Univ, Minist Educ, Dept Comp Sci & Technol, Shanghai 200092, Peoples R China.
EM hanliwang@tongji.edu.cn; 1989tiantao@tongji.edu.cn;
   2012mingma@tongji.edu.cn; wujun@tongji.edu.cn
RI Wang, Hanli/G-5111-2014; Wang, Hanli/K-5717-2019
OI Wang, Hanli/0000-0002-9999-4871; Wang, Hanli/0000-0002-9999-4871
FU National Natural Science Foundation of China [61622115, 61472281];
   Program for Professor of Special Appointment (Eastern Scholar) at
   Shanghai Institutions of Higher Learning [GZ2015005]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61622115 and Grant 61472281, and in part
   by the Program for Professor of Special Appointment (Eastern Scholar) at
   Shanghai Institutions of Higher Learning under Grant GZ2015005. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Shu-Ching Chen.
CR [Anonymous], 2015, EURASIP J WIRELESS C
   [Anonymous], P NIST TREVCID WORKS
   [Anonymous], 2004, PROC 3 INT C MOBILE
   Au O., 2012, 2012 International Conference on Audio, Language and Image Processing (ICALIP 2012). Proceedings, P84, DOI 10.1109/ICALIP.2012.6376591
   Bhattacharya S, 2014, IEEE T MULTIMEDIA, V16, P686, DOI 10.1109/TMM.2014.2300833
   Bjontegaard G., 2001, ITUTSG16Q6VCEG
   Chen CP, 2004, IEEE IMAGE PROC, P1289
   Cheng XG, 2011, IEEE T MULTIMEDIA, V13, P1333, DOI 10.1109/TMM.2011.2167222
   Cherubini Mauro., 2009, Proceedings of the 17th ACM International Conference on Multimedia, number April in MM '09, P35, DOI DOI 10.1145/1631272.1631280
   Chou CL, 2015, IEEE T MULTIMEDIA, V17, P382, DOI 10.1109/TMM.2015.2391674
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hanli Wang, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P518, DOI 10.1007/978-3-319-04114-8_44
   Hua XS, 2004, IEEE IMAGE PROC, P685
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Joint Collaborative Team for 3DV, 2014, 3D HTM SOFTW PLATF
   Kasutani E, 2001, IEEE IMAGE PROC, P674, DOI 10.1109/ICIP.2001.959135
   Kim C, 2005, IEEE T CIRC SYST VID, V15, P127
   Ling YG, 2014, IEEE INT SYMP CIRC S, P1917, DOI 10.1109/ISCAS.2014.6865535
   Liu JJ, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2501654.2501658
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mahfoodh AT, 2013, PICT COD SYMP, P21, DOI 10.1109/PCS.2013.6737673
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Shi ZB, 2014, IEEE J EM SEL TOP C, V4, P17, DOI 10.1109/JETCAS.2014.2298291
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wang HL, 2015, MULTIMED TOOLS APPL, V74, P10515, DOI 10.1007/s11042-014-2185-x
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu A. G., 2007, P ACM MM, P218
   Wu H, 2016, IEEE T IMAGE PROCESS, V25, P2684, DOI 10.1109/TIP.2016.2551366
   Wu X, 2009, IEEE T MULTIMEDIA, V11, P196, DOI 10.1109/TMM.2008.2009673
   Wu ZP, 2014, INT J MULTIMED INF R, V3, P1, DOI 10.1007/s13735-013-0049-1
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
   Zou RB, 2013, IEEE INT SYMP CIRC S, P1428, DOI 10.1109/ISCAS.2013.6572124
NR 40
TC 9
Z9 9
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 908
EP 920
DI 10.1109/TMM.2016.2645398
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000002
DA 2024-07-18
ER

PT J
AU Song, ZC
   Liu, SG
AF Song, Zhi-Chao
   Liu, Shi-Guang
TI Sufficient Image Appearance Transfer Combining Color and Texture
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Appearance transfer; color; image editing; point set expansion; texture
AB Traditional color transfer methods can achieve satisfactory results for transferring the color style from a reference image to a source image, provided that the source image shares the similar color mood with the reference image. However, color transfer solutions are always sensitive to color category, which cannot generate natural results when the contents of the reference image and the source image are different, e.g., a lush tree in the reference image and a bare tree in the source image. In this situation, it is insufficient only through color transfer to transfer the appearance from the reference image to the source image only through color transfer, since other information such as texture should also be considered. To obtain sufficient appearance transfer results, we propose a new image appearance transfer method combining both color and texture features. Given a source image and a reference image, our method starts with feature detection and matching between the source image and the reference image. Then, we design a new method for expanding feature point sets to get texture transfer mark (TTM) and color transfer mark (CTM). TTM and CTM will guide texture transfer and color transfer, respectively. We demonstrate our appearance transfer algorithm between quantities of images and compare with results of existing methods. Experiment results show that given only a single reference image, our approach can produce more sufficient appearance transfer results than the state-of-the-art algorithms.
C1 [Song, Zhi-Chao; Liu, Shi-Guang] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
   [Liu, Shi-Guang] Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.; Liu, SG (corresponding author), Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300072, Peoples R China.
EM songzc@tju.edu.cn; lsg@tju.edu.cn
FU Natural Science Foundation of China [61170118, 61672375]
FX work was supported by the Natural Science Foundation of China under
   Grant 61170118 and Grant 61672375. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Shu-Ching Chen. (Corresponding author: Shiguang Liu.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bhattacharyya A, 1942, B CALCUTTA MATH SOC, V35, P99
   BONNEEL N., 2010, P VIS MOD VIS WOKSH, P87
   DARABI S, 2012, ACM T GRAPHIC, V31, DOI DOI 10.1145/2185520.2185578
   Diamanti O, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766906
   Dong WM, 2016, IEEE T VIS COMPUT GR, V22, P1088, DOI 10.1109/TVCG.2015.2440255
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Faridul H.S., 2014, Eurographics State of the Art Reports, DOI 10.2312/egst.20141035
   Grundland M, 2005, PROC SPIE, P610, DOI 10.1117/12.596953
   Gupta R. K., 2012, P 20 ACM INT C MULT, P369, DOI DOI 10.1145/2393347.2393402
   HaCohen Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964965
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hu S. M., 2013, ACM T GRAPHIC, V32, P2504
   Hwang Y, 2014, PROC CVPR IEEE, P3342, DOI 10.1109/CVPR.2014.427
   IIZUKA S, 2016, ACM T GRAPHIC, V35, P1, DOI DOI 10.1145/2897824.2925974
   Johnson MK, 2011, IEEE T VIS COMPUT GR, V17, P1273, DOI 10.1109/TVCG.2010.233
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Nguyen RMH, 2014, COMPUT GRAPH FORUM, V33, P319, DOI 10.1111/cgf.12500
   Okura F, 2015, COMPUT GRAPH FORUM, V34, P53, DOI 10.1111/cgf.12678
   Oskam T, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P49, DOI 10.1109/3DIMPVT.2012.36
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Piti'e F., 2007, PROC 4 IEEE EUROPEAN, P1
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Shih Y., 2014, ACM T GRAPHIC, V32, P1
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Shrivastava N, 2014, VISUAL COMPUT, V30, P1223, DOI 10.1007/s00371-013-0887-0
   Tai YW, 2007, IEEE T PATTERN ANAL, V29, P1520, DOI 10.1109/TPAMI.2007.1168
   Wen CL, 2008, COMPUT GRAPH FORUM, V27, P1765, DOI 10.1111/j.1467-8659.2008.01321.x
   Xiao Y, 2013, IEEE T MULTIMEDIA, V15, P549, DOI 10.1109/TMM.2012.2233725
   Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296
   Yi-Chin Huang, 2005, 13th Annual ACM International Conference on Multimedia, P351, DOI 10.1145/1101149.1101223
   Zhang W, 2013, IEEE T MULTIMEDIA, V15, P1594, DOI 10.1109/TMM.2013.2265675
NR 37
TC 31
Z9 34
U1 3
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 702
EP 711
DI 10.1109/TMM.2016.2631123
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500003
DA 2024-07-18
ER

PT J
AU Zheng, YF
   Yuan, XL
   Wang, XY
   Jiang, JH
   Wang, C
   Gui, XL
AF Zheng, Yifeng
   Yuan, Xingliang
   Wang, Xinyu
   Jiang, Jinghua
   Wang, Cong
   Gui, Xiaolin
TI Toward Encrypted Cloud Media Center With Secure Deduplication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud media center; layer-level deduplication; scalable video coding
   (SVC); secure deduplication
ID VIDEO; EFFICIENT; SCHEME
AB The explosive growth of multimedia contents, especially videos, is pushing forward the paradigm of cloud-based media hosting today. However, the wide attacking surface of the public cloud and the growing security awareness from the society are both calling for data encryption before outsourcing to cloud. Under the circumstance of encrypted videos, how to still preserve all the service benefits of cloud media center remains to be fully explored. In this paper, we present a secure system architecture design as our initial effort toward this direction, which bridges together the advancements of video coding techniques and secure deduplication. Our design enables the cloud with the crucial deduplication functionality to completely eliminate the extra storage and bandwidth cost, which would have been incurred by hosting encrypted videos from different entities. The design is also carefully tailored to the scalable video coding (SVC) techniques to support heterogeneous networks and devices for high-quality adaptive video dissemination. We show fully functional system implementations with structure-aware encryption design and structure-aware deduplication strategies that are both completely compliant with the video format in SVC. Extensive security analysis and experiments via our prototype deployed on Azure cloud platform show the practicality of the design. Our work can also be easily extended to support other media applications that employ media files with scalable structures.
C1 [Zheng, Yifeng; Yuan, Xingliang; Wang, Xinyu; Wang, Cong] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Zheng, Yifeng; Yuan, Xingliang; Wang, Xinyu; Wang, Cong] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 518057, Peoples R China.
   [Jiang, Jinghua; Gui, Xiaolin] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
   [Jiang, Jinghua] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 City University of Hong Kong; Shenzhen Research Institute, City
   University of Hong Kong; City University of Hong Kong; Xi'an Jiaotong
   University; City University of Hong Kong
RP Zheng, YF (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM yifeng.zheng@my.cityu.edu.hk; xyuancs@gmail.com; xy.w@my.cityu.edu.hk;
   jjinghua2-c@my.cityu.edu.hk; congwang@cityu.edu.hk;
   xlgui@mail.xjtu.edu.cn
RI Yuan, Xingliang/Z-4306-2019
OI Yuan, Xingliang/0000-0002-3701-4946; Zheng, Yifeng/0000-0001-7852-6051;
   Wang, Cong/0000-0003-0547-315X
FU Research Grants Council of Hong Kong [CityU 138513, CityU 11276816];
   National Natural Science Foundation of China [61572412, 61472316];
   Innovation and Technology Commission of Hong Kong [ITS/307/15]; AWS
   Education Research Grant; Science and Technology Project of Shaanxi
   Province [2016ZDJC-05]
FX This work was supported in part by the Research Grants Council of Hong
   Kong under Project CityU 138513 and Project CityU 11276816, in part by
   the National Natural Science Foundation of China under Project 61572412
   and Project 61472316, in part by the Innovation and Technology
   Commission of Hong Kong under ITF Project ITS/307/15, in part by an AWS
   Education Research Grant, and in part by the Science and Technology
   Project of Shaanxi Province under Grant 2016ZDJC-05.
CR Abadi M, 2013, LECT NOTES COMPUT SC, V8042, P374, DOI 10.1007/978-3-642-40041-4_21
   Anderson PO, 2010, PHARMACY INFORMATICS, P1, DOI 10.1145/1852658.1852665
   [Anonymous], RTP PAYL FORM SCAL V
   [Anonymous], VER INT DOWNL FIL
   [Anonymous], 2014, DATA BREACH B STAPLE
   [Anonymous], 2013, IEEE T COMPUT, DOI DOI 10.1109/TC.2011.245
   [Anonymous], SCALABLE STREAMING
   [Anonymous], 2008, Proceedings of the 4th ACM international workshop on Storage security and survivability
   [Anonymous], 2016, AKAMAI Q4 2016 STATE
   [Anonymous], P 3 USENIX C HOT TOP
   Bellare M, 2003, J CRYPTOL, V16, P185, DOI 10.1007/s00145-002-0120-1
   Bellare M, 2013, 22 USENIX SEC S USEN, P179
   Bellare M, 2013, LECT NOTES COMPUT SC, V7881, P296, DOI 10.1007/978-3-642-38348-9_18
   Blestel M., 2010, ACM MM'10, P1463
   Chen F, 2015, IEEE T MULTIMEDIA, V17, P1471, DOI 10.1109/TMM.2015.2460193
   Chen Yixin., 2016, Computer Communications, IEEE INFOCOM 2016-The 35th Annual IEEE International Conference on, P1, DOI DOI 10.1016/J.CORSCI.2016.05.039
   Deng RH, 2014, MULTIMEDIA SYST, V20, P165, DOI 10.1007/s00530-013-0326-0
   Douceur JR, 2002, INT CON DISTR COMP S, P617, DOI 10.1109/ICDCS.2002.1022312
   Duan Y., 2014, P 6 ED ACM WORKSH CL, P57
   Gao GY, 2015, IEEE T MULTIMEDIA, V17, P1286, DOI 10.1109/TMM.2015.2438713
   Jarecki S, 2009, LECT NOTES COMPUT SC, V5444, P577
   Lederer S., 2012, P 3 MULT SYST C, P89
   Li J, 2014, IEEE T PARALL DISTR, V25, P1615, DOI 10.1109/TPDS.2013.284
   Liu J, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P874, DOI 10.1145/2810103.2813623
   Ma C, 2014, INT CONF SERVICE SCI, P1, DOI 10.1109/ICSS.2014.36
   Nikolaenko V., 2013, P 2013 ACM SIGSAC C, P801, DOI DOI 10.1145/2508859.2516751
   Nikolaenko V, 2013, P IEEE S SECUR PRIV, P334, DOI 10.1109/SP.2013.30
   Oh SM, 2011, PROC CVPR IEEE
   Puzio P, 2013, INT CONF CLOUD COMP, P363, DOI 10.1109/CloudCom.2013.54
   Qin Z, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P497, DOI 10.1145/2647868.2654941
   Ren K, 2012, IEEE NETWORK, V26, P69, DOI 10.1109/MNET.2012.6375896
   Sanchez Y, 2012, SIGNAL PROCESS-IMAGE, V27, P329, DOI 10.1016/j.image.2011.10.002
   Sanchez Y., 2011, ACM MMSys'11, P257
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shoup V, 2000, LECT NOTES COMPUT SC, V1807, P207
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Stanek J, 2014, LECT NOTES COMPUT SC, V8437, P99, DOI 10.1007/978-3-662-45472-5_8
   Stefanov E., 2013, 2013 ACM SIGSAC C CO, P247
   Stütz T, 2012, IEEE T CIRC SYST VID, V22, P325, DOI 10.1109/TCSVT.2011.2162290
   Wei Z, 2014, IEEE T INF FOREN SEC, V9, P543, DOI 10.1109/TIFS.2014.2301916
   Wei Z, 2012, SIGNAL PROCESS-IMAGE, V27, P1011, DOI 10.1016/j.image.2012.06.005
   Wu YD, 2013, IEEE T MULTIMEDIA, V15, P778, DOI 10.1109/TMM.2013.2238910
   Yuan XL, 2014, INT CON DISTR COMP S, P198, DOI 10.1109/ICDCS.2014.28
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zhang WJ, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL CONFERENCE ON MATERIAL SCIENCE AND ENVIRONMENTAL ENGINEERING (MSEE 2013), P195
   Zheng X., 2015, P 10 ACM S INF COMP, P63
   Zhou YP, 2013, IEEE T MULTIMEDIA, V15, P802, DOI 10.1109/TMM.2013.2239628
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 48
TC 36
Z9 37
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 251
EP 265
DI 10.1109/TMM.2016.2612760
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800003
DA 2024-07-18
ER

PT J
AU Xiao, J
   Hu, RM
   Liao, L
   Chen, Y
   Wang, ZY
   Xiong, ZX
AF Xiao, Jing
   Hu, Ruimin
   Liao, Liang
   Chen, Yu
   Wang, Zhongyuan
   Xiong, Zixiang
TI Knowledge-Based Coding of Objects for Multisource Surveillance Video
   Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Global object redundancy (GOR); large spatial and temporal scale;
   multisource; surveillance video; video coding
ID PREDICTION
AB Global object redundancy (GOR), as opposed to local spatial/temporal redundancies in a single video clip, is a new form of redundancy common in multisource surveillance video data (MSVD). GOR is induced by the repetition of foreground objects across multiple cameras, and becomes influential as the number of objects increases. Eliminating GOR considerably improves MSVD coding efficiency. In an effort to accomplish this, this study first proposes a knowledge-based representation of objects based on careful analysis of GOR composition. The representation contains a constant part and a variational part: the former is used to represent the common knowledge shared by an object across multiple cameras, while the latter is used to represent local variations on the object's surfaces. Based on the proposed representation, a knowledge-based coding (KBC) method is then proposed in which each foreground object is encoded with a hybrid prediction scheme, where the constant part of the object is generated via global prediction from a model library and the variational part is predicted via local reference frames with pose-based, short-term prediction. Experimental results showed that the KBC method saves more than 39% bits on average for encoding foreground objects in high-resolution video clips (compared to 16% for the entire videos). Applying the proposed coding method to surveillance videos in large spatial and temporal scale allows storage savings at the PB level.
C1 [Xiao, Jing] Wuhan Univ, State Key Lab Software Engn, Wuhan 430072, Peoples R China.
   [Hu, Ruimin; Liao, Liang; Chen, Yu; Wang, Zhongyuan] Wuhan Univ, Sch Comp Sci, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Peoples R China.
   [Xiong, Zixiang] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77840 USA.
C3 Wuhan University; Wuhan University; Texas A&M University System; Texas
   A&M University College Station
RP Xiao, J (corresponding author), Wuhan Univ, State Key Lab Software Engn, Wuhan 430072, Peoples R China.
EM jing@whu.edu.cn; hrm@whu.edu.cn; liaoliangwhu@whu.edu.cn;
   cynercms@whu.edu.cn; wzy_hope@163.com; zx@ece.tamu.edu
RI Wang, Zhongyuan/ABD-2189-2020
FU National Nature Science Foundation of China [61502348]; Fundamental
   Research Funds of State Key Laboratory of Software Engineering
   [SKLSE-2015-A-01]; EU FP7 QUICK Project [PIRSES-GA-2013-612652]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grant 61502348, in part by the Fundamental
   Research Funds of State Key Laboratory of Software Engineering
   (SKLSE-2015-A-01), and in part by the EU FP7 QUICK Project under Grant
   PIRSES-GA-2013-612652. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Zhu Li.
CR AIZAWA K, 1995, P IEEE, V83, P259, DOI 10.1109/5.364463
   [Anonymous], IMAGE COMMUN
   Azizpour H, 2012, LECT NOTES COMPUT SC, V7572, P836, DOI 10.1007/978-3-642-33718-5_60
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bjontegaard G., 2001, Document VCEG-M33
   Chen D., IEEE T PARA IN PRESS
   Chen D, 2015, IEEE T COMPUT, V64, P707, DOI 10.1109/TC.2013.2295806
   Chen D, 2014, IEEE T NEUR SYS REH, V22, P33, DOI 10.1109/TNSRE.2013.2258939
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Gao W, 2014, IEEE INTELL SYST, V29, P30, DOI 10.1109/MIS.2013.101
   Hakeem A., 2005, 13th Annual ACM International Conference on Multimedia, P608, DOI 10.1145/1101149.1101289
   Kampmann M, 2002, IEEE T CIRC SYST VID, V12, P172, DOI 10.1109/76.993438
   Kang MK, 2014, IEEE T MULTIMEDIA, V16, P1563, DOI 10.1109/TMM.2014.2323939
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Liao L, 2015, IEEE IMAGE PROC, P745, DOI 10.1109/ICIP.2015.7350898
   Ng KT, 2010, IEEE T CIRC SYST VID, V20, P548, DOI 10.1109/TCSVT.2010.2041820
   OSTERMANN J, 1990, P SOC PHOTO-OPT INS, V1260, P240, DOI 10.1117/12.20024
   Paul M, 2014, IEEE T CIRC SYST VID, V24, P1729, DOI 10.1109/TCSVT.2014.2302555
   PEARSON DE, 1995, P IEEE, V83, P892, DOI 10.1109/5.387091
   Shi Z., 2013, P IEEE INT C MULT EX, P1
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   Tan TN, 1996, PATTERN RECOGN, V29, P449, DOI 10.1016/0031-3203(95)00113-1
   Tan TN, 1998, INT J COMPUT VISION, V27, P5, DOI 10.1023/A:1007924428535
   Tsai TH, 2012, IEEE T MULTIMEDIA, V14, P669, DOI 10.1109/TMM.2011.2180705
   Weinzaepfel P, 2011, PROC CVPR IEEE, P337, DOI 10.1109/CVPR.2011.5995616
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiao J, 2015, IEEE DATA COMPR CONF, P33, DOI 10.1109/DCC.2015.37
   Xiao J, 2015, CLUSTER COMPUT, V18, P531, DOI 10.1007/s10586-015-0434-z
   Yang HT, 2009, IEEE T CIRC SYST VID, V19, P887, DOI 10.1109/TCSVT.2009.2017410
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Yin LM, 2015, LECT NOTES COMPUT SC, V9314, P212, DOI 10.1007/978-3-319-24075-6_21
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zhang XG, 2014, IEEE T IMAGE PROCESS, V23, P769, DOI 10.1109/TIP.2013.2294549
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 35
TC 35
Z9 35
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1691
EP 1706
DI 10.1109/TMM.2016.2581590
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800002
DA 2024-07-18
ER

PT J
AU Kim, S
   Lee, D
   Kim, JS
   Lee, HJ
AF Kim, Sunwoong
   Lee, Donghyeon
   Kim, Jin-Sung
   Lee, Hyuk-Jae
TI A High-Throughput Hardware Design of a One-Dimensional SPIHT Algorithm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Discrete wavelet transform (DWT); embedded compression; one-dimensional
   (1D) compression; set partitioning in hierarchical trees (SPIHT); VLSI
   implementation
ID FRAME RECOMPRESSION ALGORITHM; IMAGE COMPRESSION; LINE LOAD; TRUNCATION;
   REDUCTION; ERRORS; CODEC
AB Video display systems include frame memory, which stores video data for display. To reduce system cost, video data are often compressed for storage in frame memory. A desirable characteristic for display memory compression is support for the raster-scan processing order and the fixed target compression ratio. Set partitioning in hierarchical trees (SPIHT) is an efficient two-dimensional compression algorithm that guarantees a fixed target compression ratio, but its one-dimensional (1D) variation has received little attention, even though its 1D nature supports the raster-scan processing order. This paper proposes a novel hardware design for 1D SPIHT. The algorithm is modified to exploit parallelism for effective hardware implementation. For the encoder, dependences that prohibit parallel execution are resolved and a pipelined schedule is proposed. For the parallel execution of the decoder, the algorithm is modified to enable estimation of the bitstream length of each pass prior to decoding. This modification allows parallel and pipelined decoding operations, leading to a high-throughput design for both encoder and decoder. Although the modifications slightly decrease compression efficiency, additional optimizations are proposed to improve such efficiency. As a result, the peak signal-to-noise ratio drop is reduced from 1.40 dB to 0.44 dB. The throughputs of the proposed encoder and decoder are 7.04 Gbps and 7.63 Gbps, respectively, and their respective gate counts are 37.2 K and 54.1 K.
C1 [Kim, Sunwoong; Lee, Donghyeon; Lee, Hyuk-Jae] Seoul Natl Univ, Dept Elect Engn, Inter Univ Semicond Res Ctr, Seoul 151742, South Korea.
   [Kim, Jin-Sung] Sun Moon Univ, Dept Elect Engn, Asan 336708, South Korea.
C3 Seoul National University (SNU); Sun Moon University
RP Kim, JS (corresponding author), Sun Moon Univ, Dept Elect Engn, Asan 336708, South Korea.
EM jinsungk@sunmoon.ac.kr
RI KIM, JIN-SUNG/W-1974-2019; Kim, Jaejoon/KHD-0962-2024; Kim,
   Jin/AAS-5810-2021; Lee, Donghyeon/GPT-4322-2022; Lee,
   Donghyeon/IUN-4155-2023
OI Kim, Jin/0000-0002-7667-9588; Lee, Donghyeon/0000-0002-4417-5641; Lee,
   Donghyeon/0000-0002-4417-5641; Kim, Sunwoong/0000-0002-1471-0228
FU LG Display Co. LTD; MSIP (Ministry of Science, ICT and Future Planning),
   Korea, under the C-ITRC (Convergence Information Technology Research
   Center) [IITP-2015-H8601-15-1008]
FX This work was supported in part by the LG Display Co. LTD and in part by
   the MSIP (Ministry of Science, ICT and Future Planning), Korea, under
   the C-ITRC (Convergence Information Technology Research Center) under
   Grant IITP-2015-H8601-15-1008, supervised by the IITP (Institute for
   Information and Communications Technology Promotion). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Leonel Sousa.
CR [Anonymous], INT J SIGNAL PROCESS
   Bao XN, 2012, IEEE T MULTIMEDIA, V14, P237, DOI 10.1109/TMM.2011.2171677
   Cheng CC, 2009, IEEE T CIRC SYST VID, V19, P141, DOI 10.1109/TCSVT.2008.2009250
   Corsonello P, 2006, IEEE T CIRC SYST VID, V16, P114, DOI 10.1109/TCSVT.2005.856925
   Franzen R, 1999, Kodak lossless true color image suite
   Fry TW, 2005, IEEE T CIRC SYST VID, V15, P1138, DOI 10.1109/TCSVT.2005.852625
   Guo L, 2014, IEEE T MULTIMEDIA, V16, P2323, DOI 10.1109/TMM.2014.2350256
   Han JW, 2008, IEEE T CONSUM ELECTR, V54, P1839, DOI 10.1109/TCE.2008.4711243
   Jin Y, 2012, IEEE T CIRC SYST VID, V22, P1064, DOI 10.1109/TCSVT.2012.2189793
   Jin Y, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/629285
   Kassim AA, 2003, IEEE T CIRC SYST VID, V13, P203, DOI 10.1109/TCSVT.2002.808427
   Kim J, 2010, IEEE T CIRC SYST VID, V20, P848, DOI 10.1109/TCSVT.2010.2045923
   Kim JS, 2008, IEEE T CIRC SYST VID, V18, P827, DOI 10.1109/TCSVT.2008.918759
   Kim JS, 2009, IEEE T CIRC SYST VID, V19, P561, DOI 10.1109/TCSVT.2009.2017075
   Kuo HC, 2012, IEEE T MULTIMEDIA, V14, P500, DOI 10.1109/TMM.2012.2191945
   Lee YJ, 2007, IEEE INT SYMP CIRC S, P1621, DOI 10.1109/ISCAS.2007.378829
   Liu K, 2012, J SIGNAL PROCESS SYS, V68, P113, DOI 10.1007/s11265-011-0581-2
   Mukherjee J, 2008, IEEE T IMAGE PROCESS, V17, P1783, DOI 10.1109/TIP.2008.2002826
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Tsai TH, 2010, IEEE T CIRC SYST VID, V20, P1277, DOI 10.1109/TCSVT.2010.2057770
   Weinberger MJ, 2000, IEEE T IMAGE PROCESS, V9, P1309, DOI 10.1109/83.855427
   Wheeler FW, 2000, INT CONF ACOUST SPEE, P2047, DOI 10.1109/ICASSP.2000.859236
   Zhang Zhi-hui, 2010, Proceedings of the 2010 International Conference on Electrical and Control Engineering (ICECE 2010), P2498, DOI 10.1109/iCECE.2010.618
NR 23
TC 22
Z9 22
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 392
EP 404
DI 10.1109/TMM.2015.2514196
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600007
DA 2024-07-18
ER

PT J
AU Zhang, YB
   Zhang, YL
   Zhang, J
   Dai, QH
AF Zhang, Yongbing
   Zhang, Yulun
   Zhang, Jian
   Dai, Qionghai
TI CCR: Clustering and Collaborative Representation for Fast Single Image
   Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clustering and collaborative representation (CCR); feature subspace;
   projection matrix; statistical prior; super-resolution (SR)
ID INTERPOLATION; INFORMATION; LIMITS
AB Clustering and collaborative representation (CCR) have recently been used in fast single image super-resolution (SR). In this paper, we propose an effective and fast single image super-resolution (SR) algorithm by combining clustering and collaborative representation. In particular, we first cluster the feature space of low-resolution (LR) images into multiple LR feature subspaces and group the corresponding high-resolution (HR) feature subspaces. The local geometry property learned from the clustering process is used to collect numerous neighbor LR and HR feature subsets from the whole feature spaces for each cluster center. Multiple projection matrices are then computed via collaborative representation to map LR feature subspaces to HR subspaces. For an arbitrary input LR feature, the desired HR output can be estimated according to the projection matrix, whose corresponding LR cluster center is nearest to the input. Moreover, by learning statistical priors from the clustering process, our clustering-based SR algorithm would further decrease the computational time in the reconstruction phase. Extensive experimental results on commonly used datasets indicate that our proposed SR algorithm obtains compelling SR images quantitatively and qualitatively against many state-of-the-art methods.
C1 [Zhang, Yongbing; Zhang, Yulun] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
   [Zhang, Yulun; Dai, Qionghai] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Zhang, Jian] Peking Univ, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Dai, Qionghai] Tsinghua Univ, TNLIST, Beijing 100084, Peoples R China.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University;
   Tsinghua University; Peking University; Tsinghua University
RP Zhang, YB; Zhang, YL (corresponding author), Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.; Zhang, YL; Dai, QH (corresponding author), Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.; Zhang, J (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.; Dai, QH (corresponding author), Tsinghua Univ, TNLIST, Beijing 100084, Peoples R China.
EM zhang.yongbing@sz.tsinghua.edu.cn; zhangyl14@mails.tsinghua.edu.cn;
   jian.zhang@pku.edu.cn; qhdai@tsinghua.edu.cn
RI Dai, Qionghai/ABD-5298-2021
OI Dai, Qionghai/0000-0001-7043-3061; Zhang, Yulun/0000-0002-2288-5079;
   Zhang, Jian/0000-0001-5486-3125
FU National Natural Science Foundation of China [61571254, U1301257,
   U1201255, 61572047]; Guangdong Natural Science Foundation
   [2014A030313751]; Postdoctoral Science Foundation of China [2015M580018]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61571254, Grant U1301257, Grant
   U1201255, and Grant 61572047, in part by the Guangdong Natural Science
   Foundation 2014A030313751, and in part by the Postdoctoral Science
   Foundation of China under Grant 2015M580018. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Ali Begen.
CR Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chakrabarti A, 2007, IEEE T MULTIMEDIA, V9, P888, DOI 10.1109/TMM.2007.893346
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Darbon J, 2008, I S BIOMED IMAGING, P1331, DOI 10.1109/ISBI.2008.4541250
   Dodgson NA, 1997, IEEE T IMAGE PROCESS, V6, P1322, DOI 10.1109/83.623195
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239546, 10.1145/1276377.1276496]
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   He L, 2013, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2013.51
   Irani M., 1993, Journal of Visual Communication and Image Representation, V4, P324, DOI 10.1006/jvci.1993.1030
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li M, 2008, IEEE T IMAGE PROCESS, V17, P1121, DOI 10.1109/TIP.2008.924289
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Lin ZC, 2004, IEEE T PATTERN ANAL, V26, P83, DOI 10.1109/TPAMI.2004.1261081
   Lu XQ, 2013, IEEE T CIRC SYST VID, V23, P2022, DOI 10.1109/TCSVT.2013.2244798
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Peleg T, 2014, IEEE T IMAGE PROCESS, V23, P2569, DOI 10.1109/TIP.2014.2305844
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sun JA, 2010, PROC CVPR IEEE, P231, DOI 10.1109/CVPR.2010.5540206
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Tang Y, 2011, INT J MACH LEARN CYB, V2, P15, DOI 10.1007/s13042-011-0011-6
   Thévenaz P, 2000, BIOMED EN S, P393
   Timofte R., 2014, P IEEE AS C COMP VIS
   Timofte R, 2014, PATTERN RECOGN LETT, V43, P127, DOI 10.1016/j.patrec.2013.08.010
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wang LF, 2013, IEEE T CIRC SYST VID, V23, P1289, DOI 10.1109/TCSVT.2013.2240915
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiong ZW, 2013, IEEE T MULTIMEDIA, V15, P1458, DOI 10.1109/TMM.2013.2264654
   Xu HT, 2013, IEEE T CIRC SYST VID, V23, P1740, DOI 10.1109/TCSVT.2013.2248305
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Yang JQ, 2008, ITESS: 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES, PT 1, P1, DOI 10.1109/CVPR.2008.4587647
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Yang SY, 2014, IEEE T IMAGE PROCESS, V23, P2793, DOI 10.1109/TIP.2014.2319742
   Yu Zhang, 2015, 2015 IEEE MTT-S International Microwave Symposium (IMS2015), P1, DOI 10.1109/MWSYM.2015.7166999
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang J, 2012, IEEE INT SYMP CIRC S, P1688
   Zhang KB, 2015, IEEE T IMAGE PROCESS, V24, P846, DOI 10.1109/TIP.2015.2389629
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang Y., P IEEE INT C VIS COM
   Zhang YL, 2015, LECT NOTES COMPUT SC, V9315, P63, DOI 10.1007/978-3-319-24078-7_7
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
   Zibetti MVW, 2007, IEEE T CIRC SYST VID, V17, P1288, DOI 10.1109/TCSVT.2007.903801
NR 52
TC 61
Z9 63
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 405
EP 417
DI 10.1109/TMM.2015.2512046
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600008
DA 2024-07-18
ER

PT J
AU Qi, F
   Zhao, DB
   Gao, W
AF Qi, Feng
   Zhao, Debin
   Gao, Wen
TI Reduced Reference Stereoscopic Image Quality Assessment Based on
   Binocular Perceptual Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binocular perceptual information (BPI); mutual information; sparse
   representation; stereoscopic image quality assessment (SIQA)
ID SPARSE REPRESENTATION
AB In this paper, we propose a novel reduced reference stereoscopic image quality assessment (RR-SIQA) metric by using binocular perceptual information (BPI). BPI is represented by the distribution statistics of visual primitives in left and right views' images, which are extracted by sparse coding and representation. Specifically, entropy of the left view's image and entropy of the right view's image are used to represent monocular cue. Their mutual information is used to represent binocular cue. Constructively, we represent BPI as three numerical indicators. The difference of the original and distorted images' BPIs is taken as perceptual loss vector. The perceptual loss vector is used to compute the quality score for a stereoscopic image by a prediction function which is trained using support vector regression (SVR). Experimental results show that the proposed metric achieves significantly higher prediction accuracy than the state-of-the-art reduced reference SIQA methods and better than several state-of-the-art full reference SIQA methods on the LIVE phase II asymmetric databases.
C1 [Qi, Feng; Zhao, Debin] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Harbin Institute of Technology; Peking University
RP Qi, F (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
EM fqi@jdl.ac.cn
RI Zhao, Debin/JEP-0204-2023
FU Major State Basic Research Development Program of China 973 Program
   [2015CB351804]; National Science Foundation of China [61272386]
FX This work was supported in part by the Major State Basic Research
   Development Program of China 973 Program under Grant 2015CB351804 and in
   part by the National Science Foundation of China under Grant 61272386.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Christian Timmerer.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Akhter R, 2010, PROC SPIE, V7524, DOI 10.1117/12.838775
   [Anonymous], 2009, NATURAL IMAGE STAT P
   [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], 2010, P INT WORKSH VID PRO
   [Anonymous], P IEEE 3DTV C JUN
   [Anonymous], P STER DISPL APPL 19
   [Anonymous], EURASIP J IMAGE VIDE
   [Anonymous], 2014, INT S BROADB MULT SY
   Bensalma R, 2013, MULTIDIM SYST SIGN P, V24, P281, DOI 10.1007/s11045-012-0178-3
   Campisi Patrizio, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P2110
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1
   Guha T, 2014, SIGNAL PROCESS-IMAGE, V29, P1138, DOI 10.1016/j.image.2014.09.010
   He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795
   Hewage CTER, 2011, IEEE T CONSUM ELECTR, V57, P1185, DOI 10.1109/TCE.2011.6018873
   Jian Zhang, 2012, Advances in Multimedia Information Processing - PCM 2012. 13th Pacific-Rim Conference on Multimedia. Proceedings, P674, DOI 10.1007/978-3-642-34778-8_63
   Lambooij M, 2011, IEEE T BROADCAST, V57, P432, DOI 10.1109/TBC.2011.2134590
   Maalouf A, 2011, INT CONF ACOUST SPEE, P1161
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Ryu S, 2014, IEEE T CIRC SYST VID, V24, P591, DOI 10.1109/TCSVT.2013.2279971
   Sazzad Z. M. Parvez, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P106, DOI 10.1109/QOMEX.2010.5517772
   Sazzad ZMP, 2009, INT WORK QUAL MULTIM, P180, DOI 10.1109/QOMEX.2009.5246956
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wang SQ, 2013, PICT COD SYMP, P193, DOI 10.1109/PCS.2013.6737716
   Wang ZJ, 2011, IEEE SIGNAL PROC MAG, V28, P2, DOI 10.1109/MSP.2011.940297
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zhou WJ, 2014, IEEE SIGNAL PROC LET, V21, P1003, DOI 10.1109/LSP.2014.2320956
NR 31
TC 50
Z9 53
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2338
EP 2344
DI 10.1109/TMM.2015.2493781
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500019
DA 2024-07-18
ER

PT J
AU Wang, YC
   Han, CC
   Hsieh, CT
   Chen, YN
   Fan, KC
AF Wang, Yu-Chen
   Han, Chin-Chuan
   Hsieh, Chen-Ta
   Chen, Ying-Nong
   Fan, Kuo-Chin
TI Biased Discriminant Analysis With Feature Line Embedding for Relevance
   Feedback-Based Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Biased discriminant analysis; content-based image retrieval (CBIR);
   feature line embedding (FLE); high-level semantic concept; low-level
   image features; relevance feedback
ID RECOGNITION; COLOR
AB The focus of content-based image retrieval (CBIR) is to narrow down the gap between low-level image features and high-level semantic concepts. In this paper, a biased discriminant analysis with feature line embedding (FLE-BDA) is proposed for performance enhancement in relevance feedback schemes. Maximizing the margin between relevant and irrelevant samples at local neighborhoods was the aim in this study. In reduced subspace, relevant images and query images can be quite close, while irrelevant samples are far away from relevant samples. The results of four benchmark datasets are given to show the performance of the proposed method.
C1 [Wang, Yu-Chen; Hsieh, Chen-Ta; Chen, Ying-Nong; Fan, Kuo-Chin] Natl Cent Univ, Dept Comp Sci & Informat Engn, Taoyuan 32001, Taiwan.
   [Han, Chin-Chuan] Natl United Univ, Dept Comp Sci & Informat Engn, Miaoli 36003, Taiwan.
C3 National Central University; National United University
RP Wang, YC (corresponding author), Natl Cent Univ, Comp Sci, Taoyuan 32001, Taiwan.
EM m09502062@chu.edu.tw; cchan@nuu.edu.tw; hsieh.chengta@gmail.com;
   yingnong1218@gmail.com; kcfan@csie.ncu.edu.tw
RI Fan, K/GXH-3734-2022
FU Ministry of Science and Technology of Taiwan
   [MOST104-2221-E-008-030-MY3, NSC 103-2221-E-239-017]
FX This work was supported by the Ministry of Science and Technology of
   Taiwan under Grant MOST104-2221-E-008-030-MY3 and Grant NSC
   103-2221-E-239-017. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Yonghong Tian.
CR [Anonymous], 2002, Introduction to MPEG- 7: Multimedia content description interface
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bian W, 2010, IEEE T IMAGE PROCESS, V19, P545, DOI 10.1109/TIP.2009.2035223
   Chang SF, 2001, IEEE T CIRC SYST VID, V11, P688, DOI 10.1109/76.927421
   Chen YN, 2011, IEEE T PATTERN ANAL, V33, P1073, DOI 10.1109/TPAMI.2010.197
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Chun YD, 2008, IEEE T MULTIMEDIA, V10, P1073, DOI 10.1109/TMM.2008.2001357
   Deselaers T, 2008, INFORM RETRIEVAL, V11, P77, DOI 10.1007/s10791-007-9039-3
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   He X, 2008, IEEE T KNOWL DATA EN, V20, P189, DOI 10.1109/TKDE.2007.190692
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   Huijsmans DP, 2005, IEEE T PATTERN ANAL, V27, P245, DOI 10.1109/TPAMI.2005.30
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Liu GH, 2013, PATTERN RECOGN, V46, P188, DOI 10.1016/j.patcog.2012.06.001
   Liu GH, 2011, PATTERN RECOGN, V44, P2123, DOI 10.1016/j.patcog.2011.02.003
   Liu Wei, 2011, Reports in Parasitology, V1, P1
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Richard B., 1961, ADAPTIVE CONTROL PRO, P143
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Rui Y, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P815, DOI 10.1109/ICIP.1997.638621
   Su JH, 2011, IEEE T KNOWL DATA EN, V23, P360, DOI 10.1109/TKDE.2010.124
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Wang YC, 2015, 2015 14TH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), P1, DOI 10.1109/MVA.2015.7153119
   Wu Y., 2002, P IEEE INT C IM PROC, V2, P581
   Xu D, 2007, IEEE T IMAGE PROCESS, V16, P2811, DOI 10.1109/TIP.2007.906769
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yen-Yu Lin, 2005, 13th Annual ACM International Conference on Multimedia, P249, DOI 10.1145/1101149.1101193
   Yu J, 2013, NEUROCOMPUTING, V120, P355, DOI 10.1016/j.neucom.2012.08.061
   Zhang LN, 2012, IEEE T IMAGE PROCESS, V21, P2294, DOI 10.1109/TIP.2011.2177846
   Zhou XS, 2001, PROC CVPR IEEE, P11
NR 35
TC 9
Z9 10
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2245
EP 2258
DI 10.1109/TMM.2015.2492926
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500012
DA 2024-07-18
ER

PT J
AU Wang, AR
   Lu, JW
   Cai, JF
   Cham, TJ
   Wang, G
AF Wang, Anran
   Lu, Jiwen
   Cai, Jianfei
   Cham, Tat-Jen
   Wang, Gang
TI Large-Margin Multi-Modal Deep Learning for RGB-D Object Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; large-margin feature learning; multi-modality; RGB-D
   object recognition
ID ALGORITHM; FEATURES
AB Most existing feature learning-based methods for RGB-D object recognition either combine RGB and depth data in an undifferentiated manner from the outset, or learn features from color and depth separately, which do not adequately exploit different characteristics of the two modalities or utilize the shared relationship between the modalities. In this paper, we propose a general CNN-based multi-modal learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, which are then connected with a carefully designed multi-modal layer. This layer is designed to not only discover the most discriminative features for each modality, but is also able to harness the complementary relationship between the two modalities. The results of the multi-modal layer are back-propagated to update parameters of the CNN layers, and the multi-modal feature learning and the back-propagation are iteratively performed until convergence. Experimental results on two widely used RGB-D object datasets show that our method for general multi-modal learning achieves comparable performance to state-of-the-art methods specifically designed for RGB-D data.
C1 [Wang, Anran; Cai, Jianfei; Cham, Tat-Jen] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Lu, Jiwen] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Wang, Gang] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; Tsinghua University; Nanyang
   Technological University
RP Wang, AR (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM awang001@e.ntu.edu.sg; elujiwen@gmail.com; asjfcai@ntu.edu.sg;
   astjcham@ntu.edu.sg; wanggang@ntu.edu.sg
RI Wang, Gang/B-7027-2013; Wang, Anran/L-5829-2017; Cai,
   Jianfei/A-3691-2011; Lu, Jiwen/C-5291-2009
OI Cai, Jianfei/0000-0002-9444-3763; Wang, Anran/0000-0002-8587-2136; Cham,
   Tat-Jen/0000-0001-5264-2572; Lu, Jiwen/0000-0002-6121-5529
FU Singapore National Research Foundation under its International Research
   Centre@Singapore Funding Initiative; Singapore Ministry of Education
   (MOE) [RG 138/14]; MOE [ARC28/14]; Singapore A*STAR Science and
   Engineering Research Council [PSF1321202099]
FX This work was supported by the Singapore National Research Foundation
   under its International Research Centre@Singapore Funding Initiative and
   administered by the IDM Programme Office. This work was supported in
   part by the Singapore Ministry of Education (MOE) Tier 1 RG 138/14, MOE
   Tier 2 ARC28/14, and the Singapore A*STAR Science and Engineering
   Research Council under Grant PSF1321202099. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Benoit Huet.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   [Anonymous], 2006, P IEEE COMPUTER SOC, DOI DOI 10.1109/CVPR.2006.95
   [Anonymous], 2014, P AS C COMP VIS
   [Anonymous], 2011, P ADV NEURAL INFORM
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Blum M, 2012, IEEE INT CONF ROBOT, P1298, DOI 10.1109/ICRA.2012.6225188
   Bo L., 2011, Neural Information Processing Systems, P2115
   Bo L., 2013, Experimental Robotics, volume 88 of Springer Tracts in Advanced Robotics, P387
   Bo LF, 2011, IEEE INT C INT ROBOT, P821, DOI 10.1109/IROS.2011.6048717
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Browatzki B., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1189, DOI 10.1109/ICCVW.2011.6130385
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   Couprie C., 2013, ARXIV13013572, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Gong BQ, 2013, IEEE T MULTIMEDIA, V15, P369, DOI 10.1109/TMM.2012.2231059
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu Junlin., 2014, COMPUTER VISION ACCV, P252
   Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Le Quoc V, 2011, Advances in neural information processing systems, P1017
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACHI, V382, P609, DOI 10.1145/1553374.1553453
   Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu WT, 2013, IEEE T MULTIMEDIA, V15, P1920, DOI 10.1109/TMM.2013.2280895
   Mikolov T., 2013, P 26 INT C NEURAL IN, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R., 2009, ARTIFICIAL INTELLIGE, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Sermanet P, 2012, INT C PATT RECOG, P3288
   Silberman N., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P601, DOI 10.1109/ICCVW.2011.6130298
   Simonyan K., 2013, arXiv preprint arXiv:1312.6034
   Socher R., 2012, NIPS, V3, P8
   Song Y, 2014, IEEE T CIRC SYST VID, V24, P952, DOI 10.1109/TCSVT.2014.2302558
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tang JH, 2015, IEEE T IMAGE PROCESS, V24, P2827, DOI 10.1109/TIP.2015.2421443
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Vincent Pascal, 2008, P 25 INT C MACHINE L, DOI DOI 10.1145/1390156.1390294
   Wu L, 2009, IEEE T MULTIMEDIA, V11, P286, DOI 10.1109/TMM.2008.2009692
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeng XY, 2014, LECT NOTES COMPUT SC, V8691, P472, DOI 10.1007/978-3-319-10578-9_31
   Zhang Xiang., 2015, CoRR
NR 53
TC 116
Z9 121
U1 0
U2 54
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 1887
EP 1898
DI 10.1109/TMM.2015.2476655
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400003
DA 2024-07-18
ER

PT J
AU Zhang, SL
   Wang, XY
   Lin, YQ
   Tian, Q
AF Zhang, Shiliang
   Wang, Xiaoyu
   Lin, Yuanqing
   Tian, Qi
TI Cross Indexing With Grouplets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image indexing; large-scale image retrieval
ID BINARY DESCRIPTOR; OBJECT RETRIEVAL; IMAGE; SIMILARITY; FEATURES; SIFT
AB Most of the current image indexing systems for retrieval view a database as a set of individual images. It limits the flexibility of the retrieval framework to conduct sophisticated cross-image analysis, resulting in higher memory consumption and sub-optimal retrieval accuracy. To conquer this issue, we propose cross indexing with grouplets, where the core idea is to view the database images as a set of grouplets, each of which is defined as a group of highly relevant images. Because a grouplet groups similar images together, the number of grouplets is smaller than the number of images, thus naturally leading to less memory cost. Moreover, the definition of a grouplet could be based on customized relations, allowing for seamless integration of advanced image features and data mining techniques like the deep convolutional neural network (DCNN) in off-line indexing. To validate the proposed framework, we construct three different types of grouplets, which are respectively based on local similarity, regional relation, and global semantic modeling. Extensive experiments on public benchmark datasets demonstrate the efficiency and superior performance of our approach.
C1 [Zhang, Shiliang] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
   [Wang, Xiaoyu] Snapchat, Venice, CA 90291 USA.
   [Lin, Yuanqing] NEC Labs Amer, Media Analyt Dept, Cupertino, CA 95014 USA.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Peking University; NEC Corporation; University of Texas System;
   University of Texas at San Antonio (UTSA)
RP Zhang, SL (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
EM slzhang.jdl@pku.edu.cn; fanghuaxue@gmail.com; ylin@nec-labs.com;
   qitian@cs.utsa.edu
RI wang, xiaoyu/HJP-6901-2023
OI Wang, Xiaoyu/0000-0002-6431-8822
FU NSFC [61429201]; National Science Foundation of China (NSFC) [61572050];
   National 1000 Youth Talents Plan of China; ARO [W911NF-15-1-0290,
   W911NF-12-1-0057]; NEC Laboratories of America Faculty Research Awards
FX This work was supported in part by the NSFC under Grant 61429201. The
   work of S. Zhang was supported in part by the National Science
   Foundation of China (NSFC) under Grant 61572050 and the National 1000
   Youth Talents Plan of China. The work of Q. Tian was supported in part
   by the ARO under Grant W911NF-15-1-0290 and Grant W911NF-12-1-0057 and
   by the NEC Laboratories of America Faculty Research Awards. The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Guo-Jun Qi.
CR [Anonymous], P INT C MULT RETR IC
   [Anonymous], 2010, P 27 INT C MACHINE L
   [Anonymous], 1972, P COMPLEXITY COMPUTE
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2010, IEEE T PATTERN ANAL, V32, P371, DOI 10.1109/TPAMI.2009.166
   Douze M, 2011, PROC CVPR IEEE, P745, DOI 10.1109/CVPR.2011.5995595
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Ferrari V., 2007, P 20 INT C NEUR INF, P433
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Hauptmann AG, 2008, P IEEE, V96, P602, DOI 10.1109/JPROC.2008.916355
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jia D, 2011, PROC CVPR IEEE, P785, DOI 10.1109/CVPR.2011.5995516
   Jia Y., 2014, P 22 ACM INT C MULT, P675
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin YQ, 2011, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR.2011.5995477
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu Z., 2012, P ACM INT C MULTIMED, P199
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Makino K., 2004, NEW ALGORITHMS ENUME
   Mikulík A, 2010, LECT NOTES COMPUT SC, V6313, P1
   Nister David, 2006, CVPR
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Perronnin F., P ECCV, P143
   Qin DF, 2013, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2013.211
   Qin DF, 2011, PROC CVPR IEEE, P777, DOI 10.1109/CVPR.2011.5995373
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen XH, 2012, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR.2012.6248031
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Szegedy C., 2013, Advances in Neural Information Processing Systems, V26, P2553
   Tomita E, 2006, THEOR COMPUT SCI, V363, P28, DOI 10.1016/j.tcs.2006.06.015
   Torresani L., P ECCV, P776
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang XY, 2011, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2011.6126244
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Yu FX, 2012, PROC CVPR IEEE, P2949, DOI 10.1109/CVPR.2012.6248023
   Zhang S., IEEE T PATT IN PRESS
   Zhang ST, 2012, LECT NOTES COMPUT SC, V7573, P660, DOI 10.1007/978-3-642-33709-3_47
   Zhang SL, 2013, IEEE I CONF COMP VIS, P1673, DOI 10.1109/ICCV.2013.210
   Zhang SL, 2014, IEEE T IMAGE PROCESS, V23, P3671, DOI 10.1109/TIP.2014.2330794
   Zhang SL, 2014, IEEE T IMAGE PROCESS, V23, P2514, DOI 10.1109/TIP.2014.2317986
   Zhang SL, 2014, COMPUT VIS IMAGE UND, V118, P16, DOI 10.1016/j.cviu.2013.03.008
   Zhang SL, 2013, IEEE T IMAGE PROCESS, V22, P2889, DOI 10.1109/TIP.2013.2251650
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
NR 54
TC 5
Z9 5
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 1969
EP 1979
DI 10.1109/TMM.2015.2478055
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400010
DA 2024-07-18
ER

PT J
AU Qin, XQ
   Tan, XY
   Chen, SC
AF Qin, Xiaoqian
   Tan, Xiaoyang
   Chen, Songcan
TI Tri-Subject Kinship Verification: Understanding the Core of A Family
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature selection; kinship verification; tri-subject relationship
ID RESEMBLANCE
AB One major challenge in computer vision is to go beyond the modeling of individual objects and to investigate the bi-(one-versus-one) or tri-(one-versus-two) relationship among multiple visual entities, answering such questions as whether a child in a photo belongs to the given parents. The child-parents relationship plays a core role in a family, and understanding such kin relationship would have a fundamental impact on the behavior of an artificial intelligent agent working in the human world. In this work, we tackle the problem of one-versus-two (tri-subject) kinship verification and our contributions are threefold: 1) a novel relative symmetric bilinear model (RSBM) is introduced to model the similarity between the child and the parents, by incorporating the prior knowledge that a child may resemble one particular parent more than the other; 2) a spatially voted method for feature selection, which jointly selects the most discriminative features for the child-parents pair, while taking local spatial information into account; and 3) a large-scale tri-subject kinship database characterized by over 1,000 child-parents families. Extensive experiments on KinFaceW, Family101, and our newly released kinship database show that the proposed method outperforms several previous state of the art methods, while could also be used to significantly boost the performance of one-versus-one kinship verification when the information about both parents are available.
C1 [Qin, Xiaoqian; Tan, Xiaoyang; Chen, Songcan] Nanjing Univ Aeronaut & Astronaut, Dept Comp Sci & Technol, Nanjing 210016, Jiangsu, Peoples R China.
   [Qin, Xiaoqian] Huaiyin Normal Univ, Huaiyin 223300, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Huaiyin Normal
   University
RP Tan, XY (corresponding author), Nanjing Univ Aeronaut & Astronaut, Dept Comp Sci & Technol, Nanjing 210016, Jiangsu, Peoples R China.
EM qinxiaoqian@hytc.edu.cn; x.tan@nuaa.edu.cn; s.chen@nuaa.edu.cn
RI tan, xiao/GZL-0264-2022; TAN, XJ/JHS-5357-2023
FU National Science Foundation of China [61373060, 61472186]; Jiangsu
   Science Foundation [BK2012793]; Qing Lan Project; Research Fund for the
   Doctoral Program [20123218110033]; Natural Science Foundation of the
   Jiangsu Higher Education Institutions of China [13KJD520002]
FX This work was supported by the National Science Foundation of China
   under Grant 61373060 and Grant 61472186, by the Jiangsu Science
   Foundation under Grant BK2012793, by the Qing Lan Project, by the
   Research Fund for the Doctoral Program under Grant 20123218110033, and
   by the Natural Science Foundation of the Jiangsu Higher Education
   Institutions of China under Grant 13KJD520002. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Tommaso Melodia. (Corresponding author: Xiaoyang
   Tan.)
CR Alvergne A, 2007, EVOL HUM BEHAV, V28, P135, DOI 10.1016/j.evolhumbehav.2006.08.008
   [Anonymous], 2006, J VIS
   [Anonymous], 2014, IDENTITY KINSHIP REL
   Benavent X, 2013, IEEE T MULTIMEDIA, V15, P2009, DOI 10.1109/TMM.2013.2267726
   Davis J. V., 2007, ICML, P209
   DeBruine LM, 2009, VISION RES, V49, P38, DOI 10.1016/j.visres.2008.09.025
   Dehghan A, 2014, PROC CVPR IEEE, P1757, DOI 10.1109/CVPR.2014.227
   Dibeklioglu H, 2013, IEEE I CONF COMP VIS, P1497, DOI 10.1109/ICCV.2013.189
   Domingue BW, 2014, P NATL ACAD SCI USA, V111, P7996, DOI 10.1073/pnas.1321426111
   Dong Wang, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P376, DOI 10.1007/978-3-642-42051-1_47
   Fang RG, 2013, IEEE IMAGE PROC, P2983, DOI 10.1109/ICIP.2013.6738614
   Fang RG, 2010, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2010.5652590
   Ghahramani M, 2014, MACH VISION APPL, V25, P919, DOI 10.1007/s00138-013-0566-1
   Guo GD, 2012, IEEE T INSTRUM MEAS, V61, P2322, DOI 10.1109/TIM.2012.2187468
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Hu Junlin., 2014, COMPUTER VISION ACCV, P252
   Ji S., 2009, An accelerated gradient method for trace norm minimization, P457
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Kohli N., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P245, DOI 10.1109/BTAS.2012.6374584
   Li XS, 2015, 2015 INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS - COMPUTING TECHNOLOGY, INTELLIGENT TECHNOLOGY, INDUSTRIAL INFORMATION INTEGRATION (ICIICII), P1, DOI 10.1109/ICIICII.2015.88
   Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134
   Ma Z, 2013, IEEE T MULTIMEDIA, V15, P1628, DOI 10.1109/TMM.2013.2264928
   Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x
   Platek SM, 2004, EVOL HUM BEHAV, V25, P394, DOI 10.1016/j.evolhumbehav.2004.08.007
   Shao M., 2011, CVPR 2011 WORKSH, P60, DOI DOI 10.1109/CVPRW.2011.5981801
   Shao M, 2014, INT J COMPUT VISION, V109, P74, DOI 10.1007/s11263-014-0696-6
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Sun Y., 2014, CoRR
   Syed N., 2014, INT J SCI RES ED, V2, P1037
   Tan XY, 2009, PROC CVPR IEEE, P1621, DOI 10.1109/CVPRW.2009.5206818
   Wang G, 2010, LECT NOTES COMPUT SC, V6315, P169, DOI 10.1007/978-3-642-15555-0_13
   Weinberger K., 2006, ADV NEURAL INF PROCE, V18, P18
   Wolf L., 2008, P WORKSH FAC REAL LI, P4
   Xia S., 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, P2539, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-422
   Xia SY, 2012, INT C PATT RECOG, P549
   Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436
   Xiaoqian Q., P INT C MAC IN PRESS
   Xu Z, 2014, IEEE T MULTIMEDIA, V16, P1986, DOI 10.1109/TMM.2014.2342658
   Yan H., IEEE T CYBE IN PRESS
   Yan HB, 2014, IEEE T INF FOREN SEC, V9, P1169, DOI 10.1109/TIFS.2014.2327757
   Zhou X., 2011, ACM Multimedia, P953
   Zhou Xiuzhuang, 2012, P 20 ACM INT C MULT, P725, DOI [DOI 10.1145/2393347, DOI 10.1145/2393347.2396297]
   Zoidi O, 2014, IEEE T MULTIMEDIA, V16, P1358, DOI 10.1109/TMM.2014.2315595
NR 43
TC 93
Z9 104
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1855
EP 1867
DI 10.1109/TMM.2015.2461462
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kofler, C
   Bhattacharya, S
   Larson, M
   Chen, T
   Hanjalic, A
   Chang, SF
AF Kofler, Christoph
   Bhattacharya, Subhabrata
   Larson, Martha
   Chen, Tao
   Hanjalic, Alan
   Chang, Shih-Fu
TI Uploader Intent for Online Video: Typology, Inference, and Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing; indexing; search intent; video audience; video
   popularity; video search; video uploader intent
ID INTERNET
AB We investigate automatic inference of uploader intent for online video, i.e., prediction of the reason for which a user has uploaded a particular video to the Internet. Users upload video for specific reasons, but rarely state these reasons explicitly in the video metadata. Information about the reasons motivating uploaders has the potential ultimately to benefit a wide range of application areas, including video production, video-based advertising, and video search. In this paper, we apply a combination of social-Web mining and crowdsourcing to arrive at a typology that characterizes the uploader intent of a broad range of videos. We then use a set of multimodal features, including visual semantic features, found to be indicative of uploader intent in order to classify videos automatically into uploader intent classes. We evaluate our approach on a dataset containing ca. 3K crowdsourcing-annotated videos and demonstrate its usefulness in prediction tasks relevant to common application areas.
C1 [Kofler, Christoph; Larson, Martha; Hanjalic, Alan] Delft Univ Technol, NL-2628 CD Delft, Netherlands.
   [Bhattacharya, Subhabrata] Siemens Corp, Imaging & Comp Vis, Corp Res, Princeton, NJ 08540 USA.
   [Chen, Tao; Chang, Shih-Fu] Columbia Univ, New York, NY 10027 USA.
C3 Delft University of Technology; Siemens AG; Columbia University
RP Kofler, C (corresponding author), Delft Univ Technol, NL-2628 CD Delft, Netherlands.
EM c.kofler@tudelft.nl
RI Chen, Tao/ABB-5983-2022
OI Hanjalic, Alan/0000-0002-5771-2549
FU Dutch national program COMMIT; Google Europe Doctoral Fellowship in
   Video Search
FX This work was supported by the Dutch national program COMMIT. This work
   was carried out when C. Kofler was visiting Columbia University. The
   work of C. Kofler was supported by the Google Europe Doctoral Fellowship
   in Video Search. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Xiao-Ping Zhang.
CR Abisheva A, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P593, DOI 10.1145/2556195.2566588
   [Anonymous], 2010, P NIPS
   [Anonymous], 2013, P 6 ACM INT C WEB SE, DOI DOI 10.1145/2433396.2433443
   [Anonymous], P INT C MULT RETR
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bornoe Nis, 2010, CHI 10 HUM FACT COMP, P3325
   Borth D., 2013, P 21 ACM INT C MULT, P459
   Campanella Marco, 2008, P 22 BRIT HCI GROU, V2, P23
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Cheng X, 2013, IEEE T MULTIMEDIA, V15, P1184, DOI 10.1109/TMM.2013.2265531
   Cheng Xu., 2007, CoRR
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Figueiredo Flavio, 2011, P 4 ACM INT C WEB SE, P745, DOI DOI 10.1145/1935826.1935925
   Fox S, 2005, ACM T INFORM SYST, V23, P147, DOI 10.1145/1059981.1059982
   Habibian A., 2013, P 21 ACM INT C MULT, P419
   Hanjalic A, 2002, IEEE T CIRC SYST VID, V12, P90, DOI 10.1109/76.988656
   Hanjalic Alan., 2012, P 20 ACM INT C MULT, P1239
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Jiang Y.-G., 2011, ACM INT C MULT RETR, V29, P8
   Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2
   Kindberg T, 2005, IEEE PERVAS COMPUT, V4, P42, DOI 10.1109/MPRV.2005.42
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Kofler C, 2014, IEEE T MULTIMEDIA, V16, P1421, DOI 10.1109/TMM.2014.2315777
   Larson M., 2011, Proceedings of the 1st ACM International Conference on Multimedia Retrieval, P1
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lux M., 2010, CHI 10 HUM FACT COMP, P3913, DOI DOI 10.1145/1753846.1754078
   Mathias Lux, 2012, IM AN MULT INT SERV, P1
   Naaman M, 2011, J AM SOC INF SCI TEC, V62, P902, DOI 10.1002/asi.21489
   Park N, 2011, COMPUT HUM BEHAV, V27, P1996, DOI 10.1016/j.chb.2011.05.006
   Riegler M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P397, DOI 10.1145/2647868.2654894
   Rudinac S, 2012, INT J MULTIMED INF R, V1, P263, DOI 10.1007/s13735-012-0018-0
   Schmiedeke S., 2012, P 2 ACM INT C MULT R
   Siersdorfer S., 2010, Proceedings of the 19th International Conference on World Wide Web, WWW'10, P891, DOI DOI 10.1145/1772690.1772781
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Warrens MJ, 2010, ADV DATA ANAL CLASSI, V4, P271, DOI 10.1007/s11634-010-0073-4
   Weber I, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P43
   Williams M, 2000, SOCIOLOGY, V34, P209, DOI 10.1177/S0038038500000146
NR 38
TC 5
Z9 7
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1200
EP 1212
DI 10.1109/TMM.2015.2445573
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000007
DA 2024-07-18
ER

PT J
AU Li, KQ
   Tao, WB
AF Li, Kunqian
   Tao, Wenbing
TI Adaptive Optimal Shape Prior for Easy Interactive Object Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Easy interactive object segmentation; segmentation refinement; shape
   consistency evaluation; shape prior; shape registration; shape space
ID MULTIPLE PIECEWISE-CONSTANT; IMAGE SEGMENTATION; ALGORITHMS; MODELS;
   FLOW; CUTS
AB For interactive segmentation approaches, object segmentation in complicated background is cumbersome, and usually needs tedious interactions to refine the incomplete segmentations. In this paper, an adaptive optimal shape prior is proposed for easy interactive object segmentation. Different from the traditional shape priors which only provide loose constraint, our adaptive shape prior gives more accurate and individualized constraint by exploiting the shape information of incomplete segmentation. Moreover, by combining the non-rigid shape registration and a local shape consistency evaluation system presented in this paper, such adaptive optimal shape prior could be achieved automatically. Both of these contributions greatly lighten the burden on users and make interactive segmentation much easier. The comparison experiments on the newly-built TypShape dataset with the related algorithms have demonstrated good performance of the proposed algorithm.
C1 [Li, Kunqian; Tao, Wenbing] Huazhong Univ Sci & Technol, Sch Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Tao, WB (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Peoples R China.
EM wenbingtao@hust.edu.cn
FU National Natural Science Foundation of China [61371140]; National
   High-Tech R&D Program of China (863 Program) [2015AA015904]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61371140, and in part by the National High-Tech R&D
   Program of China (863 Program) under Grant 2015AA015904. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Ebroul Izquierdo. (Corresponding author: Wenbing
   Tao.)
CR Alqaisi T., 2012, 2012 19th IEEE International Conference on Image Processing (ICIP 2012), P2385, DOI 10.1109/ICIP.2012.6467377
   [Anonymous], 7694 CALTECH DEP COM
   [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI 10.1109/CVPR.2007.383018
   [Anonymous], 2004, Workshop on Generative Model Based Vision
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], THESIS CALTECH PASAD
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Borenstein E, 2002, LECT NOTES COMPUT SC, V2351, P109
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Dai JF, 2013, IEEE I CONF COMP VIS, P1305, DOI 10.1109/ICCV.2013.165
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Freedman D, 2005, PROC CVPR IEEE, P755
   Funka-Lea G, 2006, I S BIOMED IMAGING, P614
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073
   Han X, 2003, IEEE T PATTERN ANAL, V25, P755, DOI 10.1109/TPAMI.2003.1201824
   HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073
   Jiang TT, 2009, PROC CVPR IEEE, P848, DOI 10.1109/CVPRW.2009.5206568
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kim J, 2012, LECT NOTES COMPUT SC, V7578, P444, DOI 10.1007/978-3-642-33786-4_33
   Kohli P, 2007, IEEE T PATTERN ANAL, V29, P2079, DOI 10.1109/TPAMI.2007.1128
   Lempitsky V, 2009, IEEE I CONF COMP VIS, P277, DOI 10.1109/ICCV.2009.5459262
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Liu LM, 2011, PATTERN RECOGN, V44, P2819, DOI 10.1016/j.patcog.2011.04.031
   Ma TY, 2011, PROC CVPR IEEE, P1441, DOI 10.1109/CVPR.2011.5995591
   Meng FM, 2013, IEEE T MULTIMEDIA, V15, P2186, DOI 10.1109/TMM.2013.2280893
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Olszewska JI, 2008, INT CONF ACOUST SPEE, P721, DOI 10.1109/ICASSP.2008.4517711
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sabzmeydani P, 2007, PROC CVPR IEEE, P1251
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Singaraju D, 2009, PROC CVPR IEEE, P1303, DOI 10.1109/CVPRW.2009.5206669
   Tao WB, 2014, PROC CVPR IEEE, P1598, DOI 10.1109/CVPR.2014.207
   Tao WB, 2015, IEEE T IMAGE PROCESS, V24, P943, DOI 10.1109/TIP.2014.2387384
   Tao WB, 2012, IEEE T IMAGE PROCESS, V21, P284, DOI 10.1109/TIP.2011.2160955
   Tao WB, 2011, IMAGE VISION COMPUT, V29, P499, DOI 10.1016/j.imavis.2011.03.002
   Toshev A, 2010, PROC CVPR IEEE, P950, DOI 10.1109/CVPR.2010.5540114
   Veksler O, 2008, LECT NOTES COMPUT SC, V5304, P454, DOI 10.1007/978-3-540-88690-7_34
   Vicente S., 2008, PROC IEEE C COMPUT V, P1
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
NR 48
TC 12
Z9 12
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 994
EP 1005
DI 10.1109/TMM.2015.2433795
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300007
DA 2024-07-18
ER

PT J
AU Lee, Z
   Nguyen, TQ
AF Lee, Zucheul
   Nguyen, Truong Q.
TI Multi-Resolution Disparity Processing and Fusion for Large
   High-Resolution Stereo Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Disparity estimation; fusion; multi-resolution; refinement; sub-pixel
   estimation
ID BELIEF PROPAGATION
AB Large panoramic views with high resolution have the advantage of a wide field of view over regular stereo views. However, the large size and high resolution impose difficulties on the stereo matching problem such as complexity and structure ambiguity, respectively. In this paper, effective multi-resolution disparity processing to resolve the difficulties is presented. We propose to adaptively determine the disparity search range based on the combined local structure from image intensity and initial disparity. The adaptive disparity range is able to propagate the smoothness property at low resolution to high resolution while preserving fine details. It reduces structure ambiguity as well as computational complexity. To reduce the disparity quantization error at the coarse level, we propose a reliable multiple fitting algorithm that is noticeably effective on the round surface. The spatial-multi-resolution total variation method is investigated to minimize inconsistency in space-scale dimension. The experimental results on the Middlebury datasets and real-world high-resolution images demonstrate that the proposed multi-resolution scheme produces high-quality and high-resolution disparity maps by fusing individual multi-scale disparity maps, while reducing complexity.
C1 [Lee, Zucheul; Nguyen, Truong Q.] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Lee, Z (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM z1lee@ucsd.edu; tqn001@eng.ucsd.edu
RI Nguyen, Truong/JXN-9786-2024
FU NSF [CCF-1065305]; Intel/CISCO under the VAWN program; Ministry of
   Knowledge Economy (MKE), Korea [10041126]
FX This work was supported in part by the NSF under Grant CCF-1065305, by
   Intel/CISCO under the VAWN program, and by the Ministry of Knowledge
   Economy (MKE), Korea, under the Technology Development Program for
   Commercializing System Semiconductor (10041126, International
   Collaborative R&BD Project for System Semiconductor). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Pal Halvorsen.
CR Accame M., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P374, DOI 10.1109/ICIP.1995.537493
   [Anonymous], 2008, WORKSH MULT MULT SEN
   [Anonymous], 2006, P BMVC CIT
   [Anonymous], P BMVC
   [Anonymous], SPIE ELECT IMAG ENG
   Brox T, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P17, DOI 10.1007/3-540-31272-2_2
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chan SH, 2011, IEEE T IMAGE PROCESS, V20, P3097, DOI 10.1109/TIP.2011.2158229
   Felzenszwalb PR, 2004, PROC CVPR IEEE, P261
   Garcia F, 2010, IEEE IMAGE PROC, P2805, DOI 10.1109/ICIP.2010.5651112
   Grauer-Gray S., 2009, Proc. IEEE WACV, P1
   JAHNE B., 2006, Digital Image Processing
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239453
   Kudo T, 2006, 2006 IEEE 12TH DIGITAL SIGNAL PROCESSING WORKSHOP & 4TH IEEE SIGNAL PROCESSING EDUCATION WORKSHOP, VOLS 1 AND 2, P522, DOI 10.1109/DSPWS.2006.265478
   Lee Z, 2013, IEEE T MULTIMEDIA, V15, P1855, DOI 10.1109/TMM.2013.2270456
   Lindeberg T., 1994, Scale-Space Theory in Computer Vision, V256
   Rav-Acha A, 2008, INT J COMPUT VISION, V78, P187, DOI 10.1007/s11263-007-0101-9
   Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372
   Scharstein D., 2010, Middlebury stereo evaluation version 2
   Shimizu M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P90, DOI 10.1109/ICCV.2001.937503
   Sizintsev M, 2008, PROCEEDINGS OF THE FIFTH CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, P97, DOI 10.1109/CRV.2008.8
   Sizintsev M, 2010, IMAGE VISION COMPUT, V28, P352, DOI 10.1016/j.imavis.2009.06.008
   Stein AN, 2006, IEEE INT CONF ROBOT, P914, DOI 10.1109/ROBOT.2006.1641826
   Strang G., 1988, LINEAR ALGEBRA ITS A
   Tang H, 2012, IEEE T CIRC SYST VID, V22, P295, DOI 10.1109/TCSVT.2011.2178729
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Van Meerbergen G, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P166, DOI 10.1109/SMBV.2001.988775
   Yang Q., 2007, PROC IEEE COMPUT VIS, P1
   Yang Q., 2006, P 17 BRIT MACHINE VI, P989
   Yang QX, 2009, IEEE T PATTERN ANAL, V31, P492, DOI 10.1109/TPAMI.2008.99
   YANG YB, 1995, ARTIF INTELL, V78, P121, DOI 10.1016/0004-3702(95)00028-3
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Zhang Z, 2012, 2012 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV), P99, DOI 10.1109/IVS.2012.6232234
   Zheng JY, 2008, INT J COMPUT VISION, V78, P169, DOI 10.1007/s11263-007-0080-x
   Zucheul Lee, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P584, DOI 10.1109/ICASSP.2014.6853663
NR 35
TC 13
Z9 14
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2015
VL 17
IS 6
BP 792
EP 803
DI 10.1109/TMM.2015.2425141
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CI1TM
UT WOS:000354527500003
DA 2024-07-18
ER

PT J
AU Yang, XS
   Zhang, TZ
   Xu, CS
AF Yang, Xiaoshan
   Zhang, Tianzhu
   Xu, Changsheng
TI Cross-Domain Feature Learning in Multimedia
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-domain; deep learning; feature learning; multi-modal
AB In the Web 2.0 era, a huge number of media data, such as text, image/video, and social interaction information, have been generated on the social media sites (e.g., Facebook, Google, Flickr, and YouTube). These media data can be effectively adopted for many applications (e.g., image/video annotation, image/video retrieval, and event classification) in multimedia. However, it is difficult to design an effective feature representation to describe these data because they have multi-modal property (e.g., text, image, video, and audio) and multi-domain property (e.g., Flickr, Google, and YouTube). To deal with these issues, we propose a novel cross-domain feature learning (CDFL) algorithm based on stacked denoising auto-encoders. By introducing the modal correlation constraint and the cross-domain constraint in conventional auto-encoder, our CDFL can maximize the correlations among different modalities and extract domain invariant semantic features simultaneously. To evaluate our CDFL algorithm, we apply it to three important applications: sentiment classification, spam filtering, and event classification. Comprehensive evaluations demonstrate the encouraging performance of the proposed approach.
C1 [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] China Singapore Inst Digital Media, Singapore 119613, Singapore.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Yang, XS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM xiaoshang.yang@nlpr.ia.ac.cn; tzzhang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Zhang, Tianzhu/AGY-9389-2022
OI Zhang, Tianzhu/0000-0003-0764-6106
FU National Program on Key Basic Research Project under the 973 Program
   [2012CB316304]; National Natural Science Foundation of China [61225009,
   61303173, 61373122, 61432019]; Beijing Natural Science Foundation
   [4131004]
FX This work was supported in part by the National Program on Key Basic
   Research Project under the 973 Program, Project No. 2012CB316304, the
   National Natural Science Foundation of China under Grants 61225009,
   61303173, 61373122, and 61432019, and the Beijing Natural Science
   Foundation under Grant 4131004. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof. K.
   Selcuk Candan.
CR [Anonymous], 2009, DATASET SHIFT MACHIN
   [Anonymous], 2011, P ICML
   [Anonymous], 2006, Advances in Neural Information Processing Systems 19
   [Anonymous], 2012, ACM MM'12'
   [Anonymous], 2013, P 3 ACM C INT C MULT
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], P 2 ACM INT C MULT R
   [Anonymous], TOMCCAP
   [Anonymous], 2012, P 29 INT C MACH LEAR
   Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100
   Bao B.-K., 2013, Proceedings of the International Conference on Multimedia Retrieval, P135
   Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blitzer J., 2006, PROC C EMPIRICAL MET, P120, DOI DOI 10.3115/1610075.1610094
   Blitzer J., 2007, Proceedings of the 45th annual meeting of the association of computational linguistics, V45, P440
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Boumal N, 2014, J MACH LEARN RES, V15, P1455
   Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57
   Felzenszwalb P., 2013, TRECVID 2013, P1
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Glorot X., 2011, P 28 INT C INT C MAC, P513
   Gong B., 2013, P INT C MACH LEARN, P222
   Gong B., 2013, NIPS, P1286
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Habrard Amaury, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P433, DOI 10.1007/978-3-642-40991-2_28
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hoffman J, 2012, LECT NOTES COMPUT SC, V7573, P702, DOI 10.1007/978-3-642-33709-3_50
   Jin Xin, 2010, P 18 ACM INT C MULT, P1235, DOI [10.1145/1873951.1874196, DOI 10.1145/1873951.1874196]
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   Liu XC, 2013, PROCEEDINGS OF THE 1ST INTERNATIONAL JOINT SYMPOSIUM ON JOINING AND WELDING, P151
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu WT, 2013, IEEE T MULTIMEDIA, V15, P1920, DOI 10.1109/TMM.2013.2280895
   Ma G., 2013, P IEEE INT C MULT EX, P1
   Ma Z., 2012, MM 2012 - Proceedings of the 20th ACM International Conference on Multimedia, P469, DOI DOI 10.1145/2393347.2393414
   Meagher M, 2007, IEEE INT CONF INF VI, P601
   Naaman M, 2012, MULTIMED TOOLS APPL, V56, P9, DOI 10.1007/s11042-010-0538-7
   Orlando S, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P1285
   Pan SJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1187
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Petkos G., 2012, P 2 ACM INT C MULT R, P231, DOI 10. 1145/2324796.2324825.
   Qi G., 2011, Proc. ACM International Conference on World Wide Web, P297
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Reuter T., 2012, PROC ANN ACM INT C M, P22
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   Smolensky P., 1986, Information processing in dynamical systems: Foundations of harmony theory
   Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252
   Tan Chenhao., 2011, P 17 ACM SIGKDD INT, P1397, DOI DOI 10.1145/2020408.2020614
   THEIL H, 1988, STAT PROBABIL LETT, V6, P137, DOI 10.1016/0167-7152(88)90107-1
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang Y., 2012, Proceedings of the 20th ACM International Conference on Multimedia, P865, DOI [DOI 10.1145/2393347.239633216, 10.1145/2393347.239633216]
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
NR 59
TC 103
Z9 110
U1 0
U2 49
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2015
VL 17
IS 1
BP 64
EP 78
DI 10.1109/TMM.2014.2375793
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AX6QW
UT WOS:000347047400007
DA 2024-07-18
ER

PT J
AU Bouten, N
   Latré, S
   Famaey, J
   Van Leekwijck, W
   De Turck, F
AF Bouten, Niels
   Latre, Steven
   Famaey, Jeroen
   Van Leekwijck, Werner
   De Turck, Filip
TI In-Network Quality Optimization for Adaptive Video Streaming Services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive systems; HTTP adaptive streaming; integer linear programming;
   multimedia communication; quality of experience; quality of service;
   streaming media
ID PROPORTIONAL FAIRNESS
AB HTTP adaptive streaming (HAS) services allow the quality of streaming video to be automatically adapted by the client application in face of network and device dynamics. Due to their advantages compared to traditional techniques, HAS-based protocols are widely used for over-the-top (OTT) video streaming. However, they are yet to be adopted in managed environments, such as ISP networks. A major obstacle is the purely client-driven design of current HAS approaches, which leads to excessive quality oscillations, suboptimal behavior, and the inability to enforce management policies. Moreover, the provider has no control over the quality that is provided, which is essential when offering a managed service. This article tackles these challenges and facilitates the adoption of HAS in managed networks. Specifically, several centralized and distributed algorithms and heuristics are proposed that allow nodes inside the network to steer the HAS client's quality selection process. The algorithms are able to enforce management policies by limiting the set of available qualities for specific clients. Additionally, simulation results show that by coordinating the quality selection process across multiple clients, the proposed algorithms significantly reduce quality oscillations by a factor of five and increase the average delivered video quality by at least 14%.
C1 [Bouten, Niels; De Turck, Filip] Ghent Univ iMinds, Dept Elect & Comp Engn, B-9000 Ghent, Belgium.
   [Latre, Steven; Famaey, Jeroen] Univ Antwerp iMinds, Dept Math & Comp Sci, B-2610 Antwerp, Belgium.
   [Van Leekwijck, Werner] Alcatel Lucent Bell Labs, B-2018 Antwerp, Belgium.
C3 Ghent University; IMEC; IMEC; Alcatel-Lucent
RP Bouten, N (corresponding author), Ghent Univ iMinds, Dept Elect & Comp Engn, B-9000 Ghent, Belgium.
EM niels.bouten@intec.ugent.be
RI Bouten, Niels/G-7517-2012; Famaey, Jeroen/AAB-6171-2022; Latre,
   Steven/N-8689-2016
OI Famaey, Jeroen/0000-0002-3587-1354; De Turck, Filip/0000-0003-4824-1199;
   Latre, Steven/0000-0003-0351-1714
FU European Commission [318488]; iMinds MISTRAL Project [10838]; Ghent
   University; Hercules Foundation; Flemish Government Department EWI
FX This work was supported in part by the Seventh Framework Programme of
   the European Commission under Flamingo, a Network of Excellence Project
   (318488), in part by the iMinds MISTRAL Project under Grant 10838, and
   by Ghent University, the Hercules Foundation, and the Flemish Government
   Department EWI via the work carried out using the Stevin Supercomputer
   Infrastructure at Ghent University. The work of N. Bouten was supported
   by the Agency for Innovation by Science and Technology (IWT). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Feng Wu.
CR Adamic LA., 2002, Glottometrics, V3, P143, DOI DOI 10.1109/S0SE.2014.50
   Akhshabi Saamer, 2013, Proceeding of the 23rd ACM Workshop on Network and Operating Systems Support for Digital Audio and Video-NOSSDAV'13
   Andelin T., 2012, Proceedings of the 3rd ACM Multimedia Systems Conference, P149
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 2006, P 25 IEEE INT C COMP
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2001, Encyclopedia of Mathematics
   Benno S, 2011, BELL LABS TECH J, V16, P101, DOI 10.1002/bltj.20505
   Bouten N., 2012, 2012 8th International Conference on Network and Service Management (CNSM 2012), P336
   Bouten N, 2013, 2013 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2013), P1399
   Bouten N, 2013, J NETW SYST MANAG, V21, P677, DOI 10.1007/s10922-013-9262-8
   Bouten N, 2012, IEEE IFIP NETW OPER, P1248, DOI 10.1109/NOMS.2012.6212059
   Famaey J, 2013, 2013 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2013), P419
   Hsiao YM, 2010, PROCEEDINGS OF THE 2010 IEEE ASIA PACIFIC CONFERENCE ON CIRCUIT AND SYSTEM (APCCAS), P160, DOI 10.1109/APCCAS.2010.5774851
   HSU C, 2007, P ACM SPIE MULT COMP, P1
   Jain R., 1984, TR301 DEC RES
   Kelly F, 1997, EUR T TELECOMMUN, V8, P33, DOI 10.1002/ett.4460080106
   Kelly FP, 1998, J OPER RES SOC, V49, P237, DOI 10.2307/3010473
   Krishnamoorthi V, 2013, I S MOD ANAL SIM COM, P182, DOI 10.1109/MASCOTS.2013.26
   Kupka T., 2013, THESIS U OSLO OSLO
   Latré S, 2013, J NETW SYST MANAG, V21, P588, DOI 10.1007/s10922-012-9255-z
   Lee H.-C., 2003, Fuzzy Optimization and Decision Making, V2, P31
   Li W, 2014, IEEE ACM T NETWORK, V22, P191, DOI 10.1109/TNET.2013.2245145
   Liu CH, 2012, SIGNAL PROCESS-IMAGE, V27, P288, DOI 10.1016/j.image.2011.10.001
   Padhye J., 1998, Computer Communication Review, V28, P303, DOI 10.1145/285243.285291
   Parakh S., 2012, P INT C SIGN PROC CO, P1
   Plissonneau L., 2012, ACM MMSys'12, P203
   Robinson DC, 2012, BELL LABS TECH J, V16, P5, DOI 10.1002/bltj.20531
   Schierl T, 2007, IEEE INT SYMP CIRC S, P3455, DOI 10.1109/ISCAS.2007.378370
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Sutinen T, 2012, EURASIP J WIREL COMM, P1, DOI 10.1186/1687-1499-2012-25
   Wang X, 2014, CHANDOS ASIAN STUD, P1, DOI 10.1007/s00382-014-2387-y
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Pu, 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P65, DOI 10.1109/PV.2012.6229745
NR 37
TC 42
Z9 45
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2281
EP 2293
DI 10.1109/TMM.2014.2362856
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300017
OA Green Published
DA 2024-07-18
ER

PT J
AU Guo, L
   Zhou, DJ
   Goto, S
AF Guo, Li
   Zhou, Dajiang
   Goto, Satoshi
TI A New Reference Frame Recompression Algorithm and Its VLSI Architecture
   for UHDTV Video Codec
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Embedded compression; H.264/AVC; HEVC; lossless reference frame
   recompression; multi-mode DPCM and averaging prediction
ID COMPRESSION ALGORITHM; MOTION-ESTIMATION; SCHEME; DESIGN
AB Video encoders and decoders for HEVC-like compression standards require huge external memory bandwidth, which occupies a significant portion of the codec power consumption. To reduce the memory bandwidth, this paper presents a new lossless reference frame recompression algorithm along with a high-throughput hardware architecture. Firstly, hybrid spatial-domain prediction is proposed to combine the merits of DPCM scanning and averaging. The prediction is then enhanced with multiple modes to accommodate various image characteristics. Finally, efficient residual regrouping based on semi-fixed-length (SFL) coding is used to improve the compression performance. Compared to no compression, the proposed scheme can reduce data traffic by an average of 57.6% with no image quality degradation. The average compression ratio is 2.49, an improvement of at least 12.2-13.2%, relative to the state-of-the-art algorithms. By applying a reordered two-step architecture and the two optimizations, residual reuse and simplified coding mode decision, the hardware cost is similar to that of previous reference frame recompression architectures. The computational complexity increase caused by multi-mode prediction affects the HW cost slightly. This work can be implemented with 45.1 k gates for the compressor and 34.5 k gates for the decompressor at 300 MHz, enough to support a 3840 x 2160@60fps video encoder and decoder.
C1 [Guo, Li; Zhou, Dajiang; Goto, Satoshi] Waseda Univ, Grad Sch Informat Prod & Syst, Kitakyushu, Fukuoka 8080135, Japan.
C3 Waseda University
RP Guo, L (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
EM guoli0104@gmail.com; zhou@fuji.waseda.jp; goto@waseda.jp
FU Grants-in-Aid for Scientific Research [25870816] Funding Source: KAKEN
CR [Anonymous], P IEEE INT S CIRC SY
   Bao XN, 2012, IEEE T MULTIMEDIA, V14, P237, DOI 10.1109/TMM.2011.2171677
   Budagavi M, 2008, INT CONF ACOUST SPEE, P1165, DOI 10.1109/ICASSP.2008.4517822
   Chao P, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1441, DOI 10.1109/ICME.2008.4607716
   Chen CY, 2006, IEEE T CIRCUITS-I, V53, P578, DOI 10.1109/TCSI.2005.858488
   Chen TC, 2007, IEEE T CIRC SYST VID, V17, P242, DOI 10.1109/TCSVT.2006.887130
   Cheng CC, 2009, IEEE T CIRC SYST VID, V19, P141, DOI 10.1109/TCSVT.2008.2009250
   Dajiang Zhou, 2012, 2012 IEEE International Solid-State Circuits Conference (ISSCC), P224, DOI 10.1109/ISSCC.2012.6176985
   Ding LF, 2010, IEEE J SOLID-ST CIRC, V45, P46, DOI 10.1109/JSSC.2009.2031787
   Guo L., 2013, P 21 EUR SIGN PROC C, P1
   Ivanov Yuri V., 2008, 2008 3rd International Conference on Digital Telecommunications (ICDT), P168, DOI 10.1109/ICDT.2008.25
   Jinjia Zhou, 2013, 2013 Symposium on VLSI Circuits, pC286
   Kim J, 2010, IEEE T CIRC SYST VID, V20, P848, DOI 10.1109/TCSVT.2010.2045923
   Kuo HC, 2012, IEEE T MULTIMEDIA, V14, P500, DOI 10.1109/TMM.2012.2191945
   Lee S, 2010, IEEE INT CON MULTI, P232, DOI 10.1109/ICME.2010.5582592
   Lee YH, 2010, IEEE INT SYMP CIRC S, P1149, DOI 10.1109/ISCAS.2010.5537318
   Lee YH, 2008, IEEE ASIAN SOLID STA, P185, DOI 10.1109/ASSCC.2008.4708759
   Lee YX, 2008, IEEE INT SYMP CIRC S, P2586, DOI 10.1109/ISCAS.2008.4541985
   Lee YY, 2008, IEEE INT SOC CONF, P233, DOI 10.1109/SOCC.2008.4641518
   Li YF, 2012, IEEE INT C COMPUT, P303, DOI 10.1109/ICCSE.2012.49
   Lin YK, 2008, IEEE T CIRCUITS-I, V55, P1526, DOI 10.1109/TCSI.2008.916681
   Song L, 2010, EUR SIGNAL PR CONF, P2017
   Song T, 2007, IEICE ELECTRON EXPR, V4, P121, DOI 10.1587/elex.4.121
   XIPH.ORG FOUNDATION, XIPH ORG VID TEST ME
   Yang HT, 2009, 2009 IEEE/ACM/IFIP 7TH WORKSHOP ON EMBEDDED SYSTEMS FOR REAL-TIME MULTIMEDIA, P28, DOI 10.1109/ESTMED.2009.5336820
   Yng TLB, 2008, IEEE T CONSUM ELECTR, V54, P1453, DOI 10.1109/TCE.2008.4637640
   Zhou DJ, 2011, IEEE J SOLID-ST CIRC, V46, P777, DOI 10.1109/JSSC.2011.2109550
   Zhu JY, 2008, IEEE INT SYMP CIRC S, P3518, DOI 10.1109/ISCAS.2008.4542218
NR 28
TC 32
Z9 34
U1 3
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2323
EP 2332
DI 10.1109/TMM.2014.2350256
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300020
DA 2024-07-18
ER

PT J
AU Lu, SY
   Wang, ZY
   Mei, T
   Guan, GL
   Feng, DD
AF Lu, Shiyang
   Wang, Zhiyong
   Mei, Tao
   Guan, Genliang
   Feng, David Dagan
TI A Bag-of-Importance Model With Locality-Constrained Coding Based Feature
   Learning for Video Summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Locality-constrained linear coding; sparse coding; video summarization
ID FRAMEWORK; SELECTION
AB Video summarization helps users obtain quick comprehension of video content. Recently, some studies have utilized local features to represent each video frame and formulate video summarization as a coverage problem of local features. However, the importance of individual local features has not been exploited. In this paper, we propose a novel Bag-of-Importance (BoI) model for static video summarization by identifying the frames with important local features as keyframes, which is one of the first studies formulating video summarization at local feature level, instead of at global feature level. That is, by representing each frame with local features, a video is characterized with a bag of local features weighted with individual importance scores and the frames with more important local features are more representative, where the representativeness of each frame is the aggregation of the weighted importance of the local features contained in the frame. In addition, we propose to learn a transformation from a raw local feature to a more powerful sparse nonlinear representation for deriving the importance score of each local feature, rather than directly utilize the hand-crafted visual features like most of the existing approaches. Specifically, we first employ locality-constrained linear coding (LCC) to project each local feature into a sparse transformed space. LCC is able to take advantage of the manifold geometric structure of the high dimensional feature space and form the manifold of the low dimensional transformed space with the coordinates of a set of anchor points. Then we calculate the norm of each anchor point as the importance score of each local feature which is projected to the anchor point. Finally, the distribution of the importance scores of all the local features in a video is obtained as the BoI representation of the video. We further differentiate the importance of local features with a spatial weighting template by taking the perceptual difference among spatial regions of a frame into account. As a result, our proposed video summarization approach is able to exploit both the inter-frame and intra-frame properties of feature representations and identify keyframes capturing both the dominant content and discriminative details within a video. Experimental results on three video datasets across various genres demonstrate that the proposed approach clearly outperforms several state-of-the-art methods.
C1 [Lu, Shiyang; Wang, Zhiyong; Guan, Genliang; Feng, David Dagan] Univ Sydney, Sch Informat Technol, Sydney, NSW 2006, Australia.
   [Mei, Tao] Microsoft Res, Beijing 100080, Peoples R China.
C3 University of Sydney; Microsoft
RP Lu, SY (corresponding author), Univ Sydney, Sch Informat Technol, Sydney, NSW 2006, Australia.
EM shiyang@it.usyd.edu.au; zhiyong.wang@sydney.edu.au; tmei@microsoft.com;
   genliang.guan@sydney.edu.au; dagan.feng@sydney.edu.au
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307; Feng, Dagan/0000-0002-3381-214X; Wang,
   Zhiyong/0000-0002-8043-0312
FU ARC
FX This work was supported in part by ARC. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. X.-P. Zhang.
CR [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   [Anonymous], 2006, ADV NEURAL INF PROCE
   [Anonymous], 2009, P ADV NEUR INF PROC
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   Besiris D, 2009, MULTIMED TOOLS APPL, V44, P161, DOI 10.1007/s11042-009-0277-9
   Chen BW, 2009, IEEE T MULTIMEDIA, V11, P295, DOI 10.1109/TMM.2008.2009703
   Cheng CC, 2006, IEEE T MULTIMEDIA, V8, P585, DOI 10.1109/TMM.2006.870726
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   DeMenthon D., 1998, Proceedings ACM Multimedia 98, P211, DOI 10.1145/290747.290773
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Furini M, 2010, MULTIMED TOOLS APPL, V46, P47, DOI 10.1007/s11042-009-0307-7
   Gong YH, 2000, PROC CVPR IEEE, P174, DOI 10.1109/CVPR.2000.854772
   Guan GL, 2013, IEEE T CIRC SYST VID, V23, P729, DOI 10.1109/TCSVT.2012.2214871
   Guan GL, 2012, IEEE INT CONF MULTI, P570, DOI 10.1109/ICMEW.2012.105
   Kogler M., 2009, P WORKSH SEM MULT DA
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Li Z, 2005, IEEE T CIRC SYST VID, V15, P1245, DOI 10.1109/TCSVT.2005.854230
   Liu G, 2012, INT CONF SOFTW ENG, P1
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo JB, 2009, IEEE T CIRC SYST VID, V19, P289, DOI 10.1109/TCSVT.2008.2009241
   Luo Y, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P347, DOI 10.1109/ICCVW.2013.53
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Mundur P, 2006, INT J DIGIT LIBRARIE, V6, P219, DOI 10.1007/s00799-005-0129-9
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Panagiotakis C, 2009, IEEE T CIRC SYST VID, V19, P447, DOI 10.1109/TCSVT.2009.2013517
   Rigamonti R, 2011, PROC CVPR IEEE, P1545, DOI 10.1109/CVPR.2011.5995313
   Sokolova M, 2006, LECT NOTES COMPUT SC, V4304, P1015
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   VANHERK M, 1992, PATTERN RECOGN LETT, V13, P517, DOI 10.1016/0167-8655(92)90069-C
   Yang CL, 2013, PATTERN RECOGN, V46, P948, DOI 10.1016/j.patcog.2012.07.011
   Yuan M., 2006, Journal of the Royal Statistical Society, Series B, V70, P53
   Zhao WL, 2009, IEEE T IMAGE PROCESS, V18, P412, DOI 10.1109/TIP.2008.2008900
   Zhou XM, 2009, IEEE T MULTIMEDIA, V11, P879, DOI 10.1109/TMM.2009.2021794
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 37
TC 62
Z9 64
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1497
EP 1509
DI 10.1109/TMM.2014.2319778
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200001
DA 2024-07-18
ER

PT J
AU Macchiavello, B
   Dorea, C
   Hung, EM
   Cheung, G
   Tan, WT
AF Macchiavello, Bruno
   Dorea, Camilo
   Hung, Edson M.
   Cheung, Gene
   Tan, Wai-Tian
TI Loss-Resilient Coding of Texture and Depth for Free-Viewpoint Video
   Conferencing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video signal processing; video coding; error correction
ID MULTIVIEW VIDEO; CONCEALMENT
AB Free-viewpoint video conferencing allows a participant to observe the remote 3D scene from any freely chosen viewpoint. An intermediate virtual viewpoint image is typically synthesized using two pairs of transmitted texture and depth maps from two neighboring captured viewpoints via depth-image-based rendering (DIBR). Tomaintain high quality of synthesized images, it is imperative to contain the adverse effects of network packet losses that may arise during texture and depth video transmission. Towards this goal, we develop an integrated approach that exploits the representation redundancy inherent in the multiple streamed videos-a voxel in the 3D scene visible to two captured views is sampled and coded twice in the two views. In particular, at the receiver we first develop an error concealment strategy that adaptively blends corresponding pixels in the two captured views during DIBR, so that pixels from the more reliable transmitted view are weighted more heavily. We then couple it with a sender-side optimization of reference picture selection (RPS) during real-time video coding, so that blocks containing pixel samples of voxels that are visible in both views are more error-resiliently coded in one view only, given adaptive blending will mitigate errors in the other view. Further, synthesized view distortion sensitivities to texture versus depth errors are analyzed, so that relative importance of texture and depth code blocks can be computed for system-wide RPS optimization. Finally, quantization parameter (QP) is adaptively selected per frame, optimally trading off source distortion due to compression with channel distortion due to potential packet losses. Experimental results show that the proposed scheme can outperform previous work by up to 2.9 dB at 5% packet loss rate.
C1 [Macchiavello, Bruno; Dorea, Camilo; Hung, Edson M.] Univ Brasilia, BR-70910900 Brasilia, DF, Brazil.
   [Cheung, Gene] Natl Inst Informat, Tokyo 1018430, Japan.
   [Tan, Wai-Tian] Cisco Syst Inc, Enterprise Networking Grp, Milpitas, CA 95035 USA.
C3 Universidade de Brasilia; Research Organization of Information & Systems
   (ROIS); National Institute of Informatics (NII) - Japan; Cisco Systems
   Inc
RP Macchiavello, B (corresponding author), Univ Brasilia, BR-70910900 Brasilia, DF, Brazil.
EM bruno@image.unb.br; camilo@image.unb.br; mintsu@image.unb.br;
   cheung@nii.ac.jp; dtan2@cisco.com
RI Macchiavello, Bruno/AAB-7995-2019; Cheung, Gene/AAB-9284-2020
OI Macchiavello, Bruno/0000-0003-4697-2422; Cheung,
   Gene/0000-0002-5571-4137
FU Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq)
   [476176/2013-1, 310375/2011-8]; JSPS KAKENHI [23700136]; Grants-in-Aid
   for Scientific Research [23700136] Funding Source: KAKEN
FX This work was supported in part by the Conselho Nacional de
   Desenvolvimento Cientifico e Tecnologico (CNPq) under grants
   476176/2013-1 and 310375/2011-8 and in part by JSPS KAKENHI Grant Number
   23700136 The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Ebroul Izquierdo.
CR Ahn I., 2012, P IEEE INT C MULT EX
   [Anonymous], P SPIE
   Cheung G., 2011, P IEEE INT C IM PROC
   Cheung G., 2010, P IEEE PICT COD S NA
   Cheung G, 2007, IEEE T CIRC SYST VID, V17, P649, DOI 10.1109/TCSVT.2007.896620
   Daribo I., 2012, P 2010 PICT COD S KR
   Daribo I., 2012, P 3DTV C 2012 ZUR SW
   Eisenberg Y., 2003, P ICIP
   Flierl M, 2007, IEEE T CIRC SYST VID, V17, P1474, DOI 10.1109/TCSVT.2007.903780
   Fujii T., 2006, P IEEE INT C MULT EX
   Gautier J., 2012, P PICT COD S 2012 KR
   Gokturk S., 2004, P C COMP VIS PATT RE
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Hou C., 2010, P IEEE INT S BROADB
   Hu W., 2012, P IEEE INT C IM PROC
   Kim W.S., 2009, P IEEE INT C IM PROC
   Kim W.-S., 2010, P SPIE VISUAL INFORM
   Kubota A, 2007, IEEE SIGNAL PROC MAG, V24, P10, DOI 10.1109/MSP.2007.905873
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Lambert P., 2006, J VISUAL COMMUN IMAG, V17
   Macchiavello B., 2012, P IS T SPIE VIS INF
   Macchiavello B., 2012, P IEEE INT C IM PROC
   Mark W., 1997, P S INT 3D GRAPH NEW
   Martinian E., 2007, P IEEE INT S INF THE
   Maugey T., 2012, P IEEE INT C IM PROC
   Merkle P., 2010, P 2010 PICT COD S KR
   Merkle P., 2007, P IEEE INT C IM PROC
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   MPEG Video Group, 2008, N9595 ISOIEC JTC1SC2
   Oh K.-J., 2009, P PICT COD S CHIC IL
   Shen G., 2010, P IEEE PICT COD S NA
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Shum H., 2007, IMAGE BASED RENDERIN
   Stockhammer T., 2004, P IEEE INT C IM PROC
   Tan AS, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/632545
   Tanimoto M., 2008, M15377 MPEG
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Tröger T, 2011, IEEE T BROADCAST, V57, P777, DOI 10.1109/TBC.2011.2173813
   Valenzise G., 2012, P PICT COD S 2012 KR
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wang YB, 2009, IEEE T MULTIMEDIA, V11, P128, DOI 10.1109/TMM.2008.2008928
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiu XY, 2012, IEEE T MULTIMEDIA, V14, P1109, DOI 10.1109/TMM.2012.2191267
   Yang Y, 2011, IEEE T INFORM THEORY, V57, P5588, DOI 10.1109/TIT.2011.2162192
   Zhang C., 2009, P IEEE INT WORKSH MU
NR 46
TC 31
Z9 32
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 711
EP 725
DI 10.1109/TMM.2014.2299768
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lee, SY
   Sim, JY
   Kim, CS
   Lee, SU
AF Lee, Soon-Young
   Sim, Jae-Young
   Kim, Chang-Su
   Lee, Sang-Uk
TI Correspondence Matching of Multi-View Video Sequences Using Mutual
   Information Based Similarity Measure
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correspondence matching; Markov random field; multi-view videos; mutual
   information based similarity measure; visual sensor networks.
ID IMAGE; MAXIMIZATION; ALIGNMENT
AB We propose a correspondence matching algorithm for multi-view video sequences, which provides reliable performance even when the multiple cameras have significantly different parameters, such as viewing angles and positions. We use an activity vector, which represents the temporal occurrence pattern of moving foreground objects at a pixel position, as an invariant feature for correspondence matching. We first devise a novel similarity measure between activity vectors by considering the joint and individual behavior of the activity vectors. Specifically, we define random variables associated with the activity vectors and measure their similarity using the mutual information between the random variables. Moreover, to find a reliable homography transform between views, we find consistent pixel positions by employing the iterative bidirectional matching. We also refine the matching results of multiple source pixel positions by minimizing a matching cost function based on the Markov random field. Experimental results show that the proposed algorithm provides more accurate and reliable matching performance than the conventional activity-based and feature-based matching algorithms, and therefore can facilitate various applications of visual sensor networks.
C1 [Lee, Soon-Young] Samsung Elect Co Ltd, Div Mobile Commun, Mobile R&D Off, Suwon, South Korea.
   [Sim, Jae-Young] Ulsan Natl Inst Sci & Technol, Sch Elect & Comp Engn, Ulsan, South Korea.
   [Kim, Chang-Su] Korea Univ, Sch Elect Engn, Seoul, South Korea.
   [Lee, Sang-Uk] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
C3 Samsung Electronics; Samsung; Ulsan National Institute of Science &
   Technology (UNIST); Korea University; Seoul National University (SNU)
RP Lee, SY (corresponding author), Samsung Elect Co Ltd, Div Mobile Commun, Mobile R&D Off, Suwon, South Korea.
EM yxoony7@snu.ac.kr; jysim@unist.ac.kr; changsukim@korea.ac.kr;
   sanguk@snu.ac.kr
OI Kim, Chang-Su/0000-0002-4276-1831
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF); Ministry of Education, Science and Technology (MEST)
   [2012-0003908]; Global Frontier R&D Program on Human-centered
   Interaction for Coexistence; NRF of Korea; Korean Government (MSIP)
   [2011-0031648]; MEST [2012-0005410]
FX This work was supported in part by the Basic Science Research Program
   through the National Research Foundation of Korea (NRF) funded by the
   Ministry of Education, Science and Technology (MEST) (2012-0003908), in
   part by the Global Frontier R&D Program on Human-centered Interaction
   for Coexistence funded by the NRF of Korea grant funded by the Korean
   Government (MSIP) (2011-0031648), and in part by the NRF of Korea grant
   funded by the MEST (2012-0005410).
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], INTRO LINEAR ALGEBRA
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Caspi Y, 2002, IEEE T PATTERN ANAL, V24, P1409, DOI 10.1109/TPAMI.2002.1046148
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Ermis EB, 2010, IEEE T IMAGE PROCESS, V19, P2595, DOI 10.1109/TIP.2010.2052824
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Heo YS, 2011, IEEE T PATTERN ANAL, V33, P807, DOI 10.1109/TPAMI.2010.136
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690
   Khan S, 2003, IEEE T PATTERN ANAL, V25, P1355, DOI 10.1109/TPAMI.2003.1233912
   Lee L, 2000, IEEE T PATTERN ANAL, V22, P758, DOI 10.1109/34.868678
   Levine M., 1973, COMPUT VISION GRAPH, V2, P131
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maes F, 1997, IEEE T MED IMAGING, V16, P187, DOI 10.1109/42.563664
   McHugh JM, 2009, IEEE SIGNAL PROC LET, V16, P390, DOI 10.1109/LSP.2009.2016447
   Obraczka K, 2002, 5TH INTERNATIONAL SYMPOSIUM ON WIRELESS PERSONAL MULTIMEDIA COMMUNICATIONS, VOLS 1-3, PROCEEDINGS, P1177, DOI 10.1109/WPMC.2002.1088364
   Rinner B, 2008, P IEEE, V96, P1565, DOI 10.1109/JPROC.2008.928742
   Sand P, 2004, ACM T GRAPHIC, V23, P592, DOI 10.1145/1015706.1015765
   Sinha SN, 2004, PROC CVPR IEEE, P195
   Soro S, 2009, ADV MULTIMED, V2009, DOI 10.1155/2009/640386
   Studholme C, 1999, PATTERN RECOGN, V32, P71, DOI 10.1016/S0031-3203(98)00091-0
   Viola P, 1997, INT J COMPUT VISION, V24, P137, DOI 10.1023/A:1007958904918
   Zhengyou Zhang, 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P666, DOI 10.1109/ICCV.1999.791289
NR 25
TC 19
Z9 20
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1719
EP 1731
DI 10.1109/TMM.2013.2271747
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900001
DA 2024-07-18
ER

PT J
AU Roy, SD
   Mei, T
   Zeng, WJ
   Li, SP
AF Roy, Suman Deb
   Mei, Tao
   Zeng, Wenjun
   Li, Shipeng
TI Towards Cross-Domain Learning for Social Video Popularity Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-domain media retrieval; social media; transfer learning; Twitter;
   video popularity
AB Previous research on online media popularity prediction concluded that the rise in popularity of online videos maintains a conventional logarithmic distribution. However, recent studies have shown that a significant portion of online videos exhibit bursty/sudden rise in popularity, which cannot be accounted for by video domain features alone. In this paper, we propose a novel transfer learning framework that utilizes knowledge from social streams (e.g., Twitter) to grasp sudden popularity bursts in online content. We develop a transfer learning algorithm that can learn topics from social streams allowing us to model the social prominence of video content and improve popularity predictions in the video domain. Our transfer learning framework has the ability to scale with incoming stream of tweets, harnessing physical world event information in real-time. Using data comprising of 10.2 million tweets and 3.5 million YouTube videos, we show that social prominence of the video topic (context) is responsible for the sudden rise in its popularity where social trends have a ripple effect as they spread from the Twitter domain to the video domain. We envision that our cross-domain popularity prediction model will be substantially useful for various media applications that could not be previously solved by traditional multimedia techniques alone.
C1 [Roy, Suman Deb; Zeng, Wenjun] Univ Missouri, Dept Comp Sci, Columbia, MO 65211 USA.
   [Mei, Tao; Li, Shipeng] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 University of Missouri System; University of Missouri Columbia;
   Microsoft Research Asia; Microsoft
RP Roy, SD (corresponding author), Univ Missouri, Dept Comp Sci, Columbia, MO 65211 USA.
EM sumandebroy9@gmail.com; tmei@microsoft.com; zengw@missouri.edu;
   spli@microsoft.com
RI Mei, Tao/GQZ-0596-2022; Li, Shipeng/AAA-3374-2020
OI Mei, Tao/0000-0002-5990-7307; Li, Shipeng/0000-0001-5368-4256
CR [Anonymous], 2010, P NEUR INF PROC SYST
   [Anonymous], P ACM MULT
   Asur S., 2010, P ACM INT C WEB INT
   Barabási AL, 2005, NATURE, V435, P207, DOI 10.1038/nature03459
   BOSER BE, 1992, P ANN WORKSH COMP LE
   Broxton T., 2010, P IEEE SOC INT AN SE
   Burgess J., 2009, YouTube: Online video and participatory culture
   Chatzopoulou G., 2010, P IEEE INFOCOM
   Cheng X., 2008, P INT WORKSH QUAL SE
   Cheng X., 2007, COMPUT RES REPOSITOR, V0707.3
   Dai W., 2009, P INT C MACH LEARN
   Deb S. R., 2012, P ACM MULT
   Figueiredo F., 2011, P WEB SEARCH DAT MIN
   Filippova K., 2011, P ACM SIGIR C RES DE
   Gursun G., 2011, P IEEE INFOCOM
   Lin F., 2010, P INT C MACH LEARN
   Ling X., 2008, P ACM C KNOWL DISC D
   Ma H, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961212
   Mavroeidis D., 2011, Proc. of IJCAI, P2692
   Naaman M, 2012, MULTIMED TOOLS APPL, V56, P9, DOI 10.1007/s11042-010-0538-7
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Qi GJ, 2011, PROC CVPR IEEE, P897, DOI 10.1109/CVPR.2011.5995312
   Ramage D., 2010, P AAAI C WEBL SOC ME
   Ratkiewicz J., 2010, Proceedings of the 2010 IEEE Second International Conference on Social Computing (SocialCom 2010). the Second IEEE International Conference on Privacy, Security, Risk and Trust (PASSAT 2010), P393, DOI 10.1109/SocialCom.2010.63
   Roy S. D., 2012, P IEEE INT C MULT EX
   Shamma David A, 2011, P ICWSM
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Toderici G., 2010, P IEEE COMP VIS PATT
   Wattenhofer M., 2012, P AAAI C WEBLOGS SOC
   Xu Q., 2011, P INT WORKSH SOC WEB
   Zhou R., 2010, P INT C INT MEAS
NR 31
TC 90
Z9 102
U1 0
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1255
EP 1267
DI 10.1109/TMM.2013.2265079
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400004
DA 2024-07-18
ER

PT J
AU Chen, Q
   Wang, M
   Huang, ZY
   Hua, Y
   Song, Z
   Yan, SC
AF Chen, Qiang
   Wang, Meng
   Huang, Zhongyang
   Hua, Yang
   Song, Zheng
   Yan, Shuicheng
TI VideoPuzzle: Descriptive One-Shot Video Composition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; one-shot video; video authoring; video transition
ID AUTOMATIC COMPOSITION; IMAGE
AB A large amount of short, single-shot videos are created by personal camcorder every day, such as the small video clips in family albums, and thus a solution for presenting and managing these video clips is highly desired. From the perspective of professionalism and artistry, long-take/shot video, also termed one-shot video, is able to present events, persons or scenic spots in an informative manner. This paper presents a novel video composition system "Video Puzzle" which generates aesthetically enhanced long-shot videos from short video clips. Our task here is to automatically composite several related single shots into a virtual long-take video with spatial and temporal consistency.
   We propose a novel framework to compose descriptive long-take video with content-consistent shots retrieved from a video pool. For each video, frame-by-frame search is performed over the entire pool to find start-end content correspondences through a coarse-to-fine partial matching process. The content correspondence here is general and can refer to the matched regions or objects, such as human body and face. The content consistency of these correspondences enables us to design several shot transition schemes to seamlessly stitch one shot to another in a spatially and temporally consistent manner. The entire long-take video thus comprises several single shots with consistent contents and fluent transitions. Meanwhile, with the generated matching graph of videos, the proposed system can also provide an efficient video browsing mode. Experiments are conducted on multiple video albums and the results demonstrate the effectiveness and the usefulness of the proposed scheme.
C1 [Chen, Qiang; Song, Zheng; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
   [Wang, Meng] Hefei Univ Technol, Hefei, Peoples R China.
   [Huang, Zhongyang; Hua, Yang] Panasonic Singapore Labs, Panasonic R&D Ctr Singapore, Singapore, Singapore.
C3 National University of Singapore; Hefei University of Technology;
   Panasonic
RP Chen, Q (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
EM chenqiang@nus.edu.sg; eric.mengwang@gmail.com; zheng.s@nus.edu.sg;
   eleyans@nus.edu.sg
RI Yan, Shuicheng/HCI-1431-2022; Wang, Meng/ITR-8699-2023; chen,
   qiang/GWZ-7308-2022; hua, yang/GSE-0594-2022; chen, qiang/HGU-5418-2022
OI Hua, Yang/0000-0001-5536-503X
FU National Basic Research Program of China (973 Program) [2013CB329604];
   Natural Science Foundation of China [61272393]; Singapore Ministry of
   Education [MOE2010-T2-1-087]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) (Grant No. 2013CB329604) and the Natural Science
   Foundation of China (Grant No: 61272393). This work was also supported
   by Singapore Ministry of Education under research Grant
   MOE2010-T2-1-087. The work was performed when Q. Chen did his part-time
   industry intern in Panasonic Singapore Laboratories. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Sheng-Wei (Kuan-Ta) Chen.
CR Ahanger G, 1998, IEEE T KNOWL DATA EN, V10, P967, DOI 10.1109/69.738360
   Barnes C., 2010, P SIGGRAPH
   Bennett EP, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276505, 10.1145/1239451.1239553]
   Bhat KS, 2004, ACM T GRAPHIC, V23, P360, DOI 10.1145/1015706.1015729
   Boreczky J. S., 2000, P CHI
   Calic J, 2007, IEEE T CIRC SYST VID, V17, P931, DOI 10.1109/TCSVT.2007.897466
   Caspi Y, 2006, VISUAL COMPUT, V22, P642, DOI 10.1007/s00371-006-0046-y
   Chiu P., 2004, P ICME
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Correa CD, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778825
   Cotsaces C, 2006, IEEE SIGNAL PROC MAG, V23, P28, DOI 10.1109/MSP.2006.1621446
   Cutting J. E., 2002, PERCEPTION
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Heath K., 2010, P CVPR
   Hua XS, 2004, IEEE T CIRC SYST VID, V14, P572, DOI 10.1109/TCSVT.2004.826750
   Huang C, 2007, IEEE T PATTERN ANAL, V29, P671, DOI 10.1109/TPAMI.2007.1011
   Kemelmacher-Shlizerman I., 2011, P SIGGRAPH
   Kim B., 2005, P CGI
   King BruceM., 2003, STAT REASONING PSYCH, VFourth
   Kobayashi J., 2010, P PSIVT
   Kovar L., 2008, P SIGGRAPH
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lee D., 2010, P ECCV
   Lee J., 2005, P ACM MM
   Liu H., 2010, P ICML
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu S., 2004, P ICME
   Mei T, 2009, VISUAL COMPUT, V25, P39, DOI 10.1007/s00371-008-0282-4
   Philbin O. C., 2008, P BMVC
   Rav-Acha A., 2005, P CVPR
   Scharcanski J., 2006, P ICIP
   Schodl A., 2000, P SIGGRAPH
   Shlizerman I. K., 2011, ACM T GRAPH, V30
   Szeliski R., 1997, P SIGGRAPH
   Tal A., 1999, COMPUT GRAPH FORUM
   Tarjan R., 1972, SIAM Journal on Computing, V1, P146, DOI 10.1137/0201010
   Uchihashi S., 1999, P ACM MM
   Vendrig J, 2002, IEEE T MULTIMEDIA, V4, P492, DOI 10.1109/TMM.2002.802021
   Wang JJ, 2008, MULTIMEDIA SYST, V14, P179, DOI 10.1007/s00530-008-0112-6
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wang T., 2009, P CVMP
   Wang TH, 2011, COMPUT GRAPH-UK, V35, P54, DOI 10.1016/j.cag.2010.11.004
   Wolberg G, 1998, VISUAL COMPUT, V14, P360, DOI 10.1007/s003710050148
   Yan WQ, 2005, MULTIMEDIA SYST, V11, P3, DOI 10.1007/s00530-005-0186-3
   Zhang GF, 2009, IEEE T VIS COMPUT GR, V15, P828, DOI 10.1109/TVCG.2009.47
NR 46
TC 11
Z9 11
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 521
EP 534
DI 10.1109/TMM.2012.2236306
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900005
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Yan, JY
   Mühlbauer, W
   Plattner, B
AF Yan, Jinyao
   Muehlbauer, Wolfgang
   Plattner, Bernhard
TI Analytical Framework for Improving the Quality of Streaming Over TCP
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Buffer size; delay; HTTP/TCP streaming; overflow; quality of experience
   (QoE); underflow
ID CONGESTION CONTROL; VIDEO; MODEL
AB Multimedia streaming applications are traditionally delivered over UDP. Recent measurements show that more and more multimedia streaming data are over TCP as web-based TV, P2P streaming, video sharing websites are getting increasingly popular. To improve the quality of experience (QoE) for users and to cope with variability in TCP throughput, streaming applications typically implement buffers. Yet, for improving the QoE and the streaming quality, e. g., playback continuity and timeliness, it is critical to dimension buffers and the initial buffering delay appropriately.
   In this paper, we first develop a model for TCP streaming systems and an analytical framework to assess the QoE. Our emphasis is on buffer occupancy, which depends on the TCP arriving rate and the playout rate (the coding rate). We observe that TCP window "bounds", namely congestion window sizes immediately before a triple duplicate or timeout event, allow to distinguish the minimum and maximum buffer occupancy for TCP streaming systems. As confirmed by experiments, the proposed analytical framework allows to estimate the frequency of buffer overflow or underflow events if buffer sizes and the initial buffering delays are known parameters, or conversely, to dimension the buffer and delay appropriately.
   We further extend our model and analysis for P2P multicast streaming systems. Simulations and experiments in real networks validate our proposed analytical framework in terms of underflow/overflow probabilities and delay.
C1 [Yan, Jinyao] Commun Univ China, Beijing 100024, Peoples R China.
   [Muehlbauer, Wolfgang; Plattner, Bernhard] ETH, Swiss Fed Inst Technol, Commun Syst Grp, CH-8092 Zurich, Switzerland.
C3 Communication University of China; Swiss Federal Institutes of
   Technology Domain; ETH Zurich
RP Yan, JY (corresponding author), Commun Univ China, Beijing 100024, Peoples R China.
EM jyan@cuc.edu.cn; wolfgang.muehlbauer@tik.ee.ethz.ch;
   plattner@tik.ee.ethz.ch
RI yan, jy/ISS-1790-2023
FU Swiss National Science Foundation [200020-121753]; NSFC (National
   Natural Science Foundation of China) [60970127, 60832004]; Program for
   New Century Excellent Talents in Chinese University [NCET-09-0709];
   Swiss National Science Foundation (SNF) [200020_121753] Funding Source:
   Swiss National Science Foundation (SNF)
FX This work was supported in part by the Swiss National Science Foundation
   under grant No. 200020-121753, by NSFC (National Natural Science
   Foundation of China) under grant No. 60970127 and No. 60832004, and by
   the Program for New Century Excellent Talents in Chinese University
   (NCET-09-0709). Part of this work was done while J. Yan was at ETH
   Zurich. The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Jon Crowcroft.
CR [Anonymous], P IPDPS 2009 ROM IT
   Chang H, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P417
   Chu YH, 2000, PERF E R SI, V28, P1, DOI 10.1145/345063.339337
   Ciullo D, 2010, IEEE T MULTIMEDIA, V12, P54, DOI 10.1109/TMM.2009.2036231
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Floyd S, 1999, IEEE ACM T NETWORK, V7, P458, DOI 10.1109/90.793002
   Guo L., 2006, Proc. ACM SIGCOMM Internet Measurement Conference (IMC), P217
   Handley M., 2000, TCP CONGESTION WINDO
   Hei XJ, 2007, IEEE J SEL AREA COMM, V25, P1640, DOI 10.1109/JSAC.2007.071204
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Hosseini M, 2007, IEEE COMMUN SURV TUT, V9, P58, DOI 10.1109/COMST.2007.4317616
   Kim T, 2006, PROC SPIE, V6077, DOI 10.1117/12.643583
   Liang GF, 2008, IEEE T MULTIMEDIA, V10, P1128, DOI 10.1109/TMM.2008.2001364
   Liu JC, 2008, P IEEE, V96, P11, DOI 10.1109/JPROC.2007.909921
   Padhye J, 2000, IEEE ACM T NETWORK, V8, P133, DOI 10.1109/90.842137
   Rhee IJ, 2007, IEEE ACM T NETWORK, V15, P852, DOI 10.1109/TNET.2007.893883
   Saxena Mohit., 2008, NOSSDAV '08: Proceedings of the 18th International Workshop on Network and Operating Systems Support for Digital Audio and Video, P39
   Schulzrinne H, 2003, RTP TRANSPORT PROTOC
   Stockhammer T, 2004, IEEE T MULTIMEDIA, V6, P268, DOI 10.1109/TMM.2003.822795
   Systems Cisco., 2011, Cisco visual networking index: Forecast and methodology, 2010-2015
   Vojnovic M, 2005, IEEE ACM T NETWORK, V13, P568, DOI 10.1109/TNET.2005.850199
   Wang B, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1352012.1352020
   Wu CC, 2009, NOSSDAV 09: 18TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P97
   Yan J., 2010, ANAL FRAMEWORK STREA
   Yan JY, 2011, LECT NOTES COMPUT SC, V6869, P370
   Yan JY, 2006, IEEE T MULTIMEDIA, V8, P196, DOI 10.1109/TMM.2005.864265
NR 26
TC 17
Z9 18
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1579
EP 1590
DI 10.1109/TMM.2012.2187182
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400008
DA 2024-07-18
ER

PT J
AU Herranz, L
   Calic, J
   Martínez, JM
   Mrak, M
AF Herranz, Luis
   Calic, Janko
   Martinez, Jose M.
   Mrak, Marta
TI Scalable Comic-Like Video Summaries and Layout Disturbance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Comics; scalability; usability; user interface; video summarization
ID FRAMEWORK; MPEG-7
AB This paper describes an efficient system for scalable video summarization that exploits comic-like summaries and multi-scale representations to facilitate interactivity and balance between content coverage and compactness. Due to the layout disturbance induced by the transitions between scales, a new heuristic algorithm is proposed to restrict changes to bounded summary segments. Conducted user evaluations show that the proposed methodology improves usability while keeping the summaries compact and informative.
C1 [Herranz, Luis] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Calic, Janko] Univ Surrey, I Lab, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
   [Martinez, Jose M.] Univ Autonoma Madrid, Video Proc & Understanding Lab, Escuela Politecn Super, E-28049 Madrid, Spain.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   University of Surrey; Autonomous University of Madrid
RP Herranz, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
RI Calic, Janko/K-7420-2012; Herranz, Luis/B-4573-2016; Martinez,
   Jose/A-1185-2008
OI Calic, Janko/0000-0001-7209-0584; Herranz, Luis/0000-0002-7022-3395;
   Martinez, Jose/0000-0002-2236-1769; Mrak, Marta/0000-0002-7777-0252
FU National Natural Science Foundation of China [61150110480]; Chinese
   Academy of Sciences [2011Y1GB05]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61150110480 and in part by the Chinese
   Academy of Sciences Fellowships for Young International Scientists under
   Grant 2011Y1GB05. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Abdulmotaleb El
   Saddik.
CR Albanese M, 2006, INFORM SYST, V31, P679, DOI 10.1016/j.is.2005.12.003
   [Anonymous], 1994, UNDERSTANDING COMICS
   Calic J., 2007, EURASIP J ADV SIG PR, V2, P14
   Calic J, 2007, IEEE T CIRC SYST VID, V17, P931, DOI 10.1109/TCSVT.2007.897466
   Fonseca PM, 2004, SIGNAL PROCESS-IMAGE, V19, P685, DOI 10.1016/j.image.2004.04.005
   Girgensohn A, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P77
   Herranz L, 2010, IEEE T CIRC SYST VID, V20, P1265, DOI 10.1109/TCSVT.2010.2057020
   Herranz L, 2009, SIGNAL PROCESS-IMAGE, V24, P499, DOI 10.1016/j.image.2009.02.010
   KRAAIJ W, 2006, P TRECVID
   Likert R., 1932, TECHNIQUE MEASUREMEN, DOI 1933-01885-001
   Mei T, 2009, VISUAL COMPUT, V25, P39, DOI 10.1007/s00371-008-0282-4
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Petersohn C., 2004, P TRECVID
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Tseng BL, 2004, IEEE MULTIMEDIA, V11, P42, DOI 10.1109/MMUL.2004.1261105
   Wang F, 2009, IEEE INT CON MULTI, P1326, DOI 10.1109/ICME.2009.5202747
   Yeung MM, 1997, IEEE T CIRC SYST VID, V7, P771, DOI 10.1109/76.633496
   Zhu XQ, 2004, MULTIMEDIA SYST, V10, P98, DOI 10.1007/s00530-004-0142-7
NR 19
TC 12
Z9 12
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1290
EP 1297
DI 10.1109/TMM.2012.2192917
PN 2
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xu, Q
   Huang, Q
   Jiang, T
   Yan, BW
   Lin, WS
   Yao, Y
AF Xu, Qianqian
   Huang, Qingming
   Jiang, Tingting
   Yan, Bowei
   Lin, Weisi
   Yao, Yuan
TI HodgeRank on Random Graphs for Subjective Video Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hodgerank; paired comparison; persistence homology; random graphs; video
   quality assessment (VQA)
ID TOPOLOGY
AB This paper introduces a novel framework, HodgeRank on Random Graphs, based on paired comparison, for subjective video quality assessment. Two types of random graph models are studied, i.e., Erdos-Renyi random graphs and random regular graphs. Hodge decomposition of paired comparison data may derive, from incomplete and imbalanced data, quality scores of videos and inconsistency of participants' judgments. We demonstrate the effectiveness of the proposed framework on LIVE video database. Both of the two random designs are promising sampling methods without jeopardizing the accuracy of the results. In particular, due to balanced sampling, random regular graphs may achieve better performances when sampling rates are small. However, when the number of videos is large or when sampling rates are large, their performances are so close that Erdos-Renyi random graphs, as the simplest independent and identically distributed sampling scheme, could provide good approximations to random regular graphs, as a dependent sampling scheme. In contrast to the traditional deterministic incomplete block designs, our random design is not only suitable for traditional laboratory studies, but also for crowdsourcing experiments on Internet where the raters are distributive and it is hard to control with fixed designs.
C1 [Xu, Qianqian] Chinese Acad Sci, Grad Univ, Beijing 100190, Peoples R China.
   [Yao, Yuan] Peking Univ, Sch Math Sci, LMAM & LMP, Beijing 100871, Peoples R China.
   [Huang, Qingming] Chinese Acad Sci, Grad Univ, Beijing 100190, Peoples R China.
   [Huang, Qingming] Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Jiang, Tingting] Peking Univ, Natl Engn Lab Video Technol, Key Lab Machine Percept MoE, Sch EECS, Beijing 100871, Peoples R China.
   [Yan, Bowei] Peking Univ, Sch Math Sci, Beijing 100871, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Nanyang 639798, Singapore.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Peking University; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute
   of Computing Technology, CAS; Peking University; Peking University;
   Nanyang Technological University
RP Xu, Q (corresponding author), Chinese Acad Sci, Grad Univ, Beijing 100190, Peoples R China.
EM qqxu@jdl.ac.cn; qmhuang@jdl.ac.cn; ttjiang@pku.edu.cn;
   yanbowei@gmail.com; wslin@ntu.edu.sg; yuany@math.pku.edu.cn
RI Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011
OI Lin, Weisi/0000-0001-9866-1947; Jiang, Tingting/0000-0002-5372-0656
FU National Basic Research Program of China (973 Program) [2012CB316400];
   National Natural Science Foundation of China [61025011, 61071157,
   60833006]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2012CB316400, and in part by the
   National Natural Science Foundation of China under Grant 61025011, Grant
   61071157, and Grant 60833006. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Sheng-Wei (Kuan-Ta) Chen.
CR Alonso Omar, 2008, SIGIR Forum, V42, P9, DOI 10.1145/1480506.1480508
   [Anonymous], 1988, GRIFFINS STAT MONOGR
   [Anonymous], 1996, ITU-T Recommendation
   [Anonymous], 2003, Oxford Stud. in Probab.
   [Anonymous], 2008, LIVE VIDEO QUALITY A
   [Anonymous], 2000, Large margin rank boundaries for ordinal regression
   Arrow KJ, 1950, J POLIT ECON, V58, P328, DOI 10.1086/256963
   Barabási AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Bollobas B, 2001, Cambridge Studies in Advanced Mathematics, V2nd, DOI DOI 10.1017/CBO9780511814068
   Carlsson G, 2009, B AM MATH SOC, V46, P255, DOI 10.1090/S0273-0979-09-01249-X
   Chen KT, 2009, MATH COMPUT SCI ENG, P491, DOI 10.1145/1631272.1631339
   Chung F., 2006, CBMS REGIONAL C SER
   Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2
   Edelsbrunner H., 2010, American Mathematical Soc., DOI DOI 10.1090/MBK/069
   Eichhorn A, 2010, NOSSDAV 2010: PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P63
   Erdos P., 1959, Publicationes Mathematicae Debrecen, V6, P18
   Freund Y, 2004, J MACH LEARN RES, V4, P933, DOI 10.1162/1532443041827916
   Hastie T, 1998, ANN STAT, V26, P451
   Howe J, 2006, WIRED, V14, P1, DOI DOI 10.1086/599595
   ITU-R, 2002, ITU R METH SUBJ ASS
   Jiang X., 2011, MATH PROGRAM, V127, P6470
   Kahle M, 2009, DISCRETE MATH, V309, P1658, DOI 10.1016/j.disc.2008.02.037
   Kendall M. G., 1948, Rank correlation methods.
   Kendall MG, 1940, BIOMETRIKA, V31, P324, DOI DOI 10.1093/BIOMET/31.3-4.324
   Kim J H., 2003, P 35 ANN ACM S THEOR, ppp 213
   Kim JH, 2004, ADV MATH, V188, P444, DOI 10.1016/j.aim.2003.10.007
   Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453
   Koutis I, 2010, ANN IEEE SYMP FOUND, P235, DOI 10.1109/FOCS.2010.29
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   NOETHER GE, 1960, PSYCHOMETRIKA, V25, P357, DOI 10.1007/BF02289753
   Nowak S., 2010, INT C MULT INF RETR
   SAATY TL, 1977, J MATH PSYCHOL, V15, P234, DOI 10.1016/0022-2496(77)90033-5
   Sexton H., 2009, JPLEX JAVA SOFTWARE
   Sorokin Alexander, 2008, PROC CVPR IEEE, P1, DOI DOI 10.1109/CVPRW.2008.4562953
   Spielman D., 2004, 36 ANN ACM S THEOR C
   Thurstone LL, 1926, J ABNORM SOC PSYCH, V21, P384
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Wormald N. C., 1999, London mathematical society lecture note series, V239
   Xu Qianqian., 2011, ACM INT C MULTIMEDIA, P393
   Zomorodian A, 2005, DISCRETE COMPUT GEOM, V33, P249, DOI 10.1007/s00454-004-1146-y
NR 40
TC 42
Z9 45
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 844
EP 857
DI 10.1109/TMM.2012.2190924
PN 2
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700016
DA 2024-07-18
ER

PT J
AU Ryu, Y
AF Ryu, Yeonseung
TI A Flash Translation Layer for NAND Flash-Based Multimedia Storage
   Devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Address translation; flash memory; flash translation layer (FTL);
   mapping table; multimedia storage
ID MEMORY
AB NAND flash memory-based storage devices are becoming an attractive storage solution for multimedia storage servers because they afford several advantages including fast read access speed and low power consumption. Multimedia storage devices differ significantly from traditional storage devices in that the user files are accessed in a sequential read manner while metadata are updated frequently. In this paper, we propose a new flash translation layer (FTL) scheme called filtering FTL (FFTL) for NAND flash-based multimedia storage devices. The main idea of the FFTL scheme is to filter update requests for metadata and manage them separately from requests for user data. Our experimental results show that the proposed FFTL scheme outperforms existing FTL schemes by dramatically reducing the garbage collection overhead.
C1 Myongji Univ, Dept Comp Engn, Yongin 449728, Gyeonggi Do, South Korea.
C3 Myongji University
RP Ryu, Y (corresponding author), Myongji Univ, Dept Comp Engn, Yongin 449728, Gyeonggi Do, South Korea.
EM ysryu@mju.ac.kr
FU Korean Government [KRF-2008-314-D00344]; Ministry of Education, Science
   and Technology [2010-0021897]
FX This work was supported in part by the Korea Research Foundation Grant
   funded by the Korean Government (KRF-2008-314-D00344) and by the Basic
   Science Research Program through the National Research Foundation of
   Korea(NRF) funded by the Ministry of Education, Science and Technology
   (2010-0021897). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhengyou Zhang.
CR Agrawal Nitin., 2008, Proc. Annual Technical Conference (ATC), P57
   [Anonymous], 2006, ACM SIGOPS OPER SYST
   [Anonymous], 2007, CHEMIA
   Ban Amir, 1995, US Patent, Patent No. [5,404,485, 5404485]
   BOBOILA S, 2010, P 8 USENIX C FIL STO, P115
   Caulfield AM, 2009, ACM SIGPLAN NOTICES, V44, P217, DOI 10.1145/1508284.1508270
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chang L.-P., 2004, ACM Trans. on Embedded Computing Syst, V3, P837
   Chang LP, 2010, IEEE T COMPUT, V59, P1337, DOI 10.1109/TC.2010.14
   Chang YH, 2010, IEEE INT CONF EMBED, P237, DOI 10.1109/RTCSA.2010.37
   Chen F, 2009, PERF E R SI, V37, P181
   Chiang ML, 1999, J SYST SOFTWARE, V48, P213, DOI 10.1016/S0164-1212(99)00059-X
   Chung TS, 2009, J SYST ARCHITECT, V55, P332, DOI 10.1016/j.sysarc.2009.03.005
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Im S, 2011, IEEE T COMPUT, V60, P80, DOI 10.1109/TC.2010.197
   Intel Corporation, 1998, UND FLASH TRANSL LAY
   Kang J., 2006, Proceedings of the International Conference on Embedded Software (EMSOFT), P161
   KAWAGUCHI A, 1995, PROCEEDINGS OF THE 1995 USENIX TECHNICAL CONFERENCE, P155
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   Kwon Hunki, 2010, P 10 ACM INT C EMB S, P169, DOI DOI 10.1145/1879021.1879044
   Lee S.W., 2008, Proceedings of the 2008 ACM SIGMOD international conference on Management of data, SIGMOD '08, P1075
   Lee S, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1234675.1234677
   Lee Sungjin, 2008, SIGOPS OPER SYST REV, V42, P36
   MOSHAYEDI M, 2008, ACM QUEUE, V6, P32
   Narayanan D, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P145
   Park B, 2010, IEEE ICC
   Park C, 2008, ACM T EMBED COMPUT S, V7, DOI 10.1145/1376804.1376806
   RYU Y, 2004, Patent No. 100638638
   *SAMS EL, NAND FLASH MEM DAT S
   Shin JY, 2009, ICS'09: PROCEEDINGS OF THE 2009 ACM SIGARCH INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, P338, DOI 10.1145/1542275.1542324
   WU M, 1994, P 6 INT C ARCH SUPP, P86, DOI DOI 10.1145/195473.195506
   Yeonseung Ryu, 2010, 2010 IEEE Proceedings of 34th Annual Computer Software and Applications Conference (COMPSAC 2010), P453, DOI 10.1109/COMPSAC.2010.74
NR 33
TC 6
Z9 6
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 563
EP 572
DI 10.1109/TMM.2011.2114333
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700015
DA 2024-07-18
ER

PT J
AU Yin, WY
   Luo, JB
   Chen, CW
AF Yin, Wenyuan
   Luo, Jiebo
   Chen, Chang Wen
TI Event-Based Semantic Image Adaptation for User-Centric Mobile Display
   Devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image adaptation; mobile display devices; perceptual experience;
   semantics; user preference
ID VISUAL-ATTENTION; MODEL
AB This paper proposes a semantic image adaptation scheme for heterogeneous mobile display devices. This scheme aims to provide mobile users with the most desired image content by integrating the content semantic importance with user preferences under limited mobile display constraints. The main contributions of the proposed scheme are: 1) seamless integration of mobile user supplied query information with low level image features to identify semantically important image contents; 2) integration of semantic importance and user feedback to dynamically update mobile user preferences; and 3) perceptually optimized adaptation for image display on mobile devices. In order to bridge the semantic gap for adaptation, we design a Bayesian fusion approach to properly integrate low level features with high level semantics. To accommodate the variation of user preferences, the system involves mobile users in the adaptation process with only a few simple feedbacks so as to present to the users most interesting content on mobile devices. Eventually, perceptually optimized adaptation is performed to present the best image content for mobile users according to mobile display capacities. Extensive experiments have been carried out based on several common events [ 1] defined in Kodak's consumer image database. These experiments show that by utilizing the proposed semantic adaptation scheme with integration of the semantics and mobile user preferences, perceptually relevant adaptation can be effectively carried out to tailor the image towards user intentions under the mobile environment constraints.
C1 [Yin, Wenyuan; Chen, Chang Wen] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
   [Luo, Jiebo] Eastman Kodak Co, Kodak Res Labs, Rochester, NY 14650 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Buffalo; Eastman Kodak
RP Yin, WY (corresponding author), SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
EM wyin4@buffalo.edu; jiebo.luo@kodak.com; chencw@buffalo.edu
RI Luo, Jiebo/AAI-7549-2020
OI Luo, Jiebo/0000-0002-4516-9729
FU NSF [0964797]; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [0964797] Funding Source: National
   Science Foundation
FX This work was supported in part by NSF Grant 0964797 and in part by a
   Gift Funding from Kodak. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Zhu Liu.
CR [Anonymous], 2007, P SIGGRAPH
   Biederman I., 2017, PERCEPTUAL ORG, P213
   CARLIER A, 2010, P ACM INT C MULT MM, P201
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Csurka G., 2004, PROC ECCV INT WORKSH
   Fan JP, 2008, IEEE T MULTIMEDIA, V10, P167, DOI 10.1109/TMM.2007.911775
   FAN X, 2006, P INT C MOB DAT MAN
   HARE JS, 2006, P SPIE, V6073
   Hou X., 2007, P CVPR
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Liang C, 2010, LECT NOTES COMPUT SC, V5916, P614, DOI 10.1007/978-3-642-11301-7_60
   Liu H., 2007, ACM MULTIMEDIA, P305
   LOUI AC, 2007, P ACM MIR
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   LUO Y, 2010, P AS C COMP VIS ACCV
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Song W., 2010, P INT C MULTIMEDIA M, P321, DOI DOI 10.1145/1873951.1873996
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Tseng BL, 2004, J VIS COMMUN IMAGE R, V15, P370, DOI 10.1016/j.jvcir.2004.04.011
   WANG YS, 2008, P SIGGRAPH
   Wei Y, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1556134.1556139
   Wolf L., 2007, P ICCV
   XU M, P ACM MDM 08 NEW YOR, P26
NR 26
TC 15
Z9 16
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 432
EP 442
DI 10.1109/TMM.2011.2129501
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700003
DA 2024-07-18
ER

PT J
AU Yeh, JB
   Wu, CH
   Chang, SX
AF Yeh, Jun-Bin
   Wu, Chung-Hsien
   Chang, Sheng-Xiong
TI Unsupervised Alignment of News Video and Text Using Visual Patterns and
   Textual Concepts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Preview generation; textual concept; unsupervised alignment; visual
   pattern
ID SEMANTIC ANNOTATION
AB A brief preview of a news video can be generated by semantically aligning the textual sentences of the anchor report, summarized by the anchor, with the visual field shots. Since accurately detecting the object in a visual shot is difficult and a textual term may generally correspond to several synonyms, the alignment of an anchor sentence with a video shot remains challenging. In this study, the temporal relation among the frames in a visual shot is characterized by a visual language model. The language model-based temporal relation is then applied to sentence-based alignment. The bag-of-word representations for the main objects in the key frames of a visual shot are firstly mapped to the visual patterns trained from the news video database. Furthermore, the textual terms in the report sentence are mapped to the textual concepts that are obtained from the HowNet knowledge base. Finally, unsupervised alignment between the textual concepts and the visual patterns in the news videos is performed using the IBM model-1. For evaluation, the visual pattern language model yields an alignment score of 0.77, exceeding that, 0.66, from the DTW method. Considering the performance for different news categories, visual pattern discovery and textual concept discovery can indeed improve the alignment performance in most news categories.
C1 [Yeh, Jun-Bin; Wu, Chung-Hsien; Chang, Sheng-Xiong] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
C3 National Cheng Kung University
RP Yeh, JB (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
EM kenny-ouyeh@gmail.com; chunghsienwu@gmail.com; sunonlist@gmail.com
RI Wu, Chung-Hsien/E-7970-2013
OI Wu, Chung-Hsien/0000-0002-3947-2123
CR [Anonymous], P ACM MULT INF RETR
   [Anonymous], P PAC RIM C MULT PCM
   [Anonymous], P ACM C INF KNOWL MA
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], INTRO HOWNET
   [Anonymous], 2005, International journal of computational linguistics & Chinese language processing
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], IEEE WORKSH APPL COM
   [Anonymous], P ACM INT C MULT INF
   [Anonymous], P AMC INT C MULT
   [Anonymous], 2011, ACM T INTEL SYST TEC
   [Anonymous], P 8 ACIS INT C SOFTW
   [Anonymous], INT J IMAG SYST TECH
   [Anonymous], MSRTR200810
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], P ACM MULT MM
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P 4 IEEE INT C DAT M
   [Anonymous], P 1 INT WORKSH MULT
   [Anonymous], P ACM MULT INF RETR
   [Anonymous], P INT C IM VID RETR
   Bosch A, 2008, IEEE T PATTERN ANAL, V30, P712, DOI 10.1109/TPAMI.2007.70716
   Brown P. F., 1993, Computational Linguistics, V19, P263
   Csurka G., 2004, PROC ECCV INT WORKSH
   Fergus R., 2005, P IEEE INT C COMP VI
   FUNT BV, 1995, IEEE T PATTERN ANAL, V17, P522, DOI 10.1109/34.391390
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Lee CS, 2005, IEEE T SYST MAN CY B, V35, P859, DOI 10.1109/TSMCB.2005.845032
   Lin FR, 2008, DECIS SUPPORT SYST, V45, P473, DOI 10.1016/j.dss.2007.06.009
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Och FJ, 2003, COMPUT LINGUIST, V29, pc, DOI 10.1162/089120103321337421
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Wong RCF, 2008, IEEE T PATTERN ANAL, V30, P1933, DOI 10.1109/TPAMI.2008.125
   Wu CH, 2007, IEEE T MULTIMEDIA, V9, P434, DOI 10.1109/TMM.2006.887995
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P421, DOI 10.1109/TMM.2008.917346
NR 36
TC 2
Z9 2
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 206
EP 215
DI 10.1109/TMM.2010.2095412
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800004
DA 2024-07-18
ER

PT J
AU Ahmad, S
   Hamzaoui, R
   Al-Akaidi, MM
AF Ahmad, Shakeel
   Hamzaoui, Raouf
   Al-Akaidi, Marwan M.
TI Unequal Error Protection Using Fountain Codes With Applications to Video
   Communication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fountain codes; unequal error protection; video transmission
AB Application-layer forward error correction (FEC) is used in many multimedia communication systems to address the problem of packet loss in lossy packet networks. One powerful form of application-layer FEC is unequal error protection which protects the information symbols according to their importance. We propose a method for unequal error protection with a Fountain code. When the information symbols were partitioned into two protection classes (most important and least important), our method required a smaller transmission bit budget to achieve low bit error rates compared to the two state-of-the-art techniques. We also compared our method to the two state-of-the-art techniques for video unicast and multicast over a lossy network. Simulations for the scalable video coding (SVC) extension of the H.264/AVC standard showed that our method required a smaller transmission bit budget to achieve high-quality video.
C1 [Ahmad, Shakeel; Hamzaoui, Raouf; Al-Akaidi, Marwan M.] De Montfort Univ, Fac Technol, Leicester LE1 9BH, Leics, England.
   [Hamzaoui, Raouf] De Montfort Univ, Dept Engn, Leicester LE1 9BH, Leics, England.
   [Al-Akaidi, Marwan M.] De Montfort Univ, Dept Elect Engn, Leicester LE1 9BH, Leics, England.
C3 De Montfort University; De Montfort University; De Montfort University
RP Ahmad, S (corresponding author), De Montfort Univ, Fac Technol, Leicester LE1 9BH, Leics, England.
EM sahmad@dmu.ac.uk; rhamzaoui@dmu.ac.uk; mma@dmu.ac.uk
FU DFG Research Training Group [GK-1042]
FX Manuscript received March 17, 2010; revised July 17, 2010 and October
   24, 2010; accepted November 04, 2010. Date of publication November 18,
   2010; date of current version January 19, 2011. This work was supported
   in part by the DFG Research Training Group GK-1042. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Z. Jane Wang.
CR Ahmad S, 2009, MESM 2009: 10TH MIDDLE EASTERN SIMULATION MULTICONFERENCE, P72
   Ahmad S, 2008, MESM '2006: 9TH MIDDLE EASTERN SIMULATION MULTICONFERENCE, P104
   Bogino MCO, 2007, IEEE INT SYMP CIRC S, P3467, DOI 10.1109/ISCAS.2007.378373
   Byers JW, 2002, IEEE J SEL AREA COMM, V20, P1528, DOI 10.1109/JSAC.2002.803996
   Cataldi P, 2010, IEEE T IMAGE PROCESS, V19, P1491, DOI 10.1109/TIP.2010.2042985
   Hamzaoui R, 2005, IEEE SIGNAL PROC MAG, V22, P91, DOI 10.1109/MSP.2005.1550192
   Kim BJ, 2000, IEEE T CIRC SYST VID, V10, P1374, DOI 10.1109/76.889025
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Rahnavard N, 2007, IEEE T INFORM THEORY, V53, P1521, DOI 10.1109/TIT.2007.892814
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sejdinovic D, 2007, CONFERENCE RECORD OF THE FORTY-FIRST ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1-5, P1020
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Tan AS, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/632545
   Taubman D., 2012, JPEG2000: image compression fundamentals, standards and practice, V642
   Vukobratovic D, 2009, IEEE T MULTIMEDIA, V11, P1094, DOI 10.1109/TMM.2009.2026087
NR 15
TC 83
Z9 98
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 92
EP 101
DI 10.1109/TMM.2010.2093511
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900010
DA 2024-07-18
ER

PT J
AU Kuo, WH
   Liao, WJ
   Liu, TH
AF Kuo, Wen-Hsing
   Liao, Wanjiun
   Liu, Tehuang
TI Adaptive Resource Allocation for Layer-Encoded IPTV Multicasting in IEEE
   802.16 WiMAX Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE IEEE 802.16; IPTV multicast; WiMAX
ID VIDEO
AB In this paper, we study the utility-based resource allocation problem for layer-encoded IPTV multicast service over WiMAX networks. In this problem, each video stream is encoded into multiple layers. We regard each layer as a multicast subsession. Each layer of a video stream is assigned a utility value, and the number of layers for each program each user can receive is adjustable. The objective is to maximize the total utility (i.e., all users' satisfaction) and the system resource utilization, subject to users' channel conditions, the popularity of a video program, and the total available radio resource. We design a polynomial-time solution to this problem, and show that the difference in the performance of our proposed mechanism and the optimal solution is tightly bounded. Our mechanism supports both unicast and multicast, and both single layer and multi-layer environments. Most importantly, it can be integrated with the multicast mechanism defined in WiMAX standards, and can also be applied to any kind of wireless networks which support adaptive modulation and coding schemes. The performance of our scheme is evaluated by simulation. The simulation results show that this scheme can allocate resource flexibly according to the utility function of each program, the popularity of each program, and the amount of total resource available in the network.
C1 [Kuo, Wen-Hsing; Liao, Wanjiun; Liu, Tehuang] Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
   [Liao, Wanjiun] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Kuo, WH (corresponding author), Yuan Ze Univ, Dept Elect Engn, Tao Yuan, Taiwan.
EM wjliao@ntu.edu.tw
RI Kuo, Wen-Hsin/I-4786-2016
OI Kuo, Wen-Hsing/0000-0002-7324-9574; Liao, Wanjiun/0000-0001-5396-8849
FU National Science Council (NSC), Taiwan [NSC95-2752-E-002-006-PAE,
   NSC97-2219-E-002-026]
FX Manuscript received October 03, 2008; accepted May 27, 2009. Date of
   publication September 30, 2010; date of current version January 19,
   2011. This work was supported in part by the National Science Council
   (NSC), Taiwan, under Center Excellence Grant NSC95-2752-E-002-006-PAE
   and in part by the NSC, Taiwan, under Grant Number NSC97-2219-E-002-026.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Song Ci.
CR [Anonymous], 80216 IEEE
   BYERS JW, 2001, P IEEE INFOCOM 01 AP, P275
   Garey M.R., 1979, Computers and Intractability, P247
   *ITU T, P8001 ITUT
   KIM J, 2005, P WIMOB
   KIM T, P IEEE INFOCOM 2001
   KUO WH, P IEEE ICC 2007
   Lai JR, 2001, IEEE T CONSUM ELECTR, V47, P199, DOI 10.1109/30.920441
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   LIAO W, 1997, P IEEE INFOCOM
   Liu JC, 2004, IEEE T WIREL COMMUN, V3, P656, DOI 10.1109/TWC.2003.821216
   MCCANNE S, P ACM SICOMM
   NAGARAJ C, 2004, P IEEE C PERF COMP C
   Ohm JR, 2005, P IEEE, V93, P42, DOI 10.1109/JPROC.2004.839611
   OHM JR, 1998, JTCISC29WG11 ISOIEC
   Vickers BJ, 2000, IEEE ACM T NETWORK, V8, P720, DOI 10.1109/90.893869
   Zhao SJ, 2005, IEEE T MOBILE COMPUT, V4, P56, DOI 10.1109/TMC.2005.13
NR 17
TC 43
Z9 47
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 116
EP 124
DI 10.1109/TMM.2010.2082350
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shroff, N
   Turaga, P
   Chellappa, R
AF Shroff, Nitesh
   Turaga, Pavan
   Chellappa, Rama
TI Video Precis: Highlighting Diverse Aspects of Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Exemplar selection; K-means; Ncut; shot segmentation; video
   summarization
ID EXTRACTION; SYSTEM
AB Summarizing long unconstrained videos is gaining importance in surveillance, web-based video browsing, and video-archival applications. Summarizing a video requires one to identify key aspects that contain the essence of the video. In this paper, we propose an approach that optimizes two criteria that a video summary should embody. The first criterion, "coverage," requires that the summary be able to represent the original video well. The second criterion, "diversity," requires that the elements of the summary be as distinct from each other as possible. Given a user-specified summary length, we propose a cost function to measure the quality of a summary. The problem of generating a precis is then reduced to a combinatorial optimization problem of minimizing the proposed cost function. We propose an efficient method to solve the optimization problem. We demonstrate through experiments (on KTH data, unconstrained skating video, a surveillance video, and a YouTube home video) that optimizing the proposed criterion results in meaningful video summaries over a wide range of scenarios. Summaries thus generated are then evaluated using both quantitative measures and user studies.
C1 [Shroff, Nitesh; Turaga, Pavan; Chellappa, Rama] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   [Shroff, Nitesh; Turaga, Pavan; Chellappa, Rama] Univ Maryland, Ctr Automat Res, Inst Adv Comp Studies, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park;
   University System of Maryland; University of Maryland College Park
RP Shroff, N (corresponding author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
EM nshroff@umiacs.umd.edu; pturaga@umiacs.umd.edu; rama@umiacs.umd.edu
RI Turaga, Pavan/W-6186-2019; Chellappa, Rama/B-6573-2012; Chellappa,
   Rama/AAV-8690-2020
OI Turaga, Pavan/0000-0002-5263-5943
FU Office of Naval Research [N00014-09-1-0044]
FX Manuscript received September 03, 2009; revised February 16, 2010 and
   June 18, 2010; accepted June 23, 2010. Date of publication July 15,
   2010; date of current version November 17, 2010. This work was supported
   in part by the Office of Naval Research under Grant N00014-09-1-0044.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Christophe De Vleeschouwer.
CR [Anonymous], 1995, PROC ICJAI, DOI DOI 10.1145/217279.215068
   [Anonymous], 2018, WORKSHOP IIPHDW
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], 2001, HPL2001191
   [Anonymous], P ACM INT C MULT OTT
   [Anonymous], P IEEE INT C IM PROC
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Chen BW, 2009, IEEE T MULTIMEDIA, V11, P295, DOI 10.1109/TMM.2008.2009703
   DeMenthon D., 1998, Proceedings ACM Multimedia 98, P211, DOI 10.1145/290747.290773
   Dhillon I. S., 2004, PROC ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118
   Divakaran A, 2003, INT SER VIDEO COMPUT, V6, P91
   Divakaran A, 2001, J ELECTRON IMAGING, V10, P909, DOI 10.1117/1.1406507
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Ferman AM, 1997, P SOC PHOTO-OPT INS, V3229, P23, DOI 10.1117/12.290352
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gunsel B, 1997, P SOC PHOTO-OPT INS, V3229, P46, DOI 10.1117/12.290364
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   IRANI M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P605, DOI 10.1109/ICCV.1995.466883
   Li Z, 2005, IEEE T CIRC SYST VID, V15, P1245, DOI 10.1109/TCSVT.2005.854230
   Liu K, 2008, PROC VLDB ENDOW, V1, P1444, DOI 10.14778/1454159.1454196
   Maybury Mani, 1999, Advances in automatic text summarization
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Oh J., 2004, VIDEO DATA MANAGEMEN, P321
   Pritch Y., 2007, PROC IEEE INT C COMP, P1
   Radhakrishnan R, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/89013
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   SHAHRARAY B, 1995, P SOC PHOTO-OPT INS, V2417, P512, DOI 10.1117/12.206078
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Smoliar S. W., 1994, IEEE Multimedia, V1, P62, DOI 10.1109/93.311653
   Soatto S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P439, DOI 10.1109/ICCV.2001.937658
   Taskiran CM, 2005, COMP ENG SER, P215
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Turaga P, 2009, COMPUT VIS IMAGE UND, V113, P353, DOI 10.1016/j.cviu.2008.08.009
   Tuzel Oncel., 2006, PROC EUROPEAN C COMP, V2, P589
   Vasconcelos N, 1998, PROC CVPR IEEE, P361, DOI 10.1109/CVPR.1998.698631
   WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981
   Xie LX, 2004, PATTERN RECOGN LETT, V25, P767, DOI 10.1016/j.patrec.2004.01.005
   Zelnik-Manor L, 2001, PROC CVPR IEEE, P123
   Zhang HJ, 1997, PATTERN RECOGN, V30, P643, DOI 10.1016/S0031-3203(96)00109-4
   Zhu XQ, 2005, IEEE T MULTIMEDIA, V7, P648, DOI 10.1109/TMM.2005.850977
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 44
TC 32
Z9 40
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2010
VL 12
IS 8
BP 853
EP 868
DI 10.1109/TMM.2010.2058795
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 681ZW
UT WOS:000284365100007
DA 2024-07-18
ER

PT J
AU Baek, J
   Fisher, PS
   Jo, M
   Chen, HH
AF Baek, Jinsuk
   Fisher, Paul S.
   Jo, Minho
   Chen, Hsiao-Hwa
TI A Lightweight SCTP for Partially Reliable Overlay Video Multicast
   Service for Mobile Terminals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Overlay multicast; partially reliable multicast; scalable multicast;
   stream control transmission protocol; video multicast
ID SCHEME
AB In this article, a video multicast protocol for multi-homed mobile terminals is proposed as an alternative stream control transmission protocol (SCTP) for partially reliable multicast services. It works with overlay peer-to-peer video multicast facility in the application layer. For a multi-homed mobile terminal, an error burst may occur when a handover is in process in the primary path switching procedure. The key issue concerned in this protocol is the ability to predict packet loss and to retransmit the lost packets as soon as a mobile terminal completes its primary path switching procedure. This property controls the delay sensitivity of transmissions. Conversely, the protocol can tolerate partial loss in video transmission as long as the loss is limited to a relatively short error burst. In addition, it reduces the message overhead significantly and provides a scalable communication mechanism for multicast applications. The performance improvement of the proposed protocol comes from 1) the estimation of temporal velocity of mobile terminals with lost packet prediction in a long error burst, and 2) the requirement for each mobile terminal to indicate which packets can be safely discarded from its agent.
C1 [Baek, Jinsuk; Fisher, Paul S.] Winston Salem State Univ, Dept Comp Sci, Winston Salem, NC 27110 USA.
   [Jo, Minho] Korea Univ, Grad Sch Informat Management & Secur, Seoul, South Korea.
   [Chen, Hsiao-Hwa] Natl Cheng Kung Univ, Dept Engn Sci, Tainan 70101, Taiwan.
C3 Winston-Salem State University; Korea University; National Cheng Kung
   University
RP Baek, J (corresponding author), Winston Salem State Univ, Dept Comp Sci, Winston Salem, NC 27110 USA.
EM minhojo@korea.ac.kr; hshwchen@ieee.org
RI Jo, Minho/AAM-3058-2020
OI Jo, Minho/0000-0001-7311-6459
FU Brain Korea 21 program; Ministry of Education, Science, and Technology;
   Korean Government; National Science Council of Taiwan
   [NSC98-2219-E-006-011]
FX Manuscript received August 11, 2009; revised November 15, 2009 and March
   22, 2010; accepted June 08, 2010. Date of publication June 21, 2010;
   date of current version October 15, 2010. This work was supported in
   part by the Brain Korea 21 program; Ministry of Education, Science, and
   Technology; the Korean Government; and the National Science Council of
   Taiwan (NSC98-2219-E-006-011). The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Zhihai (Henry) He.
CR ALMISBAHI H, 2007, IEEE 2 INT C WIR BRO, P79
   Baek J, 2005, ETRI J, V27, P1, DOI 10.4218/etrij.05.0104.0037
   BAEK J, 2009, WILEY INT J COMMUN S, P20, DOI DOI 10.1002/DAC.1009
   Baek J, 2008, KSII T INTERNET INF, V2, P82, DOI 10.3837/tiis.2008.02.002
   Banerjee S, 2003, IEEE INFOCOM SER, P1521
   Birman KP, 1999, ACM T COMPUT SYST, V17, P41, DOI 10.1145/312203.312207
   Cohen R, 2001, IEEE INFOCOM SER, P440, DOI 10.1109/INFCOM.2001.916727
   Costello AM, 1999, IEEE INFOCOM SER, P1256, DOI 10.1109/INFCOM.1999.752143
   DAHAL M, 2006, P INT C ONCOMMUNICAT, P1, DOI DOI 10.1109/ICCT.2006.341983
   Floyd S, 1997, IEEE ACM T NETWORK, V5, P784, DOI 10.1109/90.650139
   Funasaka J, 2005, ISADS 2005: INTERNATIONAL SYMPOSIUM ON AUTONOMOUS DECENTRALIZED SYSTEMS,PROCEEDINGS, P536, DOI 10.1109/ISADS.2005.1452131
   Guo K., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P814, DOI 10.1109/INFCOM.2000.832256
   *IEEE COMP SOC, 2005, 02 11 WIR LAN MAC PH
   Jo J, 2008, KSII T INTERNET INF, V2, P171, DOI 10.3837/tiis.2008.04.001
   Jung O, 2005, 7th International Conference on Advanced Communication Technology, Vols 1 and 2, Proceedings, P269, DOI 10.1109/ICACT.2005.245844
   Kashihara S, 2004, 2004 INTERNATIONAL SYMPOSIUM ON APPLICATIONS AND THE INTERNET, PROCEEDINGS, P273
   Kerry S., 2007, IEEE STD 80211 2007, P1
   Kim DP, 2008, 10TH INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY, VOLS I-III, P900
   Kim M, 2008, KSII T INTERNET INF, V2, P23, DOI 10.3837/tiis.2008.01.002
   Kwon GI, 2004, IEEE INFOCOM SER, P385
   LEE KJ, 2006, P IEEE WCNC NEW ORL, P69
   Li YQ, 2008, 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES: ITESS 2008, VOL 2, P53, DOI 10.1109/CHICC.2008.4605487
   MAHDAVI J, 2009, TCP FRIENDLY UNICAST
   MORIN T, 2007, 4384 IETF RFC
   Qiao Y, 2007, LECT NOTES COMPUT SC, V4517, P43
   Shi SY, 2002, IEEE INFOCOM SER, P1200, DOI 10.1109/INFCOM.2002.1019370
   Shin M, 2009, KSII T INTERNET INF, V3, P5, DOI 10.3837/tiis.2009.01.001
   STEWART R, 2004, IETF INTERN IN PRESS
   THALER D, 1999, 2715 IEFT RFC
   Whetten B, 2000, IEEE NETWORK, V14, P37, DOI 10.1109/65.819170
   WHETTEN B, 2001, 3048 IETF RFC
   Xiao Z, 2002, INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS, PROCEEDINGS, P187, DOI 10.1109/DSN.2002.1028899
   YE G, 2004, IEEE J SEL AREA COMM, V22, P727
   YONG FK, 2005, P NAT COMP SCI POSTG, P1
   Zhang BX, 2005, IEEE COMMUN MAG, V43, P115, DOI 10.1109/MCOM.2005.1381884
   Zhu Y, 2004, IEEE J SEL AREA COMM, V22, P107, DOI 10.1109/JSAC.2003.818801
NR 36
TC 10
Z9 12
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2010
VL 12
IS 7
BP 754
EP 766
DI 10.1109/TMM.2010.2053523
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 670SV
UT WOS:000283448500012
DA 2024-07-18
ER

PT J
AU Fanelli, G
   Gall, J
   Romsdorfer, H
   Weise, T
   Van Gool, L
AF Fanelli, Gabriele
   Gall, Juergen
   Romsdorfer, Harald
   Weise, Thibaut
   Van Gool, Luc
TI A 3-D Audio-Visual Corpus of Affective Communication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual database; emotional speech; face tracking; visual speech
   modeling; 3-D face modeling
ID EMOTION; RECOGNITION; INDUCTION; SPEECH; MOOD
AB Communication between humans deeply relies on the capability of expressing and recognizing feelings. For this reason, research on human-machine interaction needs to focus on the recognition and simulation of emotional states, prerequisite of which is the collection of affective corpora. Currently available datasets still represent a bottleneck for the difficulties arising during the acquisition and labeling of affective data. In this work, we present a new audio-visual corpus for possibly the two most important modalities used by humans to communicate their emotional states, namely speech and facial expression in the form of dense dynamic 3-D face geometries. We acquire high-quality data by working in a controlled environment and resort to video clips to induce affective states. The annotation of the speech signal includes: transcription of the corpus text into the phonological representation, accurate phone segmentation, fundamental frequency extraction, and signal intensity estimation of the speech signals. We employ a real-time 3-D scanner to acquire dense dynamic facial geometries and track the faces throughout the sequences, achieving full spatial and temporal correspondences. The corpus is a valuable tool for applications like affective visual speech synthesis or view-independent facial expression recognition.
C1 [Fanelli, Gabriele; Gall, Juergen; Van Gool, Luc] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
   [Romsdorfer, Harald] Graz Univ Technol, Signal Proc & Speech Commun Lab, A-8010 Graz, Austria.
   [Weise, Thibaut] EPFL Lausanne, Lab Informat Graph & Geometr, Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich; Graz
   University of Technology; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne
RP Fanelli, G (corresponding author), Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
EM gfanelli@vision.ee.ethz.ch; gall@vi-sion.ee.ethz.ch;
   roms-dorfer@ieee.org; weiset@vision.ee.ethz.ch;
   vangool@vision.ee.ethz.ch
FU SNF [200021-130224]
FX Manuscript received December 14, 2009; revised March 24, 2010; accepted
   May 16, 2010. Date of current version September 15, 2010. This work was
   supported in part by the SNF fund Vision-Supported Speech-Based Human
   Machine Interaction (200021-130224). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Caifeng Shan. G. Fanelli, J. Gall, and L. Van Gool are with the Computer
   Vision Laboratory, ETH Zurich, Zurich, Switzerland (e-mail:
   gfanelli@vision.ee.ethz.ch; gall@vision.ee.ethz.ch;
   vangool@vision.ee.ethz.ch).
CR [Anonymous], 2000, THESIS U ILLINOIS UR
   Bänziger T, 2007, LECT NOTES COMPUT SC, V4738, P476
   Banse R, 1996, J PERS SOC PSYCHOL, V70, P614, DOI 10.1037/0022-3514.70.3.614
   Bradley MM, 1996, PSYCHOPHYSIOLOGY, V33, P662, DOI 10.1111/j.1469-8986.1996.tb02362.x
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Cao Y, 2005, ACM T GRAPHIC, V24, P1283, DOI 10.1145/1095878.1095881
   CLARK DM, 1983, ADV BEHAV RES THER, V5, P27, DOI 10.1016/0146-6402(83)90014-0
   Cowie R, 2005, NEURAL NETWORKS, V18, P371, DOI 10.1016/j.neunet.2005.03.002
   Cowie R, 2003, SPEECH COMMUN, V40, P5, DOI 10.1016/S0167-6393(02)00071-7
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   Craggs R., 2004, P AAAI SPRING S EXPL
   Douglas-Cowie E, 2003, SPEECH COMMUN, V40, P33, DOI 10.1016/S0167-6393(02)00070-5
   Douglas-Cowie E, 2007, LECT NOTES COMPUT SC, V4738, P488
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Ekman P, 1978, FACIAL ACTION CODING
   FRIGO S, 2006, P CORP RES EM AFF LR
   Grimm M, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P865, DOI 10.1109/ICME.2008.4607572
   GROSS JJ, 1995, COGNITION EMOTION, V9, P87, DOI 10.1080/02699939508408966
   Martin JC, 2009, PERS UBIQUIT COMPUT, V13, P69, DOI 10.1007/s00779-007-0167-y
   MEHRABIAN A, 1968, PSYCHOL TODAY, V2, P53
   Morlec Y, 2001, SPEECH COMMUN, V33, P357, DOI 10.1016/S0167-6393(00)00065-0
   Picard RW, 2000, IBM SYST J, V39, P705, DOI 10.1147/sj.393.0705
   ROMSDORFER H, 2009, TIK SCHRIFTENREIHE, V101
   Scherer K. R., 1998, P INT WORKSH SPEECH
   SCHRODER M, 2008, AFFECTIVE INFORM PRO
   SEBE N, 2004, P AFGR
   TURK U, 2001, TECHNICAL PROCESSING
   Valstar MF, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P38
   VELTEN E, 1968, BEHAV RES THER, V6, P473, DOI 10.1016/0005-7967(68)90028-4
   Wampler K, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P53
   WEISE T, 2007, P IEEE CVPR
   WEISE T, 2009, P SCA
   West AC, 1996, J APPL ELECTROCHEM, V26, P557, DOI 10.1007/BF00253453
   WHISSELL CM, 1972, DICT AFFECT LANGUAGE
   YIN L, 2008, P FG
   ZARA A, 2007, P ACII, P464
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
NR 37
TC 67
Z9 76
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2010
VL 12
IS 6
BP 591
EP 598
DI 10.1109/TMM.2010.2052239
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 668TB
UT WOS:000283291900012
OA Green Published
DA 2024-07-18
ER

PT J
AU Hegde, RM
   Kurniawan, J
   Rao, BD
AF Hegde, Rajesh M.
   Kurniawan, Joseph
   Rao, Bhaskar D.
TI On the Design and Prototype Implementation of a Multimodal Situation
   Aware System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bidirectional FTP synchronization; environmental audio; GPS; speech
   recognition; ubiquitous computing
AB In this paper we describe the design concepts and prototype implementation of a situation aware ubiquitous computing system using multiple modalities such as National Marine Electronics Association (NMEA) data from global positioning system (GPS) receivers, text, speech, environmental audio, and handwriting inputs. While most mobile and communication devices know where and who they are, by accessing context information primarily in the form of location, time stamps, and user identity, the concept of sharing of this information in a reliable and intelligent fashion is crucial in many scenarios. A framework which takes the concept of context aware computing to the level of situation aware computing by intelligent information exchange between context aware devices is designed and implemented in this work. Four sensual modes of contextual information like text, speech, environmental audio, and handwriting are augmented to conventional contextual information sources like location from GPS, user identity based on IP addresses (IPA), and time stamps. Each device derives its context not necessarily using the same criteria or parameters but by employing selective fusion and fission of multiple modalities. The processing of each individual modality takes place at the client device followed by the summarization of context as a text file. Exchange of dynamic context information between devices is enabled in real time to create multimodal situation aware devices. A central repository of all user context profiles is also created to enable self-learning devices in the future. Based on the results of simulated situations and real field deployments it is shown that the use of multiple modalities like speech, environmental audio, and handwriting inputs along with conventional modalities can create devices with enhanced situational awareness.
C1 [Hegde, Rajesh M.] Indian Inst Technol, Dept Elect Engn, Kanpur 208016, Uttar Pradesh, India.
   [Kurniawan, Joseph] Networks Mot, Aliso Viejo, CA 91748 USA.
   [Rao, Bhaskar D.] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kanpur; University of California System; University
   of California San Diego
RP Hegde, RM (corresponding author), Indian Inst Technol, Dept Elect Engn, Kanpur 208016, Uttar Pradesh, India.
EM rhegde@iitk.ac.in; joseph.k13@gmail.com; brao@ucsd.edu
CR [Anonymous], 2005, SNACK SOUND TOOLKIT
   [Anonymous], P 1998 IM UND WORKSH
   [Anonymous], 1999, SPEECH COMMUNICATION
   [Anonymous], 1997, PERS TECHNOL, DOI DOI 10.1007/BF01682024
   ARISOYLU M, 2005, P AM MED INF ASS AMI
   ASHISH N, 2007, TERRORISM INFORM KNO
   Clarkson B, 1998, SECOND INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS - DIGEST OF PAPERS, P154, DOI 10.1109/ISWC.1998.729542
   Feiner S., 1997, P 1 IEEE INT S WEAR
   Hagen A, 2003, CODES(PLUS)ISSS 2003: FIRST IEEE/ACM/IFIP INTERNATIONAL CONFERENCE ON HARDWARE/SOFTWARE CODESIGN & SYSTEM SYNTHESIS, P65, DOI 10.1109/CODESS.2003.1275257
   IOANNIDIS J, 1991, P ACM SIGCOMM S COMM, P235
   Lave J., 1991, SITUATED LEARNING LE, DOI DOI 10.1017/CBO9780511815355
   MA L, 2003, LNCS, P360
   MARMASSE N, 2004, THESIS MIT CAMBRIDGE
   MOCKAPETRIS PV, 1988, P ACM SIGCOMM 88 AUG
   Pascoe J, 1998, SECOND INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS - DIGEST OF PAPERS, P92, DOI 10.1109/ISWC.1998.729534
   *PROJ RESCUE, RESP CRIS UN EV
   Ranganathan A., 2002, IEEE Pervasive Computing, V1, P51, DOI 10.1109/MPRV.2002.1037722
   Rose RC, 2001, INT CONF ACOUST SPEE, P17, DOI 10.1109/ICASSP.2001.940756
   ROSE RC, 2003, IEEE INT C AC SPEECH, V1, P316
   Ross DA, 1997, FIRST INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS - DIGEST OF PAPERS, P161, DOI 10.1109/ISWC.1997.629934
   SALBER D, 1999, P ACM SIGCHI 99 MAY
   *SAMI PROJ, SIT AW MULT INP
   Sawhney N., 1997, SITUATIONAL AWARENES
   SAWHNEY N, ACM T COMPU IN PRESS
   Schilit B., 1994, 1994 1 WORKSHOP MOBI, P85, DOI [DOI 10.1109/WMCSA.1994.16, 10.1109/WMCSA.1994.16]
   Starner T, 1998, SECOND INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS - DIGEST OF PAPERS, P50, DOI 10.1109/ISWC.1998.729529
   SUCHMAN LA, 1985, ISL6
   Tridgell A., 1999, EFFICIENT ALGORITHMS
   VALLEJO G, 2003, THESIS MIT MEDIA LAB
   VASILACHE M, 2004, IEEE INT C AC SPEECH, V5, P113
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
   SONY VAIO UX490N C M
   UC SAN DIEGO ACTIVE
   SONY LINEAR PCM DI R
NR 34
TC 6
Z9 6
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 645
EP 657
DI 10.1109/TMM.2009.2017631
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900007
DA 2024-07-18
ER

PT J
AU Qian, XM
   Liu, GZ
   Wang, H
AF Qian, Xueming
   Liu, Guizhong
   Wang, Huan
TI Recovering Connected Error Region Based on Adaptive Error Concealment
   Order Determination
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Boundary matching algorithm; error concealment; error concealment order
   determination; external boundary pattern; H.264; packet loss; video
   transmission
ID MPEG-4 VIDEO; ALGORITHM
AB Parts of compressed video streams may be lost or corrupted when being transmitted over bandwidth limited networks and wireless communication networks with error-prone channels. Error concealment (EC) techniques are often adopted at the decoder side to improve the quality of the reconstructed video. Under the conditions of a high rate of data packets that arrives at the decoder corrupted, it is likely that the incorrectly decoded macro-blocks (MBs) are concentrated in a connected region, where important spatial reference information is lost. The conventional EC methods usually carry out the block concealment following a lexicographic scan (from top to bottom and from left to right of the image), which would make the methods ineffective for the case that the corrupted blocks are grouped in a connected region. In this paper, a temporal error concealment method, adaptive error concealment order determination (AECOD), is proposed to recover connected corrupted regions. The processing order of an MB in a connected corrupted region is adaptively determined by analyzing the external boundary patterns of the MBs in its neighborhood. The performances, on several video sequences, of the proposed EC scheme have been compared with those obtained by using other error concealment methods reported in the literature. Experimental results show that the AECOD algorithm can improve the recovery performance with respect to the other considered EC methods.
C1 [Qian, Xueming; Liu, Guizhong; Wang, Huan] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
EM qianxm@xjtu.edu.cn; liugz@xjtu.edu.cn; wang.huan.1006@stu.xjtu.edu.cn
RI Qian, Xueming/E-9867-2015
FU National Natural Science Foundation of China (NSFC) [60572045]; Ministry
   of Education of China Pb.D. Program Foundation [20050698033]
FX Manuscript received February 20, 2008; revised December 05, 2008. First
   published April 28. 20W current version published May 15, 2009. This
   work was supported in part by the National 973 Project No. 2007CB311002,
   in part by the National Natural Science Foundation of China (NSFC) under
   Project No. 60572045, and in part by the Ministry of Education of China
   Pb.D. Program Foundation under Project No. 20050698033. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Ling Guan.
CR Agrafiotis D, 2006, IEEE T CIRC SYST VID, V16, P960, DOI 10.1109/TCSVT.2006.879988
   [Anonymous], JVT REFERENCE SOFTWA
   [Anonymous], 144962 ISOIEC
   CASTLEMAN KR, 1999, DIGITAL IMAGE PROCES, P499
   Chen MJ, 2005, IEEE T CIRC SYST VID, V15, P1385, DOI 10.1109/TCSVT.2005.857301
   CHI M, 2005, P INT C CIRC SYST MA, V3, P2895
   Feng J, 1997, IEEE T CONSUM ELECTR, V43, P183, DOI 10.1109/30.585539
   Gallant M, 2001, IEEE T CIRC SYST VID, V11, P357, DOI 10.1109/76.911161
   Hsia SC, 2004, IEEE SIGNAL PROC LET, V11, P577, DOI 10.1109/LSP.2004.827916
   IM SK, 2005, P IEEE INF COMM SIGN, P1135
   *ISO IEC, 1997, JTCISC29WG11MEPGM291
   *ITU T, 2003, H264ISOIEC1448610 AV
   *ITU T, 1998, H263 ITUT
   JUNG YH, 2000, P INT C IM PROC ICIP, V3, P384
   Kang LW, 2005, J VIS COMMUN IMAGE R, V16, P288, DOI 10.1016/j.jvcir.2004.11.004
   KIM D, 2005, P INT C IM PROC SEP, V3, P28
   Kim ET, 1999, SIGNAL PROCESS, V73, P291, DOI 10.1016/S0165-1684(98)00242-4
   KUNG WY, 2003, P IEEE INT C MULT EX, V2, P145
   LAM WM, 1993, P ICASSP, V5, P417
   Lie WN, 2006, IEEE T CIRC SYST VID, V16, P982, DOI 10.1109/TCSVT.2006.879119
   Nemethova O, 2005, 2005 International Conference on Wireless Networks, Communications and Mobile Computing, Vols 1 and 2, P1255
   Peng Q, 2002, 2002 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, CIRCUITS AND SYSTEMS AND WEST SINO EXPOSITION PROCEEDINGS, VOLS 1-4, P10, DOI 10.1109/ICCCAS.2002.1180560
   QIAN X, 2007, SIGNAL IMAGE VIDEO P, V1, P179
   Qian XM, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P739
   RICHARDSON EG, H 264 MPEG 4 VIDEO C, P167
   Su YB, 2005, IEEE T CIRC SYST VID, V15, P232, DOI 10.1109/TCSVT.2004.841656
   Su YP, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P703, DOI 10.1109/ICME.2004.1394289
   SUB J, 1997, IEEE T CONSUM ELECTR, V43, P295
   Suh JW, 2002, IEEE T BROADCAST, V48, P299, DOI 10.1109/TBC.2002.806797
   SUN HF, 1995, IEEE T IMAGE PROCESS, V4, P470, DOI 10.1109/83.370675
   Tsai TH, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 5, PROCEEDINGS, P433
   Tsekeridou S, 2000, IEEE T CIRC SYST VID, V10, P646, DOI 10.1109/76.845010
   Tso CD, 2003, INTERNATIONAL CONFERENCE ON POLITICS AND INFORMATION SYSTEMS: TECHNOLOGIES AND APPLICATIONS, PROCEEDINGS, P55
   Valente S, 2001, IEEE T CONSUM ELECTR, V47, P568, DOI 10.1109/30.964147
   VARSA V, 2001, ITUTVCEGN62
   Vetro A, 2005, IEEE WIREL COMMUN, V12, P14, DOI 10.1109/MWC.2005.1497854
   WANG Y, 1993, IEEE T COMMUN, V41, P1544, DOI 10.1109/26.237889
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   Wang YK, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P729
   Wang Z, 1998, IEEE T IMAGE PROCESS, V7, P1056, DOI 10.1109/83.701166
   Wenger S, 2003, IEEE T CIRC SYST VID, V13, P645, DOI 10.1109/TCSVT.2003.814966
   WENGER S, 2002, JVTC089
   Xu YL, 2004, IEEE T CONSUM ELECTR, V50, P1135, DOI 10.1109/TCE.2004.1362510
   Yan B, 2002, IEEE T CONSUM ELECTR, V48, P863, DOI 10.1109/TCE.2003.1196414
   Yu GS, 1998, IEEE T CIRC SYST VID, V8, P422, DOI 10.1109/76.709409
   Zeng WJ, 1999, IEEE T CIRC SYST VID, V9, P648, DOI 10.1109/76.767129
   Zhai F, 2006, IEEE T IMAGE PROCESS, V15, P40, DOI 10.1109/TIP.2005.860353
   Zhang J, 2000, IEEE T CIRC SYST VID, V10, P659, DOI 10.1109/76.845011
NR 48
TC 28
Z9 35
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 683
EP 695
DI 10.1109/TMM.2009.2017609
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900010
DA 2024-07-18
ER

PT J
AU Lin, WS
   Zhao, HV
   Liu, KJR
AF Lin, W. Sabrina
   Zhao, H. Vicky
   Liu, K. J. Ray
TI Incentive Cooperation Strategies for Peer-to-Peer Live Multimedia
   Streaming Social Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cooperation strategy; game theory; multimedia live streaming;
   peer-to-peer; social network
AB Multimedia social networks have become an emerging research area, in which analysis and modeling of the behavior of users who share multimedia are of ample importance in understanding the impact or human dynamics on multimedia systems. In peer-to-peer live-streaming social networks, users cooperate with each other to provide a distributed, highly scalable and robust platform for live streaming applications. However, every user wishes to use as much bandwidth as possible to receive a high-quality video, while full cooperation cannot be guaranteed. This paper proposes a game-theoretic framework to model user behavior and designs incentive-based strategies to stimulate user cooperation in peer-to-peer live streaming. We first analyze the Nash equilibrium and the Pareto optimality of two-person game and then extend to multiuser case. We also take into consideration selfish users' cheating behavior and malicious users' attacking behavior. Both our analytical and simulation results show that the proposed strategies can effectively stimulate user cooperation, achieve cheat free, attack resistance and help to provide reliable services.
C1 [Lin, W. Sabrina; Liu, K. J. Ray] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   [Zhao, H. Vicky] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2V4, Canada.
C3 University System of Maryland; University of Maryland College Park;
   University of Alberta
RP Lin, WS (corresponding author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
EM wylin@umd.edu; vzhao@ece.ualberta.ca; kjrliu@umd.edu
RI Liu, K.J. Ray/C-2798-2009
CR [Anonymous], P 2005 ACM SIGCOMM W
   Buragohain C, 2003, THIRD INTERNATIONAL CONFERENCE ON PEER-TO-PEER COMPUTING (P2P2003), PROCEEDINGS, P48, DOI 10.1109/PTP.2003.1231503
   Cohen B., 2003, INCENTIVES BUILD ROB
   FELDMAN M, 2004, P ACM C EL COMM EC 0
   Golle P., 2001, P ACM EL COMM EC 01
   Habib A, 2004, INT WORKSH QUAL SERV, P171
   Hei XJ, 2007, IEEE J SEL AREA COMM, V25, P1640, DOI 10.1109/JSAC.2007.071204
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   KALLENBERG O, 1977, FDN MODERN PROBABILI
   LIANG J, 2005, P IEEE C COMP COMM I, V2
   LIN WS, 2008, P IEEE INT C AC SPEE
   LIU Z, 2007, P ACM SPEC INT GROUP
   MARPE D, 2005, JVTO202
   Naoumov N., 2006, P 1 INT C SCALABLE I
   Nocedal J., 2000, Numerical Optimization
   Osborne Martin J, 1994, COURSE GAME THEORY
   Owen G., 1995, GAME THEORY
   Puri R., 1999, P 33 AS C SIGN SYST
   Qiu D., 2004, P ACM SIGCOMM 04, P367
   TAN G, 2006, P INT WORKSH QUAL SE
   Tourapis A.M., 2005, JVTQ042
   Yu W, 2007, IEEE T MOBILE COMPUT, V6, P459, DOI 10.1109/TMC.2007.1026
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 23
TC 68
Z9 75
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 396
EP 412
DI 10.1109/TMM.2009.2012915
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Bardeli, R
AF Bardeli, Rolf
TI Similarity Search in Animal Sound Databases
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Similarity search; animal sounds; feature extraction; indexing;
   retrieval; structure tensor
ID AUDIO CLASSIFICATION; RETRIEVAL; RECOGNITION
AB In the past, similarity search for audio data has largely been focused on music. Recent digitization efforts in some of the larger animal sound archives bring other types of audio recordings into the focus of interest. Although recordings in animal sound archives are usually very well annotated by metadata, it is almost impossible to manually annotate all sounds made by animals in each recording. Complementary to classical text-based querying of databases that exploit available annotations, algorithms capable of automatically finding sections of recordings similar to a given query fragment provide a promising approach for content-based navigation. In our work, we present algorithms for feature extraction, as well as indexing and retrieval of animal sound recordings. Making use of a concept from image processing, the structure tensor, our feature extraction algorithm is adapted to the typical curve-like spectral features that are characteristic for many types of animal sounds. We propose a method for similarity search in animal sound databases which is obtained by adding a novel ranking scheme to an existing inverted file based approach for multimedia retrieval. Evaluation of our methods is based on recordings from the Animal Sound Archive, Berlin.
C1 [Bardeli, Rolf] Univ Bonn, D-53117 Bonn, Germany.
C3 University of Bonn
RP Bardeli, R (corresponding author), Fraunhofer IAIS, D-53754 St Augustin, Germany.
EM rolf.bardeli@gmail.com
OI Bardeli, Rolf/0000-0002-6392-1451
FU German Research Foundation (DFG) [LIS 1-554 95 (1)]; German Federal
   Agency for Nature Conservation [806 82 060-K2]
FX Manuscript received January 14, 2008; revised September 17, 2008.
   Current version published January 08, 2009. This work was supported by
   the German Research Foundation (DFG) under Grant LIS 1-554 95 (1) Bonn
   uv and by the German Federal Agency for Nature Conservation (BfN) under
   Grant 806 82 060-K2. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Xian-Sheng Hua.
CR ALLAMANCHE E, 2001, P 2 OTH AES CONV AMS
   [Anonymous], 1987, PROC ISPRS INTERCOMM
   [Anonymous], 2007, P INT SOC MUS INF RE
   Berenzweig A, 2004, COMPUT MUSIC J, V28, P63, DOI 10.1162/014892604323112257
   CANO P, 2002, P 112 AES CONV MUN G
   Casey M, 2001, IEEE T CIRC SYST VID, V11, P737, DOI 10.1109/76.927433
   Clausen M, 2004, NATO SCI SER II-MATH, V136, P29
   Clausen M, 2004, IEEE T MULTIMEDIA, V6, P717, DOI 10.1109/TMM.2004.834859
   Deecke VB, 1999, J ACOUST SOC AM, V105, P2499, DOI 10.1121/1.426853
   Dixon S., 2005, Proceedings of the International Conference on Music Information Retrieval, P492
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Frommolt Karl-Heinz, 2006, Slovenska Akademija Znanosti in Umetnosti Razred za Naravoslovne Vede Razprave, V47, P139
   Guo GD, 2003, IEEE T NEURAL NETWOR, V14, P209, DOI 10.1109/TNN.2002.806626
   Härmä A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P545
   Kim HG, 2004, IEEE T CIRC SYST VID, V14, P716, DOI 10.1109/TCSVT.2004.826766
   Kogan JA, 1998, J ACOUST SOC AM, V103, P2185, DOI 10.1121/1.421364
   Kurth F, 2008, IEEE T AUDIO SPEECH, V16, P382, DOI 10.1109/TASL.2007.911552
   Mills DM, 2000, J ACOUST SOC AM, V107, P2586, DOI 10.1121/1.428646
   MITROVIC D, 2006, 12 MULT MOD C P
   MITSAKAKIS N, 1996, J ACOUST SOC AM, V100, P2644
   Müller M, 2005, IEEE WORK APPL SIG, P275, DOI 10.1109/ASPAA.2005.1540223
   Ranft R, 2004, AN ACAD BRAS CIENC, V76, P455
   Scaringella N, 2006, IEEE SIGNAL PROC MAG, V23, P133, DOI 10.1109/MSP.2006.1598089
   Schwedes J, 2003, GRANUL MATTER, V5, P1, DOI 10.1007/s10035-002-0124-4
   Somervuo P., 2003, WORKSH SELF ORG MAPS
   TAYLOR A, 1995, P 8 AUST JOINT C ART, P209
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
NR 27
TC 28
Z9 35
U1 2
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 68
EP 76
DI 10.1109/TMM.2008.2008920
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700006
OA Green Published
DA 2024-07-18
ER

PT J
AU Davies, SJC
   Agrafiotis, D
   Canagarajah, CN
   Bull, DR
AF Davies, Sam J. C.
   Agrafiotis, Dimitris
   Canagarajah, C. Nishan
   Bull, David R.
TI A Multicue Bayesian State Estimator for Gaze Prediction in Open Signed
   Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Eye-tracking; face detection; gaze prediciotn; video coding
ID OF-INTEREST; LANGUAGE
AB We propose a multicue gaze prediction framework for open signed video content, the benefits of which include coding gains without loss of perceived quality. We investigate which cues are relevant for gaze prediction and find that shot changes, facial orientation of the signer and face locations are the most useful. We then design a face orientation tracker based upon grid-based likelihood ratio trackers, using profile and frontal face detections. These cues are combined using a grid-based Bayesian state estimation algorithm to form a probability surface for each frame. We find that this gaze predictor outperforms a static gaze prediction and one based on face locations within the frame.
C1 [Davies, Sam J. C.; Agrafiotis, Dimitris; Canagarajah, C. Nishan; Bull, David R.] Univ Bristol, Dept Elect & Elect Engn, Bristol BS8 1UB, Avon, England.
C3 University of Bristol
RP Davies, SJC (corresponding author), Univ Bristol, Dept Elect & Elect Engn, Bristol BS8 1UB, Avon, England.
EM sam.davies@bristol.ac.uk; d.agrafiotis@bris.ac.uk;
   nishan.canagarajah@bris.ac.uk; dave.bull@bris.ac.uk
FU British Broadcasting Corporation (BBC)
FX Manuscript received May 21, 2008; revised September 08,2008. Current
   version published January 08, 2009. The work of S. J. C. Davies was
   supported by the British Broadcasting Corporation (BBC). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Shrikanth Narayanan.
CR Agrafiotis D, 2003, ELECTRON LETT, V39, P1703, DOI 10.1049/el:20031140
   Agrafiotis D., 2007, ACM T MULTIM COMPUT, V3, P1
   ARGRAFIOTIS D, 2003, P SOC PHOTO-OPT INS, V5150, P1244
   Barth E, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P245, DOI 10.1109/ISSPA.2003.1224686
   Boccignone G, 2004, PHYSICA A, V331, P207, DOI 10.1016/j.physa.2003.09.011
   Cheng WH, 2005, IEICE T INF SYST, VE88D, P1578, DOI 10.1093/ietisy/e88-d.7.1578
   Cherniavsky N, 2007, ASSETS'07: PROCEEDINGS OF THE NINTH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P163
   Chernyak DA, 2001, IEEE T SYST MAN CY B, V31, P514, DOI 10.1109/3477.938257
   CIARAMELLO FM, 2007, P SPIE OBJECTIVE MET, V6492, P64920
   Ciaramello FM, 2008, PROC SPIE, V6822, DOI 10.1117/12.768053
   Davies SJC, 2007, ELECTRON LETT, V43, P1135, DOI 10.1049/el:20071901
   Hall D.L., 2001, HDB MULTISENSOR DATA
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Muir LJ, 2005, J DEAF STUD DEAF EDU, V10, P390, DOI 10.1093/deafed/eni037
   Niebur E, 1998, ATTENTIVE BRAIN, P163
   Privitera Claudio M., 2005, P296, DOI 10.1016/B978-012375731-9/50052-5
   Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520
   Rajashekar U, 2006, IEEE IMAGE PROC, P453, DOI 10.1109/ICIP.2006.312491
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
NR 19
TC 4
Z9 4
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 39
EP 48
DI 10.1109/TMM.2008.2008916
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700004
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhang, YX
   Ieee, CZM
   Yap, KH
AF Zhang, Yixuan
   Ieee, Ce ZhuSenior Member
   Yap, Kim-Hui
TI A Joint Source-Channel Video Coding Scheme Based on Distributed Source
   Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distributed source coding; error propagation; error resilient
   compression; joint source-channel video coding
ID SIDE INFORMATION; BINARY SOURCES; COSET CODES; COMPRESSION
AB Recently, several error resilient schemes have been proposed to tackle the error propagation problem in the motion-compensated predictive video coding based on a promising technique-distributed source coding (DSC). However, these schemes mainly apply the distributed source codes for channel error correction, while under-utilizing their capability for data compression. A channel-aware joint source-channel video coding scheme based on DSC is proposed to eliminate the error propagation problem in predictive video coding in a more efficient way. It is known that near Slepian-Wolf bound DSC is achieved using powerful channel codes, assuming the source and its reference (also known as side-information) are connected by a virtual error-prone channel. In the proposed scheme, the virtual and real error-prone channels are fused so that a unified single channel code is applied to encode the current frame thus accomplishing a joint source-channel coding. Our analysis of the rate efficiency in recovering error propagation shows that the joint scheme can achieve a lower rate compared with performing source and channel coding separately. Simulation results show that the number of bits used for recovering from error propagation can be reduced by up to 10% using the proposed scheme compared to Sehgal-Jagmohan-Ahuja's DSC-based error resilient scheme.
C1 [Zhang, Yixuan; Ieee, Ce ZhuSenior Member; Yap, Kim-Hui] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Zhang, YX (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM zhan0078@ntu.edu.sg; eczhu@ntu.edu.sg; ekhyap@ntu.edu.sg
RI Zhu, Ce/AEN-1875-2022; Yap, Kim-Hui/A-5157-2011
OI Yap, Kim-Hui/0000-0003-1933-4986
CR Aaron A, 2004, IEEE IMAGE PROC, P3097
   Aaron A, 2002, IEEE DATA COMPR CONF, P252, DOI 10.1109/DCC.2002.999963
   AARON A, 2006, P IEEE INT PICT COD
   Bajcsy J, 2001, GLOB TELECOMM CONF, P1400, DOI 10.1109/GLOCOM.2001.965721
   Cover T. M., 1991, ELEMENTS INFORM THEO
   FORNEY GD, 1988, IEEE T INFORM THEORY, V34, P1123, DOI 10.1109/18.21245
   FORNEY GD, 1988, IEEE T INFORM THEORY, V34, P1152, DOI 10.1109/18.21246
   Garcia-Frias J, 2001, IEEE COMMUN LETT, V5, P417, DOI 10.1109/4234.957380
   Girod B, 2005, P IEEE, V93, P71, DOI 10.1109/JPROC.2004.839619
   Liveris AD, 2002, IEEE COMMUN LETT, V6, P440, DOI 10.1109/LCOMM.2002.804244
   LIVERIS AD, 2002, MULT SIGN PROC IEEE
   Sehgal A, 2004, IEEE T MULTIMEDIA, V6, P249, DOI 10.1109/TMM.2003.822995
   SLEPIAN D, 1973, IEEE T INFORM THEORY, V19, P471, DOI 10.1109/TIT.1973.1055037
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   Varodayan D, 2006, SIGNAL PROCESS, V86, P3123, DOI 10.1016/j.sigpro.2006.03.012
   Wang Y., 2002, VIDEO PROCESSING COM
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   WYNER AD, 1976, IEEE T INFORM THEORY, V22, P1, DOI 10.1109/TIT.1976.1055508
   WYNER AD, 1974, IEEE T INFORM THEORY, V20, P2, DOI 10.1109/TIT.1974.1055171
   XU Q, 2005, P IEEE INT C IM PROC, V2, P674
   Xu Q, 2006, IEEE T IMAGE PROCESS, V15, P3791, DOI 10.1109/TIP.2006.884925
   [No title captured]
NR 22
TC 23
Z9 24
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1648
EP 1656
DI 10.1109/TMM.2008.2007324
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600019
DA 2024-07-18
ER

PT J
AU Xu, CS
   Zhang, YF
   Zhu, GY
   Rui, Y
   Lu, HQ
   Huang, QM
AF Xu, Changsheng
   Zhang, Yi-Fan
   Zhu, Guangyu
   Rui, Yong
   Lu, Hanqing
   Huang, Qingming
TI Using Webcast Text for Semantic Event Detection in Broadcast Sports
   Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Broadcast video; semantic event detection; Webcast text
AB Sports video semantic event detection is essential for sports video summarization and retrieval. Extensive research efforts have been devoted to this area in recent years. However, the existing sports video event detection approaches heavily rely on either video content itself, which face the difficulty of high-level semantic information extraction from video content using computer vision and image processing techniques, or manually generated video ontology, which is domain specific and difficult to be automatically aligned with the video content. In this paper, we present a novel approach for sports video semantic event detection based on analysis and alignment of webcast text and broadcast video. Webcast text is a text broadcast channel for sports game which is co-produced with the broadcast video and is easily obtained from the web. We first analyze webcast text to cluster and detect text events in an unsupervised way using probabilistic latent semantic analysis (pLSA). Based on the detected text event and video structure analysis, we employ a conditional random field model (CRFM) to align text event and video event by detecting event moment and event boundary in the video. Incorporation of webcast text into sports video analysis significantly facilitates sports video semantic event detection. We conducted experiments on 33 hours of soccer and basketball games for webcast analysis, broadcast video analysis and text/video semantic alignment. The results are encouraging and compared with the manually labeled ground truth.
C1 [Xu, Changsheng; Zhang, Yi-Fan; Lu, Hanqing] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100080, Peoples R China.
   [Xu, Changsheng; Zhang, Yi-Fan; Lu, Hanqing] China Singapore Inst Digital Media, Singapore, Singapore.
   [Zhu, Guangyu] Harbin Inst Technol, Sch Comp Sci, Harbin 150001, Peoples R China.
   [Rui, Yong] Microsoft China R&D Grp, Beijing 100080, Peoples R China.
   [Huang, Qingming] Chinese Acad Sci, Grad Univ, Beijing 100039, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Harbin
   Institute of Technology; Microsoft; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100080, Peoples R China.
EM csxu@nlpr.ia.ac.cn; yfzhang@nlpr.ia.ac.cn; gyzhu@jdl.ac.cn;
   yongrui@microsoft.com; luhq@nlpr.ia.ac.cn; qmhuang@jdl.ac.cn
RI zhang, yifan/ABB-5853-2021; Zhu, Guangyu/H-3805-2013; xu,
   cj/HJZ-3488-2023; Huang, Qingming/GLR-3473-2022
OI Huang, Qingming/0000-0002-3025-7099
FU National Sciences Foundation of China [60475010]
FX Manuscript received July 16, 2007 revised May 24, 2008. Current version
   published November 17, 2008. The work of Y.-F. Zhang and H. Lu was
   suppoted by National Sciences Foundation of China under Grant 60475010.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Kiyoharu Aizawa.
CR [Anonymous], TRECVID: TREC Video Retrieval Evaluation
   [Anonymous], 2006, P 14 ACM INT C MULTI
   [Anonymous], 1995, P IEEE INT C MULT CO
   Assfalg E, 2003, COMPUT VIS IMAGE UND, V92, P285, DOI 10.1016/j.cviu.2003.06.004
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   BERTINI M, 2005, P 2 EUR SEM WEB C HE
   CHESHIRE D, 1990, COMPLETE BOOK VIDEO
   Chong E. K, 2013, An Introduction to Optimization
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Duan LY, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P709
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   ERIK F, P CONLL 2003, P142
   Han M., 2002, Proc. ACM Multimedia, P347, DOI [DOI 10.1145/641007.641081, 10.1145/641007.641081]
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   JAIMES A, 2003, P IEEE INT C MULT EX
   JAIMES A, 2003, P INT C IM VID RETR
   Lafferty John, 2001, INT C MACH LEARN ICM
   Mezaris V, 2004, IEEE T CIRC SYST VID, V14, P606, DOI 10.1109/TCSVT.2004.826768
   Naphade MilindR., 2004, P 12 ANN ACM INT C M, P660, DOI DOI 10.1145/1027527.1027680
   Nepal S., 2001, ACM Multimedia, P261
   Nitta N, 2005, MULTIMED TOOLS APPL, V25, P59, DOI 10.1023/B:MTAP.0000046382.62218.e1
   NITTA N, 2002, P 8 INT WORKSH MULT, P110
   PAN H, 2002, P INT C AC SPEECH SI
   REIDSMA D, 2003, P ICCS03 DRESD GER S
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   STRINTZIS MG, 2004, P EUR WORKSH INT KNO
   Wan K, 2004, INT C PATT RECOG, P973, DOI 10.1109/ICPR.2004.1334691
   WANG J, 2005, P INT C AC SPEECH SI
   Wang ZC, 2005, COMMUN THEOR PHYS, V44, P735, DOI 10.1088/6102/44/4/735
   Xu C., 2006, PROC 14 ANN ACM INT, P221, DOI DOI 10.1145/1180639.1180699
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P421, DOI 10.1109/TMM.2008.917346
   XU H, 2004, P WORKSH MULT INF RE
   Xu H, 2006, ACM T MULTIM COMPUT, V2, P44, DOI 10.1145/1126004.1126007
   Xu M, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P1526
   Xu M, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P189
   Xu M, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P281
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
   Zhang D., 2002, ACM Multimedia, P315
   FLEXCRFS FLEXIBLE CO
NR 39
TC 101
Z9 143
U1 3
U2 52
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1342
EP 1355
DI 10.1109/TMM.2008.2004912
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yu, Y
   Li, Z
   Shi, L
   Chen, EYC
   Xu, H
AF Yu, Yang
   Li, Zhu
   Shi, Larry
   Chen, Ethan Yi-Chiun
   Xu, Hua
TI Cross-layer optimization for state update in mobile gaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 16th International Conference on Computer Communications and Networks
CY AUG 13-16, 2007
CL Honolulu, HI
SP IEEE Commun Soc, Nokia
DE bandwidth allocation; Lagrangian relaxation; mobile gaming; state
   update; traffic-distortion tradeoffs; WiMAX
AB In a large-scale mobile gaming environment with limited wireless network bandwidth, efficient mechanisms for state update are crucial to allow graceful real-time interaction for a large number of players. By using the state updating threshold as a key parameter that bridges the resulting state distortion and the network traffic, we are able to study the fundamental traffic-distortion tradeoffs via both theoretical modeling and numerical analysis using real game traces. We consider a WiMAX link model, where the bandwidth allocation is driven by the underlying physical layer link quality as well as application layer gaming behaviors. Such a cross-layer optimization problem can be solved using standard convex programming techniques. By exploring the temporal locality of gaming behavior, we also propose a prediction method for on-line bandwidth adaptation. Using real data traces from a multiplayer driving game, TORCS, the proposed network-aware bandwidth allocation method (NABA) is able to achieve significant reduction in state distortion compared to two baselines: uniform and proportional policies.
C1 [Yu, Yang] Motorola Labs, Applicat Res Ctr, Schaumburg, IL 60196 USA.
   [Shi, Larry] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Chen, Ethan Yi-Chiun; Xu, Hua] Motorola Home & Networks Mobil, Arlington Hts, IL 60004 USA.
   [Li, Zhu] Motorola Labs, Multimedia Res Lab, Schaumburg, IL 60196 USA.
C3 Legend Holdings; Lenovo; University System of Georgia; Georgia Institute
   of Technology; Legend Holdings; Lenovo; Legend Holdings; Lenovo
RP Yu, Y (corresponding author), Motorola Labs, Applicat Res Ctr, Schaumburg, IL 60196 USA.
EM yang@trotorola.com; zhu.li@motorola.com; ccmod1937@yahoo.com;
   y.chen@motorola.com; hua.xu@motorola.com
CR AGGARWAL S, 2004, ACM SIGCOMM WORKSH N, P161
   [Anonymous], MOB WIMAX 1
   Borella MS, 2000, COMPUT COMMUN, V23, P403, DOI 10.1016/S0140-3664(99)00197-8
   CHEN KT, 2005, ACM WORKSH NETW OP S
   Chiang M, 2007, P IEEE, V95, P255, DOI 10.1109/JPROC.2006.887322
   *CQICH, CQICH CHANN CLAR
   FAURBER J, 2002, ACM SIGCOMM WORKSH N
   Feng WC, 2005, IEEE ACM T NETWORK, V13, P488, DOI 10.1109/TNET.2005.850221
   GOEL S, 1992, P AIAA FLIGHT SIM TE
   Li Z, 2006, INT CONF ACOUST SPEE, P5239
   MAUVE M, 2000, WORKSH INT DISTR MUL
   PANTEL L, 2002, ACM SIGCOMM WORKSH N, P79
   Singhal S.K., 1994, USING POSITION HIST
   SINHAL S, 1999, NETWORKED VIRTUAL EN
   *TORCS, OP RAC CAR SIM 2007
   YASUI T, 2005, ACM SIGCOMM WORKSH N, P1
   YU Y, 2007, P INT C COMP COMM NE
   Zhou S., 2004, ACM Transactions on Modeling and Computer Simulation, V14, P31, DOI 10.1145/974734.974736
NR 18
TC 2
Z9 4
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 701
EP 710
DI 10.1109/TMM.2008.922777
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800004
DA 2024-07-18
ER

PT J
AU Melnyk, MA
   Jukan, A
   Polychronopoulos, CD
AF Melnyk, Miguel A.
   Jukan, Admela
   Polychronopoulos, Constantine D.
TI A cross-layer analysis of session setup delay in IP multimedia subsystem
   (IMS) with EV-DO wireless transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE call setup delay; cellular networks; cross-layer; EV-DO; Internet
   Multimedia Subsystem (IMS)
ID TCP
AB This paper analyzes the session setup delay in the IP Multimedia Subsystem (IMS) with the CDMA2000 Evolution Data Only Rev. A (EV-DO rev. A) standard for wireless transmission. Session setup delay is particularly critical for interactive multimedia applications, such as gaming, push-to-X and Voice over IP (VoIP), as it directly translates in user perception of service quality. Keeping signaling delay low, however, is a challenge in IMS due to the text-based nature of the Session Initiation Protocol (SIP) for signaling, and, more significantly, due to the lossy and capacity constrained wireless links. To address this challenge, we analyze the session setup delay end-to-end, by taking into account key system properties across all layers, ranging from radio links to IMS signaling architecture. We present a model for cross-layer performance analysis and simulation, which includes the statistical properties of the EV-DO (rev. A) wireless channel, and also takes into consideration the properties of transport protocols (TCP, UDP) and SIP signaling (message size and compression). By means of analysis and simulations, we study the setup delay performance of a generic, multi-operator IMS communication scenario between two mobile users. We describe how session setup delay can be estimated and reduced in realistic IMS settings and we propose architecture alternatives to the basic IMS scenario. The results derived from this study show that the proposed methods can incrementally lead to a lower setup delay and less sensitivity to the radio transmission quality and frame error rate compared to the base IMS scenario.
C1 Univ Illinois, Coordinated Sci Lab, Champaign, IL 61820 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Melnyk, MA (corresponding author), Bytemobile Inc, Mountain View, CA 94043 USA.
EM mmelnyk@bytemobile.com; cdp@uiuc.edu
CR *3GPP2, XS0013002 3GGP2
   *3GPP2, THIRD GEN PARTN PROJ
   Bi Q, 2006, IEEE J SEL AREA COMM, V24, P36, DOI 10.1109/JSAC.2005.858882
   BI Q, 2004, BELL LABS TECH J, V10, P5
   BOUTREMANS C, 2002, IMPACT LINK FAILURES
   CAMARILLO G, 2004, 3P IP MULTIMEDIA SUB
   CURCIO I, 2002, P IEEE S COMP COMM 0
   DAS S, 2002, P IEEE INFOCOM 2002
   EYERS T, 2000, P IP TEL WORKSH BERL
   FATHI H, 2006, IEEE T VEH TECHN JAN
   Fathi H, 2006, IEEE T MOBILE COMPUT, V5, P1121, DOI 10.1109/TMC.2006.135
   FRIDRICH M, 2003, SIP COMPRESS
   GUO K, 2004, PROVIDING END END QO
   HANNU H, 2003, 3321 RFC
   Korowajczuk L., 2004, Designing cdma2000 Systems
   LEE Y, 2005, KOREAN SIC ENG FOUND
   Lin HT, 2005, IEEE T MOBILE COMPUT, V4, P489, DOI 10.1109/TMC.2005.70
   Mellia M, 2002, IEEE COMMUN LETT, V6, P85, DOI 10.1109/4234.984705
   MELNYK M, UNPUB IEEE INT C COM
   MELNYK M, 2006, P CONS COMM NETW C L
   Paxson Vern., 2000, Computing tcp's retransmission timer
   PRICE R, 2003, 3321 RFC
   ROSENBERG J, 2002, 3261 RFC SIP INT ENG
   YI S, 2004, P INT C INF NETW 200
NR 24
TC 11
Z9 13
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 869
EP 881
DI 10.1109/TMM.2007.895680
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200016
DA 2024-07-18
ER

PT J
AU Yu, ZW
   Wong, HS
AF Yu, Zhiwen
   Wong, Hau-San
TI A rule based technique for extraction of visual attention regions based
   on real-time clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE clustering; knowledge extraction; real time processing; visual attention
   regions; visualization
ID ALGORITHM
AB Recently, the detection of visual attention regions (VAR) is becoming more important due to its useful application in the area of multimedia. Although there exist a lot of approaches to detect visual attention regions, few of them consider the semantic gap between the visual attention regions and high-level semantics. In this paper, we propose a rule based technique for the extraction of visual attention regions at the object level based on real-time clustering, such that VAR detection can be performed in a very efficient way. The proposed technique consists of four stages: 1) a fast segmentation technique which is called the real time clustering algorithm (RTCA); 2) a refined specification of VAR which is known as the hierarchical visual attention regions (HVAR); 3) a new algorithm known as the rule based detection algorithm (RADA) to obtain the set of HVARs in real time, and 4) a new adaptive image display module and the corresponding adaptation operations using HVAR. We also define a new background measure which combines both feature contrast and the geometric property of the region to identify the background region, and a confidence factor which is used to extract the set of hierarchical visual attention regions. Compared with existing techniques, our approach has two advantages: 1) the approach detects the visual attention region at the object level, which bridges the gap between traditional visual attention regions and high-level semantics; 2) our approach is efficient and easy to implement.
C1 City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Yu, ZW (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM yuzhiwen@cs.cityu.edu.hk; cshswong@cityu.edu.hk
OI WONG, Hau-San/0000-0002-1530-7529
CR Ahmad, 1991, ADV NEURAL INFORM PR, V4, P420
   [Anonymous], 2003, ACMMM
   [Anonymous], 1994, Morphological Image Operators
   [Anonymous], 2004, Proc. BMVC, DOI [DOI 10.5244/C.18.98, 10.5244/C.18.98]
   Baluja S, 1997, ROBOT AUTON SYST, V22, P329, DOI 10.1016/S0921-8890(97)00046-8
   Bamidele A, 2004, BT TECHNOL J, V22, P151, DOI 10.1023/B:BTTJ.0000047129.83260.79
   BORMANS J, 2002, ISOIECJTC1SC29WG11N5
   Bradley AP, 2003, J VIS COMMUN IMAGE R, V14, P232, DOI 10.1016/S1047-3203(03)00037-3
   Broadbent DE, 2013, PERCEPTION COMMUNICA
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   CRICK F, 1990, COLD SH Q B, V55, P953
   DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909
   DEUTSCH JA, 1963, PSYCHOL REV, V70, P80, DOI 10.1037/h0039515
   Goh K., 2004, PROC ACM INT C MULTI, P564
   Heijmans HJAM, 1999, COMPUT VIS IMAGE UND, V73, P99, DOI 10.1006/cviu.1998.0703
   Hoi C.-H., 2004, PROC 12 ANN ACM INT, P24
   Hu Y., 2004, P 12 ANN ACM INT C M, P340
   HU Y, 2004, ICME 2004, P1079
   HU Y, 2005, ACM MULTIMEDIA 2005, P716
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   James W., 1890, The Principles of Psychology, V1
   Jing F, 2004, IEEE T IMAGE PROCESS, V13, P699, DOI 10.1109/TIP.2004.826125
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   KIM S, 2005, ACM MULTIMEDIA, P215
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Kyungjoo Cheoi, 2002, Structural, Syntactic, and Statistical Pattern Recognition. Joint IAPR International Workshops SSPR 2002 and SPR 2002 (Lecture Notes in Computer Science Vol. 2396), P329
   Lazebnik S, 2005, IEEE I CONF COMP VIS, P832, DOI 10.1109/ICCV.2005.10
   Lee K, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P475, DOI 10.1109/ICIP.2001.958531
   LEE K, 2001, P 8 INT C IM PROC IC, V2, P475
   Li CS, 1998, INT CONF ACOUST SPEE, P3789, DOI 10.1109/ICASSP.1998.679709
   LI Y, 2003, P IEEE INT C MULT EX, V2, P72
   LIN YY, 2005, ACM MULTIMEDIA, P249
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   MUKHERJEE D, 2003, ISOIECJTC1SC29WG11
   Niebur E, 1998, ATTENTIVE BRAIN, P163
   Paek S, 1998, P SOC PHOTO-OPT INS, V3305, P151, DOI 10.1117/12.304628
   Panis G, 2003, SIGNAL PROCESS-IMAGE, V18, P721, DOI 10.1016/S0923-5965(03)00061-4
   Rutishauser U, 2004, PROC CVPR IEEE, P37
   Smith JR, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P7, DOI 10.1109/ICIP.1998.998987
   Su MS, 2001, IEEE T PATTERN ANAL, V23, P674, DOI 10.1109/34.927466
   TREISMAN A, 1988, PSYCHOL REV, V95, P15, DOI 10.1037/0033-295X.95.1.15
   TREISMAN A, 1998, VISUAL ATTENTION
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   VETRO A, 2003, ISOIECJTC1SC29WG11N5
   WALTHER D, 2004, 2 WORKSH ATT PERF CO, P96
   Walther D., 2005, COMPUT VIS IMAGE UND, P745
   WOLFE JM, 1990, GUIDED SEARCH MODEL, P79
   YANG C, 2005, ACM MULTIMEDIA 2005, P415
NR 49
TC 31
Z9 36
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 766
EP 784
DI 10.1109/TMM.2007.893351
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200009
DA 2024-07-18
ER

PT J
AU Potamianos, A
   Fosler-Lussier, E
   Ammicht, E
   Perakakis, M
AF Potamianos, Alexandros
   Fosler-Lussier, Eric
   Ammicht, Egbert
   Perakakis, Manolis
TI Information seeking spoken dialogue systems - Part II: Multimodal
   dialogue
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia communication; natural language interfaces; speech
   communication
AB In this paper, the task and user interface modules of a multimodal dialogue system development platform are presented. The main goal of this work is to provide a simple, application-independent solution to the problem of multimodal dialogue design for information seeking applications. The proposed system architecture clearly separates the task and interface components of the system. A task manager is designed and implemented that consists of two main submodules: the electronic form module that handles the list of attributes that have to be instantiated by the user, and the agenda module that contains the sequence of user and system tasks. Both the electronic forms and the agenda can be dynamically updated by the user. Next a spoken dialogue module is designed that implements the speech interface for the task manager. The dialogue manager can handle complex error correction and clarification user input, building on the semantics and pragmatic modules presented in Part I of this paper. The spoken dialogue system is evaluated for a travel reservation task of the DARPA Communicator research program and shown to yield over 90% task completion and good performance for both objective and subjective evaluation metrics. Finally, a multimodal dialogue system which combines graphical and speech interfaces, is designed, implemented and evaluated. Minor modifications to the unimodal semantic and pragmatic modules were required to build the multimodal system. It is shown that the multimodal system significantly outperforms the unimodal speech-only system both in terms of efficiency (task success and time to completion) and user satisfaction for a travel reservation task.
C1 Tech Univ Crete, Dept Elect & Comp Engn, Khania 73100, Greece.
   Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
   Bell Labs, Lucent Technol, Whippany, NJ 07981 USA.
C3 Technical University of Crete; University System of Ohio; Ohio State
   University; AT&T; Alcatel-Lucent; Lucent Technologies
RP Potamianos, A (corresponding author), Tech Univ Crete, Dept Elect & Comp Engn, Kounoupidiana Univ Campus, Khania 73100, Greece.
EM potam@telecom.tuc.gr; fosier@cse.ohio-state.edu; eammicht@lucent.com;
   perak@telecom.tuc.gr
OI Fosler-Lussier, Eric/0000-0001-8004-5169
CR ABELLA A, 1996, P EUR C ART INT BUD
   ABELLA A, 1999, P ANN M ASS COMP LIN
   AMMICHT E, 2001, P EUR C SPEECH COMM
   AMMICHT E, 2007, IN PRESS IEEE T MULT, V9
   BERNSEN NO, 1998, P INT C SPEECH LANG
   BILICI V, 2000, P INT C SPEECH LANG
   Carpenter Bob., 1998, Type-Logical Semantics
   CHUCARROLL J, 2000, P 6 ACL C APPL NAT L
   CHUCARROLL J, 1999, P EUR C SPEECH COMM, P1519
   COHEN P, 1998, P INT C SPEECH LANG
   De Mori R., 1998, SPOKEN DIALOGUE COMP, P523
   DENECKE M, 1999, P EUR C SPEECH COMM
   DENECKE M, 2002, P INT C COMP LING TA
   GALLEY M, 2001, P EUR C SPEECH COMM
   GODDEAU D, 1996, P INT C SPEECH LANG
   HUANG X, 2000, P INT C SPEECH LANG
   Johnston M., 2005, Natural Language Engineering, V11, P159, DOI 10.1017/S1351324904003572
   JOHNSTON M, 2002, P ANN M ASS COMP LIN
   LARSEN LB, 1999, P ESCA WORKSH INT DI
   Larsson S., 2000, Natural Language Engineering, V6, P323, DOI 10.1017/S1351324900002539
   LEE S, 2002, P HUM LANG TECHN C S
   Levin E, 2000, IEEE T SPEECH AUDI P, V8, P11, DOI 10.1109/89.817450
   LEVIN E, 1999, P WORKSH AUT SPEECH
   MATHESON C, 2000, P ANN M N AM ASS COM
   MCTEAR MF, 1998, P INT C SPEECH LANG
   Narayanan S, 2002, IEEE T SPEECH AUDI P, V10, P65, DOI 10.1109/89.985544
   Oviatt S, 2000, HUM-COMPUT INTERACT, V15, P263, DOI 10.1207/S15327051HCI1504_1
   PERAKAKIS M, 2005, P INT C MULT INT TRE
   PERAKAKIS M, 2006, IEEE ACM WORKSH SPOK
   POTAMIANOS A, 1999, P ESCA WORKSH INT DI
   POTAMIANOS A, 2000, P INT C SPEECH LANG
   RUDNICKY A, 1999, P EUR C SPEECH COMM
   RUDNICKY A, 1999, P WORKSH AUT SPEECH
   SENEFF S, 1998, P INT C SPEECH LANG
   Shortliffe E, 2012, COMPUTER BASED MED C, V2
   TSANGARIS M, 2002, P HUM LANG TECHN C S
   WALKER M, 2000, P INT C LANG RES EV
   Walker M., 2001, P ANN M ASS COMP LIN
   WALKER M, 2000, NAT LANG ENG SPECIAL
   WALKER MA, 2001, P EUR C SPPECH COMM
   WALKER MA, 2002, P INT C SPEECH LANG
   WARD W, 1995, P 1995 ARPA SPOK LAN
   WARD W, 1999, P WORKSH AUT SPEECH
   XU W, 2002, P 3 SIGDIAL WORKSH D
   ZHOU Q, 2000, P INT C SPEECH LANG
   FREETTS SPEECH SYNTH
NR 46
TC 7
Z9 8
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 550
EP 566
DI 10.1109/TMM.2006.887999
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100010
DA 2024-07-18
ER

PT J
AU Bajic, IV
AF Bajic, Ivan V. u
TI Noncausal error control for video streaming over wireless packet
   networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE ARQ; error concealment; error control; power management; video streaming
ID CONCEALMENT; TRANSMISSION; EZBC
AB Video streaming systems usually employ a reasonably arge receiver buffer to deal with jitter. The existence of such a buffer allows us to implement noncausal algorithms for video processing at the receiver. In this paper, we describe an error control scheme that takes advantage of this fact, and employs a non-causal error-concealment algorithm. We use the knowledge of the)jacket-loss realization for the previously transmitted data to adjust the transmission strategy for the future data, knowing that this future data will be used to conceal the data in the previously lost packets. We demonstrate that such a strategy can be several times note energy efficient than ARQ.
C1 Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP Bajic, IV (corresponding author), Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
EM ibajic@sfu.ca
RI Bajic, Ivan/I-1241-2013
OI Bajic, Ivan/0000-0003-3154-5743
CR Apostolopoulos JG, 2001, PROC SPIE, V4310, P392
   Bajic IV, 2003, PROC SPIE, V5150, P512, DOI 10.1117/12.502885
   BAJIC IV, ERROR CONCEALMENT SC
   BAJIC IV, 2005, P IEEE WIR COM 05 MA, P1106
   Chen PS, 2004, IEEE T CIRC SYST VID, V14, P1183, DOI 10.1109/TCSVT.2004.833165
   Eisenberg Y, 2002, IEEE T CIRC SYST VID, V12, P411, DOI 10.1109/TCSVT.2002.800309
   HASKELL P, 1992, P ICASSP, V3, P545
   Hsiang ST, 2001, SIGNAL PROCESS-IMAGE, V16, P705, DOI 10.1016/S0923-5965(01)00002-9
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   LAM WM, 1993, P ICASSP, V5, P417
   Lee YC, 2002, IEEE T IMAGE PROCESS, V11, P1314, DOI 10.1109/TIR2002.804275
   OZAROW LH, 1994, IEEE T VEH TECHNOL, V43, P359, DOI 10.1109/25.293655
   Parthasarathy V, 1999, IEEE T IMAGE PROCESS, V8, P361, DOI 10.1109/83.748891
   Perkins C, 1998, IEEE NETWORK, V12, P40, DOI 10.1109/65.730750
   Puri R, 2001, IEEE T MULTIMEDIA, V3, P18, DOI 10.1109/6046.909591
   Tsekeridou S, 2000, IEEE T CIRC SYST VID, V10, P646, DOI 10.1109/76.845010
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   Wang Y., 2002, VIDEO PROCESSING COM
   Wolsey L.A., 1999, WIL INT S D
   Zhang J, 2000, IEEE T CIRC SYST VID, V10, P659, DOI 10.1109/76.845011
NR 21
TC 3
Z9 3
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1263
EP 1273
DI 10.1109/TMM.2006.884608
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700015
DA 2024-07-18
ER

PT J
AU Sarangan, V
   Ghosh, D
   Acharya, R
AF Sarangan, Venkatesh
   Ghosh, Donna
   Acharya, Raj
TI Capacity-aware state aggregation for interdomain QoS routing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia traffic management; quality-of-service
AB Quality-of-service (QoS) routing between domains is an essential component for providing service differentiation in the Internet. State aggregation is a technique that makes QoS routing scalable to large internetworks, by presenting a concise and accurate representation of the domain to the routing process. The concept of using a domain's routing capacity as a bandwidth aggregate has been in existence for some time. However, no methodology has been suggested in the literature for its estimation and usage. Also, the impact of a domain's routing capacity on the routing performance has not been studied before. This paper aims to fill these voids by presenting a framework based on "network flows" for estimating a domain's routing capacity and evaluating its efficacy on the routing performance. The routing capacity is used in conjunction with the conventional widest path bandwidth as the domain aggregate. Analytical and experimental results show that appropriate use of routing capacity along with the widest path bandwidth reduces the tendency of the advertised aggregate to overestimate bandwidth availability, and makes the routing process more robust to the frequency of domain state updates. During periods of congestion, the use of routing capacity can improve the bandwidth admitted into the network by as much as 20%.
C1 Oklahoma State Univ, Dept Comp Sci, Stillwater, OK 74078 USA.
   Penn State Univ, Dept Comp Sci & Engn, University Pk, PA 16802 USA.
C3 Oklahoma State University System; Oklahoma State University -
   Stillwater; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Pennsylvania State University; Pennsylvania State University -
   University Park
RP Sarangan, V (corresponding author), Oklahoma State Univ, Dept Comp Sci, Stillwater, OK 74078 USA.
EM saranga@cs.okstate.edu; dghosh@cse.psu.edu; acharya@cse.psu.edu
CR Calvert KL, 1997, IEEE COMMUN MAG, V35, P160, DOI 10.1109/35.587723
   Gibbens R. J., 1990, Disorder in Physical Systems, P113
   Guérin RA, 1999, IEEE ACM T NETWORK, V7, P350, DOI 10.1109/90.779203
   IWATA A, 1998, P IEEE ICC 98, V1, P243
   Lui KS, 2004, IEEE ACM T NETWORK, V12, P17, DOI 10.1109/TNET.2003.822647
   Medhi D, 2002, IEEE COMMUN MAG, V40, P106, DOI 10.1109/MCOM.2002.1106166
   MEIERHELLSTERN KS, 1989, IEEE T COMMUN, V37, P367, DOI 10.1109/26.20117
   Nelakuditi S, 2004, COMPUT NETW, V44, P79, DOI 10.1016/S1389-1286(03)00346-3
   ROSS KW, 1989, IEEE T COMMUN, V37, P740, DOI 10.1109/26.31166
   Wang Z, 1996, IEEE J SEL AREA COMM, V14, P1228, DOI 10.1109/49.536364
   Xiao XP, 1999, IEEE NETWORK, V13, P8, DOI 10.1109/65.768484
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 15
TC 5
Z9 5
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 792
EP 808
DI 10.1109/TMM.2006.876280
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300013
DA 2024-07-18
ER

PT J
AU Androutsos, P
   Androutsos, D
   Venetsanopoulos, AN
AF Androutsos, P
   Androutsos, D
   Venetsanopoulos, AN
TI A distributed fault-tolerant MPEG-7 retrieval scheme based on small
   world theory
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE distributed; fault-tolerant; indexing; MPEG-7; retrieval; small world
AB A small world search agent employing peer-to-peer (P2P) concepts borrowed from sociology is employed for performing image retrievals in a small world distributed media index. The Small World Indexing Method (SWIM) allows for a highly networked architecture where index information does not exist as a separate entity on a specific server, but rather is stored within the actual media objects themselves. Since each media object is only responsible for a small portion of the overall index, the loss of portions of the overall network (data objects) accounts for only a small degradation in the overall retrieval performance. Building upon previous work, the graceful degradation which is provided by the SWIM system is addressed here for retrievals which are performed using small world user agents on a large set of MPEG-7 described images.
C1 Domin Voting Syst, Toronto, ON M6K 3E8, Canada.
   Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
   Univ Toronto, Edward S Rogers Sr Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.
C3 Toronto Metropolitan University; University of Toronto
RP Domin Voting Syst, Toronto, ON M6K 3E8, Canada.
EM peter@dvscorp.com; dimitri@ee.ryerson.ca; anv@dsp.toronto.edu
CR Albert R, 1999, NATURE, V401, P130, DOI 10.1038/43601
   ANDROUTSOS P, 2004, P IEEE INT C IM PROC
   [Anonymous], LECT NOTES COMPUTER
   Antani S, 2002, PATTERN RECOGN, V35, P945, DOI 10.1016/S0031-3203(01)00086-3
   Barabási AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Bollobás B, 2003, SIAM PROC S, P132
   Broder A, 2000, COMPUT NETW, V33, P309, DOI 10.1016/S1389-1286(00)00083-9
   BRUNELLI R, 2000, P IEEE INT C MULT JU, V1, P145
   Chang SF, 2001, IEEE T CIRC SYST VID, V11, P688, DOI 10.1109/76.927421
   Erdos P., 1959, Publicationes Mathematicae Debrecen, V6, P18
   Granovetter Mark, 1983, SOCIOLOGICAL THEORY, V1, P201, DOI [10.2307/202051, DOI 10.2307/202051]
   GRANOVETTER MS, 1973, AM J SOCIOL, V78, P1360, DOI 10.1086/225469
   Guare J., 1990, 6 DEGREES SEPARATION
   Kleinberg J, 2001, SCIENCE, V294, P1849, DOI 10.1126/science.1067014
   MILGRAM S, 1967, PSYCHOL TODAY, V1, P61
   NEWMAN MEJ, 1999, 9905034 SANT FE I
   NEWMAN MEJ, 1999, 9912080 SANT FE I
   NG CH, 2002, P 11 WORLD WID WEB C
   NIBLACK W, 1993, P SOC PHOTO-OPT INS, V1908, P173
   OBRIEN D, 1996, CHICAGO TRIBUNE 0708, P1
   POOL ID, 1978, SOC NETWORKS, V1, P5, DOI 10.1016/0378-8733(78)90011-4
   Santini S, 1999, IEEE T PATTERN ANAL, V21, P871, DOI 10.1109/34.790428
   Stoica I., 2001, P ACM SIGCOMM AUG
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
NR 24
TC 6
Z9 7
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 278
EP 288
DI 10.1109/TMM.2005.864276
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300009
OA Green Published
DA 2024-07-18
ER

PT J
AU Liu, JC
   Li, B
   Zhang, YQ
AF Liu, JC
   Li, B
   Zhang, YQ
TI Optimal stream replication for video simulcasting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bandwidth allocation; multicast; simulcasting; stream replication
ID MULTICAST
AB Video simulcasting enables a sender to generate replicated streams of different rates, serving receivers of diverse access bandwidths. As replication introduces noticeable redundancy, balancing bandwidth consumption with user satisfaction becomes a critical concern in simulcasting. This paper investigates the above issue; more explicitly, we seek answers to the following two questions: what is the number of streams that should be generated, and what is the bandwidth that should be allocated to each stream? We derive optimal and efficient solutions, and evaluate their performance under a variety of configurations. The results demonstrate that an optimal and adaptive bandwidth allocation significantly improves user satisfaction under stringent resource constraints, and an optimal choice of the stream number yields further improvements.
C1 Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V6B 5K3, Canada.
   Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   Microsoft Corp, Redmond, WA 98052 USA.
C3 Simon Fraser University; Hong Kong University of Science & Technology;
   Microsoft
RP Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V6B 5K3, Canada.
EM jcliu@cs.sfu.ca; bli@cs.ust.hk; yzhang@microsoft.com
RI Li, Bo/AAA-8968-2020
OI Li, Bo/0000-0002-7294-6888
CR Banerjee S., 2002, A comparative study of application layer multicast protocols
   Cheung SY, 1996, IEEE INFOCOM SER, P553, DOI 10.1109/INFCOM.1996.493348
   DAN A, 1994, P ACM MULT 94 OCT
   DECUETOS P, 2001, P PACK VID WORKS APR
   HARTANTO F, 2002, P IEEE INT C MULT EX
   JIANG T, 1999, P NOSSDAV 99 JUN
   KIM T, 2001, P NOSSDAV 01 JUN
   KUHN P, 2001, P PACKETVIDEO 01 APR
   Legout A, 2001, IEEE ACM T NETWORK, V9, P464, DOI 10.1109/90.944344
   LI X, 1996, P HPDC 96 AUG
   LIU J, 2004, P IEEE INFOCOM 04 HO
   LIU J, 2002, P IEEE INFOCOM 0 JUN
   Liu JC, 2003, IEEE MULTIMEDIA, V10, P22, DOI 10.1109/MMUL.2003.1167919
   MCCANNE S, 1996, P ACM SIGCOMM 96 STA, P117
   SISALEM D, 2000, P IEEE IFIP IWQOS 00
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   Wang Y., 2001, VIDEO PROCESSING COM
   WU L, 1997, P NOSSDAV 97 MAY
   YANG Y, 2000, P IEEE ICNP 00 NOV
   YOUN J, 2000, P IEEE INT CIRC SYST
   ZHANG X, 2005, P IEEE INFOCOM 05 MI
NR 21
TC 9
Z9 9
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 162
EP 169
DI 10.1109/TMM.2005.861279
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000015
DA 2024-07-18
ER

PT J
AU Wu, MY
   Ma, SJ
   Shu, W
AF Wu, MY
   Ma, SJ
   Shu, W
TI Scheduled video delivery - A scalable on-demand video delivery scheme
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content delivery; continuous media; incentives; multicast;
   Video-on-Demand
AB Continuous media, such as digital movies, video clips, and music, are becoming an increasingly common way to convey information., entertain and educate people. However, limited system and network resources have delayed the widespread usage of continuous media. Most existing on-demand services are not scalable for a large content repository. In this paper, we propose a scalable and inexpensive video delivery scheme, named Scheduled Video Delivery (SVD). In the SVD scheme, users submit requests with a specified start time. Incentives are provided so that users will specify the start times that reflect their real needs. The SVD system combines requests to form the multicasting groups and schedules these groups to meet the deadline. SVD scheduling has a different objective from many existing scheduling schemes. Its focus has been shifted from minimizing the waiting time toward meeting deadlines and at the same time combining requests to form multicasting groups. SVD compliments most existing video delivery schemes as it can be combined with them. It requires much less resources than other schemes. Simulation study for the SVD performance and the comparison to other schemes are presented.
C1 Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China.
   Univ New Mexico, Dept Elect & Comp Engn, Albuquerque, NM 87131 USA.
C3 Shanghai Jiao Tong University; University of New Mexico
RP Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China.
EM wu-my@cs.sjtu.edu.cn; sjma@ece.unm.edu; shu@ece.unm.edu
RI Shu, Wei/A-9329-2009; Wu, MinYou/B-3780-2009
CR AGGARWAL C, 1996, IEEE INT C MULT COMP
   Aggarwal CC, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P118, DOI 10.1109/MMCS.1996.534963
   AKAMAI J, 1999, INTERNET BOTTLENECKS
   ALMEROTH K, 2001, IEEE T CIRCUITS SYST, P426
   ALMEROTH K, 1998, 7 INT WORLS WID WEB
   Almeroth KC, 1996, IEEE J SEL AREA COMM, V14, P1110, DOI 10.1109/49.508282
   [Anonymous], 1998, SIGCOMM
   [Anonymous], 1949, Human behaviour and the principle of least-effort
   BEY HCD, 1995, PROGRAM TRANSMISSION
   CARTER S, 1997, ICCCN 97 LAS VEG NV, P200
   CHAN S, 1999, P 1999 IEEE INT C CO
   Chan SHG, 2003, IEEE T BROADCAST, V49, P150, DOI 10.1109/TBC.2003.813438
   Chu Y.H., 2000, ACM SIGMETRICS
   Cocchi R, 1993, IEEE ACM T NETWORK, V1, P614, DOI 10.1109/90.266050
   COFFMAN JEG, 2001, 6 INT WORKSH WEB CAC
   Dan A., 1994, Proceedings ACM Multimedia '94, P15, DOI 10.1145/192593.192614
   Dan A, 1996, MULTIMEDIA SYST, V4, P112, DOI 10.1007/s005300050016
   Dan A, 1996, P SOC PHOTO-OPT INS, V2667, P344, DOI 10.1117/12.235887
   Eager D, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P199, DOI 10.1145/319463.319601
   GAO L, 1999, 7 ACM INT MULT C ACM, P203
   Gao LX, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P117, DOI 10.1109/MMCS.1999.778179
   HUA KA, 1997, SIGCOMM 97, P89
   Lee JYB, 2002, IEEE T MULTIMEDIA, V4, P38, DOI 10.1109/6046.985552
   Little T. D. C., 1994, IEEE Multimedia, V1, P14, DOI 10.1109/MMUL.1994.318978
   MACKIEMASON JK, 1995, IEEE J SEL AREA COMM, V13, P1141, DOI 10.1109/49.414634
   SEN S, 1999, P 9 INT WORKSH NETW
   SHEU S, 1997, 5 DASFAA APR
   Shu W, 2004, IEEE MULTIMEDIA, V11, P24, DOI 10.1109/MMUL.2004.1289039
   SHU W, 2004, INT C COMP THEIR APP
   STOLLER SD, 1995, NETWORK OPERATING SY, P330
   WANG X, 2002, IEEE J SEL AREA COMM, V18, P2514
   WANG X, 2000, INT WORKSH NETW OP S, P1
   WU M, 2002, INT WORKSH NETW OP S
   WU M, 2002, IEEE T MULTIMEDIA, V4
   WU M, 2005, COMPUTER NETWORKS
   WU M, 2001, MULTIMEDIA TOOLS APP, V14
NR 36
TC 7
Z9 8
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 179
EP 187
DI 10.1109/TMM.2005.861280
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000017
DA 2024-07-18
ER

PT J
AU Lin, HYS
   Liao, HYM
   Lu, CS
   Lin, JC
AF Lin, HYS
   Liao, HYM
   Lu, CS
   Lin, JC
TI Fragile watermarking for authenticating 3-D polygonal meshes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE authentication; fragile watermarking; parameterization; polygonal
   meshes; tampering detection
ID 3D; MODELS
AB Designing a powerful fragile watermarking technique for authenticating three-dimensional (3-D) polygonal meshes is a very difficult task. Yeo and Yeung [34] were first to propose a fragile watermarking method to perform authentication of 3-D polygonal meshes. Although their method can authenticate the integrity of 3-D polygonal meshes, it cannot be used for localization of changes. In addition, it is unable to distinguish malicious attacks from incidental data processings. In this paper, we trade off the causality problem in Yen and Yeung's method for a new fragile watermarking scheme. The proposed scheme can not only achieve localization of malicious modifications in visual inspection, but also is immune to certain incidental data processings (such as quantization of vertex coordinates and vertex reordering). During the process of watermark embedding, a local mesh parameterization approach is employed to perturb the coordinates of invalid vertices while cautiously maintaining the visual appearance of the original model. Since the proposed embedding method is independent of the order of vertices, the hidden watermark is immune to some attacks, such as vertex reordering. In addition, the proposed method can be used to perform region-based tampering detection. The experimental results have shown that the proposed fragile watermarking scheme is indeed powerful.
C1 Natl Chiao Tung Univ, Dept Comp & Informat Sci, Hsinchu 300, Taiwan.
   Acad Sinica, Inst Sci Informat, Taipei 115, Taiwan.
C3 National Yang Ming Chiao Tung University; Academia Sinica - Taiwan
RP Natl Chiao Tung Univ, Dept Comp & Informat Sci, Hsinchu 300, Taiwan.
EM hylin@cis.nctu.edu; liao@iis.sinica.edu.tw; lcs@iis.sinica.edu.tw;
   jclin@cis.nctu.edu.tw
CR ASPERT N, 2002, P SPIE, V4970
   BENEDENS O, 2002, P INF HID NOORDW NET, P177
   BENEDENS O, 2000, P INF SEC WORKSH WOL, P20
   BIERMANN H, 2002, P ACM SIGGRAPH 02, P312, DOI DOI 10.1145/566570.566583
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Cayre F, 2003, IEEE T SIGNAL PROCES, V51, P939, DOI 10.1109/TSP.2003.809380
   Cayre F, 2003, SIGNAL PROCESS-IMAGE, V18, P309, DOI 10.1016/S0923-5965(02)00147-9
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Fornaro C, 2000, COMPUT AIDED DESIGN, V32, P727, DOI 10.1016/S0010-4485(00)00048-8
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Johnson AE, 1998, GRAPH MODEL IM PROC, V60, P261, DOI 10.1006/gmip.1998.0474
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   KANAI S, 1998, P 6 IFIP WG 5 2 GEO, P296
   KANKANHALLI MS, 2001, P 6 EUR WORKSH MULT
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   KOH B, 1999, P IEEE WORKSH MULT S, P71
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Mao XY, 2001, PROC SPIE, V4314, P253, DOI 10.1117/12.435406
   Ohbuchi R, 1998, IEEE J SEL AREA COMM, V16, P551, DOI 10.1109/49.668977
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   Ohbuchi R, 1999, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P180, DOI 10.1109/CGI.1999.777952
   Ohbuchi R, 1998, COMPUT COMMUN, V21, P1344, DOI 10.1016/S0140-3664(98)00202-3
   Ohbuchi R., 2001, Graphics Interface, P9
   Praun E, 2001, COMP GRAPH, P179, DOI 10.1145/383259.383277
   PRAUN E, 1999, P SIGGRAPH, P154
   ROSSL C, 2000, S SMART GRAPH STANF
   SWELDENS W, 2001, SIGGRAPH 2001 COURSE
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   TURK G, 1999, GRAPHICS GEMS
   Wagner M. G., 2000, Proceedings Geometric Modeling and Processing 2000. Theory and Applications, P201, DOI 10.1109/GMAP.2000.838252
   Yeo BL, 1999, IEEE COMPUT GRAPH, V19, P36, DOI 10.1109/38.736467
   Yeung MM, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P680, DOI 10.1109/ICIP.1997.638587
   Yin KK, 2001, COMPUT GRAPH-UK, V25, P409, DOI 10.1016/S0097-8493(01)00065-6
NR 37
TC 63
Z9 69
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 997
EP 1006
DI 10.1109/TMM.2005.858412
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200001
DA 2024-07-18
ER

PT J
AU Hsia, SC
   Cheng, SC
   Chou, SW
AF Hsia, SC
   Cheng, SC
   Chou, SW
TI Efficient adaptive error concealment technique for video decoding system
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive process; error concealment; motion vector; spatial
   interpolation
ID SCENE-CHANGE DETECTION; IMAGE SEQUENCES; MISSING DATA; TRANSMISSION;
   INTERPOLATION; INFORMATION; ALGORITHM; RECOVERY
AB This paper presents a novel error concealment method for video decoding system. The proposed algorithm adaptively combines the spatial interpolation and the temporal prediction technique based on block variance and interframe correlation, to recover the lost data. The adaptive function depends on the scene change detection, motion distance and spatial information from the nearby blocks of the previous and current frames to determine the weighting of the spatial interpolation and the temporal compensation. Simulations demonstrate that the proposed technique can achieve well subjective and quantitative results, and outperforms all the others against which are compared. Even if the scene changes in the videos, this algorithm also can efficiently recover the damaged blocks for Intra(I), Predictive(P), and Bidirectional (B) frames.
C1 Natl Kaohsiung First Univ Sci & Technol, Dept Comp & Commun Engn, Kaohsiung 824, Taiwan.
C3 National Kaohsiung University of Science & Technology
RP Hsia, SC (corresponding author), Natl Kaohsiung First Univ Sci & Technol, Dept Comp & Commun Engn, Kaohsiung 824, Taiwan.
EM hsia@ccms.nkfust.edu.tw
CR Cen S, 2003, IEEE T MULTIMEDIA, V5, P1, DOI 10.1109/TMM.2003.808825
   Chang PC, 2000, IEEE T CIRC SYST VID, V10, P600, DOI 10.1109/76.845005
   Chu WJ, 1998, IEEE T CIRC SYST VID, V8, P74, DOI 10.1109/76.660830
   Chung YJ, 1999, IEEE T CIRCUITS-II, V46, P951, DOI 10.1109/82.775393
   Côté G, 1998, IEEE T CIRC SYST VID, V8, P849, DOI 10.1109/76.735381
   Frossard P, 2001, IEEE T CIRC SYST VID, V11, P989, DOI 10.1109/76.946516
   HEMAMI SS, 1996, IEEE T CIRCUITS SYST, V6, P1063
   Hsia SC, 1997, IEEE T CIRC SYST VID, V7, P924, DOI 10.1109/76.644073
   Huang CL, 2001, IEEE T CIRC SYST VID, V11, P1281, DOI 10.1109/76.974682
   ISMAEIL I, 2000, P IM PROC, V3, P388
   *ISO IEC, 1993, 138182 ISOIEC
   Kang EK, 1999, IEEE T CONSUM ELECTR, V45, P932, DOI 10.1109/30.793648
   Kim CS, 1999, IEEE T CIRC SYST VID, V9, P1063, DOI 10.1109/76.795059
   KOKARAM AC, 1995, IEEE T IMAGE PROCESS, V4, P1496, DOI 10.1109/83.469931
   KOKARAM AC, 1995, IEEE T IMAGE PROCESS, V4, P1509, DOI 10.1109/83.469932
   LAM WM, 1995, IEEE T IMAGE PROCESS, V4, P533, DOI 10.1109/83.382489
   LAM WM, 1993, P ICASSP, V5, P417
   Lee PJ, 1999, IEEE T CONSUM ELECTR, V45, P851, DOI 10.1109/30.793622
   Lee YS, 1999, IEEE T CIRCUITS-II, V46, P742, DOI 10.1109/82.769782
   LI C, 2001, P IEEE WORKSH SIGN P, P247
   Li HZ, 2001, IEEE T CIRC SYST VID, V11, P1183, DOI 10.1109/76.964785
   MUALLA MA, 1999, ELECTRON LETT, V135, P215
   Park JW, 1997, IEEE T CIRC SYST VID, V7, P845, DOI 10.1109/76.644064
   Park JW, 1999, IEEE T CIRC SYST VID, V9, P1003, DOI 10.1109/76.795052
   Shanableh T, 2003, IEEE T MULTIMEDIA, V5, P257, DOI 10.1109/TMM.2003.811624
   SHANABLEH T, 2000, P ICIP, V3, P396
   Suh JW, 1997, IEEE T CONSUM ELECTR, V43, P295, DOI 10.1109/30.628616
   Tsekeridou S, 2000, IEEE T CIRC SYST VID, V10, P646, DOI 10.1109/76.845010
   Wang Z, 1998, IEEE T IMAGE PROCESS, V7, P1056, DOI 10.1109/83.701166
   WEI BWY, 1995, IEEE T CIRC SYST VID, V5, P175, DOI 10.1109/76.388067
   YALIN F, 1998, IEEE C COMM TECHN
   Zhang J, 2000, IEEE T CIRC SYST VID, V10, P659, DOI 10.1109/76.845011
   Zhu QF, 1993, IEEE T CIRC SYST VID, V3, P248, DOI 10.1109/76.224235
   Zhu WW, 1998, IEEE T CIRC SYST VID, V8, P713, DOI 10.1109/76.728413
NR 34
TC 17
Z9 23
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 860
EP 868
DI 10.1109/TMM.2005.854432
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900007
DA 2024-07-18
ER

PT J
AU Liao, WJ
   Ju, HJ
AF Liao, WJ
   Ju, HJ
TI Adaptive slot allocation in DOCSIS-based CATV networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE asymmetric network; DOCSIS; HFC; slot allocation; TCP
ID PERFORMANCE; TCP
AB This paper proposes an adaptive slot allocation mechanism to improve the performance of Transmission Control Protocol (TCP) in DOCSIS-based hybrid fiber coaxial (HFC) networks. The proposed mechanism is comprised of two parts: fast request transmission (FRT) and long packet deferment (LPD). FRT is designed to handle one-way TCP transfers, while LDP targets two-way transfers. Here "one-way transfers" means all active cable modems perform downloading; while "two-way transfers" indicates some perform downloading and some perform uploading. We analyze the proposed mechanism and conduct simulations using network simulator ns-2 to evaluate the performance of our mechanism. The results show that the proposed mechanism has better performance in terms of aggregate downstream throughput, access delay, and required buffer size, as compared to the original control mechanism of DOCSIS.
C1 Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
   Natl Taiwan Univ, Grad Inst Commun Engn, Taipei, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
EM wjliao@cc.ee.ntu.edu.tw
OI Liao, Wanjiun/0000-0001-5396-8849
CR Ali MT, 2000, IEEE J SEL AREA COMM, V18, P1261, DOI 10.1109/49.857926
   Balakrishnan H, 1999, MOBILE NETW APPL, V4, P219, DOI 10.1023/A:1019155000496
   CHIT ITM, 1999, P ICON 1999, P372
   Cohen R, 1998, COMPUT COMMUN, V20, P1502, DOI 10.1016/S0140-3664(97)00160-6
   Cohen R, 1998, IEEE ACM T NETWORK, V6, P15, DOI 10.1109/90.663937
   Droubi M, 2000, IEEE IC COMP COM NET, P54, DOI 10.1109/ICCCN.2000.885470
   ELLOUMI O, P IEEE GLOBECOM 98, P545
   Ju HJ, 2002, IEEE IC COMP COM NET, P543, DOI 10.1109/ICCCN.2002.1043122
   Lakshman TV, 1997, IEEE INFOCOM SER, P38, DOI 10.1109/INFCOM.1997.635112
   LIA YD, 2001, P IEEE ICC 2001, P1824
   Lin YD, 1998, IEEE T BROADCAST, V44, P427, DOI 10.1109/11.735904
   ORFANOUDAKIS T, 2000, P INT ZUR SEM BROADB, P133
   SALA D, 1998, P BROADB COMM, P83
   SALA D, P IEEE INFOCOM 98 SA, P1392
   Sdralia V, 1999, IEEE T BROADCAST, V45, P196, DOI 10.1109/11.796261
   Varma S, 1999, IEEE INFOCOM SER, P1548, DOI 10.1109/INFCOM.1999.752177
NR 16
TC 18
Z9 20
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 479
EP 488
DI 10.1109/TMM.2004.827498
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200011
OA Green Published
DA 2024-07-18
ER

PT J
AU Roy-Chowdhury, AK
   Chellappa, R
   Keaton, T
AF Roy-Chowdhury, AK
   Chellappa, R
   Keaton, T
TI Wide baseline image registration with application to 3-D face modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE biometrics; face modeling; feature correspondence; image registration
AB Establishing correspondence between features in two images of the same scene taken from different viewing angles is a challenging problem in image processing and computer vision. However, its solution is an important step in many applications like wide baseline stereo, three-dimensional (3-D) model alignment, creation of panoramic views, etc. In this paper, we propose a technique for registration of two images of a face obtained from different viewing angles. We show that prior information about the general characteristics of a face obtained from video sequences of different faces can be used to design a robust correspondence algorithm. The method works by matching two-dimensional (2-D) shapes of the different features of the face (e.g., eyes, nose etc.). A doubly stochastic matrix, representing the probability of match between the features, is derived using the Sinkhorn normalization procedure. The final correspondence is obtained by minimizing the probability of error of a match between the entire constellation of features in the two sets, thus taking into account the global spatial configuration of the features. The method is applied for creating holistic 3-D models of a face from partial representations. Although this paper focuses primarily on faces, the algorithm can also be used for other objects with small modifications.
C1 Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA.
   Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   HRL Labs LLC, Dept Signal & Image Proc, Malibu, CA 90265 USA.
C3 University System of Maryland; University of Maryland College Park;
   University System of Maryland; University of Maryland College Park; HRL
   Laboratories
RP Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA.
EM amitrc@ee.ucr.edu; rama@cfar.umd.edu; pakeaton@hrl.com
RI Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/B-6573-2012
OI Roy-Chowdhury, Amit/0000-0001-6690-9725
CR [Anonymous], P EUR C COMP VIS
   [Anonymous], Probability, Random Variables and Stochastic Processes
   [Anonymous], INTRO STAT SIGNAL PR
   Badra F, 1999, INT J PATTERN RECOGN, V13, P685, DOI 10.1142/S0218001499000409
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BLAIS G, 1995, IEEE T PATTERN ANAL, V17, P820, DOI 10.1109/34.400574
   BURL M, 1998, EUR C COMP VIS
   Cham TJ, 1998, PROC CVPR IEEE, P442, DOI 10.1109/CVPR.1998.698643
   CHOWDHURY AR, 2002, INT C MULT EXP LAUS
   Cover Thomas A., 1991, ELEM INF THEORY, DOI 10.1002/047174882X
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   FITZGIBBON AW, INT C COMP VIS
   FITZGIBBON AW, ROBUST REGISTRATION
   FU K, 1982, SYNTACTIC PATTERN EC
   GRAHAM D., 1998, COMPUTER SYSTEMS SCI, V163, P446
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   KOCH R, 1998, P EUR C COMP VIS, P55
   Ritter N, 1999, IEEE T MED IMAGING, V18, P404, DOI 10.1109/42.774168
   Schmid C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P230, DOI 10.1109/ICCV.1998.710723
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591
   Vemuri B. C., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P435
   Viola P, 1997, INT J COMPUT VISION, V24, P137, DOI 10.1023/A:1007958904918
   ZHANG Z, 1992, 3D D YNAMIC SCENE AN
   [No title captured]
   2002, THESIS U MARYLAND CO
NR 26
TC 6
Z9 6
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 423
EP 434
DI 10.1109/TMM.2004.827511
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200005
DA 2024-07-18
ER

PT J
AU Loui, AC
   Savakis, A
AF Loui, AC
   Savakis, A
TI Automated event clustering and quality screening of consumer pictures
   for digital albuming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE digital albuming; event clustering; image indexing; image quality
AB In this paper, algorithms for automatic albuming of consumer photographs are described. Specifically, two core algorithms namely event clustering and screening of low-quality images, are introduced and their performance is evaluated. Event clustering and image quality screening have many applications including albuming services, image management and organization, and digital photofinishing. These are difficult tasks because there is, in general, none (or very limited) contextual information about picture content, and the final interpretation could be subjective. A novel event-clustering algorithm is created to automatically segment pictures into events and subevents for albuming, based on date/time metadata information, as well as color content of the pictures. A block-based color histogram correlation technique is developed for image content comparison of general consumer pictures. A new quality-screening algorithm is developed based on object quality measures, to detect problematic images caused by underexposure, low contrast, and camera defocus or movement. Performance testing of these algorithms was conducted using a database of real consumer photos and showed that these functions provide a useful first-cut album layout for typical rolls of consumer pictures. The proposed techniques and system represent a major step toward automatic albuming of consumer pictures. Automatic albuming application software which, based on the above image event clustering and quality screening algorithms, has been successfully tested and validated through a recent consumer trial in the United States.
C1 Eastman Kodak Co, Elect Imaging Prod, R&D, Rochester, NY 14650 USA.
   Rochester Inst Technol, Dept Comp Engn, Rochester, NY 14623 USA.
C3 Eastman Kodak; Rochester Institute of Technology
RP Eastman Kodak Co, Elect Imaging Prod, R&D, Rochester, NY 14650 USA.
EM alexander.loui@kodak.com; savakis@mail.rit.edu
CR Ballard D.H., 1982, Computer Vision
   Flickner M., 1995, IEEE COMPUT, V28, P23, DOI DOI 10.1109/2.410146
   Huang J, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P325, DOI 10.1145/266180.266383
   JAIN AK, 1988, ALGORITHMS CLUSTERIN, P96
   Katajamäki J, 1998, J IMAGING SCI TECHN, V42, P250
   Kuchinsky Allan., 1999, P SIGCHI C HUMAN FAC, P496, DOI [DOI 10.1145/302979.303143, 10.1145/302979.303143]
   LOUI A, 1999, P ACM MULT 99 ORL FL
   LOUI AC, 2000, P IEEE INT C MULT EX
   LUO J, 2001, IEEE INT C IM P 01 T
   MINKA T, 1996, P COMP VIS PATT REC
   NIBLACK W, 1993, P SOC PHOTO-OPT INS, V1908, P173
   PACK S, 1999, THESIS TEMPLE U PHIL
   Pass G., 1996, IEEE WORKSH APPL COM
   Pentland A, 1996, INT J COMPUT VISION, V18, P233, DOI 10.1007/BF00123143
   PLATT J, 2000, P IEEE WORKSH CONT B
   RODDEN JK, 1999, P BCS IRSG 21 ANN C
   SMITH JR, 1996, ACM MULT 96 BOST MA
   STENT A, 2001, P ACM SIGIR 01 NEW O
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Vailaya A, 2001, IEEE T IMAGE PROCESS, V10, P117, DOI 10.1109/83.892448
NR 20
TC 53
Z9 82
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 390
EP 402
DI 10.1109/TMM.2003.814723
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500011
DA 2024-07-18
ER

PT J
AU Guan, XD
   Li, F
   Zhang, YF
   Cosman, PC
AF Guan, Xiaodi
   Li, Fan
   Zhang, Yangfan
   Cosman, Pamela C.
TI End-to-End Blind Video Quality Assessment Based on Visual and Memory
   Attention Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video quality assessment; UGC videos; human visual system; visual
   attention; memory attention
ID MECHANISMS
AB Developing an objective quality assessment model for user-generated content (UGC) videos is significant for multimedia applications, and also a challenge due to the diversity of video content and unpredictability of distortions. To predict the perceived quality, it is necessary to consider the human visual system, in which attention in visual and memory domains is an essential component. With the idea that the stimulus-driven bottom-up mechanism and cognition-driven top-down mechanism work in synergy to generate quality-aware attention, we propose an end-to-end blind video quality assessment (VQA) algorithm based on visual and memory attention modeling. First, a quality-aware visual attention module is established to obtain spatial-temporal attention-guided representations for frame-level quality perception. Specifically, an attention selection and confluence method is developed by circularly integrating the quality-aware attention information to spatial-temporal content features. Then, with the aid of a quality-aware memory attention module, the video-level attention-guided features are inferred through the dimension and attention reshaping of frame-level representations. The video quality is predicted with the guidance of frame-level visual attention and video-level memory attention in an end-to-end structure. Experimental results on five UGC-VQA databases (CVD2014, LIVE-Qualcomm, KoNViD-1 k, LIVE-VQC and Youtube-UGC) demonstrate the effectiveness of our modules.
C1 [Guan, Xiaodi; Li, Fan; Zhang, Yangfan] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Shaanxi Key Lab Deep Space Explorat Intelligent In, Xian 710049, Peoples R China.
   [Cosman, Pamela C.] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
C3 Xi'an Jiaotong University; University of California System; University
   of California San Diego
RP Li, F (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Shaanxi Key Lab Deep Space Explorat Intelligent In, Xian 710049, Peoples R China.
EM gxd1997@stu.xjtu.edu.cn; lifan@mail.xjtu.edu.cn; zyf-18@stu.xjtu.edu.cn;
   pcosman@eng.ucsd.edu
OI Cosman, Pamela/0000-0002-4012-0176; Guan, Xiaodi/0000-0002-0373-7011
FU National Natural Science Foundation of China [62071369]; Xi'an Science
   and Technology Project [21RGZN0009]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62071369, and in part by Xi'an Science
   and Technology Project under Grant 21RGZN0009.
CR Alers H, 2015, SIGNAL PROCESS-IMAGE, V32, P69, DOI 10.1016/j.image.2015.01.006
   Banitalebi-Dehkordi M, 2020, IEEE T BROADCAST, V66, P676, DOI 10.1109/TBC.2019.2957670
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Buschman TJ, 2007, SCIENCE, V315, P1860, DOI 10.1126/science.1138071
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Culibrk D, 2011, IEEE T IMAGE PROCESS, V20, P948, DOI 10.1109/TIP.2010.2080279
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Duanmu ZF, 2018, IEEE T IMAGE PROCESS, V27, P6135, DOI 10.1109/TIP.2018.2855403
   Engelke U, 2011, IEEE SIGNAL PROC MAG, V28, P50, DOI 10.1109/MSP.2011.942473
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Feng X, 2011, IEEE T BROADCAST, V57, P81, DOI 10.1109/TBC.2010.2092150
   Ghadiyaram D, 2018, IEEE T CIRC SYST VID, V28, P2061, DOI 10.1109/TCSVT.2017.2707479
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosu V, 2017, INT WORK QUAL MULTIM
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kastner S, 2000, ANNU REV NEUROSCI, V23, P315, DOI 10.1146/annurev.neuro.23.1.315
   Kim W, 2018, LECT NOTES COMPUT SC, V11205, P224, DOI 10.1007/978-3-030-01246-5_14
   Korhonen J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3311, DOI 10.1145/3394171.3413845
   Korhonen J, 2019, IEEE T IMAGE PROCESS, V28, P5923, DOI 10.1109/TIP.2019.2923051
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Li DQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2351, DOI 10.1145/3343031.3351028
   Li F, 2021, IEEE T CIRC SYST VID, V31, P4798, DOI 10.1109/TCSVT.2021.3055197
   Li F, 2018, IEEE T MULTIMEDIA, V20, P1154, DOI 10.1109/TMM.2017.2764329
   Li Y, 2023, IEEE T MULTIMEDIA, V25, P154, DOI 10.1109/TMM.2021.3122347
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Mittal A, 2016, IEEE T IMAGE PROCESS, V25, P289, DOI 10.1109/TIP.2015.2502725
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2009, IEEE J-STSP, V3, P193, DOI 10.1109/JSTSP.2009.2015374
   Nuutinen M, 2016, IEEE T IMAGE PROCESS, V25, P3073, DOI 10.1109/TIP.2016.2562513
   POSNER MI, 1990, ANNU REV NEUROSCI, V13, P25, DOI 10.1146/annurev.ne.13.030190.000325
   Seshadrinathan K, 2011, INT CONF ACOUST SPEE, P1153
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sinno Z, 2019, IEEE T IMAGE PROCESS, V28, P612, DOI 10.1109/TIP.2018.2869673
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tu Z, 2021, IEEE OPEN J SIGNAL P, V2, P425, DOI 10.1109/OJSP.2021.3090333
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P4449, DOI 10.1109/TIP.2021.3072221
   Tu ZZ, 2020, IEEE IMAGE PROC, P141, DOI 10.1109/ICIP40778.2020.9191169
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xu L, 2016, IEEE T MULTIMEDIA, V18, P590, DOI 10.1109/TMM.2016.2525004
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Yang XH, 2021, IEEE T MULTIMEDIA, V23, P4326, DOI 10.1109/TMM.2020.3040529
   Yang XH, 2020, NEUROCOMPUTING, V401, P209, DOI 10.1016/j.neucom.2020.03.072
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Yim JG, 2020, IEEE IMAGE PROC, P131, DOI [10.1109/ICIP40778.2020.9191194, 10.1109/icip40778.2020.9191194]
   Zhang HZ, 2020, IEEE T MULTIMEDIA, V22, P3210, DOI 10.1109/TMM.2020.2973828
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P1275, DOI 10.1109/TIP.2017.2651410
NR 55
TC 8
Z9 8
U1 5
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5206
EP 5221
DI 10.1109/TMM.2022.3189251
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300043
DA 2024-07-18
ER

PT J
AU Han, XF
   Jin, YF
   Cheng, HX
   Xiao, GQ
AF Han, Xian-Feng
   Jin, Yi-Fei
   Cheng, Hui-Xian
   Xiao, Guo-Qiang
TI Dual Transformer for Point Cloud Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Classification; point cloud; segmentation; self attention; transformer;
   visual affordance understanding
ID NETWORK
AB Feature representation learning is a key component in 3D point cloud analysis. However, the powerful convolutional neural networks (CNNs) cannot be applied due to the irregular structure of point clouds. Therefore, following the tremendous success of transformer in natural language processing and image understanding tasks, in this paper, we present a novel point cloud representation learning architecture, named Dual Transformer Network (DTNet), which mainly consists of Dual Point Cloud Transformer (DPCT) module. Specifically, by aggregating the well-designed point-wise and channel-wise self-attention models simultaneously, DPCT module can capture much richer contextual dependencies semantically from the perspective of position and channel. With the DPCT model as a fundamental component, we construct the DTNet for performing 3D point cloud analysis in an end-to-end manner. Extensive quantitative and qualitative experiments on publicly available benchmarks demonstrate the effectiveness of our transformer framework for the tasks of 3D point cloud classification, segmentation and visual object affordance understanding, achieving highly competitive performance in comparison with the state-of-the-art approaches.
C1 [Han, Xian-Feng; Jin, Yi-Fei; Cheng, Hui-Xian; Xiao, Guo-Qiang] South west Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
C3 Southwest University - China
RP Han, XF (corresponding author), South west Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
EM xianfenghan@swu.edu.cn; thehulk@email.swu.edu.cn;
   chenghuixian@email.swu.edu.cn; gqxiao@swu.edu.cn
RI Jin, Yifei/GYA-4900-2022
OI Han, Xianfeng/0000-0002-4869-4537
FU National Natural Science Foundation of China [62002299]; Natural Science
   Foundation of Chongqing, China [cstc2020jcyj-msxmX0126]; Fundamental
   Research Funds for the Central Universities [SWU120005]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62002299, in part by the Natural Science
   Foundation of Chongqing, China under Grant cstc2020jcyj-msxmX0126, and
   in part by the Fundamental Research Funds for the Central Universities
   under Grant SWU120005.
CR [Anonymous], 2015, P 19 C COMP NAT LANG
   [Anonymous], 2018, Proceedings of the 32nd International Conference on Neural Information Processing Systems
   [Anonymous], 2017, P INT C LEARN REPR
   Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Ben-Shabat Y, 2018, IEEE ROBOT AUTOM LET, V3, P3145, DOI 10.1109/LRA.2018.2850061
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chen M, 2020, PR MACH LEARN RES, V119
   Cheng SL, 2021, IEEE T IMAGE PROCESS, V30, P4436, DOI 10.1109/TIP.2021.3072214
   Deng SH, 2021, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR46437.2021.00182
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Duan YQ, 2019, PROC CVPR IEEE, P949, DOI 10.1109/CVPR.2019.00104
   Engel N, 2021, IEEE ACCESS, V9, P134826, DOI 10.1109/ACCESS.2021.3116304
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Fujiwara K., 2020, P IEEE CVF C COMP VI, P11734
   Gojcic Z, 2020, PROC CVPR IEEE, P1756, DOI 10.1109/CVPR42600.2020.00183
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Han WK, 2020, AAAI CONF ARTIF INTE, V34, P10925
   Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang H., 2020, IEEECVF C COMPUTER V, P12796
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Lei H, 2021, IEEE T PATTERN ANAL, V43, P3664, DOI 10.1109/TPAMI.2020.2983410
   Lei H, 2019, PROC CVPR IEEE, P9623, DOI 10.1109/CVPR.2019.00986
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Lin YQ, 2020, PROC CVPR IEEE, P4292, DOI 10.1109/CVPR42600.2020.00435
   Lin ZH, 2020, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR42600.2020.00187
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Liu Z, 2020, AAAI CONF ARTIF INTE, V34, P11677
   Luo Luqing, 2021, P IEEE CVF INT C COM, P16208
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Nezhadarya Ehsan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12953, DOI 10.1109/CVPR42600.2020.01297
   Parmar N, 2018, PR MACH LEARN RES, V80
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Qiu S, 2021, IEEE WINT CONF APPL, P3812, DOI 10.1109/WACV48630.2021.00386
   Rao YM, 2019, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2019.00054
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Wu W., 2019, PROC IEEECVF CVPR, P9621, DOI DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Xun Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13703, DOI 10.1109/CVPR42600.2020.01372
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
NR 67
TC 19
Z9 19
U1 11
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5638
EP 5648
DI 10.1109/TMM.2022.3198318
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300073
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hong, JH
   Zhang, W
   Feng, ZW
   Zhang, WQ
AF Hong, Jiahao
   Zhang, Wei
   Feng, Zhiwei
   Zhang, Wenqiang
TI Dual Cross-Attention for Video Object Segmentation via Uncertainty
   Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video object segmentation; space-time memory; attention mechanism;
   uncertainty refinement
AB In this paper, we propose a novel approach to video object segmentation where dual streams consisting of a shared network and a special network are designed to constitute the feature memory of history frames. Cues of spatial position and time stamp are explicitly explored to learn the context for each frame in the video sequence. Self-attention and cross-attention are simultaneously exploited to extract more powerful features for segmentation. In contrast to STM and its variants, the proposed dual cross-attention performs in both appearance space and semantic space such that the derived features are more distinctive and then robust to similar overlapping objects. During decoding for segmentation, a local refinement technique is designed for the uncertain boundaries to obtain more precise and smooth object contours. Experimental results on the challenging benchmark datasets DAVIS-2016, DAVIS-2017, and YouTube-VOS demonstrate the effectiveness of our proposed approach to video object segmentation.
C1 [Hong, Jiahao; Zhang, Wei; Feng, Zhiwei; Zhang, Wenqiang] Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Sch Comp Sci, Shanghai 200433, Peoples R China.
C3 Fudan University
RP Zhang, W (corresponding author), Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Sch Comp Sci, Shanghai 200433, Peoples R China.
EM jhhong20@fudan.edu.cn; weizh@fudan.edu.cn; zwfeng20@fudan.edu.cn;
   wqzhang@fudan.edu.cn
FU National Natural Science Foundation of China [62072112]; National Key
   R&D Program of China [2020AAA0108301]; Scientific and Technological
   Innovation Action Plan of Shanghai Science and Technology Committee
   [22511101502, 21DZ2203300]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62072112, in part by the National Key
   R&D Program of China under Grant 2020AAA0108301, and in part by
   Scientific and Technological Innovation Action Plan of Shanghai Science
   and Technology Committee under Grants 22511101502 and 21DZ2203300.
CR Bao LC, 2018, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2018.00626
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P777, DOI 10.1007/978-3-030-58536-5_46
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chen YD, 2019, IEEE T MULTIMEDIA, V21, P1934, DOI 10.1109/TMM.2018.2890361
   Cheng H.K., 2021, Advances in Neural Information Processing Systems, P11781
   Cheng HK, 2021, PROC CVPR IEEE, P5555, DOI 10.1109/CVPR46437.2021.00551
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Duke B, 2021, PROC CVPR IEEE, P5908, DOI 10.1109/CVPR46437.2021.00585
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ge WB, 2021, PROC CVPR IEEE, P16831, DOI 10.1109/CVPR46437.2021.01656
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hu L, 2021, PROC CVPR IEEE, P4142, DOI 10.1109/CVPR46437.2021.00413
   Hu YT, 2018, LECT NOTES COMPUT SC, V11212, P56, DOI 10.1007/978-3-030-01237-3_4
   Hu YT, 2017, ADV NEUR IN, V30
   Jampani V, 2017, PROC CVPR IEEE, P3154, DOI 10.1109/CVPR.2017.336
   Jang WD, 2017, PROC CVPR IEEE, P7474, DOI 10.1109/CVPR.2017.790
   Ji GP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4902, DOI 10.1109/ICCV48922.2021.00488
   Johnander J, 2019, PROC CVPR IEEE, P8945, DOI 10.1109/CVPR.2019.00916
   Kingma D. P., 2015, PROC INT C LEA REP
   Kirillov A., 2020, P IEEECVF C COMPUTER, P9799, DOI DOI 10.48550/ARXIV.1912.08193
   Kuo WC, 2019, IEEE I CONF COMP VIS, P9206, DOI 10.1109/ICCV.2019.00930
   Li G, 2021, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR46437.2021.00823
   Liang Y., 2020, Adv. Neural Inf. Proces. Syst., V33, P3430
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35
   Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670
   Mao YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9650, DOI 10.1109/ICCV48922.2021.00953
   Miao J., 2020, P IEEE CVF C COMP VI, P10366
   Miao JX, 2021, PROC CVPR IEEE, P4131, DOI 10.1109/CVPR46437.2021.00412
   Milan A., 2016, ARXIV160300831
   Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/iccv.2019.00932
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Peng QM, 2019, IEEE T MULTIMEDIA, V21, P3083, DOI 10.1109/TMM.2019.2918730
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Pont-Tuset J., The 2017 DAVIS challenge on video object segmentation
   Robinson Andreas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7404, DOI 10.1109/CVPR42600.2020.00743
   Seong H., 2020, EUR C COMP VIS, P629
   Seong H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12869, DOI 10.1109/ICCV48922.2021.01265
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Tokmakov P, 2017, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2017.480
   Vaswani A, 2017, ADV NEUR IN, V30
   Ventura C, 2019, PROC CVPR IEEE, P5272, DOI 10.1109/CVPR.2019.00542
   Voigtlaender P., 2019, BoLTVOS: Box-level tracking for video object segmentation
   Voigtlaender P., 2017, BMVC, P1000
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wang HC, 2021, PROC CVPR IEEE, P1296, DOI 10.1109/CVPR46437.2021.00135
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Wang Z, 2021, IEEE T MULTIMEDIA, V23, P4027, DOI 10.1109/TMM.2020.3037461
   Wang ZQ, 2019, IEEE I CONF COMP VIS, P3977, DOI 10.1109/ICCV.2019.00408
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xi Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9381, DOI 10.1109/CVPR42600.2020.00940
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P435, DOI 10.1007/978-3-030-58520-4_26
   Xiankai Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P661, DOI 10.1007/978-3-030-58580-8_39
   Xiao HX, 2018, PROC CVPR IEEE, P1140, DOI 10.1109/CVPR.2018.00125
   Xie HZ, 2021, PROC CVPR IEEE, P1286, DOI 10.1109/CVPR46437.2021.00134
   Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36
   Xu N, 2018, INT EL DEVICES MEET
   Xuhua Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8876, DOI 10.1109/CVPR42600.2020.00890
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Yang SS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8023, DOI 10.1109/ICCV48922.2021.00794
   Yang ZX, 2022, IEEE T PATTERN ANAL, V44, P4701, DOI 10.1109/TPAMI.2021.3081597
   Yoon JS, 2017, IEEE I CONF COMP VIS, P2186, DOI 10.1109/ICCV.2017.238
   Yu Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P735, DOI 10.1007/978-3-030-58607-2_43
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhang YZ, 2020, PROC CVPR IEEE, P6947, DOI 10.1109/CVPR42600.2020.00698
   Zhang ZY, 2016, PROC CVPR IEEE, P669, DOI 10.1109/CVPR.2016.79
   Zhao H., 2020, P IEEE CVF C COMP VI, V2020, P10076, DOI 10.1109/CVPR42600.2020.01009
   Zongxin Yang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P332, DOI 10.1007/978-3-030-58558-7_20
NR 76
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7710
EP 7725
DI 10.1109/TMM.2022.3225720
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400008
DA 2024-07-18
ER

PT J
AU Hou, XX
   Zhang, XK
   Li, YD
   Shen, LL
AF Hou, Xianxu
   Zhang, Xiaokang
   Li, Yudong
   Shen, Linlin
TI TextFace: Text-to-Style Mapping Based Face Generation and Manipulation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE GANs; text-to-image generation; cross modal; text-to-face generation;
   text-guided semantic face manipulation
AB As a subtopic of text-to-image synthesis, text-to-face generation has great potential in face-related applications. In this paper, we propose a generic text-to-face framework, namely, TextFace, to achieve diverse and high-quality face image generation from text descriptions. We introduce text-to-style mapping, a novel method where the text description can be directly encoded into the latent space of a pretrained StyleGAN. Guided by our text-image similarity matching and face captioning-based text alignment, the textual latent code can be fed into the generator of a well-trained StyleGAN to produce diverse face images with high resolution (1024x1024). Furthermore, our model inherently supports semantic face editing using text descriptions. Finally, experimental results quantitatively and qualitatively demonstrate the superior performance of our model.
C1 [Hou, Xianxu; Zhang, Xiaokang; Li, Yudong; Shen, Linlin] Shenzhen Univ, Comp Vis Inst, Sch Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Hou, Xianxu; Zhang, Xiaokang; Li, Yudong; Shen, Linlin] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518060, Peoples R China.
C3 Shenzhen University; Shenzhen Institute of Artificial Intelligence &
   Robotics for Society
RP Shen, LL (corresponding author), Shenzhen Univ, Comp Vis Inst, Sch Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM hxianxu@gmail.com; zhangxiaokang2019@email.szu.edu.cn;
   liyudong123@hotmail.com; llshen@szu.edu.cn
OI YUDONG, LI/0000-0001-6779-8836
FU National Natural Science Foundation of China [91959108]; Guangdong Basic
   and Applied Basic Research Foundation [2020A1515111199]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 91959108 and in part by the Guangdong
   Basic and Applied Basic Research Foundation under Grant
   2020A1515111199.& nbsp;
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447648
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Avrahami O, 2022, Arxiv, DOI arXiv:2111.14818
   Brock A., 2018, PROC INT C LEARN REP
   Cheng J., 2020, IEEE C COMP VIS PATT, P10911
   Creswell A, 2019, IEEE T NEUR NET LEAR, V30, P1967, DOI 10.1109/TNNLS.2018.2875194
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gou YC, 2020, Arxiv, DOI arXiv:2005.12444
   Guan SY, 2020, Arxiv, DOI arXiv:2007.01758
   Harkonen E., 2020, Advances in Neural Information Processing Systems, V33, P9841
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Hou XX, 2022, NEURAL NETWORKS, V145, P209, DOI 10.1016/j.neunet.2021.10.017
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Karnewar Animesh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7796, DOI 10.1109/CVPR42600.2020.00782
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B., 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P7880
   Li B., 2020, Advances in Neural Information Processing Systems, V33, P22020, DOI [10.48550/arxiv.2010.12136, DOI 10.48550/ARXIV.2010.12136]
   Li B, 2021, J NEUROL, V268, P2042, DOI 10.1007/s00415-019-09596-3
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Liu YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1357, DOI 10.1145/3394171.3413505
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Nam S, 2018, ADV NEUR IN, V31
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Perarnau G, 2016, Arxiv, DOI [arXiv:1611.06355, 10.48550/arXiv.1611.06355]
   Qiao TT, 2019, ADV NEUR IN, V32
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Radford A, 2021, PR MACH LEARN RES, V139
   Reed S, 2016, ADV NEUR IN, V29
   Reed S, 2016, PR MACH LEARN RES, V48
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Stap D., 2020, IEEECVF C COMPUT VIS
   Sun JX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2290, DOI 10.1145/3474085.3475391
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tao M, 2022, Arxiv, DOI arXiv:2008.05865
   Tewari A, 2020, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR42600.2020.00618
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang T., 2021, P IEEE CVF WINT C AP, P3380
   Wu YH, 2016, Arxiv, DOI [arXiv:1609.08144, DOI 10.48550/ARXIV.1609.08144]
   Xia WH, 2021, Arxiv, DOI arXiv:2104.08910
   Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhou YT, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2940, DOI 10.1145/3474085.3481026
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 68
TC 5
Z9 5
U1 3
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3409
EP 3419
DI 10.1109/TMM.2022.3160360
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200036
DA 2024-07-18
ER

PT J
AU Islam, MM
   Yasar, MS
   Iqbal, T
AF Islam, Md Mofijul
   Yasar, Mohammad Samin
   Iqbal, Tariq
TI MAVEN: A Memory Augmented Recurrent Approach for Multimodal Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Task analysis; Fuses; Sensors; Data
   mining; Noise measurement; Deep learning; human activity recognition;
   multimodal learning
ID REPRESENTATION
AB Multisensory systems provide complementary information that aids many machine learning approaches in perceiving the environment comprehensively. These systems consist of heterogeneous modalities, which have disparate characteristics and feature distributions. Thus, extracting, aligning, and fusing complementary representations from heterogeneous modalities (e.g., visual, skeleton, and physical sensors) remains challenging. To address these challenges, we have used the insights from several neuroscience studies of animal multisensory systems to develop MAVEN, a memory-augmented recurrent approach for multimodal fusion. MAVEN generates unimodal memory banks comprised of spatial-temporal features and uses our proposed recurrent representation alignment approach to align and refine unimodal representations iteratively. MAVEN then utilizes a multimodal variational attention-based fusion approach to produce a robust multimodal representation from the aligned unimodal features. Our extensive experimental evaluations on three multimodal datasets suggest that MAVEN outperforms state-of-the-art multimodal learning approaches in the challenging human activity recognition task across all evaluation conditions (cross-subject, leave-one-subject-out, and cross-session). Additionally, our extensive ablation studies suggest that MAVEN significantly outperforms the feed-forward fusion-based learning models (p < 0.05). Finally, the robust performance ofMAVEN in extracting complementary multimodal representation from occluded and noisy data suggests its applicability on real-world datasets.
C1 [Islam, Md Mofijul; Yasar, Mohammad Samin; Iqbal, Tariq] Univ Virginia, Sch Engn & Appl Sci, Charlottesville, VA 22903 USA.
C3 University of Virginia
RP Islam, MM (corresponding author), Univ Virginia, Sch Engn & Appl Sci, Charlottesville, VA 22903 USA.
EM mi8uu@virginia.edu; msy9an@virginia.edu; tiqbal@virginia.edu
OI Islam, Md. Mofijul/0000-0003-4207-5863; Yasar,
   Mohammad/0000-0002-4684-2823
CR BAHULEYAN H, 2017, VARIATIONAL ATTENTIO
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bulbul MF, 2015, INT J MULTIMED DATA, V6, P23, DOI 10.4018/IJMDEM.2015100102
   Chen C, 2015, IEEE IMAGE PROC, P168, DOI 10.1109/ICIP.2015.7350781
   Cho, 2017, ADV NEURAL INFORM PR, P5173
   Dean J., 2015, NIPS DEEP LEARNING R
   Deng Y, 2018, ADV NEURAL INFORM PR, V31, P9712
   Dror R, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2773
   Eslami SM Ali, 2016, NeurIPS, P3225
   Falcon William, 2019, PyTorch Lightning
   Fan LJ, 2018, PROC CVPR IEEE, P6016, DOI 10.1109/CVPR.2018.00630
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Frank AE, 2019, IEEE INT C INT ROBOT, P449, DOI [10.1109/IROS40897.2019.8968615, 10.1109/iros40897.2019.8968615]
   Garcia NC, 2018, LECT NOTES COMPUT SC, V11212, P106, DOI 10.1007/978-3-030-01237-3_7
   Gilbert CD, 2013, NAT REV NEUROSCI, V14, P350, DOI 10.1038/nrn3476
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   Goyal Anirudh, 2019, RECURRENT INDEPENDEN
   Guo W, 2019, IEEE ACCESS, V7, P121037, DOI 10.1109/ACCESS.2019.2936875
   Han K., 2018, P ADV NEUR INF PROC, P9201
   Hasan MK, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2046
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu D, 2019, INT CONF ACOUST SPEE, P3941, DOI 10.1109/ICASSP.2019.8683898
   Hu Ronghang, 2020, 2020 IEEE CVF C COMP, P9992, DOI DOI 10.1109/CVPR42600.2020.01001
   Imran J, 2016, 2016 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P144, DOI 10.1109/ICACCI.2016.7732038
   Islam MM, 2020, IEEE INT C INT ROBOT, P10285, DOI 10.1109/IROS45743.2020.9340987
   Islam MM, 2021, IEEE ROBOT AUTOM LET, V6, P1729, DOI 10.1109/LRA.2021.3059624
   ISLAM MM, 2022, P C ASS ADV ARTIF IN
   Issa EB, 2018, ELIFE, V7, DOI 10.7554/eLife.42870
   Kar K, 2019, NAT NEUROSCI, V22, P974, DOI 10.1038/s41593-019-0392-5
   Kazakos E, 2019, IEEE I CONF COMP VIS, P5491, DOI 10.1109/ICCV.2019.00559
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   KINGMA DP, 2013, P 2ND INT C LEARN RE
   Kong Q, 2019, IEEE I CONF COMP VIS, P8657, DOI 10.1109/ICCV.2019.00875
   Kubilius J., 2019, Adv Neural Inf Process Syst, V32, P12805, DOI 10.5555/3454287
   Kubota A, 2019, IEEE INT CONF ROBOT, P6533, DOI 10.1109/ICRA.2019.8793954
   Lamme VAF, 2000, TRENDS NEUROSCI, V23, P571, DOI 10.1016/S0166-2236(00)01657-X
   LI B, 2019, P AAAI C ARTIF INTEL, V33, P3288
   Li C., 2018, IJCAI
   Li CK, 2017, IEEE SIGNAL PROC LET, V24, P624, DOI 10.1109/LSP.2017.2678539
   LI LH, 2019, P ADV NEURAL INF PRO, P5265
   Liu GY, 2019, IEEE INT C INT ROBOT, P258, DOI [10.1109/IROS40897.2019.8967570, 10.1109/iros40897.2019.8967570]
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Liu TS, 2019, IEEE SENS J, V19, P1862, DOI 10.1109/JSEN.2018.2884443
   Locatello Francesco, 2020, NEURIPS
   Long X, 2018, AAAI CONF ARTIF INTE, P7202
   LOSHCHILOV I, 2016, P INT C LEARN REPRES
   LU J, 2019, ADV NEURAL INF PROCE
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Macaluso E, 2006, NEUROSCIENTIST, V12, P327, DOI 10.1177/1073858406287908
   MAKHZANI A, 2016, INT C LEARN REPRESEN
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   MEREDITH MA, 1992, EXP BRAIN RES, V88, P181, DOI 10.1007/BF02259139
   MEREDITH MA, 1986, J NEUROPHYSIOL, V56, P640, DOI 10.1152/jn.1986.56.3.640
   MNZNER S, 2017, P ACM INT S WEARABLE, P158
   Molchanov P, 2016, PROC CVPR IEEE, P4207, DOI 10.1109/CVPR.2016.456
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Ofli F, 2013, IEEE WORK APP COMP, P53, DOI 10.1109/WACV.2013.6474999
   Paszke A., 2019, ADV NEURAL INFORM PR, P8026, DOI DOI 10.48550/ARXIV.1912.01703
   Ramachandram D, 2017, IEEE SIGNAL PROC MAG, V34, P96, DOI 10.1109/MSP.2017.2738401
   Ramalingam N, 2013, J NEUROSCI, V33, P1773, DOI 10.1523/JNEUROSCI.3825-12.2013
   Ramaswamy J, 2020, IEEE WINT CONF APPL, P2959, DOI 10.1109/WACV45572.2020.9093616
   Sathian K, 2020, MULTISENSORY PERCEPTION: FROM LABORATORY TO CLINIC, P1
   SCHRIMPF M, 2017, BRAIN INSPIRED RECUR
   Schroeder CE, 2005, CURR OPIN NEUROBIOL, V15, P454, DOI 10.1016/j.conb.2005.06.008
   SPENCE C, 2018, MULTISENSORY PERCEPT, P1
   Tang HL, 2018, P NATL ACAD SCI USA, V115, P8835, DOI 10.1073/pnas.1719397115
   Vaezi Joze Hamid Reza, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13286, DOI 10.1109/CVPR42600.2020.01330
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Wallace MT, 1997, J NEUROSCI, V17, P2429
   Wang L., 2016, P 2016 INT C SUP ICS, P1
   Wang PC, 2017, IEEE INT CONF COMP V, P1005, DOI 10.1109/ICCVW.2017.123
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang XH, 2021, PROC CVPR IEEE, P5075, DOI 10.1109/CVPR46437.2021.00504
   Wu Y, 2021, PROC CVPR IEEE, P1326, DOI 10.1109/CVPR46437.2021.00138
   Wu Y, 2019, IEEE I CONF COMP VIS, P6301, DOI 10.1109/ICCV.2019.00639
   Xiao Fanyi, 2020, ARXIV200108740
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yonghong Hou, 2018, IEEE Transactions on Circuits and Systems for Video Technology, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
NR 80
TC 2
Z9 2
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3694
EP 3708
DI 10.1109/TMM.2022.3164261
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500013
DA 2024-07-18
ER

PT J
AU Li, L
   Fan, K
   Yuan, C
AF Li, Lei
   Fan, Kai
   Yuan, Chun
TI StrokeNet: Stroke Assisted and Hierarchical Graph Reasoning Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scene text detection; hierarchical representations; graph networks
ID SCENE TEXT DETECTION
AB Scene text detection is still a challenging task, as there may be extremely small or low-resolution strokes and close or arbitrary-shaped texts. In this paper, StrokeNet proposes to effectively detect the texts by capturing the fine-grained strokes and inferring structural relations between the hierarchical representations of each text area in the graph-based network. Different from existing approaches that represent the text area by a series of points or rectangular boxes, we directly localize the strokes of each text instance. We introduce Stroke Assisted Prediction Network (SAPN), which performs hierarchical representation learning of text areas, effectively capturing extremely small or low-resolution texts. We extract a series of text- and stroke-level rectangular boxes on the predicted text areas, which are treated as graph nodes and grouped to form the corresponding local graphs. Hierarchical Relation Graph Network (HRGN) then performs relational reasoning and predicts the likelihood of linkages among graph nodes of different levels. It efficiently splits the close text instances and grouping node classification results into the arbitrary-shaped text area. We introduce a novel dataset with stroke-level annotations, namely SynthStroke, for offline pre-training of widespread text detectors. Experiments on benchmarks verify the State-of-the-Art performance of our method.
C1 [Yuan, Chun] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Yuan, Chun] Peng Cheng Natl Lab, Shenzhen 518055, Peoples R China.
   [Li, Lei] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100083, Peoples R China.
   [Fan, Kai] Alibaba Grp US Inc, Sunnyvale, CA 94085 USA.
C3 Tsinghua University; Tsinghua Shenzhen International Graduate School;
   Tsinghua University
RP Yuan, C (corresponding author), Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
EM lei-li18@mails.tsinghua.edu.cn; interfk@gmail.com;
   yuanc@sz.tsinghua.edu.cn
OI Li, Lei/0000-0002-4514-3617
FU SZSTC [JCYJ20190809172201639]; Shenzhen Key Laboratory
   [ZDSYS20210623092001004]
FX This work was supported in part by SZSTC under Grants
   JCYJ20190809172201639 and WDZC20200820200655001 and in part by Shenzhen
   Key Laboratory under Grant ZDSYS20210623092001004.
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Bundy A., 1984, Catalogue of Artificial Intelligence Tools, P13, DOI DOI 10.1007/978-3-642-96868-6_25
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Cui CW, 2021, NEUROCOMPUTING, V464, P252, DOI 10.1016/j.neucom.2021.08.026
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Guan TK, 2022, IEEE T CIRC SYST VID, V32, P6073, DOI 10.1109/TCSVT.2022.3156390
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   He T, 2016, IEEE T IMAGE PROCESS, V25, P2529, DOI 10.1109/TIP.2016.2547588
   He WH, 2018, IEEE T IMAGE PROCESS, V27, P5406, DOI 10.1109/TIP.2018.2855399
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jianya Y. Y. G., 1999, J. Wuhan Tech. Univ. Surveying Mapping, V3, P1
   Jiayun Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11502, DOI 10.1109/CVPR42600.2020.01152
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   Kipf TN, 2016, ARXIV
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JC, 2019, Arxiv, DOI arXiv:1903.11800
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XQ, 2012, IEEE T MULTIMEDIA, V14, P482, DOI 10.1109/TMM.2011.2177646
   Liu YL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3052
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5
   Ma CX, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107684
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Nayef Nibal, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1582, DOI 10.1109/ICDAR.2019.00254
   Patel H. A., 2007, Int. J. Innov. Res. Comput. Commun. Eng., V3297, P1
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rothe R, 2015, LECT NOTES COMPUT SC, V9003, P290, DOI 10.1007/978-3-319-16865-4_19
   Shanyu Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P108, DOI 10.1007/978-3-030-58526-6_7
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shivakumara P, 2014, MULTIMED TOOLS APPL, V72, P515, DOI 10.1007/s11042-013-1385-0
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Tian SX, 2017, IEEE I CONF COMP VIS, P1501, DOI 10.1109/ICCV.2017.166
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2017, ARXIV171010903, DOI DOI 10.48550/ARXIV.1710.10903
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P1316, DOI 10.1109/TMM.2020.2995290
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1500, DOI 10.1145/3343031.3350929
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Xue ML, 2021, IEEE T MULTIMEDIA, V23, P2706, DOI 10.1109/TMM.2020.3015037
   Yang J, 2021, VISAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL. 5: VISAPP, P545, DOI 10.5220/0010298905450552
   Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515
   Ye J, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P516
   Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang S, 2021, IEEE T MULTIMEDIA, V23, P454, DOI 10.1109/TMM.2020.2978630
   Zhang SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1285, DOI 10.1109/ICCV48922.2021.00134
   Zhang Shi-Xue, 2020, IEEE C COMPUTER VISI, P9699
   Zhang ST, 2019, AAAI CONF ARTIF INTE, P801
   Zhong YH, 2022, KNOWL-BASED SYST, V236, DOI 10.1016/j.knosys.2021.107767
   Zhong ZY, 2019, INT J DOC ANAL RECOG, V22, P315, DOI 10.1007/s10032-019-00335-y
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314
   Zhu YX, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107336
NR 64
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5614
EP 5625
DI 10.1109/TMM.2022.3198017
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300071
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, YA
   Chen, HZ
   Miao, QG
   Ge, DH
   Liang, SY
   Ma, ZQ
   Zhao, BC
AF Li, Yunan
   Chen, Huizhou
   Miao, Qiguang
   Ge, Daohui
   Liang, Siyu
   Ma, Zhuoqi
   Zhao, Bocheng
TI Image Hazing and Dehazing: From the Viewpoint of Two-Way Image
   Translation With a Weakly Supervised Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Image restoration; Atmospheric modeling; Supervised
   learning; Meteorology; Image color analysis; Entropy; Attention
   mechanism; image hazing/dehazing; two-way image translation; weakly
   supervised learning
AB Image dehazing is an important task since it is the prerequisite for many downstream high-level computer vision tasks. Previous dehazing methods depend on either the hand-designed priors/assumptions or supervised learning with plenty of data, which are not easy to implement in practice. Meanwhile, synthesizing hazy images is also significant in many scenes like multi-weather image generation. In this paper, we change the viewpoint of this task to image translation and develop a weakly supervised framework to achieve it. Instead of simply considering the hazy image as the source domain and the haze-free image as the target domain for translation, we design a feature representation scheme that generates a domain indicator, and embed it into the decoder to achieve both hazing and dehazing within one network. This design significantly reduces the complexity of network and can be more easily extended to multi-domain translation tasks than the previous methods, which need one pair of generator-discriminator for each direction of the translation. Meanwhile, aiming at solving the haze-relevant task, we design a haze attention module, which takes the local entropy map as the input. Unlike the previous weakly supervised dehazing methods, our approach only requires unpaired hazy and haze-free images rather than any intermediate supervising data like the transmission map or atmospheric light defined in the atmospheric scattering model. Experimental results on synthetic datasets show our method can achieve competitive results when compared with the state-of-the-art methods and yield more appealing dehazing and hazing results on real-world images.
C1 [Li, Yunan; Chen, Huizhou; Miao, Qiguang; Ge, Daohui; Liang, Siyu; Ma, Zhuoqi; Zhao, Bocheng] Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
C3 Xidian University
RP Miao, QG (corresponding author), Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
EM yunanli@xidian.edu.cn; huizhouchen@stu.xidian.edu.cn;
   qgmiao@xidian.edu.cn; dhge@stu.xidian.edu.cn;
   syliang_233@stu.xidian.edu.cn; zhuoqi_ma@hotmail.com;
   zhaobo-cheng@xidian.edu.cn
OI Miao, Qiguang/0000-0002-2872-388X; Li, Yunan/0000-0001-7316-4354; Liang,
   Siyu/0000-0001-7232-7860; Ma, Zhuoqi/0000-0003-0729-9706
FU National Key R&D Program of China [2018YFC0807500]; National Natural
   Science Foundation of China [62002271, 61902296]; Province Key R&D
   Program of Shaanxi [2020LSFP3-15]; China Postdoctoral Science Foundation
   [2019M663640]; Guangxi Key Laboratory of Trusted Software [KX202061];
   Key R&D Projects of Qingdao Science and Technology Plan [21-1-2-18-xx];
   Fundamental Research Funds for the Central Universities [XJS210310];
   Academy of Integrated Circuit Innovation of Xidian University in Chong
   Qing Foundation [CQIRI-CXYHT-2021-06]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018YFC0807500, in part by the National Natural Science
   Foundation of China under Grants 62002271 and 61902296, in part by the
   Province Key R&D Program of Shaanxi under Grant 2020LSFP3-15, in part by
   the China Postdoctoral Science Foundation under Grant 2019M663640, in
   part by the Guangxi Key Laboratory of Trusted Software under Grant
   KX202061,in part by the Key R&D Projects of Qingdao Science and
   Technology Plan under Grant 21-1-2-18-xx, in part by the Fundamental
   Research Funds for the Central Universities under Grant XJS210310, in
   part by the Academy of Integrated Circuit Innovation of Xidian
   University in Chong Qing Foundation under Grant CQIRI-CXYHT-2021-06. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Prof.Erkut Erdem
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Galdran A, 2018, PROC CVPR IEEE, P8212, DOI 10.1109/CVPR.2018.00857
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Krull A, 2019, PROC CVPR IEEE, P2124, DOI 10.1109/CVPR.2019.00223
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li BY, 2020, IEEE T IMAGE PROCESS, V29, P8457, DOI 10.1109/TIP.2020.3016134
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Li XL, 2021, Arxiv, DOI arXiv:2103.05422
   Li XL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1318, DOI 10.1145/3123266.3123382
   Li YA, 2019, IEEE I CONF COMP VIS, P3275, DOI 10.1109/ICCV.2019.00337
   Li ZG, 2018, IEEE T IMAGE PROCESS, V27, P442, DOI 10.1109/TIP.2017.2750418
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu T., 2017, P INT C ADV NEUR INF, V30, P700, DOI DOI 10.48550/ARXIV.1703.00848
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan S. G., 2003, P IEEE INT C COMP VI, P1
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Park Taesung, 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Shen LH, 2019, IEEE T MULTIMEDIA, V21, P1093, DOI 10.1109/TMM.2018.2871955
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shin J, 2020, IEEE T MULTIMEDIA, V22, P30, DOI 10.1109/TMM.2019.2922127
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Sun S., 2018, Advances in Neural Information Processing Systems, P760
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Wan ZY, 2020, PROC CVPR IEEE, P2744, DOI 10.1109/CVPR42600.2020.00282
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang XY, 2023, IEEE T NEUR NET LEAR, V34, P1852, DOI 10.1109/TNNLS.2019.2962815
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 72
TC 2
Z9 2
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4704
EP 4717
DI 10.1109/TMM.2022.3181447
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300008
DA 2024-07-18
ER

PT J
AU Liu, WD
   Kong, XF
   Hung, TY
   Lin, GS
AF Liu, Weide
   Kong, Xiangfei
   Hung, Tzu-Yi
   Lin, Guosheng
TI Cross-Image Region Mining With Region Prototypical Network for Weakly
   Supervised Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Prototypes; Training; Semantics; Image segmentation; Task analysis;
   Robustness; Annotations; Cross-image; weakly-supervised; segmentation
ID NET
AB Weakly supervised image segmentation trained with image-level labels usually suffers from inaccurate coverage of object areas during the generation of the pseudo groundtruth. This is because the object activation maps are trained with the classification objective and lack the ability to generalize. To improve the generality of the object activation maps, we propose a region prototypical network (RPNet) to explore the cross-image object diversity of the training set. Similar object parts across images are identified via region feature comparison. Object confidence is propagated between regions to discover new object areas while background regions are suppressed. Experiments show that the proposed method generates more complete and accurate pseudo object masks while achieving state-of-the-art performance on PASCAL VOC 2012 and MS COCO. In addition, we investigate the robustness of the proposed method on reduced training sets. The code is available at https://github.com/liuweide01/RPNet-Weakly-Supervised-Segmentation.
C1 [Liu, Weide; Lin, Guosheng] Nanyang Technol Univ NTU, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Liu, Weide] ASTAR, Inst Infocomm Res I2R, Singapore 138632, Singapore.
   [Kong, Xiangfei] Ant Grp, Singapore 189554, Singapore.
   [Kong, Xiangfei] Guangdong Univ Technol, Sch Comp Sci, Guangzhou 510000, Peoples R China.
   [Hung, Tzu-Yi] Delta Res Ctr, Singapore 639798, Singapore.
C3 Nanyang Technological University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R);
   Guangdong University of Technology
RP Lin, GS (corresponding author), Nanyang Technol Univ NTU, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM weide001@e.ntu.edu.sg; xiangfei.kong@antgroup.com;
   tzuyi.hung@deltaww.com; gslin@ntu.edu.sg
OI LIU, WEIDE/0000-0002-9855-4479
FU Delta-NTU Corporate Laboratory; Delta Electronics Inc; National Research
   Foundation (NRF) Singapore [SMA-RP10]; National Research Foundation
   Singapore under its AI Singapore Programme [AISG-RP-2018-003]; MOETier-1
   research [RG28/18, RG22/19, RG95/20]; National Natural Science
   Foundation of China [61902077]
FX This work was supported in part by the Delta-NTU Corporate Laboratory
   with funding support from Delta Electronics Inc. and the National
   Research Foundation (NRF) Singapore under Grant SMA-RP10, in part by the
   National Research Foundation Singapore under its AI Singapore Programme
   under Award AISG-RP-2018-003, in part by the MOETier-1 research under
   Grants RG28/18 (S), RG22/19 (S), and RG95/20, and in part by the
   National Natural Science Foundation of China under Grant 61902077.
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Chaudhry A., 2017, PROC BRIT MACH VIS C
   Chen L., 2020, P 16 EUR C COMP VIS, P347
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding HH, 2020, IEEE T IMAGE PROCESS, V29, P3520, DOI 10.1109/TIP.2019.2962685
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762
   Fan RC, 2018, LECT NOTES COMPUT SC, V11213, P371, DOI 10.1007/978-3-030-01240-3_23
   Ge WF, 2018, PROC CVPR IEEE, P1277, DOI 10.1109/CVPR.2018.00139
   Geusebroek JM, 2003, IEEE T IMAGE PROCESS, V12, P938, DOI 10.1109/TIP.2003.812429
   Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   Hong S, 2017, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2017.239
   Hou QB, 2018, ADV NEUR IN, V31
   Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733
   Jiang PT, 2019, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2019.00216
   Jin B., 2018, PROC IEEE C COMPUT V, P1363
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kim D, 2017, IEEE I CONF COMP VIS, P3554, DOI 10.1109/ICCV.2017.382
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee J, 2019, IEEE I CONF COMP VIS, P6807, DOI 10.1109/ICCV.2019.00691
   Lee J, 2019, PROC CVPR IEEE, P5262, DOI 10.1109/CVPR.2019.00541
   Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W., 2016, PROC INT C LEARN REP
   Liu WD, 2022, Arxiv, DOI arXiv:2108.08518
   Liu WD, 2021, Arxiv, DOI arXiv:2108.05293
   Liu WD, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2085, DOI 10.1145/3394171.3413652
   Liu WD, 2021, IEEE T CIRC SYST VID, V31, P1607, DOI 10.1109/TCSVT.2020.3010293
   Liu WD, 2020, PROC CVPR IEEE, P4164, DOI 10.1109/CVPR42600.2020.00422
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535
   Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203
   Pathak D., 2015, PROC INT C LEARN REP
   Pathak D, 2015, IEEE I CONF COMP VIS, P1796, DOI 10.1109/ICCV.2015.209
   Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780
   Qi XJ, 2016, LECT NOTES COMPUT SC, V9912, P90, DOI 10.1007/978-3-319-46484-8_6
   Roy A, 2017, PROC CVPR IEEE, P7282, DOI 10.1109/CVPR.2017.770
   Saleh F, 2016, LECT NOTES COMPUT SC, V9912, P413, DOI 10.1007/978-3-319-46484-8_25
   Shen T, 2018, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2018.00148
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Shimoda W, 2019, IEEE I CONF COMP VIS, P5207, DOI 10.1109/ICCV.2019.00531
   Shimoda W, 2016, LECT NOTES COMPUT SC, V9908, P218, DOI 10.1007/978-3-319-46493-0_14
   Snell J, 2017, ADV NEUR IN, V30
   Tianyi Zhang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P663, DOI 10.1007/978-3-030-58542-6_40
   Tokmakov P, 2016, LECT NOTES COMPUT SC, V9908, P388, DOI 10.1007/978-3-319-46493-0_24
   Wan WT, 2019, IEEE I CONF COMP VIS, P3404, DOI 10.1109/ICCV.2019.00350
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang X, 2018, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2018.00147
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Yu F., 2015, ARXIV
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732
   Zhang D., 2020, P ADV NEUR INF PROC, P547
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang X, 2021, AUTOPHAGY, V17, P1519, DOI 10.1080/15548627.2020.1840796
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou SH, 2020, IEEE T IMAGE PROCESS, V29, P461, DOI 10.1109/TIP.2019.2919937
NR 67
TC 6
Z9 6
U1 3
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1148
EP 1160
DI 10.1109/TMM.2021.3139459
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, Z
   Wang, T
   Zhang, JR
   Zheng, F
   Jiang, WH
   Lu, K
AF Liu, Zhu
   Wang, Teng
   Zhang, Jinrui
   Zheng, Feng
   Jiang, Wenhao
   Lu, Ke
TI Show, Tell and Rephrase: Diverse Video Captioning via Two-Stage
   Progressive Training
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Training; Visualization; Task analysis; Computer science;
   Decoding; Electronic mail; Diverse video captioning; conditional
   variational autoencoders; progressive training
AB Describing a video using natural language is an inherently one-to-many translation task. To generate diverse captions, existing VAE-based generative models typically learn factorized latent codes via one-stage training merely from stand-alone video-caption pairs. However, such a paradigm neglects set-level relationships among captions from the same video, not fully capturing the underlying multimodality of the generative process. To overcome this shortcoming, we leverage neighbouring descriptions for the same video that are articulated with noticeable topics and language variations (i.e., paraphrases). To this end, we propose a novel progressive training method by decomposing the learning of latent variables into two stages that are topic-oriented and paraphrase-oriented, respectively. Specifically, the model learns from divergent topic sentences obtained by semantic-based clustering in the first stage. It is then trained again through paraphrases with a cluster-aware adaptive regularization, allowing more intra-cluster variations. Furthermore, we introduce an overall metric DAUM, a Diversity-Accuracy Unified Metric to consider both the precision of the generated caption set and its coverage on the reference set, which has proved to have a higher correlation with human judgment than previous precision-only metrics. Extensive experiments on three large-scale video datasets show that the proposed training strategy can achieve superior performance in terms of accuracy, diversity, and DAUM over several baselines.
C1 [Liu, Zhu; Wang, Teng; Zhang, Jinrui; Zheng, Feng] Southern Univ Sci & Technol, Dept Comp Sci & Engn, Shenzhen 518055, Peoples R China.
   [Liu, Zhu] Tsinghua Univ, Dept Humanities, Beijing 100190, Peoples R China.
   [Wang, Teng] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Zheng, Feng] Southern Univ Sci & Technol, Res Inst Trustworthy Autonomous Syst, Shenzhen 518055, Peoples R China.
   [Jiang, Wenhao] Tencent, Data Platform, Shenzhen 518001, Peoples R China.
   [Lu, Ke] Univ Chinese Acad Sci, Sch Engn Sci, Beijing 100049, Peoples R China.
   [Lu, Ke] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Southern University of Science & Technology; Tsinghua University;
   University of Hong Kong; Southern University of Science & Technology;
   Tencent; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Peng Cheng Laboratory
RP Zheng, F (corresponding author), Southern Univ Sci & Technol, Dept Comp Sci & Engn, Shenzhen 518055, Peoples R China.; Zheng, F (corresponding author), Southern Univ Sci & Technol, Res Inst Trustworthy Autonomous Syst, Shenzhen 518055, Peoples R China.
EM liuz2019@mail.sustech.edu.cn; tengwang@connect.hku.hk;
   zhangjr2018@mail.sustech.edu.cn; f.zheng@ieee.org; cswhjiang@gmail.com;
   luk@ucas.ac.cn
RI Wang, Teng/GNH-1848-2022; Jiang, Wenhao/M-8932-2015; Zheng,
   Feng/AAH-5643-2019
OI Zheng, Feng/0000-0002-1701-9141; Wang, Teng/0000-0003-2331-3619
FU National Key R&D Program of China [2022YFF1202903]; National Natural
   Science Foundation of China [61972188, 62122035]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2022YFF1202903) and in part by the National Natural Science
   Foundation of China under Grants 61972188 and 62122035.
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Aafaq N, 2020, ACM COMPUT SURV, V52, DOI 10.1145/3355390
   Aneja J, 2019, IEEE I CONF COMP VIS, P4260, DOI 10.1109/ICCV.2019.00436
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Bowman S. R., 2015, GENERATING SENTENCES
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen FH, 2019, ADV NEUR IN, V32
   Chen JW, 2019, AAAI CONF ARTIF INTE, P8167
   Chen L, 2021, PROC CVPR IEEE, P16841, DOI 10.1109/CVPR46437.2021.01657
   Chen SZ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1838, DOI 10.1145/3123266.3123420
   Deb T, 2022, IEEE WINT CONF APPL, P2493, DOI 10.1109/WACV51458.2022.00255
   Deshpande A, 2019, PROC CVPR IEEE, P10687, DOI 10.1109/CVPR.2019.01095
   Fang H., 2021, arXiv
   Gurari Danna, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P417, DOI 10.1007/978-3-030-58520-4_25
   Iashin V., 2020, Proc. British Mach. Vis. Conf
   Jin T, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P630
   Kalyan A., 2018, INT C MACH LEARN, V80, P2454
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Lin K, 2022, PROC CVPR IEEE, P17928, DOI 10.1109/CVPR52688.2022.01742
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu FL, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P281
   Liu LX, 2019, IEEE I CONF COMP VIS, P4239, DOI 10.1109/ICCV.2019.00434
   Liu S, 2021, IEEE T PATTERN ANAL, V43, P3259, DOI 10.1109/TPAMI.2019.2940007
   Lucas J., 2019, P INT C LEARN REPR, P1
   Mahajan S., 2020, P INT C LEARN REPR, P1
   Mahajan S., 2020, P ADV NEUR INF PROC, V33, P3613
   Mun J, 2019, PROC CVPR IEEE, P3581, DOI 10.1109/CVPR.2019.00675
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Seo PH, 2022, PROC CVPR IEEE, P17938, DOI 10.1109/CVPR52688.2022.01743
   Sohn K, 2015, ADV NEUR IN, V28
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Song JK, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2737
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan GC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P745
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang JW, 2018, PROC CVPR IEEE, P7190, DOI 10.1109/CVPR.2018.00751
   Wang LW, 2017, ADV NEUR IN, V30
   Wang QZ, 2019, PROC CVPR IEEE, P4190, DOI 10.1109/CVPR.2019.00432
   Wang T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6827, DOI 10.1109/ICCV48922.2021.00677
   Wang X, 2019, IEEE I CONF COMP VIS, P4580, DOI 10.1109/ICCV.2019.00468
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Ye HH, 2022, PROC CVPR IEEE, P17918, DOI 10.1109/CVPR52688.2022.01741
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
   Zheng Q., 2020, P IEEE CVF C COMP VI, P13096
NR 58
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7894
EP 7905
DI 10.1109/TMM.2022.3232022
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400022
DA 2024-07-18
ER

PT J
AU Nie, J
   Wang, CL
   Yu, SS
   Shi, JJ
   Lv, XW
   Wei, ZQ
AF Nie, Jie
   Wang, Chenglong
   Yu, Shusong
   Shi, Jinjin
   Lv, Xiaowei
   Wei, Zhiqiang
TI MIGN: Multiscale Image Generation Network for Remote Sensing Image
   Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic segmentation; remote sensing; multiscale; multi-attention; edge
   supervised; image fusion
ID CONVOLUTIONAL NETWORKS; CLASSIFICATION
AB With the development of computer vision, the semantic segmentation of remote sensing images, which has become an important topic, has been utilized in various applications for image content analysis and understanding, such as urban planning, natural disaster monitoring, and land resource management. Many approaches have been proposed to address these problems. However, due to obvious differences in resolution, spatial structure, and semantics between remote sensing images and ordinary images, the semantic segmentation of remote sensing images is still challenging. In this paper, we propose a novel multiscale image generation network (MIGN) that can efficiently generate high-resolution segmentation results by considering both details and boundary information. In particular, a multi-attention mechanism method for semantic segmentation of remote sensing images is designed. The attention weight is calculated by capturing the interaction of cross dimensions in a two-branch structure, which can learn the underlying feature information and guarantee the performance of each pixel feature for final classification. We also propose an edge supervised module to ensure that the segmentation boundary has a more accurate performance. A multiscale image fusion algorithm based on the Bayes model is proposed to improve the accuracy of the segmentation module. The performance of our model is evaluated on the ISPRS Vaihingen and Potsdam datasets. The results show that our method is superior to the most advanced image segmentation methods in terms of MIoU and pixel accuracy.
C1 [Nie, Jie; Wang, Chenglong; Yu, Shusong; Shi, Jinjin; Lv, Xiaowei; Wei, Zhiqiang] Ocean Univ China, Fac Informat Sci & Engn, Qingdao 266000, Peoples R China.
C3 Ocean University of China
RP Yu, SS (corresponding author), Ocean Univ China, Fac Informat Sci & Engn, Qingdao 266000, Peoples R China.
EM niejie@ouc.edu.cn; wangchenglong@stu.ouc.edu.cn; yushusong@ouc.edu.cn;
   shijinjin@stu.ouc.edu.cn; lvxiaowei@stu.ouc.edu.cn;
   weizhiqiang@ouc.edu.cn
RI Nie, Jie/ABG-9228-2021; wang, chenglong/GQQ-2852-2022; wei,
   zhiqiang/M-8868-2013
OI Nie, Jie/0000-0003-4952-7666; wang, chenglong/0000-0002-3567-5759; Yu,
   Shusong/0000-0003-1554-5313; Wang, Chenglong/0000-0003-2262-4167
FU Fundamental Research Funds for the Central Universities [202042008];
   National Natural Science Foundation of China [62172376, 62072418]; Major
   Scientific and Technological Innovation Project of Shandong
   [2019JZZY020705]; Key Research and Development Program of Qingdao
   Science and Technology Plan [21-1-2-18-xx]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant 202042008, in part by the National
   Natural Science Foundation of China under Grants 62172376 and 62072418,
   in part bythe Major Scientific and Technological Innovation Project of
   Shandong under Grant 2019JZZY020705, and in part by the Key Research and
   Development Program of Qingdao Science and Technology Plan under Grant
   21-1-2-18-xx.
CR Ahmed OS, 2017, INT J REMOTE SENS, V38, P2037, DOI 10.1080/01431161.2017.1294781
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bian X, 2016, IEEE WINT CONF APPL
   Blaschke T, 2010, ISPRS J PHOTOGRAMM, V65, P2, DOI 10.1016/j.isprsjprs.2009.06.004
   Chen L., 2015, 2015 IEEE CUSTOM INT, P1, DOI [10.1021/la504333j, DOI 10.1021/LA504333J]
   Chen L.Z, 2017, CORR ABS170605587
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YL, 2019, FRONT GENET, V10, DOI 10.3389/fgene.2019.01110
   Cui W, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11091044
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Haoxiang M., 2021, 32 BRIT MACH VIS C 2, P340
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Längkvist M, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8040329
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li H., 2018, P BRIT MACH VIS C
   Li HF, 2021, IEEE GEOSCI REMOTE S, V18, P905, DOI 10.1109/LGRS.2020.2988294
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10091339
   Liu Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103232
   Liu YC, 2018, ISPRS J PHOTOGRAMM, V145, P78, DOI 10.1016/j.isprsjprs.2017.12.007
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo HF, 2019, IEEE J-STARS, V12, P3492, DOI 10.1109/JSTARS.2019.2930724
   Marmanis D, 2018, ISPRS J PHOTOGRAMM, V135, P158, DOI 10.1016/j.isprsjprs.2017.11.009
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Nie WZ, 2021, IEEE T IMAGE PROCESS, V30, P4371, DOI 10.1109/TIP.2021.3071687
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P3793, DOI 10.1109/TMM.2020.3032037
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Paisitkriangkrai Sakrapee, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P36, DOI 10.1109/CVPRW.2015.7301381
   Peng C, 2019, IEEE J-STARS, V12, P2612, DOI 10.1109/JSTARS.2019.2906387
   Raj A., 2015, Tech. Rep. CMU-RITR-15-21
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith L.I., 2002, TUTORIAL PRINCIPAL C
   Song L., 2009, P INT WORKSH INT SYS, P1
   Sun Ke, 2019, CoRR abs/1904.04514
   Sun S., 2018, P 10 IAPR WORKSH PAT, P1
   Vakalopoulou M, 2015, INT GEOSCI REMOTE SE, P1873, DOI 10.1109/IGARSS.2015.7326158
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang WX, 2016, J TRAFFIC TRANSP ENG, V3, P271, DOI 10.1016/j.jtte.2016.05.005
   Yu F., 2015, ARXIV
   Zhang C, 2018, IEEE T GEOSCI REMOTE, V56, P4507, DOI 10.1109/TGRS.2018.2822783
   Zhang F, 2019, IEEE I CONF COMP VIS, P6797, DOI 10.1109/ICCV.2019.00690
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944
   Zhang ZL, 2018, LECT NOTES COMPUT SC, V11214, P273, DOI 10.1007/978-3-030-01249-6_17
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3085889
NR 56
TC 3
Z9 3
U1 7
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5601
EP 5613
DI 10.1109/TMM.2022.3197369
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300070
DA 2024-07-18
ER

PT J
AU Qin, C
   Hu, JC
   Li, FY
   Qian, ZX
   Zhang, XP
AF Qin, Chuan
   Hu, Jinchuan
   Li, Fengyong
   Qian, Zhenxing
   Zhang, Xinpeng
TI JPEG Image Encryption With Adaptive DC Coefficient Prediction and RS
   Pair Permutation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE DC coefficient prediction; Histogram encryption; Image encryption; JPEG
   image; RS pair permutation
ID COMPRESSION; SCHEME
AB JPEG image encryption aims at effectively converting the original JPEG image into a noise-like image that does not contain any useful information of original image. Existing schemes for JPEG image encryption, however, may not attain a good balance in terms of file size increment and encryption security. To address the problem, we design a novel JPEG image encryption scheme. Different from existing schemes, we first predict DC coefficients by an adaptive prediction method. Subsequently, the histogram of DC coefficient prediction errors is encrypted by combining the prediction errors and random integers to reduce the encoded length, which can ensure a very small increment of file size. Furthermore, we construct the RS (run/size) pairs in each DCT block and then implement the permutation for both RS pairs extracted from the upper left corner of each DCT block and all DCT blocks excluding DC coefficients, which can further distort the image contents. Extensive experiments demonstrate that, compared with existing JPEG image encryption schemes, our scheme can ensure not only the JPEG format compatibility for encrypted image, but also keep a very small file size increment and the superior security performance.
C1 [Qin, Chuan; Hu, Jinchuan] Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
   [Li, Fengyong] Shanghai Univ Elect Power, Coll Comp Sci & Technol, Shanghai 200090, Peoples R China.
   [Qian, Zhenxing; Zhang, Xinpeng] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
C3 University of Shanghai for Science & Technology; Shanghai University of
   Electric Power; Fudan University
RP Qin, C (corresponding author), Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.; Li, FY (corresponding author), Shanghai Univ Elect Power, Coll Comp Sci & Technol, Shanghai 200090, Peoples R China.
EM qin@usst.edu.cn; fredric.hu@outlook.com; fyli@shiep.edu.cn;
   zxqian@fudan.edu.cn; zhangxinpeng@fudan.edu.cn
RI Qin, Chuan/C-1106-2017; Qian, Zhenxing/AHC-9176-2022
OI Qin, Chuan/0000-0002-0370-4623; Li, Fengyong/0000-0002-3385-8164
FU National Natural Science Foundation of China [62172280, U20B2051];
   Natural Science Foundation of Shanghai [21ZR1444600, 20ZR1421600]; STCSM
   Capability Construction Project for Shanghai Municipal Universities
   [20060502300]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172280 and U20B2051, in part by the
   Natural Science Foundation of Shanghai under Grants 21ZR1444600 and
   20ZR1421600,and in part by the STCSM Capability Construction Project for
   Shanghai Municipal Universities under Grant 20060502300.& nbsp;
CR Auer S, 2013, INT J DIGIT CRIME FO, V5, P1, DOI 10.4018/jdcf.2013070101
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Chen GR, 2004, CHAOS SOLITON FRACT, V21, P749, DOI 10.1016/j.chaos.2003.12.022
   Cheng H, 2016, J VIS COMMUN IMAGE R, V40, P111, DOI 10.1016/j.jvcir.2016.06.016
   Dufaux F, 2008, IEEE IMAGE PROC, P1688, DOI 10.1109/ICIP.2008.4712098
   He JH, 2019, IEEE T CIRC SYST VID, V29, P3501, DOI 10.1109/TCSVT.2018.2882850
   He JH, 2018, IEEE T MULTIMEDIA, V20, P2645, DOI 10.1109/TMM.2018.2817065
   He K., 2016, ECCV, P1
   Hua ZY, 2018, SIGNAL PROCESS, V149, P148, DOI 10.1016/j.sigpro.2018.03.010
   Kishore B, 2015, J REAL-TIME IMAGE PR, V10, P551, DOI 10.1007/s11554-012-0282-5
   Li PY, 2019, J VIS COMMUN IMAGE R, V58, P12, DOI 10.1016/j.jvcir.2018.11.018
   Li PY, 2018, IEEE T MULTIMEDIA, V20, P1960, DOI 10.1109/TMM.2017.2786860
   Li PY, 2017, J VIS COMMUN IMAGE R, V44, P61, DOI 10.1016/j.jvcir.2017.01.021
   Li SS, 2016, KSII T INTERNET INF, V10, P1790, DOI 10.3837/tiis.2016.04.018
   Li SJ, 2011, IEEE IMAGE PROC, P1537, DOI 10.1109/ICIP.2011.6115738
   Li WH, 2007, INT J COMPUT MATH, V84, P1367, DOI 10.1080/00207160701294376
   Lian SG, 2004, IEEE INFOR VIS, P217, DOI 10.1109/IV.2004.1320147
   Lian SG, 2007, IEEE T CIRC SYST VID, V17, P774, DOI 10.1109/TCSVT.2007.896635
   Minemura K, 2012, IEEE IMAGE PROC, P261, DOI 10.1109/ICIP.2012.6466845
   Mitchell J., 1992, ITU-Rec. T. 81
   Ong S, 2013, IEEE IMAGE PROC, P4574, DOI 10.1109/ICIP.2013.6738942
   Ong SY, 2015, SIGNAL PROCESS-IMAGE, V31, P47, DOI 10.1016/j.image.2014.11.008
   Puteaux P, 2021, EUR SIGNAL PR CONF, P725, DOI 10.23919/Eusipco47968.2020.9287376
   Qian ZX, 2018, IEEE T DEPEND SECURE, V15, P1055, DOI 10.1109/TDSC.2016.2634161
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Qin C, 2019, INFORM SCIENCES, V487, P176, DOI 10.1016/j.ins.2019.03.008
   Shreyamsha Kumar BK, 2010, SIGNAL IMAGE VIDEO P, V4, P419, DOI 10.1007/s11760-009-0131-6
   Stinson D. R., 2018, Cryptography Theory and Practice
   Takayama M, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1349, DOI 10.1109/ICME.2006.262789
   Tang L., 1997, P 4 ACM INT C MULT, P219
   Ting J, 2019, IEEE IMAGE PROC, P4559, DOI [10.1109/ICIP.2019.8803623, 10.1109/icip.2019.8803623]
   Uehara T, 2006, IEEE T IMAGE PROCESS, V15, P3592, DOI 10.1109/TIP.2006.881939
   Weber A. G., 1997, USC-SIPI Report, V315
   Yi S, 2017, SIGNAL PROCESS, V133, P40, DOI 10.1016/j.sigpro.2016.10.017
   Zeng WJ, 2003, IEEE T MULTIMEDIA, V5, P118, DOI 10.1109/TMM.2003.808817
   Zhang Y, 2020, INFORM SCIENCES, V526, P180, DOI 10.1016/j.ins.2020.03.054
NR 36
TC 13
Z9 13
U1 13
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2528
EP 2542
DI 10.1109/TMM.2022.3148591
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600006
DA 2024-07-18
ER

PT J
AU Tania, S
   Karmakar, G
   Teng, SW
   Murshed, M
AF Tania, Sheikh
   Karmakar, Gour
   Teng, Shyh Wei
   Murshed, Manzur
TI A Robust Local Texture Descriptor in the Parametric Space of the Weibull
   Distribution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Local texture descriptor; the Weibull distribution; the Sobel operator;
   image segmentation
ID BINARY PATTERNS; FEATURES; CLASSIFICATION; SCALE
AB Research in texture feature approximation is still in the embryonic stage because of difficulties in developing a sound theoretical model to express the unique pattern in the intensity-variation of pixels in the neighbourhood of the pixel-of-interest so that it can sufficiently discriminate different textures. Local texture descriptors are widely used in image segmentation as they comprise pixel-wise features. The Weber local descriptor (WLD) with differential excitation and gradient orientation components, inspired by Weber's Law, has been leveraged in the state-of-the-art iterative contraction and merging (ICM) image segmentation technique. However, WLD has inherent drawbacks in the formulation of the components that limit its discriminatory capability. This paper introduces a novel texture descriptor by directly modelling the distribution of intensity-variation in the parametric space of the Weibull distribution using its shape and scale parameters. A unified 'joint scale' texture property is introduced, which can discriminate textures better than the individual parameters while keeping the length of the descriptor shorter. Additionally, the accuracy of WLD's gradient orientation component is improved by using an extended Sobel operator and expressing gradients in [-pi/2, pi/2) range. When incorporated in ICM, the proposed texture descriptor has consistently outperformed WLD and a recent enhancement with radial mean WLD (RM-WLD) on three benchmark datasets. It has also outperformed two other texture segmentation techniques and their deep learning based improvements.
C1 [Tania, Sheikh; Karmakar, Gour; Teng, Shyh Wei; Murshed, Manzur] Federat Univ Australia, Ctr Smart Analyt, Churchill, Vic 3842, Australia.
C3 Federation University Australia
RP Tania, S (corresponding author), Federat Univ Australia, Ctr Smart Analyt, Churchill, Vic 3842, Australia.
EM sheikhtania@students.federation.edu.au; gour.karmakar@federation.edu.au;
   shyh.wei.teng@federation.edu.au; manzur.murshed@federation.edu.au
OI Teng, Shyh Wei/0000-0003-0347-3797; Karmakar, Gour/0000-0002-1308-7315;
   Murshed, Manzur/0000-0001-7079-9717
CR Abdelmounaime Safia, 2013, ISRN Machine Vision, DOI 10.1155/2013/876386
   Andrearczyk V, 2017, Arxiv, DOI arXiv:1703.05230
   Andrearczyk V, 2016, PATTERN RECOGN LETT, V84, P63, DOI 10.1016/j.patrec.2016.08.016
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Brodatz P., 1999, Textures: a photographic album for artists and designers
   Cameron A., 2005, MICROECONOMETRICS ME, DOI [10.1017/CBO9780511811241, DOI 10.1017/CBO9780511811241]
   Chen J, 2013, IEEE T IMAGE PROCESS, V22, P326, DOI 10.1109/TIP.2012.2210234
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Chiu LC, 2013, IEEE T IMAGE PROCESS, V22, P3158, DOI 10.1109/TIP.2013.2259841
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   COHEN AC, 1965, TECHNOMETRICS, V7, P579, DOI 10.2307/1266397
   Creswell Kasey G, 2015, Nicotine Tob Res, V17, P566, DOI 10.1093/ntr/ntu192
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Galloway MM., 1975, COMPUTER GRAPHICS IM, V4, P172, DOI DOI 10.1016/S0146-664X(75)80008-6
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Haindl M., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1792, DOI 10.1109/ICPR.2010.442
   Haindl M, 2008, INT C PATT RECOG, P2933
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hossain MT, 2012, 2012 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING TECHNIQUES AND APPLICATIONS (DICTA)
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0
   Kekre H B., 2010, International Journal on Computer Science and Engineering (IJCSE), V2, P1086
   KELLER JM, 1989, COMPUT VISION GRAPH, V45, P150, DOI 10.1016/0734-189X(89)90130-8
   Lateef R.A.R., 2008, Baghdad College Econ. Sci. Univ., V1, P336
   Laws KI., 1980, THESIS U SO CALIFORN
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638
   Liao S, 2007, LECT NOTES COMPUT SC, V4844, P672
   Liu L, 2014, IEEE T IMAGE PROCESS, V23, P3071, DOI 10.1109/TIP.2014.2325777
   Liu L, 2016, IEEE T IMAGE PROCESS, V25, P1368, DOI 10.1109/TIP.2016.2522378
   Liu L, 2012, IMAGE VISION COMPUT, V30, P86, DOI 10.1016/j.imavis.2012.01.001
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Meila M., 2005, P 22 INT C MACH LEAR, P577, DOI DOI 10.1145/1102351.1102424
   Mevenkamp N, 2016, IEEE WINT CONF APPL
   Nanni L, 2010, ARTIF INTELL MED, V49, P117, DOI 10.1016/j.artmed.2010.02.006
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Papoulis A., 1994, Probability, Random Variables and Stochastic Processes, V3rd
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Soille P., 2003, Morphological image analysis: principles and applications, Vsecond
   Syu JH, 2017, IEEE T IMAGE PROCESS, V26, P2246, DOI 10.1109/TIP.2017.2651395
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tania S, 2020, IEEE IMAGE PROC, P1526, DOI 10.1109/ICIP40778.2020.9190895
   Tomita F., 1990, Computer Analysis of Visual Textures, V66, P13
   Unnikrishnan R, 2007, IEEE T PATTERN ANAL, V29, P929, DOI 10.1109/TPAMI.2007.1046
   Wolf L, 2011, IEEE T PATTERN ANAL, V33, P1978, DOI 10.1109/TPAMI.2010.230
   Yuan JY, 2015, IEEE T IMAGE PROCESS, V24, P3488, DOI 10.1109/TIP.2015.2446948
   Zhang H, 2017, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR.2017.309
   Zhao WL, 2013, IEEE T IMAGE PROCESS, V22, P980, DOI 10.1109/TIP.2012.2226043
NR 54
TC 0
Z9 0
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6053
EP 6066
DI 10.1109/TMM.2022.3204220
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500030
DA 2024-07-18
ER

PT J
AU Wang, JX
   Li, CL
   Zheng, AH
   Tang, J
   Luo, B
AF Wang, Jiaxiang
   Li, Chenglong
   Zheng, Aihua
   Tang, Jin
   Luo, Bin
TI Looking and Hearing Into Details: Dual-Enhanced Siamese Adversarial
   Network for Audio-Visual Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Task analysis; Generative adversarial networks;
   Synthetic aperture sonar; Representation learning; Visualization;
   Optimization; Adaptive masked cross-entropy; audio-visual cross-modal
   matching; dual enhancement mechanism; siamese adversarial network
ID FACE; IDENTITY; VOICE
AB Audio-visual cross-modal matching aims to explore the intrinsic correspondence between face images and audio clips. Existing methods usually focus on the salient features of identities between visual images and voice clips, while neglecting their subtle differences, which are crucial to distinguishing cross-modal samples. To deal with this problem, we propose a novel Dual-enhanced Siamese Adversarial Network (DSANet), which pursues the adversarial dual enhancement to highlight both salient and subtle features for robust audio-visual cross-modal matching. First, we designed a dual enhancement mechanism to enhance potential subtle features by randomly selecting a region feature for salient feature suppression, while enhancing salient features in the corresponding region to ensure the global discriminative ability. Second, to establish the correlation of subtle features in the process of eliminating cross-modal heterogeneity, we design a siamese adversarial structure to perform modal heterogeneity elimination for both enhanced salient and subtle features in a parallel manner. Moreover, we propose an adaptive masked cross-entropy loss to force the network to focus on the feature differences among hard classes. Experiments on public benchmark datasets validate the effectiveness of the proposed algorithm.
C1 [Wang, Jiaxiang; Tang, Jin; Luo, Bin] Anhui Univ, Sch Comp Sci & Technol, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.
   [Li, Chenglong; Zheng, Aihua] Anhui Univ, Sch Artificial Intelligence, Informat Mat & Intelligent Sensing Lab Anhui Prov, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.
C3 Anhui University; Anhui University
RP Zheng, AH (corresponding author), Anhui Univ, Sch Artificial Intelligence, Informat Mat & Intelligent Sensing Lab Anhui Prov, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.
EM netizenwjx@foxmail.com; lcl1314@foxmail.com; ahzheng214@foxmail.com;
   ahu_tj@163.com; ahu_lb@163.com
RI Wang, Jiaxiang/JPK-2167-2023; Li, Chenglong/AAH-4234-2019; Wang,
   Jiaxiang/JGM-8866-2023; lu, bin/HPE-4790-2023
OI Wang, Jiaxiang/0000-0003-3059-798X; 
FU National Natural Science Foundation of China [61976002, 62102344];
   University Synergy Innovation Program of Anhui Province [GXXT-2022-036];
   Natural Science Foundation of Anhui Higher Education Institutions of
   China [KJ2021A0901]
FX This work was supported in part by the National Natural Science
   Foundation of China, under Grants 61976002 and Grants 62102344, in part
   by the University Synergy Innovation Program of Anhui Province under
   Grants GXXT-2022-036, and in part by the Natural Science Foundation of
   Anhui Higher Education Institutions of China under Grants KJ2021A0901
CR Ayinde BO, 2019, IEEE T NEUR NET LEAR, V30, P2650, DOI 10.1109/TNNLS.2018.2885972
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   Chen XS, 2020, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR42600.2020.00336
   Cheng K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P448, DOI 10.1145/3394171.3413710
   Gao RH, 2021, PROC CVPR IEEE, P15490, DOI 10.1109/CVPR46437.2021.01524
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He R, 2020, IEEE T PATTERN ANAL, V42, P1025, DOI 10.1109/TPAMI.2019.2961900
   Hu D., 2020, Advances in Neural Information Processing Systems, P10077
   Hu P, 2019, KNOWL-BASED SYST, V180, P38, DOI 10.1016/j.knosys.2019.05.017
   Kamachi M, 2003, CURR BIOL, V13, P1709, DOI 10.1016/j.cub.2003.09.005
   Kim M, 2022, IEEE T MULTIMEDIA, V24, P4342, DOI 10.1109/TMM.2021.3115626
   Kingma D. P., 2014, arXiv
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Nagrani A, 2018, LECT NOTES COMPUT SC, V11217, P73, DOI 10.1007/978-3-030-01261-8_5
   Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Nawaz S., 2019, PROC DIGIT IMAGE COM, P1
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen X, 2018, IEEE T NEUR NET LEAR, V29, P3926, DOI 10.1109/TNNLS.2017.2750679
   Smith HMJ, 2016, ATTEN PERCEPT PSYCHO, V78, P868, DOI 10.3758/s13414-015-1045-8
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Sun GL, 2020, AAAI CONF ARTIF INTE, V34, P12047
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang R, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1881, DOI 10.1145/3397271.3401302
   Wei YK, 2022, Arxiv, DOI arXiv:2208.09579
   Wen PS, 2021, PROC CVPR IEEE, P16342, DOI 10.1109/CVPR46437.2021.01608
   Wen Y, 2019, P 7 INT C LEARN REPR
   Xu RQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P982
   Xue C, 2023, IEEE T MULTIMEDIA, V25, P418, DOI 10.1109/TMM.2021.3127029
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zheng AH, 2022, IEEE T MULTIMEDIA, V24, P338, DOI 10.1109/TMM.2021.3050089
   Zhu B, 2019, PROC CVPR IEEE, P11469, DOI 10.1109/CVPR.2019.01174
   Zhu H, 2021, INT J AUTOM COMPUT, V18, P351, DOI 10.1007/s11633-021-1293-0
NR 40
TC 1
Z9 1
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7505
EP 7516
DI 10.1109/TMM.2022.3222936
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000056
DA 2024-07-18
ER

PT J
AU Wang, ZQ
   Liu, Z
   Li, GY
   Wang, Y
   Zhang, TH
   Xu, LH
   Wang, JJ
AF Wang, Ziqiang
   Liu, Zhi
   Li, Gongyang
   Wang, Yang
   Zhang, Tianhong
   Xu, Lihua
   Wang, Jijun
TI Spatio-Temporal Self-Attention Network for Video Saliency Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Computational modeling; Visualization; Solid
   modeling; Task analysis; Semantics; Computer vision; Attention
   mechanism; self-attention; spatio-temporal feature; video saliency
   prediction
ID VISUAL-ATTENTION; NEURAL-NETWORK; MODEL
AB 3D convolutional neural networks have achieved promising results for video tasks in computer vision, including video saliency prediction that is explored in this paper. However, 3D convolution encodes visual representation merely on fixed local spacetime according to its kernel size, while human attention is always attracted by relational visual features at different time. To overcome this limitation, we propose a novel Spatio-Temporal Self-Attention 3D Network (STSANet) for video saliency prediction, in which multiple Spatio-Temporal Self-Attention (STSA) modules are employed at different levels of 3D convolutional backbone to directly capture long-range relations between spatio-temporal features of different time steps. Besides, we propose an Attentional Multi-Scale Fusion (AMSF) module to integrate multi-level features with the perception of context in semantic and spatio-temporal subspaces. Extensive experiments demonstrate the contributions of key components of our method, and the results on DHF1K, Hollywood-2, UCF, and DIEM benchmark datasets clearly prove the superiority of the proposed model compared with all state-of-the-art models.
C1 [Wang, Ziqiang; Liu, Zhi; Li, Gongyang] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Wang, Ziqiang; Liu, Zhi; Li, Gongyang] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Li, Gongyang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Wang, Yang] Univ Manitoba, Dept Comp Sci, Winnipeg, MB R3T 2N2, Canada.
   [Zhang, Tianhong; Xu, Lihua; Wang, Jijun] Shanghai Jiao Tong Univ, Sch Med, Shanghai Mental Hlth Ctr, Shanghai Key Lab Psychot Disorders, Shanghai 200030, Peoples R China.
C3 Shanghai University; Shanghai University; Nanyang Technological
   University; University of Manitoba; Shanghai Jiao Tong University
RP Liu, Z (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
EM wziqiang@shu.edu.cn; liuzhisjtu@163.com; ligongyang@shu.edu.cn;
   ywang@cs.umanitoba.ca; zhang_tianhong@126.com; dr_xulihua@163.com;
   dr_wangjijun@126.com
RI Lin, Fan/JZT-1441-2024; Fan, xiao/AEF-7654-2022; Xu, Lihua/P-8054-2019;
   CHEN, WENJIE/JQW-1608-2023; yang, zhou/JKI-3744-2023; LIU,
   Zhi/D-4518-2012; zhou, chuyue/JOJ-9001-2023; Lu, Jia/JVO-6891-2024;
   wang, ziqiang/E-5993-2016; Li, Gongyang/IXD-9078-2023
OI Lin, Fan/0000-0002-7330-3833; Fan, xiao/0000-0001-5147-6701; LIU,
   Zhi/0000-0002-8428-1131; , Gongyang Li/0000-0001-7324-1196; Wang,
   Ziqaing/0000-0002-4083-5411
FU National Natural Science Foundation of China [62171269, 82171544];
   Science and Technology Commission of Shanghai Municipality
   [21S31903100]; China Scholarship Council [202006890079]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62171269 and 82171544, in part by the
   Science and Technology Commission of Shanghai Municipality under Grant
   21S31903100, and in part by the China Scholarship Council under Grant
   202006890079. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Song Wang.
CR [Anonymous], 2008, Advances in neural information processing systems
   Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Bellitto G, 2021, INT J COMPUT VISION, V129, P3216, DOI 10.1007/s11263-021-01519-y
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang QY, 2021, Arxiv, DOI arXiv:2105.04213
   Che ZH, 2020, IEEE T IMAGE PROCESS, V29, P2287, DOI 10.1109/TIP.2019.2945857
   Chen J, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107615
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dodge SF, 2018, IEEE T IMAGE PROCESS, V27, P4080, DOI 10.1109/TIP.2018.2834826
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P3910, DOI 10.1109/TIP.2014.2336549
   Gorji S, 2018, PROC CVPR IEEE, P7501, DOI 10.1109/CVPR.2018.00783
   Hadizadeh H, 2014, IEEE T IMAGE PROCESS, V23, P19, DOI 10.1109/TIP.2013.2282897
   Harel J., 2007, P ADV NEUR INF PROC
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jain S, 2021, IEEE INT C INT ROBOT, P3520, DOI 10.1109/IROS51168.2021.9635989
   Jetley S, 2016, PROC CVPR IEEE, P5753, DOI 10.1109/CVPR.2016.620
   Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kingma D.P., 2014, ARXIV14126980
   Kroner A, 2020, NEURAL NETWORKS, V129, P261, DOI 10.1016/j.neunet.2020.05.004
   Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620
   Kümmerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513
   Kummerer M., 2015, INT C LEARN REPR ICL
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Le Meur O, 2006, IEEE T PATTERN ANAL, V28, P802, DOI 10.1109/TPAMI.2006.86
   Lei Ba J., 2016, arXiv
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P1461, DOI 10.1109/TIP.2020.3044440
   Li GY, 2019, NEUROCOMPUTING, V368, P180, DOI 10.1016/j.neucom.2019.08.051
   Linardos P., 2019, P BRIT MACH VIS C, P1
   Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Lu JS, 2016, ADV NEUR IN, V29
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Mahadevan V, 2010, IEEE T PATTERN ANAL, V32, P171, DOI 10.1109/TPAMI.2009.112
   Mathe S, 2015, IEEE T PATTERN ANAL, V37, P1408, DOI 10.1109/TPAMI.2014.2366154
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Mital PK, 2011, COGN COMPUT, V3, P5, DOI 10.1007/s12559-010-9074-z
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Paszke A, 2019, ADV NEUR IN, V32
   Reddy N, 2020, IEEE INT C INT ROBOT, P10241, DOI 10.1109/IROS45743.2020.9341574
   Ren QH, 2021, IEEE T MULTIMEDIA, V23, P1442, DOI 10.1109/TMM.2020.2997178
   Seoung Wug Oh, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P9225, DOI 10.1109/ICCV.2019.00932
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Souza LS, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107028
   Vaswani A., 2017, P 31 INT C NEUR INF, V30, DOI DOI 10.5555/3295222.3295349
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Wang H, 2021, IEEE IMAGE PROC, P2254, DOI [10.1109/ICIP42928.2021.9506731, 10.1109/iSPEC53008.2021.9735960]
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang ZQ, 2021, IMAGE VISION COMPUT, V109, DOI 10.1016/j.imavis.2021.104149
   Wu XY, 2020, AAAI CONF ARTIF INTE, V34, P12410
   Wu Z, 2019, IEEE T CIRC SYST VID, V29, P2960, DOI 10.1109/TCSVT.2018.2870954
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xiong C., 2017, P 5 INT C LEARN REPR, P1
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Zhang K, 2021, IEEE T IMAGE PROCESS, V30, P572, DOI 10.1109/TIP.2020.3036749
   Zhang K, 2019, IEEE T CIRC SYST VID, V29, P3544, DOI 10.1109/TCSVT.2018.2883305
   Zhang W, 2010, IEEE T MULTIMEDIA, V12, P300, DOI 10.1109/TMM.2010.2047607
   Zhou L, 2020, IEEE T IMAGE PROCESS, V29, P694, DOI 10.1109/TIP.2019.2928144
   Zhou XF, 2018, IEEE T MULTIMEDIA, V20, P2993, DOI 10.1109/TMM.2018.2829605
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 81
TC 15
Z9 15
U1 8
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1161
EP 1174
DI 10.1109/TMM.2021.3139743
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, Z
   Zhang, DH
   Xie, CY
   Yu, C
   Chen, JB
   Hu, Y
   Chen, Y
AF Wu, Zhi
   Zhang, Dongheng
   Xie, Chunyang
   Yu, Cong
   Chen, Jinbo
   Hu, Yang
   Chen, Yan
TI RFMask: A Simple Baseline for Human Silhouette Segmentation With Radio
   Signals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE RF signals; Feature extraction; Radar; Lighting; Cameras; Radio
   frequency; Image segmentation; Wireless sensing; deep learning; FMCW
   radar; semantic segmentation
AB Human silhouette segmentation, which is originally defined in computer vision, has achieved promising results for understanding human activities. However, the physical limitation makes existing systems based on optical cameras suffer from severe performance degradation under low illumination, smoke, and/or opaque obstruction conditions. To overcome such limitations, in this paper, we propose to utilize the radio signals, which can traverse obstacles and are unaffected by the lighting conditions to achieve silhouette segmentation. The proposed RFMask framework is composed of three modules. It first transforms RF signals captured by millimeter wave radar on two planes into spatial domain and suppress interference with the signal processing module. Then, it locates human reflections on RF frames and extract features from surrounding signals with human detection module. Finally, the extracted features from RF frames are aggregated with an attention based mask generation module. To verify our proposed framework, we collect a dataset containing 804,760 radio frames and 402,380 camera frames with human activities under various scenes. Experimental results show that the proposed framework can achieve impressive human silhouette segmentation even under the challenging scenarios (such as low light and occlusion scenarios) where traditional optical-camera-based methods fail. To the best of our knowledge, this is the first investigation towards segmenting human silhouette based on millimeter wave signals. We hope that our work can serve as a baseline and inspire further research that perform vision tasks with radio signals. The dataset and codes will be made in public.
C1 [Wu, Zhi; Zhang, Dongheng; Chen, Jinbo; Chen, Yan] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
   [Hu, Yang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230026, Peoples R China.
   [Xie, Chunyang; Yu, Cong] Univ Elect Sci & Technol China, Chengdu 611731, Sichuan, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; University of Electronic Science & Technology
   of China
RP Chen, Y (corresponding author), Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
EM wzwyyx@mail.ustc.edu.cn; dongheng@ustc.edu.cn;
   chunyangxie@std.uestc.edu.cn; congyu@std.uestc.edu.cn;
   jinbochen@mail.ustc.edu.cn; eeyhu@ustc.edu.cn; eecyan@ustc.edu.cn
RI Yu, Zhou/KBP-8384-2024
OI Xie, Chunyang/0000-0002-5074-314X; wu, zhi/0000-0001-7097-4837; Chen,
   Jinbo/0000-0003-4532-3236; Zhang, Dongheng/0000-0001-6309-6626
FU National Natural Science Foundation of China [62172381]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62172381.
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bowen Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12472, DOI 10.1109/CVPR42600.2020.01249
   Chen Y, 2021, IEEE INTERNET THINGS, V8, P2762, DOI 10.1109/JIOT.2020.3022071
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Gong LY, 2016, AD HOC NETW, V38, P38, DOI 10.1016/j.adhoc.2015.09.005
   Gong W, 2017, INT CON DISTR COMP S, P847, DOI 10.1109/ICDCS.2017.142
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu C.-Y., 2019, PROC CHI C HUM FACTO, P1
   Kirillov A., 2020, P IEEECVF C COMPUTER, P9799, DOI DOI 10.48550/ARXIV.1912.08193
   Kosba AE, 2012, INT CONF PERVAS COMP, P180, DOI 10.1109/PerCom.2012.6199865
   Lazarow Justin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10717, DOI 10.1109/CVPR42600.2020.01073
   Li HC, 2018, Arxiv, DOI [arXiv:1805.10180, DOI 10.48550/ARXIV.1805.10180]
   Li TH, 2022, IEEE WINT CONF APPL, P1091, DOI 10.1109/WACV51458.2022.00116
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2015, Arxiv, DOI [arXiv:1506.04579, 10.48550/arXiv.1506.04579]
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mohan R, 2021, INT J COMPUT VISION, V129, P1551, DOI 10.1007/s11263-021-01445-z
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sungha Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9370, DOI 10.1109/CVPR42600.2020.00939
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang W, 2017, IEEE J SEL AREA COMM, V35, P1118, DOI 10.1109/JSAC.2017.2679658
   Wu CS, 2015, IEEE J SEL AREA COMM, V33, P2329, DOI 10.1109/JSAC.2015.2430294
   Yu BH, 2021, IEEE INTERNET THINGS, V8, P13900, DOI 10.1109/JIOT.2021.3068798
   Zhang DH, 2021, IEEE INTERNET THINGS, V8, P3904, DOI 10.1109/JIOT.2020.3025820
   Zhang H, 2020, Arxiv, DOI arXiv:2004.08955
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao M., 2017, ICML, P4100
   Zhao MM, 2018, PROC CVPR IEEE, P7356, DOI 10.1109/CVPR.2018.00768
   Zhao MM, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P267, DOI 10.1145/3230543.3230579
NR 31
TC 7
Z9 8
U1 4
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4730
EP 4741
DI 10.1109/TMM.2022.3181455
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, ZQ
   Ma, BP
   Chang, H
   Shan, SG
AF Wu, Ziqiang
   Ma, Bingpeng
   Chang, Hong
   Shan, Shiguang
TI Refined Knowledge Transfer for Language-Based Person Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Language-based person search; knowledge enhancement; knowledge
   enhancement; cross-modal knowledge transfer; intra-modal knowledge
   transfer; intra-modal knowledge transfer; knowledge refiner
AB This paper proposes a novel method, named Refined Knowledge Transfer (RKT), for language-based person search. Existing state-of-the-art methods do not deal with knowledge imbalance between image and text. In detail, textual identity knowledge is limited, but the image contains more identity knowledge. We propose Cross-Modal Knowledge Transfer (CMKT) to enhance textual identity knowledge by image to address this problem. Besides, multiple texts of one image include more identity knowledge than a single text. Thus, we propose Intra-Modal Knowledge Transfer (IMKT) to enhance textual identity knowledge by other texts. These two types of knowledge transfer will enhance the identity knowledge in text. Additionally, by considering that identity-irrelevant knowledge is transferred to text, we propose Knowledge Refiner (KR) to refine the knowledge in text. KR is capable of preserving identity knowledge and discarding identity-irrelevant knowledge. By combining CMKT, IMKT, and KR, RKT makes textual identity knowledge more salient. Extensive experiments show the state-of-the-art performance of RKT on the CUHK-PEDES and our proposed PRW-PEDES-CN datasets. In addition, the decent generalization ability of RKT is also validated on the Flickr30K, CUB, and Flowers datasets.
C1 [Wu, Ziqiang; Ma, Bingpeng] Univ Chinese Acad Sci UCAS, Sch Sci & Technol, Beijing 100049, Peoples R China.
   [Chang, Hong] Chinese Acad Sci, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Chang, Hong; Shan, Shiguang] UCAS, Sch Comp Sci & Technol, Beijing 100049, Peoples R China.
   [Chang, Hong; Shan, Shiguang] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Shan, Shiguang] Chinese Acad Sci, Key Lab Intelligent Informat Proc, ICT, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Peng Cheng Laboratory; Chinese Academy of
   Sciences
RP Ma, BP (corresponding author), Univ Chinese Acad Sci UCAS, Sch Sci & Technol, Beijing 100049, Peoples R China.
EM wuziqiang19@mails.ucas.ac.cn; bpma@ucas.ac.cn; changhong@ict.ac.cn;
   sgshan@ict.ac.cn
OI Ziqiang, Wu/0000-0003-2724-3478; Shan, Shiguang/0000-0002-8348-392X
FU National Natural Science Foundation of China
FX No Statement Available
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Chen DP, 2018, LECT NOTES COMPUT SC, V11220, P56, DOI 10.1007/978-3-030-01270-0_4
   Chen TL, 2018, IEEE WINT CONF APPL, P1879, DOI 10.1109/WACV.2018.00208
   Chen YC, 2021, IEEE T IMAGE PROCESS, V30, P4057, DOI 10.1109/TIP.2021.3068825
   Chen YH, 2022, NEUROCOMPUTING, V494, P171, DOI 10.1016/j.neucom.2022.04.081
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, COMPUT SCI, V2
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou RB, 2019, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR.2019.00735
   Huang Y, 2020, IEEE T PATTERN ANAL, V42, P636, DOI 10.1109/TPAMI.2018.2883466
   Jing Y, 2020, AAAI CONF ARTIF INTE, V34, P11189
   Kai Niu, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P4426, DOI 10.1145/3503161.3547753
   Kaselimi M, 2019, INT CONF ACOUST SPEE, P2747, DOI 10.1109/ICASSP.2019.8683110
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Lee S., 2019, P BRIT MACH VIS C, P1
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li S, 2017, IEEE I CONF COMP VIS, P1908, DOI 10.1109/ICCV.2017.209
   Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Liu C., 2020, P IEEE CVF C COMP VI, P10921, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu CX, 2019, INT CONF ACOUST SPEE, P3970, DOI [10.1109/icassp.2019.8683869, 10.1109/ICASSP.2019.8683869]
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Loper E, 2002, ETMTNLP 02 P ACL 02, P63, DOI [10.3115/1225403.1225421, DOI 10.3115/1225403.1225421, DOI 10.3115/1118108.1118117]
   Luo RX, 2022, Arxiv, DOI arXiv:1906.11455
   Matsubara T, 2021, IEICE T INF SYST, VE104D, P24, DOI 10.1587/transinf.2020MUP0003
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Niu K, 2020, IEEE T IMAGE PROCESS, V29, P5542, DOI 10.1109/TIP.2020.2984883
   Olszewska J. I., 2011, Proceedings of the 2011 15th IEEE International Conference on Intelligent Engineering Systems (INES), P369, DOI 10.1109/INES.2011.5954775
   Olszewska J. I., 2013, PROC ANN M ASS COMPU, P3
   Olszewska JI, 2022, ICAART, P996, DOI 10.5220/0010993000003116
   Olszewska JI, 2019, PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE (ICAART), VOL 2, P850, DOI 10.5220/0007585208500856
   Peng YX, 2023, IEEE T MULTIMEDIA, V25, P2393, DOI 10.1109/TMM.2022.3146775
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romero A., 2014, ARXIV14126550
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang YY, 2019, INT CONF ACOUST SPEE, P2057, DOI 10.1109/ICASSP.2019.8682456
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Yan YC, 2020, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR42600.2020.00297
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Yu T., 2020, PREPRINT, DOI DOI 10.48550/ARXIV.2003.05689
   Zha ZJ, 2020, IEEE T MULTIMEDIA, V22, P1836, DOI 10.1109/TMM.2020.2972168
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhang ZZ, 2020, INT CONF CONDIT MON, P404, DOI 10.1109/CVPR42600.2020.01042
   Zhe Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P402, DOI 10.1007/978-3-030-58610-2_24
   Zheng KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3441, DOI 10.1145/3394171.3413864
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhu AC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P209, DOI 10.1145/3474085.3475369
NR 60
TC 7
Z9 7
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9315
EP 9329
DI 10.1109/TMM.2023.3251104
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200011
DA 2024-07-18
ER

PT J
AU Xia, K
   Wang, L
   Shen, YC
   Zhou, SP
   Hua, G
   Tang, W
AF Xia, Kun
   Wang, Le
   Shen, Yichao
   Zhou, Sanpin
   Hua, Gang
   Tang, Wei
TI Exploring Action Centers for Temporal Action Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Location awareness; Videos; Feature extraction;
   Visualization; Reliability; Object detection; Temporal action detection;
   temporal action localization; temporal action proposal generation
AB Temporal action localization aims at detecting the temporal intervals of human actions in untrimmed videos. Most previous methods rely on locating and matching the start and end times of actions. However, action boundaries are ambiguous and uncertain in nature, which leads to inaccurate action localization and a lot of false positives. In this paper, we introduce a new framework for temporal action localization. It explicitly models temporal action centers to reduce unreliable action detection results caused by ambiguous action boundaries. Since action centers are highly related to semantic actions, they can be detected more reliably than the conventional action boundaries. As a result, our framework can exclude false positives and promote high-quality proposals. Based on action centers, we propose a triplet feature fusion mechanism. It performs neural message passing among the boundaries and the center as well as contextual regions outside of the proposal to enrich its representation. In addition, we introduce a centerness scoring method to suppress proposals deviating from the centers of action instances. Consequently, our network can retrieve high-quality action proposals and locate actions more precisely. Experimental results show our method outperforms state-of-the-art methods on the THUMOS14 and ActivityNet v1.3 datasets.
C1 [Xia, Kun; Wang, Le; Shen, Yichao; Zhou, Sanpin] Xi An Jiao Tong Univ, Natl Key Lab Human Machine Hybrid Augmented Intell, Natl Engn Res Ctr Visual Informat & Applicat, Xian, Shaanxi, Peoples R China.
   [Xia, Kun; Wang, Le; Shen, Yichao; Zhou, Sanpin] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Shaanxi, Peoples R China.
   [Hua, Gang] Wormpex AI Res, Bellevue, WA 98004 USA.
   [Tang, Wei] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; University of
   Illinois System; University of Illinois Chicago; University of Illinois
   Chicago Hospital
RP Wang, L (corresponding author), Xi An Jiao Tong Univ, Natl Key Lab Human Machine Hybrid Augmented Intell, Natl Engn Res Ctr Visual Informat & Applicat, Xian, Shaanxi, Peoples R China.; Wang, L (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Shaanxi, Peoples R China.
EM xiakun@stu.xjtu.edu.cn; lewang@mail.xjtu.edu.cn;
   yichaoshen@stu.xjtu.edu.cn; spzhou@mail.xjtu.edu.cn; ganghua@gmail.com;
   tangw@uic.edu
OI Wang, Le/0000-0001-6636-6396
FU National Key Ramp;D Program of China
FX No Statement Available
CR Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen G, 2022, AAAI CONF ARTIF INTE, P248
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng F, 2022, LECT NOTES COMPUT SC, V13694, P503, DOI 10.1007/978-3-031-19830-4_29
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Dong Z., 2020, P IEEE C COMP VIS PA, P10519, DOI DOI 10.1109/CVPR42600.2020.01053
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao Jiyang, 2017, BMVC
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Huang JJ, 2018, AAAI CONF ARTIF INTE, P6951
   Huang YP, 2019, IEEE INT CON MULTI, P1288, DOI 10.1109/ICME.2019.00224
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Kang H, 2022, PROC CVPR IEEE, P20041, DOI 10.1109/CVPR52688.2022.01944
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kong Y, 2022, INT J COMPUT VISION, V130, P1366, DOI 10.1007/s11263-022-01594-9
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li J, 2020, AAAI CONF ARTIF INTE, V34, P4626
   Lin CM, 2021, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR46437.2021.00333
   Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu Y, 2022, IEEE T CIRC SYST VID, V32, P5, DOI 10.1109/TCSVT.2021.3075607
   Liu YF, 2019, PROC CVPR IEEE, P7099, DOI [10.1109/CVPR.2019.00726, 10.1109/CVPR.2019.00372]
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754
   Ma SG, 2018, INT J COMPUT VISION, V126, P314, DOI 10.1007/s11263-016-0980-8
   Mettes P, 2019, INT J COMPUT VISION, V127, P263, DOI 10.1007/s11263-018-1120-4
   Nag S, 2022, LECT NOTES COMPUT SC, V13663, P645, DOI 10.1007/978-3-031-20062-5_37
   Peisen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P539, DOI 10.1007/978-3-030-58598-3_32
   Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Su HS, 2021, AAAI CONF ARTIF INTE, V35, P2602
   Tan J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13506, DOI 10.1109/ICCV48922.2021.01327
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang Q, 2022, PROC CVPR IEEE, P13556, DOI 10.1109/CVPR52688.2022.01320
   Xia K, 2022, PROC CVPR IEEE, P13874, DOI 10.1109/CVPR52688.2022.01351
   Xia K, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108725
   Xiong YJ, 2017, Arxiv, DOI arXiv:1703.02716
   Xiong YJ, 2016, Arxiv, DOI arXiv:1608.00797
   Xu M., 2020, CVPR, P10156
   Yang K, 2018, AAAI CONF ARTIF INTE, P7477
   Yang Le, 2022, IEEE Trans Image Process, VPP, DOI 10.1109/TIP.2022.3180925
   Yang L, 2022, IEEE T PATTERN ANAL, V44, P9814, DOI 10.1109/TPAMI.2021.3132058
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Yixuan Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P68, DOI 10.1007/978-3-030-58517-4_5
   Yuan ZH, 2017, PROC CVPR IEEE, P3215, DOI 10.1109/CVPR.2017.342
   Yueran Bai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P121, DOI 10.1007/978-3-030-58604-1_8
   Zeng RH, 2022, IEEE T PATTERN ANAL, V44, P6209, DOI 10.1109/TPAMI.2021.3090167
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang CL, 2022, LECT NOTES COMPUT SC, V13664, P492, DOI 10.1007/978-3-031-19772-7_29
   Zhang H, 2019, IEEE T MULTIMEDIA, V21, P1450, DOI 10.1109/TMM.2018.2884478
   Zhao T, 2021, INT J COMPUT VISION, V129, P2474, DOI 10.1007/s11263-021-01473-9
   Zhao Y, 2020, INT J COMPUT VISION, V128, P74, DOI 10.1007/s11263-019-01211-2
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
   Zhu ZX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13496, DOI 10.1109/ICCV48922.2021.01326
NR 69
TC 3
Z9 3
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9425
EP 9436
DI 10.1109/TMM.2023.3252176
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200012
DA 2024-07-18
ER

PT J
AU Xiong, JW
   Zhou, Y
   Zhang, P
   Xie, L
   Huang, W
   Zha, YF
AF Xiong, Junwen
   Zhou, Yu
   Zhang, Peng
   Xie, Lei
   Huang, Wei
   Zha, Yufei
TI Look&listen: Multi-Modal Correlation Learning for Active Speaker
   Detection and Speech Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active speaker detection; speech enhancement; audio-visual correlation
   learning
ID MULTISENSORY INTEGRATION; TRACKING
AB Active speaker detection and speech enhancement have become two increasingly attractive topics in audio-visual scenario understanding. According to their respective characteristics, the scheme of independently designed architecture has been widely used in correspondence to each single task. This may lead to the representation learned by the model being task-specific, and inevitably result in the lack of generalization ability of the feature based on multi-modal modeling. More recent studies have shown that establishing cross-modal relationship between auditory and visual stream is a promising solution for the challenge of audio-visual multi-task learning. Therefore, as a motivation to bridge the multi-modal associations in audio-visual tasks, a unified framework is proposed to achieve target speaker detection and speech enhancement with joint learning of audio-visual modeling in this study. With the assistance of audio-visual channels of videos in challenging real-world scenarios, the proposed method is able to exploit inherent correlations in both audio and visual signals, which is used to further anticipate and model the temporal audio-visual relationships across spatial-temporal space via a cross-modal conformer. In addition, a plug-and-play multi-modal layer normalization is introduced to alleviate the distribution misalignment of multi-modal features. Based on cross-modal circulant fusion, the proposed model is capable to learned all audio-visual representations in a holistic process. Substantial experiments demonstrate that the correlations between different modalities and the associations among diverse tasks can be learned by the optimized model more effectively. In comparison to other state-of-the-art works, the proposed work shows a superior performance for active speaker detection and audio-visual speech enhancement on three benchmark datasets, also with a favorable generalization in diverse challenges.
C1 [Xiong, Junwen; Zhou, Yu; Zhang, Peng; Xie, Lei; Zha, Yufei] Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China.
   [Zhang, Peng; Zha, Yufei] Northwestern Polytech Univ, Ningbo Inst, Ningbo 710129, Peoples R China.
   [Huang, Wei] Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Jiangxi, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University; Nanchang University
RP Zhang, P (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China.
EM 358975248xjw@gmail.com; 1152490501@qq.com; zh0036ng@nwpu.edu.cn;
   lxie@nwpu.edu.cn; n060101@e.ntu.edu.sg; zhayufei@126.com
RI Li, Chun/KBC-9591-2024; Xiong, Junwen/GRR-2398-2022; Wang,
   YUJIE/JXY-8442-2024; Xie, Lei/JWO-8567-2024
OI Xiong, Junwen/0000-0001-5180-1295; Zha, yufei/0000-0001-5013-2501;
   Huang, Wei/0000-0002-0541-8612
FU National Natural Science Foundation of China [61971352, 61862043];
   Natural Science Foundation of Ningbo [2021J048, 2021J049]; Natural
   Science Foundation of Jiangxi Province in China [20204BC J22011]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61971352 and 61862043, in part by the
   Natural Science Foundation of Ningbo under Grants 2021J048 and 2021J049,
   and in part by the Natural Science Foundation of Jiangxi Province in
   China under Grant 20204BC J22011.
CR Afouras T., 2020, P EUR C COMP VIS, V12363, P208, DOI 10.1007/978-3-030- 58523-5-13
   Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Afouras T, 2018, Arxiv, DOI arXiv:1809.00496
   Afouras T, 2018, INTERSPEECH, P3244
   Alcazar J. L., 2020, IEEECVF C COMPUTER V, p12 465
   Aldeneh Z, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P8423, DOI 10.1109/ICASSP39728.2021.9414263
   [Anonymous], 2000, AUDIO-VISUAL SPEECH RECOGNITION
   [Anonymous], 2001, International Journal of Image and Graphics (IJIG), Vol, DOI DOI 10.1142/S021946780100027X
   Barnard M, 2014, IEEE T MULTIMEDIA, V16, P864, DOI 10.1109/TMM.2014.2301977
   BULTHOFF HH, 1988, J OPT SOC AM A, V5, P1749, DOI 10.1364/JOSAA.5.001749
   Burr D, 2006, PROG BRAIN RES, V155, P243, DOI 10.1016/S0079-6123(06)55014-9
   Chakravarty P, 2016, LECT NOTES COMPUT SC, V9909, P285, DOI 10.1007/978-3-319-46454-1_18
   Chung JS, 2018, INTERSPEECH, P1086
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   Gabbay A, 2018, INTERSPEECH, P1170
   Gao RH, 2021, PROC CVPR IEEE, P15490, DOI 10.1109/CVPR46437.2021.01524
   Gao RH, 2020, PROC CVPR IEEE, P10454, DOI 10.1109/CVPR42600.2020.01047
   Golumbic EZ, 2013, J NEUROSCI, V33, P1417, DOI 10.1523/JNEUROSCI.3675-12.2013
   Gulati A, 2020, INTERSPEECH, P5036, DOI 10.21437/Interspeech.2020-3015
   Haider F, 2016, IEEE GLOB CONF SIG, P1207, DOI 10.1109/GlobalSIP.2016.7906033
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey JR, 2016, INT CONF ACOUST SPEE, P31, DOI 10.1109/ICASSP.2016.7471631
   Hu D, 2022, IEEE T PATTERN ANAL, V44, P9844, DOI 10.1109/TPAMI.2021.3137988
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ito K, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P6668, DOI 10.1109/ICASSP39728.2021.9414133
   Kazakos E, 2019, IEEE I CONF COMP VIS, P5491, DOI 10.1109/ICCV.2019.00559
   King AJ, 2005, CURR BIOL, V15, pR339, DOI 10.1016/j.cub.2005.04.022
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Kopuklu O., 2021, P IEEE CVF INT C COM, P1193
   Lei Ba J., 2016, arXiv
   Alcázar JL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P265, DOI 10.1109/ICCV48922.2021.00033
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZC, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2624140
   Li ZC, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2822907
   Liu L, 2021, IEEE T MULTIMEDIA, V23, P292, DOI 10.1109/TMM.2020.2976493
   Luo Y, 2019, IEEE-ACM T AUDIO SPE, V27, P1256, DOI 10.1109/TASLP.2019.2915167
   Martin O., 2006, 22 INT C DATA ENG WO, P8, DOI [DOI 10.1109/ICDEW.2006.145, 10.1109/ICDEW.2006.145]
   Michelsanti D, 2021, IEEE-ACM T AUDIO SPE, V29, P1368, DOI 10.1109/TASLP.2021.3066303
   Minotto VP, 2014, IEEE T MULTIMEDIA, V16, P1032, DOI 10.1109/TMM.2014.2305632
   Moattar M. H., 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P2549
   Morrone G, 2019, INT CONF ACOUST SPEE, P6900, DOI 10.1109/ICASSP.2019.8682061
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P3793, DOI 10.1109/TMM.2020.3032037
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Patrona F, 2016, IEEE T MULTIMEDIA, V18, P967, DOI 10.1109/TMM.2016.2535357
   Rao VR, 2022, AAAI CONF ARTIF INTE, P2144
   Ray R., 2021, P 12 INT C COMP COMM, P1
   Rix AW, 2001, INT CONF ACOUST SPEE, P749, DOI 10.1109/ICASSP.2001.941023
   Roth J, 2020, INT CONF ACOUST SPEE, P4492, DOI [10.1109/icassp40776.2020.9053900, 10.1109/ICASSP40776.2020.9053900]
   Shahid M, 2021, IEEE WINT CONF APPL, P2331, DOI 10.1109/WACV48630.2021.00238
   Sharma R, 2021, Arxiv, DOI arXiv:2003.04358
   Shvets M, 2019, IEEE I CONF COMP VIS, P9755, DOI 10.1109/ICCV.2019.00985
   Snyder D, 2015, Arxiv, DOI arXiv:1510.08484
   Chung JS, 2019, Arxiv, DOI arXiv:1906.10555
   Stekelenburg JJ, 2007, J COGNITIVE NEUROSCI, V19, P1964, DOI 10.1162/jocn.2007.19.12.1964
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   Tao F, 2019, SPEECH COMMUN, V113, P25, DOI 10.1016/j.specom.2019.07.003
   Tao F, 2017, INTERSPEECH, P1938, DOI 10.21437/Interspeech.2017-1573
   Tao RJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3927, DOI 10.1145/3474085.3475587
   Truong T.D., 2021, P IEEE CVF INT C COM, P1105, DOI 10.48550/arXiv.2108.03256
   Van den Oord A., 2021, P 9 ISCA SPEECH SYNT
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   von Kriegstein K, 2008, P NATL ACAD SCI USA, V105, P6747, DOI 10.1073/pnas.0710826105
   Wang SZ, 2022, AAAI CONF ARTIF INTE, P2531
   Wu CH, 2013, IEEE T MULTIMEDIA, V15, P1880, DOI 10.1109/TMM.2013.2269314
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhang Y.-H., 2019, PROC ACTIVITYNET LAR, P1
   Zhang YJ, 2023, ARAB J SCI ENG, V48, P4141, DOI 10.1007/s13369-021-06028-1
   Zhang YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3964, DOI 10.1145/3474085.3475275
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
NR 73
TC 3
Z9 3
U1 8
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5800
EP 5812
DI 10.1109/TMM.2022.3199109
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500012
DA 2024-07-18
ER

PT J
AU Zhai, W
   Cao, Y
   Xie, HY
   Zha, ZJ
AF Zhai, Wei
   Cao, Yang
   Xie, HaiYong
   Zha, Zheng-Jun
TI Deep Texton-Coherence Network for Camouflaged Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Camouflaged object detection; texture representation; deep learning
ID TEXTURE; FEATURES; PATTERN
AB Camouflaged object detection is a challenging visual task since the appearance and morphology of foreground objects and background regions are highly similar in nature. Recent CNN-based studies gradually integrated the high-level semantic information and the low-level local features of images through hierarchical and progressive structures to achieve camouflaged object detection. However, these methods ignore the <bold>spatial statistical properties</bold> of the local context, which is a critical cue for distinguishing and describing camouflaged objects. To address this problem, we propose a novel Deep Texton-Coherence Network (DTC-Net) that leverages the spatial organization of textons in the foreground and background regions as discriminative cues for camouflaged object detection. Specifically, a Local Bilinear module (LB) is devised to obtain the robust representation of texton to trivial details and illumination changes, by replacing the classic first-order linearization operations with bilinear second-order statistical operations in the convolution process. Next, these texton representations are associated with a Spatial Coherence Organization module (SCO) to capture irregular spatial coherence via a deformable convolutional strategy, and then the descriptions of the textons extracted by the LB module are used as weights to suppress features that are spatially adjacent but have different representations. Finally, the texton-coherence representation is integrated with the original features at different levels to achieve camouflaged object detection. Evaluation on the three most challenging camouflaged object detection datasets demonstrats the superiority of the proposed model when compared to the state-of-the-art methods. Furthermore, our ablation studies and performance analyses demonstrate the effectiveness of the texton-coherence module.
C1 [Zhai, Wei; Xie, HaiYong; Zha, Zheng-Jun] Univ Sci & Technol China, Dept Informat Sci & Technol, Hefei 230000, Peoples R China.
   [Cao, Yang] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Cao, Y (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Anhui, Peoples R China.
EM wzhai056@mail.ustc.edu.cn; forrest@ustc.edu.cn; hxie@ustc.edu.cn;
   zhazj@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020
FU National Key R&D Program of China [2020AAA0105700]; National Natural
   Science Foundation of China (NSFC) [61872327, U19B2038]; Major Special
   Science and Technology Project of Anhui [012223665049]; University
   Synergy Innovation Program of Anhui Province [GXXT-2019-025]
FX This work was supported in part by National Key R&D Program of China
   under Grant 2020AAA0105700, in part by the National Natural Science
   Foundation of China (NSFC) under Grants 61872327 and U19B2038, in part
   by Major Special Science and Technology Project of Anhui under Grant
   012223665049, and in part by the University Synergy Innovation Program
   of Anhui Province under Grant GXXT-2019-025.
CR Allili MS, 2014, IEEE T MULTIMEDIA, V16, P772, DOI 10.1109/TMM.2014.2298832
   Bhajantri NU, 2006, ICIT 2006: 9TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY, PROCEEDINGS, P145
   Caputo B, 2005, IEEE I CONF COMP VIS, P1597, DOI 10.1109/iccv.2005.54
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Cuthill IC, 2005, NATURE, V434, P72, DOI 10.1038/nature03312
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai XY, 2017, PROC CVPR IEEE, P6100, DOI 10.1109/CVPR.2017.646
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2020, IEEE T MED IMAGING, V39, P2626, DOI 10.1109/TMI.2020.2996645
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0
   Kingma D. P., 2014, arXiv
   Li JX, 2021, IEEE T MULTIMEDIA, V23, P1397, DOI 10.1109/TMM.2020.2997192
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin TY, 2016, PROC CVPR IEEE, P2791, DOI 10.1109/CVPR.2016.305
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Mallikarjuna P., 2006, The KTH-TIPS2 database
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Nawaz M, 2021, IEEE T MULTIMEDIA, V23, P2902, DOI 10.1109/TMM.2020.3019688
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Sharan L., 2009, J VISION, V9, P784, DOI 10.1167/9.8.784
   Sharan L, 2013, INT J COMPUT VISION, V103, P348, DOI 10.1007/s11263-013-0609-0
   Skurowski P., 2018, Animal camouflage analysis: Chameleon database
   Song L., 2010, P INT C MULT TECHN, P1
   Sultana M, 2021, IEEE T MULTIMEDIA, V23, P2005, DOI 10.1109/TMM.2020.3006419
   Tankus A, 2001, COMPUT VIS IMAGE UND, V82, P208, DOI 10.1006/cviu.2001.0912
   Troscianko T, 2009, PHILOS T R SOC B, V364, P449, DOI 10.1098/rstb.2008.0218
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xie SN, 2017, INT J COMPUT VISION, V125, P3, DOI 10.1007/s11263-017-1004-z
   Xue F, 2016, MULTIMED TOOLS APPL, V75, P4065, DOI 10.1007/s11042-015-2946-1
   Xue F, 2015, MULTIMEDIA SYST, V21, P169, DOI 10.1007/s00530-014-0368-y
   Zhai W, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1283, DOI 10.1109/ICASSP.2018.8462364
   Zhang W, 2018, IEEE T MULTIMEDIA, V20, P880, DOI 10.1109/TMM.2017.2760102
   Zhang X, 2017, IEEE T CIRC SYST VID, V27, P2001, DOI 10.1109/TCSVT.2016.2555719
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zheng YF, 2019, IEEE SIGNAL PROC LET, V26, P29, DOI 10.1109/LSP.2018.2825959
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zhu K, 2021, IEEE T MULTIMEDIA, V23, P3726, DOI 10.1109/TMM.2020.3031062
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 53
TC 8
Z9 8
U1 4
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5155
EP 5165
DI 10.1109/TMM.2022.3188401
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300039
DA 2024-07-18
ER

PT J
AU Zhang, H
   Tang, JK
   Cao, YH
   Chen, YR
   Wang, YA
   Wu, QMJ
AF Zhang, Hui
   Tang, Junkun
   Cao, Yihong
   Chen, Yurong
   Wang, Yaonan
   Wu, Q. M. Jonathan
TI Cycle Consistency Based Pseudo Label and Fine Alignment for Unsupervised
   Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Task analysis; Training; Semantics; Optimization;
   Technological innovation; Supervised learning; Domain adaptation;
   cycle-consistency; pseudo label; distribution fine-alignment
ID SUBDOMAIN ADAPTATION; NETWORK
AB Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a well-labeled source domain to an unlabeled target domain with a correlative distribution. Numerous existing approaches process this hard nut by directly matching the marginal distribution between two domains, which confront the obstacle of rough alignment and blurred decision boundary. Recent advances in UDA introduce target pseudo-label and subdomain adaptation to reduce misalignment and distribution discrepancy. Whereas, they frequently ignore that the production of target pseudo-label is so dependent on the source-trained classifier, which without reasonable restriction to discriminate generated pseudo-label is whether confident. Meanwhile, many methods in the subdomain alignment metric ignore exploring the potential distribution discrepancy between same-class samples of the intra-domain. To address these two issues simultaneously, this paper proposes a Cycle Consistency based Pseudo Label and Fine Alignment (CCPLFA) approach for UDA. In particular, firstly, a novel cycle-consistency based pseudo label module is designed, which is a simple yet effective way to alleviate the noise of pseudo labels and improve their semantic correctness. Secondly, we develop a Fine-Alignment distribution matching metric. Which can maximize the feature distribution density of intra-class cross-domains and not overlook the distribution structure of the global aspect. Comprehensive experiment results on four benchmarks demonstrate the capability of plug and play and the well generalization performance of our proposed method.
C1 [Zhang, Hui; Chen, Yurong; Wang, Yaonan] Hunan Univ, Sch Robot, Natl Engn Lab Robot Visual Percept & Control Techn, Changsha 410082, Hunan, Peoples R China.
   [Tang, Junkun] Changsha Univ Sci & Technol, Sch Elect & Informat Engn, Changsha 410082, Peoples R China.
   [Cao, Yihong] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
   [Cao, Yihong] Hunan Univ, Natl Engn Res Ctr Robot Vis Percept & Control, Sch Robot, Changsha 410082, Hunan, Peoples R China.
   [Wu, Q. M. Jonathan] Univ Windsor, Coll Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
C3 Hunan University; Changsha University of Science & Technology; Hunan
   University; Hunan University; University of Windsor
RP Zhang, H (corresponding author), Hunan Univ, Sch Robot, Natl Engn Lab Robot Visual Percept & Control Techn, Changsha 410082, Hunan, Peoples R China.
EM zhanghuihby@126.com; jadekintang@126.com; caoyihong@hnu.edu.cn;
   chenyurong1998@outlook.com; yaonan@hnu.edu; jwu@uwindsor.ca
OI Chen, Yurong/0000-0002-6171-4555; Tang, Junkun/0000-0001-7348-2723; Cao,
   Yihong/0000-0003-1751-5505
FU National Key RD Program of China
FX No Statement Available
CR Araslanov N, 2021, PROC CVPR IEEE, P15379, DOI 10.1109/CVPR46437.2021.01513
   Arora S, 2017, PR MACH LEARN RES, V70
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Berthelot D, 2019, ADV NEUR IN, V32
   Chen CQ, 2019, PROC CVPR IEEE, P627, DOI 10.1109/CVPR.2019.00072
   Chen HH, 2021, Arxiv, DOI arXiv:2105.04729
   Cicek S, 2019, IEEE I CONF COMP VIS, P1416, DOI 10.1109/ICCV.2019.00150
   Deng WX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1303, DOI 10.1145/3474085.3475579
   Deng WX, 2021, IEEE T IMAGE PROCESS, V30, P7842, DOI 10.1109/TIP.2021.3109530
   Dong J., 2021, IEEE Trans. Pattern Anal, Mach. Intell., DOI [10.1109/ΡΑΙ.2021.3128560, DOI 10.1109/&URHO;&UALPHA;&UIOTA;.2021.3128560]
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ghorvei M, 2023, NEUROCOMPUTING, V517, P44, DOI 10.1016/j.neucom.2022.10.057
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hui Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8722, DOI 10.1109/CVPR42600.2020.00875
   Jiang X, 2020, PR MACH LEARN RES, V119
   Kang GL, 2022, IEEE T PATTERN ANAL, V44, P1793, DOI 10.1109/TPAMI.2020.3029948
   Korotin A, 2020, Arxiv, DOI arXiv:1909.13082
   Kumaret A., 2018, P ADV NEUR INF PROC, P1
   Lee D.-H., 2013, PSEUDOLABEL SIMPLE E, V3, P896
   Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050
   Li JJ, 2019, IEEE T NEUR NET LEAR, V30, P1381, DOI 10.1109/TNNLS.2018.2868854
   Li LS, 2021, IEEE T CYBERNETICS, V51, P3404, DOI 10.1109/TCYB.2020.2983337
   Li SJ, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3157007
   Li S, 2022, IEEE T PATTERN ANAL, V44, P4093, DOI 10.1109/TPAMI.2021.3062644
   Liang J., 2020, INT C MACH LEARN, P6028, DOI DOI 10.48550/ARXIV.2002.08546
   Liang J, 2021, PROC CVPR IEEE, P16627, DOI 10.1109/CVPR46437.2021.01636
   Lin CT, 2021, IEEE T INTELL TRANSP, V22, P951, DOI 10.1109/TITS.2019.2961679
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu BC, 2021, AAAI CONF ARTIF INTE, V35, P2073
   Liu YX, 2022, IEEE T IND INFORM, V18, P6038, DOI 10.1109/TII.2022.3141783
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Luo YW, 2021, PROC CVPR IEEE, P13984, DOI 10.1109/CVPR46437.2021.01377
   Morerio P, 2020, IEEE WINT CONF APPL, P3119, DOI [10.1109/WACV45572.2020.9093579, 10.1109/wacv45572.2020.9093579]
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   Peng XC, 2017, Arxiv, DOI arXiv:1710.06924
   Qiang WW, 2023, IEEE T KNOWL DATA EN, V35, P3014, DOI 10.1109/TKDE.2021.3112815
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito Kuniaki, 2020, Adv. Neural Inf. Process. Syst.
   Sener O, 2016, ADV NEUR IN, V29
   Shi WW, 2018, LECT NOTES COMPUT SC, V11209, P311, DOI 10.1007/978-3-030-01228-1_19
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang H, 2022, IEEE T PATTERN ANAL, V44, P6517, DOI 10.1109/TPAMI.2021.3087830
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang JD, 2017, IEEE DATA MINING, P1129, DOI 10.1109/ICDM.2017.150
   Wang M, 2021, NEUROCOMPUTING, V422, P186, DOI 10.1016/j.neucom.2020.07.124
   Wang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9123, DOI 10.1109/ICCV48922.2021.00901
   Wei GQ, 2021, PROC CVPR IEEE, P16638, DOI 10.1109/CVPR46437.2021.01637
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiang Gu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9098, DOI 10.1109/CVPR42600.2020.00912
   Xie S., 2018, P 35 INT C MACHINE L, P5423
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Yan L, 2020, IEEE T GEOSCI REMOTE, V58, P3558, DOI 10.1109/TGRS.2019.2958123
   Yan Y, 2016, AAAI CONF ARTIF INTE, P2244
   Yang YC, 2020, PROC CVPR IEEE, P4084, DOI 10.1109/CVPR42600.2020.00414
   Yosinski J, 2014, ADV NEUR IN, V27
   Zellinger W, 2019, Arxiv, DOI [arXiv:1702.08811, DOI 10.48550/ARXIV.1702.08811]
   Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400
   Zhao SC, 2022, IEEE T NEUR NET LEAR, V33, P473, DOI 10.1109/TNNLS.2020.3028503
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu YC, 2021, IEEE T NEUR NET LEAR, V32, P1713, DOI 10.1109/TNNLS.2020.2988928
   Zhuang FZ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4119
NR 72
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8051
EP 8063
DI 10.1109/TMM.2022.3233306
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000012
DA 2024-07-18
ER

PT J
AU Zhu, DH
   Du, B
   Dong, YN
   Zhang, LP
AF Zhu, Dehui
   Du, Bo
   Dong, Yanni
   Zhang, Liangpei
TI Target Detection With Spatial-Spectral Adaptive Sample Generation and
   Deep Metric Learning for Hyperspectral Imagery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Hyperspectral imaging; Object detection; Feature
   extraction; Detectors; Training; Learning systems; Deep metric learning;
   hyperspectral imagery; sample generation; target detection
ID ORTHOGONAL SUBSPACE PROJECTION; DETECTION ALGORITHMS; OBJECT DETECTION;
   SPARSE; REPRESENTATION; MODEL
AB In hyperspectral target detection, the conventional metric learning-based algorithms provide unique advantages in detecting targets as they do not require specific assumptions and adapt to the condition of limited training samples. Nevertheless, they usually learn a linear transformation for metric space, which is unable to capture nonlinear mapping where the hyperspectral imageries possess, especially occurs in the spectra variability and nonlinear mixing problems. To alleviate this limitation, this study investigates a new spatial-spectral adaptive sample generation and deep metric learning-based method for hyperspectral target detection (denoted as DMLTD). The proposed DMLTD employs a spatial-spectral adaptive sample generation strategy and subpixel synthetic method for background sample generation and target sample augmentation, respectively. With sufficient samples, the proposed DMLTD trains a deep discriminative metric learning network to learn hierarchical nonlinear mappings, so that to address the spectra variability and nonlinear mixing problems, thus exploiting discriminative information between targets and backgrounds for detection. Experiments and analyses conducted on three real-world hyperspectral datasets indicate that our DMLTD yields competitive performance in hyperspectral image target detection.
C1 [Zhu, Dehui; Zhang, Liangpei] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430079, Peoples R China.
   [Du, Bo] Wuhan Univ, Sch Comp Sci, Inst Artificial Intelligence, Natl Engn Res Ctr Multimedia Software, Wuhan 430079, Peoples R China.
   [Du, Bo] Wuhan Univ, Hubei Key Lab Multimedia & Network Commun Engn, Wuhan 430079, Peoples R China.
   [Dong, Yanni] China Univ Geosci, Inst Geophys & Geomat, Wuhan 430074, Peoples R China.
C3 Wuhan University; Wuhan University; Wuhan University; China University
   of Geosciences
RP Zhang, LP (corresponding author), Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430079, Peoples R China.
EM dhzhu95@163.com; gunspace@163.com; dongyanni@cug.edu.cn;
   zlp62@whu.edu.cn
RI Dong, Yanni/ADV-7368-2022
OI Dong, Yanni/0000-0003-0592-7887; Zhu, Dehui/0000-0003-1482-4433
FU National Natural Science Foundation of China [41820104006, 41871243,
   62222116, 62171417]; Science and Technology Major Project of Hubei
   Province, Next-Generation AI Technologies [2019AEA170]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 41820104006 and 41871243, in part by
   the Science and Technology Major Project of Hubei Province,
   Next-Generation AI Technologies under Grant 2019AEA170, and in part by
   the National Natural Science Foundation of China under Grants 62222116
   and 62171417. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. XinxiaoWu.
CR Nguyen B, 2017, PATTERN RECOGN, V64, P215, DOI 10.1016/j.patcog.2016.11.010
   Bioucas-Dias JM, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2244672
   Bitar AW, 2019, IEEE T GEOSCI REMOTE, V57, P5239, DOI 10.1109/TGRS.2019.2897635
   Broadwater J, 2007, IEEE T PATTERN ANAL, V29, P1891, DOI 10.1109/TPAMI.2007.1104
   Cao Q, 2013, IEEE I CONF COMP VIS, P2408, DOI 10.1109/ICCV.2013.299
   Chang CI, 2021, IEEE T GEOSCI REMOTE, V59, P5131, DOI 10.1109/TGRS.2020.3021671
   Chang CI, 2005, IEEE T GEOSCI REMOTE, V43, P502, DOI 10.1109/TGRS.2004.839543
   Chang CI, 2001, INT GEOSCI REMOTE SE, P2355, DOI 10.1109/IGARSS.2001.978000
   Chang CI, 2001, IEEE T GEOSCI REMOTE, V39, P760, DOI 10.1109/36.917889
   Chen L, 2023, IEEE T MULTIMEDIA, V25, P2710, DOI 10.1109/TMM.2022.3150185
   Chen Y, 2011, IEEE J-STSP, V5, P629, DOI 10.1109/JSTSP.2011.2113170
   Cheng G, 2018, IEEE T GEOSCI REMOTE, V56, P6712, DOI 10.1109/TGRS.2018.2841823
   Dong YN, 2022, IEEE T CYBERNETICS, V52, P11093, DOI 10.1109/TCYB.2021.3070909
   Dong YN, 2015, ISPRS J PHOTOGRAMM, V108, P138, DOI 10.1016/j.isprsjprs.2015.07.003
   Du B, 2017, IEEE T MULTIMEDIA, V19, P67, DOI 10.1109/TMM.2016.2608780
   Du B, 2016, IEEE T IMAGE PROCESS, V25, P5345, DOI 10.1109/TIP.2016.2601268
   Du Q, 2003, IEEE T GEOSCI REMOTE, V41, P1525, DOI 10.1109/TGRS.2003.813704
   Eismann MT, 2009, P IEEE, V97, P1031, DOI 10.1109/JPROC.2009.2013561
   Giannandrea A, 2013, PROC SPIE, V8743, DOI 10.1117/12.2015935
   Goetz AFH, 2009, REMOTE SENS ENVIRON, V113, pS5, DOI 10.1016/j.rse.2007.12.014
   Green RO, 1998, REMOTE SENS ENVIRON, V65, P227, DOI 10.1016/S0034-4257(98)00064-9
   Han JW, 2018, IEEE T CIRC SYST VID, V28, P2473, DOI 10.1109/TCSVT.2017.2706264
   Han JW, 2018, IEEE SIGNAL PROC MAG, V35, P84, DOI 10.1109/MSP.2017.2749125
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Kraut S, 1999, IEEE T SIGNAL PROCES, V47, P2538, DOI 10.1109/78.782198
   Kwon H, 2006, IEEE T PATTERN ANAL, V28, P178, DOI 10.1109/TPAMI.2006.39
   Kwon H., 2004, IEEE C COMP VIS PATT, P127
   Landgrebe D, 2002, IEEE SIGNAL PROC MAG, V19, P17, DOI 10.1109/79.974718
   Li K, 2020, ISPRS J PHOTOGRAMM, V159, P296, DOI 10.1016/j.isprsjprs.2019.11.023
   Li W, 2017, INT GEOSCI REMOTE SE, P5177, DOI 10.1109/IGARSS.2017.8128168
   Li W, 2015, PATTERN RECOGN, V48, P3904, DOI 10.1016/j.patcog.2015.05.024
   Li YS, 2022, IEEE T IMAGE PROCESS, V31, P1418, DOI 10.1109/TIP.2022.3141843
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu MY, 2011, PROC CVPR IEEE
   Liu YJ, 2017, IEEE T GEOSCI REMOTE, V55, P1967, DOI 10.1109/TGRS.2016.2632863
   Lu B, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12162659
   Lu XQ, 2018, IEEE T GEOSCI REMOTE, V56, P1704, DOI 10.1109/TGRS.2017.2767068
   Manolakis D., 2003, Lincoln Laboratory Journal, V14, P79
   Manolakis D, 2002, IEEE SIGNAL PROC MAG, V19, P29, DOI 10.1109/79.974724
   Manolakis D, 2014, IEEE SIGNAL PROC MAG, V31, P24, DOI 10.1109/MSP.2013.2278915
   Mignon A, 2012, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2012.6247987
   Nasrabadi NM, 2014, IEEE SIGNAL PROC MAG, V31, P34, DOI 10.1109/MSP.2013.2278992
   Pike R, 2016, IEEE T BIO-MED ENG, V63, P653, DOI 10.1109/TBME.2015.2468578
   ROBEY FC, 1992, IEEE T AERO ELEC SYS, V28, P208, DOI 10.1109/7.135446
   Shi C, 2020, IEEE T MULTIMEDIA, V22, P487, DOI 10.1109/TMM.2019.2928491
   Shi YZ, 2021, IEEE T GEOSCI REMOTE, V59, P6894, DOI 10.1109/TGRS.2020.3032528
   Theiler J, 2019, IEEE GEOSC REM SEN M, V7, P8, DOI 10.1109/MGRS.2019.2890997
   Tian J, 2008, PROCEEDINGS OF 2008 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P2964, DOI 10.1109/ICMLC.2008.4620915
   Tiwari KC, 2011, INT J APPL EARTH OBS, V13, P730, DOI 10.1016/j.jag.2011.03.007
   Wang Q, 2016, IEEE T NEUR NET LEAR, V27, P1279, DOI 10.1109/TNNLS.2015.2477537
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wu X, 2019, IEEE T GEOSCI REMOTE, V57, P5146, DOI 10.1109/TGRS.2019.2897139
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Xie WY, 2020, IEEE T GEOSCI REMOTE, V58, P2015, DOI 10.1109/TGRS.2019.2952091
   Xu XY, 2023, IEEE T PATTERN ANAL, V45, P1189, DOI 10.1109/TPAMI.2022.3152495
   Xu XY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3996
   Yan JX, 2021, PROC CVPR IEEE, P12460, DOI 10.1109/CVPR46437.2021.01228
   Yang M., 2020, Advances in Neural Information Processing Systems, V33, P17792
   Yue J, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3057768
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P5866, DOI 10.1109/TPAMI.2021.3074313
   Zhang GG, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091489
   Zhang LF, 2014, IEEE T GEOSCI REMOTE, V52, P1030, DOI 10.1109/TGRS.2013.2246837
   Zhang YX, 2015, IEEE T GEOSCI REMOTE, V53, P1346, DOI 10.1109/TGRS.2014.2337883
   Zhao R, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111310
   Zhong YF, 2015, IEEE T GEOSCI REMOTE, V53, P1411, DOI 10.1109/TGRS.2014.2340734
   Zhu DH, 2021, IEEE T GEOSCI REMOTE, V59, P6907, DOI 10.1109/TGRS.2020.3031902
   Zhu DH, 2021, IEEE T GEOSCI REMOTE, V59, P1487, DOI 10.1109/TGRS.2020.2995775
   Zhu DH, 2019, IEEE J-STARS, V12, P1254, DOI 10.1109/JSTARS.2019.2902430
NR 68
TC 7
Z9 7
U1 17
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6538
EP 6550
DI 10.1109/TMM.2022.3210389
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500064
DA 2024-07-18
ER

PT J
AU Zhu, HC
   Zhou, Y
   Li, LD
   Li, YQ
   Guo, YD
AF Zhu, Hancheng
   Zhou, Yong
   Li, Leida
   Li, Yaqian
   Guo, Yandong
TI Learning Personalized Image Aesthetics From Subjective and Objective
   Attributes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Personalized image aesthetics assessment; aesthetic attributes;
   personality traits; aesthetic prior model; aesthetic preferences
AB Due to the widespread popularity of social media, researchers have developed a strong interest in learning the personalized image aesthetics of online users. Personalized image aesthetics assessment (PIAA) aims to study the aesthetic preferences of individual users for images, which should be affected by the properties of both users and images. Existing PIAA approaches usually use the generic aesthetics learned from images as a prior model and adapt it to PIAA models through a small number of data annotated by individual users. However, the prior model merely learns the objective attributes of images, which is agnostic to the subjective attributes of users, complicating efficient learning of the personalized image aesthetics of individual users. Therefore, we propose a personalized image aesthetics assessment method that integrates the subjective attributes of users and objective attributes of images simultaneously. To characterize these two attributes jointly, an attribute extraction module is introduced to learn users' personality traits and image aesthetic attributes. Then, an aesthetic prior model is built from numerous individual users' annotated data, which leverages the personality traits of users and the aesthetic attributes of rated images as prior knowledge to model both the image aesthetic distribution and users' residual scores relative to generic aesthetics simultaneously. Finally, a PIAA model is obtained by fine-tuning the aesthetic prior model with an individual user's annotated data. Experiments demonstrate that the proposed method is superior to existing PIAA methods in learning individual users' personalized image aesthetics.
C1 [Zhu, Hancheng; Zhou, Yong] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
   [Li, Leida] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Li, Yaqian; Guo, Yandong] OPPO Res Inst, Shanghai 200032, Peoples R China.
C3 China University of Mining & Technology; Xidian University
RP Zhou, Y (corresponding author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.; Li, LD (corresponding author), Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
EM zhuhancheng@cumt.edu.cn; yzhou@cumt.edu.cn; ldli@xidian.edu.cn;
   liyaqian@oppo.com; yandong.guo@live.com
RI Wang, zhenhua/KFA-8731-2024; Wang, Zejun/KBB-8454-2024; TIAN,
   YI/KHU-9704-2024; chen, huan/KEC-2019-2024; Yin, Jing/KDO-6274-2024; Li,
   Kunpeng/KFS-6306-2024
FU National Natural Science Foundation of China [62101555, 62171340,
   61771473, 61991451]; Natural Science Foundation of Jiangsu Province
   [BK20210488, BK20201346, BK20181354]; OPPO Research Fund; Key Project of
   Shaanxi Provincial Department of Education (Collaborative Innovation
   Center) [20JY024]; Six Talent Peaks High-level Talents in Jiangsu
   Province [2015-DZXX-010, XYDXX-063]; Fundamental Research Funds for the
   Central Universities [2021QN1071, JBF211902]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62101555, 62171340, 61771473,and
   61991451, in part by the Natural Science Foundation of Jiangsu Province
   under Grants BK20210488, BK20201346, and BK20181354, in part by OPPO
   Research Fund, in part by the Key Project of Shaanxi Provincial
   Department of Education (Collaborative Innovation Center) under Grant
   20JY024, in partby Six Talent Peaks High-level Talents in Jiangsu
   Province under Grants 2015-DZXX-010 and XYDXX-063, and in part by the
   Fundamental Research Funds for the Central Universities under Grants
   2021QN1071 and JBF211902.
CR Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Cui C., IEEE T MULTIMEDIA, V21, P1209
   Cui CR, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3414843
   Cui CR, 2020, INFORM SCIENCES, V512, P780, DOI 10.1016/j.ins.2019.10.011
   de Cruys SV, 2011, I-PERCEPTION, V2, P1035, DOI 10.1068/i0466aap
   Deng X, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2043, DOI 10.1145/3132847.3133052
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Gelli F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2263, DOI 10.1145/3343031.3350574
   Guntuku SC, 2018, IEEE T AFFECT COMPUT, V9, P130, DOI 10.1109/TAFFC.2016.2581168
   Guo GJ, 2018, IEEE T MULTIMEDIA, V20, P2073, DOI 10.1109/TMM.2018.2794262
   Han-Ul Kim, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P374, DOI 10.1007/978-3-030-58577-8_23
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Karlsson K, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1061, DOI 10.1145/2647868.2655060
   Kim WH, 2020, IEEE T AFFECT COMPUT, V11, P493, DOI 10.1109/TAFFC.2018.2809752
   Kingma D. P., 2015, INT C LEARNING REPRE
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kucer M, 2018, IEEE T IMAGE PROCESS, V27, P5100, DOI 10.1109/TIP.2018.2845100
   Lee HJ, 2017, IEEE T MULTIMEDIA, V19, P1921, DOI 10.1109/TMM.2017.2687759
   Lee JT, 2019, IEEE I CONF COMP VIS, P1191, DOI 10.1109/ICCV.2019.00128
   Li LD, 2020, IEEE T IMAGE PROCESS, V29, P3898, DOI 10.1109/TIP.2020.2968285
   Lu P, 2021, IEEE T MULTIMEDIA, V23, P3618, DOI 10.1109/TMM.2020.3029882
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Lv P, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1328, DOI 10.1145/3240508.3240635
   Matthews Gerald., 2009, Personality Traits
   McCrae RR, 2009, CAMBRIDGE HANDBOOK OF PERSONALITY PSYCHOLOGY, P148
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   O'Donovan P., 2014, Proceedings of the Workshop on Computational Aesthetics, P33
   Palmer SE, 2013, ANNU REV PSYCHOL, V64, P77, DOI 10.1146/annurev-psych-120710-100504
   Pan BW, 2019, AAAI CONF ARTIF INTE, P679
   Park K, 2017, IEEE WINT CONF APPL, P1206, DOI 10.1109/WACV.2017.139
   Paszke A., 2017, PROC 31 INT C NEURAL
   Rammstedt B, 2007, J RES PERS, V41, P203, DOI 10.1016/j.jrp.2006.02.001
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Segalin C, 2017, IEEE T AFFECT COMPUT, V8, P268, DOI 10.1109/TAFFC.2016.2516994
   She DY, 2021, PROC CVPR IEEE, P8471, DOI 10.1109/CVPR46437.2021.00837
   Song XM, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P320, DOI 10.1145/3343031.3350956
   Sun WT, 2017, IEEE T MULTIMEDIA, V19, P1870, DOI 10.1109/TMM.2017.2688929
   Swami V., 2014, PERSONALITY AESTHETI
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Vinciarelli A, 2014, IEEE T AFFECT COMPUT, V5, P273, DOI 10.1109/TAFFC.2014.2330816
   Wang GL, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P957
   Wang JW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2548, DOI 10.1145/3343031.3356059
   Wang WN, 2019, IEEE IMAGE PROC, P1875, DOI [10.1109/icip.2019.8803119, 10.1109/ICIP.2019.8803119]
   Wu O, 2016, IEEE T MULTIMEDIA, V18, P1062, DOI 10.1109/TMM.2016.2538722
   Zhang XD, 2019, IEEE T MULTIMEDIA, V21, P2815, DOI 10.1109/TMM.2019.2911428
   Zhu HC, 2022, IEEE T CYBERNETICS, V52, P1798, DOI 10.1109/TCYB.2020.2984670
   Zhu HC, 2020, NEURAL PROCESS LETT, V51, P2105, DOI 10.1007/s11063-019-09987-7
   Zhu HC, 2018, PATTERN RECOGN LETT, V116, P121, DOI 10.1016/j.patrec.2018.09.027
NR 52
TC 18
Z9 18
U1 3
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 179
EP 190
DI 10.1109/TMM.2021.3123468
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400013
DA 2024-07-18
ER

PT J
AU Zhu, HQ
   Deng, JJ
   Zhang, Y
   Ji, JM
   Mao, QY
   Li, HQ
   Zhang, YY
AF Zhu, Hanqi
   Deng, Jiajun
   Zhang, Yu
   Ji, Jianmin
   Mao, Qiuyu
   Li, Houqiang
   Zhang, Yanyong
TI VPFNet: Improving 3D Object Detection With Virtual Point Based LiDAR and
   Stereo Data Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D object detection; multiple sensors; point clouds; stereo images
ID R-CNN
AB It has been well recognized that fusing the complementary information from depth-aware LiDAR point clouds and semantic-rich stereo images would benefit 3D object detection. Nevertheless, it is non-trivial to explore the inherently unnatural interaction between sparse 3D points and dense 2D pixels. To ease this difficulty, the recent approaches generally project the 3D points onto the 2D image plane to sample the image data and then aggregate the data at the points. However, these approaches often suffer from the mismatch between the resolution of point clouds and RGB images, leading to sub-optimal performance. Specifically, taking the sparse points as the multi-modal data aggregation locations causes severe information loss for high-resolution images, which in turn undermines the effectiveness of multi-sensor fusion. In this paper, we present VPFNet -a new architecture that cleverly aligns and aggregates the point cloud and image data at the "virtual" points. Particularly, with their density lying between that of the 3D points and 2D pixels, the virtual points can nicely bridge the resolution gap between the two sensors, and thus preserve more information for processing. Moreover, we also investigate the data augmentation techniques that can be applied to both point clouds and RGB images, as the data augmentation has made non-negligible contribution towards 3D object detectors to date. We have conducted extensive experiments on KITTI dataset, and have observed good performance compared to the state-of-the-art methods. Remarkably, our VPFNet achieves 83.21% moderate $AP_{3D}$ and 91.86% moderate $AP_{BEV}$ on the KITTI test set. The network design also takes computation efficiency into consideration - we can achieve a FPS of 15 on a single NVIDIA RTX 2080Ti GPU.
C1 [Zhu, Hanqi; Zhang, Yu; Ji, Jianmin; Mao, Qiuyu; Zhang, Yanyong] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.
   [Deng, Jiajun; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Zhang, YY (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.
EM zhuhanqi@mail.ustc.edu.cn; dengjj@ustc.edu.cn; yuzhang@ustc.edu.cn;
   jianmin@ustc.edu.cn; qymao@mail.ustc.edu.cn; lihq@ustc.edu.cn;
   yanyongz@ustc.edu.cn
RI Deng, Jiajun/KIK-3592-2024; Ji, Jianmin/GZM-4863-2022
OI Ji, Jianmin/0000-0002-1515-0402; Zhang, Yu/0000-0001-6638-6442; Zhu,
   Hanqi/0000-0001-5315-1839; Deng, Jiajun/0000-0001-9624-7451
FU National Key Research and Development Program of China [2018AAA0100500];
   Anhui Provincial Development and Reform Commission 2020 New Energy
   Vehicle Industry Innovation Development Project
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0100500 and in part by
   Anhui Provincial Development and Reform Commission 2020 New Energy
   Vehicle Industry Innovation Development Project "Key System Research and
   Vehicle Development for Mass Production Oriented Highly Autonomous
   Driving."
CR Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI [10.1109/iccv.2019.00987, 10.1109/ICCV.2019.00987]
   Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189
   Cui YD, 2022, IEEE T INTELL TRANSP, V23, P722, DOI 10.1109/TITS.2020.3023541
   Deng JJ, 2021, IEEE T CIRC SYST VID, V31, P4722, DOI 10.1109/TCSVT.2021.3100848
   Deng JJ, 2021, AAAI CONF ARTIF INTE, V35, P1201
   Garg D., 2020, P ADV NEUR INF PROC, P22517
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Li CY, 2020, Arxiv, DOI arXiv:2003.05505
   Li PX, 2021, AAAI CONF ARTIF INTE, V35, P1930
   Liang Du, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13326, DOI 10.1109/CVPR42600.2020.01334
   Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752
   Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Liu Z, 2020, AAAI CONF ARTIF INTE, V34, P11677
   Liu ZJ, 2019, ADV NEUR IN, V32
   Lu HH, 2019, INT CONF ACOUST SPEE, P1992, DOI 10.1109/ICASSP.2019.8682746
   Pang S, 2020, IEEE INT C INT ROBOT, P10386, DOI 10.1109/IROS45743.2020.9341791
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P68, DOI 10.1007/978-3-030-58589-1_5
   Qi L, 2019, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR.2019.00313
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Sida Peng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8530, DOI 10.1109/CVPR42600.2020.00856
   Simonelli A, 2019, IEEE I CONF COMP VIS, P1991, DOI 10.1109/ICCV.2019.00208
   Sindagi VA, 2019, IEEE INT CONF ROBOT, P7276, DOI [10.1109/ICRA.2019.8794195, 10.1109/icra.2019.8794195]
   Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252
   Tengteng Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P35, DOI 10.1007/978-3-030-58555-6_3
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vora S, 2020, PROC CVPR IEEE, P4603, DOI 10.1109/CVPR42600.2020.00466
   Wang CW, 2021, PROC CVPR IEEE, P11789, DOI 10.1109/CVPR46437.2021.01162
   Wang D., 2020, P IEEECVF C COMPUTER, P12695, DOI DOI 10.1109/CVPR42600.2020.01271
   Wang Y, 2021, IEEE INT C INT ROBOT, P3383, DOI 10.1109/IROS51168.2021.9635875
   Wang Y, 2019, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2019.00864
   Wang YJ, 2021, Arxiv, DOI arXiv:2106.12735
   Wang ZX, 2019, IEEE INT C INT ROBOT, P1742, DOI [10.1109/IROS40897.2019.8968513, 10.1109/iros40897.2019.8968513]
   Xie L, 2020, AAAI CONF ARTIF INTE, V34, P12460
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Yilun Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12533, DOI 10.1109/CVPR42600.2020.01255
   Yoo JH, 2020, SELECTED PAPERS FROM THE NINETEENTH BIENNIAL IEEE CONFERENCE ON ELECTROMAGNETIC FIELD COMPUTATION (IEEE CEFC 2020), DOI [10.1109/CEFC46938.2020.9451336, 10.1007/978-3-030-58583-9_43]
   You Y., 2020, ICLR, P1
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhao X, 2019, AAAI CONF ARTIF INTE, P9267
   Zheng W, 2021, PROC CVPR IEEE, P14489, DOI 10.1109/CVPR46437.2021.01426
   Zheng W, 2021, AAAI CONF ARTIF INTE, V35, P3555
   Zhou DF, 2019, INT CONF 3D VISION, P85, DOI 10.1109/3DV.2019.00019
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 58
TC 38
Z9 39
U1 25
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5291
EP 5304
DI 10.1109/TMM.2022.3189778
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300048
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Ben, HX
   Pan, YW
   Li, YA
   Yao, T
   Hong, RC
   Wang, M
   Mei, T
AF Ben, Huixia
   Pan, Yingwei
   Li, Yehao
   Yao, Ting
   Hong, Richang
   Wang, Meng
   Mei, Tao
TI Unpaired Image Captioning With semantic-Constrained Self-Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Image recognition; Training; Visualization; Decoding; Task
   analysis; Dogs; Encoder-decoder networks; image captioning;
   self-supervised learning
AB Image captioning has been an emerging and fast-developing research topic. Nevertheless, most existing works heavily rely on large amounts of image-sentence pairs and therefore hinder the practical applications of captioning in the wild. In this paper, we present a novel Semantic-Constrained Self-learning (SCS) framework that explores an iterative self-learning strategy to learn an image captioner with only unpaired image and text data. Technically, SCS consists of two stages, i.e., pseudo pair generation and captioner re-training, iteratively producing "pseudo" image-sentence pairs via a pre-trained captioner and re-training the captioner with the pseudo pairs, respectively. Particularly, both stages are guided by the recognized objects in the image, that act as semantic constraint to strengthen the semantic alignment between the input image and the output sentence. We leverage a semantic-constrained beam search for pseudo pair generation to regularize the decoding process with the recognized objects via forcing the inclusion/exclusion of the recognized/irrelevant objects in output sentence. For captioner re-training, a self-supervised triplet loss is utilized to preserve the relative semantic similarity ordering among generated sentences with regard to the input image triplets. Moreover, an object inclusion reward and an adversarial reward are adopted to encourage the inclusion of the predicted objects in the output sentence and pursue the generation of more realistic sentences during self-critical training, respectively. Experiments conducted on both dependent and independent unpaired data validate the superiority of SCS. More remarkably, we obtain the best published CIDEr score to-date of 74.7\% on COCO Karpathy test split for unpaired image captioning.
C1 [Ben, Huixia; Hong, Richang; Wang, Meng] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
   [Pan, Yingwei; Li, Yehao; Yao, Ting; Mei, Tao] JD AI Res, CV Lab, Beijing 100105, Peoples R China.
C3 Hefei University of Technology
RP Hong, RC (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.; Yao, T (corresponding author), JD AI Res, CV Lab, Beijing 100105, Peoples R China.
EM huixiaben@mail.hfut.edu.cn; panyw.ustc@gmail.com;
   yehaoli.sysu@gmail.com; tingyao.ustc@gmail.com; hongrc.hfut@gmail.com;
   eric.mengwang@gmail.com; tmei@live.com
RI Mei, Tao/GQZ-0596-2022; Lin, Yi/KEH-1784-2024; Pan, Yingwei/T-7649-2019;
   Wang, Meng/ITR-8699-2023
OI Mei, Tao/0000-0002-5990-7307; Pan, Yingwei/0000-0002-4344-8898; Ben,
   Huixia/0000-0001-7946-8199; Yao, Ting/0000-0001-7587-101X
FU National Key Research and Development Program of China [2019YFA0706200];
   National Natural Science Foundation of China [61732007, 61932009,
   62020106007]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2019YFA0706200, and in part by the National
   Natural Science Foundation of China under Grants 61732007, 61932009, and
   62020106007.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson Peter, 2017, P 2017 C EMPIRICAL M, P936
   Artetxe Mikel., 2018, INT C LEARN REPR, DOI DOI 10.18653/V1/D18-1399
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Cao D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1685, DOI 10.1145/3343031.3351067
   Carvalho M, 2018, ACM/SIGIR PROCEEDINGS 2018, P35, DOI 10.1145/3209978.3210036
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2018, LECT NOTES COMPUT SC, V11205, P519, DOI 10.1007/978-3-030-01246-5_31
   Gu JX, 2019, IEEE I CONF COMP VIS, P10322, DOI 10.1109/ICCV.2019.01042
   Han JF, 2019, IEEE I CONF COMP VIS, P5137, DOI 10.1109/ICCV.2019.00524
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Krasin I., 2017, Openimages: A public dataset for large-scale multi-label and multi-class image classification
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Laina I, 2019, IEEE I CONF COMP VIS, P7413, DOI 10.1109/ICCV.2019.00751
   Lample G., 2018, C TRACK P
   Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li XR, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P271, DOI 10.1145/2911996.2912049
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Peng L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1202, DOI 10.1145/3343031.3350925
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Raina R., 2007, ICML, P759
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Song YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P784, DOI 10.1145/3343031.3350996
   Sutskever I, 2014, ADV NEUR IN, V27
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang QZ, 2019, PROC CVPR IEEE, P4190, DOI 10.1109/CVPR.2019.00432
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xiao XY, 2019, IEEE T MULTIMEDIA, V21, P2942, DOI 10.1109/TMM.2019.2915033
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang TH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1184, DOI 10.1145/3343031.3350969
   Yang ZL, 2016, ADV NEUR IN, V29
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yao T, 2017, PROC CVPR IEEE, P5263, DOI 10.1109/CVPR.2017.559
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yunsheng Li, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6929, DOI 10.1109/CVPR.2019.00710
   Zha ZJ, 2022, IEEE T PATTERN ANAL, V44, P710, DOI 10.1109/TPAMI.2019.2909864
NR 53
TC 21
Z9 22
U1 3
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 904
EP 916
DI 10.1109/TMM.2021.3060948
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100030
DA 2024-07-18
ER

PT J
AU Chen, XS
   Lei, CY
   Liu, D
   Wang, GX
   Tang, HH
   Zha, ZJ
   Li, HQ
AF Chen, Xusong
   Lei, Chenyi
   Liu, Dong
   Wang, Guoxin
   Tang, Haihong
   Zha, Zheng-Jun
   Li, Houqiang
TI E-Commerce Storytelling Recommendation Using Attentional Domain-Transfer
   Network and Adversarial Pre-Training
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; IEEE merchandise; Training; Testing; Task analysis;
   Streaming media; Predictive models; Adversarial learning; attentional
   domain-transfer network; cross-domain recommendation; E-commerce;
   storytelling recommendation
ID VIDEO
AB In e-commerce platforms, there is an emerging type of content that tells a "story" about some merchandise in the form of multimedia (text, images, video), which is named storytelling. Well told stories, like advertisements, can inspire users to purchase the related products. Thus, e-commerce service provider is keen to disseminate storytelling items to potentially interested users. We address this requirement by a cross-domain personalized recommendation approach. Because storytelling is a new type of content, its related user actions are much less, more sparse than product-related user actions, thus we propose to use product-domain user actions to assist the identification of user preferences and to make storytelling recommendations. Our method has two technical contributions. First, since the user behavior patterns are different across the storytelling domain and the product domain, we propose an attentional domain-transfer network, which effectively selects the relevant items in the two domains to characterize user preferences. Second, although storytelling is about product, between the two domains there is a large gap: product description is objective and categorical, like "keywords," but storytelling is close to human language. To bridge the domain gap, we propose a dual-domain contrastive adversarial learning method to pre-train the feature extractors for storytelling and product simultaneously. We conduct experiments on two industrial datasets, and the results demonstrate the advantage of our proposed method that consistently outperforms the state-of-the-art methods. Besides, our method can be used to recommend storytelling to products, which is a desired functionality for product providers. Our code and models are publicly available.
C1 [Chen, Xusong; Liu, Dong; Zha, Zheng-Jun; Li, Houqiang] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Lei, Chenyi; Wang, Guoxin; Tang, Haihong] Alibaba Grp, Hangzhou 311121, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Alibaba Group
RP Liu, D (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM cxs2016@mail.ustc.edu.cn; leichy@mail.ustc.edu.cn; dongeliu@ustc.edu.cn;
   xiaogong.wgx@taobao.com; piaoxue@taobao.com; zhazj@ustc.edu.cn;
   lihq@ustc.edu.cn
RI liu, dong/GRJ-9115-2022; Liu, DY/JPL-4171-2023; liu,
   dongsheng/IWM-1597-2023; Li, Houqiang Li/B-6259-2013; Zha,
   Zheng-Jun/AAF-8667-2020
OI Liu, Dong/0000-0001-9100-2906; Chen, Xusong/0000-0003-3425-1791
FU Natural Science Foundation of China [61772483]
FX This work was supported by the Natural Science Foundation of China under
   Grant 61772483. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Wen-Huang Cheng.
CR [Anonymous], 2019, P 27 ACM INT C MULT, DOI DOI 10.1145/3343031.3356051
   Bousmalis K, 2016, ADV NEUR IN, V29
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen QW, 2019, 1ST INTERNATIONAL WORKSHOP ON DEEP LEARNING PRACTICE FOR HIGH-DIMENSIONAL SPARSE DATA WITH KDD (DLP-KDD 2019), DOI 10.1145/3326937.3341261
   Chen XS, 2021, IEEE T MULTIMEDIA, V23, P484, DOI 10.1109/TMM.2020.2978618
   Chen XS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1146, DOI 10.1145/3240508.3240617
   Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190
   Deldjoo Y, 2016, J DATA SEMANT, V5, P99, DOI 10.1007/s13740-016-0060-9
   Devlin J., 2018, BERT PRE TRAINING DE
   Du XZ, 2020, IEEE T KNOWL DATA EN, V32, P492, DOI 10.1109/TKDE.2018.2885520
   Elkahky A, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P278, DOI 10.1145/2736277.2741667
   Ganin Y., 2015, ICML
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gao C, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P491, DOI 10.1145/3308558.3313538
   Gao JY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P127, DOI 10.1145/3123266.3123433
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu YL, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P223, DOI 10.1145/3336191.3371827
   Han TT, 2017, IEEE T MULTIMEDIA, V19, P712, DOI 10.1109/TMM.2016.2631881
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Hu GN, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P667, DOI 10.1145/3269206.3271684
   Hu Liang, 2013, P 22 INT C WORLD WID, P595, DOI DOI 10.1145/2488388.2488441
   Huang C, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2613, DOI 10.1145/3292500.3330790
   Jrvelin Kalervo, 2017, ACM SIGIR Forum, V51, P243, DOI 10.1145/3130348.3130374
   Kanagawa Heishiro, 2019, Advances in Information Retrieval. 41st European Conference on IR Research, ECIR 2019. Proceedings: Lecture Notes in Computer Science (LNCS 11438), P20, DOI 10.1007/978-3-030-15719-7_3
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lei CY, 2016, PROC CVPR IEEE, P2545, DOI 10.1109/CVPR.2016.279
   Li B., 2011, IJCAI 2011 P 22 INT, P2293, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-382
   Li B, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P2052
   Li YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1464, DOI 10.1145/3343031.3350950
   Lian JX, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P817, DOI 10.1145/3041021.3054207
   Lin TH, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P515, DOI 10.1145/3331184.3331191
   Loni Babak, 2014, Advances in Information Retrieval. 36th European Conference on IR Research, ECIR 2014. Proceedings: LNCS 8416, P656, DOI 10.1007/978-3-319-06028-6_72
   Loshchilov Ilya, 2017, ARXIV171105101
   Ma MY, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P685, DOI 10.1145/3331184.3331200
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Pi Q, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2671, DOI 10.1145/3292500.3330666
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Sheng Gao, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P161, DOI 10.1007/978-3-642-40991-2_11
   Singh A. P., 2008, P 14 ACM SIGKDD INT, P650
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HX, 2019, COMPANION OF THE WORLD WIDE WEB CONFERENCE (WWW 2019 ), P398, DOI 10.1145/3308560.3316596
   Wang JZ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P839, DOI 10.1145/3219819.3219869
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2762, DOI 10.1109/TMM.2019.2912124
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zhou GR, 2019, AAAI CONF ARTIF INTE, P5941
   Zhou GR, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1059, DOI 10.1145/3219819.3219823
   Zhu QS, 2013, IEEE INT SYM MULTIM, P219, DOI 10.1109/ISM.2013.41
   Zhuang FZ, 2010, IEEE T KNOWL DATA EN, V22, P1664, DOI 10.1109/TKDE.2009.205
NR 51
TC 8
Z9 8
U1 3
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 506
EP 518
DI 10.1109/TMM.2021.3054525
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300039
DA 2024-07-18
ER

PT J
AU Emara, S
   Fong, SL
   Li, BC
   Khisti, A
   Tan, WT
   Zhu, XQ
   Apostolopoulos, J
AF Emara, Salma
   Fong, Silas L.
   Li, Baochun
   Khisti, Ashish
   Tan, Wai-Tian
   Zhu, Xiaoqing
   Apostolopoulos, John
TI Low-Latency Network-Adaptive Error Control for Interactive Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Delays; Forward error correction; Streaming media; Decoding; Parity
   check codes; Internet; WebRTC; Forward error correction; streaming
   codes; teleconferencing
ID CODES; VIDEO
AB We introduce a novel network-adaptive algorithm that is suitable for alleviating network packet losses for low-latency interactive communications between a source and a destination. Our network-adaptive algorithm estimates in real-time the best parameters of a recently proposed streaming code that uses forward error correction (FEC) to correct both arbitrary and burst losses, which cause a crackling noise and undesirable jitters, respectively in audio. In particular, the destination estimates appropriate coding parameters based on its observed packet loss pattern and sends them back to the source for updating the underlying code. Besides, a new explicit construction of practical low-latency streaming codes that achieve the optimal tradeoff between the capability of correcting arbitrary losses and the capability of correcting burst losses is used. Simulation evaluations based on statistical losses and real-world packet loss traces reveal the following: (i) Our proposed network-adaptive algorithm combined with our optimal streaming codes can achieve significantly higher performance compared to uncoded and non-adaptive FEC schemes over UDP (User Datagram Protocol); (ii) Our explicit streaming codes can significantly outperform traditional MDS (maximum-distance separable) streaming schemes when they are used along with our network-adaptive algorithm. In addition, we study different factors that can affect the performance of our network-adaptive algorithm.
C1 [Fong, Silas L.] Qualcomm Flar Technol, Bridgewater Township, NJ 08807 USA.
   [Emara, Salma; Li, Baochun; Khisti, Ashish] Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.
   [Tan, Wai-Tian; Apostolopoulos, John] Cisco Syst, San Jose, CA 95134 USA.
   [Zhu, Xiaoqing] Netflix Inc, 100 Winchester Cir, Los Gatos, CA 95032 USA.
C3 Qualcomm; University of Toronto; Cisco Systems Inc; Netflix, Inc.
RP Emara, S (corresponding author), Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.
EM salma@ece.utoronto.ca; silas.fong@ieee.org; bli@ece.utoronto.edu;
   akhisti@ece.utoronto.ca; dtan2@cisco.com; xzhu@netflix.com;
   johnapos@cisco.com
RI Khisti, Ashish J/F-9908-2010; Emara, Salma/AAX-5612-2021; Fong, Silas
   L./I-5086-2019; baochun, Li/AAD-3188-2022
OI Emara, Salma/0000-0002-0913-5855; Fong, Silas L./0000-0002-8762-5294; 
FU Cisco Systems Inc.; NSERC Collaborative Research and Development (CRD)
   grant
FX This work was support in part by the Cisco Systems Inc., as well as a
   NSERC Collaborative Research and Development (CRD) grant.
CR [Anonymous], 1988, Linear Least Squares Computations
   [Anonymous], 2010, TS26346 3GPP
   Badr A, 2017, IEEE SIGNAL PROC MAG, V34, P95, DOI 10.1109/MSP.2016.2639062
   Chen H, 2020, IEEE T MULTIMEDIA, V22, P459, DOI 10.1109/TMM.2019.2928497
   Chen S., 2020, IEEE C EVOL COMPUTAT, P1
   Cohen A, 2020, IEEE T COMMUN, V68, P4325, DOI 10.1109/TCOMM.2020.2989827
   ETSI, 2014, DVBS2ETSIEN3023071
   ETSI, 2016, TS102 ETSI
   Fong SL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P438, DOI 10.1145/3343031.3350942
   Fong SL, 2019, IEEE T INFORM THEORY, V65, P4274, DOI 10.1109/TIT.2019.2894124
   FRITCHMAN BD, 1967, IEEE T INFORM THEORY, V13, P221, DOI 10.1109/TIT.1967.1053975
   GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/tit.1962.1057683
   Hameed A, 2016, IEEE T MULTIMEDIA, V18, P764, DOI 10.1109/TMM.2016.2525862
   Hohlfeld Oliver, 2008, 2008 16th International Workshop on Quality of Service, P239, DOI 10.1109/IWQOS.2008.33
   Holmer S, 2013, IEEE IMAGE PROC, P1860, DOI 10.1109/ICIP.2013.6738383
   Huang TY, 2010, IEEE NETWORK, V24, P42, DOI 10.1109/MNET.2010.5430143
   ITU, 1998, G711 ITU
   ITU, 2003, G714 ITU
   ITU, 2007, P8622 ITU
   Joshi G., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2856, DOI 10.1109/ISIT.2012.6284046
   Kochmanand Y., 2012, P IEEE C COMP COMM W, P227
   Krishnan MN, 2018, IEEE INT SYMP INFO, P1809, DOI 10.1109/ISIT.2018.8437570
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   MacKay DJC, 1997, ELECTRON LETT, V33, P457, DOI 10.1049/el:19970362
   Malak D, 2019, IEEE J SEL AREA COMM, V37, P809, DOI 10.1109/JSAC.2019.2898747
   Roca V., 2020, SLIDINGWINDOW RANDOM
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Stankovic V, 2004, IEEE T MULTIMEDIA, V6, P240, DOI 10.1109/TMM.2003.822789
   Stockhammer T, 2005, IEEE WIREL COMMUN, V12, P6, DOI 10.1109/MWC.2005.1497853
   Valin J., 2012, DEFINITION OPUS AUDI
   Wang J., 2010, THESIS MASSACHUSETTS
   Watson M., 2012, RAPTOR FORWARD ERROR
   Wu JY, 2018, IEEE T MULTIMEDIA, V20, P457, DOI 10.1109/TMM.2017.2741425
   Yang XK, 2005, IEEE T MULTIMEDIA, V7, P753, DOI 10.1109/TMM.2005.846782
   Yuan L, 2016, IEEE T MULTIMEDIA, V18, P1389, DOI 10.1109/TMM.2016.2557079
NR 35
TC 2
Z9 3
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1691
EP 1706
DI 10.1109/TMM.2021.3070134
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200034
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Feng, FX
   Niu, TR
   Li, RF
   Wang, XJ
AF Feng, Fangxiang
   Niu, Tianrui
   Li, Ruifan
   Wang, Xiaojie
TI Modality Disentangled Discriminator for Text-to-Image Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Correlation; Image synthesis; Image reconstruction;
   Generative adversarial networks; Image representation; Visualization;
   text-to-image synthesis; generative adversarial networks; multi-modal
   disentangled representation learning
AB Text-to-image (T2I) synthesis aims at generating photo-realistic images from text descriptions, which is a particularly important task in bridging vision and language. Each generated image consists of two parts: the content part related to the text and the style part irrelevant to the text. The existing discriminator does not distinguish between the content part and the style part. This not only precludes the T2I synthesis models from generating the content part effectively but also makes it difficult to manipulate the style of the generated image. In this paper, we propose a modality disentangled discriminator that distinguishes between the content part and the style part at a specific layer. Specifically, we enforce the early layers of a certain number in the discriminator to become the disentangled representation extractor through two losses. The extracted common representation for the content part can make the discriminator more effective for capturing the text-image correlation, while the extracted modality-specific representation for the style part can be directly transferred to other images. The combination of these two representations can also improve the quality of the generated images. Our proposed discriminator is used to substitute the discriminator of each stage in the representative model AttnGAN and the SOTA model DM-GAN. Extensive experiments are conducted on three widely used datasets, i.e. CUB, Oxford-102, and COCO, for the T2I synthesis task, demonstrating the superior performance of the modality disentangled discriminator over the base models. Code for DM-GAN with our modality disentangled discriminator is available at https://github.com/FangxiangFeng/DM-GAN-MDD.
C1 [Feng, Fangxiang; Niu, Tianrui; Li, Ruifan; Wang, Xiaojie] Beijing Univ Posts & Commun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
   [Feng, Fangxiang; Li, Ruifan; Wang, Xiaojie] Minist Educ, Engn Res Ctr Informat Networks, Beijing 100876, Peoples R China.
RP Feng, FX (corresponding author), Beijing Univ Posts & Commun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.; Feng, FX (corresponding author), Minist Educ, Engn Res Ctr Informat Networks, Beijing 100876, Peoples R China.
EM f.fangxiang@gmail.com; niwtr@bupt.edu.cn; rfli@bupt.edu.cn;
   xjwang@bupt.edu.cn
RI Xiaojie, Wang/T-5052-2019; LI, Ruifan/AFM-1702-2022
OI Niu, Tianrui/0000-0001-7743-3822; Feng, Fangxiang/0000-0002-4798-4233
FU National Key Research, and Development Program of China
   [2020YFF0305302]; National Natural Science Foundation of China
   [61906018, 62076032]; Fundamental Research Funds for the Central
   Universities [2021RC36]
FX This work was supported in part by the National Key Research, and
   Development Program of China under Grant of 2020YFF0305302, in part by
   the National Natural Science Foundation of China under Grants 61906018,
   and 62076032, in part by the Fundamental Research Funds for the Central
   Universities [2021RC36].
CR Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Binkowski Mikolaj, 2020, P ICLR
   Chen R., 2020, PROC IEEECVF C COMPU, P8165, DOI DOI 10.1109/CVPR42600.2020.00819
   Chen X, 2016, ADV NEUR IN, V29
   Chen Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P647, DOI 10.1145/3343031.3350937
   Donahue C., 2018, Adversarial audio synthesis
   Frolov S., 2021, ARXIV210109983
   Gonzalez-Garcia A, 2018, ADV NEUR IN, V31
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Havrylov S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1118
   Hensel M, 2017, ADV NEUR IN, V30
   Hsu W.-N., 2017, Advances in neural information processing systems, P1878
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Jiadong Liang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P491, DOI 10.1007/978-3-030-58548-8_29
   Jun Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10908, DOI 10.1109/CVPR42600.2020.01092
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li B., 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P7880
   Li B., 2020, Advances in Neural Information Processing Systems, V33, P22020, DOI [10.48550/arxiv.2010.12136, DOI 10.48550/ARXIV.2010.12136]
   Li B., 2019, ADV NEURAL INFORM PR, P2065
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Lin JX, 2021, IEEE T PATTERN ANAL, V43, P1254, DOI 10.1109/TPAMI.2019.2950198
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu MY, 2017, ADV NEUR IN, V30
   Liu YB, 2020, AAAI CONF ARTIF INTE, V34, P4916
   Liu Y, 2018, PROC CVPR IEEE, P2080, DOI 10.1109/CVPR.2018.00222
   Locatello F, 2019, PR MACH LEARN RES, V97
   Ma JX, 2019, ADV NEUR IN, V32
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Odena A, 2017, PR MACH LEARN RES, V70
   Pan SY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4244, DOI 10.1145/3394171.3413636
   Pan YW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1789, DOI 10.1145/3123266.3127905
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Radford A., 2015, ARXIV
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Reed S, 2016, PR MACH LEARN RES, V48
   Saha A, 2018, IEEE WINT CONF APPL, P557, DOI 10.1109/WACV.2018.00067
   Salimans T, 2016, ADV NEUR IN, V29
   Tan HC, 2019, IEEE I CONF COMP VIS, P10500, DOI 10.1109/ICCV.2019.01060
   Tao Ming, 2020, ARXIV200805865
   Tsai YH, 2019, IEEE IJCNN, DOI [10.1109/ijcnn.2019.8852023, 10.1109/ipcon.2019.8908433]
   Vondrick C., 2016, ADV NEURAL INFORM PR, P613, DOI DOI 10.13016/M26GIH-TNYZ
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang CY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2901
   Wang T., 2018, ARXIV
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Yang XW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P374, DOI 10.1145/3240508.3240716
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Yuheng Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8036, DOI 10.1109/CVPR42600.2020.00806
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhu B, 2020, PROC CVPR IEEE, P5518, DOI 10.1109/CVPR42600.2020.00556
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 61
TC 6
Z9 8
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2112
EP 2124
DI 10.1109/TMM.2021.3075997
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200026
DA 2024-07-18
ER

PT J
AU Gao, LL
   Huang, ZJ
   Song, JK
   Yang, Y
   Shen, HT
AF Gao, Lianli
   Huang, Zijie
   Song, Jingkuan
   Yang, Yang
   Shen, Heng Tao
TI Push & Pull: Transferable Adversarial Examples With Attentive Attack
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Perturbation methods; Feature extraction; Computational modeling; Task
   analysis; Predictive models; Neural networks; Iterative methods; Image
   classification; adversarial attack; transferability; targeted attack
AB Targeted attack aims to mislead the classification model to a specific class, and it can be further divided into black-box and white-box targeted attack depending on whether the classification model is known. A growing number of approaches rely on disrupting the image representations to craft adversarial examples. However, this type of methods often suffer from either low white-box targeted attack success rate or poor black-box targeted attack transferability. To address these problems, we propose a Transferable Attentive Attack (TAA) method which adds perturbation to clean images based on the attended regions and features. This is motivated by one important observation that deep-learning based classification models (or even shallow-learning based models like SIFT) make the prediction mainly based on the informative and discriminative regions of an image. Specifically, the corresponding features of the informative regions are firstly extracted, and the anchor image's features are iteratively "pushed" away from the source class and simultaneously "pulled" closer to the target class along with attacking. Moreover, we introduce a new strategy that the attack selects the centroids of source and target class cluster as the input of triplet loss to achieve high transferability. Experimental results demonstrate that our method improves the transferability of adversarial example, while maintaining higher success rate for white-box targeted attacks compared with the state-of-the-arts. In particular, TAA attacks on image-representation based task like VQA also result in a significant performance drop in terms of accuracy.
C1 [Gao, Lianli; Song, Jingkuan] Univ Elect Sci & Technol China, Inst Neurol, Sichuan Prov Peoples Hosp, Chengdu 611731, Peoples R China.
   [Gao, Lianli; Huang, Zijie; Yang, Yang; Shen, Heng Tao] Univ Elect Sci & Technol China, Future Media Ctr, Chengdu 611731, Peoples R China.
   [Gao, Lianli; Huang, Zijie; Yang, Yang; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China; Sichuan
   Provincial People's Hospital; University of Electronic Science &
   Technology of China; University of Electronic Science & Technology of
   China
RP Song, JK (corresponding author), Univ Elect Sci & Technol China, Inst Neurol, Sichuan Prov Peoples Hosp, Chengdu 611731, Peoples R China.
EM lianli.gao@uestc.edu.cn; Zijie-Huang@Outlook.com;
   jinglcuan.song@gmail.com; dlyyang@gmail.com; shenhengtao@hotmail.com
RI yang, yang/HGT-7999-2022; Shen, Heng Tao/ABD-5331-2021; yang,
   yang/GVT-5210-2022; Lang, Ming/HIK-0758-2022
OI song, jingkuan/0000-0002-2549-8322
FU National Key Research and Development Program of China [2018AAA0102200];
   National Natural Science Foundation of China [61772116, 61872064,
   62020106008]; Sichuan Science and Technology Program [2019JDTD0005];
   Open Project of Zhejiang Laboratory [2019KD0AB05]; Open Project of Key
   Laboratory of Artificial Intelligence, Ministry of Education [AI2019005]
FX This work is supported in part by National Key Research and Development
   Program of China under Grant 2018AAA0102200, in part by the National
   Natural Science Foundation of China under Grants 61772116, 61872064,
   62020106008, in part by Sichuan Science and Technology Program under
   Grant 2019JDTD0005, in part by The Open Project of Zhejiang Laboratory
   under Grant 2019KD0AB05 and in part by the Open Project of Key
   Laboratory of Artificial Intelligence, Ministry of Education under Grant
   AI2019005.
CR Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chaoning Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14509, DOI 10.1109/CVPR42600.2020.01453
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Duan YQ, 2018, PROC CVPR IEEE, P2780, DOI 10.1109/CVPR.2018.00294
   Ganeshan A, 2019, IEEE I CONF COMP VIS, P8068, DOI 10.1109/ICCV.2019.00816
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu GY, 2020, IEEE T MULTIMEDIA, V22, P2207, DOI 10.1109/TMM.2019.2953325
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Q, 2019, IEEE I CONF COMP VIS, P4732, DOI 10.1109/ICCV.2019.00483
   Inkawhich N, 2019, PROC CVPR IEEE, P7059, DOI 10.1109/CVPR.2019.00723
   Jeddi A, 2020, PROC CVPR IEEE, P1238, DOI 10.1109/CVPR42600.2020.00132
   Kazemi V., 2017, ARXIV170403162
   Kurakin A., 2017, P INT C LEARN REPR O
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y., 2017, P INT C LEARN REPR O
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mao CZ, 2019, ADV NEUR IN, V32
   Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Papernot N., 2016, CORR
   Papernot N, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P506, DOI 10.1145/3052973.3053009
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Rozsa A, 2016, IEEE COMPUT SOC CONF, P410, DOI 10.1109/CVPRW.2016.58
   Sabour S., 2016, P INT C LEARN REPR
   Shen F., IEEE T IMAGE PROCESS, V25, P5610
   Simonyan K., 2014, CORR
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858
   Sugase Y, 1999, NATURE, V400, P869, DOI 10.1038/23703
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Vladu Adrian, 2018, PROC 6 INT C LEARN R
   Wang YL, 2020, IEEE T MULTIMEDIA, V22, P1796, DOI 10.1109/TMM.2019.2949872
   Wei XX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P954
   Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284
   Xu X, 2020, IEEE T CYBERNETICS, V50, P2400, DOI 10.1109/TCYB.2019.2928180
   Yosinski J, 2014, ADV NEUR IN, V27
   Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
NR 43
TC 20
Z9 20
U1 3
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2329
EP 2338
DI 10.1109/TMM.2021.3079723
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1F3XL
UT WOS:000795103700001
DA 2024-07-18
ER

PT J
AU Huang, XP
   An, P
   Chen, YL
   Liu, DY
   Shen, LQ
AF Huang, Xinpeng
   An, Ping
   Chen, Yilei
   Liu, Deyang
   Shen, Liquan
TI Low Bitrate Light Field Compression With Geometry and Content
   Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Bit rate; Image reconstruction; Geometry; Encoding;
   Prediction algorithms; Decoding; Light field compression; low bitrate;
   disparity; consistency
ID REPRESENTATION; DEPTH
AB Light field imaging can simultaneously record the position and direction information of light rays; thus, digital refocusing and full depth-of-field extension - functions that are inaccessible for conventional images - can be achieved using the structural consistency of light field data. To meet the challenges of limited bandwidth and storage, such vast numbers of light field data must be compressed to a low bitrate. However, current compression solutions ignore the intrinsic consistency of light fields in pursuit of a low bitrate, thereby leading to the loss of light field capabilities. To solve this issue, this work focuses on structural consistency to achieve efficient light field compression with a low bitrate. The proposed light field compression method encodes the sparsely selected sub-aperture images (SAIs) and the disparity maps corresponding to the unselected SAIs. From the perspective of geometry consistency, the consistency of the initially estimated disparity maps is improved by using a color-guided refinement algorithm, thereby reducing the bitrate of the disparity maps. From the perspective of content consistency, the consistency of the SAI-transformed pseudo sequence is improved by the proposed content-similarity-based arrangement algorithm along with a specific prediction structure; thereby, the bitrate of the sparsely selected SAIs is reduced. The experimental results show that the proposed compression method can reduce the total bitrate while preserving good structural consistency.
C1 [Huang, Xinpeng; An, Ping; Chen, Yilei; Shen, Liquan] Shanghai Univ, Sch Commun & Informat Engn, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Huang, Xinpeng; An, Ping; Chen, Yilei; Shen, Liquan] Minist Educ, Key Lab Adv Display & Syst Applicat, Shanghai 200072, Peoples R China.
   [Liu, Deyang] Anqing Normal Univ, Sch Comp & Informat, Anqing 246000, Peoples R China.
C3 Shanghai University; Anqing Normal University
RP An, P (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.; An, P (corresponding author), Minist Educ, Key Lab Adv Display & Syst Applicat, Shanghai 200072, Peoples R China.
EM xinpeng_huang@163.com; anping@shu.edu.cn; yileichen@shu.edu.cn;
   deyangliu@hotmail.com; jsslq@163.com
RI Liu, Deyang/AAB-1184-2020; Huang, xp/JRX-2837-2023; Shen,
   Liquan/D-4832-2012
OI Huang, Xinpeng/0000-0002-2373-642X; Shen, Liquan/0000-0002-2148-6279;
   Chen, Yilei/0000-0002-8811-3549
FU NSFC [62 001 279, 62020106011, 61 828 105, 61 801 006]; China
   Postdoctoral Science Foundation [2020M671073]
FX This work was supported in part by the NSFC under Grants 62 001 279,
   62020106011, 61 828 105 and 61 801 006, and in part by Project funded by
   China Postdoctoral Science Foundation under Grant 2020M671073. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Zixiang Xiong.
CR [Anonymous], 2005, 200502 CSTR
   Bakir N, 2018, IEEE IMAGE PROC, P1128, DOI 10.1109/ICIP.2018.8451597
   Bjotegaard G., 2001, VCEGM33
   BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525
   BRITES C, IN PRESS, DOI DOI 10.1109/TCSVT.2020.2976784
   Chen J, 2018, IEEE T IMAGE PROCESS, V27, P314, DOI 10.1109/TIP.2017.2750413
   Chen YL, 2020, IEEE SIGNAL PROC LET, V27, P1135, DOI 10.1109/LSP.2020.3003533
   Chuchvara A, 2020, IEEE T IMAGE PROCESS, V29, P2492, DOI 10.1109/TIP.2019.2959233
   Conceiçao R, 2018, IEEE IMAGE PROC, P3174, DOI 10.1109/ICIP.2018.8451345
   Conti C, 2018, IEEE T MULTIMEDIA, V20, P2905, DOI 10.1109/TMM.2018.2825882
   Conti C, 2016, SIGNAL PROCESS-IMAGE, V42, P59, DOI 10.1016/j.image.2016.01.008
   Dai F, 2015, IEEE IMAGE PROC, P4733, DOI 10.1109/ICIP.2015.7351705
   Dansereau DG, 2013, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2013.137
   Dib E, 2019, IEEE IMAGE PROC, P3751, DOI [10.1109/icip.2019.8803756, 10.1109/ICIP.2019.8803756]
   Ebrahimi T, 2016, IEEE MULTIMEDIA, V23, P14, DOI 10.1109/MMUL.2016.64
   Farrugia RA, 2020, IEEE T PATTERN ANAL, V42, P1162, DOI 10.1109/TPAMI.2019.2893666
   Ghassab VK, 2020, IEEE T MULTIMEDIA, V22, P1447, DOI 10.1109/TMM.2019.2946094
   Gu JW, 2019, IEEE INT CON MULTI, P344, DOI 10.1109/ICME.2019.00067
   Han HX, 2017, IEEE INT CON MULTI, P1177, DOI 10.1109/ICME.2017.8019424
   Hou JH, 2019, IEEE T CIRC SYST VID, V29, P517, DOI 10.1109/TCSVT.2018.2802943
   Huang XP, 2019, OPT EXPRESS, V27, P3557, DOI 10.1364/OE.27.003557
   HUANG XJ, 2018, IEEE INT CON MULTI, P1
   Ihrke I, 2016, IEEE SIGNAL PROC MAG, V33, P59, DOI 10.1109/MSP.2016.2582220
   Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762
   Jiang F, 2017, IEEE INT CON MULTI, P67, DOI 10.1109/ICME.2017.8019522
   Jiang X, 2017, INT CONF ACOUST SPEE, P1313, DOI 10.1109/ICASSP.2017.7952369
   Jin X, 2018, IEEE T IMAGE PROCESS, V27, P3954, DOI 10.1109/TIP.2018.2832449
   Jin X, 2017, IEEE J-STSP, V11, P1173, DOI 10.1109/JSTSP.2017.2741108
   Levoy M, 2006, ACM T GRAPHIC, V25, P924, DOI 10.1145/1141911.1141976
   Li L, 2017, IEEE J-STSP, V11, P1107, DOI 10.1109/JSTSP.2017.2725198
   Li Y, 2016, IEEE T CIRC SYST VID, V26, P1308, DOI 10.1109/TCSVT.2015.2450333
   Lin Q, 2020, CELL, V180, P536, DOI 10.1016/j.cell.2019.12.018
   Lin RJ, 2019, NAT NANOTECHNOL, V14, P227, DOI 10.1038/s41565-018-0347-0
   Liu D., 2016, ARXIV PREPRINT ARXIV, P1, DOI DOI 10.1109/ICMEW.2016.7574674
   Liu DY, 2020, IEEE T MULTIMEDIA, V22, P846, DOI 10.1109/TMM.2019.2934426
   Liu W, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2612826
   Mehajabin N, 2019, IEEE IMAGE PROC, P3567, DOI [10.1109/icip.2019.8803668, 10.1109/ICIP.2019.8803668]
   Meng CL, 2020, IEEE SIGNAL PROC LET, V27, P525, DOI 10.1109/LSP.2020.2982060
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Monteiro RJS, 2017, IEEE J-STSP, V11, P1120, DOI 10.1109/JSTSP.2017.2721358
   Rizkallah M, 2020, IEEE T IMAGE PROCESS, V29, P602, DOI 10.1109/TIP.2019.2928873
   Shin C, 2018, PROC CVPR IEEE, P4748, DOI 10.1109/CVPR.2018.00499
   Trottnow J, 2019, SA'19: SIGGRAPH ASIA 2019 TECHNICAL BRIEFS, P71, DOI 10.1145/3355088.3365158
   Verhack R, 2020, IEEE T MULTIMEDIA, V22, P579, DOI 10.1109/TMM.2019.2932614
   Wang J, 2020, IEEE DATA COMPR CONF, P397, DOI 10.1109/DCC47342.2020.00047
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Zhao ZN, 2019, IEEE ACM T COMPUT BI, V16, P1753, DOI 10.1109/TCBB.2017.2706682
   Zhong R, 2019, IEEE T CIRC SYST VID, V29, P1116, DOI 10.1109/TCSVT.2018.2826052
NR 48
TC 16
Z9 17
U1 1
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 152
EP 165
DI 10.1109/TMM.2020.3046860
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300012
DA 2024-07-18
ER

PT J
AU Jin, Y
   Jiang, WH
   Yang, Y
   Mu, YD
AF Jin, Yang
   Jiang, Wenhao
   Yang, Yi
   Mu, Yadong
TI Zero-Shot Video Event Detection With High-Order Semantic Concept
   Discovery and Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Event detection; Visualization; Libraries; Streaming media;
   Task analysis; Feature extraction; Multimedia event detection; zero-shot
   learning; high-order concept
ID RECOGNITION; FEATURES
AB Multimedia event detection aims to precisely retrieve videos that contain complex semantic events from a large pool. This work addresses this task under a zero-shot setting, where only brief event-specific textural information (such as event names, a few descriptive sentences, etc.) is known yet none positive video example is provided. Mainstream approaches to tackling this task are middle-level semantic concept-based, where meticulously-crafted concept banks (e.g., LSCOM) are adopted. We argue that these concept banks are still inadequate facing video semantic complexity. Existing semantic concepts are essentially first-order, mainly designed for atomic objects, scenes or human actions, etc. This work advocates the utilization of high-order concepts (such as subject-predicate-object triplets or adjective-object). The main contributions are two-fold. First, we harvest a comprehensive albeit compact high-order concept library through distilling information from three large public datasets (MS-COCO, Visual Genome, and Kinetics-600), mainly related to visual relations and human-object interactions. Secondly, zero-shot events are often only briefly and partially described via textual input. The resultant semantic ambiguity makes the pursuit of the most indicative high-order concepts challenging. We thus design a novel query-expanding scheme that enriches ambiguous event-specific keywords by searching over either large common knowledge bases (e.g., WikiHow) or top-ranked webpages retrieved from modern search engines. This way sets up a more faithful connection between zero-shot events and high-order concepts. To our best knowledge, this is the first work that strives for concept-based video search beyond first-order concepts. Extensive experiments have been conducted on several large video benchmarks (TRECVID 2013, TRECVID 2014, and ActivityNet-1.3). The evaluations clearly demonstrate the superiority of our constructed high-order concept library and its complementariness to existing concepts.
C1 [Jin, Yang] Peking Univ, Ctr Big Data Res, Beijing 100871, Peoples R China.
   [Jiang, Wenhao] Tencent AI Lab, Shenzhen 518001, Peoples R China.
   [Yang, Yi] Univ Technol Sydney UTS, Fac Engn & Informat Technol, Ultimo, NSW 2007, Australia.
   [Mu, Yadong] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
C3 Peking University; Tencent; University of Technology Sydney; Peking
   University
RP Mu, YD (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
EM jiny@stu.pku.edu.cn; cswhjiang@gmail.com; yi.yang@uts.edu.au;
   muyadong@gmail.com
RI yang, yang/HGT-7999-2022; Yang, Yi/B-9273-2017; Lang,
   Ming/HIK-0758-2022; Jiang, Wenhao/M-8932-2015
OI Yang, Yi/0000-0002-0512-880X; Jiang, Wenhao/0000-0002-0795-366X
FU National Key R&D Program of China [2018AAA0100702]; National Natural
   Science Foundation of China [61772037]; Beijing Natural Science
   Foundation [Z190001]; Tencent AI Laboratory Rhino-Bird Focused Research
   Program [JR202021]
FX This work was supported in part by National Key R&D Program of China
   under Grant 2018AAA0100702, in part by the National Natural Science
   Foundation of China under Grant 61772037, in part by Beijing Natural
   Science Foundation under Grant Z190001, and in part by Tencent AI
   Laboratory Rhino-Bird Focused Research Program under Grant JR202021. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Zhu Liu.
CR Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111
   [Anonymous], 2014, P TREC VID RETR EV
   [Anonymous], 2013, P 3 ACM C INT C MULT
   Aslam Asra, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P261, DOI 10.1145/3372278.3390722
   Bucher M., 2019, NEURIPS, P466
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J., 2018, arXiv
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang XJ, 2016, AAAI CONF ARTIF INTE, P3464
   Chang XJ, 2017, IEEE T CYBERNETICS, V47, P1180, DOI 10.1109/TCYB.2016.2539546
   Chang XJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2234
   Cheng Y, 2014, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2014.286
   de Boer MHT, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3131288
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J., 2019, P 2019 C N AM CHAPTE, P4171
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Geng J, 2015, IEEE T MULTIMEDIA, V17, P498, DOI 10.1109/TMM.2015.2398195
   Habibian A, 2014, ICMR
   Habibian A, 2017, IEEE T PATTERN ANAL, V39, P2089, DOI 10.1109/TPAMI.2016.2627563
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Kanagaraj K, 2021, SIGNAL IMAGE VIDEO P, V15, P779, DOI 10.1007/s11760-020-01796-z
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, CORR ABS170506950
   Koniusz P, 2017, IEEE T PATTERN ANAL, V39, P313, DOI 10.1109/TPAMI.2016.2545667
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Lee WY, 2018, IEEE T MULTIMEDIA, V20, P142, DOI 10.1109/TMM.2017.2726184
   Li ZH, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P297, DOI 10.1145/3394486.3403072
   Li ZH, 2019, PATTERN RECOGN, V88, P595, DOI 10.1016/j.patcog.2018.12.010
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu JE, 2013, IEEE WORK APP COMP, P339, DOI 10.1109/WACV.2013.6475038
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma ZG, 2018, IEEE T NEUR NET LEAR, V29, P2921, DOI 10.1109/TNNLS.2017.2709308
   Ma ZG, 2014, IEEE T PATTERN ANAL, V36, P1789, DOI 10.1109/TPAMI.2014.2306419
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Mazloom M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P123, DOI 10.1145/2671188.2749402
   Mazloom M, 2016, IEEE T MULTIMEDIA, V18, P1378, DOI 10.1109/TMM.2016.2559947
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Over P, 2013, P TRECVID WORKSH, V2, P1
   Qin Z, 2017, IEEE T IMAGE PROCESS, V26, P5680, DOI 10.1109/TIP.2017.2745209
   Rahman S, 2019, LECT NOTES COMPUT SC, V11361, P547, DOI 10.1007/978-3-030-20887-5_34
   Ramos J, 2003, P 1 INSTRUCTIONAL C, V242, P29
   Rastegari M, 2013, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2013.425
   Shi BT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5182
   Socher R., 2013, ADV NEURAL INFORM PR, V26, P935, DOI DOI 10.1007/978-3-319-46478-7
   Song H, 2018, IEEE T MULTIMEDIA, V20, P1088, DOI 10.1109/TMM.2017.2763322
   Soomro K., 2012, CoRR, V2
   Speer R, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P3679
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Xia XJ, 2020, IEEE T MULTIMEDIA, V22, P569, DOI 10.1109/TMM.2019.2933330
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Ye GG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2733373.2806221
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang H, 2019, IEEE T MULTIMEDIA, V21, P1450, DOI 10.1109/TMM.2018.2884478
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 61
TC 7
Z9 7
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1896
EP 1908
DI 10.1109/TMM.2021.3073624
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0I7PA
UT WOS:000779607000004
DA 2024-07-18
ER

PT J
AU Liu, XY
   Meng, GF
   Chang, JL
   Hu, RG
   Xiang, SM
   Pan, CH
AF Liu, Xiyan
   Meng, Gaofeng
   Chang, Jianlong
   Hu, Ruiguang
   Xiang, Shiming
   Pan, Chunhong
TI Decoupled Representation Learning for Character Glyph Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Generative adversarial networks; Gallium nitride;
   Topology; Standards; Electronic mail; Decoding; Character glyph
   synthesis; decoupled representation; generative adversarial networks
ID IMAGE; TEXT
AB Character glyph synthesis is still an open challenging problem, which involves two related aspects, i.e., font style transfer and content consistency. In this paper, we propose a novel model named FontGAN, which integrates the character structure stylization, de-stylization and texture transfer into a unified framework. Specifically, we decouple character images into style representation and content representation, which offers fine-grained control of these two types of variables, thus improving the quality of the generated results. To effectively capture the style information, a style consistency module (SCM) is introduced. Technically, SCM exploits category-guided Kullback-Leibler divergence to explicitly model the style representation into different prior distributions. In this way, our model is capable of implementing transformations between multiple domains in one framework. In addition, we propose content prior module (CPM) to provide content prior for the model to guide the content encoding process and alleviates the problem of stroke deficiency during structure de-stylization. Benefiting from the idea of decoupling and regrouping, our FontGAN suffices to achieve many-to-many translation tasks for glyph structure. Experimental results demonstrate that the proposed FontGAN achieves the state-of-the-art performance in character glyph synthesis.
C1 [Liu, Xiyan; Meng, Gaofeng; Chang, Jianlong; Xiang, Shiming; Pan, Chunhong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Liu, Xiyan; Meng, Gaofeng; Chang, Jianlong; Xiang, Shiming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Meng, Gaofeng] Chinese Acad Sci, HK Inst Sci & Innovat, Ctr Artificial Intelligence & Robot, Hong Kong 999077, Peoples R China.
   [Hu, Ruiguang] Beijing Aerosp Automat Control Inst, Beijing 100854, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences
RP Meng, GF (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Meng, GF (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM xiyan.liu@nlpr.ia.ac.cn; gfmeng@nlpr.ia.ac.cn;
   jianlong.chang@nlpr.ia.ac.cn; rghu258@163.com; smxiang@nlpr.ia.ac.cn;
   chpan@nlpr.ia.ac.cn
OI Liu, Xiyan/0000-0002-0102-9636
FU National Natural Science Foundation of China [61976208, 62071466,
   62076242]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61976208, 62071466, and 62076242.
CR [Anonymous], 2019, P IEEECVF INT C COMP
   [Anonymous], 2017, ZI2ZI
   [Anonymous], 2017, REWRITE
   Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789
   Balashova E, 2019, COMPUT GRAPH FORUM, V38, P429, DOI 10.1111/cgf.13540
   Chang B, 2018, IEEE WINT CONF APPL, P199, DOI 10.1109/WACV.2018.00028
   Chen X, 2016, ADV NEUR IN, V29
   Clark Aidan, 2019, Adversarial video generation on complex datasets
   Donahue C., 2019, Adversarial audio synthesis
   Esser P, 2019, IEEE I CONF COMP VIS, P2699, DOI 10.1109/ICCV.2019.00279
   Gao Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356574
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu Q, 2019, IEEE I CONF COMP VIS, P10480, DOI 10.1109/ICCV.2019.01058
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu CC, 2017, INTERSPEECH, P3364, DOI 10.21437/Interspeech.2017-63
   Hsu WN, 2019, INT CONF ACOUST SPEE, P5901, DOI 10.1109/ICASSP.2019.8683561
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang Y, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149440
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kingma D. P., 2014, arXiv
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Lian Z., 2016, SIGGRAPH ASIA 2016 T, P1, DOI DOI 10.1145/3005358.3005371
   Lian Z., 2012, P SIGGRAPH AS 2012 T, P2, DOI DOI 10.1145/2407746.2407748
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600
   Liu XY, 2018, INT C PATT RECOG, P988, DOI 10.1109/ICPR.2018.8545383
   Liu Y, 2018, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR.2018.00394
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Lyu PY, 2017, PROC INT CONF DOC, P1095, DOI 10.1109/ICDAR.2017.181
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Miyazaki T, 2020, IEEE COMPUT GRAPH, V40, P99, DOI 10.1109/MCG.2019.2931431
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Siddharth N., 2017, NIPS
   Sun DY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P920
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Villegas R., 2017, International Conference on Machine Learning, P3560
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Xu SH, 2009, IEEE INTELL SYST, V24, P44, DOI 10.1109/MIS.2009.23
   Yang S, 2019, IEEE I CONF COMP VIS, P4441, DOI 10.1109/ICCV.2019.00454
   Yang S, 2019, AAAI CONF ARTIF INTE, P1238
   Yang S, 2017, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR.2017.308
   Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243
   You S, 2018, IEEE T PATTERN ANAL, V40, P505, DOI 10.1109/TPAMI.2017.2675980
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang YX, 2018, PROC CVPR IEEE, P8447, DOI 10.1109/CVPR.2018.00881
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zong A, 2014, AAAI CONF ARTIF INTE, P3024
NR 54
TC 6
Z9 6
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1787
EP 1799
DI 10.1109/TMM.2021.3072449
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200004
DA 2024-07-18
ER

PT J
AU Ma, XQ
   Liu, WF
   Tian, Q
   Gao, Y
AF Ma, Xueqi
   Liu, Weifeng
   Tian, Qi
   Gao, Yue
TI Learning Representation on Optimized High-Order Manifold for Visual
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High-order manifold; hypergraph; graph neural networks; visual
   classification
ID P-LAPLACIAN; HYPERGRAPH; NETWORKS
AB Graph convolutional networks (GCNs) and graph neural networks (GNNs) have demonstrated convincing performance on many tasks by learning the intrinsic structure of the data. However, it is still valuable and challenging to consider the complex and complete correlations of objects, i.e., high-order manifold structures, for representation learning. In this paper, we present a novel representation learning method that utilizes the optimized high-order manifold of the data for classification tasks of nonstructural data and graph-structure data. In the method, we fully explore the complicated relationship of samples by highlighting the high-order manifold information in a hypergraph. Specifically, we incorporate high-order manifold information by graph p-Laplacian into a hypergraph and propose p-Laplacian-based hypergraph neural networks (pLapHGNN) to significantly learn hidden layer representations that encode both the high-order structure of data and the high-order manifold geometrical information. Confronting the difficulties of obtaining optimized high-order manifolds of the data, we propose an effective approximate approach by graph p-Laplacian representing the relationship of hyperedges in the hypergraph. Furthermore, we study the weights of hyperedges in a hypergraph with high-order manifold information. Experiments on the ModelNet40 dataset and NTU dataset demonstrate that the proposed method is more effective than the other popular methods for 3D shape recognition. Extensive experiments on other visual classification tasks and citation networks also show the superiority of our proposed method for representation learning.
C1 [Ma, Xueqi; Gao, Yue] Tsinghua Univ, Sch Software, KLISS, BNRist,THUIBCS, Beijing 100084, Peoples R China.
   [Ma, Xueqi; Liu, Weifeng] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710126, Peoples R China.
   [Liu, Weifeng] China Univ Petr East China, Coll Control Sci & Engn, Qingdao 266580, Peoples R China.
   [Tian, Qi] Huawei Cloud & AI, Shenzhen 518129, Peoples R China.
C3 Tsinghua University; Xidian University; China University of Petroleum;
   Huawei Technologies
RP Gao, Y (corresponding author), Tsinghua Univ, Sch Software, KLISS, BNRist,THUIBCS, Beijing 100084, Peoples R China.
EM xueqima@s.upc.edu.cn; liuwf@upc.edu.cn; tian.qi1@huawei.com;
   kevin.gaoy@gmail.com
RI Gao, Yue/B-3376-2012; liu, weifeng/B-7909-2008
FU National Natural Science Funds of China [U1701262, U1801263, 62088102,
   62021002]; National Laboratory of Pattern Recognition (NLPR)
   [202000009]; Major Scientific and Technological Projects of CNPC
   [ZD2019-183-008]
FX This work was supported in part by the National Natural Science Funds of
   China under Grants U1701262, U1801263, 62088102, and 62021002, in part
   by theOpen Project Program of the National Laboratory of Pattern
   Recognition (NLPR) under Grant 202000009, and in part by the Major
   Scientific and Technological Projects of CNPC under Grant
   ZD2019-183-008. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Sebastian Knorr.
CR Allegretto W, 1998, NONLINEAR ANAL-THEOR, V32, P819
   Amghibech S, 2003, ARS COMBINATORIA, V67, P283
   An L, 2017, IEEE T NEUR NET LEAR, V28, P2763, DOI 10.1109/TNNLS.2016.2602082
   Atwood J, 2016, ADV NEUR IN, V29
   Bruna J., 2013, INT C LEARNING REPRE
   Buhler T., 2009, P 26 ANN INT C MACH, P81, DOI DOI 10.1145/1553374.1553385
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Defferrard M, 2016, ADV NEUR IN, V29
   Fang Q, 2014, IEEE T MULTIMEDIA, V16, P796, DOI 10.1109/TMM.2014.2298216
   Feng YF, 2019, AAAI CONF ARTIF INTE, P3558
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Feng YT, 2019, AAAI CONF ARTIF INTE, P8279
   Fu SC, 2019, NEUROCOMPUTING, V362, P166, DOI 10.1016/j.neucom.2019.06.068
   Fuxiang Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9579, DOI 10.1109/CVPR42600.2020.00960
   Gao SH, 2013, IEEE T PATTERN ANAL, V35, P92, DOI 10.1109/TPAMI.2012.63
   Gao Y, 2022, IEEE T PATTERN ANAL, V44, P2548, DOI 10.1109/TPAMI.2020.3039374
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Hamilton WL, 2017, ADV NEUR IN, V30
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XW, 2019, IEEE I CONF COMP VIS, P7514, DOI 10.1109/ICCV.2019.00761
   Henaff M, 2015, Arxiv, DOI arXiv:1506.05163
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Huang JJ, 2022, IEEE T MULTIMEDIA, V24, P188, DOI 10.1109/TMM.2020.3047762
   Hwang TH, 2008, IEEE DATA MINING, P293, DOI 10.1109/ICDM.2008.37
   Jiang JW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2635
   Jiang JW, 2019, AAAI CONF ARTIF INTE, P8513
   Jin TS, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2670
   Kipf Thomas N., 2017, 5 INT C LEARN REPRES
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li YZ, 2018, ADV NEUR IN, V31
   Liu QS, 2011, PATTERN RECOGN, V44, P2255, DOI 10.1016/j.patcog.2010.07.014
   Liu WF, 2019, IEEE T CYBERNETICS, V49, P2927, DOI 10.1109/TCYB.2018.2833843
   Liu WF, 2016, IEEE T IND ELECTRON, V63, P5120, DOI 10.1109/TIE.2016.2552147
   Lu Q., 2003, 20 INT C MACHINE LEA, P496
   Luo DJ, 2010, MACH LEARN, V81, P37, DOI 10.1007/s10994-010-5201-z
   Ma XQ, 2019, IEEE T GEOSCI REMOTE, V57, P1585, DOI 10.1109/TGRS.2018.2867570
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Niepert M, 2016, PR MACH LEARN RES, V48
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157
   Shi HY, 2019, IEEE T NEUR NET LEAR, V30, P2963, DOI 10.1109/TNNLS.2018.2869747
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su Jong-Chyi, 2018, P EUR C COMP VIS ECC, P1
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Takeuchi H, 2003, ILLINOIS J MATH, V47, P939, DOI 10.1215/ijm/1258138202
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P2837, DOI 10.1109/TMM.2019.2909860
   Wang LQ, 2016, NEUROCOMPUTING, V171, P242, DOI 10.1016/j.neucom.2015.06.064
   Wang T, 2021, IEEE T MULTIMEDIA, V23, P3239, DOI 10.1109/TMM.2020.3021979
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Yadati N., 2019, Advances in Neural Information Processing Systems, P1509
   Yang Z, 2016, PR MACH LEARN RES, V48
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P3262, DOI 10.1109/TIP.2012.2190083
   Zhang L, 2022, IEEE T PATTERN ANAL, V44, P456, DOI 10.1109/TPAMI.2020.3009758
   Zhang ZZ, 2018, IEEE T IMAGE PROCESS, V27, P5957, DOI 10.1109/TIP.2018.2862625
   Zhao W, 2018, IEEE T NEUR NET LEAR, V29, P5834, DOI 10.1109/TNNLS.2018.2812888
   Zhou D., 2006, ADV NEURAL INFORM PR, P1601, DOI DOI 10.7551/MITPRESS/7503.003.0205
   Zhou DY, 2005, LECT NOTES COMPUT SC, V3663, P361
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
   Ziwei Zhang, 2022, IEEE Transactions on Knowledge and Data Engineering, V34, P249, DOI 10.1109/TKDE.2020.2981333
NR 66
TC 2
Z9 2
U1 5
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3989
EP 4001
DI 10.1109/TMM.2021.3111500
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400022
DA 2024-07-18
ER

PT J
AU Moniruzzaman, M
   Yin, ZZ
   He, ZH
   Qin, RW
   Leu, MC
AF Moniruzzaman, Md
   Yin, Zhaozheng
   He, Zhihai
   Qin, Ruwen
   Leu, Ming C.
TI Human Action Recognition by Discriminative Feature Pooling and Video
   Segment Attention Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Feature extraction; Two dimensional
   displays; Annotations; Training; Image recognition; Task analysis;
   Action recognition; attentional pooling; discriminative features;
   fully-supervised; weakly-supervised
ID NETWORK; FUSION
AB We Introduce a simple yet effective network that embeds a novel Discriminative Feature Pooling (DFP) mechanism and a novel Video Segment Attention Model (VSAM), for video-based human action recognition from both trimmed and untrimmed videos. Our DFP module introduces an attentional pooling mechanism for 3D Convolutional Neural Networks that attentionally pools 3D convolutional feature maps to emphasize the most critical spatial, temporal, and channel-wise features related to the actions within a video segment, while our VSAM ensembles these most critical features from all video segments and learns (1) class-specific attention weights to classify the video segments into the corresponding action categories, and (2) class-agnostic attention weights to rank the video segments based on their relevance to the action class. Our action recognition network can be trained from both trimmed videos in a fully-supervised way and untrimmed videos in a weakly-supervised way. For untrimmed videos with weak labels, our network learns attention weights without the requirement of precise temporal annotations of action occurrences in videos. Evaluated on the untrimmed video datasets of THUMOS14 and ActivityNet1.2, and trimmed video datasets of HMDB51, UCF101, and HOLLYWOOD2, our network achieves promising performance, compared to the latest state-of-the-art method. The implementation code is available at https://github.com/MoniruzzamanMd/DFP-VSAM-Networks.
C1 [Moniruzzaman, Md; Yin, Zhaozheng] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Yin, Zhaozheng] SUNY Stony Brook, Dept Biomed Informat, Stony Brook, NY 11794 USA.
   [He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Qin, Ruwen] SUNY Stony Brook, Dept Civil Engn, Stony Brook, NY 11794 USA.
   [Leu, Ming C.] Missouri Univ Sci & Technol, Dept Mech & Aerosp Engn, Rolla, MO 65409 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook; University of Missouri
   System; University of Missouri Columbia; State University of New York
   (SUNY) System; State University of New York (SUNY) Stony Brook;
   University of Missouri System; Missouri University of Science &
   Technology
RP Yin, ZZ (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.; Yin, ZZ (corresponding author), SUNY Stony Brook, Dept Biomed Informat, Stony Brook, NY 11794 USA.
EM mmoniruzzama@cs.stonybrook.edu; zyin@cs.stonybrook.edu;
   hezhi@missouri.edu; ruwen.qin@stonybrook.edu; mleu@mst.edu
RI Moniruzzaman, Md/AFD-5446-2022
OI Yin, Zhaozheng/0000-0002-9602-6488; Moniruzzaman, Md/0000-0003-3217-5094
FU National Science Foundation via CPS Synergy Project [CMMI-1646162];
   National Robotics Initiative Project [NRI-1830479]; Future of Work at
   the Human-Technology Frontier Project [ECCS-2025929]
FX This research work was supported in part by the National Science
   Foundation via CPS Synergy Project CMMI-1646162, in part by National
   Robotics Initiative Project NRI-1830479, and in part by Future of Work
   at the Human-Technology Frontier Project ECCS-2025929.
CR Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744
   Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Bojanowski P, 2013, IEEE I CONF COMP VIS, P2280, DOI 10.1109/ICCV.2013.283
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   De-An Huang, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9908, P137, DOI 10.1007/978-3-319-46493-0_9
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
   Feichtenhofer C., 2016, ADV NEURAL INFORM PR, P3468
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fernando B, 2016, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR.2016.212
   Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176
   Fu ZY, 2019, IEEE T MULTIMEDIA, V21, P2277, DOI 10.1109/TMM.2019.2902480
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Girdhar R., 2017, NIPS, P33
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Hasan M, 2015, IEEE T MULTIMEDIA, V17, P1909, DOI 10.1109/TMM.2015.2477242
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Ioffe S., 2015, P INT C LEARN REPR S
   Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li C, 2019, PROC CVPR IEEE, P7864, DOI 10.1109/CVPR.2019.00806
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557
   Meng LL, 2019, IEEE INT CONF COMP V, P1513, DOI 10.1109/ICCVW.2019.00189
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Qiu ZF, 2019, PROC CVPR IEEE, P12048, DOI 10.1109/CVPR.2019.01233
   Sharma S., 2015, NEURAL INFORM PROCES
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K., 2015, P ICLR
   Simonyan K, 2014, ADV NEUR IN, V27
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Soomro K., 2012, ARXIV12120402CS
   Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tao QY, 2019, IEEE T MULTIMEDIA, V21, P1135, DOI 10.1109/TMM.2018.2875597
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Tran A, 2017, IEEE INT CONF COMP V, P3110, DOI 10.1109/ICCVW.2017.368
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang J, 2018, PROC CVPR IEEE, P1149, DOI 10.1109/CVPR.2018.00126
   Wang LA, 2003, PATTERN RECOGN, V36, P585, DOI 10.1016/S0031-3203(02)00100-0
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang Y, 2017, IEEE INT CONF COMP V, P2129, DOI 10.1109/ICCVW.2017.249
   Wei Yunchao., 2014, CoRR
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang WL, 2010, PROC CVPR IEEE, P2030, DOI 10.1109/CVPR.2010.5539879
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Yao BP, 2010, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2010.5540235
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yu TZ, 2019, IEEE T MULTIMEDIA, V21, P2504, DOI 10.1109/TMM.2019.2907060
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang YB, 2020, IEEE T MULTIMEDIA, V22, P1345, DOI 10.1109/TMM.2019.2939747
   Zhu JG, 2018, INT C PATT RECOG, P645, DOI 10.1109/ICPR.2018.8545710
NR 71
TC 8
Z9 8
U1 1
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 689
EP 701
DI 10.1109/TMM.2021.3058050
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100014
OA Bronze
DA 2024-07-18
ER

PT J
AU Tang, H
   Sebe, N
AF Tang, Hao
   Sebe, Nicu
TI Total Generate: Cycle in Cycle Generative Adversarial Networks for
   Generating Human Faces, Hands, Bodies, and Natural Scenes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Image synthesis; Task analysis; Generative adversarial
   networks; Optimization; Image reconstruction; Skeleton; GANs; cycle in
   cycle; cycle consistency; guided image-to-image translation
AB We propose a novel and unified Cycle in CycleGenerative Adversarial Network (C2GAN) for generating human faces, hands, bodies, and natural scenes. Our proposed C2GAN is a cross-modal model exploring the joint exploitation of the input image data and guidance data in an interactive manner. C2GAN contains two different generators, i.e., an image-generation generator and a guidance-generation generator. Both generators are mutually connected and trained in an end-to-end fashion and explicitly form three cycled subnets, i.e., one image generation cycle and two guidance generation cycles. Each cycle aims at reconstructing the input domain and simultaneously produces a useful output involved in the generation of another cycle. In this way, the cycles constrain each other implicitly providing complementary information from both image and guidance modalities and bringing an extra supervision gradient across the cycles, facilitating a more robust optimization of the whole model. Extensive results on four guided image-to-image translation subtasks demonstrate that the proposed C2GAN is effective in generating more realistic images compared with state-of-the-art models.
C1 [Tang, Hao; Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci DISI, I-38123 Trento, Italy.
C3 University of Trento
RP Tang, H (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci DISI, I-38123 Trento, Italy.
EM hao.tang@unitn.it; sebe@disi.unitn.it
RI Sebe, Niculae/KEC-2000-2024
OI Sebe, Niculae/0000-0002-6597-7248; Tang, Hao/0000-0002-2077-1246
FU EU [951911]; Italy-China collaboration [TALENT:2018YFE0118400]; PRIN
   project PREVUE
FX This work was supported in part by EU H2020 AI4Media under Project
   951911, in part by Italy-China collaboration under Project
   TALENT:2018YFE0118400, and in part by PRIN project PREVUE.
CR [Anonymous], 2016, CMU-CS-16-118
   [Anonymous], 2018, NIPS
   Anoosheh A, 2018, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2018.00122
   Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen XY, 2018, LECT NOTES COMPUT SC, V11206, P167, DOI 10.1007/978-3-030-01216-8_11
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Di X, 2018, INT C PATT RECOG, P1079, DOI 10.1109/ICPR.2018.8545081
   Dolhansky B, 2018, PROC CVPR IEEE, P7902, DOI 10.1109/CVPR.2018.00824
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kim TH, 2017, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2017.435
   Kingma D.P., 2014, ARXIV14126980
   Korshunova I, 2017, IEEE I CONF COMP VIS, P3697, DOI 10.1109/ICCV.2017.397
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Li B., 2019, ADV NEURAL INFORM PR, P2065
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Ma LQ, 2017, ADV NEUR IN, V30
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Memo A, 2018, MULTIMED TOOLS APPL, V77, P27, DOI 10.1007/s11042-016-4223-3
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mo S, 2019, P INT C LEARN REPR
   Odena A., 2016, PROC INT C MACH LEAR
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Paszke A, 2019, ADV NEUR IN, V32
   Perarnau G., 2016, arXiv preprint arXiv: 1611.06355
   Qi X., 2020, ARXIV200313898
   QIAO F, 2018, ARXIV180201822
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Regmi K, 2018, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR.2018.00369
   Regmi K, 2019, COMPUT VIS IMAGE UND, V187, DOI 10.1016/j.cviu.2019.07.008
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Song LX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P627, DOI 10.1145/3240508.3240612
   Taigman Y., 2017, INT C LEARN REPR ICL, P1
   Tang H., 2018, P AS C COMP VIS ACCV, P3
   Tang H., 2020, P IEEE CVPR, P7870
   Tang H, 2019, IEEE IJCNN
   Tang H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2052, DOI 10.1145/3343031.3350980
   Tang H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P774, DOI 10.1145/3240508.3240704
   Tang H, 2019, PROC CVPR IEEE, P2412, DOI 10.1109/CVPR.2019.00252
   Tang Hao, 2019, ARXIV191111897
   Vo NN, 2016, LECT NOTES COMPUT SC, V9905, P494, DOI 10.1007/978-3-319-46448-0_30
   Wang M, 2019, PROC CVPR IEEE, P1495, DOI 10.1109/CVPR.2019.00159
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang W, 2018, PROC CVPR IEEE, P7083, DOI 10.1109/CVPR.2018.00740
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu XL, 2020, IEEE T IND INFORM, V16, P6172, DOI 10.1109/TII.2019.2959258
   Xu X, 2020, IEEE T NEUR NET LEAR, V31, P5412, DOI 10.1109/TNNLS.2020.2967597
   Yan YC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P199, DOI 10.1145/3123266.3123277
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yu Xiaoming, 2019, P ADV NEUR INF PROC, P2990
   Zhang J, ARXIV 190600805, V2019
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou S., 2017, PROC BRIT MACH VIS C
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 63
TC 10
Z9 10
U1 2
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2963
EP 2974
DI 10.1109/TMM.2021.3091847
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000022
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Wang, M
   Zhou, WG
   Tian, Q
   Li, HQ
AF Wang, Min
   Zhou, Wengang
   Tian, Qi
   Li, Houqiang
TI Deep Enhanced Weakly-Supervised Hashing With Iterative Tag Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Quantization (signal); Semantics; Noise measurement;
   Binary codes; Insects; Neural networks; Enhanced weakly-supervised
   hashing; deep hashing; image retrieval
ID NEURAL-NETWORK; IMAGE; QUANTIZATION
AB On image-sharing websites, images are usually associated with user-generated tags which contain semantic information and are more easily accessible than accurate labels. It is beneficial to utilize such tags as supervised information to learn image feature representation. However, some tags are not related with the image content and disturb the feature learning process. In this paper, we are dedicated to refining such noisy tags and upgrading the image feature learning. To this end, we propose a novel deep enhanced weakly-supervised hashing method, in which tags are adaptively refined according to image content. In our approach, we first map the deep image feature representation into the tag embedding space, and learn the discriminative as well as compact feature representations with the corresponding tags. After that, by referring to the learned feature representation in the first step, we refine the tags to become consistent with image content. The above two steps are alternated until convergence. Finally, we can obtain more content-relevant tags, better image features and binary hashing functions. The experiments on two image datasets prove that the proposed method outperforms the state-of-the-art weakly-supervised deep hashing methods on image retrieval task.
C1 [Wang, Min] Inst Artificial Intelligence, Hefei Comprehens Natl Sci Ctr, Hefei 230027, Peoples R China.
   [Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc, Hefei 230027, Peoples R China.
   [Tian, Qi] Huawei Technol Co Ltd, Shenzhen 518129, Guangdong, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Huawei Technologies
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc, Hefei 230027, Peoples R China.
EM wangmin@iai.ustc.edu.cn; zhwg@ustc.edu.cn; tian.qi1@huawei.com;
   lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
OI Wang, Min/0000-0003-3048-6980
FU National Key Research and Development Program of China [2018YFB1402605];
   National Natural Science Foundation of China [61836011, 61822208,
   62021001]; Youth Innovation Promotion Association CAS [2018497]; GPU
   cluster built by MCC Laboratory of Information Science and Technology
   Institution, USTC
FX This work was supported in part by the National Key Research and
   Development Program of China under Contract 2018YFB1402605, in part by
   the National Natural Science Foundation of China under Contracts
   61836011, 61822208, and 62021001, in part by Youth Innovation Promotion
   Association CAS 2018497, and in part by GPU cluster built by MCC
   Laboratory of Information Science and Technology Institution, USTC.
CR [Anonymous], 2009, Proceedings of the 18th international conference on World wide web, DOI [10.1145/1526709.1526758, DOI 10.1145/1526709.1526758]
   Bai C, 2018, NEUROCOMPUTING, V303, P60, DOI 10.1016/j.neucom.2018.04.034
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chaudhary C, 2020, IEEE T MULTIMEDIA, V22, P897, DOI 10.1109/TMM.2019.2937181
   Chen JJ, 2019, AAAI CONF ARTIF INTE, P8183
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen Y., 2019, P IEEECVF INT C COMP, P9796
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cong Bai, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P525, DOI 10.1145/3372278.3390711
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gattupalli V, 2019, PROC CVPR IEEE, P10367, DOI 10.1109/CVPR.2019.01062
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Guan ZY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3776
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Hu P, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1721, DOI 10.1145/3343031.3351078
   Hu QH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1584, DOI 10.1145/3123266.3123403
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai ZH, 2018, IEEE T IMAGE PROCESS, V27, P6147, DOI 10.1109/TIP.2018.2867956
   Li WJ, 2016, IJCAI, P1711
   Li ZC, 2017, AAAI CONF ARTIF INTE, P4154
   Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P755, DOI 10.1145/3240508.3240543
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Ma Q, 2019, PATTERN RECOGN, V92, P156, DOI 10.1016/j.patcog.2019.03.022
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen YM, 2020, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR42600.2020.00289
   Su S., 2018, P 32 INT C NEURAL IN, P798
   Tang J, 2020, ACM MM, P916
   Tang JH, 2019, IEEE T PATTERN ANAL, V41, P2027, DOI 10.1109/TPAMI.2019.2906603
   Tang JH, 2018, IEEE T CIRC SYST VID, V28, P2730, DOI 10.1109/TCSVT.2017.2715227
   Tang Jinhui., 2009, Proceedings of ACM international conference on Multimedia, P223, DOI DOI 10.1145/1631272.1631305
   Do TT, 2016, LECT NOTES COMPUT SC, V9909, P219, DOI 10.1007/978-3-319-46454-1_14
   Wang M, 2020, IEEE T MULTIMEDIA, V22, P1507, DOI 10.1109/TMM.2019.2943778
   Wang M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1707, DOI 10.1145/3123266.3123415
   Wang X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1730, DOI 10.1145/3343031.3350934
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yan C, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1535, DOI 10.1145/3343031.3350927
   Yang EK, 2019, PROC CVPR IEEE, P2941, DOI 10.1109/CVPR.2019.00306
   Yang EK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1064
   Yu LT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P861, DOI 10.1145/3240508.3240590
   Yu T, 2018, LECT NOTES COMPUT SC, V11205, P191, DOI 10.1007/978-3-030-01246-5_12
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zhang HW, 2016, AAAI CONF ARTIF INTE, P3669
   Zhang JJ, 2018, AAAI CONF ARTIF INTE, P7550
   Zhang T, 2014, PR MACH LEARN RES, V32, P838
   Zheng CQ, 2021, IEEE T MULTIMEDIA, V23, P4079, DOI 10.1109/TMM.2020.3037456
   Zhu L, 2021, IEEE T CIRC SYST VID, V31, P1478, DOI 10.1109/TCSVT.2020.3001583
   Zhu L, 2017, IEEE T MULTIMEDIA, V19, P2066, DOI 10.1109/TMM.2017.2729025
NR 63
TC 10
Z9 10
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2779
EP 2790
DI 10.1109/TMM.2021.3087356
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000007
DA 2024-07-18
ER

PT J
AU Wu, ZN
   Xia, XB
   Wang, RX
   Li, JT
   Yu, J
   Mao, YN
   Liu, TL
AF Wu, Zhengning
   Xia, Xiaobo
   Wang, Ruxin
   Li, Jiatong
   Yu, Jun
   Mao, Yinian
   Liu, Tongliang
TI LR-SVM plus : Learning Using Privileged Information with Noisy Labels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Support vector machines; Training; Optimization;
   Robustness; Linear programming; Task analysis; SVM plus; privileged
   information; noisy labels
AB The paradigm of Learning Using Privileged Information (LUPI) always assumes that labels are annotated precisely. However, in practice, this assumption may be violated, as the labels may be heavily noisy, which inevitably degenerates the performance of learning algorithms in the LUPI paradigm. To handle the side effect of noisy labels, we propose a novel Label Noise Robust SVM+ (LR-SVM+) algorithm. Specifically, as the privileged information contains rich information of the latent labels, we first utilize it to infer underlying clean labels. Then we use the inference to modify the noisy labels. Comprehensive experiments demonstrate the necessity of studying label noise robust SVM+ and the effectiveness of the proposed method.
C1 [Wu, Zhengning; Xia, Xiaobo; Liu, Tongliang] Univ Sydney, Trustworthy Machine Learning Lab, Darlington, NSW 2008, Australia.
   [Yu, Jun] Univ Sci & Technol China, Dept Automat, Hefei 230026, Peoples R China.
   [Wang, Ruxin] Yunnan Univ, Natl Pilot Sch Software, Kunming 650091, Yunnan, Peoples R China.
   [Li, Jiatong; Mao, Yinian] Meituan, Dianping Grp, Beijing, Peoples R China.
C3 University of Sydney; Chinese Academy of Sciences; University of Science
   & Technology of China, CAS; Yunnan University
RP Yu, J (corresponding author), Univ Sci & Technol China, Dept Automat, Hefei 230026, Peoples R China.
EM zhwu2112@uni.sydney.edu.au; xxia5420@uni.sydney.edu.au;
   rosinwang@gmail.com; lijiatong02@meituan.com; harryjun@ustc.edu.cn;
   maoyinian@meituan.com; tongliang.liu@sydney.edu.au
RI Liu, Tongliang/AAA-1506-2021
OI Liu, Tongliang/0000-0002-9640-6472
FU Autonomous Drone Delivery Business Unit of Meituan
FX The authors acknowledge the support from the Autonomous Drone Delivery
   Business Unit of Meituan.
CR Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829
   Anguita D, 2013, ESANN, P437, DOI DOI 10.3390/S20082200
   Biggio B., 2011, P 3 ASIAN C MACHINE, P97
   Chen J, 2020, LECT NOTES CIVIL ENG, V37, P437, DOI 10.1007/978-981-13-7603-0_43
   Cheng Hao, 2020, ARXIV201002347
   Chew HG, 2005, APPL OPTIMIZAT, V96, P157
   Feyereisl J, 2012, INFORM SCIENCES, V194, P4, DOI 10.1016/j.ins.2011.04.025
   Guan NY, 2019, IEEE T PATTERN ANAL, V41, P246, DOI 10.1109/TPAMI.2017.2777841
   Han B, 2020, INT C MACHINE LEARNI, V119
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Harutyunyan H, 2020, PR MACH LEARN RES, V119
   Hernandez-Lobato Daniel, 2014, Advances in Neural Information Processing Systems, P837
   Hu W, 2020, P INT C LEARN REPR
   Ji Y, 2012, INT C PATT RECOG, P2323
   Jiang L., 2018, ICML, P2304
   Lambert J, 2018, PROC CVPR IEEE, P8886, DOI 10.1109/CVPR.2018.00926
   Lapin M, 2014, NEURAL NETWORKS, V53, P95, DOI 10.1016/j.neunet.2014.02.002
   Lee Kimin, 2019, P MACHINE LEARNING R, V97
   Li J., 2020, P INT C LEARN REPR, P1
   Li X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2411
   Li YC, 2017, IEEE I CONF COMP VIS, P1928, DOI 10.1109/ICCV.2017.211
   Likert R., 1932, Arch. Psychol., V22, P44, DOI DOI 10.4135/9781412961288.N454
   Liu Sheng, 2020, Advances in Neural Information Processing Systems
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Liu Yang, 2020, PMLR, P6226
   Ma X., 2020, INT C MACHINE LEARNI, P6543
   Ma XJ, 2018, PR MACH LEARN RES, V80
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Pechyony D., 2010, Advances in neural information processing systems, P1894
   Reed S. E, 2015, P INT C LEARN REPR
   Ren MY, 2018, PR MACH LEARN RES, V80
   Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Vahdat A., 2017, ROBUSTNESS LABEL NOI, P5601
   Vapnik V, 2015, J MACH LEARN RES, V16, P2023
   Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042
   Wang XB, 2019, IEEE I CONF COMP VIS, P9357, DOI 10.1109/ICCV.2019.00945
   Wang YS, 2019, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2019.00041
   Wei H., 2020, P INT C LEARN REPR
   Wu Pengxiang, 2020, ADV NEURAL INFORM PR, P21382
   Wu SH, 2021, INT C MACHINE LEARNI, V139
   Xia X., 2021, P INT C LEARN REPR, P1
   Xia X, 2020, NEURAL INFORM PROCES
   Xia X., 2020, ARXIV201200932
   Xia XB, 2019, ADV NEUR IN, V32
   Xiaobo X., 2021, ARXIV210600445
   Yang S., 2021, ARXIV210513001
   Yao Quanming, 2020, INT C MACH LEARN
   Yao Y, 2020, P INT C NEUR INF PRO, P7597
   Yao Y., 2021, ARXIV210902986
   You S, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3336
   Yu Xingrui, 2019, PROC INT C MACH LEAR, P7164
   Zheng S., 2020, P MACH LEARN RES, P11447
   Zhou J. T., 2016, ARXIV160504034
NR 54
TC 6
Z9 6
U1 4
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1080
EP 1092
DI 10.1109/TMM.2021.3116417
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800006
DA 2024-07-18
ER

PT J
AU Zeng, M
   Zheng, YL
   Lin, JP
   Cheng, X
   Liao, J
   Wu, ZZ
   Deng, WJ
AF Zeng, Ming
   Zheng, Yinglin
   Lin, Jinpeng
   Cheng, Xuan
   Liao, Jing
   Wu, Zizhao
   Deng, Wenjin
TI Controllable Facial Caricaturization With Localized Deformation and
   Personalized Semantic Attentions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Strain; Shape; Semantics; Geometry; Faces; Generative adversarial
   networks; Task analysis; Caricature; image-to-image translation; style
   transfer
AB The facial caricature shows the distinct characteristics of a person via exaggerations of both shape and appearance. This paper presents a novel framework that automatically generates vivid facial caricatures by encoding personalized semantic information. To this end, we first design a part-based scheme for geometry warping, which composes local semantic deformation into a global warping field, equipped with sufficient warping freedom of different facial components. Second, under the scheme of Part-based Warping, we design a photo-to-caricature translation network called PbWarpGAN, and adopt several novel losses to capture the personalized characteristics of each input face and preserve its identity better. Third, based on PbWarpGAN, we develop a user-friendly interface by introducing an attention scheme on each facial component, allowing ordinary users to adjust the automatically generated caricature by PbWarpGAN according to their preference conveniently. Experimental results show that our PbWarpGAN is more effective in capturing personalized characteristics than counterparts, and provides an efficient tool for caricature designing application.
C1 [Zeng, Ming; Zheng, Yinglin; Cheng, Xuan; Deng, Wenjin] Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
   [Lin, Jinpeng] Alibaba Grp, Beijing 100020, Peoples R China.
   [Liao, Jing] City Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
   [Wu, Zizhao] Hangzhou Dianzi Univ, Sch Media & Design, Hangzhou 310018, Peoples R China.
C3 Xiamen University; Alibaba Group; City University of Hong Kong; Hangzhou
   Dianzi University
RP Cheng, X (corresponding author), Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
EM chengxuan@xmu.edu.cn
OI LIAO, Jing/0000-0001-7014-5377; wu, zizhao/0000-0003-2103-5037; deng,
   wenjin/0000-0002-9213-6022; Zheng, Yinglin/0000-0003-4671-6111
FU NSFC [62072382, 61802322, 61402387]; Hong Kong ECS [21209119];
   Donation-RMGS of CityU Hong Kong [9229064]; Guiding Project of Fujian
   Province, China [2018H0037]; Fundamental Research Funds for the Central
   Universities, China [20720190003]
FX This work was supported in part by the NSFC under Grants 62072382,
   61802322, and 61402387, in part by Hong Kong ECS under Grant 21209119,
   in part by the Donation-RMGS of CityU Hong Kong under Grant 9229064, in
   part by Guiding Project of Fujian Province, China under Grant 2018H0037,
   and in part by Fundamental Research Funds for the Central Universities,
   China under Grant 20720190003. The Associate Editor coordinating the
   review of this manuscript and approving it for publication was Xin Geng.
CR Akleman E., 1997, ACM SIGGRAPH, DOI DOI 10.1145/259081.259231
   [Anonymous], 2000, P INT C VIS COMP
   BRENNAN SE, 1985, LEONARDO, V18, P170, DOI 10.2307/1578048
   Chen H., 2002, Proceedings of the Tenth ACM International Conference on Multimedia, P171
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chiang Pei-Ying, 2004, PROCEEDING 2004 ASIA, P2
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cole F, 2017, PROC CVPR IEEE, P3386, DOI 10.1109/CVPR.2017.361
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han XG, 2020, IEEE T VIS COMPUT GR, V26, P2349, DOI 10.1109/TVCG.2018.2886007
   Hensel M, 2017, ADV NEUR IN, V30
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huo Jing, 2018, BMVC, P223
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kaidi Cao, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3272127.3275046
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P, 2014, 3 INT C LEARN REPR I, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Lewiner T, 2011, COMPUT GRAPH-UK, V35, P586, DOI 10.1016/j.cag.2011.03.005
   Li ZY, 2021, IEEE T MULTIMEDIA, V23, P2694, DOI 10.1109/TMM.2020.3015015
   Liang L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P386, DOI 10.1109/PCCGA.2002.1167882
   Liu MY, 2017, ADV NEUR IN, V30
   Mo Z., 2004, ACM SIGGRAPH 2004 SK, P57
   Nguyen KHL, 2011, LECT NOTES COMPUT SC, V6523, P536
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi YC, 2019, PROC CVPR IEEE, P10754, DOI 10.1109/CVPR.2019.01102
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y., 2017, INT C LEARN REPR ICL, P1
   Wu QY, 2018, PROC CVPR IEEE, P7336, DOI 10.1109/CVPR.2018.00766
   Wu WY, 2019, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2019.00820
   Xiaobin Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P763, DOI 10.1007/978-3-030-58548-8_44
   Xu K, 2019, INT CONF INFO SCI, P36, DOI [10.1109/ICIST.2019.8836944, 10.1109/icist.2019.8836944]
   Yang F, 2012, PROC CVPR IEEE, P861, DOI 10.1109/CVPR.2012.6247759
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 43
TC 0
Z9 0
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4041
EP 4053
DI 10.1109/TMM.2021.3111711
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400026
DA 2024-07-18
ER

PT J
AU Zhu, H
   Peng, H
   Xu, GX
   Deng, LZ
   Cheng, YY
   Song, AG
AF Zhu, Hu
   Peng, Hao
   Xu, Guoxia
   Deng, Lizhen
   Cheng, Yueying
   Song, Aiguo
TI Bilateral Weighted Regression Ranking Model With Spatial-Temporal
   Correlation Filter for Visual Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Correlation; Data models; Computational modeling;
   Visualization; Robustness; Mathematical model; Bilateral Weighted
   Regression; Spatial-Temporal; Ranking; Visual Tracking
ID ROBUST OBJECT TRACKING
AB Many discriminative correlation filter (DCF)-based methods have successfully leveraged the guidance for solving two problems (i.e., the boundary effect and temporal filtering degradation) as a model prior to visual tracking. The intuitive motivation of these methods is to control the degeneration of the updating loss of the objective function with a structural framework. While these methods rely mostly on various regularization items, they always ignore the loss from data fidelity term. Therefore, we propose a bilateral weighted regression ranking model termed as BWRR. Here, we resort to two procedures for solving the above problems. First, BWRR introduces a bilateral constraint into the data fidelity term to control the loss of rows and columns of the filter learning data term. The weighted matrices could impose an adaptive penalty for large data loss during the learning process to avoid the model degradation problem. Second, the data of the updated weighted matrices is not directly applied to the calculation of the filter during each iteration. Instead, a new weighted product matrix is obtained by ranking and numerical transformation for updating the filter. We show that the proposed model converts the original correlation filter regression problem into a regression-with-ranking problem, thus avoiding the problem of positive and negative sample imbalance. Overall, the BWRR model is iteratively solved by the alternating direction method of multipliers(ADMM). Qualitative and quantitative evaluations demonstrate the effectiveness and superiority of our proposed method by extensive and quantitative experiments on the OTB, VOT, and UAV datasets.
C1 [Zhu, Hu; Peng, Hao; Cheng, Yueying] Nanjing Univ Posts & Telecommun, Jiangsu Prov Key Lab Image Proc & Image Commun, Nanjing 210003, Peoples R China.
   [Xu, Guoxia] Norwegian Univ Sci & Technol, Dept Comp Sci, N-2815 Gjovik, Norway.
   [Deng, Lizhen] Nanjing Univ Posts & Telecommun, Natl Engn Res Ctr Commun & Network Technol, Nanjing 210003, Peoples R China.
   [Song, Aiguo] Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Norwegian University
   of Science & Technology (NTNU); Nanjing University of Posts &
   Telecommunications; Southeast University - China
RP Deng, LZ (corresponding author), Nanjing Univ Posts & Telecommun, Natl Engn Res Ctr Commun & Network Technol, Nanjing 210003, Peoples R China.
EM peter.hu.zhu@gmail.com; penghao0321@126.com; gxxu.re@gmail.com;
   alicedenglzh@gmail.com; 13851523654@163.com; a.g.song@seu.edu.cn
RI Peng, Hao/L-7364-2019; Xu, Guoxia/AAQ-3396-2021; Deng,
   Lizhen/AAV-4582-2021; zhu, hu/GQQ-8365-2022
OI Peng, Hao/0000-0001-7422-630X; Zhu, Hu/0000-0002-5528-8721; Song,
   Aiguo/0000-0002-1982-6780; Xu, Guoxia/0000-0002-0036-8820
FU National Natural Science Foundation of China [62072256]
FX This work is supported by the National Natural Science Foundation of
   China under Grant 62072256.
CR [Anonymous], ARXIV150104587
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bibi A, 2016, LECT NOTES COMPUT SC, V9910, P419, DOI 10.1007/978-3-319-46466-4_25
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cehovin L, 2016, IEEE WINT CONF APPL
   Cehovin L, 2014, IEEE WINT CONF APPL, P540, DOI 10.1109/WACV.2014.6836055
   Crammer K, 2006, J MACH LEARN RES, V7, P551
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M., 2016, PROC IEEE INT C COMP, P58
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hu B, 2021, IEEE T IND ELECTRON, V68, P2427, DOI 10.1109/TIE.2020.2972451
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Khan S, 2017, EXPERT SYST APPL, V90, P427, DOI 10.1016/j.eswa.2017.08.039
   Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Liu GG, 2018, IEEE T MULTIMEDIA, V20, P2949, DOI 10.1109/TMM.2018.2844685
   Liu S, 2016, PROC CVPR IEEE, P4312, DOI 10.1109/CVPR.2016.467
   Lu XH, 2017, 2017 INTERNATIONAL CONFERENCE ON SECURITY, PATTERN ANALYSIS, AND CYBERNETICS (SPAC), P100, DOI 10.1109/SPAC.2017.8304258
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Qian Q., 2020, P IEEE CVF C COMP VI, P169
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song HH, 2014, ELECTRON LETT, V50, P1931, DOI 10.1049/el.2014.1911
   Tang M, 2015, IEEE I CONF COMP VIS, P3038, DOI 10.1109/ICCV.2015.348
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu GX, 2019, WORLD WIDE WEB, V22, P791, DOI 10.1007/s11280-018-0555-4
   Xu GX, 2018, NEUROCOMPUTING, V315, P33, DOI 10.1016/j.neucom.2018.05.108
   Xu J, 2018, LECT NOTES COMPUT SC, V11212, P21, DOI 10.1007/978-3-030-01237-3_2
   Xu TY, 2019, IEEE I CONF COMP VIS, P7949, DOI 10.1109/ICCV.2019.00804
   Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201
   Yang HX, 2011, NEUROCOMPUTING, V74, P3823, DOI 10.1016/j.neucom.2011.07.024
   Yang Y, 2011, P AM MATH SOC, V139, P3171, DOI 10.1090/S0002-9939-2011-10735-4
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang JH, 2020, IEEE T IND ELECTRON, V67, P8659, DOI 10.1109/TIE.2019.2946557
   Zhang TZ, 2016, PROC CVPR IEEE, P3880, DOI 10.1109/CVPR.2016.421
   Zheng YH, 2019, IEEE T NEUR NET LEAR, V30, P3024, DOI 10.1109/TNNLS.2018.2855686
   Zhong W, 2014, IEEE T IMAGE PROCESS, V23, P2356, DOI 10.1109/TIP.2014.2313227
   Zuo WM, 2019, IEEE T PATTERN ANAL, V41, P1158, DOI 10.1109/TPAMI.2018.2829180
NR 63
TC 15
Z9 15
U1 3
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2098
EP 2111
DI 10.1109/TMM.2021.3075876
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200025
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hao, JM
   Dun, YJ
   Zhao, GS
   Wu, YX
   Qian, XM
AF Hao, Junmei
   Dun, Yujie
   Zhao, Guoshuai
   Wu, Yuxia
   Qian, Xueming
TI Annular-Graph Attention Model for Personalized Sequential Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Recommender systems; Collaboration; Recurrent neural networks; Deep
   learning; Computational modeling; Measurement; Markov processes;
   Attention mechanism; graph attention; personalized recommendation;
   sequential recommendation; user preferences
AB Sequential recommendations aim to predict the user's next behaviors items based on their successive historical behaviors sequence. It has been widely applied in lots of online services. However, current sequential recommendations use the adjacent behaviors to capture the features of the sequence, ignoring the features among nonadjacent sequential items and the summarized features of the sequence. To address the above problems, in this paper, we propose an annular-graph attention based sequential recommendation (AGSR) model by exploring user's long-term and short-term preferences for the personalized sequential recommendation. For user's short-term preferences, AGSR builds an annular-graph on the sequence of user behavior. Then, AGSR proposes an annular-graph attention applying on the sub annular-graph to explore local features and applying annular-graph attention on entire annular-graph to explore the global features and the skip features. For user's long-term preferences, the latent factor model are introduced in AGSR. The experimental results on two public datasets show that our model outperforms the state-of-the-art methods.
C1 [Hao, Junmei; Wu, Yuxia] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
   [Dun, Yujie] Xi An Jiao Tong Univ, Sch Informat & Commun, Xian 710049, Peoples R China.
   [Zhao, Guoshuai] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Smiles Lab, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong
   University; Xi'an Jiaotong University; Xi'an Jiaotong University
RP Dun, YJ (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun, Xian 710049, Peoples R China.; Zhao, GS (corresponding author), Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, Smiles Lab, Xian 710049, Peoples R China.
EM haojunmei1996@stu.xjtu.edu.cn; dunyj@mail.xjtu.edu.cn;
   guoshuai.zhao@xjtu.edu.cn; wuyuxia@stu.xjtu.edu.cn;
   qianxm@mail.xjtu.edu.cn
FU NSFC [61732008, 61772407]
FX This work was supported in part by NSFC under Grants 61732008 and
   61772407 and in part byMicrosoft Research Asia and Pazhou Lab,
   Guangzhou. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Wen-Huang Cheng.
CR Agarwal D, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P19
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bansal T, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P107, DOI 10.1145/2959100.2959180
   Bharadhwaj H, 2018, KUNSTL INTELL, V32, P267, DOI 10.1007/s13218-018-0560-x
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Cheng ZY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3654
   Cheng ZY, 2016, ACM T INFORM SYST, V34, DOI 10.1145/2846092
   Cheng ZY, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1267, DOI 10.1145/2600428.2611187
   Cho E., 2011, P 17 ACM SIGKDD INT, P1082, DOI DOI 10.1145/2020408.2020579
   Chu WT, 2017, WORLD WIDE WEB, V20, P1313, DOI 10.1007/s11280-017-0437-1
   Donkers T, 2017, PROCEEDINGS OF THE ELEVENTH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'17), P152, DOI 10.1145/3109859.3109877
   Feng SS, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2069
   Gao XY, 2020, IEEE T MULTIMEDIA, V22, P1647, DOI 10.1109/TMM.2019.2945180
   Gong Y., 2016, IJCAI, P2782
   He RN, 2017, PROCEEDINGS OF THE ELEVENTH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'17), P161, DOI 10.1145/3109859.3109882
   He RN, 2016, IEEE DATA MINING, P191, DOI [10.1109/ICDM.2016.88, 10.1109/ICDM.2016.0030]
   He XN, 2018, IEEE T KNOWL DATA EN, V30, P2354, DOI 10.1109/TKDE.2018.2831682
   Hidayati SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P438, DOI 10.1145/3240508.3240546
   Hsieh CK, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P193, DOI 10.1145/3038912.3052639
   Jing H, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P515, DOI 10.1145/3018661.3018719
   Kim D, 2017, INFORM SCIENCES, V417, P72, DOI 10.1016/j.ins.2017.06.026
   Kim Y, 2014, P EMNLP ACL, P1746, DOI DOI 10.3115/V1/D14-1181
   Kingma DP., 2014, ADAM METHOD STOCHAST
   Konstan J., 2001, P 10 INT C WORLD WID, P285
   Koren Y, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P447
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Li J, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1419, DOI 10.1145/3132847.3132926
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Liang DW, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P59, DOI 10.1145/2959100.2959182
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Lyu F, 2019, IEEE T MULTIMEDIA, V21, P1971, DOI 10.1109/TMM.2019.2894964
   Nie J, 2019, ARXIV PREPRINT190300
   Pan R, 2008, IEEE DATA MINING, P502, DOI 10.1109/ICDM.2008.16
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Rendle S., 2010, P 19 INT C WORLD WID, P811, DOI DOI 10.1145/1772690.1772773
   Rodríguez P, 2020, IEEE T MULTIMEDIA, V22, P502, DOI 10.1109/TMM.2019.2928494
   Salakhutdinov R., 2007, P 24 INT C MACHINE L, P791
   Shuhui Jiang, 2016, IEEE Transactions on Big Data, V2, P43, DOI 10.1109/TBDATA.2016.2541160
   Srebro N., 2003, P 20 INT C MACH LEAR, P720
   Tang JX, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P565, DOI 10.1145/3159652.3159656
   Tay Y, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P729, DOI 10.1145/3178876.3186154
   Tay Y, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2309, DOI 10.1145/3219819.3220086
   Tikk D, 2016, ICLR C TRACK P
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JL, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1101, DOI 10.1145/3397271.3401133
   Wang PF, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P209, DOI 10.1145/3397271.3401134
   Wu CY, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P495, DOI 10.1145/3018661.3018689
   Wu QT, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2091, DOI 10.1145/3308558.3313442
   Wu YX, 2022, IEEE T KNOWL DATA EN, V34, P1944, DOI 10.1109/TKDE.2020.3002531
   Yang CL, 2019, INT CONF ACOUST SPEE, P1577, DOI 10.1109/ICASSP.2019.8682809
   Ying HC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3926
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2762, DOI 10.1109/TMM.2019.2912124
   Zhang S., 2018, ABS180204606 CORR
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
   Zhao GS, 2021, IEEE T KNOWL DATA EN, V33, P3160, DOI 10.1109/TKDE.2020.2966971
   Zhao GS, 2020, KNOWL-BASED SYST, V196, DOI 10.1016/j.knosys.2020.105849
NR 57
TC 15
Z9 15
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 14
PY 2021
VL 24
BP 3381
EP 3391
DI 10.1109/TMM.2021.3097186
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NU
UT WOS:000824707900003
DA 2024-07-18
ER

PT J
AU Al-Halah, Z
   Grauman, K
AF Al-Halah, Ziad
   Grauman, Kristen
TI Modeling Fashion Influence From Photos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision for fashion; style discovery; influence; style
   forecasting
AB The evolution of clothing styles and their migration across the world is intriguing, yet difficult to describe quantitatively. We propose to discover and quantify fashion influences fromcatalog and social media photos. We explore fashion influence along two channels: geolocation and fashion brands. We introduce an approach that detects which of these entities influence which other entities in terms of propagating their styles. We then leverage the discovered influence patterns to inform a novel forecasting model that predicts the future popularity of any given style within any given city or brand. To demonstrate our idea, we leverage public large-scale datasets of 7.7M Instagram photos from 44 major world cities (where styles are worn with variable frequency) as well as 41K Amazon product photos (where styles are purchased with variable frequency). Our model learns directly from the image data how styles move between locations and how certain brands affect each other's designs in a predictable way. The discovered influence relationships reveal how both cities and brands exert and receive fashion influence for an array of visual styles inferred from the images. Furthermore, the proposed forecasting model achieves state-of-the-art results for challenging style forecasting tasks. Our results indicate the advantage of grounding visual style evolution both spatially and temporally, and for the first time, they quantify the propagation of inter-brand and inter-city influences. Project page: https:// www.cs.utexas.edu/ (similar to)ziad/influence_from_photos.html
C1 [Al-Halah, Ziad; Grauman, Kristen] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
   [Grauman, Kristen] Facebook AI Res, Austin, TX 78701 USA.
C3 University of Texas System; University of Texas Austin; Facebook Inc
RP Al-Halah, Z (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
EM ziadlhlh@gmail.com; grauman@cs.utexas.edu
OI Al-Halah, Ziad/0000-0001-6887-0385
FU NSF [911 IIS1514118]
FX UT Austin is supported in part by NSF 911 IIS1514118.
CR Al-Halah Z, 2017, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2017.50
   Al-Halah Ziad, 2020, P IEEE CVF C COMP VI, P10136
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2010, P 16 ACM SIGKDD INT, DOI DOI 10.1145/1835804.1835884
   Banica Logica., 2014, Em Intelligent Fashion Forecasting Systems: Models and Applications, de Tsan-Ming Choi, Chi-Leung Hui e Yong Yu, P161
   Bauknecht T.E., 2017, INTERNET FASHION MAP
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Bossard Lukas, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P321, DOI 10.1007/978-3-642-37447-0_25
   Chen CY, 2018, AAAI CONF ARTIF INTE, P2111
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Chen KT, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P177, DOI 10.1145/2733373.2809930
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   GEURTS M, 1977, J MARKETING RES, V14, P269, DOI 10.2307/3150485
   GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791
   Han XT, 2019, IEEE I CONF COMP VIS, P4480, DOI 10.1109/ICCV.2019.00458
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He RN, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P507, DOI 10.1145/2872427.2883037
   Hidayati SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P197, DOI 10.1145/2647868.2656405
   Hidayati SC, 2021, IEEE T MULTIMEDIA, V23, P365, DOI 10.1109/TMM.2020.2980195
   Hidayati SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P438, DOI 10.1145/3240508.3240546
   Hsiao WL, 2019, IEEE I CONF COMP VIS, P5046, DOI 10.1109/ICCV.2019.00515
   Hsiao WL, 2018, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR.2018.00748
   Hsiao WL, 2017, IEEE I CONF COMP VIS, P4213, DOI 10.1109/ICCV.2017.451
   Hsiao Wei-Lin, 2020, CVPR, P11059
   Hu CW, 2015, LECT NOTES ARTIF INT, V9285, P53, DOI 10.1007/978-3-319-23525-7_4
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Kataoka H, 2019, IEEE COMPUT SOC CONF, P305, DOI 10.1109/CVPRW.2019.00040
   Kaya Murat., 2014, Em Intelligent Fashion Forecasting Systems: Models and Applications, de Tsan-Ming Choi, Chi-Leung Hui e Yong Yu, P123
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Kiapour M. Hadi, 2018, P EUR C COMP VIS WOR
   Kim G, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P404, DOI 10.1109/ICCVW.2013.60
   Kuang ZH, 2019, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2019.00316
   Kwak IS, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.14
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lu Z, 2013, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR.2013.350
   Mall U, 2019, IEEE I CONF COMP VIS, P411, DOI 10.1109/ICCV.2019.00050
   Matzen Kevin, 2017, ARXIV170601869
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Misra R, 2018, 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS), P422, DOI 10.1145/3240323.3240398
   Murillo AnaC., 2012, CVPR Workshops, P28
   Ren SY, 2017, ANN OPER RES, V257, P335, DOI 10.1007/s10479-016-2204-6
   Ren SY, 2015, IEEE T SYST MAN CY-S, V45, P411, DOI 10.1109/TSMC.2014.2342194
   Simo-Serra E, 2015, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2015.7298688
   Song Z, 2011, IEEE I CONF COMP VIS, P1084, DOI 10.1109/ICCV.2011.6126355
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Thomassey S., 2014, Intelligent fashion forecasting systems: Models and applications, P9
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Vittayakorn S, 2015, IEEE WINT CONF APPL, P951, DOI 10.1109/WACV.2015.131
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Yu C, 2019, IEEE I CONF COMP VIS, P9045, DOI 10.1109/ICCV.2019.00914
   Yu WJ, 2019, PROC CVPR IEEE, P2932, DOI 10.1109/CVPR.2019.00305
   Zalando, MOST EL CIT WORLD
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
   Zhao B, 2017, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2017.652
NR 60
TC 4
Z9 4
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4143
EP 4157
DI 10.1109/TMM.2020.3037459
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900018
OA hybrid, Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, BJ
   Tan, WJ
   Coatrieux, G
   Zheng, YH
   Shi, YQ
AF Chen, Beijing
   Tan, Weijin
   Coatrieux, Gouenou
   Zheng, Yuhui
   Shi, Yun-Qing
TI A Serial Image Copy-Move Forgery Localization Scheme With Source/Target
   Distinguishment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; Feature extraction; Forgery; Correlation; Standards; Task
   analysis; Decoding; Copy-move; image forgery; deep neural network;
   atrous convolution; attention mechanism
AB In this paper, we improve the parallel deep neural network (DNN) scheme BusterNet for image copy-move forgery localization with source/target region distinguishment. BusterNet is based on two branches, i.e., Simi-Det and Mani-Det, and suffers from two main drawbacks: (a) it should ensure that both branches correctly locate regions; (b) the Simi-Det branch only extracts single-level and low-resolution features using VGG16 with four pooling layers. To ensure the identification of the source and target regions, we introduce two subnetworks that are constructed serially: the copy-move similarity detection network (CMSDNet) and the source/target region distinguishment network (STRDNet). Regarding the second drawback, the CMSDNet subnetwork improves Simi-Det by removing the last pooling layer in VGG16 and by introducing atrous convolution into VGG16 to preserve field-of-views of filters after the removal of the fourth pooling layer; double-level self-correlation is also considered for matching hierarchical features. Moreover, atrous spatial pyramid pooling and attention mechanism allow the capture of multiscale features and provide evidence for important information. Finally, STRDNet is designed to determine the similar regions obtained from CMSDNet directly as tampered regions and untampered regions. It determines regions at the image-level rather than at the pixel-level as made by Mani-Det of BusterNet. Experimental results on four publicly available datasets (new synthetic dataset, CASIA, CoMoFoD, and COVERAGE) demonstrate that the proposed algorithm is superior to the state-of-the-art algorithms in terms of similarity detection ability and source/target distinguishment ability.
C1 [Chen, Beijing; Tan, Weijin; Zheng, Yuhui] Nanjing Univ Informat Sci & Technol, Minist Educ, Engn Res Ctr Digital Forens, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
   [Chen, Beijing; Tan, Weijin; Zheng, Yuhui] Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Technol, Nanjing 210044, Jiangsu, Peoples R China.
   [Chen, Beijing; Tan, Weijin; Zheng, Yuhui] Nanjing Univ Informat Sci & Technol, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Nanjing 210044, Jiangsu, Peoples R China.
   [Coatrieux, Gouenou] IMT Atlantique Bretagne Pays Loire, INSERM, UMR1101, LaTIM, F-29000 Brest, France.
   [Shi, Yun-Qing] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; Nanjing University of
   Information Science & Technology; Institut National de la Sante et de la
   Recherche Medicale (Inserm); IMT - Institut Mines-Telecom; IMT
   Atlantique; Universite de Bretagne Occidentale; New Jersey Institute of
   Technology
RP Zheng, YH (corresponding author), Nanjing Univ Informat Sci & Technol, Minist Educ, Engn Res Ctr Digital Forens, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.; Zheng, YH (corresponding author), Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Technol, Nanjing 210044, Jiangsu, Peoples R China.
EM nbutimage@126.com; 13913301359@163.com;
   gouenou.coatrieux@imt-atlantique.fr; zheng_yuhui@nuist.edu.cn;
   shi@njit.edu
FU National Natural Science Foundation of China [62072251, 61972206];
   Priority Academic Program Development of Jiangsu Higher Education
   Institutions (PAPD) fund
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072251 and 61972206; in part by the
   Priority Academic Program Development of Jiangsu Higher Education
   Institutions (PAPD) fund. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Yongdong
   Zhang.
CR Bayar B, 2018, IEEE T INF FOREN SEC, V13, P2691, DOI 10.1109/TIFS.2018.2825953
   Chen BJ, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103015
   Chen BJ, 2019, MULTIMED TOOLS APPL, V78, P8057, DOI 10.1007/s11042-018-6595-z
   Chen BJ, 2018, IEEE ACCESS, V6, P56637, DOI 10.1109/ACCESS.2018.2871952
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Christlein V., 2012, IEEE T INF FOREN SEC, V7, P1841, DOI DOI 10.1109/TIFS.2012.2218597
   Cozzolino D., 2015, IEEE T INF FOREN SEC, V10, P2284, DOI DOI 10.1109/TIFS.2015.2455334
   Dong Jing., 2011, CASIA TAMPERED IMAGE
   Fridrich J., 2003, P DIG FOR RES WORKSH, V3, P652
   Kakar P, 2011, IEEE T MULTIMEDIA, V13, P443, DOI 10.1109/TMM.2011.2121056
   Larochelle Hugo, 2010, ADV NEURAL INFORM PR, V23
   Li YM, 2019, IEEE T INF FOREN SEC, V14, P1307, DOI 10.1109/TIFS.2018.2876837
   Long FC, 2020, IEEE T MULTIMEDIA, V22, P1577, DOI 10.1109/TMM.2019.2943204
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Muhammad G, 2012, DIGIT INVEST, V9, P49, DOI 10.1016/j.diin.2012.04.004
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Popescu A.C., 2004, Comput. Sci. Tech. Rep, P1
   Pun CM, 2015, IEEE T INF FOREN SEC, V10, P1705, DOI 10.1109/TIFS.2015.2423261
   Ryu SJ, 2010, LECT NOTES COMPUT SC, V6387, P51, DOI 10.1007/978-3-642-16435-4_5
   Shivakumar B., 2011, Int J Comput Sci Issues (IJCSI), V8, P199
   Simonyan K., 2014, 14091556 ARXIV
   Stamm MC, 2013, IEEE ACCESS, V1, P167, DOI 10.1109/ACCESS.2013.2260814
   Su L., 2017, IEEE T MULTIMEDIA, V20, P1
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tralic Dijana, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P49
   Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu TY, 2019, IEEE INT CON MULTI, P940, DOI 10.1109/ICME.2019.00166
   Wu Y, 2018, LECT NOTES COMPUT SC, V11210, P170, DOI 10.1007/978-3-030-01231-1_11
   Wu Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1480, DOI 10.1145/3123266.3123411
   Xie HT, 2019, IEEE T MULTIMEDIA, V21, P1248, DOI 10.1109/TMM.2018.2872898
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xu K, 2020, IEEE T MULTIMEDIA, V22, P394, DOI 10.1109/TMM.2019.2929931
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Zandi M, 2016, IEEE T INF FOREN SEC, V11, P2499, DOI 10.1109/TIFS.2016.2585118
   Zhong JL, 2016, NONLINEAR DYNAM, V84, P189, DOI 10.1007/s11071-015-2374-9
NR 37
TC 56
Z9 61
U1 2
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3506
EP 3517
DI 10.1109/TMM.2020.3026868
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100006
DA 2024-07-18
ER

PT J
AU Duh, PJ
   Sung, YC
   Chiang, LYF
   Chang, YJ
   Chen, KW
AF Duh, Ping-Jung
   Sung, Yu-Cheng
   Chiang, Liang-Yu Fan
   Chang, Yung-Ju
   Chen, Kuan-Wen
TI V-Eye: A Vision-Based Navigation System for the Visually Impaired
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Visualization; Real-time systems; Simultaneous localization and
   mapping; Servers; Global Positioning System; Visually impaired;
   navigation system; user study; global localization; scene understanding
ID MONOCULAR SLAM; MEAN SHIFT
AB Numerous systems for helping visually impaired people navigate in unfamiliar places have been proposed. However, few can detect and warn about moving obstacles, provide correct orientation in real time, or support navigation between indoor and outdoor spaces. Accordingly, this paper proposes V-Eye, which fulfills these needs by utilizing a novel global localization method (VB-GPS) and image-segmentation techniques to achieve better scene understanding with a single camera. Our experiments establish that the proposed system can reliably provide precise locations and orientation information (with a median error of approximately 0.27 m and 0.95 degrees); detect unpredictable obstacles; and support navigating both within and between indoor and outdoor environments. The results of a user-experience study of V-eye further indicate that it helped the participants not only with navigation, but also improved their awareness of obstacles, enhanced their spatial awareness more generally, and led them to feel more secure and independent while walking.
C1 [Duh, Ping-Jung; Chiang, Liang-Yu Fan] Natl Chiao Tung Univ, Inst Multimedia Engn, Hsinchu 300, Taiwan.
   [Sung, Yu-Cheng] Natl Chiao Tung Univ, Inst Comp Sci & Engn, Hsinchu 300, Taiwan.
   [Chang, Yung-Ju; Chen, Kuan-Wen] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
C3 National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung
   University; National Yang Ming Chiao Tung University
RP Chen, KW (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
EM jonntw211@gmail.com; abc87941@gmail.com; aidr10030625@gmail.com;
   armuro@cs.nctu.edu.tw; kuanwen@cs.nctu.edu.tw
OI Chen, Kuan-Wen/0000-0002-4159-201X
FU Ministry of Science and Technology of Taiwan [MOST
   107-2221-E-009-148-MY2, MOST 108-2221-E-009-067-MY3, MOST
   108-2218-E-369-001-]
FX This work was supported in part by the Ministry of Science and
   Technology of Taiwan (MOST 107-2221-E-009-148-MY2, in part by MOST
   108-2221-E-009-067-MY3, and in part by MOST 108-2218-E-369-001-). The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Dr. Han Hu.
CR Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148
   Arth C, 2009, INT SYM MIX AUGMENT, P73, DOI 10.1109/ISMAR.2009.5336494
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Cardonha C., 2013, Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility, P1, DOI DOI 10.1145/2461121.2461129
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Chen H. E., 2015, P ACM C EXT ABSTR HU
   Chen KW, 2017, IEEE T INTELL TRANSP, V18, P364, DOI 10.1109/TITS.2016.2570811
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   DAHLBACK N, 1993, KNOWL-BASED SYST, V6, P258, DOI 10.1016/0950-7051(93)90017-N
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Ducasse J, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P2186, DOI 10.1145/2858036.2858058
   Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022
   Eade E., 2006, P 2006 IEEE COMPUTER, VVolume 1, P469, DOI DOI 10.1109/CVPR.2006.263
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Feng C, 2015, P 10 ANN ACM IEEE IN, P107, DOI DOI 10.1145/2701973.2702060
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gong JX, 2002, GEOGR ANAL, V34, P155, DOI 10.1353/geo.2002.0010
   Guerreiro J, 2017, PROCEEDINGS OF THE 19TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY (ASSETS'17), P280, DOI 10.1145/3132525.3132545
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kanwal N, 2015, APPL BIONICS BIOMECH, V2015, DOI 10.1155/2015/479857
   Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Klein George, 2007, P1
   Ko E, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17081882
   Lee YH, 2016, COMPUT VIS IMAGE UND, V149, P3, DOI 10.1016/j.cviu.2016.03.019
   Li YP, 2012, LECT NOTES COMPUT SC, V7572, P15, DOI 10.1007/978-3-642-33718-5_2
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Middelberg S, 2014, LECT NOTES COMPUT SC, V8690, P268, DOI 10.1007/978-3-319-10605-2_18
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Nagaonkar V, 2015, CAPSTONE PROJECT
   Piao JC, 2019, IEEE T MULTIMEDIA, V21, P2827, DOI 10.1109/TMM.2019.2913324
   Pohlen T, 2017, PROC CVPR IEEE, P3309, DOI 10.1109/CVPR.2017.353
   Rice MT, 2013, CARTOGR GEOGR INF SC, V40, P210, DOI 10.1080/15230406.2013.799737
   Sato D, 2017, PROCEEDINGS OF THE 19TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY (ASSETS'17), P270, DOI 10.1145/3132525.3132535
   Sattler T, 2011, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2011.6126302
   Scheuerman MorganKlaus., 2017, Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems, P2922, DOI DOI 10.1145/3027063.3053260
   Schmid K, 2013, IEEE INT CONF ROBOT, P4671, DOI 10.1109/ICRA.2013.6631242
   Silva PM, 2016, IEEE T MULTIMEDIA, V18, P2432, DOI 10.1109/TMM.2016.2601029
   Song YF, 2016, IEEE T MULTIMEDIA, V18, P1542, DOI 10.1109/TMM.2016.2568743
   Soto MA, 2017, PROCEEDINGS OF THE 19TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY (ASSETS'17), P300, DOI 10.1145/3132525.3132556
   Stein GP, 2003, IEEE IV2003: INTELLIGENT VEHICLES SYMPOSIUM, PROCEEDINGS, P120, DOI 10.1109/IVS.2003.1212895
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   Tscharn R, 2016, ASSETS'16: PROCEEDINGS OF THE 18TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P295, DOI 10.1145/2982142.2982195
   Ungar S, 1997, J VISUAL IMPAIR BLIN, V91, P163
   Usenko V, 2016, IEEE INT CONF ROBOT, P1885, DOI 10.1109/ICRA.2016.7487335
   Wang ZS, 2010, IEEE T MULTIMEDIA, V12, P233, DOI 10.1109/TMM.2010.2046267
   Wu C., 2011, VisualSFM: A visual structure from motion system
   Yang RY, 2011, ASSETS 11: PROCEEDINGS OF THE 13TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P27
   Zeng Limin., 2017, Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services, P1
NR 52
TC 30
Z9 31
U1 4
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1567
EP 1580
DI 10.1109/TMM.2020.3001500
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300008
DA 2024-07-18
ER

PT J
AU Gu, XL
   Yu, J
   Wong, YK
   Kankanhalli, MS
AF Gu, Xiaoling
   Yu, Jun
   Wong, Yongkang
   Kankanhalli, Mohan S.
TI Toward Multi-Modal Conditioned Fashion Image Translation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generative adversarial network; fashion image synthesis; image-to-image
   translation
ID RETRIEVAL
AB Having the capability to synthesize photo-realistic fashion product images conditioned on multiple attributes or modalities would bring many new exciting applications. In this work, we propose an end-to-end network architecture that built upon a new generative adversarial network for automatically synthesizing photo-realistic images of fashion products under multiple conditions. Given an input pose image that consists of a 2D skeleton pose and a sentence description of products, our model synthesizes a fashion image preserving the same pose and wearing the fashion products described as the text. Specifically, the generator G tries to generate realistic-looking fashion images based on a < pose, text > pair condition to fool the discriminator. An attention network is added for enhancing the generator, which predicts a probability map indicating which part of the image needs to be attended for translation. In contrast, the discriminator D distinguishes real images from the translated ones based on the input pose image and text description. The discriminator is divided into two multi-scale sub-discriminators for improving image distinguishing task. Quantitative and qualitative analysis demonstrates that our method is capable of synthesizing realistic images that retain the poses of given images while matching the semantics of provided sentence descriptions.
C1 [Gu, Xiaoling; Yu, Jun] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China.
   [Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 119613, Singapore.
C3 Hangzhou Dianzi University; National University of Singapore
RP Yu, J (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China.
EM gxlisl@163.com; yujun@hdu.edu.cn; yongkang.wong@nus.edu.sg;
   mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015; Wong,
   Yongkang/0000-0002-1239-4428
FU National Science Foundation of China [61802100, 61836002, 61972119,
   61971172]; National Research Foundation, Singapore under its Strategic
   Capability Research Centres Funding Initiative
FX This work was supported in part by the National Science Foundation of
   China under Grants 61802100, 61836002, 61972119, and 61971172 and in
   part by the National Research Foundation, Singapore under its Strategic
   Capability Research Centres Funding Initiative. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Jianguo Zhang.
CR [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2018, ARXIV180606195
   [Anonymous], IN PRESS, DOI DOI 10.1109/TCSVT.2019.2947482
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Gu XL, 2019, IEEE T MULTIMEDIA, V21, P1524, DOI 10.1109/TMM.2018.2876822
   Han XT, 2019, IEEE I CONF COMP VIS, P4480, DOI 10.1109/ICCV.2019.00458
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Lassner C, 2017, IEEE I CONF COMP VIS, P853, DOI 10.1109/ICCV.2017.98
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Luan Q., 2007, P 18 EUR C CREND TEC, P309
   Ma LQ, 2017, ADV NEUR IN, V30
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Nasrollahi K, 2014, MACH VISION APPL, V25, P1423, DOI 10.1007/s00138-014-0623-4
   Park H., 2018, BRIT MACH VIS C BMVC
   Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899
   Raj A, 2018, LECT NOTES COMPUT SC, V11216, P679, DOI 10.1007/978-3-030-01258-8_41
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Rostamzadeh Negar, 2018, ARXIV180608317
   Salimans T, 2016, ADV NEUR IN, V29
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Wang C, 2018, LECT NOTES COMPUT SC, V11205, P796, DOI 10.1007/978-3-030-01246-5_47
   Wang CY, 2018, IEEE T IMAGE PROCESS, V27, P4066, DOI 10.1109/TIP.2018.2836316
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang W., 2019, PROC CVPR IEEE, P3064, DOI DOI 10.1109/CVPR.2019.00318
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie Junyuan, 2012, ADV NEURAL INFORM PR, P341, DOI [DOI 10.5555/2999134.2999173, DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605]
   Xiu Y., 2018, BMVC
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yu JC, 2019, IEEE T MAGN, V55, DOI 10.1109/TMAG.2019.2896177
   Yu J, 2020, IEEE T NEUR NET LEAR, V31, P661, DOI 10.1109/TNNLS.2019.2908982
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
   Zhao B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P383, DOI 10.1145/3240508.3240536
   Zhao B, 2016, IEEE T MULTIMEDIA, V18, P1111, DOI 10.1109/TMM.2016.2537783
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 57
TC 11
Z9 11
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2361
EP 2371
DI 10.1109/TMM.2020.3009500
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800016
OA hybrid
DA 2024-07-18
ER

PT J
AU Ionescu, B
   Rohm, M
   Boteanu, B
   Gînsca, AL
   Lupu, M
   Müller, H
AF Ionescu, Bogdan
   Rohm, Maia
   Boteanu, Bogdan
   Ginsca, Alexandru Lucian
   Lupu, Mihai
   Mueller, Henning
TI Benchmarking Image Retrieval Diversification Techniques for Social Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Diversification of image search results; social media information
   retrieval; multi-modal content description; MediaEval benchmarking
   initiative
ID INFORMATION
AB Image retrieval has been an active research domain for over 30 years and historically it has focused primarily on precision as an evaluation criterion. Similar to text retrieval, where the number of indexed documents became large and many relevant documents exist, it is of high importance to highlight diversity in the search results to provide better results for the user. The Retrieving Diverse Social Images Task of the MediaEval benchmarking campaign has addressed exactly this challenge of retrieving diverse and relevant results for the past years, specifically in the social media context. Multimodal data (e.g., images, text) was made available to the participants including metadata assigned to the images, user IDs, and precomputed visual and text descriptors. Many teams have participated in the task over the years. The large number of publications employing the data and also citations of the overview articles underline the importance of this topic. In this paper, we introduce these publicly available data resources as well as the evaluation framework, and provide an in-depth analysis of the crucial aspects of social image search diversification, such as the capabilities and the evolution of existing systems. These evaluation resources will help researchers for the coming years in analyzing aspects of multimodal image retrieval and diversity of the search results.
C1 [Ionescu, Bogdan; Boteanu, Bogdan] Univ Politehn Bucuresti, CAMPUS Res Ctr, Bucharest 060042, Romania.
   [Rohm, Maia] Vienna Univ Technol, Inst Visual Comp, A-1040 Vienna, Austria.
   [Rohm, Maia] Vienna Univ Technol, Human Ctr Technol, A-1040 Vienna, Austria.
   [Ginsca, Alexandru Lucian] Atos, F-95877 Bezons, France.
   [Lupu, Mihai] Vienna Univ Technol, Informat Management & Preservat Grp, A-1040 Vienna, Austria.
   [Mueller, Henning] Univ Appl Sci Western Switzerland HES SO, MedGIFT Lab, CH-3960 Sierre, Switzerland.
C3 National University of Science & Technology POLITEHNICA Bucharest;
   Technische Universitat Wien; Technische Universitat Wien; Technische
   Universitat Wien; University of Applied Sciences & Arts Western
   Switzerland
RP Ionescu, B (corresponding author), Univ Politehn Bucuresti, CAMPUS Res Ctr, Bucharest 060042, Romania.
EM bogdan.ionescu@upb.ro; maia.zaharieva@gmail.com;
   bboteanu@alpha.imag.pub.ro; alexgansca@gmail.com; mihai@mihailupu.net;
   henning.mueller@hevs.ch
RI Ionescu, Bogdan/IWU-7778-2023
OI Muller, Henning/0000-0001-6800-9878; Rohm, Maia/0000-0002-5108-6874
CR Abdi H, 2007, CONCISE ENCY STAT, P508
   Agrawal Rakesh, 2009, P 2 ACM INT C WEB SE, P5, DOI DOI 10.1145/1498759.1498766
   [Anonymous], 2014, ARXIV14116909
   [Anonymous], 2014, MULT SYST C 2014
   [Anonymous], 2010, P ACM INT C MULT
   [Anonymous], 2005, JOENS LEARN INSTR S
   [Anonymous], 2012, UAI 12
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Benevenuto F, 2012, IEEE T SYST MAN CY B, V42, P688, DOI 10.1109/TSMCB.2011.2173799
   Boato G, 2016, MULTIMED TOOLS APPL, V75, P5581, DOI 10.1007/s11042-015-2526-4
   Boteanu B, 2017, MULTIMED TOOLS APPL, V76, P11889, DOI 10.1007/s11042-016-3678-6
   Bradski G., 2008, LEARNING OPENCV COMP, DOI DOI 10.1109/MRA.2009.933612
   Buckley C., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P25, DOI 10.1145/1008992.1009000
   Bursuc A., 2013, P MEDIAEVAL WORKSH
   Calumby R. T., 2015, P MEDIEVAL WORKSH
   Capannini G, 2011, PROC VLDB ENDOW, V4, P451, DOI 10.14778/1988776.1988781
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Castillo C., 2011, P 20 INT C WORLD WID, P675, DOI DOI 10.1145/1963405.1963500
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   ChengXiang Zhai, 2008, Foundations and Trends in Information Retrieval, V2, P137, DOI 10.1561/1500000008
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Clarke C. L., 2008, P 31 ANN INT ACM SIG, P659, DOI [DOI 10.1145/1390334.1390446, 10.1145/1390334.1390446]
   Clarke C.L. A., 2012, TREC, P1
   Dang-Nguyen D.-T., 2019, P CLEF2019 WORKSH NO
   Dang-Pham D., 2015, Investigating the formation of information security climate perceptions with social network analysis: A research proposal, P1
   Deselaers Thomas., 2009, Proceedings of the ACM international conference on image and video retrieval, P1
   ENSER PGB, 1995, J DOC, V51, P126, DOI 10.1108/eb026946
   Ferreira, 2016, P MEDIEVAL WORKSH
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Gibbons JD., 2003, Nonparametric statistical inference, V4th, P645
   Ginsca Alexandru Lucian, 2015, Experimental IR Meets Multilinguality, Multimodality, and Interaction. 6th International Conference of the CLEF Association, CLEF'15. Proceedings: LNCS 9283, P41, DOI 10.1007/978-3-319-24027-5_4
   Ginsca AL, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1021, DOI 10.1145/2647868.2655033
   Grubinger M., 2006, P INT WORSKH ONT
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Ionescu, 2016, P INT C MULT SYST, P1
   Ionescu B., 2016, P MEDIAEVAL WORKSH
   Ionescu B, 2016, MULTIMED TOOLS APPL, V75, P1301, DOI 10.1007/s11042-014-2369-4
   Ionescu B, 2014, IEEE IMAGE PROC, P3072, DOI 10.1109/ICIP.2014.7025621
   Ionescu Bogdan, 2015, P 6 ACM MULT SYST C, P207
   Jain N., 2013, P WORK NOT MEDIAEVAL
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Joe Yue-Hei Ng, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P53, DOI 10.1109/CVPRW.2015.7301272
   KATO T, 1992, P SOC PHOTO-OPT INS, V1662, P112, DOI 10.1117/12.58497
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Levy O., 2015, Transactions of the Association for Computational Linguistics, V3, P211
   Lidon, 2015, P WORKK NOT MEDIEA
   Ludwig O, 2009, 2009 12TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC 2009), P432
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   McFee B., 2010, ICML, P775
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Müller H, 2001, PATTERN RECOGN LETT, V22, P593, DOI 10.1016/S0167-8655(00)00118-5
   Muller H., 2005, P C VIS INF SYST
   Negi S, 2016, INT CONF COMMUN SYST
   Negi S, 2014, INT C PATT RECOG, P3624, DOI 10.1109/ICPR.2014.623
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Olteanu Alexandra, 2013, Advances in Information Retrieval. 35th European Conference on IR Research, ECIR 2013. Proceedings, P557, DOI 10.1007/978-3-642-36973-5_47
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pang YW, 2011, COMPUT VIS IMAGE UND, V115, P352, DOI 10.1016/j.cviu.2010.10.010
   Paramita ML, 2010, LECT NOTES COMPUT SC, V6242, P45, DOI 10.1007/978-3-642-15751-6_6
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Popescu Adrian, 2013, P MEDIAEVAL WORKSH
   Radu Anca-Livia, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P25, DOI 10.1007/978-3-319-04114-8_3
   Radu A.-L., 2014, P MEDIAEVAL WORKSH
   Ravindranath S. S., 2015, P WORK NOT MEDIAEVAL
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   ROBERTSON SE, 1977, J DOC, V33, P294, DOI 10.1108/eb026647
   Roelleke T, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1187, DOI 10.1145/2348283.2348535
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P921, DOI 10.1109/TMM.2013.2237896
   Sabetghadam S., 2015, P MEDIAEVAL WORKSH
   Sanderson M, 2009, LECT NOTES COMPUT SC, V5478, P562, DOI 10.1007/978-3-642-00958-7_51
   Schedl M, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P947, DOI 10.1145/2766462.2767763
   Sermane P., 2013, ARXIV13126229
   Shi Y, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P175, DOI 10.1145/2348283.2348310
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith JR, 1998, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES - PROCEEDINGS, P112, DOI 10.1109/IVL.1998.694520
   Spyromitros-Xioufis E., 2015, P MEDIAEVAL WORKSH, V1436
   Spyromitros-Xioufis E., 2014, P WORK NOT MEDIAEVAL
   Spyromitros-Xioufis E, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P323, DOI 10.1145/2671188.2749334
   Stricker M., 1995, SPIE STORAGE RETRIEV, V2420
   Sun A., 2009, Proc. SIGMM Workshop on Social Media, P19
   Szucs, 2013, P MEDIAEVAL 2013 WOR
   Tanaka, 2011, P ACM S APPL COMP, P1724, DOI DOI 10.1145/1982185.1982546
   Tang XO, 1998, IEEE T IMAGE PROCESS, V7, P1602, DOI 10.1109/83.725367
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tollari S, 2016, P MEDIAEVAL WORKSH
   Tsikrika T, 2011, LECT NOTES COMPUT SC, V6941, P95, DOI 10.1007/978-3-642-23708-9_12
   Urbano J, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P393
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   Voorhees EM, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P806, DOI 10.1145/1571941.1572138
   Wan J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P157, DOI 10.1145/2647868.2654948
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Yang K., 2011, SOCIAL MEDIA MODEL C
   Yu Cong., 2009, EDBT
   ZHAI C.X., 2003, Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in informaion Retrieval (Toronto, Canada, July 28 - August 01, 2003), P10, DOI [10.1145/860435.860440, DOI 10.1145/860435.860440]
   Zheng KP, 2017, KNOWL INF SYST, V51, P1, DOI 10.1007/s10115-016-0990-4
NR 97
TC 7
Z9 7
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 677
EP 691
DI 10.1109/TMM.2020.2986579
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jiang, JM
   Fares, A
   Zhong, SH
AF Jiang, Jianmin
   Fares, Ahmed
   Zhong, Sheng-Hua
TI A Brain-Media Deep Framework Towards Seeing Imaginations Inside Brains
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Electroencephalography; Visualization; Brain; Deep learning; Gallium
   nitride; Feature extraction; Decoding; EEG; image generation; deep
   learning; brain-media; bi-directional computation; variant LSTM
ID REPRESENTATIONS; NETWORKS; EEG
AB While current research on multimedia is essentially dealing with the information derived from our observations of the world, internal activities inside human brains, such as imaginations and memories of past events etc., could become a brand new concept of multimedia, for which we coin as "brain-media". In this paper, we pioneer this idea by directly applying natural images to stimulate human brains and then collect the corresponding electroencephalogram (EEG) sequences to drive a deep framework to learn and visualize the corresponding brain activities. By examining the relevance between the visualized image and the stimulation image, we are able to assess the performance of our proposed deep framework in terms of not only the quality of such visualization but also the feasibility of introducing the new concept of "brain-media". To ensure that our explorative research is meaningful, we introduce a dually conditioned learning mechanism in the proposed deep framework. One condition is analyzing EEG sequences through deep learning to extract a more compact and class-dependent brain features via exploiting those unique characteristics of human brains such as hemispheric lateralization and biological neurons myelination (neurons importance), and the other is to analyze the content of images via computing approaches and extract representative visual features to exploit artificial intelligence in assisting our automated analysis of brain activities and their visualizations. By combining the brain feature space with the associated visual feature space of those images that are candidates of the stimuli, we are able to generate a combined-conditional space to support the proposed dual-conditioned and lateralization-supported GAN framework. Extensive experiments carried out illustrate that our proposed deep framework significantly outperforms the existing relevant work, indicating that our proposed does provide a good potential for further research upon the introduced concept of "brain-media", a new member for the big family of multimedia. To encourage more research along this direction, we make our source codes publicly available for downloading at GitHub. (1) (1) https://github.com/aneeg/LS-GAN.
C1 [Jiang, Jianmin; Fares, Ahmed; Zhong, Sheng-Hua] Shenzhen Univ, Coll Comp Sci & Software Engn, Res Inst Future Media Comp, Shenzhen 518060, Peoples R China.
   [Jiang, Jianmin; Zhong, Sheng-Hua] Shenzhen Univ, Guangdong Lab Artificial Intelligence & Digital E, Shenzhen 518060, Peoples R China.
   [Fares, Ahmed] Benha Univ, Fac Engn Shoubra, Comp Engn Branch, Dept Elect Engn, Cairo 2900, Egypt.
C3 Shenzhen University; Shenzhen University; Guangming Laboratory; Egyptian
   Knowledge Bank (EKB); Benha University
RP Fares, A (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Res Inst Future Media Comp, Shenzhen 518060, Peoples R China.
EM jianmin.jiang@szu.edu.cn; ahmed.fares@szu.edu.cn; csshzhong@szu.edu.cn
FU Natural Science Foundation China (NSFC) [61620106008]; Shenzhen
   University Research Foundation
FX This work was supported in part by the Natural Science Foundation China
   (NSFC) under Grant 61620106008 and in part by Shenzhen University
   Research Foundation funding (2018-2020).
CR Al-Hadithi N, 2016, ASIAN J MED SCI, V7, P47, DOI [10.3126/ajms.v7i6.15205, DOI 10.3126/AJMS.V7I6.15205]
   Cong FY, 2013, IEEE T MULTIMEDIA, V15, P1060, DOI 10.1109/TMM.2013.2253452
   Das K, 2010, NEUROIMAGE, V51, P1425, DOI 10.1016/j.neuroimage.2010.03.030
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gogna A, 2017, IEEE T BIO-MED ENG, V64, P2196, DOI 10.1109/TBME.2016.2631620
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Green AM, 2011, TRENDS NEUROSCI, V34, P61, DOI 10.1016/j.tins.2010.11.003
   Heusel M., 2017, ADV NEURAL INFORM PR, V30, P6626
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jiang JM, 2019, IEEE T HUM-MACH SYST, V49, P611, DOI 10.1109/THMS.2019.2904615
   Jinyoung Moon, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P447, DOI 10.1007/978-3-319-14442-9_50
   Kaneshiro B, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0135697
   Kavasidis I, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1809, DOI 10.1145/3123266.3127907
   Kingma D. P., 2014, arXiv
   Kriegeskorte N, 2008, FRONT SYST NEUROSCI, V2, DOI 10.3389/neuro.06.004.2008
   Kriegeskorte N, 2008, NEURON, V60, P1126, DOI 10.1016/j.neuron.2008.10.043
   Kulasingham JP, 2016, IEEE EMBS CONF BIO, P127, DOI 10.1109/IECBES.2016.7843428
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li R., 2018, ARXIV181207697
   Liu XW, 2020, IEEE T MULTIMEDIA, V22, P949, DOI 10.1109/TMM.2019.2934425
   Lu N, 2017, IEEE T NEUR SYS REH, V25, P566, DOI 10.1109/TNSRE.2016.2601240
   Makeig S, 2002, SCIENCE, V295, P690, DOI 10.1126/science.1066168
   Moon SE, 2017, IEEE T MULTIMEDIA, V19, P340, DOI 10.1109/TMM.2016.2614880
   Müller-Putz GR, 2008, IEEE T BIO-MED ENG, V55, P361, DOI 10.1109/TBME.2007.897815
   Mukherjee P, 2019, IEEE IMAGE PROC, P4539, DOI [10.1109/icip.2019.8803717, 10.1109/ICIP.2019.8803717]
   Niedermeyer E., 1982, ELECTROEN CLIN NEURO, P752
   Ogawa T, 2018, IEEE ACCESS, V6, P61401, DOI 10.1109/ACCESS.2018.2876710
   Palazzo S, 2017, IEEE I CONF COMP VIS, P3430, DOI 10.1109/ICCV.2017.369
   Palazzo S., 2020, IEEE SYST J, P1
   Radford A., 2015, ARXIV
   Rodríguez P, 2020, IEEE T MULTIMEDIA, V22, P502, DOI 10.1109/TMM.2019.2928494
   Salimans T, 2016, ADV NEUR IN, V29
   Schirrmeister RT, 2017, HUM BRAIN MAPP, V38, P5391, DOI 10.1002/hbm.23730
   Spampinato C, 2017, PROC CVPR IEEE, P4503, DOI 10.1109/CVPR.2017.479
   Stober S., 2015, ARXIV151104306
   Tirupattur P, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P950, DOI 10.1145/3240508.3240641
   Wang E., 2009, P 17 ACM INT C MULTI, P945
   Wang J, 2016, COGN NEUROPSYCHOL, V33, P257, DOI 10.1080/02643294.2016.1182480
   Wu HB, 2019, IEEE T AUTOM SCI ENG, V16, P1698, DOI 10.1109/TASE.2019.2895104
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Yin Z, 2017, CHIN CONTR CONF, P10966, DOI 10.23919/ChiCC.2017.8029107
   Zhang DL, 2018, AAAI CONF ARTIF INTE, P1703
   Zhang H, 2019, PROCEEDINGS OF 2019 IEEE 3RD INTERNATIONAL ELECTRICAL AND ENERGY CONFERENCE (CIEEC), P735, DOI 10.1109/CIEEC47146.2019.CIEEC-2019293
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhong SH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1295, DOI 10.1145/3343031.3350886
NR 45
TC 5
Z9 5
U1 1
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1454
EP 1465
DI 10.1109/TMM.2020.2999183
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200021
DA 2024-07-18
ER

PT J
AU Li, MY
   Zhang, ZY
   Yu, J
   Chen, CW
AF Li, Mengyan
   Zhang, Zhaoyu
   Yu, Jun
   Chen, Chang Wen
TI Learning Face Image Super-Resolution Through Facial Semantic Attribute
   Transformation and Self-Attentive Structure Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face; Semantics; Feature extraction; Task analysis; Facial features;
   Face super-resolution; face hallucination; facial attribute
   transformation; facial structure enhancement
ID GENERATIVE ADVERSARIAL NETWORKS; HALLUCINATION; REGRESSION; RETRIEVAL
AB Face super-resolution is a domain-specific super-resolution (SR) problem of generating high-resolution (HR) face images from low-resolution (LR) inputs. Even though existing face SR methods have achieved great performance on the global region evaluation, most of them cannot restore local attributes and structure reasonably, especially to ultra-resolve tiny LR face images (16 x 16 pixels) to its larger version (8 x upscaling factor). In this paper, we propose an open source face SR framework based on facial semantic attribute transformation and self-attentive structure enhancement. Specifically, the proposed framework introduces face semantic information (i.e., face attributes) and face structure information (i.e., face boundaries) in a successive two-stage fashion. In the first stage, an Attribute Transformation Network (AT-Net) is established. It upsamples LR face images to HR feature maps and then combines facial attributes with these features to generate the intermediate HR results with rational attributes. In the second stage, a Structure Enhancement Network (SE-Net) is built. It simultaneously extracts face features and estimates facial boundary heatmaps from the inputs, and then fuses them to output the final HR face images. Extensive experiments demonstrate that our method achieves superior super-resolved results and outperforms the state-of-the-art methods.
C1 [Li, Mengyan; Zhang, Zhaoyu; Yu, Jun] Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
   [Chen, Chang Wen] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; State University of New York (SUNY) System; State University
   of New York (SUNY) Buffalo
RP Yu, J (corresponding author), Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
EM limmy@mail.ustc.edu.cn; zzy95@mail.ustc.edu.cn; harryjun@ustc.edu.cn;
   chencw@buffalo.edu
OI Chen, Chang Wen/0000-0002-6720-234X
FU National Natural Science Foundation of China [U1736123, 61572450]; USTC
   Research Funds of the Double First-Class Initiative [YD2350002001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1736123 and 61572450 and in part by
   USTC Research Funds of the Double First-Class Initiative (YD2350002001).
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.90
   Arjovsky M, 2017, arXiv preprint arXiv:1701.07875
   Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616
   Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao Q., 2017, CVPR, P690
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Ding CX, 2018, IEEE T PATTERN ANAL, V40, P1002, DOI 10.1109/TPAMI.2017.2700390
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Grm K., 2014, P C COMP VIS PATT RE
   Gulrajani Ishaan, 2017, P ADV NEUR INF PROC, P5767
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Huang GQ, 2009, INT J COMPUT INTEG M, V22, P579, DOI 10.1080/09511920701724934
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Ignatov Andrey, 2018, CVPRW, P691
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang JJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P771
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Jing CC, 2019, IEEE T MULTIMEDIA, V21, P782, DOI 10.1109/TMM.2018.2866222
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Junjun Jiang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P212, DOI 10.1109/ICME.2012.152
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kolouri S, 2015, PROC CVPR IEEE, P4876, DOI 10.1109/CVPR.2015.7299121
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Lee C.-H., 2018, P C COMP VIS PATT RE, P721
   Li YC, 2014, PATTERN RECOGN, V47, P1261, DOI 10.1016/j.patcog.2013.09.012
   Liu C, 2001, PROC CVPR IEEE, P192
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lü Z, 2018, INT CONF SYST INFORM, P288, DOI 10.1109/ICSAI.2018.8599363
   Ma X, 2010, PATTERN RECOGN, V43, P2224, DOI 10.1016/j.patcog.2009.12.019
   Mao QR, 2017, IEEE T MULTIMEDIA, V19, P861, DOI 10.1109/TMM.2016.2629282
   Masi I, 2019, IEEE T PATTERN ANAL, V41, P379, DOI 10.1109/TPAMI.2018.2792452
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Shi JG, 2019, IEEE T MULTIMEDIA, V21, P2223, DOI 10.1109/TMM.2019.2898752
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Song YB, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4537
   Tong T., 2017, P IEEE INT C COMP VI, P4799
   Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171
   Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Yang CY, 2013, PROC CVPR IEEE, P1099, DOI 10.1109/CVPR.2013.146
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yu X., 2017, CVPR
   Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14
   Yu X, 2020, IEEE T PATTERN ANAL, V42, P2926, DOI 10.1109/TPAMI.2019.2916881
   Yu X, 2020, IEEE T PATTERN ANAL, V42, P2148, DOI 10.1109/TPAMI.2019.2914039
   Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101
   Yu X, 2017, AAAI CONF ARTIF INTE, P4327
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   Zhang K, 2018, EUR C COMP VIS ECCV, P183
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhou EJ, 2015, AAAI CONF ARTIF INTE, P3871
   Zhou JT, 2019, IEEE T INF FOREN SEC, V14, P2537, DOI 10.1109/TIFS.2019.2900907
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37
NR 65
TC 18
Z9 21
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 468
EP 483
DI 10.1109/TMM.2020.2984092
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600037
DA 2024-07-18
ER

PT J
AU Lin, QB
   Cao, WM
   He, ZQ
   He, ZH
AF Lin, Qiubin
   Cao, Wenming
   He, Zhiquan
   He, Zhihai
TI Mask Cross-Modal Hashing Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic mask; inter-modal similarity; intra-modal similarity; hashing
   network; cross-modal retrieval
AB Due to the rapid development of deep learning, cross-modal retrieval has achieved significant progress in recent years. Moreover, cross-modal hashing has recently attracted considerable attention to multi-modal retrieval applications due to its advantages of low storage costs and fast retrieval speed. However, it is still a challenging problem due to an existing semantic heterogeneity gap between different modalities. In order to further narrow the gap and obtain more effective hash codes, we put forward a novel mask deep cross-modal hashing (MDCH) approach to explore the similarity between inter-modal instances. The main contributions of this paper are that: (1) we attempt to introduce semantic mask information into cross-modal hashing retrieval, (2) we alternately train intra-modal and inter-modal networks to fully mine the semantic relationship between different modalities. The semantic mask can improve the semantic information of the image feature. While inter-modal similarity, explored by inter-modal networks, focuses on enforcing images and their corresponding text tags to have similar hash codes, intra-modal similarity, explored by intra-modal networks, can retain local structural information embedded in each modality to achieve internal similarity. A large number of experiments conducted on three datasets demonstrate that our proposed MDCH approach is superior to several state-of-the-art cross-modal hashing approaches.
C1 [Lin, Qiubin; Cao, Wenming; He, Zhiquan] Shenzhen Univ, Guangdong Multimedia Informat Serv Engn Technol R, Shenzhen 518060, Peoples R China.
   [He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Video Proc & Commun Lab, Columbia, MO 65211 USA.
C3 Shenzhen University; University of Missouri System; University of
   Missouri Columbia
RP Cao, WM (corresponding author), Shenzhen Univ, Guangdong Multimedia Informat Serv Engn Technol R, Shenzhen 518060, Peoples R China.
EM 2170269126@email.szu.edu.cn; wmcao@szu.edu.cn; zhiquan@szu.edu.cn;
   hezi@missouri.edu
RI cao, wenming/Y-5293-2019
OI cao, wenming/0000-0002-8174-6167; Lin, Qiubin/0000-0003-0221-2464
FU National Science Foundation of China [61771322, 61375015]; China
   Scholarship Council; Fundamental Research Foundation of Shenzhen
   [JCYJ20160307154630057]
FX This work was supported in part by the National Science Foundation of
   China under Grants 61771322 and 61375015, in part by China Scholarship
   Council, and in part by the Fundamental Research Foundation of Shenzhen
   under Grant JCYJ20160307154630057. The associate editor coordinating the
   reviewof this manuscript and approving it for publication was Dr Guo-Jun
   Qi.
CR [Anonymous], 2012, NEURIPS
   Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Chen LC, 2018, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2018.00422
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Hu Y, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P527, DOI 10.1145/2647868.2654906
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Huang X, 2019, IEEE T MULTIMEDIA, V21, P2850, DOI 10.1109/TMM.2019.2911456
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Jin L, 2018, IEEE T IMAGE PROCESS, V27, P1405, DOI 10.1109/TIP.2017.2776745
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li K, 2017, IEEE T PATTERN ANAL, V39, P1825, DOI 10.1109/TPAMI.2016.2610969
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Pinheiro P.O., 2015, NEURIPS, P1990
   Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5
   Rastegari Mohammad, 2013, Book Predictable Dual-View Hashing
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890
   Yang EK, 2017, AAAI CONF ARTIF INTE, P1618
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang X, 2018, LECT NOTES COMPUT SC, V11219, P614, DOI 10.1007/978-3-030-01267-0_36
   Zhang ZY, 2016, PROC CVPR IEEE, P669, DOI 10.1109/CVPR.2016.79
   Zhang ZY, 2015, IEEE I CONF COMP VIS, P2614, DOI 10.1109/ICCV.2015.300
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
NR 42
TC 29
Z9 31
U1 4
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 550
EP 558
DI 10.1109/TMM.2020.2984081
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200003
DA 2024-07-18
ER

PT J
AU Su, YT
   Li, YQ
   Song, D
   Liu, AA
   Nie, J
AF Su, Yuting
   Li, Yuqian
   Song, Dan
   Liu, Anan
   Nie, Jie
TI Joint Intermediate Domain Generation and Distribution Alignment for 2D
   Image-Based 3D Objects Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Two dimensional displays; Visualization;
   Task analysis; Shape; Feature extraction; Computational modeling; 3D
   Object retrieval; domain adaptation; distribution alignment; feature
   learning
ID NETWORKS
AB 2D image-based 3D object retrieval provides a convenient way to manage 3D big data with easily accessed 2D images. It is also a challenging task due to the significant differences between 2D images and 3D objects. In this paper, we propose a 2D image-based 3D object retrieval method, which can reduce the distribution discrepancy between 2D images and 3D objects and learn invariant features between them. Specifically, we first construct an intermediate domain module based on maximum mean discrepancy (MMD) in an unsupervised way, which can reduce the 2D and 3D distribution discrepancy by marginal distribution constraint. Second, to further reduce conditional distribution discrepancy and learn invariant features, we use source domain labels as semantic information to dynamically guide distribution alignment. Moreover, in order to support the research in 3D object retrieval, we contribute a new dataset, MDI3D. We conducted extensive experiments on MDI3D and some popular datasets, such as MI3DOR and SHREC2013. The experimental results demonstrate the superiority of the proposed method by comparing with the state-of-the-art methods.
C1 [Su, Yuting; Li, Yuqian; Song, Dan; Liu, Anan] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Nie, Jie] Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
C3 Tianjin University; Ocean University of China
RP Song, D; Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Nie, J (corresponding author), Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
EM ytsu@tju.edu.cn; liyq@tju.edu.cn; dan.song@tju.edu.cn;
   anan0422@gmail.com; niejie@ouc.edu.cn
RI Li, Yuqian/K-4137-2015; Nie, Jie/ABG-9228-2021
OI Nie, Jie/0000-0003-4952-7666
FU National Natural Science Foundation of China [61572356, 61772359,
   61872267, 61902277, 61702471]; grant of Tianjin New Generation
   Artificial Intelligence Major Program [19ZXZNGX00110, 18ZXZNGX00150];
   Open Project Program of the State Key Lab of CAD AMP; CG, Zhejiang
   University [A2005, A2012]; grant of Elite Scholar Program of Tianjin
   University [2019XRX-0035]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61572356, 61772359, 61872267, 61902277,
   and 61702471, in part by the grant of Tianjin New Generation Artificial
   Intelligence Major Program (19ZXZNGX00110, 18ZXZNGX00150), in part by
   the Open Project Program of the State Key Lab of CAD & CG, Zhejiang
   University under Grants A2005 and A2012, and in part by the grant of
   Elite Scholar Program of Tianjin University (2019XRX-0035).
CR [Anonymous], 2008, AAAI
   Baktashmotlagh M, 2014, PROC CVPR IEEE, P2481, DOI 10.1109/CVPR.2014.318
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen JX, 2019, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2019.00088
   Chen JX, 2018, LECT NOTES COMPUT SC, V11217, P624, DOI 10.1007/978-3-030-01261-8_37
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai GX, 2017, AAAI CONF ARTIF INTE, P4002
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gao Y, 2014, IEEE MULTIMEDIA, V21, P52, DOI 10.1109/MMUL.2014.20
   Gao Z, 2018, AAAI CONF ARTIF INTE, P2223
   Ghadai S, 2019, IEEE COMPUT SOC CONF, P1152, DOI [10.1109/CVPRW.2019.00150, 10.1109/cvprw.2019.00150]
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Grabner A, 2019, INT CONF 3D VISION, P583, DOI 10.1109/3DV.2019.00070
   Grabner A, 2018, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2018.00319
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P5814, DOI 10.1109/TIP.2016.2614132
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee T, 2018, INT CONF 3D VISION, P258, DOI 10.1109/3DV.2018.00038
   Li B., 2013, EUR WORKSH 3D OBJ RE, P89, DOI DOI 10.2312/3DOR/3DOR13/089
   Li B, 2014, EUR WORKSH 3D OBJ RE, P121
   Li JJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P747, DOI 10.1145/3343031.3350902
   Li Wenhui, 2019, PROC 3DOR EUROGRAPHI, P103
   Li ZQ, 2019, AAAI CONF ARTIF INTE, P8682
   Liu AA, 2018, IEEE T CYBERNETICS, V48, P916, DOI 10.1109/TCYB.2017.2664503
   Liu AA, 2016, IEEE T IMAGE PROCESS, V25, P2103, DOI 10.1109/TIP.2016.2540802
   Liu AA, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P828
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Massa F, 2016, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2016.648
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Savva M., 2017, P WORKSH 3D OBJ RETR, P39, DOI DOI 10.2312/3DOR.20171050
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su YT, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P913
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Wang Q, 2020, IEEE T IMAGE PROCESS
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiao C., 2019, PROC CVPR IEEE, P12436, DOI DOI 10.1109/CVPR.2019.01272
   Xie SA, 2018, PR MACH LEARN RES, V80
   Xu C, 2018, AAAI CONF ARTIF INTE, P7428
   Yan C., 2020, IEEE T PATTERN ANAL
   Yao T, 2015, PROC CVPR IEEE, P2142, DOI 10.1109/CVPR.2015.7298826
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhang L, 2019, IEEE T NEUR NET LEAR, V30, P3759, DOI 10.1109/TNNLS.2019.2899037
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
   Zhu J, 2019, PATTERN RECOGN LETT, V119, P24, DOI 10.1016/j.patrec.2017.09.041
NR 58
TC 7
Z9 8
U1 4
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2127
EP 2138
DI 10.1109/TMM.2020.3008056
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100022
DA 2024-07-18
ER

PT J
AU Tao, F
   Busso, C
AF Tao, Fei
   Busso, Carlos
TI End-to-End Audiovisual Speech Recognition System With Multitask Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Feature extraction; Speech processing;
   Acoustics; Robustness; Timing; Audiovisual speech recognition; deep
   learning; multitask learning; end-to-end speech systems
ID NEURAL-NETWORK
AB An automatic speech recognition (ASR) system is a key component in current speech-based systems. However, the surrounding acoustic noise can severely degrade the performance of an ASR system. An appealing solution to address this problem is to augment conventional audio-based ASR systems with visual features describing lip activity. This paper proposes a novel end-to-end, multitask learning (MTL), audiovisual ASR (AV-ASR) system. A key novelty of the approach is the use of MTL, where the primary task is AV-ASR, and the secondary task is audiovisual voice activity detection (AV-VAD). We obtain a robust and accurate audiovisual system that generalizes across conditions. By detecting segments with speech activity, the AV-ASR performance improves as its connectionist temporal classification (CTC) loss function can leverage from the AV-VAD alignment information. Furthermore, the end-to-end system learns from the raw audiovisual inputs a discriminative high-level representation for both speech tasks, providing the flexibility to mine information directly from the data. The proposed architecture considers the temporal dynamics within and across modalities, providing an appealing and practical fusion scheme. We evaluate the proposed approach on a large audiovisual corpus (over 60 hours), which contains different channel and environmental conditions, comparing the results with competitive single task learning (STL) and MTL baselines. Although our main goal is to improve the performance of our ASR task, the experimental results show that the proposed approach can achieve the best performance across all conditions for both speech tasks. In addition to state-of-the-art performance in AV-ASR, the proposed solution can also provide valuable information about speech activity, solving two of the most important tasks in speech-based applications.
C1 [Tao, Fei; Busso, Carlos] Univ Texas Dallas, Dept Elect & Comp Engn, Richardson, TX 75080 USA.
C3 University of Texas System; University of Texas Dallas
RP Busso, C (corresponding author), Univ Texas Dallas, Dept Elect & Comp Engn, Richardson, TX 75080 USA.
EM tctf86@gmail.com; busso@utdallas.edu
OI Busso, Carlos/0000-0002-4075-4072; Tao, Fei/0000-0003-4834-8949
FU National Science Foundation (NSF) [IIS-1718944]
FX This work was supported by the National Science Foundation (NSF) under
   Grant IIS-1718944.
CR Abdelwahab M, 2018, IEEE-ACM T AUDIO SPE, V26, P2423, DOI 10.1109/TASLP.2018.2867099
   Ahmad R, 2013, 2013 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND DATA MINING (CIDM), P174, DOI 10.1109/CIDM.2013.6597233
   Amodei D, 2016, PR MACH LEARN RES, V48
   [Anonymous], 2011, P WORKSH NEW TOOLS M
   [Anonymous], 2008, SIGN PROC C 2008 16
   [Anonymous], 2009, PROC AVSP
   [Anonymous], 2008, PROC INT C MACHINE L
   [Anonymous], 2011, P ICML
   Ariav I, 2018, SIGNAL PROCESS, V142, P69, DOI 10.1016/j.sigpro.2017.07.006
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chaudhuri S, 2018, INTERSPEECH, P1239
   Chen NX, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P185
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   Dov D, 2015, IEEE-ACM T AUDIO SPE, V23, P732, DOI 10.1109/TASLP.2015.2405481
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Elmadany NE, 2019, IEEE T MULTIMEDIA, V21, P1317, DOI 10.1109/TMM.2018.2875510
   Fan X, 2011, EUR SIGNAL PR CONF, P1500
   Fei Tao, 2018, IEEE/ACM Transactions on Audio, Speech and Language Processing, V26, P1286, DOI 10.1109/TASLP.2018.2815268
   Gao L, 2019, IEEE T MULTIMEDIA, V21, P375, DOI 10.1109/TMM.2018.2859590
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Graves A, 2014, PR MACH LEARN RES, V32, P1764
   Harte N, 2015, IEEE T MULTIMEDIA, V17, P603, DOI 10.1109/TMM.2015.2407694
   Huang J, 2013, INT CONF ACOUST SPEE, P7596, DOI 10.1109/ICASSP.2013.6639140
   Katsaggelos AK, 2015, P IEEE, V103, P1635, DOI 10.1109/JPROC.2015.2459017
   Kingma D. P., 2014, arXiv
   Li NX, 2013, IEEE T MULTIMEDIA, V15, P1213, DOI 10.1109/TMM.2013.2241416
   Lotfian R, 2018, INTERSPEECH, P951, DOI 10.21437/Interspeech.2018-2464
   Lyons J, 2016, PYTHON SPEECH FEATUR
   NETI C, 2000, WORKSH 2000 FIN REP
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Parthasarathy S, 2018, INTERSPEECH, P3698, DOI 10.21437/Interspeech.2018-1391
   Parthasarathy S, 2017, INTERSPEECH, P1103, DOI 10.21437/Interspeech.2017-1494
   Parveen S., 2003, P EUR C SPEECH COMM, P1813
   Petridis S, 2018, IEEE W SP LANG TECH, P513, DOI 10.1109/SLT.2018.8639643
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6548, DOI 10.1109/ICASSP.2018.8461326
   Petridis S, 2016, INT CONF ACOUST SPEE, P2304, DOI 10.1109/ICASSP.2016.7472088
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   Potamianos G., 2017, HDB MULTIMODAL MULTI, V1, P489
   Sanabria R., 2016, ARXIV161106986, P1
   Saon G, 2017, INTERSPEECH, P132, DOI 10.21437/Interspeech.2017-405
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stafylakis T, 2017, INTERSPEECH, P3652, DOI 10.21437/Interspeech.2017-85
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   Tran T, 2013, INT CONF ACOUST SPEE, P8101, DOI 10.1109/ICASSP.2013.6639243
   Tao F, 2019, SPEECH COMMUN, V113, P25, DOI 10.1016/j.specom.2019.07.003
   Tao F, 2018, INTERSPEECH, P1244
   Tao F, 2017, INTERSPEECH, P1938, DOI 10.21437/Interspeech.2017-1573
   Tao F, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6209, DOI 10.1109/ICASSP.2018.8461617
   Tao F, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2906, DOI 10.1109/ICASSP.2018.8461750
   Tao F, 2016, INTERSPEECH, P2130, DOI 10.21437/Interspeech.2016-406
   Tao F, 2014, INTERSPEECH, P1154
   Tao F, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2302
   Thermos S, 2016, IEEE W SP LANG TECH, P579, DOI 10.1109/SLT.2016.7846321
   Wu PP, 2016, IEEE T MULTIMEDIA, V18, P326, DOI 10.1109/TMM.2016.2520091
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Xu Q, 2018, 2018 5TH IEEE INTERNATIONAL CONFERENCE ON CYBER SECURITY AND CLOUD COMPUTING (IEEE CSCLOUD 2018) / 2018 4TH IEEE INTERNATIONAL CONFERENCE ON EDGE COMPUTING AND SCALABLE CLOUD (IEEE EDGECOM 2018), P1, DOI 10.1109/CSCloud/EdgeCom.2018.00010
   Yan JJ, 2016, IEEE T MULTIMEDIA, V18, P1319, DOI 10.1109/TMM.2016.2557721
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
NR 59
TC 51
Z9 52
U1 3
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1
EP 11
DI 10.1109/TMM.2020.2975922
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600001
OA hybrid
DA 2024-07-18
ER

PT J
AU Wang, ZJ
   Zhang, Z
   Luo, YD
   Huang, Z
   Shen, HT
AF Wang, Zijian
   Zhang, Zheng
   Luo, Yandan
   Huang, Zi
   Shen, Heng Tao
TI Deep Collaborative Discrete Hashing With Semantic-Invariant Structure
   Construction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Visualization; Binary codes; Quantization (signal); Image
   coding; Collaboration; Correlation; Collaborative hashing; semantic
   encoding; image retrieval; discriminative learning
ID BINARY-CODES; SCALE; REPRESENTATION; QUANTIZATION; RETRIEVAL
AB While deep hashing has made great progress in large-scale multimedia retrieval, most of the existing approaches under-explore the semantic correlations and neglect the effect of context-aware visual learning. In this paper, we propose a dual-stream learning framework, termed as Deep Collaborative Discrete Hashing (DCDH), which constructs a discriminative common discrete space by collaboratively incorporating the shared and individual semantics deduced from visual features and semantics. Specifically, DCDH generates context-aware representations by employing the outer product of visual embeddings and semantic encodings. To further preserve the original semantics and alleviate the class imbalance problem, we introduce the focal loss to take advantage of frequent and rare concepts. Furthermore, a common binary code space is constructed based on the joint learning of the visual representations, the context-aware representations, and the label distribution calibration. Three losses, i.e., the pairwise similarity loss, the quantization loss, and the balanced classification loss, are collaboratively optimized in the general learning framework of DCDH. Extensive experiments conducted on three large-scale benchmark datasets demonstrate the superiority of the proposed method, yielding the state-of-the-art image retrieval performance.
C1 [Wang, Zijian; Luo, Yandan; Huang, Zi] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
   [Zhang, Zheng] Harbin Inst Technol, Biocomp Res Ctr, Shenzhen 518055, Peoples R China.
   [Zhang, Zheng] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.
   [Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 University of Queensland; Harbin Institute of Technology; Harbin
   Institute of Technology; University of Electronic Science & Technology
   of China; University of Electronic Science & Technology of China
RP Zhang, Z (corresponding author), Harbin Inst Technol, Biocomp Res Ctr, Shenzhen 518055, Peoples R China.
EM zijian.wang@uq.net.au; darrenzz219@gmail.com; lyadanluol@gmail.com;
   huang@itee.uq.edu.au; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021; Luo, Yadan/AAY-5893-2021; Wang,
   Zijian/IQU-2128-2023; Zhang, Zhang/JAX-2097-2023
OI Wang, Zijian/0000-0001-8770-1755; Wang, Zijian/0000-0002-7190-9620; Luo,
   Yadan/0000-0001-6272-2971; Zhang, Zheng/0000-0003-1470-6998; HUANG,
   ZI/0000-0002-9738-4949
FU ARC [DP190102353]; Guangdong Basic and Applied Basic Research Foundation
   [2019A1515110475]
FX This work was supported in part by the ARC Discovery Project underGrant
   DP190102353, and in part by the Guangdong Basic and Applied Basic
   Research Foundation under Grant 2019A1515110475.
CR [Anonymous], 2017, 31 C NEUR INF PROC S
   [Anonymous], 2009, NIPS
   Cao Y, 2017, PROC CVPR IEEE, P916, DOI 10.1109/CVPR.2017.104
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gui J, 2018, IEEE T PATTERN ANAL, V40, P490, DOI 10.1109/TPAMI.2017.2678475
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu MQ, 2018, IEEE T IMAGE PROCESS, V27, P545, DOI 10.1109/TIP.2017.2749147
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jiang QY, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2248
   Kang WC, 2016, AAAI CONF ARTIF INTE, P1230
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li Q, 2017, ADV NEUR IN, V30
   Li W.-J., 2015, ARXIV151103855
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P755, DOI 10.1145/3240508.3240543
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Luo YD, 2018, PATTERN RECOGN, V75, P128, DOI 10.1016/j.patcog.2017.02.034
   Norouzi M.E., 2011, ICML
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Rao C, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, RELIABILITY AND SECURITY (QRS 2018), P397, DOI 10.1109/QRS.2018.00053
   Salakhutdinov R., 2007, AISTATS, V2, P412
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Shen FM, 2016, IEEE T IMAGE PROCESS, V25, P5610, DOI 10.1109/TIP.2016.2612883
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen FM, 2015, IEEE T IMAGE PROCESS, V24, P1839, DOI 10.1109/TIP.2015.2405340
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang ZJ, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P905, DOI 10.1145/3331184.3331275
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xie L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3133
   Xu C., 2013, arXiv
   Xu X, 2020, IEEE T CYBERNETICS, V50, P2400, DOI 10.1109/TCYB.2019.2928180
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812
   Yang Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1286, DOI 10.1145/2964284.2964319
   Yang Y, 2015, IEEE T CYBERNETICS, V45, P1069, DOI 10.1109/TCYB.2014.2344015
   Zhang JQ, 2019, INT J IMMUNOPATH PH, V33, P1, DOI 10.1177/2058738419857550
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zhang PF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1762, DOI 10.1145/3123266.3123320
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhang Z, 2019, IEEE T IMAGE PROCESS, V28, P4803, DOI 10.1109/TIP.2019.2912290
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhou X, 2020, IEEE T CYBERNETICS, V50, P1460, DOI 10.1109/TCYB.2018.2883970
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
   Zhu L, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3365841
   Zhu XF, 2017, IEEE T MULTIMEDIA, V19, P2033, DOI 10.1109/TMM.2017.2703636
NR 64
TC 25
Z9 25
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1274
EP 1286
DI 10.1109/TMM.2020.2995267
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200008
DA 2024-07-18
ER

PT J
AU Wu, XP
   Chen, QC
   Xiao, YL
   Li, W
   Liu, X
   Hu, BT
AF Wu, Xiangping
   Chen, Qingcai
   Xiao, Yulun
   Li, Wei
   Liu, Xin
   Hu, Baotian
TI LCSegNet: An Efficient Semantic Segmentation Network for Large-Scale
   Complex Chinese Character Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Character recognition; Feature extraction; Image segmentation;
   Semantics; Task analysis; Encoding; Text recognition; Character
   recognition; complex scene; handwriting recognition; label coding;
   large-scale categories; semantic segmentation
ID SCENE TEXT RECOGNITION; NEURAL-NETWORK; COOCCURRENCE; MULTICLASS;
   RETRIEVAL; CONTEXT
AB Complex scene character recognition is a challenging yet important task in machine learning, especially for languages with large character sets, such as Chinese, which is composed of hieroglyphics with large-scale categories and similar glyphs. Recently, state-of-the-art methods based on semantic segmentation have achieved great success in scene parsing and have been applied in scene text recognition. However, because of limitations in terms of memory and computation, they are only applied in the small category recognition tasks, such as tasks involving English alphabets and digits. In this paper, we propose an efficient semantic segmentation model based on label coding (LC), called LCSegNet, to recognize large-scale Chinese characters. First, to reduce the number of labels, we design a new label coding method based on the Wubi Chinese characters code, called Wubi-CRF. In this method, glyphs and structure information of Chinese characters are encoded into 140-bit labels. Second, we employ an efficient semantic segmentation model for pixel-wise prediction and utilize a conditional random field (CRF) module to learn the constraint rules of Wubi-like coding. Finally, experiments are conducted on three benchmarks: a large Chinese text dataset in the wild (CTW), ICDAR2019-ReCTS, and HIT-OR3C dataset. Results show that the proposed method achieves state-of-the-art performances in both complex scene and handwritten character recognition tasks.
C1 [Wu, Xiangping; Chen, Qingcai; Xiao, Yulun; Li, Wei; Liu, Xin; Hu, Baotian] Harbin Inst Technol Shenzhen, Shenzhen Chinese Callig Digital Simulat Engn Lab, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology
RP Chen, QC (corresponding author), Harbin Inst Technol Shenzhen, Shenzhen Chinese Callig Digital Simulat Engn Lab, Shenzhen 518055, Peoples R China.
EM wxpleduole@gmail.com; qingcai.chen@hit.edu.cn; xiaoyulun@stu.hit.edu.cn;
   weili_hitwh@163.com; hit.liuxin@gmail.com; baotian.nlp@gmail.com
RI Chen, Qingcai/JVN-1580-2024; Hu, Baotian/AAA-4102-2022; Wu,
   Xiangping/AAX-8958-2021
OI Hu, Baotian/0000-0001-7490-684X; Wu, Xiangping/0000-0002-5267-2250; liu,
   xin/0000-0003-2802-594X
FU Natural Science Foundation of China [61872113, 61876052, U1813215];
   Strategic Emerging Industry Development Special Funds of Shenzhen
   [XMHT20190108009, JCYJ20190806112, JCYJ20180306172232154]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61872113, 61876052, and U1813215, and in part by the
   Strategic Emerging Industry Development Special Funds of Shenzhen under
   Grants XMHT20190108009, JCYJ20190806112, and JCYJ20180306172232154.
CR Bai JF, 2014, IEEE IMAGE PROC, P2560, DOI 10.1109/ICIP.2014.7025518
   Bhunia AK, 2018, INT C PATT RECOG, P3645, DOI 10.1109/ICPR.2018.8545184
   Bhunia AK, 2019, PROC CVPR IEEE, P4762, DOI 10.1109/CVPR.2019.00490
   Chen K., 2019, MMDETECTION OPEN MML, V1906
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dezhi Peng, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P25, DOI 10.1109/ICDAR.2019.00014
   Dietterich TG, 1994, J ARTIF INTELL RES, V2, P263
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu Jun, 2019, IEEE Trans Image Process, DOI 10.1109/TIP.2019.2895460
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   García-Pedrajas N, 2006, IEEE T PATTERN ANAL, V28, P1001, DOI 10.1109/TPAMI.2006.123
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Hahnloser R. H., 2018, RES PAPERS, P10
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Iandola F. N., 2016, ARXIV160207360, DOI 10.1007/978-3-319-24553-9
   Jianshu Zhang, 2020, Pattern Recognition, V103, DOI 10.1016/j.patcog.2020.107305
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liao MH, 2019, AAAI CONF ARTIF INTE, P8714
   Liao Minghui, 2019, PAMI
   Lin GS, 2018, IEEE T PATTERN ANAL, V40, P1352, DOI 10.1109/TPAMI.2017.2708714
   Lin QX, 2018, LECT NOTES COMPUT SC, V11258, P41, DOI 10.1007/978-3-030-03338-5_4
   Liu CX, 2019, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2019.00017
   Lu SJ, 2008, IEEE T PATTERN ANAL, V30, P1913, DOI 10.1109/TPAMI.2008.89
   Lu SJ, 2008, PATTERN RECOGN, V41, P1799, DOI 10.1016/j.patcog.2007.10.017
   Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rocha A, 2014, IEEE T NEUR NET LEAR, V25, P289, DOI 10.1109/TNNLS.2013.2274735
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rui Zhang, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1577, DOI 10.1109/ICDAR.2019.00253
   Sermanet P., 2014, INT C LEARN REPR
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Sheng FF, 2017, LECT NOTES COMPUT SC, V10636, P180, DOI 10.1007/978-3-319-70090-8_19
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, PROC INT CONF DOC, P1429, DOI 10.1109/ICDAR.2017.233
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi XL, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P594
   Shuyong Bai, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P331, DOI 10.1109/ICDAR.2009.54
   Su BL, 2017, PATTERN RECOGN, V63, P397, DOI 10.1016/j.patcog.2016.10.016
   Su BL, 2015, LECT NOTES COMPUT SC, V9003, P35, DOI 10.1007/978-3-319-16865-4_3
   Su BL, 2014, INT C PATT RECOG, P2926, DOI 10.1109/ICPR.2014.504
   Sui BL, 2015, PROC INT CONF DOC, P386, DOI 10.1109/ICDAR.2015.7333789
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang W, 2019, IEEE INT SYMP PARAL, P784, DOI 10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00117
   Tang YB, 2018, IEEE T MULTIMEDIA, V20, P2276, DOI 10.1109/TMM.2018.2802644
   Tian SX, 2016, PATTERN RECOGN, V51, P125, DOI 10.1016/j.patcog.2015.07.009
   Tian SX, 2013, PROC INT CONF DOC, P912, DOI 10.1109/ICDAR.2013.186
   Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
   Wang QF, 2012, IEEE T PATTERN ANAL, V34, P1469, DOI 10.1109/TPAMI.2011.264
   Wang TQ, 2018, AAAI CONF ARTIF INTE, P2540
   Wang TQ, 2018, IEEE DATA MINING, P1302, DOI 10.1109/ICDM.2018.00176
   Wang Y., 2019, INT J DOCUMENT ANAL, P1
   Wang YN, 2017, PROC INT CONF DOC, P363, DOI 10.1109/ICDAR.2017.67
   Wang YN, 2017, PATTERN RECOGN LETT, V97, P69, DOI 10.1016/j.patrec.2017.07.011
   Wu CJ, 2019, PROC INT CONF DOC, P122, DOI 10.1109/ICDARW.2019.40092
   Wu SL, 2019, IET IMAGE PROCESS, V13, P2744, DOI 10.1049/iet-ipr.2018.6588
   Wu XP, 2019, IEEE SIGNAL PROC LET, V26, P597, DOI 10.1109/LSP.2019.2895967
   Xiao XF, 2017, PATTERN RECOGN, V72, P72, DOI 10.1016/j.patcog.2017.06.032
   Xie ZC, 2018, IEEE T PATTERN ANAL, V40, P1903, DOI 10.1109/TPAMI.2017.2732978
   Yang F, 2018, LECT NOTES ARTIF INT, V11108, P184, DOI 10.1007/978-3-319-99495-6_16
   Yuan TL, 2019, J COMPUT SCI TECH-CH, V34, P509, DOI 10.1007/s11390-019-1923-y
   Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216
   Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang JS, 2018, IEEE INT CON MULTI
   Zhang QZ, 2018, INT CONF DAT MIN WOR, P1134, DOI 10.1109/ICDMW.2018.00163
   Zhang XY, 2017, PATTERN RECOGN, V61, P348, DOI 10.1016/j.patcog.2016.08.005
   Zhang Z, 2016, PROCEEDINGS OF 12TH IAPR WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS, (DAS 2016), P340, DOI 10.1109/DAS.2016.32
   Zhang Z, 2018, IEEE ACCESS, V6, P35734, DOI 10.1109/ACCESS.2018.2848930
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhong ZY, 2015, PROC INT CONF DOC, P846, DOI 10.1109/ICDAR.2015.7333881
   Zhou SS, 2011, PROC INT CONF DOC, P1150, DOI 10.1109/ICDAR.2011.232
   Zhou Shusen, 2010, P INT WORKSH DOC AN, P223, DOI 10.1145/1815330.1815359
   Zhou XD, 2013, IEEE T PATTERN ANAL, V35, P2413, DOI 10.1109/TPAMI.2013.49
NR 83
TC 10
Z9 12
U1 8
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3427
EP 3440
DI 10.1109/TMM.2020.3025696
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000037
DA 2024-07-18
ER

PT J
AU Wu, Y
   Liu, XL
   Qin, HT
   Xia, K
   Hu, S
   Ma, YQ
   Wang, M
AF Wu, Yan
   Liu, Xianglong
   Qin, Haotong
   Xia, Ke
   Hu, Sheng
   Ma, Yuqing
   Wang, Meng
TI Boosting Temporal Binary Coding for Large-Scale Video Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Binary codes; Boosting; Encoding; Semantics; Indexing;
   Feature extraction; Large-scale video search; binary code learning;
   locality-sensitive hashing; temporal consistency; multi-table indexing
ID QUANTIZATION; IMAGE; CODES
AB In recent years, there has been an explosive increase in the amount of existing visual data. Hashing techniques have been successfully applied to deal with the large-scale nearest neighbor search problem among data on this massive scale. However, existing hashing methods usually learn a single hash code for each data point, and only by taking the content correlations among them into account. In practice, however, when handling complex visual data such as video, strong temporal relations exist among the successive frames. Moreover, if the preferred performance for large-scale video search is to be delivered, multiple hash codes are required for each data point in order to build multiple hash table indices. To address these problems, in this paper, we first study the multi-table learning problem for video search and attempt to learn binary codes by capturing the intrinsic video similarities from both the visual and the temporal aspects. By regarding the search over multiple tables as an ensemble prediction, the whole multi-table learning problem can be solved in a boosting learning manner to complementarily cover the nearest neighbors. For each table, a temporal binary coding solution is devised that thinks over the intrinsic relations among the visual content and the temporal consistency among the successive frames simultaneously. More specifically, we approximate the intrinsic visual similarities using a low-rank matrix based on sparse, non-negative feature expression. Furthermore, to essentially preserve the temporal consistency, we introduce a subspace rotation to model the variation among the successive frames. Under the boosting learning framework, the binary codes, hash functions and temporal variation of each table can be efficiently and jointly optimized. Extensive experiments on three large video datasets demonstrate that the proposed approach significantly outperforms a number of state-of-the-art hashing methods.
C1 [Wu, Yan] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Liu, Xianglong; Qin, Haotong; Xia, Ke; Hu, Sheng; Ma, Yuqing] Beihang Univ, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
   [Liu, Xianglong] Beihang Univ, Beijing Adv Innovat Ctr Big Data Based Precis Med, Beijing 100191, Peoples R China.
   [Wang, Meng] Hefei Univ Technol, Sch Comp & Informat Sci, Hefei 230009, Peoples R China.
C3 Beihang University; Beihang University; Beihang University; Hefei
   University of Technology
RP Liu, XL (corresponding author), Beihang Univ, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
EM wuy@bjast.ac.cn; xlliu@nlsde.buaa.edu.cn; qinhaotong@nlsde.buaa.edu.cn;
   xiake@nlsde.buaa.edu.cn; husheng@nlsde.buaa.edu.cn;
   mayuqing@nlsde.buaa.edu.cn; eric.mengwang@gmail.com
RI Qin, Haotong/AGP-1834-2022; Wang, Meng/ITR-8699-2023
OI Qin, Haotong/0000-0001-7391-7539; Liu, Xianglong/0000-0002-7618-3275; ,
   Yuqing/0000-0003-1936-9396
FU National Natural Science Foundation of China [61872021, 61690202];
   Beijing Nova Program of Science and Technology [Z191100001119050]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 61872021 and 61690202, and in part by Beijing Nova
   Program of Science and Technology (Z191100001119050). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Guo-Jun Qi.
CR [Anonymous], 2013, P IEEE C COMP VIS PA
   [Anonymous], 2016, P 24 ACM INT C MULT
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.553
   [Anonymous], 2009, NIPS
   [Anonymous], 2014, IEEE CVPR
   [Anonymous], 2009, NEURIPS
   Cao Liangliang., 2012, Proceedings of the 20th ACM International Conference on Multimedia, P299
   Chen WC, 2011, 2011 IEEE PACIFIC RIM CONFERENCE ON COMMUNICATIONS, COMPUTERS AND SIGNAL PROCESSING (PACRIM), P422, DOI 10.1109/PACRIM.2011.6032930
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dean T., 2013, PROC IEEE C COMPUT V, P1
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gu SM, 2013, INT CONF MACH LEARN, P108, DOI 10.1109/ICMLC.2013.6890453
   He J., 2010, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P1129
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hu H, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P905, DOI 10.1145/2939672.2939774
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jain P., 2010, NIPS, P928
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Jin L, 2019, IEEE T NEUR NET LEAR, V30, P1429, DOI 10.1109/TNNLS.2018.2869601
   Jin L, 2018, IEEE T IMAGE PROCESS, V27, P1405, DOI 10.1109/TIP.2017.2776745
   Jing CC, 2019, IEEE T MULTIMEDIA, V21, P782, DOI 10.1109/TMM.2018.2866222
   Joly A, 2011, PROC CVPR IEEE, P873, DOI 10.1109/CVPR.2011.5995709
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Li K., 2018, T MULTIMEDIA COMPUT, V14
   Li K, 2017, IEEE T PATTERN ANAL, V39, P1825, DOI 10.1109/TPAMI.2016.2610969
   Li M, 2015, IEEE T INF FOREN SEC, V10, P1727, DOI 10.1109/TIFS.2015.2425362
   Li SY, 2020, IEEE T MULTIMEDIA, V22, P1542, DOI 10.1109/TMM.2019.2946096
   Li WJ, 2016, IJCAI, P1711
   Li Y, 2015, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2015.7299108
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liu W., 2010, PROC ICML, P679
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu X., 2013, PROC ICCREM 2013 CON, P626, DOI DOI 10.1061/9780784413135.05
   Liu XL, 2020, IEEE T NEUR NET LEAR, V31, P5312, DOI 10.1109/TNNLS.2020.2965992
   Liu XL, 2017, AAAI CONF ARTIF INTE, P4183
   Liu XL, 2015, IEEE I CONF COMP VIS, P1107, DOI 10.1109/ICCV.2015.132
   Liu XL, 2015, IEEE T CYBERNETICS, V45, P1811, DOI 10.1109/TCYB.2014.2360856
   Liu XL, 2014, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2014.275
   Liu XL, 2013, PROC CVPR IEEE, P1570, DOI 10.1109/CVPR.2013.206
   Liu XL, 2014, PATTERN RECOGN, V47, P748, DOI 10.1016/j.patcog.2013.08.022
   Lv Q, 2007, P 33 INT C VER LARG, P950
   Ma C, 2018, NEURAL PROCESS LETT, V47, P877, DOI 10.1007/s11063-018-9812-x
   Mu YD, 2014, PROC CVPR IEEE, P979, DOI 10.1109/CVPR.2014.130
   Mu YD, 2010, AAAI CONF ARTIF INTE, P539
   Nie L., 2012, P 20 ACM INT C MULTI, P59, DOI DOI 10.1145/2393347.2393363
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Rao NS, 2017, 2017 INTERNATIONAL CONFERENCE ON CURRENT TRENDS IN COMPUTER, ELECTRICAL, ELECTRONICS AND COMMUNICATION (CTCEEC), P477, DOI 10.1109/CTCEEC.2017.8455022
   Sandeep R, 2017, PROCEEDINGS OF THE ASME 12TH INTERNATIONAL MANUFACTURING SCIENCE AND ENGINEERING CONFERENCE - 2017, VOL 1
   Shen FM, 2016, IEEE T IMAGE PROCESS, V25, P5610, DOI 10.1109/TIP.2016.2612883
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Sun JF, 2016, STUD COMPUT INTELL, V647, P1, DOI 10.1007/978-3-319-33353-3_1
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2013, IEEE I CONF COMP VIS, P3032, DOI 10.1109/ICCV.2013.377
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang Jun., 2010, ICML, P1127
   Wang W, 2015, 2015 IEEE 16TH INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT), P309, DOI 10.1109/ICCT.2015.7399849
   Wang X., 2019, ARXIV190200617
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Weiss Y, 2012, LECT NOTES COMPUT SC, V7576, P340, DOI 10.1007/978-3-642-33715-4_25
   Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1
   Wu GS, 2019, IEEE T IMAGE PROCESS, V28, P1993, DOI 10.1109/TIP.2018.2882155
   Xia K, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P333, DOI 10.1145/3123266.3123273
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Xu D, 2012, IEEE INFOCOM SER, P2881, DOI 10.1109/INFCOM.2012.6195720
   Xu H, 2011, IEEE I CONF COMP VIS, P1631, DOI 10.1109/ICCV.2011.6126424
   Ye GN, 2013, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2013.282
   Ye J, 2020, IEEE T PATTERN ANAL, V42, P126, DOI 10.1109/TPAMI.2018.2874455
   Ye J, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P99, DOI 10.1145/2671188.2749340
   Zhang HW, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P781, DOI 10.1145/2964284.2964308
   Zhou L, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107040
NR 78
TC 8
Z9 8
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 353
EP 364
DI 10.1109/TMM.2020.2978593
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600028
DA 2024-07-18
ER

PT J
AU Zhuang, PQ
   Wang, YL
   Qiao, Y
AF Zhuang, Peiqin
   Wang, Yali
   Qiao, Yu
TI Wildfish plus plus : A Comprehensive Fish Benchmark for Multimedia
   Research
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Benchmark testing; Fish; Visualization; Feature
   extraction; Morphology; WildFish plus plus; Fine-Grained Recognition
   with Comparison Texts; Open-Set Classification; Cross-Modal Retrieval;
   Automatic Fish Classification
ID RECOGNITION
AB In this paper, we develop a large-scale vision-language fish benchmark, namely WildFish++, for comprehensive studies in multimedia research. Concretely, WildFish++ consists of 2,348 fish categories with 103,034 images in the wild, and 3,817 fish descriptions with 213,858 words. Based on these distinct characteristics, we mainly introduce four challenging research tasks on WildFish++. (1) Fine-Grained Recognition with Comparison Texts. WildFish++ naturally contains subtle difference among fish categories, which leads to fine-grained classification. Most approaches resort to tackle this problem by capturing discriminative regions in the view of each image. However, this paradigm may be still far way from extracting the most distinct features when the context on visual difference is not available. In this case, we propose to introduce comparison fish descriptions, a unique corpus that can directly point out subtle difference between highly-confused species and naturally serve as a kind of valuable context information. With such texts, we creatively elaborate a multi-modal fish network, aiming at incorporating those comparison textual information as prior knowledge and consequently leveraging it to guide CNNs to find subtle yet distinct regions in the context of comparison texts. (2) Open-Set Classification. We often confront with unknown categories in practice, e.g., there may still exist unknown fishes in our planet. Hence, we creatively adapt WildFish++ for a novel open-set classification task, which aims at correctly assigning each test image into the unknown class or one of known classes. More importantly, we investigate a number of practical designs to boost accuracy of deep learning models in open-set scenarios. (3) Cross-Modal Retrieval. WildFish++ not only contains diversified fish images in the wild but also has rich fish descriptions about morphology diagnosis, biology information, etc. Hence, we design a challenging cross-modal retrieval task, which leverages three subtasks such as text-to-text, text-to-image, image-to-text retrieval in a unified end-to-end framework. (4) Automatic Fish Classification. Automatic fish classification is a long-term research in marine biology, while current studies are unsatisfactory due to the lack of large-scale data. In this case, we train a number of CNNs with WildFish++, and use its pre-trained models to boost fish classification on most existing benchmarks of wild fishes. We will release WildFish++ with codes/protocols (https://github.com/PeiqinZhuang/WildFish++). We believe it can promote relevant studies in multimedia and beyond.
C1 [Zhuang, Peiqin] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Wang, Yali; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Hong Kong Macao Joint Lab Human Machine, Shenzhen 518055, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese Academy of Sciences; Shenzhen Institute of Advanced
   Technology, CAS
RP Qiao, Y (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Hong Kong Macao Joint Lab Human Machine, Shenzhen 518055, Peoples R China.
EM zpq0316@163.com; yl.wang@siat.ac.cn; yu.qiao@siat.ac.cn
RI wang, ya/HQZ-7558-2023; Wang, Yahui/JCO-6233-2023; Qiao,
   Yu/ABD-5787-2021; Yu, Qiao/IAP-6999-2023
OI Zhuang, Peiqin/0000-0002-1724-8980
FU Guangdong Special Support Program [2016TX03X276]; National Natural
   Science Foundation of China [61876176, U1713208]; Shenzhen Basic
   Research Program [CXB201104220032 A]; Joint Laboratory of CAS-HK
FX This work was supported in part by Guangdong Special Support Program
   under Grant 2016TX03X276, in part by the National Natural Science
   Foundation of China underGrants 61876176, U1713208, and in part by the
   Shenzhen Basic Research Program (CXB201104220032 A), the Joint
   Laboratory of CAS-HK. This work was done during his internship at
   Shenzhen Institutes of Advanced Technology ChineseAcademy of Sciences.
   The associate editor coordinating the reviewof this manuscript and
   approving it for publication was Prof. Jinhui Tang. (Peiqin Zhuang and
   YaliWang contributed equally to this work.) (Corresponding author: Yu
   Qiao.)
CR Anantharajah K, 2014, IEEE WINT CONF APPL, P309, DOI 10.1109/WACV.2014.6836084
   [Anonymous], 2013, Tech. rep.
   [Anonymous], P 2017 C EMPIRICAL M
   Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173
   Bendale A, 2015, PROC CVPR IEEE, P1893, DOI 10.1109/CVPR.2015.7298799
   Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259
   Bingham E., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P245, DOI 10.1145/502512.502546
   Boom BJ, 2012, INT C PATT RECOG, P1542
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Fu YW, 2018, IEEE SIGNAL PROC MAG, V35, P112, DOI 10.1109/MSP.2017.2763441
   Ge Zongyuan, 2017, BMVC
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1740, DOI 10.1145/3343031.3350974
   He XT, 2017, AAAI CONF ARTIF INTE, P4075
   He XT, 2017, PROC CVPR IEEE, P7332, DOI 10.1109/CVPR.2017.775
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hou SH, 2017, IEEE I CONF COMP VIS, P541, DOI 10.1109/ICCV.2017.66
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   iNaturalist Competition, 2018, P C COMP VIS PATT RE, P241
   Jain LP, 2014, LECT NOTES COMPUT SC, V8691, P393, DOI 10.1007/978-3-319-10578-9_26
   Khosla A., 2011, P IEEE C COMP VIS PA, V2
   Kiros R, 2015, 29 ANN C NEURAL INFO, V28
   Kiros Ryan., 2014, ABS14112539 CORR
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li D., 2003, P 11 ACM INT C MULTI, P604
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Mac Aodha O, 2019, IEEE I CONF COMP VIS, P9595, DOI 10.1109/ICCV.2019.00969
   Mikolov T., 2013, INT C LEARNING REPRE, P1
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Rashtchian C, 2009, P N AM CHAPT ASS COM, P248
   Rasiwasia N, 2012, P ACM MULT, P1949
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sattar H, 2015, PROC CVPR IEEE, P981, DOI 10.1109/CVPR.2015.7298700
   Scheirer WJ, 2014, IEEE T PATTERN ANAL, V36, P2317, DOI 10.1109/TPAMI.2014.2321392
   Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256
   Scheirer WJ, 2011, IEEE T PATTERN ANAL, V33, P1689, DOI 10.1109/TPAMI.2011.54
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sutskever I., 2012, P C NEUR INF PROC SY, P730
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhuang PQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1301, DOI 10.1145/3240508.3240616
NR 65
TC 17
Z9 19
U1 7
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3603
EP 3617
DI 10.1109/TMM.2020.3028482
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100014
DA 2024-07-18
ER

PT J
AU Xu, YZ
   Hu, JC
   Wattanachote, K
   Zeng, K
   Gong, YY
AF Xu, Yongzhe
   Hu, Jiangchuan
   Wattanachote, Kanoksak
   Zeng, Kun
   Gong, YongYi
TI Sketch-Based Shape Retrieval via Best View Selection and a Cross-Domain
   Similarity Measure
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Three-dimensional displays; Feature extraction; Rendering
   (computer graphics); Shape measurement; Solid modeling; Computer vision;
   Sketch; 3D shape; similarity model; best view selection
AB Retrieving 3D shapes from 2D human sketches has received increasing attention in computer vision and computer graphics. Most previous methods projected 3D shapes from numerous viewpoints and then extracted features of 3D shapes from these projections and calculated the similarity with sketches. However, due to the unknown pose of 3D shapes, viewpoints were usually sampled uniformly from a sphere coordinate. Hence, some projections acquired insufficient descriptions of 3D shapes. In this paper, we proposed a view selection algorithm to find the most reasonable viewpoints, which can benefit representation learning for 3D shapes. Additionally, to indicate the apparent discrepancy between sketches and 3D shapes, we leveraged a generalized similarity model to encourage the accuracy of cross-domain feature matching. We first computed line renderings of 3D shapes from an enormous number of viewpoints. Then, we calculated the similarity of shapes between line renderings and sketches. In this vein, we obtained several superior projections. Second, we implemented a sketch network to extract features of the sketch and a shape network to extract features of projections. We combined the features of different projections to secure the compact representation for 3D shapes. Finally, a metric network was constructed using a cross-domain similarity model, and we trained the metric network with triplet loss. Online hard sample mining was leveraged to accelerate the convergence of the network. We evaluated our method on SHREC13 and SHREC14 sketch track benchmark datasets. The experimental results demonstrated that both view selection and cross-domain similarity models were able to encourage retrieval performance.
C1 [Xu, Yongzhe; Hu, Jiangchuan; Zeng, Kun] Sun Yat Sen Univ, Sch Software, Guangzhou 510006, Peoples R China.
   [Wattanachote, Kanoksak; Gong, YongYi] Guangdong Univ Foreign Studies, Sch Informat, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University; Guangdong University of Foreign Studies
RP Zeng, K (corresponding author), Sun Yat Sen Univ, Sch Software, Guangzhou 510006, Peoples R China.
EM xuyzh6@mail2.sysu.edu.cn; hujch3@mail2.sysu.edu.cn;
   kanoksak.wattanachote@gmail.com; zengkun2@mail.sysu.edu.cn;
   gongyongyi@gdufs.edu.cn
FU National Science Foundation Grant of China [U1711266, 61370160,
   61772149]; Natural Science Foundation of Guangdong Province
   [201904010228]; Guangdong Basic and Applied Basic Research Foundation
   [2019A1515011078, 2015A030313578]
FX This work was supported in part by the National Science Foundation Grant
   of China under Grants U1711266, 61370160, and 61772149, in part by the
   Natural Science Foundation of Guangdong Province under Grant
   201904010228, and in part by the Guangdong Basic and Applied Basic
   Research Foundation under Grants 2019A1515011078 and 2015A030313578. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Z. Liu.
CR [Anonymous], 2017, CORR
   Bae SH, 2008, UIST 2008: PROCEEDINGS OF THE 21ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P151
   Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Chen JX, 2019, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2019.00088
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai GX, 2018, IEEE T IMAGE PROCESS, V27, P3374, DOI 10.1109/TIP.2018.2817042
   Dawei Lu, 2013, Advances in Multimedia Information Processing - PCM 2013. 14th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 8294, P396, DOI 10.1007/978-3-319-03731-8_37
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Eissen Koss., 2011, Sketching - The Basics
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185527
   Eitz M, 2011, IEEE T VIS COMPUT GR, V17, P1624, DOI 10.1109/TVCG.2010.266
   Furuya T, 2013, 2013 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P274, DOI 10.1109/CW.2013.60
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470
   Khan FA., 2018, BMC PLANT BIOL, V8, P800, DOI [DOI 10.1186/s12870-014-0327-y, 10.3390/met8100800]
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A., 2012, ADV NEURAL INF PROCE, V25, P1097
   Li B, 2014, EUR WORKSH 3D OBJ RE, P121
   Li B, 2015, COMPUT VIS IMAGE UND, V131, P1, DOI 10.1016/j.cviu.2014.10.006
   Li B, 2014, COMPUT VIS IMAGE UND, V119, P57, DOI 10.1016/j.cviu.2013.11.008
   Li B, 2013, MULTIMED TOOLS APPL, V65, P363, DOI 10.1007/s11042-012-1009-0
   Li HH, 2017, IEEE INT CON MULTI, P1434, DOI 10.1109/ICME.2017.8019464
   Li Y, 2015, COMPUT VIS IMAGE UND, V137, P1, DOI 10.1016/j.cviu.2015.02.003
   Lin L, 2017, IEEE T PATTERN ANAL, V39, P1089, DOI 10.1109/TPAMI.2016.2567386
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Nie L., 2016, Synthesis Lectures on Information Concepts Retrieval & Services, V8, P1, DOI 10.2200/S00714ED1V01Y201603ICR048
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qi YG, 2016, IEEE IMAGE PROC, P2460, DOI 10.1109/ICIP.2016.7532801
   Qu LL, 2013, PROCEEDINGS OF 2013 IEEE 11TH INTERNATIONAL CONFERENCE ON ELECTRONIC MEASUREMENT & INSTRUMENTS (ICEMI), P84, DOI 10.1109/ICEMI.2013.6743046
   Radenovic F, 2018, LECT NOTES COMPUT SC, V11209, P774, DOI 10.1007/978-3-030-01228-1_46
   Saavedra J.M., 2012, 3DOR, P47
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Schneider RG, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661231
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Seddati O, 2015, INT WORK CONTENT MUL
   Shao C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185541
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Song JF, 2017, IEEE I CONF COMP VIS, P5552, DOI 10.1109/ICCV.2017.592
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang WX, 2018, PERIOD POLYTECH-CHEM, V62, P8, DOI 10.3311/PPch.11148
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie J, 2017, PROC CVPR IEEE, P3615, DOI 10.1109/CVPR.2017.385
   Xie J, 2017, IEEE T MULTIMEDIA, V19, P2463, DOI 10.1109/TMM.2017.2698200
   Xu BX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601128
   Yasseen Z, 2017, VISUAL COMPUT, V33, P565, DOI 10.1007/s00371-016-1328-7
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Yu Q, 2017, INT J COMPUT VISION, V122, P411, DOI 10.1007/s11263-016-0932-3
   Zhang X., 2019, Pattern recognition letters
   Zhao L, 2015, VISUAL COMPUT, V31, P765, DOI 10.1007/s00371-015-1091-1
   Zou CQ, 2014, IEEE SIGNAL PROC LET, V21, P966, DOI 10.1109/LSP.2014.2321764
NR 60
TC 16
Z9 18
U1 3
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2950
EP 2962
DI 10.1109/TMM.2020.2966882
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900015
DA 2024-07-18
ER

PT J
AU Li, L
   Zhang, WM
   Chen, KJ
   Yu, NH
AF Li, Li
   Zhang, Weiming
   Chen, Kejiang
   Yu, Nenghai
TI Steganographic Security Analysis From Side Channel Steganalysis and Its
   Complementary Attacks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Steganographer detection; social websites; side channel steganalysis;
   complementary attack; secure region
ID BATCH STEGANOGRAPHY; SOCIAL NETWORKS; ROBUST; FEATURES; PAYLOAD
AB Side channel steganalysis refers to detecting a steganographer in social websites via behavior analysis. In this paper, we first design a side channel steganalysis based on the correlation between image sequences of social users, which aims to find out the behaviorally anomalous steganographer. According to the experimental results of side channel steganalysis, it is intuitively secure for the steganographer to act identically to normal social users since she can avoid being detected by side channel steganalysis. However, when faced with various detection methods, is it still secure to behave similar to a normal user? To comprehensively consider the detection means and further explore the secure behavior region of the steganographer, we design a complementary attack of side channel steganalysis. Specifically, we take the correlation of contents of images as side information and take the images with similar content as references to calibrate steganalysis features, which helps improve traditional steganalysis. The proposed side channel steganalysis and its complementary attack efficiently detect steganographers from two different aspects. When the average rank of the steganographer is used to measure the performance, side channel steganalysis can rank the steganographer within the top ten in 100 actors, and the complementary attack can raise the average rank of the steganographer by three places compared with the previous method. From the perspective of the steganographer on social networks, it can help her behave in a more secure region, where her behavior should neither deviate from that of normal users nor be too similar to that of normal users.
C1 [Li, Li; Zhang, Weiming; Chen, Kejiang; Yu, Nenghai] Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhang, WM (corresponding author), Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
EM Erin@mail.ustc.edu.cn; zhangwm@ustc.edu.cn; chenkj@mail.ustc.edu.cn;
   ynh@ustc.edu.cn
RI Chen, Kejiang/ABD-7057-2020
OI Chen, Kejiang/0000-0002-9868-3414; Zhang, Weiming/0000-0001-5576-6108
FU Natural Science Foundation of China [U1636201, 61572452]; Anhui
   Initiative in Quantum Information Technologies [AHY150400]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants U1636201 and 61572452, and in part by the Anhui
   Initiative in Quantum Information Technologies under Grant AHY150400.
CR Abdoun A., 2017, IMPERIAL J INTERDISC, V3, P225
   [Anonymous], [No title captured]
   Borges PVK, 2008, IEEE T MULTIMEDIA, V10, P1479, DOI 10.1109/TMM.2008.2007294
   Cogranne R, 2017, INT CONF ACOUST SPEE, P2122, DOI 10.1109/ICASSP.2017.7952531
   Fei Geli., 2013, ICWSM
   Fridrich J, 2003, LECT NOTES COMPUT SC, V2578, P310
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Guo LJ, 2015, IEEE T INF FOREN SEC, V10, P2669, DOI 10.1109/TIFS.2015.2473815
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Hu X, 2014, AAAI CONF ARTIF INTE, P59
   Ker A.D., 2013, P 1 ACM WORKSH INF H, P4558, DOI DOI 10.1145/2482513.2482965
   Ker AD, 2012, P MULT SEC, P1
   Ker AD, 2007, LECT NOTES COMPUT SC, V4437, P265
   Ker AD, 2014, IEEE T INF FOREN SEC, V9, P1424, DOI 10.1109/TIFS.2014.2336380
   Kodovsky J, 2012, PROC SPIE, V8303, DOI 10.1117/12.907495
   Kodovsky J, 2009, MM&SEC'09: PROCEEDINGS OF THE 2009 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P63
   Li B, 2018, IEEE T INF FOREN SEC, V13, P1242, DOI 10.1109/TIFS.2017.2780805
   Li FY, 2018, IEEE ACCESS, V6, P29912, DOI 10.1109/ACCESS.2018.2841415
   Li FY, 2016, IEEE T INF FOREN SEC, V11, P344, DOI 10.1109/TIFS.2015.2496910
   Li L, 2019, MULTIMED TOOLS APPL, V78, P8041, DOI 10.1007/s11042-018-6582-4
   Lie WN, 2005, IEEE T MULTIMEDIA, V7, P1007, DOI 10.1109/TMM.2005.858377
   Liu SS, 2018, IEEE ACCESS, V6, P51297, DOI 10.1109/ACCESS.2018.2869699
   Lu Y, 2014, P INT CONF BUS INTEL, P36, DOI 10.1109/BIFE.2013.9
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Pevny T, 2007, PROC SPIE, V6505, DOI 10.1117/12.696774
   Pevny T, 2013, PROC SPIE, V8665, DOI 10.1117/12.2006790
   Qian YL, 2015, PROC SPIE, V9409, DOI 10.1117/12.2083479
   Russell A. M., 2013, MINING SOCIAL WEB DA
   Subhedar MS, 2018, MULTIMED TOOLS APPL, V77, P8115, DOI 10.1007/s11042-017-4706-x
   Wang ZC, 2018, IEEE SIGNAL PROC LET, V25, P1530, DOI 10.1109/LSP.2018.2865888
   Whitaker JM, 2015, PROC SPIE, V9409, DOI 10.1117/12.2083213
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Ye J, 2017, IEEE T INF FOREN SEC, V12, P2545, DOI 10.1109/TIFS.2017.2710946
   Zhang X, 2018, IEEE T MULTIMEDIA, V20, P3223, DOI 10.1109/TMM.2018.2838334
   Zhou H, 2019, IEEE T MULTIMEDIA, V21, P1384, DOI 10.1109/TMM.2018.2882088
NR 35
TC 9
Z9 10
U1 2
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2526
EP 2536
DI 10.1109/TMM.2019.2959909
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000004
DA 2024-07-18
ER

PT J
AU Liu, JY
   Xia, SF
   Yang, WH
AF Liu, Jiaying
   Xia, Sifeng
   Yang, Wenhan
TI Deep Reference Generation With Multi-Domain Hierarchical Constraints for
   Inter Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High efficient video coding (HEVC); inter prediction; frame
   interpolation; deep learning; multi-domain hierarchical constraints;
   factorized kernel convolution
ID NETWORK; CNN
AB Inter prediction is an important module in video coding for temporal redundancy removal, where similar reference blocks are searched from previously coded frames and employed to predict the block to be coded. Although existing video codecs can estimate and compensate for block-level motions, their inter prediction performance is still heavily affected by the remaining inconsistent pixel-wise displacement caused by irregular rotation and deformation. In this paper, we address the problem by proposing a deep frame interpolation network to generate additional reference frames in coding scenarios. First, we summarize the previous adaptive convolutions used for frame interpolation and propose a factorized kernel convolutional network to improve the modeling capacity and simultaneously keep its compact form. Second, to better train this network, multi-domain hierarchical constraints are introduced to regularize the training of our factorized kernel convolutional network. For spatial domain, we use a gradually down-sampled and up-sampled auto-encoder to generate the factorized kernels for frame interpolation at different scales. For quality domain, considering the inconsistent quality of the input frames, the factorized kernel convolution is modulated with quality-related features to learn to exploit more information from high quality frames. For frequency domain, a sum of absolute transformed difference loss that performs frequency transformation is utilized to facilitate network optimization from the view of coding performance. With the well-designed frame interpolation network regularized by multi-domain hierarchical constraints, our method surpasses HEVC on average 3.8% BD-rate saving for the luma component under the random access configuration and also obtains on average 0.83% BD-rate saving over the upcoming VVC.
C1 [Liu, Jiaying; Xia, Sifeng; Yang, Wenhan] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Liu, JY (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
EM liujiaying@pku.edu.cn; xsfatpku@pku.edu.cn; yangwenhan@pku.edu.cn
RI Liu, JY/GYJ-0138-2022
OI Liu, Jiaying/0000-0002-0468-9576
FU National Natural Science Foundation of China [61772043]; Beijing Natural
   Science Foundation [L182002]; Huawei Technologies
FX This work was supported in part the by National Natural Science
   Foundation of China under Contract 61772043, in part by the Beijing
   Natural Science Foundation under Contract L182002, and in part by Huawei
   Technologies. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Marta Mrak.
CR Alexander A, 2016, IEEE DATA COMPR CONF, P83, DOI 10.1109/DCC.2016.125
   Alshin A., 2010, 2010 28th Picture Coding Symposium (PCS 2010), P422, DOI 10.1109/PCS.2010.5702525
   Alshina E., 2015, ITU-T SG16/Q6 Doc. VCEG-AZ05
   [Anonymous], 2013, Technical Report JCTVC-L1100
   [Anonymous], 2016, DISTILL, DOI [10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Bossen F., 2019, JVET 14 M MAR
   Bross B., 2019, JVETO2001
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gu JW, 2017, IEEE DATA COMPR CONF, P442, DOI 10.1109/DCC.2017.33
   Gysel P, 2018, IEEE T NEUR NET LEAR, V29, P5784, DOI 10.1109/TNNLS.2018.2808319
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu P, 2018, IEEE T MULTIMEDIA, V20, P2814, DOI 10.1109/TMM.2018.2815784
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Jing X, 2004, IEEE T MULTIMEDIA, V6, P435, DOI 10.1109/TMM.2004.827517
   Kang J, 2017, IEEE IMAGE PROC, P26, DOI 10.1109/ICIP.2017.8296236
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Laude T., 2016, IEEE PICT COD S PCS, P1
   Laude T., 2019, PICT COD SYMP, P1
   Li BH, 2018, IEEE DATA COMPR CONF, P13, DOI 10.1109/DCC.2018.00009
   Li JH, 2018, IEEE T IMAGE PROCESS, V27, P3236, DOI 10.1109/TIP.2018.2817044
   Lin S., 2015, M37535TIUTSG16COM16C
   Liu JY, 2019, IEEE T IMAGE PROCESS, V28, P2140, DOI 10.1109/TIP.2018.2882923
   Liu ZY, 2016, IEEE T IMAGE PROCESS, V25, P5088, DOI 10.1109/TIP.2016.2601264
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747
   Niklaus S., 2018, P IEEE INT C COMP VI, P1410
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Pfaff J., 2018, JVET J0037 ISO IEC J
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Seidel I, 2016, IEEE IMAGE PROC, P2395, DOI 10.1109/ICIP.2016.7532788
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun DQ, 2020, IEEE T PATTERN ANAL, V42, P1408, DOI 10.1109/TPAMI.2019.2894353
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Wang YJ, 2019, ELECTROPHORESIS, V40, P969, DOI 10.1002/elps.201800302
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xu JY, 2019, IEEE T MULTIMEDIA, V21, P1633, DOI 10.1109/TMM.2018.2885921
   Xu M, 2018, IEEE T IMAGE PROCESS, V27, P5044, DOI 10.1109/TIP.2018.2847035
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yan N, 2017, IEEE INT SYMP CIRC S, P822
   Yang H, 2018, 2018 IEEE INT C COMM, P1
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhao L, 2018, IEEE IMAGE PROC, P206, DOI 10.1109/ICIP.2018.8451465
NR 52
TC 15
Z9 16
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2497
EP 2510
DI 10.1109/TMM.2019.2961504
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, PL
   Xu, T
   Yin, ZZ
   Liu, D
   Chen, EH
   Lv, GY
   Li, CL
AF Zhou, Peilun
   Xu, Tong
   Yin, Zhizhuo
   Liu, Dong
   Chen, Enhong
   Lv, Guangyi
   Li, Changliang
TI Character-Oriented Video Summarization With Visual and Textual Cues
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Character-oriented video summarization; person search; natural language
   processing
ID OBJECTS
AB With the booming of content "re-creation" in social media platforms, character-oriented video summary has become a crucial form of user-generated video content. However, artificial extraction could be time-consuming with high missing rate, while traditional techniques on person search may incur heavy burden of computing resources. At the same time, in social media platforms, videos are usually accompanied with rich textual information, e.g., subtitles or bullet-screen comments which provide the multi-view description of videos. Thus, there exists a potential to leverage textual information to enhance the character-oriented video summarization. To that end, in this paper, we propose a novel framework for jointly modeling visual and textual information. Specifically, we first locate characters indiscriminately through detection methods, and then identify these characters via re-identification to extract potential keyframes, in which appropriate source of textual information will be automatically selected and integrated based on the features of specific frame. Finally, key-frames will be aggregated as the character-oriented summarization. Experiments on real-world data sets validate that our solution outperforms several state-of-the-art baselines on both person search and summarization tasks, which prove the effectiveness of our solution on the character-oriented video summarization problem.
C1 [Zhou, Peilun; Xu, Tong; Yin, Zhizhuo; Liu, Dong; Chen, Enhong; Lv, Guangyi] Univ Sci & Technol China, Sch Comp Sci, Anhui Prov Key Lab Big Data Anal & Applicat, Hefei 230026, Peoples R China.
   [Li, Changliang] Kingsoft AI Lab, Beijing 100085, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Xu, T; Chen, EH (corresponding author), Univ Sci & Technol China, Sch Comp Sci, Anhui Prov Key Lab Big Data Anal & Applicat, Hefei 230026, Peoples R China.
EM zpl@mail.ustc.edu.cn; tongxu@ustc.edu.cn; yzz1223@mail.ustc.edu;
   dongeliu@ustc.edu.cn; cheneh@ustc.edu.cn; gylv@mail.ustc.edu.cn;
   lichangliang@kingsoft.com
RI liu, dong/GRJ-9115-2022
OI Liu, Dong/0000-0001-9100-2906; Chen, Enhong/0000-0002-4835-4102
FU National Key Research and Development Program of China [2018YFB1402600];
   National Natural Science Foundation of China [61703386, U1605251,
   61931014]
FX This work was supported in part by grants from the National Key Research
   and Development Program of China under Grant 2018YFB1402600, and in part
   by the National Natural Science Foundation of China under Grants
   61703386, U1605251, and 61931014.
CR Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen W, 2018, EVID-BASED COMPL ALT, V2018, DOI 10.1155/2018/5080764
   Guo YY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1865, DOI 10.1145/3240508.3240541
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Hamdoun O, 2008, 2008 SECOND ACM/IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS, P140
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu A, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P350, DOI 10.1145/3219819.3219853
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Hwang SY, 2011, PURE APPL CHEM, V83, P233, DOI 10.1351/PAC-CON-10-09-35
   Kanehira A, 2018, PROC CVPR IEEE, P7435, DOI 10.1109/CVPR.2018.00776
   Kingma DP, 2013, ARXIV
   Lajugie Remi., 2014, Proceedings of the 27th Advances in Neural Information Processing Systems (Neurips), P1817
   Lee S, 2018, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2018.00153
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Leibe B., 2017, ARXIV170307737CS
   Li S, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P138
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu H, 2017, IEEE I CONF COMP VIS, P493, DOI 10.1109/ICCV.2017.61
   Lv G., IEEE T BIG DATA
   Lv GY, 2019, LECT NOTES ARTIF INT, V11441, P412, DOI 10.1007/978-3-030-16142-2_32
   Lv GY, 2016, AAAI CONF ARTIF INTE, P3000
   Ma BP, 2012, LECT NOTES COMPUT SC, V7583, P413, DOI 10.1007/978-3-642-33863-2_41
   Mademlis I, 2015, EUR SIGNAL PR CONF, P819, DOI 10.1109/EUSIPCO.2015.7362497
   Mirzasoleiman B, 2018, AAAI CONF ARTIF INTE, P1379
   Panda R, 2017, IEEE T MULTIMEDIA, V19, P2010, DOI 10.1109/TMM.2017.2708981
   Plummer BA, 2017, PROC CVPR IEEE, P1052, DOI 10.1109/CVPR.2017.118
   Prosser B., 2010, PERSON RE IDENTIFICA, V2, P1
   Ren S., 2017, IEEETransactionsonPatternAnalysisandMachineIntelligence, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sak H, 2014, INTERSPEECH, P338
   Sharghi A, 2017, PROC CVPR IEEE, P2127, DOI 10.1109/CVPR.2017.229
   Shen YT, 2018, PROC CVPR IEEE, P6886, DOI 10.1109/CVPR.2018.00720
   Spain M, 2008, LECT NOTES COMPUT SC, V5302, P523, DOI 10.1007/978-3-540-88682-2_40
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Varini P, 2017, IEEE T MULTIMEDIA, V19, P2832, DOI 10.1109/TMM.2017.2705915
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wu H, 2019, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2019.00677
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xu T, 2014, KNOWL INF SYST, V41, P251, DOI 10.1007/s10115-013-0717-8
   Xu YL, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P937, DOI 10.1145/2647868.2654965
   YU S, 2016, ARXIV160407468
   Zhang SS, 2018, PROC CVPR IEEE, P6995, DOI 10.1109/CVPR.2018.00731
   Zhang SS, 2017, PROC CVPR IEEE, P4457, DOI 10.1109/CVPR.2017.474
   Zhang T, 2013, PROC SPIE, V8664, DOI 10.1117/12.2009127
   Zhang Y., 2018, P BRIT MACH VIS C
   Zhao B, 2018, PROC CVPR IEEE, P7405, DOI 10.1109/CVPR.2018.00773
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
NR 48
TC 11
Z9 11
U1 4
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2684
EP 2697
DI 10.1109/TMM.2019.2960594
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000014
DA 2024-07-18
ER

PT J
AU Ning, K
   Cai, M
   Xie, D
   Wu, F
AF Ning, Ke
   Cai, Ming
   Xie, Di
   Wu, Fei
TI An Attentive Sequence to Sequence Translator for Localizing Video Clips
   by Natural Language
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporal action localization; sequence to sequence learning; natural
   language guided detection
AB We propose a novel attentive sequence to sequence translator (ASST) for localizing video clips by natural language descriptions. We make two contributions. First, we propose an attentive mechanism that aligns natural language descriptions and video content. A bi-directional Recurrent Neural Network (RNN) parses natural language descriptions in two directions. Given a video-description pair, ASST generates a vector sequence representation. Each vector represents a video frame, conditioned by the description. The vector sequence representation not only preserves the temporal dependencies between the frames, but also provides an effective way to perform frame-level video-language matching. The attentive model then aligns words to each frame, thereby resulting in a more detailed understanding of video content and description semantics. Second, we design a hierarchical architecture for the network to jointly model language descriptions and video content. The hierarchical architecture exploits video content with multiple granularities, ranging from subtle details to global context. The integration of the multiple granularities yields a robust representation for multi-level video-language abstraction. We validate the effectiveness of our ASST on two large-scale datasets. Our ASST outperforms the state-of-the-art by 4.28% in Rank@1 on the DiDeMo dataset. On the Charades-STA dataset, we significantly improve the state-of-the-art by 13.41% in Recall@1,IoU = 0.5.
C1 [Ning, Ke] Zhejiang Univ, Hangzhou 310027, Zhejiang, Peoples R China.
   [Cai, Ming; Wu, Fei] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Zhejiang, Peoples R China.
   [Wu, Fei] Zhejiang Univ, Inst Artificial Intelligence, Hangzhou 310027, Zhejiang, Peoples R China.
   [Xie, Di] Hikvis Res Inst, Hangzhou 310051, Peoples R China.
C3 Zhejiang University; Zhejiang University; Zhejiang University
RP Cai, M (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Zhejiang, Peoples R China.
EM ningke@zju.edu.cn; cm@zju.edu.cn; xiedi@hikvision.com;
   wufei@cs.zju.edu.cn
OI Ning, Ke/0000-0001-6682-2224; , Di/0000-0001-8065-5901
FU National Natural Science Foundation of China [61625107, U1509206,
   61751209]; Zhejiang Lab [2018EC0ZX01]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61625107, U1509206, and 61751209 and in
   part by the Zhejiang Lab under Grant 2018EC0ZX01. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Chi-Chun Lee.
CR Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
   [Anonymous], 2016, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-46448-0_31
   [Anonymous], 2018, ROUTL CONTEMP CHINA
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P2225, DOI 10.1109/TMM.2015.2481711
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Dauphin YN, 2017, PR MACH LEARN RES, V70
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Frome A., 2013, P C NEUR INF PROC SY, P2121
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gehring J, 2017, PR MACH LEARN RES, V70
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jiang Y.-G., 2014, THUMOS CHALLENGE ACT
   Kalchbrenner N., 2016, CORR
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kingma D P, 2015, 3 INT C LEARN REPR I
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Lin J, 2017, IEEE T MULTIMEDIA, V19, P1968, DOI 10.1109/TMM.2017.2713410
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu JY, 2017, IEEE I CONF COMP VIS, P4866, DOI 10.1109/ICCV.2017.520
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Mazloom M, 2016, IEEE T MULTIMEDIA, V18, P1378, DOI 10.1109/TMM.2016.2559947
   Odena A, 2016, ARXIV161009585
   Pennington J, 2014, P C EMP METH NAT LAN, P1532
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shrivastava A., 2016, CORR
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song H, 2018, IEEE T MULTIMEDIA, V20, P1088, DOI 10.1109/TMM.2017.2763322
   Soomro K., 2012, ARXIV12120402
   van der Oord A., 2016, CORR
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang L., 2015, Bats and viruses: A new frontier of emerging infectious diseases
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wu Y., 2016, arXiv preprint arXiv:1609.08144
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Yu F., 2016, ARXIV151107122
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhou K, 2016, DESTECH TRANS COMP
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
   Zhu LC, 2017, INT J COMPUT VISION, V124, P409, DOI 10.1007/s11263-017-1033-7
NR 54
TC 10
Z9 10
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2434
EP 2443
DI 10.1109/TMM.2019.2957854
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200017
DA 2024-07-18
ER

PT J
AU Mahmoudpour, S
   Schelkens, P
AF Mahmoudpour, Saeed
   Schelkens, Peter
TI A Multi-Attribute Blind Quality Evaluator for Tone-Mapped Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Image color analysis; Dynamic range;
   Brightness; Imaging; High dynamic range imaging; Tone mapping; Blind
   image quality assessment; Entropy; Contrast sensitivity; Color harmony
ID VARIABILITY; GRADIENT; FUSION; INDEX; MODEL
AB High dynamic range (HDR) imaging enables capturing a wide range of luminance levels existing in real-world scenes. While HDR capturing devices become widespread in the market, the display technology is yet limited in representing full luminance ranges and standard low dynamic range (LDR) displays are currently more prevalent. To visualize the HDR content on traditional displays, tone mapping (TM) operators are introduced that convert HDR content into LDR. The dynamic range compression and different processing steps during TM can lead to loss of scene details, as well as luminance and chrominance changes. Such signal deviations will affect image naturalness and consequently disturb the visual quality of experience. Therefore, research into objective methods for quality evaluation of tone-mapped images has received attention in recent years. In this paper, we proposed a completely blind image quality evaluator for tone-mapped images based on a multi-attribute feature extraction scheme. Due to the diversity of TM distortions, various image characteristics are taken into account to develop an effective metric. The features are designed by considering spectral and spatial entropy, detection probability of visual information, image exposure, sharpness, and color properties. The quality-relevant features are then fed into a machine-learning regression framework to pool a quality score. The validation tests on two benchmark datasets reveal the superior performance of the proposed approach compared to the competing metrics.
C1 [Mahmoudpour, Saeed; Schelkens, Peter] Vrije Univ Brussel, Dept Elect & Informat, B-1050 Brussels, Belgium.
   [Mahmoudpour, Saeed; Schelkens, Peter] IMEC, B-3001 Leuven, Belgium.
C3 Vrije Universiteit Brussel; IMEC
RP Mahmoudpour, S (corresponding author), Vrije Univ Brussel, Dept Elect & Informat, B-1050 Brussels, Belgium.
EM smahmoud@etrovub.be; peter.schelkens@vub.be
RI Schelkens, Peter/B-7831-2008
OI Schelkens, Peter/0000-0003-0908-1655; Mahmoudpour,
   Saeed/0000-0003-1006-1838
FU imec.icon project "Highly Delightful Dynamic Range for Next Generation
   Digital Cinema and Television" (HD2R)
FX This work was supported in part by the imec.icon project "Highly
   Delightful Dynamic Range for Next Generation Digital Cinema and
   Television" (HD2R). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xiaokang Yang.
CR [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Barten Peter G. J, 1999, Contrast sensitivity of the human eye and its effects on image quality
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Carmona EJ, 2008, PATTERN RECOGN LETT, V29, P272, DOI 10.1016/j.patrec.2007.10.007
   Chalmers A, 2017, SIGNAL PROCESS-IMAGE, V54, P49, DOI 10.1016/j.image.2017.02.003
   Choi K, 2012, PATTERN RECOGN, V45, P2868, DOI 10.1016/j.patcog.2012.02.002
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Crozier WJ, 1936, J GEN PHYSIOL, V19, P0503, DOI 10.1085/jgp.19.3.503
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Fedorovskaya E, 2008, IEEE IMAGE PROC, P121, DOI 10.1109/ICIP.2008.4711706
   Galton Francis., 1875, Philosophical Magazine, V49, P33, DOI DOI 10.1080/14786447508641172
   Gao F, 2018, PATTERN RECOGN, V81, P432, DOI 10.1016/j.patcog.2018.04.016
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guo D, 2010, PROC CVPR IEEE, P515, DOI 10.1109/CVPR.2010.5540170
   Guo JM, 2013, IEEE T CIRC SYST VID, V23, P1809, DOI 10.1109/TCSVT.2013.2269011
   Hou WL, 2015, IEEE T NEUR NET LEAR, V26, P1275, DOI 10.1109/TNNLS.2014.2336852
   Huang GB, 2004, IEEE IJCNN, P985
   Jiang QP, 2019, IEEE T CIRC SYST VID, V29, P323, DOI 10.1109/TCSVT.2017.2783938
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P4725, DOI 10.1109/TIP.2017.2713945
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Lan Y., 1999, THESIS
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu RT, 2008, PROC CVPR IEEE, P954
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Mahmoudpour S, 2016, IEEE IMAGE PROC, P2613, DOI 10.1109/ICIP.2016.7532832
   Mahmoudpour S, 2016, SIGNAL IMAGE VIDEO P, V10, P1465, DOI 10.1007/s11760-016-0957-7
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Matsuda Y., 1995, Asakura Shoten, V2
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Murching A. M., 1995, IEEE T CIRCUITS SYST, V5, P161
   Nafchi HZ, 2015, IEEE SIGNAL PROC LET, V22, P1026, DOI 10.1109/LSP.2014.2381458
   Olatunji SO, 2014, INFORM FUSION, V16, P29, DOI 10.1016/j.inffus.2012.06.001
   Paul S, 2016, J CIRCUIT SYST COMP, V25, DOI 10.1142/S0218126616501231
   Pece F., 2010, Proceedings 2010 Conference on Visual Media Production (CVMP 2010). 7th European Conference on Visual Media Production, P1, DOI 10.1109/CVMP.2010.8
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Si L., 2019, ADV COMPUTER VISION, V943, P416
   Song Y, 2016, APPL OPTICS, V55, P10084, DOI 10.1364/AO.55.010084
   Tokumaru M, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOL 1 & 2, P378, DOI 10.1109/FUZZ.2002.1005020
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Westland S, 2006, COLOR RES APPL, V31, P315, DOI 10.1002/col.20230
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Yeganeh H, 2016, IEEE IMAGE PROC, P899, DOI 10.1109/ICIP.2016.7532487
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Zhan YB, 2018, IEEE T MULTIMEDIA, V20, P1796, DOI 10.1109/TMM.2017.2780770
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
NR 55
TC 7
Z9 7
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 1939
EP 1954
DI 10.1109/TMM.2019.2950570
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500003
DA 2024-07-18
ER

PT J
AU Zhang, M
   You, HX
   Kadam, P
   Liu, S
   Kuo, CCJ
AF Zhang, Min
   You, Haoxuan
   Kadam, Pranav
   Liu, Shan
   Kuo, C-C Jay
TI PointHop: An Explainable Machine Learning Method for Point Cloud
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Feature extraction; Transforms; Training;
   Computational modeling; Deep learning; Buildings; Explainable machine
   learning; point cloud cla-ssification; 3D object recognition; computer
   vision; Saab transform
ID CONVOLUTIONAL NEURAL-NETWORKS; 3D
AB An explainable machine learning method for point cloud classification, called the PointHop method, is proposed in this work. The PointHop method consists of two stages: 1) local-to-global attribute building through iterative one-hop information exchange and 2) classification and ensembles. In the attribute building stage, we address the problem of unordered point cloud data using a space partitioning procedure and developing a robust descriptor that characterizes the relationship between a point and its one-hop neighbor in a PointHop unit. When we put multiple PointHop units in cascade, the attributes of a point will grow by taking its relationship with one-hop neighbor points into account iteratively. Furthermore, to control the rapid dimension growth of the attribute vector associated with a point, we use the Saab transform to reduce the attribute dimension in each PointHop unit. In the classification and ensemble stage, we feed the feature vector obtained from multiple PointHop units to a classifier. We explore ensemble methods to improve the classification performance furthermore. It is shown by experimental results that the PointHop method offers classification performance that is comparable with state-of-the-art methods while demanding much lower training complexity.
C1 [Zhang, Min; Kadam, Pranav] Univ Southern Calif, Viterbi Sch Engn, Dept Elect & Comp Engn, Los Angeles, CA 90007 USA.
   [You, Haoxuan] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
   [Liu, Shan] Tencent Amer, Tencent Media Lab, Palo Alto, CA 94306 USA.
   [Kuo, C-C Jay] Univ Southern Calif, Dept Elect & Comp Engn, Media Commun Lab, Los Angeles, CA 90007 USA.
C3 University of Southern California; Columbia University; University of
   Southern California
RP You, HX (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
EM zhan980@usc.edu; hy2612@columbia.edu; pranavka@usc.edu;
   shanl@tencent.com; cckuo@sipi.usc.edu
RI Kadam, Pranav/JGE-5707-2023; Zhang, Min/HPF-7130-2023; huang,
   shan/JVN-1240-2024; Zhang, Min/ADG-4442-2022; Kuo, C.-C. Jay/A-7110-2011
OI Zhang, Min/0000-0002-6940-7146; Kuo, C.-C. Jay/0000-0001-9474-5035; ,
   Shan/0000-0002-1442-1207; Kadam, Pranav/0000-0001-7645-3506; You,
   Haoxuan/0000-0002-7912-4035
FU Tencent
FX This work was supported by a research grant from Tencent.
CR Achlioptas P., 2017, ARXIV170702392
   [Anonymous], IEEE SIGNAL PROCESS
   [Anonymous], COMP GRAPH FOR S GEO
   [Anonymous], 2003, TECH REP
   [Anonymous], 2016, NEURIPS 3D DEEP LEAR
   [Anonymous], 2018, ARXIV181111424
   [Anonymous], 2018, ACM T GRAPHICS
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], ARXIV190102154
   [Anonymous], 2019, J VIS COMMUN IMAGE R
   [Anonymous], P IEEE C COMP VIS PA
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Breiman L., 2001, Mach. Learn., V45, P5
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen YR, 2018, PICT COD SYMP, P174, DOI 10.1109/PCS.2018.8456277
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1
   Eldar Y, 1997, IEEE T IMAGE PROCESS, V6, P1305, DOI 10.1109/83.623193
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Katsavounidis I, 1994, IEEE SIGNAL PROC LET, V1, P144, DOI 10.1109/97.329844
   Kuo CCJ, 2018, J VIS COMMUN IMAGE R, V50, P237, DOI 10.1016/j.jvcir.2017.11.023
   Kuo CCJ, 2016, J VIS COMMUN IMAGE R, V41, P406, DOI 10.1016/j.jvcir.2016.11.003
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Li YZ, 2018, ADV NEUR IN, V31
   Ma Y, 2012, ENSEMBLE MACHINE LEARNING: METHODS AND APPLICATIONS, P1, DOI 10.1007/978-1-4419-9326-7
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Qi C. R., 2017, Advances in neural information processing systems, P5099
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Rokach L, 2010, ARTIF INTELL REV, V33, P1, DOI 10.1007/s10462-009-9124-7
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798
   Yang Bin, 2018, C ROBOT LEARNING, P146
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702
   Zhang ZZ, 2018, IEEE T IMAGE PROCESS, V27, P5957, DOI 10.1109/TIP.2018.2862625
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 45
TC 72
Z9 75
U1 3
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1744
EP 1755
DI 10.1109/TMM.2019.2963592
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hu, HM
   Zhang, HD
   Zhao, ZC
   Li, B
   Zhenge, J
AF Hu, Hai-Miao
   Zhang, Hongda
   Zhao, Zichen
   Li, Bo
   Zhenge, Jin
TI Adaptive Single Image Dehazing Using Joint Local-Global Illumination
   Adjustment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lighting; Atmospheric modeling; Image color analysis; Estimation;
   Scattering; Adaptation models; Image edge detection; Atmospheric light;
   global illumination compensation; local illumination estimation; single
   image haze removal
ID ENHANCEMENT; VISION; RETINEX; REMOVAL
AB Haze has a serious impact on the outdoor optical imaging systems, and it will result in image blurring, color shift, and saturation reduction. Recently, many single image dehazing algorithms have been proposed for practical applications, such as surveillance. However, since the widely-used global atmospheric light in image dehazing fails to well describe the local illumination differences of images, these algorithms fail to well adapt to scenes with different haze concentrations and lighting conditions. Therefore, this paper proposes an adaptive single image dehazing algorithm using joint local-global illumination adjustment. A local illumination estimation for hazy image is proposed to replace the global atmospheric light constant in the atmospheric scattering model, and it can better adapt to the local differences of image illumination. Correspondingly, the global atmospheric light constant is proposed to be utilized to adaptively compensate the illumination intensity, which may better overcome the dark illumination problem within the dehazed image. The experimental results demonstrate that the proposed algorithm can outperform the state-of-the-art algorithms in terms of not only the dehazing effect but also the adaptability.
C1 [Hu, Hai-Miao; Zhang, Hongda; Zhao, Zichen; Li, Bo; Zhenge, Jin] Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
C3 Beihang University
RP Zhenge, J (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
EM frank0139@163.com; zhd739156725@vip.qq.com; 2558542623@qq.com;
   boli@buaa.edu.cn; jinzheng@buaa.edu.cn
RI Li, bo/IWL-9318-2023; , 郑锦/ABD-4196-2020; Li, Ye/JBS-2949-2023
OI , 郑锦/0000-0002-5218-9250; Zhang, Hongda/0000-0002-7679-6215
FU National Natural Science Foundation of China [61772058, 61421003];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China (61772058 and 61421003), and in part by the
   Fundamental Research Funds for the Central Universities. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Balakrishnan Prabhakaran.
CR [Anonymous], 2018, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2018.2807593
   Berman D, 2017, IEEE INT CONF COMPUT, P115
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Gao YY, 2014, SIGNAL PROCESS, V103, P380, DOI 10.1016/j.sigpro.2014.02.016
   Han JW, 2013, IEEE T IMAGE PROCESS, V22, P2723, DOI 10.1109/TIP.2013.2256919
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu HM, 2019, IEEE T IMAGE PROCESS, V28, P2882, DOI 10.1109/TIP.2019.2891901
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Kim JY, 2001, IEEE T CIRC SYST VID, V11, P475, DOI 10.1109/76.915354
   Kim TK, 1998, IEEE T CONSUM ELECTR, V44, P82, DOI 10.1109/30.663733
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Liu F, 2015, APPL OPTICS, V54, P8116, DOI 10.1364/AO.54.008116
   Long J, 2012, PROCEEDINGS OF INTERNATIONAL CONFERENCE ON COMPUTER VISION IN REMOTE SENSING, P132
   Luo Y, 2014, IEEE T IMAGE PROCESS, V23, P3789, DOI 10.1109/TIP.2014.2332398
   McCartney E.J., 1976, OPTICS ATMOSPHERE
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Namer E, 2009, OPT EXPRESS, V17, P472, DOI 10.1364/OE.17.000472
   Narasimhan S. G., 2003, Interactive (de) weathering of an image using physical models, V6, P1
   Narasimhan SG, 2001, PROC CVPR IEEE, P186
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Seow MJ, 2006, NEUROCOMPUTING, V69, P954, DOI 10.1016/j.neucom.2005.07.003
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Sun W, 2015, COMPUT ELECTR ENG, V46, P371, DOI 10.1016/j.compeleceng.2015.02.009
   Tarel JP, 2012, IEEE INTEL TRANSP SY, V4, P6, DOI 10.1109/MITS.2012.2189969
   Wang JB, 2015, NEUROCOMPUTING, V149, P718, DOI 10.1016/j.neucom.2014.08.005
   Woodell G, 2006, PROC SPIE, V6246, DOI 10.1117/12.666767
   Yeh CH, 2013, OPT EXPRESS, V21, P27127, DOI 10.1364/OE.21.027127
   Yuan F, 2018, IEEE T IMAGE PROCESS, V27, P4395, DOI 10.1109/TIP.2018.2837900
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 47
TC 28
Z9 32
U1 0
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1485
EP 1495
DI 10.1109/TMM.2019.2944260
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100009
DA 2024-07-18
ER

PT J
AU Covaci, A
   Saleme, EB
   Mesfin, G
   Hussain, N
   Kani-Zabihi, E
   Ghinea, G
AF Covaci, Alexandra
   Saleme, Estevao Bissoli
   Mesfin, Gebremariam
   Hussain, Nadia
   Kani-Zabihi, Elahe
   Ghinea, Gheorghita
TI How Do We Experience Crossmodal Correspondent Mulsemedia Content?
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of experience; Visualization; Olfactory; Color; Shape; Media;
   Heart rate; Mulsemedia; QoE; crossmodal correspondence; heart rate; eye
   gaze
ID COLOR; OLFACTION; QUALITY; MODELS; ODORS
AB Sensory studies emerged as a significant influence upon Human Computer Interaction and traditional multimedia. Mulsemedia is an area that extends multimedia addressing issues of multisensorial response through the combination of at least three media, typically a non-traditional media with traditional audio-visual content. In this paper, we explore the concepts of Quality of Experience and crossmodal correspondences through a case study of different types of mulsemedia setups. The content is designed following principles of crossmodal correspondence between different sensory dimensions and delivered through olfactory, auditory and vibrotactile displays. The Quality of Experience is evaluated through both subjective (questionnaire) and objective means (eye gaze and heart rate). Results show that the auditory experience has an influence on the olfactory sensorial responses and lessens the perception of lingering odor. Heat maps of the eye gazes suggest that the crossmodality between olfactory and visual content leads to an increased visual attention on the factors of the employed crossmodal correspondence (e.g., color, brightness, shape).
C1 [Covaci, Alexandra] Univ Kent, Canterbury CT2 7NZ, Kent, England.
   [Saleme, Estevao Bissoli] Univ Fed Espirito Santo, Vitoria 29075910, ES, Brazil.
   [Mesfin, Gebremariam; Hussain, Nadia; Ghinea, Gheorghita] Brunel Univ, London UB8 3PH, England.
   [Kani-Zabihi, Elahe] Univ West London, London W5 5RF, England.
C3 University of Kent; Universidade Federal do Espirito Santo; Brunel
   University; University of West London
RP Ghinea, G (corresponding author), Brunel Univ, London UB8 3PH, England.
EM a.covaci@kent.ac.uk; estevaobissoli@gmail.com;
   gebremariam.assres@brunel.ac.uk; Nadia.Hussain@brunel.ac.uk;
   Elahe.Kani@uwl.ac.uk; george.ghinea@brunel.ac.uk
RI Saleme, Estevao B./AAZ-7161-2020; Assres, Gebremariam
   Mesfin/AFM-0811-2022; Hussain, Nadia/GXH-7350-2022; Ghinea,
   Gheorghita/AAG-6770-2020
OI Saleme, Estevao B./0000-0003-1856-3824; Assres, Gebremariam
   Mesfin/0000-0002-6760-690X; Ghinea, Gheorghita/0000-0003-2578-5580;
   Covaci, Alexandra/0000-0002-3205-2273; Kani-Zabihi,
   Elahe/0000-0002-5679-8512
FU European Union's Horizon 2020 Research and Innovation programme
   [688503]; Coordenacao de Aperfeicoamento de Pessoal de Nivel
   Superior-Brasil (CAPES) [88881.187844/2018-01]
FX This work was supported in part by the European Union's Horizon 2020
   Research and Innovation programme under Grant Agreement 688503 and in
   part by the Coordenacao de Aperfeicoamento de Pessoal de Nivel
   SuperiorBrasil (CAPES)-Finance Code 88881.187844/2018-01.
CR Abboud S, 2014, RESTOR NEUROL NEUROS, V32, P247, DOI 10.3233/RNN-130338
   Ademoye OA, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2957753
   Ademoye OA, 2009, IEEE T MULTIMEDIA, V11, P561, DOI 10.1109/TMM.2009.2012927
   [Anonymous], 2013, White Paper
   [Anonymous], MULTIMEDIA TOOLS APP
   Brkic Belma R., 2009, P 25 SPRING C COMP G, P161, DOI DOI 10.1145/1980462.1980494
   Covaci A, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3233774
   Crisinel AS, 2009, NEUROSCI LETT, V464, P39, DOI 10.1016/j.neulet.2009.08.016
   Dalmaijer E.S., 2014, PeerJ Preprints, V4, P1
   Demattè ML, 2006, CHEM SENSES, V31, P531, DOI 10.1093/chemse/bjj057
   Diego MA, 1998, INT J NEUROSCI, V96, P217, DOI 10.3109/00207459808986469
   Dmitrenko D, 2017, AUTOMOTIVEUI 2017: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, P154, DOI 10.1145/3122986.3122998
   Egan D, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Eid M, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON VIRTUAL ENVIRONMENTS, HUMAN-COMPUTER INTERFACES AND MEASUREMENT SYSTEMS, P5, DOI 10.1109/VECIMS.2008.4592743
   Ghinea G., 2011, MULTIPLE SENSORIAL M
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071398
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2379790.2379794
   Gilbert AN, 1996, AM J PSYCHOL, V109, P335, DOI 10.2307/1423010
   Hagtvedt H, 2016, J MARKETING RES, V53, P551, DOI 10.1509/jmr.14.0414
   Hamilton-Fletcher G, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P2162, DOI 10.1145/2858036.2858241
   Hanson-Vaux G, 2013, CHEM SENSES, V38, P161, DOI 10.1093/chemse/bjs087
   Hu D, 2016, PROC CVPR IEEE, P3574, DOI 10.1109/CVPR.2016.389
   Jezler Olivia., 2016, Extended Abstracts of the ACM Conference on Human Factors in Computing Systems, P1677, DOI [10.1145/2851581.2892471, DOI 10.1145/2851581.2892471]
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Keighrev C., 2017, Quality of Multimedia Experience (QoMEX), 2017 Ninth International Conference on, P1
   Kemp SE, 1997, AM J PSYCHOL, V110, P35, DOI 10.2307/1423699
   Lu XX, 2018, IEEE T GEOSCI REMOTE, V56, P2183, DOI 10.1109/TGRS.2017.2776321
   Ludwig VU, 2013, CORTEX, V49, P1089, DOI 10.1016/j.cortex.2012.04.004
   Maggioni E, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P45, DOI 10.1145/3242969.3242975
   MARKS LE, 1987, J EXP PSYCHOL HUMAN, V13, P384, DOI 10.1037/0096-1523.13.3.384
   Metatla O, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1055, DOI 10.1145/2858036.2858456
   Munster G., 2015, gene, V612, P303
   Murray N, 2018, IEEE T BROADCAST, V64, P539, DOI 10.1109/TBC.2018.2825297
   Murray N, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Murray N, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3108243
   Murray N, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2637293
   Murray Niall., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys'13, P162, DOI [https://doi.org/10.1145/2483977.2483999, DOI 10.1145/2483977.2483999]
   Obrist M, 2017, IEEE MULTIMEDIA, V24, P9, DOI 10.1109/MMUL.2017.33
   Ranasinghe N., 2014, Proceedings of the 8th International Conference on Tangible, Embedded and Embodied Interaction, P133, DOI [DOI 10.1145/2540930, DOI 10.1145/2540930.2540939, 10.1145/2540930.2540939]
   Ranasinghe N, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1139, DOI 10.1145/3123266.3123440
   Rehman S., 2014, P 51 ANN DES AUT C D, P1, DOI DOI 10.7873/DATE.2014.119
   Sakai N, 2005, CHEM SENSES, V30, pI244, DOI 10.1093/chemse/bjh205
   Schiavone G, 2013, IEEE INT C MICROELEC, P13, DOI 10.1109/ICMTS.2013.6528138
   Spence C, 2011, ATTEN PERCEPT PSYCHO, V73, P971, DOI 10.3758/s13414-010-0073-7
   Spence C, 2013, FOOD QUAL PREFER, V28, P206, DOI 10.1016/j.foodqual.2012.08.002
   Streeter NL, 2011, CHEMOSENS PERCEPT, V4, P1, DOI 10.1007/s12078-010-9082-0
   Tag B, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2017 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (UBICOMP/ISWC '17 ADJUNCT), P289, DOI 10.1145/3123024.3123190
   Timmerer C, 2014, T-LAB SER TELECOMMUN, P351, DOI 10.1007/978-3-319-02681-7_24
   Tsiros A, 2017, ELECT VISUALISATION, P175
   Waltl Markus, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P124, DOI 10.1109/QOMEX.2010.5517704
   Wang JC, 2011, INT CONF ACOUST SPEE, P1325
   Yang Y, 2016, INT C MANAGE SCI ENG, P341
   Yau JM, 2009, CURR BIOL, V19, P561, DOI 10.1016/j.cub.2009.02.013
   Yeo H, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P645
   You J, 2013, INT CONF AGRO-GEOINF, P446
   Yuan Y, 2018, IEEE ACCESS, V6, P5573, DOI 10.1109/ACCESS.2018.2796118
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P957, DOI 10.1109/TMM.2015.2431915
   Yuan ZH, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2661329
NR 58
TC 14
Z9 14
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1249
EP 1258
DI 10.1109/TMM.2019.2941274
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200011
OA Green Accepted, Green Published
DA 2024-07-18
ER

PT J
AU Cheng, ZX
   Sun, HM
   Takeuchi, M
   Katto, J
AF Cheng, Zhengxue
   Sun, Heming
   Takeuchi, Masaru
   Katto, Jiro
TI Energy Compaction-Based Image Compression Using Convolutional
   AutoEncoder
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image compression; convolutional autoencoder; optimum bit allocation;
   energy compaction
AB Image compression has been an important research topic for many decades. Recently, deep learning has achieved great success in many computer vision tasks, and its use in image compression has gradually been increasing. In this paper, we present an energy compaction-based image compression architecture using a convolutional autoencoder (CAE) to achieve high coding efficiency. Our main contributions include three aspects: 1) we propose a CAE architecture for image compression by decomposing it into several down(up)sampling operations; 2) for our CAE architecture, we offer a mathematical analysis on the energy compaction property and we are the first work to propose a normalized coding gain metric in neural networks, which can act as a measurement of compression capability; 3) based on the coding gain metric, we propose an energy compaction-based bit allocation method, which adds a regularizer to the loss function during the training stage to help the CAE maximize the coding gain and achieve high compression efficiency. The experimental results demonstrate our proposed method outperforms BPG (HEVC-intra), in terms of the MS-SSIM quality metric. Additionally, we achieve better performance in comparison with existing bit allocation methods, and provide higher coding efficiency compared with state-of-the-art learning compression methods at high bit rates.
C1 [Cheng, Zhengxue; Takeuchi, Masaru; Katto, Jiro] Waseda Univ, Grad Sch Fundamental Sci & Engn, Dept Comp Sci & Commun Engn, Tokyo 1698555, Japan.
   [Sun, Heming] Waseda Res Inst Sci & Engn, Tokyo 1698555, Japan.
   [Sun, Heming] JST, PRESTO, 4-1-8 Honcho, Kawaguchi, Saitama 3320012, Japan.
C3 Waseda University; Waseda University; Japan Science & Technology Agency
   (JST)
RP Sun, HM (corresponding author), Waseda Res Inst Sci & Engn, Tokyo 1698555, Japan.
EM zxcheng@asagi.waseda.jp; hemingsun@aoni.waseda.jp;
   masaru-t@aoni.waseda.jp; katto@waseda.jp
RI Heming, Sun/G-6882-2018; Katto, Jiro/AAH-2223-2020
OI Katto, Jiro/0000-0002-1671-2614; Takeuchi, Masaru/0000-0001-8953-0697;
   Cheng, Zhengxue/0000-0001-8258-6364
FU Japan Society for the Promotion of Science (JSPS) Research Fellowship
   DC2 [201914620]; JST, PRESTO, Japan [JPMJPR19M5]; JSPS KAKENHI
   [15H01684]; Waseda University [2019Q-049]
FX This work was supported in part by the Japan Society for the Promotion
   of Science (JSPS) Research Fellowship DC2 under Grant 201914620, in part
   by JST, PRESTO under Grant JPMJPR19M5, Japan, in part by JSPS KAKENHI
   under Grant 15H01684, and in part by Waseda University Grant for Special
   Research Projects 2019Q-049.
CR Agustsson E, 2017, ADV NEUR IN, V30
   [Anonymous], 1999, Kodak lossless true color image database
   [Anonymous], WORKSHOP CHALLENGE L
   Balle J, 2018, ICLR
   Cheng ZX, 2019, IEEE IMAGE PROC, P719, DOI [10.1109/icip.2019.8803824, 10.1109/ICIP.2019.8803824]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jayant N.C., 1984, Digital Coding of Waveforms: Principles and Applications to Speech and Video
   Johnston N, 2018, PROC CVPR IEEE, P4385, DOI 10.1109/CVPR.2018.00461
   King DB, 2015, ACS SYM SER, V1214, P1
   Li MH, 2018, PROC CVPR IEEE, P6644, DOI 10.1109/CVPR.2018.00695
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Mentzer F, 2018, PROC CVPR IEEE, P4394, DOI 10.1109/CVPR.2018.00462
   Rabbani M, 2002, SIGNAL PROCESS-IMAGE, V17, P3, DOI 10.1016/S0923-5965(01)00024-8
   Rippel O., 2017, P 34 INT C MACH LEAR, P2922
   Santos-Lasaosa S, 2019, PAIN MED, V20, P1032, DOI 10.1093/pm/pny238
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tao W, 2017, IEEE DATA COMPR CONF, P463, DOI 10.1109/DCC.2017.54
   Theis L., 2017, ICLR
   Toderici G., 2016, P INT C LEARN REPR I
   Vincent Pascal, 2008, P 25 INT C MACHINE L, DOI DOI 10.1145/1390156.1390294
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Zhu SY, 2018, IEEE T MULTIMEDIA, V20, P525, DOI 10.1109/TMM.2017.2749162
NR 24
TC 38
Z9 40
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 860
EP 873
DI 10.1109/TMM.2019.2938345
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400003
DA 2024-07-18
ER

PT J
AU Kirmizioglu, RA
   Tekalp, AM
AF Kirmizioglu, Riza Arda
   Tekalp, A. Murat
TI Multi-Party WebRTC Services Using Delay and Bandwidth Aware SDN-Assisted
   IP Multicasting of Scalable Video Over 5G Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE WebRTC; Streaming media; Bandwidth; IP networks; Delays; Multicast
   communication; Protocols; SDN-assisted IP multicast; multi-party WebRTC;
   scalable video coding; managed service; network slicing
AB At present, multi-party WebRTC videoconferencing between peers with heterogenous network resources and terminals is enabled over the best-effort Internet using a central selective forwarding unit (SFU), where each peer sends a scalable encoded video stream to the SFU. This connection model avoids the upload bandwidth bottleneck associated with mesh connections; however, it increases peer delay and overall network load (resource consumption) in addition to requiring investment in servers since all video traffic must go through SFU servers. To this effect, we propose a new multi-party WebRTC service model over future 5G networks, where a video service provider (VSP) collaborates with a network service providers (NSP) to offer an NSP-managed service to stream scalable video layers using software-defined networking (SDN)-assisted Internet protocol (IP) multicasting between peers using NSP infrastructure. In the proposed service model, each peer sends a scalable coded video upstream, which is selectively duplicated and forwarded as layer streams at SDN switches in the network, instead of at a central SFU, in a multi-party WebRTC session managed by multicast trees maintained by the SDN controller. Experimental results show that the proposed SDN-assisted IP multicast service architecture is more efficient than the SFU model in terms of end-to-end service delay and overall network resource consumption, while avoiding peer upload bandwidth bottleneck and distributing traffic more evenly across the network. The proposed architecture enables efficient provisioning of premium managed WebRTC services over bandwidth-reserved SDN slices to provide videoconferencing experience with guaranteed video quality over 5G networks.
C1 [Kirmizioglu, Riza Arda; Tekalp, A. Murat] Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
C3 Koc University
RP Kirmizioglu, RA (corresponding author), Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
EM rkirmizioglu@ku.edu.tr; mtekalp@ku.edu.tr
RI Tekalp, Murat/AAW-1060-2020
OI Kirmizioglu, Riza Arda/0000-0001-7195-5742
FU Turkish Academy of Sciences
FX This work was supported in part by EUCELTIC-Next project RELIANCE. A. M.
   Tekalp was also supported by the Turkish Academy of Sciences.
CR Akyildiz IF, 2014, COMPUT NETW, V71, P1, DOI 10.1016/j.comnet.2014.06.002
   Alexandros E., 2006, Journal of Zhejiang University (Science), V7, P696, DOI 10.1631/jzus.2006.A0696
   [Anonymous], 2015, NETWORK AS A SERVICE
   [Anonymous], P 7 INT C MULT SYST
   [Anonymous], 2017, OVERVIEW REAL TIME P
   [Anonymous], [No title captured]
   [Anonymous], 2018, 2018 IEEE ICME SAN D
   [Anonymous], [No title captured]
   [Anonymous], 2017, ZETT ER TRENDS AN
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   Bagci KT, 2017, IEEE T MULTIMEDIA, V19, P2152, DOI 10.1109/TMM.2017.2736638
   Baugher M., 2005, 4046 RFC
   De Cock J, 2016, PROC SPIE, V9971, DOI 10.1117/12.2238495
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Grange A., 2016, VP9 Bitstream & Decoding Process Specification
   Handley M, 2006, 4566 RFC
   Jero S, 2016, IEEE IFIP NETW OPER, P895, DOI 10.1109/NOMS.2016.7502922
   Karakus M, 2017, J NETW COMPUT APPL, V80, P200, DOI 10.1016/j.jnca.2016.12.019
   Kwok-Fai Ng, 2014, International Journal of Future Computer and Communication, V3, P319, DOI 10.7763/IJFCC.2014.V3.319
   Malkin G, 1993, 1393 RFC
   McCanne S., 1996, Computer Communication Review, V26, P117, DOI 10.1145/248157.248168
   ROSENBERG J., 2010, Interactive Connectivity Establishment (ICE): A Protocol for Network Address Translator (NAT) Traversal for Offer/Answer Protocols
   Shenoy Saahil, 2015, 2015 IEEE Power & Energy Society General Meeting, P1, DOI 10.1109/PESGM.2015.7286589
   Sheu JP, 2015, 2015 IEEE 26TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC), P2089, DOI 10.1109/PIMRC.2015.7343642
   Sunghyun Yoon, 2016, 2016 IEEE International Conference on Consumer Electronics (ICCE), P133, DOI 10.1109/ICCE.2016.7430552
   Wenger S., 2011, 6190 RFC
   Xue NN, 2015, IEEE T MULTIMEDIA, V17, P1617, DOI 10.1109/TMM.2015.2450014
   Yang J, 2018, IEEE T MULTIMEDIA, V20, P1260, DOI 10.1109/TMM.2017.2760630
   Zegura EW, 1996, IEEE INFOCOM SER, P594, DOI 10.1109/INFCOM.1996.493353
NR 30
TC 10
Z9 10
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 1005
EP 1015
DI 10.1109/TMM.2019.2937170
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400015
DA 2024-07-18
ER

PT J
AU Liu, XW
   Tao, XM
   Xu, M
   Zhan, YF
   Lu, JH
AF Liu, Xiwen
   Tao, Xiaoming
   Xu, Mai
   Zhan, Yafeng
   Lu, Jianhua
TI An EEG-Based Study on Perception of Video Distortion Under Various
   Content Motion Conditions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Electroencephalography; Quality assessment; Distortion
   measurement; Sensitivity; Visualization; Cognitive processes;
   Distortion; perceptibility threshold; EEG; P300; LDA classifier; AUC
AB Human perception sensitivity to video distortion is vital for visual quality assessment (VQA). Different from the perception mechanism of image distortion that has been thoroughly studied, the perception of video distortion is inevitably influenced by motion of dynamic content due to the characteristics of the human visual system (HVS). In this paper, electroencephalography (EEG) is used as a novel psychophysiological method to study the human perception sensitivity to quantification-aroused video distortion under various content motion conditions. For this purpose, we conduct experiments to record the EEG signals of the subjects when they are watching distorted videos. According to the feature analysis of EEG data, the P300 component aroused by human perception of video quality change is selected as the indicator of human perception of distortion. By the means of classification based on linear discriminant analysis (LDA), it is found that the separability of the P300 component, which is measured by the area under curve (AUC) of the receiver operating characteristic (ROC), is positively correlated with the perceptibility of distortion. The correlation provides a valid psychophysiological method, which is exempt from being influenced by subjective bias due to human high-level cognitive activities, for evaluating distortion perceptibility. In addition, the regression analysis results demonstrate a sigmoid-typed quantitative relation between the perceptibility of distortion and separability of the P300 component. Based on such relation, the perceptibility thresholds of distortion corresponding to various content motion speeds are calibrated by EEG signals and it is found that the content motion speed has a significant impact on distortion perceptibility.
C1 [Liu, Xiwen; Zhan, Yafeng; Lu, Jianhua] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Liu, Xiwen; Zhan, Yafeng; Lu, Jianhua] Tsinghua Univ, Space Ctr, Beijing 100084, Peoples R China.
   [Tao, Xiaoming] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Xu, Mai] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
C3 Tsinghua University; Tsinghua University; Tsinghua University; Beihang
   University
RP Tao, XM (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
EM liu-xw15@mails.tsinghua.edu.cn; taoxm@mail.tsinghua.edu.cn;
   MaiXu@buaa.edu.cn; zhanyf@mail.tsinghua.edu.cn;
   lhh-dee@mail.tsinghua.edu.cn
RI Tao, XiaoMing/A-9992-2010; liu, xiwen/GSJ-3861-2022; Tao,
   Xiaoming/ABG-9019-2021
OI Tao, Xiaoming/0000-0002-2406-0695
FU National Natural Science Foundation of China (NSFC) [61622110, 61471220,
   91538107]; National Basic Research Program of China (973 Program)
   [2013CB329006]; National Key R&D Program of China [2018YFB1800804]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC, 61622110, 61471220, 91538107), in part by the
   National Basic Research Program of China (973 Program, 2013CB329006) and
   in part by the National Key R&D Program of China (2018YFB1800804).
NR 0
TC 13
Z9 14
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 949
EP 960
DI 10.1109/TMM.2019.2934425
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400010
DA 2024-07-18
ER

PT J
AU Rahman, S
   Khan, S
   Barnes, N
AF Rahman, Shafin
   Khan, Salman
   Barnes, Nick
TI <i>Deep0Tag</i>: Deep Multiple Instance Learning for Zero-Shot Image
   Tagging
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; Multiple instance learning; Feature pooling; Object
   detection; Zero-shot tagging
AB Zero-shot learning aims to perform visual reasoning about unseen objects. In-line with the success of deep learning on object recognition problems, several end-to-end deep models for zero-shot recognition have been proposed in the literature. These models are successful in predicting a single unseen label given an input image but do not scale to cases where multiple unseen objects are present. Here, we focus on the challenging problem of zero-shot image tagging, where multiple labels are assigned to an image, that may relate to objects, attributes, actions, events, and scene type. Discovery of these scene concepts requires the ability to process multi-scale information. To encompass global as well as local image details, we propose an automatic approach to locate relevant image patches and model image tagging within the Multiple Instance Learning (MIL) framework. To the best of our knowledge, we propose the first end-to-end trainable deep MIL framework for the multi-label zero-shot tagging problem. We explore several alternatives for instance-level evidence aggregation and perform an extensive ablation study to identify the optimal pooling strategy. Due to its novel design, the proposed framework has several interesting features: 1) unlike previous deep MIL models, it does not use any off-line procedure (e.g., Selective Search or EdgeBoxes) for bag generation. 2) During test time, it can process any number of unseen labels given their semantic embedding vectors. 3) Using only image-level seen labels as weak annotation, it can produce a localized bounding box for each predicted label. We experiment with the large-scale NUS-WIDE and MS-COCO datasets and achieve superior performance across conventional, zero-shot, and generalized zero-shot tagging tasks.
C1 [Rahman, Shafin; Khan, Salman; Barnes, Nick] Australian Natl Univ, Res Sch Engn, Canberra, ACT 2601, Australia.
   [Rahman, Shafin; Barnes, Nick] CSIRO, Data61, Canberra, ACT 2601, Australia.
   [Khan, Salman] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
C3 Australian National University; Commonwealth Scientific & Industrial
   Research Organisation (CSIRO)
RP Rahman, S (corresponding author), Australian Natl Univ, Res Sch Engn, Canberra, ACT 2601, Australia.
EM shafin.rahman@anu.edu.au; salman.khan@anu.edu.au;
   Nick.Barnes@data61.csiro.au
RI Rahman, Shafin/N-1939-2019; Khan, Salman Hameed/M-4834-2016; Barnes,
   Nick/Y-2744-2018
OI Rahman, Shafin/0000-0001-7169-0318; Khan, Salman
   Hameed/0000-0002-9502-1749; Barnes, Nick/0000-0002-9343-9535
CR Akata Z, 2016, PROC CVPR IEEE, P59, DOI 10.1109/CVPR.2016.14
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   [Anonymous], ELEMENTS MATH THEORI
   [Anonymous], 2018, Polarity loss for zero-shot object detection
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], P EUR C COMP VIS SEP
   [Anonymous], 2000, ICML WORKSHOP ATTRIB
   [Anonymous], 2018, P AS C COMP VIS
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2013, ARXIV13124894
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Demirel B, 2017, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2017.139
   Deutsch S, 2017, PROC CVPR IEEE, P5292, DOI 10.1109/CVPR.2017.562
   Durand T, 2017, PROC CVPR IEEE, P5957, DOI 10.1109/CVPR.2017.631
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Han XT, 2017, IEEE T MULTIMEDIA, V19, P1583, DOI 10.1109/TMM.2017.2671414
   Hassoun MH., 1995, FUNDAMENTALS ARTIFIC
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ilse M, 2018, PR MACH LEARN RES, V80
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li XR, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P879, DOI 10.1145/2766462.2767773
   Li YN, 2017, PROC CVPR IEEE, P5207, DOI 10.1109/CVPR.2017.553
   Li YC, 2017, PROC CVPR IEEE, P1837, DOI 10.1109/CVPR.2017.199
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Mensink T, 2014, PROC CVPR IEEE, P2441, DOI 10.1109/CVPR.2014.313
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220
   Pappas N, 2017, J ARTIF INTELL RES, V58, P591, DOI 10.1613/jair.5240
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Quellec Gwenole, 2017, IEEE Rev Biomed Eng, V10, P213, DOI 10.1109/RBME.2017.2651164
   Rahman S, 2019, LECT NOTES COMPUT SC, V11361, P547, DOI 10.1007/978-3-030-20887-5_34
   Rahman S, 2018, IEEE T IMAGE PROCESS, V27, P5652, DOI 10.1109/TIP.2018.2861573
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren Z, 2017, PROCEEDINGS OF THE ASME TURBO EXPO: TURBINE TECHNICAL CONFERENCE AND EXPOSITION, 2017, VOL 5A
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang P, 2017, PATTERN RECOGN, V71, P446, DOI 10.1016/j.patcog.2017.05.001
   Tang P, 2017, IEEE T IMAGE PROCESS, V26, P3385, DOI 10.1109/TIP.2016.2642781
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wah Catherine, 2011, Technical report
   Wang XG, 2015, IEEE I CONF COMP VIS, P1224, DOI 10.1109/ICCV.2015.145
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Weston J, 2011, IJCAI
   Wu JJ, 2015, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2015.7298968
   Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Zaheer Manzil, 2017, P ADV NEURAL INFORM, P3391
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang Y, 2016, PROC CVPR IEEE, P5985, DOI 10.1109/CVPR.2016.644
   Zhou K, 2016, DESTECH TRANS COMP
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 67
TC 19
Z9 21
U1 2
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 242
EP 255
DI 10.1109/TMM.2019.2924511
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000021
DA 2024-07-18
ER

PT J
AU Yan, B
   Bare, B
   Tan, WM
AF Yan, Bo
   Bare, Bahetiyaer
   Tan, Weimin
TI Naturalness-Aware Deep No-Reference Image Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE No-reference image quality assessment; natural scene statistics;
   multi-task learning; naturalness-aware deep image quality assessment
ID FREE-ENERGY PRINCIPLE; STRUCTURAL SIMILARITY; DATABASE
AB No-reference image quality assessment (NR-IQA) is a non-trivial task, because it is hard to find a pristine counterpart for an image in real applications, such as image selection, high quality image recommendation, etc. In recent years, deep learning-based NR-IQA methods emerged and achieved better performance than previous methods. In this paper, we present a novel deep neural networks-based multi-task learning approach for NR-IQA. Our proposed network is designed by a multi-task learning manner that consists of two tasks, namely, natural scene statistics (NSS) features prediction task and the quality score prediction task. NSS features prediction is an auxiliary task, which helps the quality score prediction task to learn better mapping between the input image and its quality score. The main contribution of this work is to integrate the NSS features prediction task to the deep learning-based image quality prediction task to improve the representation ability and generalization ability. To the best of our knowledge, it is the first attempt. We conduct the same database validation and cross database validation experiments on LIVE1, TID2013(2), CSIQ(3), LIVE multiply distorted image quality database (LIVE MD)(4), CID2013(5), and LIVE in the wild image quality challenge (LIVE challenge)(6) databases to verify the superiority and generalization ability of the proposed method. Experimental results confirm the superior performance of our method on the same database validation; our method especially achieves 0.984 and 0.986 on the LIVE image quality assessment database in terms of the Pearson linear correlation coefficient (PLCC) and Spearman rank-order correlation coefficient (SROCC), respectively. Also, experimental results from cross database validation verify the strong generalization ability of our method. Specifically, our method gains significant improvement up to 21.8% on unseen distortion types.
C1 [Yan, Bo; Bare, Bahetiyaer; Tan, Weimin] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.
C3 Fudan University
RP Yan, B (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.
EM byan@fudan.edu.cn
RI bare, bahetiyaer/AAH-5096-2019; Yan, Bo/AFQ-7025-2022
OI Yan, Bo/0000-0002-7775-1270
FU NSFC [61772137]
FX This work was supported by NSFC under Grant No. 61772137.
CR [Anonymous], P 14 INT C ART INT S
   [Anonymous], 2015, Live in the wild image quality challenge database
   Bare B, 2017, IEEE INT CON MULTI, P1356, DOI 10.1109/ICME.2017.8019508
   Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Gu J, 2018, IEEE T MULTIMEDIA, V20, P1140, DOI 10.1109/TMM.2017.2761993
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guan JW, 2017, IEEE T MULTIMEDIA, V19, P2505, DOI 10.1109/TMM.2017.2703148
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou WL, 2015, IEEE T NEUR NET LEAR, V26, P1275, DOI 10.1109/TNNLS.2014.2336852
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2019, IEEE T NEUR NET LEAR, V30, P11, DOI 10.1109/TNNLS.2018.2829819
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee D, 2016, IEEE T IMAGE PROCESS, V25, P3875, DOI 10.1109/TIP.2016.2579308
   Liang YD, 2016, LECT NOTES COMPUT SC, V9909, P3, DOI 10.1007/978-3-319-46454-1_1
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Pei SC, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440172
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Saad MA, 2010, IEEE SIGNAL PROC LET, V17, P583, DOI 10.1109/LSP.2010.2045550
   Sampat MP, 2009, IEEE T IMAGE PROCESS, V18, P2385, DOI 10.1109/TIP.2009.2025923
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1700, DOI 10.1109/TMM.2013.2266093
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xu L, 2017, IEEE T CIRC SYST VID, V27, P1833, DOI 10.1109/TCSVT.2016.2543099
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 53
TC 77
Z9 81
U1 2
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2603
EP 2615
DI 10.1109/TMM.2019.2904879
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400014
DA 2024-07-18
ER

PT J
AU Shi, JG
   Zhao, GY
AF Shi, Jingang
   Zhao, Guoying
TI Face Hallucination via Coarse-to-Fine Recursive Kernel Regression
   Structure
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face hallucination; low-resolution; patch-based; kernel regression;
   super-resolution
ID SINGLE IMAGE SUPERRESOLUTION
AB In recent years, patch-based face hallucination algorithms have attracted considerable interest due to their effectiveness. These approaches produce a high-resolution (HR) face image according to the corresponding low-resolution (LR) input by learning a reconstruction model from the given training image set. The critical problem in these algorithms is establishing the underlying relationship between LR and HR patch pairs. Most previous methods aim to denote each input LR patch by the linear combination of the training set in the LR space while utilizing the combination weights to reconstruct the target HR patch. However, this assumes that the same combination weights should be shared between various resolution spaces, which is truly difficult to satisfy because of the one-to-many mapping relation between LR and HR patches. In this paper, we directly train a series of adaptive kernel regression mappings for predicting the lost high-frequency information from the LR patch, which avoids dealing with the above difficult problem. During the training process, we first establish a local optimization function on each LR/HR training pair according to the geometric structure of neighboring patches. The objective of local optimization can be presented in two aspects: 1) ensure the reconstruction consistency between each IR patch and the corresponding HR patch and 2) preserve the intrinsic geometry between each HR training patch and its original neighbors after the reconstruction process. The local optimizations are finally incorporated as the global optimization for calculating the optimal kernel regression function. To better approximate the target HR patch, we further propose a recursive structure to compensate for the residual reconstruction error of high-frequency details by a series of regression mappings. The proposed method is rather fast yet very effective in producing HR face images. Experimental results show that the proposed approach achieves superior performance with reasonable computational time compared with the state-of-the-art methods.
C1 [Shi, Jingang; Zhao, Guoying] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland.
   [Zhao, Guoying] Northwest Univ, Sch Informat & Technol, Xian 710069, Shaanxi, Peoples R China.
C3 University of Oulu; Northwest University Xi'an
RP Zhao, GY (corresponding author), Northwest Univ, Sch Informat & Technol, Xian 710069, Shaanxi, Peoples R China.
EM jingang.shi@oulu.fi; guoying.zhao@oulu.fi
RI Zhao, Guoying/ABE-7716-2020
OI Zhao, Guoying/0000-0003-3694-206X
FU National Natural Science Foundation of China [61772419]; Business
   Finland Project [3116/31/2017]; Academy of Finland; Tekes Fidipro
   Program [1849/31/2015]; Infotech Oulu; Tekniikan Edistamissaatio
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772419, in part by the Business
   Finland Project under Grant 3116/31/2017, in part by the Academy of
   Finland, in part by the Tekes Fidipro Program under Grant 1849/31/2015,
   in part by Infotech Oulu, and in part by Tekniikan Edistamissaatio. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Lei Zhang.
CR An L, 2014, SIGNAL PROCESS, V103, P184, DOI 10.1016/j.sigpro.2013.10.004
   Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616
   Cao QX, 2017, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2017.180
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chen HG, 2018, SIGNAL PROCESS-IMAGE, V66, P1, DOI 10.1016/j.image.2018.04.012
   Chen HG, 2017, IEEE T MULTIMEDIA, V19, P1702, DOI 10.1109/TMM.2017.2688920
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2016, IEEE T IMAGE PROCESS, V25, P2337, DOI 10.1109/TIP.2016.2542360
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677
   Hu YT, 2016, IEEE T IMAGE PROCESS, V25, P4091, DOI 10.1109/TIP.2016.2580942
   Huang H, 2011, IEEE T CIRC SYST VID, V21, P1363, DOI 10.1109/TCSVT.2011.2163461
   Huang H, 2010, PATTERN RECOGN, V43, P2532, DOI 10.1016/j.patcog.2010.02.007
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Jia K, 2008, IEEE T IMAGE PROCESS, V17, P873, DOI 10.1109/TIP.2008.922421
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Jiang JJ, 2014, IEEE T IMAGE PROCESS, V23, P4220, DOI 10.1109/TIP.2014.2347201
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Jung CK, 2011, IEEE SIGNAL PROC LET, V18, P367, DOI 10.1109/LSP.2011.2140370
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Li B, 2009, IEEE SIGNAL PROC LET, V16, P957, DOI 10.1109/LSP.2009.2027657
   Li YB, 2018, IEEE T IMAGE PROCESS, V27, P4638, DOI 10.1109/TIP.2018.2837865
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Liu LC, 2018, IEEE T CYBERNETICS, V48, P1189, DOI 10.1109/TCYB.2017.2682853
   Ma X, 2015, IEEE T HUM-MACH SYST, V45, P238, DOI 10.1109/THMS.2014.2375329
   Ma X, 2010, PATTERN RECOGN, V43, P2224, DOI 10.1016/j.patcog.2009.12.019
   Park JS, 2008, IEEE T IMAGE PROCESS, V17, P1806, DOI 10.1109/TIP.2008.2001394
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Shi JG, 2018, IEEE T IMAGE PROCESS, V27, P2980, DOI 10.1109/TIP.2018.2813163
   Shi JG, 2015, IEEE SIGNAL PROC LET, V22, P1189, DOI 10.1109/LSP.2015.2390972
   Shi JG, 2014, PATTERN RECOGN, V47, P3520, DOI 10.1016/j.patcog.2014.04.023
   Song YB, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4537
   Sun J, 2011, IEEE T IMAGE PROCESS, V20, P1529, DOI 10.1109/TIP.2010.2095871
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9
   Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2014, IEEE T CIRC SYST VID, V24, P802, DOI 10.1109/TCSVT.2013.2290574
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14
   Yu X, 2017, PROC CVPR IEEE, P5367, DOI 10.1109/CVPR.2017.570
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20
   Zeng X, 2018, IEEE T CYBERNETICS, V48, P716, DOI 10.1109/TCYB.2017.2655027
   Zhang KB, 2019, SIGNAL PROCESS, V154, P324, DOI 10.1016/j.sigpro.2018.09.002
   Zhang KB, 2017, IEEE T NEUR NET LEAR, V28, P1109, DOI 10.1109/TNNLS.2015.2511069
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang W, 2011, IEEE T IMAGE PROCESS, V20, P2769, DOI 10.1109/TIP.2011.2142001
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37
   Zhuang YT, 2007, PATTERN RECOGN, V40, P3178, DOI 10.1016/j.patcog.2007.03.011
NR 55
TC 21
Z9 21
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2223
EP 2236
DI 10.1109/TMM.2019.2898752
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200006
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Wei, P
   Sun, HB
   Zheng, NN
AF Wei, Ping
   Sun, Hongbin
   Zheng, Nanning
TI Learning Composite Latent Structures for 3D Human Action Representation
   and Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D human action; action representation; action recognition; composite
   latent structure
ID HUMAN-OBJECT INTERACTIONS; EVENT; FEATURES; MODEL
AB 3D human action representation and recognition are important issues in many multimedia applications. While latent state approaches have been widely used for action modeling, previous works assume the latent states of actions are single attribute. This assumption is inaccurate for representing structures of complex actions. In this paper, we propose that latent states have composite attributes and introduce a novel composite latent structure (CLS) model to represent and recognize 3D human actions with skeleton sequences. A human action is modeled with a hierarchical graph, which represents the action sequence as sequential atomic actions. An atomic action is represented as a composite latent state, which is composed of a latent semantic attribute and a latent geometric attribute. A discriminative EM-like algorithm is proposed to learn the model parameters and the composite latent structures of human actions. Given a 3D skeleton sequence, a composite attribute iterative programming algorithm is proposed to recognize the action and infer the action's latent temporal structure. We evaluate the proposed method on three challenging 3D action datasets-MSR 3D Action Dataset, Multiview 3D Event Dataset, and UTKinect-Action 3D Dataset. Extensive experimental results on these datasets demonstrate the effectiveness and advantage of the proposed method.
C1 [Wei, Ping; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
   [Sun, Hongbin] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Sun, HB (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Shaanxi, Peoples R China.
EM pingwei@xjtu.edu.cn; hsun@xjtu.edu.cn; nnzheng@xjtu.edu.cn
FU National Natural Science Foundation of China [61876149, 61503297,
   61722406]; China Postdoctoral Science Foundation [2018M643657]; National
   Key Research and Development Program of China [2016YFB1000903]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61876149, 61503297, 61722406, in part
   by China Postdoctoral Science Foundation 2018M643657, and in part by the
   National Key Research and Development Program of China 2016YFB1000903.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Manoranjan Paul.
CR Anirudh R, 2016, INT J COMPUT VISION, V116, P161, DOI 10.1007/s11263-015-0835-8
   [Anonymous], 2011, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2011.5995316, 10.1109/CVPR.2011.5995316]
   [Anonymous], 2014, ASIAN C COMPUT VIS
   Ben Amor B, 2016, IEEE T PATTERN ANAL, V38, P1, DOI 10.1109/TPAMI.2015.2439257
   Ben Tanfous A, 2018, PROC CVPR IEEE, P2840, DOI 10.1109/CVPR.2018.00300
   Bertschek I, 2006, CONTRIB STAT, P130, DOI 10.1007/3-7908-1701-5_9
   Bhattacharya S, 2014, PROC CVPR IEEE, P2243, DOI 10.1109/CVPR.2014.287
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Borghi G, 2016, INT C PATT RECOG, P997, DOI 10.1109/ICPR.2016.7899766
   Boulahia SY, 2016, INT C PATT RECOG, P985, DOI 10.1109/ICPR.2016.7899764
   Cai XY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1075, DOI 10.1145/2733373.2806285
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Cheng JC, 1999, IEEE T MULTIMEDIA, V1, P144, DOI 10.1109/6046.766736
   Cheng Y, 2014, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2014.286
   Cui P, 2012, IEEE T MULTIMEDIA, V14, P102, DOI 10.1109/TMM.2011.2176110
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Ding WW, 2015, J VIS COMMUN IMAGE R, V26, P329, DOI 10.1016/j.jvcir.2014.10.009
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Hou YH, 2018, IEEE T CIRC SYST VID, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Hu JF, 2017, IEEE T PATTERN ANAL, V39, P2186, DOI 10.1109/TPAMI.2016.2640292
   Hu NH, 2015, IEEE T ROBOT, V31, P1472, DOI 10.1109/TRO.2015.2495002
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia CC, 2016, IEEE T IMAGE PROCESS, V25, P4641, DOI 10.1109/TIP.2016.2589320
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Liang B, 2017, IEEE T IMAGE PROCESS, V26, P5560, DOI 10.1109/TIP.2017.2740122
   Lillo I, 2016, PROC CVPR IEEE, P1981, DOI 10.1109/CVPR.2016.218
   Lillo I, 2014, PROC CVPR IEEE, P812, DOI 10.1109/CVPR.2014.109
   Lin L, 2016, INT J COMPUT VISION, V118, P256, DOI 10.1007/s11263-015-0876-z
   Liu F, 2016, IEEE T IMAGE PROCESS, V25, P949, DOI 10.1109/TIP.2015.2512107
   Liu J, 2011, 2011 AASRI CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INDUSTRY APPLICATION (AASRI-AIIA 2011), VOL 4, P333
   Liu MY, 2018, IEEE T MULTIMEDIA, V20, P1932, DOI 10.1109/TMM.2017.2786868
   Luo JJ, 2013, IEEE I CONF COMP VIS, P1809, DOI 10.1109/ICCV.2013.227
   Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359
   MARTENS J, 2011, P 28 INT C MACH LEAR, P1033
   Nie BX, 2015, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2015.7298734
   Pei MT, 2013, COMPUT VIS IMAGE UND, V117, P1369, DOI 10.1016/j.cviu.2012.12.003
   Pei Y, 2015, LECT NOTES COMPUT SC, V8925, P528, DOI 10.1007/978-3-319-16178-5_37
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389
   Rahmani H, 2014, IEEE WINT CONF APPL, P626, DOI 10.1109/WACV.2014.6836044
   Shahroudy A, 2016, IEEE T PATTERN ANAL, V38, P2123, DOI 10.1109/TPAMI.2015.2505295
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shukla P, 2017, 2017 13TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY AND INTERNET-BASED SYSTEMS (SITIS), P339, DOI 10.1109/SITIS.2017.63
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Slama R, 2015, PATTERN RECOGN, V48, P556, DOI 10.1016/j.patcog.2014.08.011
   Su YT, 2014, ELECTRON LETT, V50, P1436, DOI 10.1049/el.2014.1316
   Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808
   Wang CY, 2016, PROC CVPR IEEE, P2639, DOI 10.1109/CVPR.2016.289
   Wang J, 2013, IEEE I CONF COMP VIS, P2688, DOI 10.1109/ICCV.2013.334
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang LM, 2013, IEEE I CONF COMP VIS, P2680, DOI 10.1109/ICCV.2013.333
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang Y, 2011, IEEE T PATTERN ANAL, V33, P1310, DOI 10.1109/TPAMI.2010.214
   Wei P, 2017, IEEE T PATTERN ANAL, V39, P1165, DOI 10.1109/TPAMI.2016.2574712
   Wei P, 2013, IEEE I CONF COMP VIS, P3272, DOI 10.1109/ICCV.2013.406
   Wei P, 2013, IEEE I CONF COMP VIS, P3136, DOI 10.1109/ICCV.2013.389
   Wei SH, 2017, IEEE IMAGE PROC, P91, DOI 10.1109/ICIP.2017.8296249
   Xia L., 2012, IEEE COMP SOC C COMP, P20, DOI DOI 10.1109/CVPRW.2012.6239233
   Xia L, 2013, PROC CVPR IEEE, P2834, DOI 10.1109/CVPR.2013.365
   Yang HY, 2012, APPL SOFT COMPUT, V12, P872, DOI 10.1016/j.asoc.2011.09.014
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Yao BP, 2011, IEEE I CONF COMP VIS, P1331, DOI 10.1109/ICCV.2011.6126386
   Zhang BC, 2017, IEEE T IMAGE PROCESS, V26, P4648, DOI 10.1109/TIP.2017.2718189
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhao WT, 2012, INT C APPL ROBOT POW, P557, DOI 10.1109/CARPI.2012.6356377
   Zhen XT, 2017, IEEE T MULTIMEDIA, V19, P2056, DOI 10.1109/TMM.2017.2700204
   Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137
   Zhu Y, 2013, IEEE COMPUT SOC CONF, P486, DOI 10.1109/CVPRW.2013.78
NR 75
TC 28
Z9 28
U1 2
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2195
EP 2208
DI 10.1109/TMM.2019.2897902
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200004
OA Bronze
DA 2024-07-18
ER

PT J
AU Shamieh, F
   Wang, XB
AF Shamieh, Fuad
   Wang, Xianbin
TI Dynamic Cross-Layer Signaling Exchange for Real-Time and On-Demand
   Multimedia Streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cost reduction; HTTP adaptive streaming; multimedia communication;
   quality of service; steganography
ID VIDEO
AB Multimedia streams consume a significant chunk of the consumer Internet traffic exchanged and will continue to do so due to the ever-increasing connection among people, businesses, and industries. To cope with the deviation of the Internet's intended use, unreliable underlying infrastructure, and best effort protocols while leveraging existing technologies, Hypertext Transfer Protocol Adaptive Streaming is utilized by numerous multimedia services. Performance of HAS-based streaming services is limited by the growing control overhead generated by the Transmission Control Protocol/Internet Protocol (TCP/IP) stack as the stream length, multimedia fidelity, and network conditions vary. In this paper, a novel cross-layer steganographic-enabled signaling scheme is proposed to reduce service provider costs while improving multimedia session performance and maintaining expected Quality-of-Service (QoS). The proposed scheme is designed to encode control stream messages from any TCP/IP layer within payload messages to reduce the total amount of overhead exchanged, thereby decreasing resource utilization within source and intermediate nodes. Furthermore, the encoding scheme probes network conditions and session statistics for adaptive decision-making to enable real-time pliability of the proposed process. A utility function is developed to find the optimal cost savings where simulations are conducted to verify the designs. The proposed solution is then implemented using VideoLan Media Player transceivers residing in linux containers virtual machines, where a multimedia file is exchanged in the popular Advanced Video Coding (H. 264) format. The results show a decrease in bandwidth and average queue waiting time costs of 4.71% and 29.61%, respectively, with a throughput increase of 5.77%.
C1 [Shamieh, Fuad; Wang, Xianbin] Univ Western Ontario, Dept Elect & Comp Engn, London, ON N6A 5B9, Canada.
C3 Western University (University of Western Ontario)
RP Wang, XB (corresponding author), Univ Western Ontario, Dept Elect & Comp Engn, London, ON N6A 5B9, Canada.
EM fshamieh@uwo.ca; xianbin.wang@uwo.ca
RI Wang, Xianbin/AAY-3303-2020
OI Wang, Xianbin/0000-0003-4890-0748; Shamieh, Fuad/0000-0002-2493-8912
FU Natural Sciences and Engineering Research Council of Canada Discovery
   [RGPIN-2018-06254]
FX This work was supported by the Natural Sciences and Engineering Research
   Council of Canada Discovery under Grant RGPIN-2018-06254. The associate
   editor coordinating the reviewof this manuscript and approving it for
   publication was Dr. Jun Wu. (Corresponding author: Xianbin Wang.)
CR Addie RG, 1998, IEEE COMMUN MAG, V36, P88, DOI 10.1109/35.707822
   Al-Jubari AM, 2013, WIRELESS PERS COMMUN, V69, P307, DOI 10.1007/s11277-012-0575-9
   Allman M., 1998, Computer Communication Review, V28, P4, DOI 10.1145/303297.303301
   Ammar D., 2011, Proceedings of the 4th International ICST Conference on Simulation Tools and Techniques, P81
   [Anonymous], P ACM SIGMM C MULT S
   [Anonymous], 2017, VLC OFF SIT FREE MUL
   [Anonymous], 2017, The Zettabyte Era: Trends and Analysis
   [Anonymous], 2018 RFC
   [Anonymous], 2012, Found. Comput.-Aided Process. Oper.
   Bagci KT, 2017, IEEE T MULTIMEDIA, V19, P2152, DOI 10.1109/TMM.2017.2736638
   CASNER S, 1999, 2508 RFC NETW WORK G
   Degermark M., 1999, 2507 RFC NETW WORK G
   Floyd S., 2006, 4340 RFC NETW WORK G
   Fridrich J., 2001, IEEE Multimedia, V8, P22, DOI 10.1109/93.959097
   Ge C, 2017, IEEE T MULTIMEDIA, V19, P2222, DOI 10.1109/TMM.2017.2735301
   Gordon JJ, 1996, BROADBAND COMMUNICATIONS, P28
   Handley M., 2013, 6824 RFC NETW WORK G
   Huysegems R, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P541, DOI 10.1145/2733373.2806264
   Koh C., 2013, SMPTE Mot. Imag. J, V122, P32
   Kondoz E., 2009, Visual Media Coding and Transmission, P32
   Koren T., 2003, 3545 RFC NETW WORK G
   Landstrm S., 2000, COMPUT COMMUN REV, V37, P5
   Lin YT, 2017, IEEE T MULTIMEDIA, V19, P196, DOI 10.1109/TMM.2016.2605499
   Miller K, 2015, IEEE T MULTIMEDIA, V17, P1309, DOI 10.1109/TMM.2015.2441002
   Moulin P, 2005, P IEEE, V93, P2083, DOI 10.1109/JPROC.2005.859599
   Padhye J., 1998, Computer Communication Review, V28, P303, DOI 10.1145/285243.285291
   Priya S., 2016, Proceedings of the International Conference on Soft Computing Systems, P479, DOI DOI 10.1007/978-81-322-2674-1T_44
   Rainer B, 2017, IEEE T MULTIMEDIA, V19, P849, DOI 10.1109/TMM.2016.2629761
   Rudman K., 2016, SMPTE Motion Imaging Journal, V125, P34, DOI 10.5594/j18662
   Samain J, 2017, IEEE T MULTIMEDIA, V19, P2166, DOI 10.1109/TMM.2017.2733340
   Sandlund K., 2010, 5795 RFC NETW WORK G
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Sun W., 2014, ADAPTIVE DYNAMIC PRO, P1, DOI DOI 10.1109/MMSP.2014.6958796
   Tanenbaum A., 2014, Computer Networks, P226
   Thompson B., 2005, 4170 RFC NETW WORK G
   Trappe W, 2003, IEEE T MULTIMEDIA, V5, P544, DOI 10.1109/TMM.2003.813279
   van der Hooft J, 2016, IEEE IFIP NETW OPER, P104, DOI 10.1109/NOMS.2016.7502802
   Wei Y, 2015, PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON MATERIALS SCIENCE AND ENERGY ENGINEERING (CMSEE 2014), P1
   Wu JY, 2017, IEEE T MOBILE COMPUT, V16, P1090, DOI 10.1109/TMC.2016.2584049
   Xiao M, 2016, INT CONF INSTR MEAS, P1, DOI 10.1109/IMCCC.2016.115
   Yu QY, 2013, IEEE J SEL AREA COMM, V31, P142, DOI 10.1109/JSAC.2013.SUP.0513013
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zukerman M, 2003, IEEE INFOCOM SER, P587
NR 44
TC 1
Z9 1
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 1893
EP 1904
DI 10.1109/TMM.2019.2892007
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700001
DA 2024-07-18
ER

PT J
AU Dai, P
   Wang, X
   Zhang, WH
   Chen, JF
AF Dai, Peng
   Wang, Xue
   Zhang, Weihang
   Chen, Junfeng
TI Instance Segmentation Enabled Hybrid Data Association and Discriminative
   Hashing for Online Multi-Object Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-object Tracking (MOT); tracking-by-detection; instance
   segmentation; hashing; hybrid data association
ID MULTITARGET TRACKING; OBJECT TRACKING; ALGORITHM; MODEL
AB Online multi-object tracking remains a difficult problem in complex scenes because of inaccurate detections, frequent occlusions by clutter or other objects, similar appearances of different objects, and other factors. In this paper, we propose a hybrid data association strategy combined with instance segmentation and online feature learning to handle these difficulties effectively. First, we utilize an instance segmentation algorithm to separate each object from other objects and backgrounds in a pixel-to-pixel manner, which will help resolve typical difficulties in multi-object tracking, such as ID-switches and track drifting. Moreover, we propose a local-to-global hybrid data association strategy to take advantage of the superiorities of both online and batch data association methods. The local data association between observations in consecutive frames reduces the computational complexity, hence ensuring the efficiency of online tracking. The global data association complements the local data association by integrating multiple video frames, thus alleviating the fragmented tracklets. Last, to improve the appearance discriminability and make it more robust in dealing with appearance variations during tracking, a lightweight semantic-preserving hashing algorithm has been proposed to learn compact hash codes online. Experiments with the MOT17 Challenge dataset demonstrate the superior performance of the proposed approach over other state-of-the-art batch and online tracking methods.
C1 [Dai, Peng; Wang, Xue; Zhang, Weihang; Chen, Junfeng] Tsinghua Univ, Dept Precis Instrument, State Key Lab Precis Measurement Technol & Instru, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Wang, X (corresponding author), Tsinghua Univ, Dept Precis Instrument, State Key Lab Precis Measurement Technol & Instru, Beijing 100084, Peoples R China.
EM daip13@mails.tsinghua.edu.cn; wangxue@mail.tsinghua.edu.cn;
   zwh15@mails.tsinghua.edu.cn; chenjf17@mails.tsinghua.edu.cn
OI Wang, Xue/0000-0003-4842-3160
FU National Natural Science Foundation of China [61472216]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61472216.
CR [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], 2018, P EUR C COMP VIS ECC
   [Anonymous], 2014, PROC IEEE C COMPUTER, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]
   [Anonymous], ARXIV160702568
   [Anonymous], IEEE T PATTERN ANAL
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bae SH, 2018, IEEE T PATTERN ANAL, V40, P595, DOI 10.1109/TPAMI.2017.2691769
   Bae SH, 2014, PROC CVPR IEEE, P1218, DOI 10.1109/CVPR.2014.159
   Bar-Shalom Y., 1980, P C INF SCI SYST, P807
   Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Breitenstein MD, 2011, IEEE T PATTERN ANAL, V33, P1820, DOI 10.1109/TPAMI.2010.232
   Brendel W, 2011, PROC CVPR IEEE, P1273, DOI 10.1109/CVPR.2011.5995395
   Chen JH, 2017, IEEE COMPUT SOC CONF, P2143, DOI 10.1109/CVPRW.2017.266
   Chen L, 2018, IEEE INT CON MULTI, DOI 10.1097/PEC.0000000000001553
   Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DEHGHAN A, 2015, PROC CVPR IEEE, P4091, DOI DOI 10.1109/CVPR.2015
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Dong XP, 2016, IEEE T IMAGE PROCESS, V25, P516, DOI 10.1109/TIP.2015.2505184
   Feng PM, 2017, IEEE T MULTIMEDIA, V19, P725, DOI 10.1109/TMM.2016.2638206
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Henschel R, 2018, IEEE COMPUT SOC CONF, P1509, DOI 10.1109/CVPRW.2018.00192
   Hong L, 2000, SIGNAL PROCESS, V80, P1561, DOI 10.1016/S0165-1684(00)00056-6
   Huang C, 2013, IEEE T PATTERN ANAL, V35, P898, DOI 10.1109/TPAMI.2012.159
   Izadinia H, 2013, IEEE WORK APP COMP, P385, DOI 10.1109/WACV.2013.6475044
   Jian-Yu Hsieh, 2007, Proceedings of the Fifth IASTED International Conference on Circuits, Signals, and Systems, P1, DOI 10.1145/1329125.1329139
   Kieritz H, 2016, 2016 13TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P122, DOI 10.1109/AVSS.2016.7738059
   Kim C, 2015, IEEE I CONF COMP VIS, P4696, DOI 10.1109/ICCV.2015.533
   Koh YJ, 2017, IEEE I CONF COMP VIS, P3621, DOI 10.1109/ICCV.2017.389
   Le N, 2016, LECT NOTES COMPUT SC, V9914, P43, DOI 10.1007/978-3-319-48881-3_4
   Leal-Taix L., 2017, ARXIV170402781
   Leal-Taixé L, 2016, IEEE COMPUT SOC CONF, P418, DOI 10.1109/CVPRW.2016.59
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu JY, 2018, IEEE T INTELL TRANSP, V19, P151, DOI 10.1109/TITS.2017.2750058
   Milan A, 2016, IEEE T PATTERN ANAL, V38, P2054, DOI 10.1109/TPAMI.2015.2505309
   Milan A, 2015, PROC CVPR IEEE, P5397, DOI 10.1109/CVPR.2015.7299178
   Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103
   Peng JT, 2016, IEEE T CIRC SYST VID, V26, P917, DOI 10.1109/TCSVT.2015.2430631
   Pirsiavash H, 2011, PROC CVPR IEEE, P1201, DOI 10.1109/CVPR.2011.5995604
   REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi SH, 2015, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2015.349
   Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41
   Shen JB, 2018, IEEE T IMAGE PROCESS, V27, P2688, DOI 10.1109/TIP.2018.2795740
   Shen JB, 2018, IEEE T INTELL TRANSP, V19, P162, DOI 10.1109/TITS.2017.2750082
   Shen JB, 2017, IEEE T IMAGE PROCESS, V26, P4911, DOI 10.1109/TIP.2017.2722691
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Tang SY, 2016, LECT NOTES COMPUT SC, V9914, P100, DOI 10.1007/978-3-319-48881-3_8
   Tian Y., 2016, ARXIV161004542
   Wang HZ, 2014, INT C PATT RECOG, P4098, DOI 10.1109/ICPR.2014.702
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang Wenguan, 2015, IEEE Trans Image Process, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Wen LY, 2017, INT J COMPUT VISION, V122, P313, DOI 10.1007/s11263-016-0943-0
   Wen LY, 2015, PROC CVPR IEEE, P2226, DOI 10.1109/CVPR.2015.7298835
   Wen LY, 2014, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2014.167
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yang B, 2012, PROC CVPR IEEE, P2034, DOI 10.1109/CVPR.2012.6247907
   Yang B, 2012, PROC CVPR IEEE, P1918, DOI 10.1109/CVPR.2012.6247892
   Yang F, 2016, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2016.234
   Yang M, 2017, IEEE T IMAGE PROCESS, V26, P5667, DOI 10.1109/TIP.2017.2745103
   Yoo H, 2017, IEEE T CIRC SYST VID, V27, P454, DOI 10.1109/TCSVT.2016.2593619
   Yoon Y.-c., 2018, ARXIV180510916
   Yu Q, 2009, IEEE T PATTERN ANAL, V31, P2196, DOI 10.1109/TPAMI.2008.253
   Yuan Y, 2015, IEEE T MULTIMEDIA, V17, P1125, DOI 10.1109/TMM.2015.2440996
   Zamir AR, 2012, LECT NOTES COMPUT SC, V7573, P343, DOI 10.1007/978-3-642-33709-3_25
   Zhang LJ, 2008, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD006286.pub2
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhou XZ, 2015, IEEE T MULTIMEDIA, V17, P145, DOI 10.1109/TMM.2014.2380914
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
NR 75
TC 22
Z9 23
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1709
EP 1723
DI 10.1109/TMM.2018.2885922
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700008
DA 2024-07-18
ER

PT J
AU Zhang, JQ
   Wang, ZZ
   Meng, JJ
   Tan, YP
   Yuan, JS
AF Zhang, Jiaqi
   Wang, Zhenzhen
   Meng, Jingjing
   Tan, Yap-Peng
   Yuan, Junsong
TI Boosting Positive and Unlabeled Learning for Anomaly Detection With
   Multi-Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anomaly detection; PU learning; semi-supervised learning; boosting
AB One of the key challenges of machine learning-based anomaly detection relies on the difficulty of obtaining anomaly data for training, which is usually rare, diversely distributed, and difficult to collect. To address this challenge, we formulate anomaly detection as a Positive and Unlabeled (PU) learning problem where only labeled positive (normal) data and unlabeled (normal and anomaly) data are required for learning an anomaly detector. As a semi-supervised learning method, it does not require providing labeled anomaly data for the training, thus it is easily deployed to various applications. As the unlabeled data can be extremely unbalanced, we introduce a novel PU learning method, which can tackle the situation where an unlabeled data set is mostly composed of positive instances. We start by using a linear model to extract the most reliable negative instances followed by a self-learning process to add reliable negative and positive instances with different speeds based on the estimated positive class prior. Furthermore, when feedback is available, we adopt boosting in the self-learning process to advantageously exploit the instability characteristic of PU learning. The classifiers in the self-learning process are weighted combined based on the estimated error rate to build the final classifier. Extensive experiments on six real datasets and one synthetic dataset show that our methods have better results under different conditions compared to existing methods.
C1 [Zhang, Jiaqi; Wang, Zhenzhen; Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 642273, Singapore.
   [Zhang, Jiaqi] GrabTaxi Holdings Pte Ltd, Singapore 573972, Singapore.
   [Meng, Jingjing; Yuan, Junsong] Nanyang Technol Univ, Singapore 642273, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Zhang, JQ (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 642273, Singapore.; Zhang, JQ (corresponding author), GrabTaxi Holdings Pte Ltd, Singapore 573972, Singapore.
EM jzhang069@e.ntu.edu.sg; ZWANG033@e.ntu.edu.sg; jingjing.meng@ntu.edu.sg;
   eyptan@ntu.edu.sg; jsyuan@ntu.edu.sg
RI meng, jingjing/HDM-6615-2022; Tan, Yap-Peng/A-5158-2011; Yuan,
   Junsong/A-5171-2011
OI meng, jingjing/0000-0002-8515-6893; Yuan, Junsong/0000-0002-7901-8793
CR Agovic A., 2007, P 1 INT WORKSH KNOWL, P435
   Ahn I, 2016, IEEE T MULTIMEDIA, V18, P1414, DOI 10.1109/TMM.2016.2551698
   Alippi C., 2012, The 2012 International Joint Conference on Neural Networks (IJCNN), P1
   Ando S, 2007, IEEE DATA MINING, P13, DOI 10.1109/ICDM.2007.53
   Bing L, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P179, DOI 10.1109/icdm.2003.1250918
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1388, DOI 10.1109/TMM.2013.2250492
   du Plessis MC, 2014, ADV NEUR IN, V27
   du Plessis MC, 2015, PR MACH LEARN RES, V37, P1386
   Du Plessis MC, 2014, IEICE T INF SYST, VE97D, P1358, DOI 10.1587/transinf.E97.D.1358
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ghosh A, 2016, 2016 IEEE IND APPL S, P1, DOI DOI 10.1109/IAS.2016.7731873
   Fusilier DH, 2015, INFORM PROCESS MANAG, V51, P433, DOI 10.1016/j.ipm.2014.11.001
   Hu WM, 2016, IEEE T MULTIMEDIA, V18, P76, DOI 10.1109/TMM.2015.2496372
   Jian M, 2016, IEEE T MULTIMEDIA, V18, P458, DOI 10.1109/TMM.2016.2515367
   Li HY, 2014, COMPUT SIST, V18, P467
   Li W, 2017, IEEE GEOSCI REMOTE S, V14, P597, DOI 10.1109/LGRS.2017.2657818
   Li X, 2003, IJCAI, V18, P587
   Liu B., 2002, ICML, P387
   Mordelet F, 2014, PATTERN RECOGN LETT, V37, P201, DOI 10.1016/j.patrec.2013.06.010
   Mordelet F, 2011, BMC BIOINFORMATICS, V12, DOI 10.1186/1471-2105-12-389
   Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085
   Ntalampiras S, 2011, IEEE T MULTIMEDIA, V13, P713, DOI 10.1109/TMM.2011.2122247
   Ren Y., 2014, P 2014 C EMP METH NA, P488, DOI DOI 10.3115/V1/D14-1055
   Saini M, 2012, IEEE T MULTIMEDIA, V14, P555, DOI 10.1109/TMM.2012.2186957
   Song J, 2008, WISTDCS 2008: WOMBAT WORKSHOP ON INFORMATION SECURITY THREATS DATA COLLECTION AND SHARING, P31, DOI 10.1109/WISTDCS.2008.10
   Tang L., 2017, U. S. Patent, Patent No. [App.15/427,654, 15427654]
   Wu S, 2018, IEEE T MULTIMEDIA, V20, P851, DOI 10.1109/TMM.2017.2758522
   Xia W, 2013, IEEE I CONF COMP VIS, P2176, DOI 10.1109/ICCV.2013.271
   Xiao LY, 2014, 2014 38TH ANNUAL IEEE INTERNATIONAL COMPUTER SOFTWARE AND APPLICATIONS CONFERENCE WORKSHOPS (COMPSACW 2014), P128, DOI 10.1109/COMPSACW.2014.25
   Xiao Y., 2011, P 22 INT JOINT C ART, P1577, DOI [10.5591/978-1-57735-516-8/IJCAI11-265, DOI 10.5591/978-1-57735-516-8/IJCAI11-265]
   Yang M, 2016, IFIP ADV INF COMM TE, V486, P132, DOI 10.1007/978-3-319-48390-0_14
   Yang P, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0097079
   Yang P, 2012, BIOINFORMATICS, V28, P2640, DOI 10.1093/bioinformatics/bts504
   Zhang JQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P854, DOI 10.1145/3123266.3123304
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhou SF, 2016, SIGNAL PROCESS-IMAGE, V47, P358, DOI 10.1016/j.image.2016.06.007
NR 37
TC 23
Z9 25
U1 2
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1332
EP 1344
DI 10.1109/TMM.2018.2871421
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600020
DA 2024-07-18
ER

PT J
AU Cheng, T
   Liu, GC
   Yang, Q
   Sun, JG
AF Cheng, Tong
   Liu, Guangchi
   Yang, Qing
   Sun, Jianguo
TI Trust Assessment in Vehicular Social Network Based on Three-Valued
   Subjective Logic
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Trust assessment; vehicular social network; three-valued subjective
   logic; community division; distributed algorithm
ID SYSTEMS
AB Trustworthiness in a vehicular network plays a vital role in facilitating data sharing among vehicles to achieve better driving safety and convenience. Without trustworthiness assessment, a vehicle may not be able to trust other vehicles and, therefore, simply drop the data shared from others to avoid potential driving dangers. This problem was traditionally approached by protecting data security; however, the study of the trustworthiness of data generators (vehicles) is unfortunately omitted. We envision the existences of a vehicular social network on road, wherein vehicles exchanging data between each other are considered socially connected. Leveraging the trust propagation and fusion within a vehicular social network, the trustworthiness of individual vehicles can be accurately assessed. We adopt the three-valued subjective logic model to study trust between vehicles, and propose a holistic solution to trust assessment in vehicular social networks. The proposed solution enables objective and subjective trust assessment of vehicles, in a distributed manner. Simulation results indicate that the proposed solution offers a more accurate trust assessment and a quicker assessing time.
C1 [Cheng, Tong; Yang, Qing] Univ North Texas, Dept Comp Sci & Engn, Denton, TX 76203 USA.
   [Liu, Guangchi] Stratifyd Inc, Charlotte, NC 28207 USA.
   [Sun, Jianguo] Harbin Engn Univ, Coll Comp Sci, Harbin 150001, Heilongjiang, Peoples R China.
C3 University of North Texas System; University of North Texas Denton;
   Harbin Engineering University
RP Yang, Q (corresponding author), Univ North Texas, Dept Comp Sci & Engn, Denton, TX 76203 USA.
EM tong.cheng@unt.edu; qing.yang@unt.edu
RI Sun, Jianguo/AAU-2796-2020
FU National Science Foundation (NSF) [NSF CNS-1761641]
FX This work was supported by the National Science Foundation (NSF) under
   Grant NSF CNS-1761641. The associate editor coordinating the review of
   this manuscript and approving it for publication was Qiang Ye.
   (Corresponding author: Qing Yang.)
CR Almeida J, 2016, DIGIT COMMUN NETW, V2, P57, DOI 10.1016/j.dcan.2016.03.001
   [Anonymous], 2004, P VLDB2004
   [Anonymous], 2009, P 8 INT C AUT AG MUL
   [Anonymous], GLOB TEL C GLOBECOM
   Borum R., 2010, MENTAL HLTH LAW POLI
   Dötzer F, 2005, I S WORLD WIREL MOBI, P454, DOI 10.1109/WOWMOM.2005.109
   Gao S, 2016, DIGIT COMMUN NETW, V2, P233, DOI 10.1016/j.dcan.2016.10.003
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Golbeck J, 2006, CONSUM COMM NETWORK, P282
   Huang D., 2010, Consumer Communications and Networking Conference (CCNC), P1
   Huang DJ, 2010, IEEE COMMUN MAG, V48, P128, DOI 10.1109/MCOM.2010.5621979
   Jiang WJ, 2014, IEEE INFOCOM SER, P1707, DOI 10.1109/INFOCOM.2014.6848108
   Josang A, 2001, INT J UNCERTAIN FUZZ, V9, P279, DOI 10.1016/S0218-4885(01)00083-1
   Josang A., 2006, C P 29 AUSTR COMP SC, P85
   Josang A, 2007, DECIS SUPPORT SYST, V43, P618, DOI 10.1016/j.dss.2005.05.019
   Kamvar M. T., 2003, P 12 INT C WORLD WID, P640
   Li XM, 2016, IEEE NETWORK, V30, P54, DOI 10.1109/MNET.2016.7513864
   Lin XD, 2011, IEEE INFOCOM SER, P2147, DOI 10.1109/INFCOM.2011.5935026
   Liu GC, 2015, IEEE CONF COMM NETW, P488, DOI 10.1109/CNS.2015.7346861
   Liu GC, 2014, IEEE INFOCOM SER, P1698, DOI 10.1109/INFOCOM.2014.6848107
   Liu SF, 2017, PROCEEDINGS OF 2017 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P1, DOI [10.1109/PLASMA.2017.8496132, 10.1109/PESGM.2017.8273885, 10.1109/GSIS.2017.8077658]
   Mai A, 2011, CONNECTED VEHICLES S
   Massa Paolo, 2005, AAAI, V1, P121
   Minhas U.F., 2010, International Journal of Computational Intelligence Theory and Practice (IJCITP), V5
   Raghavan UN, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.036106
   Ramchurn SD, 2004, KNOWL ENG REV, V19, P1, DOI 10.1017/S0269888904000116
   Raya M, 2007, J COMPUT SECUR, V15, P39, DOI 10.3233/JCS-2007-15103
   Ren ZW, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON WIRELESS AND MOBILE COMPUTING, NETWORKING AND COMMUNICATIONS, P141, DOI 10.1109/WiMob.2009.33
   Smaldone Stephen., 2008, Proceedings of the 1st Workshop on Social Network Systems, SocialNets '08, P43, DOI [DOI 10.1145/1435497.1435505, 10.1145/1435497.1435505]
   Su Z, 2019, IEEE INTERNET THINGS, V6, P4601, DOI 10.1109/JIOT.2018.2869297
   Su Z, 2018, IEEE J SEL AREA COMM, V36, P2175, DOI 10.1109/JSAC.2018.2869948
   Su Z, 2017, IEEE WIREL COMMUN, V24, P60, DOI 10.1109/MWC.2017.1600195WC
   Wang XF, 2015, INT J DISTRIB SENS N, DOI 10.1155/2015/216934
   Wang YJ, 2015, J NETW COMPUT APPL, V55, P59, DOI 10.1016/j.jnca.2015.04.007
   Wang YH, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1551
   Wu DP, 2018, IEEE INTERNET THINGS, V5, P2958, DOI 10.1109/JIOT.2017.2768073
   Wu DP, 2017, IEEE T MULTIMEDIA, V19, P2197, DOI 10.1109/TMM.2017.2733300
   Xu QC, 2019, IEEE INTERNET THINGS, V6, P4536, DOI 10.1109/JIOT.2018.2876417
   Yang Q, 2016, IEEE ACCESS, V4, P2764, DOI 10.1109/ACCESS.2016.2572206
   Yang Q, 2015, IEEE COMMUN MAG, V53, P42, DOI 10.1109/MCOM.2015.7180506
NR 40
TC 38
Z9 41
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 652
EP 663
DI 10.1109/TMM.2019.2891417
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800011
OA hybrid
DA 2024-07-18
ER

PT J
AU Jing, CC
   Dong, Z
   Pei, MT
   Jia, YD
AF Jing, Chenchen
   Dong, Zhen
   Pei, Mingtao
   Jia, Yunde
TI Heterogeneous Hashing Network for Face Retrieval Across Image and Video
   Domains
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face retrieval; image and video domains; deep CNN; hash learning
ID RECOGNITION
AB In this paper, we present a heterogeneous hashing network to generate effective and compact hash representations of both face images and face videos for face retrieval across image and video domains. The network contains an image branch and a video branch to project face images and videos into a common space, respectively. Then, the non-linear hash functions are learned in the common space to obtain the corresponding binary hash representations. The network is trained with three loss functions: 1) the Fisher loss; 2) the softmax loss; and 3) the triplet ranking loss. The Fisher loss uses the difference form of within-class and between-class scatter and is appropriate for the mini-batch-based optimization method. The Fisher loss together with the softmax loss is exploited to enhance the discriminative power of the common space. The triplet ranking loss is enforced on the final binary hash representations to improve retrieval performance. Experiments on a large-scale face video dataset and two challenging TV-series datasets demonstrate the effectiveness of the proposed method.
C1 [Jing, Chenchen; Dong, Zhen; Pei, Mingtao; Jia, Yunde] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Pei, MT (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM jingchenchen1996@bit.edu.cn; dongzhen@bit.edu.cn; peimt@bit.edu.cn;
   jiayunde@bit.edu.cn
FU Natural Science Foundation of China [61472038, 61375044]
FX This work was supported by the Natural Science Foundation of China under
   Grant 61472038 and Grant 61375044. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Lei
   Zhang. (Corresponding author: Mingtao Pei.)
CR [Anonymous], 2015, NAT CHEM BIOL
   [Anonymous], 2009, NEURIPS
   [Anonymous], ARXIV161201657
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.553
   [Anonymous], 2012, Book A probabilistic model for multimodal hash function learning, DOI DOI 10.1145/2339530.2339678
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], CVPR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2009, NEURIPS
   Arandjelovic O, 2005, PROC CVPR IEEE, P581
   Cakir F, 2015, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2015.125
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Cevikalp H, 2010, PROC CVPR IEEE, P2567, DOI 10.1109/CVPR.2010.5539965
   Chen B.-C., 2011, ACM MM'11, P1369
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Chen Z., 2017, Multimedia Systems, P1
   Dong Z, 2016, AAAI CONF ARTIF INTE, P3471
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Hu Y, 2011, PROC CVPR IEEE, P121, DOI 10.1109/CVPR.2011.5995500
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kafai M, 2014, IEEE T MULTIMEDIA, V16, P1090, DOI 10.1109/TMM.2014.2305633
   Kim TK, 2007, IEEE T PATTERN ANAL, V29, P1005, DOI 10.1109/TPAMI.2007.1037
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Lee H., 2010, 11th International Workshop on Information Security Applications, P273
   Li WJ, 2016, IJCAI, P1711
   Li Y, 2016, IEEE T IMAGE PROCESS, V25, P5905, DOI 10.1109/TIP.2016.2616297
   Li Y, 2015, IEEE I CONF COMP VIS, P3819, DOI 10.1109/ICCV.2015.435
   Li Y, 2015, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2015.7299108
   Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu LQ, 2014, IEEE T CIRC SYST VID, V24, P1874, DOI 10.1109/TCSVT.2014.2319671
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Rastegari Mohammad, 2013, Book Predictable Dual-View Hashing
   Shakhnarovich G, 2002, LECT NOTES COMPUT SC, V2352, P851, DOI 10.1007/3-540-47977-5_56
   Sun Y, 2016, PROC CVPR IEEE, P4856, DOI 10.1109/CVPR.2016.525
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Vemulapalli R, 2013, PROC CVPR IEEE, P1782, DOI 10.1109/CVPR.2013.233
   Wang DY, 2017, IEEE T PATTERN ANAL, V39, P1122, DOI 10.1109/TPAMI.2016.2582166
   Wang DY, 2014, IEEE T PATTERN ANAL, V36, P550, DOI 10.1109/TPAMI.2013.145
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang L, 2015, IEEE I CONF COMP VIS, P4570, DOI 10.1109/ICCV.2015.519
   Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965
   Wu Z, 2011, IEEE T PATTERN ANAL, V33, P1991, DOI 10.1109/TPAMI.2011.111
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yang JL, 2017, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2017.554
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Zhai Deming., 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, P2754
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhu XK, 2017, AAAI CONF ARTIF INTE, P4341
NR 60
TC 23
Z9 23
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 782
EP 794
DI 10.1109/TMM.2018.2866222
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800021
DA 2024-07-18
ER

PT J
AU Lu, Z
   de Veciana, G
AF Lu, Zheng
   de Veciana, Gustavo
TI Optimizing Stored Video Delivery for Wireless Networks: The Value of
   Knowing the Future
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Future capacity variations; opportunistic transport; optimization;
   stored video delivery
ID BIT-RATE VIDEO
AB This paper considers the design of cross-layer opportunistic transport protocols for stored video over wireless networks with a slow varying (average) capacity. We focus on two key principles: 1) scheduling data transmissions when capacity is high; and 2) exploiting knowledge of future capacity variations. The latter is possible when users' mobility is known or predictable, for example, users riding on public transportation or using navigation systems. We consider the design of cross-layer transmission schedules, which minimize system utilization (and, thus, possibly transmit/receive energy) while avoiding, if at all possible, rebuffering/delays in several scenarios. For the single-user anticipative case where all future capacity variations are known beforehand, we establish the optimal transmission schedule in a generalized piecewise constant thresholding (GPCT) scheme. For the single-user partially anticipative case where only a finite window of future capacity variations is known, we propose an online greedy fixed horizon control (GFHC). An upper bound on the competitive ratio of GFHC and GPCT is established showing how performance loss depends on the window size, receiver playback buffer, and capacity variability. We also consider the multiuser case where one can exploit both future temporal and multiuser diversity. Finally, we investigate the impact of uncertainty in knowledge of future capacity variations, and propose an offline approach as well as an online algorithm to deal with such uncertainty. Our simulations and evaluation based on a measured wireless capacity trace exhibit robust potential gains for our proposed transmission schemes.
C1 [Lu, Zheng; de Veciana, Gustavo] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
C3 University of Texas System; University of Texas Austin
RP Lu, Z (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
EM zhenglu@utexas.edu; gustavo@ece.utexas.edu
FU Intel; Cisco under the VAWN program; National Science Foundation
   [CNS-1343383]
FX This work was supported in part by Intel and Cisco under the VAWN
   program, and in part by the National Science Foundation under Grant
   CNS-1343383.
CR Abou-Zeid H, 2014, IEEE WIREL COMMUN, V21, P38, DOI 10.1109/MWC.2014.6882294
   Abou-zeid H, 2014, IEEE T VEH TECHNOL, V63, P2013, DOI 10.1109/TVT.2014.2314646
   [Anonymous], P ACM INT C EM NETW
   Atawia R, 2016, IEEE J SEL AREA COMM, V34, P1389, DOI 10.1109/JSAC.2016.2545358
   Bui Nicola, 2015, 2015 IEEE 16th International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM), P1, DOI 10.1109/WoWMoM.2015.7158161
   Bui N., 2014, Proc. European Wireless, P1
   Bui N, 2015, IEEE CONF COMPUT, P245, DOI 10.1109/INFCOMW.2015.7179392
   Feng WC, 1999, IEEE T MULTIMEDIA, V1, P302, DOI 10.1109/6046.784468
   Jurca D, 2007, IEEE T MULTIMEDIA, V9, P629, DOI 10.1109/TMM.2006.888017
   Kalampogia A, 2018, IEEE T MULTIMEDIA, V20, P171, DOI 10.1109/TMM.2017.2713642
   Kalbkhani H, 2017, IEEE T MULTIMEDIA, V19, P999, DOI 10.1109/TMM.2016.2639379
   Lu Z, 2013, IEEE INFOCOM SER, P2706
   Margolies R, 2016, IEEE ACM T NETWORK, V24, P355, DOI 10.1109/TNET.2014.2362928
   McManus J, 1998, TELECOMMUN SYST, V9, P223, DOI 10.1023/A:1019147923657
   Patil S, 2007, IEEE ACM T NETWORK, V15, P1046, DOI 10.1109/TNET.2007.896230
   Rexford J, 1999, IEEE ACM T NETWORK, V7, P202, DOI 10.1109/90.769768
   Salehi J. D., 1996, Performance Evaluation Review, V24, P222, DOI 10.1145/233008.233047
   Song CM, 2010, SCIENCE, V327, P1018, DOI 10.1126/science.1177170
   Tadrous J, 2015, IEEE ACM T NETWORK, V23, P1917, DOI 10.1109/TNET.2014.2346694
   TRIKI I, 2016, P IEEE 17 INT S WORL, P1
   Van der Auwera G, 2009, IEEE T BROADCAST, V55, P541, DOI 10.1109/TBC.2009.2027399
   Zou XK, 2015, 16TH INTERNATIONAL WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS (HOTMOBILE' 15), P57, DOI 10.1145/2699343.2699359
NR 22
TC 7
Z9 7
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 197
EP 210
DI 10.1109/TMM.2018.2847240
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700017
OA hybrid
DA 2024-07-18
ER

PT J
AU Xu, YY
   Hong, XP
   Porikli, F
   Liu, X
   Chen, J
   Zhao, GY
AF Xu, Yingyue
   Hong, Xiaopeng
   Porikli, Fatih
   Liu, Xin
   Chen, Jie
   Zhao, Guoying
TI Saliency Integration: An Arbitrator Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency integration; saliency aggregation; online model; arbitrator
   model
ID OBJECT DETECTION; ATTENTION
AB Saliency integration has attracted much attention on unifying saliency maps from multiple saliency models. Previous offline integration methods usually face two challenges: 1) if most of the candidate saliency models misjudge the saliency on an image, the integration result will lean heavily on those inferior candidate models; and 2) an unawareness of the ground truth saliency labels brings difficulty in estimating the expertise of each candidate model. To address these problems, in this paper, we propose an arbitrator model (AM) for saliency integration. First, we incorporate the consensus of multiple saliency models and the external knowledge into a reference map to effectively rectify the misleading by candidate models. Second, our quest for ways of estimating the expertise of the saliency models without ground truth labels gives rise to two distinct online model-expertise estimation methods. Finally, we derive a Bayesian integration framework to reconcile the saliency models of varying expertise and the reference map. To extensively evaluate the proposed AM model, we test 27 state-of-the-art saliency models, covering both traditional and deep learning ones, on various combinations over four datasets. The evaluation results show that the AM model improves the performance substantially compared to the existing state-of-the-art integration methods, regardless of the chosen candidate saliency models.
C1 [Xu, Yingyue; Hong, Xiaopeng; Liu, Xin; Chen, Jie; Zhao, Guoying] Univ Oulu, Ctr Machine Vision & Signal Anal, Oulu 90014, Finland.
   [Porikli, Fatih] Australian Natl Univ, Res Sch Engn, Canberra, ACT 0200, Australia.
C3 University of Oulu; Australian National University
RP Zhao, GY (corresponding author), Univ Oulu, Ctr Machine Vision & Signal Anal, Oulu 90014, Finland.
EM yingyue.xu@oulu.fi; xiaopeng.hong@oulu.fi; fatih.porikli@anu.edu.au;
   xin.liu@oulu.fi; jie.chen@oulu.fi; guoying.zhao@oulu.fi
RI Zhao, Ling/JHU-0501-2023; HONG, Xiaopeng/V-6078-2019; Zhang,
   Miao/JXY-8985-2024; JIANG, Feng/HTP-2862-2023; Zhao,
   Guoying/ABE-7716-2020; Liu, Xin/AAD-5166-2019; chen,
   qiang/JXY-6982-2024; zhang, shuai/IVU-7877-2023
OI Zhao, Ling/0000-0001-7155-034X; HONG, Xiaopeng/0000-0002-0611-0636;
   Zhao, Guoying/0000-0003-3694-206X; Liu, Xin/0000-0002-2242-6139; 
FU NVIDIA Corporation
FX The authors would like to thank NVIDIA Corporation for their support
   with the donation of the Tesla K40 and K80 GPUs used for this research.
   They also appreciate the comments and codes from Prof. Huchuan Lu and
   Mr. Mengyang Feng.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], P AS C COMP VIS
   [Anonymous], 2005, Advances in neural information processing systems, DOI DOI 10.5555/2976248.2976268
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Bruce NDB, 2016, PROC CVPR IEEE, P516, DOI 10.1109/CVPR.2016.62
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Dai Jifeng, 2016, Advances in Neural Information Processing Systems, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075
   Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11
   Feng M, 2017, POULTRY SCI, V96, P42, DOI 10.3382/ps/pew229
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   Gopalakrishnan V, 2010, IEEE T IMAGE PROCESS, V19, P3232, DOI 10.1109/TIP.2010.2053940
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Hua XS, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P1, DOI 10.1109/MMSP.2008.4665039
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang M, 2014, LECT NOTES COMPUT SC, V8695, P17, DOI 10.1007/978-3-319-10584-0_2
   Jiang P, 2015, IEEE I CONF COMP VIS, P217, DOI 10.1109/ICCV.2015.33
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kanan C, 2009, VIS COGN, V17, P979, DOI 10.1080/13506280902771138
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Le Meur O, 2006, IEEE T PATTERN ANAL, V28, P802, DOI 10.1109/TPAMI.2006.86
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu N, 2015, PROC CVPR IEEE, P362, DOI 10.1109/CVPR.2015.7298633
   Liu Q, 2017, IEEE T IMAGE PROCESS, V26, P4537, DOI 10.1109/TIP.2017.2703081
   Liu RS, 2014, PROC CVPR IEEE, P3866, DOI 10.1109/CVPR.2014.494
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Lu S, 2014, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2014.357
   MAI L, 2013, PROC CVPR IEEE, P1131, DOI DOI 10.1109/CVPR.2013.150
   Mai L, 2014, LECT NOTES COMPUT SC, V8691, P76, DOI 10.1007/978-3-319-10578-9_6
   Mairon R, 2014, LECT NOTES COMPUT SC, V8693, P708, DOI 10.1007/978-3-319-10602-1_46
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Otsu N., 2007, IEEE T SYS MAN CYBER, V9, P66, DOI [DOI 10.1109/TSMC.1979.4310076, 10.1109/TSMC.1979.4310076]
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Ponti M, 2017, PATTERN RECOGN, V61, P470, DOI 10.1016/j.patcog.2016.08.018
   Qin Y, 2018, INT J COMPUT VISION, V126, P751, DOI 10.1007/s11263-017-1062-2
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Quan R, 2018, IEEE T MULTIMEDIA, V20, P1101, DOI 10.1109/TMM.2017.2763780
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Ren Z., 2010, Proceedings of the 18th ACM international conference on Multimedia, P1099
   Shen CY, 2014, LECT NOTES COMPUT SC, V8695, P33, DOI 10.1007/978-3-319-10584-0_3
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Shi TL, 2014, PROC CVPR IEEE, P2822, DOI 10.1109/CVPR.2014.361
   Song R, 2012, PROC CVPR IEEE, P1474, DOI 10.1109/CVPR.2012.6247836
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Trichet R, 2016, 2016 13TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P51, DOI 10.1109/AVSS.2016.7738028
   von Neumann J., 1951, CEREBRAL MECH BEHAV, P1
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3
   Wang JW, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2522380
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang QS, 2016, PROC CVPR IEEE, P535, DOI 10.1109/CVPR.2016.64
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Whitehill J., 2009, Advances in Neural Information Processing Systems, P2035
   WOLFRAM S, 1983, REV MOD PHYS, V55, P601, DOI 10.1103/RevModPhys.55.601
   Xu YY, 2018, J VIS COMMUN IMAGE R, V53, P113, DOI 10.1016/j.jvcir.2018.02.015
   Xu YL, 2015, LECT NOTES ELECTR EN, V356, P637, DOI 10.1007/978-3-662-48224-7_76
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yao XW, 2017, IEEE T IMAGE PROCESS, V26, P3196, DOI 10.1109/TIP.2017.2694222
   Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang DW, 2017, IEEE T IMAGE PROCESS, V26, P1746, DOI 10.1109/TIP.2017.2658957
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 79
TC 11
Z9 11
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 98
EP 113
DI 10.1109/TMM.2018.2856126
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700009
OA Green Accepted, Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Jiang, YG
   Wu, ZX
   Tang, JH
   Li, ZC
   Xue, XY
   Chang, SF
AF Jiang, Yu-Gang
   Wu, Zuxuan
   Tang, Jinhui
   Li, Zechao
   Xue, Xiangyang
   Chang, Shih-Fu
TI Modeling Multimodal Clues in a Hybrid Deep Learning Framework for Video
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; framework; fusion; video classification
ID ACTION RECOGNITION; EVENT RECOGNITION; LATE FUSION; HISTOGRAMS; FEATURES
AB Videos are inherently multimodal. This paper studies the problem of exploiting the abundant multimodal clues for improved video classification performance. We introduce a novel hybrid deep learning framework that integrates useful clues from multiple modalities, including static spatial appearance information, motion patterns within a short time window, audio information, as well as long-range temporal dynamics. More specifically, we utilize three Convolutional Neural Networks (CNNs) operating on appearance, motion, and audio signals to extract their corresponding features. We then employ a feature fusion network to derive a unified representation with an aim to capture the relationships among features. Furthermore, to exploit the long-range temporal dynamics in videos, we apply two long short-term memory (LSTM) networks with extracted appearance and motion features as inputs. Finally, we also propose refining the prediction scores by leveraging contextual relationships among video semantics. The hybrid deep learning framework is able to exploit a comprehensive set of multimodal features for video classification. Through an extensive set of experiments, we demonstrate that: 1) LSTM networks that model sequences in an explicitly recurrent manner are highly complementary to the CNN models; 2) the feature fusion network that produces a fused representation through modeling feature relationships outperforms a large set of alternative fusion strategies; and 3) the semantic context of video classes can help further refine the predictions for improved performance. Experimental results on two challenging benchmarks-the UCF-101 and the Columbia Consumer Videos (CCV)-provide strong quantitative evidence that our framework can produce promising results: 93.1% on the UCF-101 and 84.5% on the CCV, outperforming several competing methods with clear margins.
C1 [Jiang, Yu-Gang; Wu, Zuxuan; Xue, Xiangyang] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Tang, Jinhui; Li, Zechao] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
   [Chang, Shih-Fu] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
C3 Fudan University; Nanjing University of Science & Technology; Columbia
   University
RP Tang, JH (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
EM ygj@fudan.edu.cn; zxwu@fudan.edu.cn; jinhuitang@njust.edu.cn;
   zechao.li@njust.edu.cn; xyxue@fudan.edu.cn; sfchang@ee.columbia.edu
RI ARSLAN, Okan/AAA-3232-2020; Tang, Jinhui/KBR-0891-2024
OI Chang, Shih-Fu/0000-0003-1444-1205; Tang, Jinhui/0000-0001-9008-222X
FU NSF China [61622204, 61572134]; STCSM, Shanghai, China [16QA1400500,
   16JC1420401]
FX This work was supported in part by the NSF China under Grants 61622204
   and 61572134, and in part by the STCSM, Shanghai, China under
   16QA1400500 and 16JC1420401.
CR [Anonymous], 2011, P 1 INT C MULT RETR
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2011, P ICML
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2007, P 6 INT JOINT C AUT
   [Anonymous], P BMVC
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P BMVC
   [Anonymous], INT J COMPUT VIS
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168
   Chung JY, 2015, PR MACH LEARN RES, V37, P2067
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Graves A, 2005, IEEE IJCNN, P2047
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Han XT, 2017, IEEE T MULTIMEDIA, V19, P1583, DOI 10.1109/TMM.2017.2671414
   Jhuo IH, 2014, MACH VISION APPL, V25, P33, DOI 10.1007/s00138-013-0567-0
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang Y., 2014, ECCV WORKSH
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2
   Jiang YG, 2009, IEEE I CONF COMP VIS, P1420, DOI 10.1109/ICCV.2009.5459295
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kloft M, 2011, J MACH LEARN RES, V12, P953
   Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Liu D, 2013, PROC CVPR IEEE, P803, DOI 10.1109/CVPR.2013.109
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808
   Van den Oord A., 2013, P NIPS
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, P ECCV
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang XL, 2016, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2016.291
   Wang Y, 2009, PROC CVPR IEEE, P872, DOI 10.1109/CVPRW.2009.5206709
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Xu ZW, 2013, IEEE I CONF COMP VIS, P3440, DOI 10.1109/ICCV.2013.427
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
NR 65
TC 87
Z9 93
U1 4
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3137
EP 3147
DI 10.1109/TMM.2018.2823900
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, Z
   Hou, XS
   Qian, XM
   Gong, C
AF Chen, Zan
   Hou, Xingsong
   Qian, Xueming
   Gong, Chen
TI Efficient and Robust Image Coding and Transmission Based on Scrambled
   Block Compressive Sensing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fast compressive sampling; image transmission; low-rank approximation;
   progressive quantization; robust image compression
ID RECOVERY
AB Image transmission in a wireless visual sensor network (WVSN) with limited resources over an unreliable and bandwidth-limited wireless channel is challenging. This paper presents a highly efficient and robust image coding and transmission scheme with a simple encoder based on compressive sensing (CS) for WVSNs. First, an image measurement based on scrambled block compressive sampling with a separable sensing operator is proposed to simplify the encoder. Second, a progressive nonuniform quantization, which exploits the measurement distribution at the encoder side and the measurement dependencies at the decoder side, is designed to improve the rate-distortion (R-D) performance while maintaining low complexity at the encoder. Third, to further improve the R-D performance, a progressive non-local low-rank reconstruction is designed at the decoder. The experimental results show that the proposed scheme can achieve higher R-D performance compared with the benchmark CS-based image coding and transmission schemes. Higher robustness can be achieved compared with the traditional source-channel coding, such as Consultative Committee for Space Data Systems-Image Data Compression (CCSDS-IDC) with Raptor codes under a time-varying packet loss channel, and the encoding time can be significantly reduced compared with the traditional image coding schemes. The experimental results also show that the proposed scheme achieves state-of-the-art coding efficiency with lower computational complexity at the encoder while still supporting error resilience.
C1 [Chen, Zan; Hou, Xingsong; Qian, Xueming] Xian Jitaotong Univ, Sch Elect & Informat Engn, Xian 710049, Shaanxi, Peoples R China.
   [Gong, Chen] Univ Sci & Technol China, Sch Elect & Informat Engn, Hefei 230000, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Hou, XS (corresponding author), Xian Jitaotong Univ, Sch Elect & Informat Engn, Xian 710049, Shaanxi, Peoples R China.
EM 609560870@qq.com; houxs@mail.xjtu.edu.cn; qianxm@mail.xjtu.edu.cn;
   1074460193@qq.com
RI GONG, CHEN/JDW-5727-2023
FU NSFC [61373113, u1531141, 61732008, 61772407]; National Key R&D Program
   of China [2017YFF0107700]; Guangdong Provincial Science and Technology
   Plan Project [2017A010101006, 2016A010101005]
FX This work was supported in part by the NSFC under Grants 61373113,
   u1531141, 61732008, and 61772407, in part by the National Key R&D
   Program of China under Grant 2017YFF0107700, and in part the Guangdong
   Provincial Science and Technology Plan Project under Grants
   2017A010101006 and 2016A010101005.
CR Akyildiz IF, 2007, IEEE WIREL COMMUN, V14, P32, DOI 10.1109/MWC.2007.4407225
   [Anonymous], 2013, P 2013 AS PAC SIGN I
   [Anonymous], 2015, Math. Prob. Eng.
   [Anonymous], 0906 TREE RIC U EL C
   [Anonymous], 1220BA CCCSDS
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès E, 2007, INVERSE PROBL, V23, P969, DOI 10.1088/0266-5611/23/3/008
   Candés E, 2006, IEEE IMAGE PROC, P1281, DOI 10.1109/ICIP.2006.312579
   Coluccia G, 2013, IEEE INT WORKSH MULT, P129, DOI 10.1109/MMSP.2013.6659276
   Deng CW, 2012, IEEE T MULTIMEDIA, V14, P278, DOI 10.1109/TMM.2011.2181491
   Do TT, 2008, INT CONF ACOUST SPEE, P3369, DOI 10.1109/ICASSP.2008.4518373
   Do TT, 2012, IEEE T SIGNAL PROCES, V60, P139, DOI 10.1109/TSP.2011.2170977
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Duarte MF, 2012, IEEE T IMAGE PROCESS, V21, P494, DOI 10.1109/TIP.2011.2165289
   Duran-Faundez C, 2011, SIGNAL PROCESS-IMAGE, V26, P466, DOI 10.1016/j.image.2011.07.005
   Fowler JE, 2011, EUR SIGNAL PR CONF, P564
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Gersho A., 2003, Vector Quantization and Signal Compression
   Goyal VK, 2008, IEEE SIGNAL PROC MAG, V25, P48, DOI 10.1109/MSP.2007.915001
   He LH, 2009, IEEE T SIGNAL PROCES, V57, P3488, DOI 10.1109/TSP.2009.2022003
   Kokalj-Filipovic S, 2012, BELL LABS TECH J, V16, P171, DOI 10.1002/bltj.20540
   Li SJ, 2015, IEEE T IMAGE PROCESS, V24, P4240, DOI 10.1109/TIP.2015.2459653
   Li X., 2013, PROC VIS COMMUN IMAG, P1
   Liu HX, 2014, IEEE T MULTIMEDIA, V16, P1549, DOI 10.1109/TMM.2014.2328324
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Pradhan SS, 2005, IEEE T INFORM THEORY, V51, P3457, DOI 10.1109/TIT.2005.855584
   Pudlewski S, 2012, IEEE T MOBILE COMPUT, V11, P1060, DOI 10.1109/TMC.2011.175
   Rivenson Y, 2009, IEEE SIGNAL PROC LET, V16, P449, DOI 10.1109/LSP.2009.2017817
   Soro S, 2009, ADV MULTIMED, V2009, DOI 10.1155/2009/640386
   Wang LJ, 2012, IEEE T IMAGE PROCESS, V21, P2980, DOI 10.1109/TIP.2012.2188810
   Wang Y, 2016, IEEE SENS J, V16, P3875, DOI 10.1109/JSEN.2016.2536941
   Yuan XL, 2016, IEEE T MULTIMEDIA, V18, P2002, DOI 10.1109/TMM.2016.2602758
   Zhang LY, 2016, IEEE T MULTIMEDIA, V18, P1720, DOI 10.1109/TMM.2016.2581593
NR 34
TC 50
Z9 53
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1610
EP 1621
DI 10.1109/TMM.2017.2774004
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100002
DA 2024-07-18
ER

PT J
AU Schmitt, M
   Redi, J
   Bulterman, D
   Cesar, PS
AF Schmitt, Marwin
   Redi, Judith
   Bulterman, Dick
   Cesar, Pablo S.
TI Towards Individual QoE for Multiparty Videoconferencing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-party videoconferencing; quality of experience; over-the-top;
   subjective quality; quality metrics; user study
ID VIDEO QUALITY; EXPERIENCE; IMPACT; DELAY
AB Videoconferencing is becoming an essential part in everyday life. The visual channel allows us for interactions that were not possible over audio-only communication systems, such as the telephone. However, being a de-facto over-the-top service, the quality of the delivered videoconferencing experience is subject to variations, depending on network conditions. Videoconferencing systems adapt to network conditions by changing, for example, encoding bit rate of the video. For this adaptation not to hamper the benefits related to the presence of a video channel in the communication, it needs to be optimized according to a measure of the quality of experience (QoE) as perceived by the user. The latter is highly dependent on the ongoing interaction and individual preferences, which have hardly been investigated so far. In this paper, we focus on the impact that video quality has on conversations that revolve around objects that are presented over the video channel. To this end, we conducted an empirical study where groups of four people collaboratively build a Lego model over a videoconferencing system. We examine the requirements for such a task by showing when the interaction, measured by visual and auditory cues, changes depending on the encoding bit rate and loss. We then explore the impact that prior experience with the technology and affective state have on QoE of participants. We use these factors to construct predictive models that double the accuracy compared to a model based on the system factors alone. We conclude with a discussion of how these factors could be applied in real-world scenarios.
C1 [Schmitt, Marwin; Bulterman, Dick; Cesar, Pablo S.] Ctr Wiskunde & Informat, NL-1098 XE Amsterdam, Netherlands.
   [Redi, Judith] Delft Univ Technol, NL-2628 CD Delft, Netherlands.
C3 Delft University of Technology
RP Schmitt, M (corresponding author), Ctr Wiskunde & Informat, NL-1098 XE Amsterdam, Netherlands.
EM m.r.schmitt@cwi.nl; j.a.redi@tudelft.nl; Dick.Bulterman@cwi.nl;
   p.s.cesar@cwi.nl
OI Cesar, Pablo/0000-0003-1752-6837
CR [Anonymous], 2000, ITU R P 920 INT TEST
   [Anonymous], 1995, ITU T REC P 910 SUBJ
   [Anonymous], P 21 ITC SPEC SEM MU
   [Anonymous], 1999, ITU P913 SUBJ VID QU
   [Anonymous], 2012, ITU T G 1070 OP MOD
   [Anonymous], 2014, P 3 INT WORKSHOP SOC
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Belmudez B., 2015, AUDIOVISUAL QUALITY
   Belmudez B., 2009, 2009 IEEE International Workshop on Multimedia Signal Processing, P1
   Belmudez B, 2013, EURASIP J AUDIO SPEE, DOI 10.1186/1687-4722-2013-24
   Bergstra J. A., 2015, ITU T G 107
   Berndtsson G., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P25, DOI 10.1109/PV.2012.6229740
   Bhattacharya A., 2011, P 19 ACM INT C MULT, P929, DOI DOI 10.1145/2072298.2071905
   Boyaci O, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P213, DOI 10.1109/ISM.2009.45
   Bräuer F, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P876
   Callet P.L., 2013, EUROPEAN NETWORK QUA
   Calyam P, 2007, LECT NOTES COMPUT SC, V4787, P14
   Davison A.C., 2008, STAT MODELS
   De Moor K, 2014, PROC SPIE, V9014, DOI 10.1117/12.2042243
   Diallo MT, 2013, IEEE INT SYM MULTIM, P518, DOI 10.1109/ISM.2013.104
   Domjan Michael P, 2014, The principles of learning and behavior
   Egger S., 2009, P GOOD BAD CHALL COS
   Egger S, 2014, T-LAB SER TELECOMMUN, P149, DOI 10.1007/978-3-319-02681-7_11
   Egger S, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1321
   Greengrass J, 2009, IEEE INTERNET COMPUT, V13, P74, DOI 10.1109/MIC.2009.40
   Hastie T., 2001, ELEMENTS STAT LEARNI, DOI 10.1007/978-0-387-84858-7_2
   Havashi T, 2007, GLOB TELECOMM CONF, P2735
   Hosfeld T., 2011, Proceedings of the 2011 23rd International Teletraffic Congress (ITC 2011), P103
   Hossfeld T, 2011, INT WORK QUAL MULTIM, P131, DOI 10.1109/QoMEX.2011.6065690
   Issing J., 2012, P 20 INT C SOFTW TEL, P1
   James M., 1967, PROC BERKELEY S MATH, V1, P281, DOI DOI 10.1007/S11665-016-2173-6
   Jones C, 1998, INT WORKSH QUAL SERV, P196, DOI 10.1109/IWQOS.1998.675239
   Karapanos E, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P639
   Ketchen DJ, 1996, STRATEGIC MANAGE J, V17, P441, DOI 10.1002/(SICI)1097-0266(199606)17:6<441::AID-SMJ819>3.0.CO;2-G
   Kilkki K, 2008, J UNIVERS COMPUT SCI, V14, P615
   Kirk D, 2010, 2010 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, P135
   KITAWAKI N, 1991, IEEE J SEL AREA COMM, V9, P586, DOI 10.1109/49.81952
   Kobayashi M., SPRINGERLINK, P83
   Kurniawan S, 2008, INT J HUM-COMPUT ST, V66, P889, DOI 10.1016/j.ijhcs.2008.03.002
   Kutner M. H., 2004, APPL LINEAR REGRESSI
   Lenth RV, 2016, J STAT SOFTW, V69, P1, DOI 10.18637/jss.v069.i01
   Mou W., 2016, Proceedings of the 24th ACM International Conference on Multimedia, P521, DOI DOI 10.1145/2964284.2967276
   Ndiaye M., 2015, 2015 7 INT WORKSH QU, P1
   O'Brien HL, 2010, J AM SOC INF SCI TEC, V61, P50, DOI 10.1002/asi.21229
   O'Brien HL, 2016, Why Engagement Matters: Cross-Disciplinary Perspectives of User Engagement in Digital Media, P1
   Palhais J., SPRINGERLINK, P261
   Redi JudithA., 2015, Visual Signal Quality Assessment, P31
   Reichl P, 2015, INT WORK QUAL MULTIM, DOI 10.1109/QoMEX.2015.7148138
   Rerábek M, 2014, PROC SPIE, V9217, DOI 10.1117/12.2065561
   Romaniak P, 2012, CONSUM COMM NETWORK, P597, DOI 10.1109/CCNC.2012.6181021
   SACKS H, 1974, LANGUAGE, V50, P696, DOI 10.2307/412243
   Schmitt Marwin, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P19, DOI 10.1109/QoMEX.2014.6982280
   Schmitt M., 2016, P 5 WORKSH PERC QUAL, P64
   Schmitt M., 2013, P 2 INT WORKSH SOC A, P37
   Schmitt M., 2016, P 8 INT C QUAL MULT, P1, DOI DOI 10.1109/QOMEX.2016.7498961
   Schoenenberg K., 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P31, DOI 10.1109/QoMEX.2014.6982282
   Schoenenberg K, 2014, SPEECH COMMUN, V63-64, P1, DOI 10.1016/j.specom.2014.04.005
   Schoenenberg K, 2014, INT J HUM-COMPUT ST, V72, P477, DOI 10.1016/j.ijhcs.2014.02.004
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Scott MJ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P481, DOI 10.1145/2733373.2806254
   Scott MJ, 2016, IEEE T MULTIMEDIA, V18, P1796, DOI 10.1109/TMM.2016.2574623
   SELLEN AJ, 1995, HUM-COMPUT INTERACT, V10, P401, DOI 10.1207/s15327051hci1004_2
   Shaikh J, 2013, IEEE GLOBE WORK, P1186, DOI 10.1109/GLOCOMW.2013.6825154
   Skowronek J., 2015, P 7 INT WORKSHOPQUAL, P1
   Soleymani M, 2012, IEEE SYS MAN CYBERN, P3304, DOI 10.1109/ICSMC.2012.6378301
   Song JR, 2016, IEEE T MULTIMEDIA, V18, P444, DOI 10.1109/TMM.2016.2520090
   Wang C, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1909, DOI 10.1145/2556288.2557154
   Yamakata Y, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P577, DOI 10.1145/2638728.2641337
   Zhang XG, 2013, IEEE T MULTIMEDIA, V15, P1446, DOI 10.1109/TMM.2013.2247988
   Zhu Y, 2015, COMPUT HUM BEHAV, V49, P412, DOI 10.1016/j.chb.2015.02.054
   Zhu Y, 2015, PROC SPIE, V9394, DOI 10.1117/12.2085002
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 72
TC 23
Z9 23
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1781
EP 1795
DI 10.1109/TMM.2017.2777466
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, J
   Yang, EZ
   Ran, YY
   Bi, YF
   Wang, J
AF Yang, Jian
   Yang, Enzhong
   Ran, Yongyi
   Bi, Yifeng
   Wang, Jun
TI Controllable Multicast for Adaptive Scalable Video Streaming in
   Software-Defined Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scalable video coding; multicast; software-defined networking; OpenFlow
ID EFFICIENT; OPTIMIZATION; FRAMEWORK
AB Scalable video coding is a promising technique to enable flexible video transmission for heterogeneous terminals and varying channel throughput. However, it is challenging to perform in-network adaptation in conventional networks because the network nodes are uncontrollable and transparent for media streaming applications. Software-defined networking (SDN) is an attractive network technology that supports the applications to collaborate with network nodes to achieve intelligent and dynamic service provisioning. Against this changing network landscape, we redesign the scalable multimedia multicast streaming by exploiting the complete network knowledge of the SDN controller to enable intelligent scalable video transmission. The proposed scalable multimedia multicast streaming framework is capable of in-network identifying, processing, and manipulating the media streams. In order to achieve the in-network adaptation, we apply equivalent bandwidth theory to estimate the affordable video layers that a link may accommodate, and apply finite-state machine to implement adaptive enhancement layer switching for multicast paths. In contrast to IP multicast, the proposed method is a controllable multicast scheme, which provides admission control in a multicast context, in-network adaptation, and supporting heterogeneous devices having different display capability. We further implement a prototype for illustrating the success of the proposed solution. The experimental results are also presented to show the effectiveness of the proposed equivalent bandwidth based adaptive enhancement layer switching algorithm.
C1 [Yang, Jian; Yang, Enzhong; Ran, Yongyi] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
   [Bi, Yifeng; Wang, Jun] ZTE Corp, Shenzhen 518057, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; ZTE
RP Yang, J (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
EM jianyang@ustc.edu.cn; ezhyang@mail.ustc.edu.cn; yyran@ustc.edu.cn;
   bi.yifeng@zte.com.cn; wang.jun17@zte.com.cn
FU State Key Program of National Natural Science of China [61233003];
   Research Funds from ZTE Corporation; Fundamental Research Funds for the
   Central Universities [WK2100100021]
FX This work was supported in part by the State Key Program of National
   Natural Science of China (No. 61233003), in part by the Research Funds
   from ZTE Corporation, and in part by the Fundamental Research Funds for
   the Central Universities (WK2100100021).
CR [Anonymous], 2010, OSTINATO PACKET TRAF
   [Anonymous], 2013, PROJECT OPEN SVC DEC
   [Anonymous], 2011, OPENFLOW 1 0 OPENWRT
   [Anonymous], 2012, P IEEE AS PAC SIGN I
   [Anonymous], 2014, OP NETW LAB
   Cao B, 2017, IEEE NETWORK, V31, P44, DOI 10.1109/MNET.2016.1500273NM
   Cao B, 2015, IEEE NETWORK, V29, P6, DOI 10.1109/MNET.2015.7166185
   Civanlar S, 2010, IEEE GLOBE WORK, P351, DOI 10.1109/GLOCOMW.2010.5700340
   Egilmez HE, 2014, IEEE T MULTIMEDIA, V16, P1597, DOI 10.1109/TMM.2014.2325791
   Egilmez HE, 2013, IEEE T MULTIMEDIA, V15, P710, DOI 10.1109/TMM.2012.2232645
   Fesci-Sayit M, 2009, IEEE IMAGE PROC, P945, DOI 10.1109/ICIP.2009.5414021
   Ghareeb M., 2011, 2011 International Conference on Information Networking (ICOIN), P206, DOI 10.1109/ICOIN.2011.5723179
   Habib Michel., 1998, Probabilistic Methods for Algorithmic Discrete Mathematics
   Hua S, 2011, IEEE T MULTIMEDIA, V13, P402, DOI 10.1109/TMM.2010.2103929
   Khalek AA, 2012, IEEE J SEL AREA COMM, V30, P1157, DOI 10.1109/JSAC.2012.120802
   Kim K, 2014, IEEE T PARALL DISTR, V25, P2571, DOI 10.1109/TPDS.2013.280
   Kotani D, 2012, 2012 IEEE/IPSJ 12TH INTERNATIONAL SYMPOSIUM ON APPLICATIONS AND THE INTERNET (SAINT), P60, DOI 10.1109/SAINT.2012.17
   Lahbabi Y, 2014, INT CONF MULTIMED, P1140, DOI 10.1109/ICMCS.2014.6911236
   Li MD, 2013, IEEE T MULTIMEDIA, V15, P1519, DOI 10.1109/TMM.2013.2267207
   Liu JC, 2004, IEEE T MULTIMEDIA, V6, P87, DOI 10.1109/TMM.2003.819753
   Marcondes CAC, 2012, 2012 IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATIONS (ISCC), P94, DOI 10.1109/ISCC.2012.6249274
   Matrawy A, 2005, IEEE T MULTIMEDIA, V7, P688, DOI 10.1109/TMM.2005.846778
   McCanne S., 1996, Computer Communication Review, V26, P117, DOI 10.1145/248157.248168
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Noghani KA, 2014, IEEE INT CONF MOB, P551, DOI 10.1109/MASS.2014.125
   Özkardes M, 2013, IEEE GLOB COMM CONF, P1693, DOI 10.1109/GLOCOM.2013.6831317
   Peng J, 2005, IEEE T MULTIMEDIA, V7, P356, DOI 10.1109/TMM.2005.843351
   Rodrigues PL, 2012, P 7 IB C INF SYST TE, P1
   Sanchez Y, 2012, SIGNAL PROCESS-IMAGE, V27, P329, DOI 10.1016/j.image.2011.10.002
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sezer S, 2013, IEEE COMMUN MAG, V51, P36, DOI 10.1109/MCOM.2013.6553676
   Traskov D, 2012, IEEE ACM T NETWORK, V20, P1479, DOI 10.1109/TNET.2011.2180736
   Widmer J, 2015, IEEE ACM T NETWORK, V23, P1107, DOI 10.1109/TNET.2014.2326523
   Wu PH, 2011, IEEE T MULTIMEDIA, V13, P1395, DOI 10.1109/TMM.2011.2168196
   Yang J, 2015, IEEE INTERNET COMPUT, V19, P36, DOI 10.1109/MIC.2015.87
   Zhang Z., 2002, P IEEE ICC 2002 NEW, P1657
NR 36
TC 20
Z9 22
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1260
EP 1274
DI 10.1109/TMM.2017.2760630
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400019
DA 2024-07-18
ER

PT J
AU Peixeiro, JP
   Brites, C
   Ascenso, J
   Pereira, F
AF Peixeiro, Jose Pedro
   Brites, Catarina
   Ascenso, Joao
   Pereira, Fernando
TI Holographic Data Coding: Benchmarking and Extending HEVC With Adapted
   Transforms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adapted transforms; HEVC extension; holographic coding; holographic
   data; holographic representation format
ID COMPRESSION
AB Holography is an emerging technology to represent and display visual information with high expectations in terms of user experience. A hologram is a reproduction of a light field represented through the interference pattern between two wavefields, the reference and the object wavefields. Whatever their creation process, holograms may have a digital representation using some appropriate format. Moreover, considering the huge amounts of data involved, digital holographic data have to be compressed using appropriate coding solutions, for example, available image coding standard solutions or efficient extensions of them. In this context, this paper contributes to advance the state-of-the-art on holographic data coding by: 1) benchmarking the most relevant available image coding standard solutions when using the most relevant holographic data representation formats; 2) proposing a novel mode depend directional transform-based HEVC coding solution, trained with holographic data. Experimental results, obtained under meaningful test conditions, show that the proposed coding solution outperforms the state-of-the-art HEVC coding standard for specific formats and conditions. Altogether, these two contributions are critical to understand the current status quo and advance the state-of-the-art on holographic data coding.
C1 [Peixeiro, Jose Pedro; Brites, Catarina; Pereira, Fernando] Inst Super Tecn, P-1049001 Lisbon, Portugal.
   [Ascenso, Joao] Inst Super Tecn, Dept Elect & Comp Engn, P-1049001 Lisbon, Portugal.
   [Peixeiro, Jose Pedro; Brites, Catarina; Ascenso, Joao; Pereira, Fernando] Inst Telecomunicacoes, P-1049001 Lisbon, Portugal.
C3 Universidade de Lisboa; Universidade de Lisboa; Instituto de
   Telecomunicacoes
RP Brites, C (corresponding author), Inst Super Tecn, P-1049001 Lisbon, Portugal.; Brites, C (corresponding author), Inst Telecomunicacoes, P-1049001 Lisbon, Portugal.
EM jose.peixeiro@ist.utl.pt; catarina.brites@lx.it.pt;
   joao.ascenso@lx.it.pt; fp@lx.it.pt
RI Pereira, Fernando/K-4046-2012; Pereira, Fernando/HNR-7786-2023; Brites,
   Catarina/L-6191-2013; Ascenso, Joao/B-9024-2008
OI Bernardo Pereira, Fernando Manuel/0000-0001-6100-947X; Brites,
   Catarina/0000-0002-6011-4574; Ascenso, Joao/0000-0001-9902-5926
FU Fundacao para a Ciencia e a Tecnologia (FCT) through the Project
   entitled Emerging Image Modalities Representation and Compression
   [PTDC/EEI-PRO/2849/2014-POCI-01-0145-FEDER-16693]
FX This work was supported by the Fundacao para a Ciencia e a Tecnologia
   (FCT) through the Project entitled Emerging Image Modalities
   Representation and Compression under Grant
   PTDC/EEI-PRO/2849/2014-POCI-01-0145-FEDER-16693. This paper was
   presented in part at the IEEE International Conference on Multimedia and
   Expo, Seattle, WA, July 2016. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof. Zhu
   Li. (Corresponding author: Catarina Brites.)
CR [Anonymous], 1996, Opt. Eng
   Arrufat A., 2015, THESIS
   Biswas M, 2010, IEEE IMAGE PROC, P165, DOI 10.1109/ICIP.2010.5652136
   Bjontegaard G., 2008, P ITU 35 M VCEG BERL
   Blinder D., 2015, 7 INT WORKSH QUAL MU, P1
   Blinder D, 2014, OPT ENG, V53, DOI 10.1117/1.OE.53.12.123102
   Cohen RA, 2010, IEEE IMAGE PROC, P185, DOI 10.1109/ICIP.2010.5651058
   Darakis E, 2006, APPL OPTICS, V45, P2437, DOI 10.1364/AO.45.002437
   Dufaux F., 2014, 3DTV C TRUE VIS CAPT, P1, DOI [10.1109/3DTV.2014.6874769, DOI 10.1109/3DTV.2014.6874769]
   Dufaux F, 2015, PROC SPIE, V9599, DOI 10.1117/12.2190997
   Farbiz F, 2005, IEEE T MULTIMEDIA, V7, P514, DOI 10.1109/TMM.2005.846787
   JPEG PLENO, 2015, 1SC29WG1N6922 ISO IE
   Kamisli F, 2009, INT CONF ACOUST SPEE, P789, DOI 10.1109/ICASSP.2009.4959702
   Naughton TJ, 2002, APPL OPTICS, V41, P4124, DOI 10.1364/AO.41.004124
   Peixeiro J., 2016, P 2016 IEEE INT C MU, P1, DOI DOI 10.1109/ICME.2016.7552940
   Peixeiro J., 2016, THESIS
   Puri S, 2016, INT CONF ACOUST SPEE, P1135, DOI 10.1109/ICASSP.2016.7471853
   Sezer O., 2011, THESIS
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Taubman D.S., 2002, JPEG 2000: Image Compression Fundamentals, Standards and Practice, DOI 10.1007/978-1-4615-0799-4
   Wang MH, 2014, IEEE T MULTIMEDIA, V16, P933, DOI 10.1109/TMM.2014.2305579
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xing Y, 2015, THESIS
   Xing YF, 2014, OPT ENG, V53, DOI 10.1117/1.OE.53.11.112312
   Xu H, 2007, IEEE T CIRC SYST VID, V17, P1325, DOI 10.1109/TCSVT.2007.903552
   Xu JZ, 2010, IEEE INT SYMP CIRC S, P3036, DOI 10.1109/ISCAS.2010.5537990
   Ye Y, 2008, IEEE IMAGE PROC, P2116, DOI 10.1109/ICIP.2008.4712205
   Zeng B, 2008, IEEE T CIRC SYST VID, V18, P305, DOI 10.1109/TCSVT.2008.918455
   Zhao X, 2012, IEEE T CIRC SYST VID, V22, P138, DOI 10.1109/TCSVT.2011.2158363
NR 29
TC 37
Z9 37
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 282
EP 297
DI 10.1109/TMM.2017.2742701
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200003
DA 2024-07-18
ER

PT J
AU Zhang, J
   Kwong, S
   Zhao, TS
   Pan, ZQ
AF Zhang, Jia
   Kwong, Sam
   Zhao, Tiesong
   Pan, Zhaoqing
TI CTU-Level Complexity Control for High Efficiency Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coding tree unit (CTU); complexity allocation; complexity control; high
   efficiency video coding (HEVC)
ID CU SIZE DECISION; MODE DECISION; HEVC ENCODERS; ALGORITHM; SELECTION;
   COMMUNICATION; CONSTRAINTS; STANDARD; TIME
AB Among the existing video-related applications, a large proportion have requirements for the scalability of the video coding complexity, such as live video chatting and video coding on power-limited mobile devices. Hence, the complexity control algorithms, which aim to make an effective and flexible tradeoff between coding complexity and rate-distortion (RD) performance, have a great practical value. In this paper, a novel complexity control scheme for high efficiency video coding (HEVC) is proposed by dynamically adjusting the depth range for each coding tree unit (CTU). To control the complexity accurately, a statistical model is proposed to estimate the coding complexity of each CTU. Then the complexity budget is allocated to each CTU proportionally to its estimated complexity. At last, the depth range is optimized for each CTU based on the allocated complexity and the probability that contains the actual maximum depth. Our method works well even if the ratio of target complexity to full complexity drops to 40%, The experimental results show that our proposed method outperforms other four state-of-the-art methods in terms of the RD performance, and has superior complexity control accuracy and complexity control stability compared with other one-pass complexity control strategies.
C1 [Zhang, Jia; Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Zhang, Jia; Kwong, Sam] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 518000, Peoples R China.
   [Zhao, Tiesong] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350116, Fujian, Peoples R China.
   [Pan, Zhaoqing] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
   [Pan, Zhaoqing] Hebei Univ Technol, Sch Comp Sci & Engn, Tianjin 300401, Peoples R China.
C3 City University of Hong Kong; Shenzhen Research Institute, City
   University of Hong Kong; City University of Hong Kong; Fuzhou
   University; Nanjing University of Information Science & Technology;
   Hebei University of Technology
RP Kwong, S (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM jzhang393-c@my.cityu.edu.hk; cssamk@cityu.edu.hk; t.zhao@fzu.edu.cn;
   zqpan3-c@my.cityu.edu.hk
RI Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261; Zhang, Jia/0000-0003-2939-2450
FU Natural Science Foundation of China [61672443]; Hong Kong RGC General
   Research Fund [9042322, CityU 11200116]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61672443 and in part by Hong Kong RGC General Research
   Fund 9042322 (CityU 11200116). The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Yonghong Tian.
CR [Anonymous], 2013, Technical Report JCTVC-L1100
   Bjontegaard G., 2001, Document VCEG-M33
   Choi K, 2012, ELECTRON LETT, V48, P689, DOI 10.1049/el.2012.0277
   Corrêa G, 2016, IEEE T CIRC SYST VID, V26, P1734, DOI 10.1109/TCSVT.2015.2469533
   Correa G, 2015, IEEE INT SYMP CIRC S, P1114, DOI 10.1109/ISCAS.2015.7168833
   Correa G, 2015, IEEE T CIRC SYST VID, V25, P660, DOI 10.1109/TCSVT.2014.2363753
   Correa G, 2013, IEEE DATA COMPR CONF, P43, DOI 10.1109/DCC.2013.12
   Corrêa G, 2012, IEEE T CIRC SYST VID, V22, P1899, DOI 10.1109/TCSVT.2012.2223411
   Deng X, 2016, IEEE ACCESS, V4, P7014, DOI 10.1109/ACCESS.2016.2612691
   Deng X, 2016, IEEE T CIRC SYST VID, V26, P91, DOI 10.1109/TCSVT.2015.2474075
   Fang JT, 2016, J VIS COMMUN IMAGE R, V40, P34, DOI 10.1016/j.jvcir.2016.06.004
   Grellert M, 2017, J REAL-TIME IMAGE PR, V13, P5, DOI 10.1007/s11554-016-0602-2
   He ZH, 2005, IEEE T CIRC SYST VID, V15, P645, DOI 10.1109/TCSVT.2005.846433
   Hu N, 2015, IEEE T CIRC SYST VID, V25, P1521, DOI 10.1109/TCSVT.2015.2395772
   Huang CL, 2001, IEEE T CIRC SYST VID, V11, P1281, DOI 10.1109/76.974682
   Jiménez-Moreno A, 2016, IEEE T MULTIMEDIA, V18, P563, DOI 10.1109/TMM.2016.2524995
   Kannangara CS, 2009, IEEE T MULTIMEDIA, V11, P433, DOI 10.1109/TMM.2009.2012937
   Kim HS, 2016, IEEE T CIRC SYST VID, V26, P130, DOI 10.1109/TCSVT.2015.2444672
   Li HL, 2004, IEEE T MULTIMEDIA, V6, P624, DOI [10.1109/TMM.2004.830812, 10.1109/tmm.2004.830812]
   Pan ZQ, 2016, J VIS COMMUN IMAGE R, V40, P516, DOI 10.1016/j.jvcir.2016.07.018
   Pan ZQ, 2014, IEEE T BROADCAST, V60, P405, DOI 10.1109/TBC.2014.2321682
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Rosewarne C., 2016, JCTVCW1002
   Shen LQ, 2015, SIGNAL PROCESS-IMAGE, V32, P121, DOI 10.1016/j.image.2015.01.008
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2014, IEEE T IMAGE PROCESS, V23, P4232, DOI 10.1109/TIP.2014.2341927
   Shen LQ, 2013, IEEE T CONSUM ELECTR, V59, P207, DOI 10.1109/TCE.2013.6490261
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiong J, 2015, IEEE T MULTIMEDIA, V17, P2147, DOI 10.1109/TMM.2015.2491018
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Zhang H, 2014, IEEE T CIRC SYST VID, V24, P660, DOI 10.1109/TCSVT.2013.2290578
   Zhang JL, 2016, IEEE T CIRC SYST VID, V26, P1502, DOI 10.1109/TCSVT.2015.2461991
   Zhang YH, 2013, IEEE IMAGE PROC, P2000, DOI 10.1109/ICIP.2013.6738412
   Zhang Y, 2015, IEEE T IND INFORM, V11, P1492, DOI 10.1109/TII.2015.2491646
   Zhang Y, 2015, IEEE T IMAGE PROCESS, V24, P2225, DOI 10.1109/TIP.2015.2417498
   Zhao WJ, 2015, IEEE T CIRC SYST VID, V25, P1651, DOI 10.1109/TCSVT.2015.2395751
NR 38
TC 33
Z9 33
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 29
EP 44
DI 10.1109/TMM.2017.2723238
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700003
DA 2024-07-18
ER

PT J
AU Huang, PC
   Lin, JR
   Li, GL
   Tai, KH
   Chen, MJ
AF Huang, Pin-Cheng
   Lin, Jie-Ru
   Li, Gwo-Long
   Tai, Kuang-Han
   Chen, Mei-Juan
TI Improved Depth-Assisted Error Concealment Algorithm for 3D Video
   Transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D video coding; depth map; error concealment; motion vector
   extrapolation
ID STANDARD
AB In this paper, a whole frame loss error concealment algorithm for three-dimensional video coding is proposed. The main concept of the proposed algorithm is to extrapolate the motion vectors for concealing a current error block by jointly considering the available motion vector and the depth information. In addition, the depth information is adopted to help the derivation of reference pixels for concealing errors in the case that suitable motion vectors cannot be obtained by the motion vector extrapolation process alone. Simulation results demonstrate that our proposed algorithm can achieve up to 0.52 dB PSNR, as well as subjective quality improvement, compared to previous work.
C1 [Huang, Pin-Cheng] United Microelect Corp, Tainan 744, Taiwan.
   [Lin, Jie-Ru; Chen, Mei-Juan] Natl Dong Hwa Univ, Dept Elect Engn, Hualien 974, Taiwan.
   [Li, Gwo-Long] Novatek Microelect Corp, Hsinchu 30076, Taiwan.
   [Tai, Kuang-Han] HwaCom Syst Inc, Taipei 221, Taiwan.
C3 United Microelectronics Corporation; National Dong Hwa University
RP Chen, MJ (corresponding author), Natl Dong Hwa Univ, Dept Elect Engn, Hualien 974, Taiwan.
EM sharkman781@gmail.com; 810523001@gms.ndhu.edu.tw; gwolong@gmail.com;
   dai.johnny@gmail.com; cmj@gms.ndhu.edu.tw
RI Tai, Kuang-Han/G-6768-2015
OI Tai, Kuang-Han/0000-0002-6214-0813; Chen, Mei-Juan/0000-0003-3382-8296
CR Ali Abidah, 2010, Proceedings of the 2010 IEEE Student Conference on Research and Development (SCOReD 2010). Engineering: Innovation & Beyond, P421, DOI 10.1109/SCORED.2010.5704046
   [Anonymous], 2009, JVTAE207 ISOIEC MPEG
   Bajic IV, 2006, IEEE T MULTIMEDIA, V8, P1263, DOI 10.1109/TMM.2006.884608
   Cen S, 2003, IEEE T MULTIMEDIA, V5, P1, DOI 10.1109/TMM.2003.808825
   Chen Y., 2004, P PICT COD S
   Chen Y., 2015, JCT3VJ1003
   Chien JT, 2010, IEEE T CONSUM ELECTR, V56, P1689, DOI 10.1109/TCE.2010.5606314
   Chong Cai, 2011, 2011 International Conference on Multimedia Technology, P384
   Chung TY, 2010, IEEE IMAGE PROC, P441, DOI 10.1109/ICIP.2010.5654236
   El Beheiry N, 2009, 2009 INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING AND SYSTEMS (ICCES 2009), P622, DOI 10.1109/ICCES.2009.5383056
   Farrugia RA, 2009, IEEE T MULTIMEDIA, V11, P1323, DOI 10.1109/TMM.2009.2030651
   Gao SS, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P128, DOI 10.1109/ISM.2012.32
   Hadizadeh H, 2013, IEEE T MULTIMEDIA, V15, P2099, DOI 10.1109/TMM.2013.2281024
   Hewage C., 2011, Multimedia and Expo (ICME), 2011 IEEE International Conference on, P1
   Huang P. C., 2014, P INT C IM PROC COMP
   Ji XY, 2009, IEEE T MULTIMEDIA, V11, P11, DOI 10.1109/TMM.2008.2008874
   Kuan YK, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-38
   Lie WN, 2015, J VIS COMMUN IMAGE R, V32, P237, DOI 10.1016/j.jvcir.2015.08.012
   Ma SW, 2014, IEEE T MULTIMEDIA, V16, P266, DOI 10.1109/TMM.2013.2284751
   Pahalawatta P., 2012, P AS PAC SIGN INF PR, P1
   Panahandeh G., 2010, Proc. International Conference Indoor Positioning Indoor Navigation, P1, DOI DOI 10.1109/IPIN.2010.5646832
   Pei SC, 2004, IEEE T MULTIMEDIA, V6, P158, DOI 10.1109/TMM.2003.819749
   Peng Q, 2002, 2002 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, CIRCUITS AND SYSTEMS AND WEST SINO EXPOSITION PROCEEDINGS, VOLS 1-4, P10, DOI 10.1109/ICCCAS.2002.1180560
   Phan R, 2014, IEEE T MULTIMEDIA, V16, P122, DOI 10.1109/TMM.2013.2283451
   Pinson M. H., 2011, NTIA HDB
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Shanableh T, 2003, IEEE T MULTIMEDIA, V5, P257, DOI 10.1109/TMM.2003.811624
   Shen-Chuan Tai, 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology (PSIVT), P214, DOI 10.1109/PSIVT.2010.43
   Su X, 2001, IEEE T MULTIMEDIA, V3, P123, DOI 10.1109/6046.909599
   Sullivan GJ, 2013, IEEE J-STSP, V7, P1001, DOI 10.1109/JSTSP.2013.2283657
   Vetro A, 2011, IEEE T BROADCAST, V57, P384, DOI 10.1109/TBC.2010.2102950
   Wang Y. K., 2011, 6184 RFC, V6184
   Wu ZY, 2006, IEEE INT SYMP CIRC S, P4463
   Yan B, 2007, IEEE T CONSUM ELECTR, V53, P1546, DOI 10.1109/TCE.2007.4429250
   Yan B, 2012, IEEE T MULTIMEDIA, V14, P936, DOI 10.1109/TMM.2012.2184743
   Yan B, 2010, IEEE T IMAGE PROCESS, V19, P98, DOI 10.1109/TIP.2009.2032311
NR 36
TC 6
Z9 7
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2625
EP 2632
DI 10.1109/TMM.2017.2694218
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200021
DA 2024-07-18
ER

PT J
AU Chen, HG
   He, XH
   Qing, LB
   Teng, QZ
AF Chen, Honggang
   He, Xiaohai
   Qing, Linbo
   Teng, Qizhi
TI Single Image Super-Resolution via Adaptive Transform-Based Nonlocal
   Self-Similarity Modeling and Learning-Based Gradient Regularization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image super-resolution; nonlocal self-similarity; local
   structure-adaptive transform; gradient regularization; Split Bregman
   Iteration
ID QUALITY ASSESSMENT; INTERPOLATION; REGRESSION; INFORMATION; ALGORITHM;
   SPARSITY
AB Single image super-resolution (SISR) is a challenging work, which aims to recover the missing information in an observed low-resolution (LR) image and generate the corresponding high-resolution (HR) version. As the SISR problem is severely ill-conditioned, effective prior knowledge of HR images is necessary to well pose the HR estimation. In this paper, an effective SISR method is proposed via the local structure-adaptive transform-based nonlocal self-similarity modeling and learning-based gradient regularization (LSNSGR). The LSNSGR exploits both the natural and learned priors of HR images, thus integrating the merits of conventional reconstruction-based and learning-based SISR algorithms. More specifically, on the one hand, we characterize nonlocal self-similarity prior (natural prior) in transform domain by using the designed local structure-adaptive transform; on the other hand, the gradient prior (learned prior) is learned via the jointly optimized regression model. The former prior is effective in suppressing visual artifacts, while the latter performs well in recovering sharp edges and fine structures. By incorporating the two complementary priors into the maximum a posteriori-based reconstruction framework, we optimize a hybrid L1- and L2-regularized minimization problem to achieve an estimation of the desired HR image. Extensive experimental results suggest that the proposed LSNSGR produces better HR estimations than many state-of-the-art works in terms of both perceptual and quantitative evaluations.
C1 [Chen, Honggang; He, Xiaohai; Qing, Linbo; Teng, Qizhi] Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Sichuan, Peoples R China.
C3 Sichuan University
RP He, XH (corresponding author), Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Sichuan, Peoples R China.
EM honggang.chen@stu.scu.edu.cn; hxh@scu.edu.cn; qing_lb@scu.edu.cn;
   qzteng@scu.edu.cn
FU National Natural Science Foundation of China [61471248]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61471248. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Ivan Bajic.
   (Corresponding author: Xiaohai He.)
CR Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai D., 2015, Eurographics, V7, P8
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1382, DOI 10.1109/TIP.2012.2231086
   Dong WS, 2011, IEEE T IMAGE PROCESS, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Elad M, 2010, P IEEE, V98, P972, DOI 10.1109/JPROC.2009.2037655
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Fukunaga K., 1991, INTRO STAT PATTERN R, V2nd
   Gao XB, 2012, IEEE T IMAGE PROCESS, V21, P3194, DOI 10.1109/TIP.2012.2190080
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   He C, 2012, IEEE J-STARS, V5, P1272, DOI 10.1109/JSTARS.2012.2189555
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Hung KW, 2012, IEEE T IMAGE PROCESS, V21, P1061, DOI 10.1109/TIP.2011.2168416
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Jiang JJ, 2016, IEEE T CIRC SYST VID, V26, P1674, DOI 10.1109/TCSVT.2015.2433538
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   Karimi N, 2015, J VIS COMMUN IMAGE R, V33, P94, DOI 10.1016/j.jvcir.2015.09.004
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kulkarni N, 2012, IEEE T CIRC SYST VID, V22, P778, DOI 10.1109/TCSVT.2011.2180773
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Li XY, 2015, IEEE T IMAGE PROCESS, V24, P2874, DOI 10.1109/TIP.2015.2432713
   Liu D, 2016, IEEE T IMAGE PROCESS, V25, P3194, DOI 10.1109/TIP.2016.2564643
   Liu JY, 2017, IEEE T MULTIMEDIA, V19, P302, DOI 10.1109/TMM.2016.2614427
   Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8
   Pan ZX, 2013, IEEE T GEOSCI REMOTE, V51, P4864, DOI 10.1109/TGRS.2012.2230270
   Papyan V, 2016, IEEE T IMAGE PROCESS, V25, P249, DOI 10.1109/TIP.2015.2499698
   Peleg T, 2014, IEEE T IMAGE PROCESS, V23, P2569, DOI 10.1109/TIP.2014.2305844
   Ren C, 2016, IEEE T IMAGE PROCESS, V25, P2168, DOI 10.1109/TIP.2016.2542442
   Romano Y, 2014, IEEE T IMAGE PROCESS, V23, P3085, DOI 10.1109/TIP.2014.2325774
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Sun J, 2011, IEEE T IMAGE PROCESS, V20, P1529, DOI 10.1109/TIP.2010.2095871
   Sun Y., 2015, IEEE Photon. J, V7, P1
   Sun Y, 2016, IEEE PHOTONICS J, V8, DOI 10.1109/JPHOT.2015.2511080
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Tian YP, 2016, IEEE IMAGE PROC, P2827, DOI 10.1109/ICIP.2016.7532875
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang LF, 2014, IEEE T IMAGE PROCESS, V23, P5123, DOI 10.1109/TIP.2014.2360459
   Wang LF, 2013, IEEE T CIRC SYST VID, V23, P1289, DOI 10.1109/TCSVT.2013.2240915
   Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Z, 2013, IEEE T IMAGE PROCESS, V22, P4271, DOI 10.1109/TIP.2013.2271849
   Xiong ZW, 2013, IEEE T MULTIMEDIA, V15, P1458, DOI 10.1109/TMM.2013.2264654
   Yan Q., 2016, IEEE T IMAGE PROCESS, V25, P2168
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang WM, 2016, IEEE T MULTIMEDIA, V18, P313, DOI 10.1109/TMM.2016.2515997
   Yu JF, 2014, IEEE T NEUR NET LEAR, V25, P780, DOI 10.1109/TNNLS.2013.2281313
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang HC, 2013, IEEE T CYBERNETICS, V43, P1035, DOI 10.1109/TSMCB.2012.2222375
   Zhang J, 2014, IEEE T CIRC SYST VID, V24, P915, DOI 10.1109/TCSVT.2014.2302380
   Zhang K, 2016, IEEE SIGNAL PROC LET, V23, P102, DOI 10.1109/LSP.2015.2504121
   Zhang KB, 2015, IEEE T IMAGE PROCESS, V24, P846, DOI 10.1109/TIP.2015.2389629
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang KB, 2011, IEEE J-STSP, V5, P230, DOI 10.1109/JSTSP.2010.2048606
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang L, 2010, PATTERN RECOGN, V43, P1531, DOI 10.1016/j.patcog.2009.09.023
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhang YB, 2016, IEEE T MULTIMEDIA, V18, P405, DOI 10.1109/TMM.2015.2512046
   Zhangyang Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301266
   Zhu Y, 2014, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR.2014.373
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 68
TC 45
Z9 47
U1 0
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1702
EP 1717
DI 10.1109/TMM.2017.2688920
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400002
DA 2024-07-18
ER

PT J
AU Koloda, J
   Seiler, J
   Kaup, A
AF Koloda, Jan
   Seiler, Juergen
   Kaup, Andre
TI Frequency-Selective Mesh-to-Grid Resampling for Image Communication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Frequency-selective resampling (FSR); image reconstruction; noninteger
   pixel positions
ID SCATTERED DATA INTERPOLATION; ERROR CONCEALMENT; SUPERRESOLUTION;
   RECONSTRUCTION; SIGNAL; EXTRAPOLATION
AB This paper presents a novel approach for image reconstruction from pixels located at arbitrary noninteger positions, called mesh. This task forms an intrinsic part of various multimedia applications, including superresolution, fisheye imaging, or generations of new views in multicamera systems, among others. We propose a new frequency-selective mesh-to-grid resampling algorithm that aims at producing high quality reconstructions. It is inspired by the existing frequency-selective reconstruction (FSR) algorithm that is known to exhibit high performance when pixels are located on the regular 2D grid. However, if samples that are located at noninteger positions are involved, a severe overfitting problem arises from the fact that nonorthogonal weighted bases sampled at noninteger positions are used for signal modeling. In order to overcome this issue, we propose a novel stabilizing mechanism that is based on a set of adaptively weighted initial estimates, called key points. We also show that Fourier basis, used in the classic grid-based FSR, yields complex signals when noninteger positions are involved. Since digital images are real valued, we propose to employ a 2D cosine transform basis. Experimental results show the superiority of the proposed approach over a wide range of existing reconstruction techniques.
C1 [Koloda, Jan; Seiler, Juergen; Kaup, Andre] Friedrich Alexander Univ Erlangen Nuremberg FAU, Chair Multimedia Commun & Signal Proc, D-91054 Erlangen, Germany.
C3 University of Erlangen Nuremberg
RP Koloda, J (corresponding author), Friedrich Alexander Univ Erlangen Nuremberg FAU, Chair Multimedia Commun & Signal Proc, D-91054 Erlangen, Germany.
EM jan.koloda@fau.de; juergen.seiler@fau.de; andre.kaup@fau.de
OI Koloda, Jan/0000-0001-5053-4615; Kaup, Andre/0000-0002-0929-5074
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Amidror I, 2002, J ELECTRON IMAGING, V11, P157, DOI 10.1117/1.1455013
   [Anonymous], 1968, P 1968 ACM NAT C
   Arigovindan M, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P381, DOI 10.1109/ICIP.2002.1038985
   Bätz M, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P217, DOI 10.1109/VCIP.2014.7051543
   Bao P, 2006, IEEE T MULTIMEDIA, V8, P382, DOI 10.1109/TMM.2005.864337
   Blu T, 2004, IEEE T IMAGE PROCESS, V13, P710, DOI 10.1109/TIP.2004.826093
   Bossen F, 2013, Document JCTVCL1100
   Candès EJ, 2011, APPL COMPUT HARMON A, V31, P59, DOI 10.1016/j.acha.2010.10.002
   Caner G, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P109
   Choi H, 1999, INT CONF ACOUST SPEE, P1645, DOI 10.1109/ICASSP.1999.756307
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   FRANKE R, 1982, MATH COMPUT, V38, P181, DOI 10.2307/2007474
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Inamoto N, 2007, IEEE T MULTIMEDIA, V9, P1155, DOI 10.1109/TMM.2007.902832
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Jonscher M, 2014, IEEE IMAGE PROC, P2879, DOI 10.1109/ICIP.2014.7025582
   Kanzawa Yusuke, 2008, 2008 IEEE Intelligent Vehicles Symposium (IV), P43, DOI 10.1109/IVS.2008.4621196
   Kaup A, 2005, AEU-INT J ELECTRON C, V59, P147, DOI 10.1016/j.aeue.2005.03.015
   Kim US, 2014, IEEE T CIRC SYST VID, V24, P384, DOI 10.1109/TCSVT.2013.2278142
   Koloda J., 1976, P IEEE INT C AC SPEE
   Koloda J, 2015, IEEE IMAGE PROC, P4565, DOI 10.1109/ICIP.2015.7351671
   Koloda J, 2013, IEEE T MULTIMEDIA, V15, P957, DOI 10.1109/TMM.2013.2238524
   Konrad J, 1999, P SOC PHOTO-OPT INS, V3639, P179, DOI 10.1117/12.349379
   Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177
   LAM NSN, 1983, AM CARTOGRAPHER, V10, P129, DOI 10.1559/152304083783914958
   Lee S, 1997, IEEE T VIS COMPUT GR, V3, P228, DOI 10.1109/2945.620490
   LISZKA T, 1984, INT J NUMER METH ENG, V20, P1599, DOI 10.1002/nme.1620200905
   Lukaszyk S, 2004, COMPUT MECH, V33, P299, DOI 10.1007/s00466-003-0532-2
   Ma M, 2008, IEEE T MULTIMEDIA, V10, P1638, DOI 10.1109/TMM.2008.2007282
   Maeland E., 2005, IEEE T MED IMAGING, V7, P213
   Mallat S.A., 1999, WAVELET TOUR SIGNAL
   Melo R, 2012, IEEE T BIO-MED ENG, V59, P634, DOI 10.1109/TBME.2011.2177268
   Meyer J, 2011, OPT EXPRESS, V19, P17506, DOI 10.1364/OE.19.017506
   MIYAMOTO K, 1964, J OPT SOC AM, V54, P1060, DOI 10.1364/JOSA.54.001060
   Nguyen N, 2000, CIRC SYST SIGNAL PR, V19, P321, DOI 10.1007/BF01200891
   Okabe A., 1992, SPATIAL TESSELATIONS
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Ringaby E, 2014, IEEE T IMAGE PROCESS, V23, P2302, DOI 10.1109/TIP.2014.2316377
   Schoberl M., 2011, P IEEE INT C IM PROC, P1937
   Seiler J., 2007, P PICT COD S LISB PO
   Seiler J, 2008, INT CONF ACOUST SPEE, P781, DOI 10.1109/ICASSP.2008.4517726
   Seiler J, 2015, IEEE T IMAGE PROCESS, V24, P4540, DOI 10.1109/TIP.2015.2463084
   Seiler J, 2011, EURASIP J ADV SIG PR, DOI 10.1155/2011/495394
   Seiler J, 2010, IEEE SIGNAL PROC LET, V17, P949, DOI 10.1109/LSP.2010.2078504
   Seiler J, 2008, IEEE IMAGE PROC, P2788, DOI 10.1109/ICIP.2008.4712373
   Shum HY, 2005, IEEE T MULTIMEDIA, V7, P85, DOI 10.1109/TMM.2004.840591
   Sibson R., 1980, Interpolating Multivariate Data: A Brief Description of Natural Neighbour Interpolation
   Stasinski R, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P689
   Stasinski R, 2002, SIGNAL PROCESS-IMAGE, V17, P689, DOI 10.1016/S0923-5965(02)00080-2
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Thévenaz P, 2000, IEEE T MED IMAGING, V19, P739, DOI 10.1109/42.875199
   Unser M, 1999, IEEE SIGNAL PROC MAG, V16, P22, DOI 10.1109/79.799930
   Vázquez C, 2005, IEEE T IMAGE PROCESS, V14, P713, DOI 10.1109/TIP.2005.847297
   Vázquez C, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P405, DOI 10.1109/ICIP.2002.1038991
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 57
TC 6
Z9 6
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1689
EP 1701
DI 10.1109/TMM.2017.2683267
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400001
DA 2024-07-18
ER

PT J
AU Bai, S
   Bai, X
   Zhou, ZC
   Zhang, ZX
   Tian, Q
   Latecki, LJ
AF Bai, Song
   Bai, Xiang
   Zhou, Zhichao
   Zhang, Zhaoxiang
   Tian, Qi
   Latecki, Longin Jan
TI GIFT: Towards Scalable 3D Shape Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D shape retrieval; CNN; shape retrieval contest (SHREC)
ID DESCRIPTORS; MODEL; COVARIANCE; FEATURES; SEARCH; ROBUST
AB Projective analysis is an important solution in three-dimensional (3D) shape retrieval, since human visual perceptions of 3D shapes rely on various 2D observations from different viewpoints. Although multiple informative and discriminative views are utilized, most projection-based retrieval systems suffer from heavy computational cost, and thus cannot satisfy the basic requirement of scalability for search engines. In the past three years, shape retrieval contest (SHREC) pays much attention to the scalability of 3D shape retrieval algorithms, and organizes several large scale tracks accordingly [1]-[3]. However, the experimental results indicate that conventional algorithms cannot be directly applied to large datasets. In this paper, we present a real-time 3D shape search engine based on the projective images of 3D shapes. The real-time property of our search engine results from the following aspects: 1) efficient projection and view feature extraction using GPU acceleration; 2) the first inverted file, called F-IF, is utilized to speed up the procedure of multiview matching; and 3) the second inverted file, which captures a local distribution of 3D shapes in the feature manifold, is adopted for efficient context-based reranking. As a result, for each query the retrieval task can be finished within one second despite the necessary cost of IO overhead. We name the proposed 3D shape search engine, which combines GPU acceleration and inverted file (twice), as GIFT. Besides its high efficiency, GIFT also outperforms state-of-the-art methods significantly in retrieval accuracy on various shape benchmarks (ModelNet40 dataset, ModelNet10 dataset, PSB dataset, McGill dataset) and competitions (SHREC14LSGTB, ShapeNet Core55, WM-SHREC07).
C1 [Bai, Song; Bai, Xiang; Zhou, Zhichao] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Zhang, Zhaoxiang] Chinese Acad Sci, Inst Automat, Ctr Brain Inspired Intelligence, CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing 100190, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
   [Latecki, Longin Jan] Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA.
C3 Huazhong University of Science & Technology; Chinese Academy of
   Sciences; Institute of Automation, CAS; University of Texas System;
   University of Texas at San Antonio (UTSA); Pennsylvania Commonwealth
   System of Higher Education (PCSHE); Temple University
RP Bai, X (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM songbai@hust.edu.cn; xbai@hust.edu.cn; zzc@hust.edu.cn;
   zhaoxiang.zhang@ia.ac.cn; qitian@cs.utsa.edu; latecki@temple.edu
OI Latecki, Longin Jan/0000-0002-5102-8244; Bai, Xiang/0000-0002-3449-5940
FU National Natural Science Foundation of China [61231010, 61573160,
   61429201]; China Scholarship Council; National Science Foundation
   [IIS-1302164]; ARO [W911NF-15-1-0290]; Faculty Research Gift Awards by
   NEC Laboratories of America and Blippar; Direct For Computer & Info Scie
   & Enginr; Div Of Information & Intelligent Systems [1302164] Funding
   Source: National Science Foundation
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61231010, Grant 61573160, and Grant
   61429201, in part by the China Scholarship Council, and in part by the
   National Science Foundation under Grant IIS-1302164. The work of Q. Tian
   was supported by the ARO Grant W911NF-15-1-0290, and by the Faculty
   Research Gift Awards by NEC Laboratories of America and Blippar. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Yap-Peng Tan. (Corresponding
   author: Xiang Bai.)
CR [Anonymous], 2016, P EUR WORKSH 3D OBJ
   [Anonymous], 2015, P 2015 EUR WORKSH 3D
   [Anonymous], EUR WORKSH 3D OBJ RE, DOI DOI 10.2312/3DOR/3DOR08/009-016
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2014, P BRIT MACHINE VISIO
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI 10.1145/2502081.2502171
   [Anonymous], 2009, P ACM INT C IM VID R, DOI DOI 10.1145/1646396.1646430
   Ansary TF, 2007, IEEE T MULTIMEDIA, V9, P78, DOI 10.1109/TMM.2006.886359
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Assfalg J, 2007, IEEE T MULTIMEDIA, V9, P589, DOI 10.1109/TMM.2006.886271
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Bai S, 2016, IEEE T MULTIMEDIA, V18, P1351, DOI 10.1109/TMM.2016.2557071
   Bai S, 2016, IEEE T IMAGE PROCESS, V25, P1056, DOI 10.1109/TIP.2016.2514498
   Bai X, 2015, IEEE T PATTERN ANAL, V37, P2361, DOI 10.1109/TPAMI.2015.2424863
   Bai X, 2014, IEEE T IMAGE PROCESS, V23, P3935, DOI 10.1109/TIP.2014.2336542
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Daras P, 2010, INT J COMPUT VISION, V89, P229, DOI 10.1007/s11263-009-0277-2
   Donoser M, 2013, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2013.174
   Fang Y, 2015, PROC CVPR IEEE, P2319, DOI 10.1109/CVPR.2015.7298845
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Giorgi D., 2007, SHREC COMPETITION, V8, P1
   Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Havlena M, 2014, LECT NOTES COMPUT SC, V8691, P46, DOI 10.1007/978-3-319-10578-9_4
   Jaderberg M., 2015, ICLR, DOI DOI 10.48550/ARXIV.1506.02025
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671
   Lavoué G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x
   Li B., 2013, EUR WORKSH 3D OBJ RE, P89, DOI DOI 10.2312/3DOR/3DOR13/089
   Li B, 2014, EUR WORKSH 3D OBJ RE, P121
   Li B, 2015, COMPUT VIS IMAGE UND, V131, P1, DOI 10.1016/j.cviu.2014.10.006
   Li B, 2013, MULTIMED TOOLS APPL, V62, P821, DOI 10.1007/s11042-011-0873-3
   Li CY, 2016, PROC CVPR IEEE, P5666, DOI 10.1109/CVPR.2016.611
   Li CY, 2014, PROC CVPR IEEE, P2003, DOI 10.1109/CVPR.2014.257
   Lian ZH, 2013, MACH VISION APPL, V24, P1685, DOI 10.1007/s00138-013-0501-5
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Litman R, 2014, COMPUT GRAPH FORUM, V33, P127, DOI 10.1111/cgf.12438
   Liu MZ, 2012, IEEE T PATTERN ANAL, V34, P2407, DOI 10.1109/TPAMI.2012.44
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo L, 2013, IEEE T MULTIMEDIA, V15, P1174, DOI 10.1109/TMM.2013.2242450
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Rodolà E, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.91
   Rodolà E, 2014, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2014.532
   Rodolà E, 2013, IEEE I CONF COMP VIS, P1169, DOI 10.1109/ICCV.2013.149
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tabia H., 2013, Proceedings of the Sixth Eurographics Workshop on 3D Object Retrieval, P17
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Tabia H, 2014, PROC CVPR IEEE, P4185, DOI 10.1109/CVPR.2014.533
   Tabia H, 2011, IEEE T PATTERN ANAL, V33, P852, DOI 10.1109/TPAMI.2010.202
   Tatsuma A., 2012, Signal Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Pacific, P1
   Tatsuma A, 2016, IEICE T INF SYST, VE99D, P1711, DOI 10.1587/transinf.2015EDL8212
   Tolias G, 2016, INT J COMPUT VISION, V116, P247, DOI 10.1007/s11263-015-0810-4
   Vranic DV, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P963
   Wang YH, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508393
   Wu JJ, 2016, ADV NEUR IN, V29
   Xie J, 2016, PROC CVPR IEEE, P3309, DOI 10.1109/CVPR.2016.360
   Xie J, 2015, PROC CVPR IEEE, P1275, DOI 10.1109/CVPR.2015.7298732
   Yang XW, 2009, PROC CVPR IEEE, P357, DOI 10.1109/CVPRW.2009.5206844
   Zhang ST, 2015, IEEE T PATTERN ANAL, V37, P803, DOI 10.1109/TPAMI.2014.2346201
   Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783
   Zheng L, 2015, IEEE T MULTIMEDIA, V17, P648, DOI 10.1109/TMM.2015.2408563
   Zheng Liang., 2016, Person re-identification: Past, present and future
   Zhou Y, 2016, INT J COMPUT VISION, V118, P337, DOI 10.1007/s11263-015-0879-9
   Zhou ZH, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P908
NR 76
TC 66
Z9 66
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1257
EP 1271
DI 10.1109/TMM.2017.2652071
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400012
OA hybrid
DA 2024-07-18
ER

PT J
AU Ding, K
   Fan, B
   Huo, CL
   Xiang, SM
   Pan, CH
AF Ding, Kun
   Fan, Bin
   Huo, Chunlei
   Xiang, Shiming
   Pan, Chunhong
TI Cross-Modal Hashing via Rank-Order Preserving
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal similarity search; cross-modal hashing (CMH); rank-order
   preserving
ID NEAREST-NEIGHBOR; IMAGE SIMILARITY; GRAPH; CLASSIFICATION; CODES
AB Due to the query effectiveness and efficiency, cross-modal similarity search based on hashing has acquired extensive attention in the multimedia community. Most existing methods do not explicitly employ the ranking information when learning hash functions, which is quite important for building practical retrieval systems. To solve this issue, this paper proposes a rank-order preserving hashing (RoPH) method with a novel regression-based rank-order preserving loss that has provable large margin property and is easy to optimize. Moreover, we jointly learn the binary codes and hash functions instead of using any relaxation trick. To solve the induced optimization problem, the alternating descent technique is adopted and each subproblem can be solved conveniently. Specifically, we show that the involved binary quadratic programming subproblem with respect to an introduced auxiliary binary variable satisfies submodularity, enabling us to use the off-the-shelf graph-cut algorithms to solve it exactly and efficiently. Extensive experiments on three benchmarks demonstrate that RoPH significantly improves the ranking quality over the state of the arts.
C1 [Ding, Kun; Fan, Bin; Huo, Chunlei; Xiang, Shiming; Pan, Chunhong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Ding, Kun] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Huo, CL (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM kding@nlpr.ia.ac.cn; bfan@nlpr.ia.ac.cn; clhuo@nlpr.ia.ac.cn;
   smxiang@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn
RI Fan, Bin/AAD-8307-2019; DING, KUN/HNJ-1709-2023; Fan, Bin/HKN-3438-2023;
   Huo, Chunlei/KTI-1888-2024
FU National Natural Science Foundation of China [91646207, 61573352,
   61672098, 91438105]; Strategic Priority Research Program of the CAS
   [XDB02060009]; Beijing Natural Science Foundation [4142057]; Priority
   Academic Program Development of Jiangsu Higher Education Institutions;
   Jiangsu Collaborative Innovation Center on Atmospheric Environment and
   Equipment Technology
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 91646207, Grant 61573352, Grant
   61672098, and Grant 91438105, in part by the Strategic Priority Research
   Program of the CAS under Grant XDB02060009, in part by the Beijing
   Natural Science Foundation under Grant 4142057, and in part by the
   Priority Academic Program Development of Jiangsu Higher Education
   Institutions and Jiangsu Collaborative Innovation Center on Atmospheric
   Environment and Equipment Technology.
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], 2012, NEURIPS
   [Anonymous], 2016, COMPUT RES REPOSITOR
   [Anonymous], 2009, NEURIPS
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2012, Book A probabilistic model for multimodal hash function learning, DOI DOI 10.1145/2339530.2339678
   [Anonymous], 2016, ARXIV160206697
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2009, P ACM INT C IM VID R
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2012, Advances in neural information processing systems
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Ge TZ, 2014, LECT NOTES COMPUT SC, V8695, P250, DOI 10.1007/978-3-319-10584-0_17
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Gu SM, 2013, INT CONF MACH LEARN, P108, DOI 10.1109/ICMLC.2013.6890453
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Irie G, 2015, IEEE I CONF COMP VIS, P1886, DOI 10.1109/ICCV.2015.219
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Kim SY, 2003, COMPUT OPTIM APPL, V26, P143, DOI 10.1023/A:1025794313696
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Lin GS, 2015, IEEE T PATTERN ANAL, V37, P2317, DOI 10.1109/TPAMI.2015.2404776
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lu WT, 2013, IEEE T MULTIMEDIA, V15, P1920, DOI 10.1109/TMM.2013.2280895
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Rastegari Mohammad, 2013, Book Predictable Dual-View Hashing
   Ren YJ, 2015, J INTERNET TECHNOL, V16, P317, DOI 10.6138/JIT.2015.16.2.20140918
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Schreiber H, 2014, IEEE T MULTIMEDIA, V16, P1654, DOI 10.1109/TMM.2014.2318517
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Torralba A., 2008, PROC IEEE C COMPUT V, P1
   Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938
   Wang DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2291
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang J, 2013, IEEE I CONF COMP VIS, P3032, DOI 10.1109/ICCV.2013.377
   Wang KY, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P459, DOI 10.1145/2671188.2749297
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wu BT, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3946
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Xia ZH, 2016, IEEE T INF FOREN SEC, V11, P2594, DOI 10.1109/TIFS.2016.2590944
   Xia ZH, 2016, IEEE T PARALL DISTR, V27, P340, DOI 10.1109/TPDS.2015.2401003
   Xiang SM, 2012, IEEE T NEUR NET LEAR, V23, P1738, DOI 10.1109/TNNLS.2012.2212721
   Xie L, 2016, AAAI CONF ARTIF INTE, P294
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Yao T, 2016, NEUROCOMPUTING, V193, P250, DOI 10.1016/j.neucom.2016.02.016
   Yu SX, 2004, IEEE T PATTERN ANAL, V26, P173, DOI 10.1109/TPAMI.2004.1262179
   Yu Y, 2013, IEEE T MULTIMEDIA, V15, P1969, DOI 10.1109/TMM.2013.2269313
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
   Zhuang BH, 2016, PROC CVPR IEEE, P5955, DOI 10.1109/CVPR.2016.641
   Zou FH, 2015, IEEE T MULTIMEDIA, V17, P1006, DOI 10.1109/TMM.2015.2425651
NR 64
TC 60
Z9 60
U1 2
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 571
EP 585
DI 10.1109/TMM.2016.2625747
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400012
DA 2024-07-18
ER

PT J
AU Koz, A
   Lagendijk, RL
AF Koz, Alper
   Lagendijk, R. (Inald) L.
TI Distributed Content Based Video Identification in Peer-to-Peer Networks:
   Requirements and Solutions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Access right management; content search; distributed hash table;
   fingerprint; Gaussian scales; peer-to-peer (P2P); perceptual hash; video
   copy detection; video identification
ID COPY DETECTION; SEARCH; SYSTEM; RETRIEVAL
AB In this paper, we first discuss the essential requirements for a fingerprint (perceptual hash)-based distributed video identification system in peer-to-peer (P2P) networks in comparison with traditional central database implementations of fingerprints. This discussion reveals that first, fingerprint sizes of existing video fingerprint methods are not compatible with the cache sizes of current P2P clients; second, fingerprint extraction durations during a query are not at tolerable levels for a user in the network; third, the repetitive patterns in the extracted fingerprints avoid the uniform distribution of storage and traffic load among the peers; and finally, the existing methods do not provide a solution to synchronize the fingerprint extraction from the shared video and queried video. In order to solve the mentioned requirements, we propose a baseline method using only the difference of video framemeans, which decreases the fingerprint sizes to typical cache sizes, by increasing the granularity levels from seconds to minutes. We then develop a novel algorithm which utilizes reference points on one-dimensional frame mean sequence for the synchronization of fingerprint extraction. This algorithm is extended with a hierarchical decoding approach based on Gaussian scales, which only decodes a subset of video frames without needing a full decoding. Finally, an analysis on the effect of design parameters to the fingerprint probability distribution is performed to avoid repetitive patterns. Our ultimate solution reduces the fingerprint sizes into kilobytes, extraction time to seconds, and search duration into milliseconds, and achieves about 90% detection rates with 1-4 min granularities, while enabling a fair distribution of storage load among the peers at the same time.
C1 [Koz, Alper] Middle East Tech Univ, Ctr Image Anal, TR-06800 Ankara, Turkey.
   [Lagendijk, R. (Inald) L.] Delft Univ Technol, Multimedia Signal Proc Grp, Fac Elect Engn Math & Comp Sci, NL-2628 CD Delft, Netherlands.
C3 Middle East Technical University; Delft University of Technology
RP Koz, A (corresponding author), Middle East Tech Univ, Ctr Image Anal, TR-06800 Ankara, Turkey.
EM koz@metu.edu.tr; R.L.Lagendijk@tudelft.nl
RI Koz, Alper/ISU-3909-2023
CR [Anonymous], 2014, Cisco Visual Networking Index. Global Mobile Data Traffic Forecast Update
   Asha S, 2014, 2014 FOURTH INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING AND COMMUNICATIONS (ICACC), P93, DOI 10.1109/ICACC.2014.28
   Asthana H, 2011, LECT NOTES COMPUT SC, V6931, P125, DOI 10.1007/978-3-642-23318-0_13
   Bahmani Bahman., 2012, P 21 ACM INT C INFOR, P2174, DOI DOI 10.1145/2396761.2398596
   Bakker A., 2009, TRIBLER PROTOCOL SPE
   Chaisorn L, 2010, IEEE IMAGE PROC, P2129, DOI 10.1109/ICIP.2010.5652053
   Chen L, 2008, PATTERN RECOGN LETT, V29, P1824, DOI 10.1016/j.patrec.2008.05.015
   Chen S, 2010, INT CONF ACOUST SPEE, P2378, DOI 10.1109/ICASSP.2010.5496165
   Cohen Bram, 2008, The BitTorrent protocol specification
   Coskun B, 2006, IEEE T MULTIMEDIA, V8, P1190, DOI 10.1109/TMM.2006.884614
   Cox IJ, 2009, LECT NOTES COMPUT SC, V5766, P2, DOI 10.1007/978-3-642-04417-5_2
   Esmaeili MM, 2011, IEEE T INF FOREN SEC, V6, P213, DOI 10.1109/TIFS.2010.2097593
   Garbacki P., 2007, Distributed Computing Systems, P31
   Haghani Parisa., 2009, PROC 12 INT C EXTEND, P744
   Jantunen A., 2006, Peer-to-Peer Analysis State-of-the-art
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Kalker T, 2004, P IEEE, V92, P961, DOI 10.1109/JPROC.2004.827368
   Kim C, 2005, IEEE T CIRC SYST VID, V15, P127
   Koz A., 2016, SUPPLEMENTARY NOTE C, P1
   Koz A, 2010, INT CONF ACOUST SPEE, P1842, DOI 10.1109/ICASSP.2010.5495380
   Kundur D, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P404
   Law-To J., 2006, P ACM INT C MULTIMED, P835
   Law-To Julien., 2007, P 6 ACM INT C IMAGE, P371
   Liang J., 2004, P 19 IEEE ANN COMPUT, P17
   Lindeberg Tony, 1991, THESIS
   Liu H, 2013, IEEE T KNOWL DATA EN, V25, P1706, DOI 10.1109/TKDE.2012.92
   Mikolajczyk K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P525, DOI 10.1109/ICCV.2001.937561
   MOHAN R, 1998, P INT C AUD SPEECH S, V6, P3679
   Oostveen J., 2002, Recent Advances in Visual Information Systems. 5th International Conference VISUAL 2002. Proceedings (Lecture Notes in Computer Science Vol.2314), P117
   Oram A., 2001, PEER TO PEER HARNESS
   Özkan S, 2014, IEEE IMAGE PROC, P2527, DOI 10.1109/ICIP.2014.7025511
   Pomelse JA, 2008, TELECOMMUN POLICY, V32, P701, DOI 10.1016/j.telpol.2008.09.004
   Pouwelse J, 2005, LECT NOTES COMPUT SC, V3640, P205, DOI 10.1007/11558989_19
   Pouwelse JA, 2008, CONCURR COMP-PRACT E, V20, P127, DOI 10.1002/cpe.1189
   Ratnasamy S, 2001, ACM SIGCOMM COMP COM, V31, P161, DOI 10.1145/964723.383072
   Rowstron A., 2001, Proceedings of the Middleware 2001, P329, DOI DOI 10.1007/3-540-45518-3_18
   Schulze H., 2009, IPOQUE INTERNET STUD
   Shrestha P., 2004, P INT S MUS INF RETR, P341
   Sripanidkulchai K, 2003, IEEE INFOCOM SER, P2166
   Stoica I, 2003, IEEE ACM T NETWORK, V11, P17, DOI 10.1109/TNET.2002.808407
   Tzanetakis G, 2004, COMPUT MUSIC J, V28, P24, DOI 10.1162/014892604323112220
   Uchida Y, 2010, IEEE IMAGE PROC, P1021, DOI 10.1109/ICIP.2010.5650242
   Zhang RM, 2007, IEEE T PARALL DISTR, V18, P1146, DOI 10.1109/TPDS.2007.1035
   Zhao WL, 2013, IEEE T IMAGE PROCESS, V22, P980, DOI 10.1109/TIP.2012.2226043
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
NR 45
TC 4
Z9 4
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 475
EP 491
DI 10.1109/TMM.2016.2625198
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400005
DA 2024-07-18
ER

PT J
AU Li, WX
   Joo, J
   Qi, H
   Zhu, SC
AF Li, Weixin
   Joo, Jungseock
   Qi, Hang
   Zhu, Song-Chun
TI Joint Image-Text News Topic Detection and Tracking by Multimodal Topic
   And-Or Graph
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cluster sampling; multimodal topic and-or graph (MT-AOG); news topic
   detection and tracking
AB This paper presents a novel method for automatically detecting and tracking news topics from multimodal TV news data. We propose a multimodal topic and-or graph (MT-AOG) to jointly represent textual and visual elements of news stories and their latent topic structures. An MT-AOG leverages a contextsensitive grammar that can describe the hierarchical composition of news topics by semantic elements about people involved, related places, and what happened, and model contextual relationships between elements in the hierarchy. We detect news topics through a cluster sampling process which groups stories about closely related events together. Swendsen- Wang cuts, an effective cluster sampling algorithm, is adopted for traversing the solution space and obtaining optimal clustering solutions by maximizing a Bayesian posterior probability. The detected topics are then continuously tracked and updated with incoming news streams. We generate topic trajectories to show how topics emerge, evolve, and disappear over time. The experimental results show that our method can explicitly describe the textual and visual data in news videos and produce meaningful topic trajectories. Our method also outperforms previous methods for the task of document clustering on Reuters-21578 dataset and our novel dataset, UCLA Broadcast News dataset.
C1 [Li, Weixin; Qi, Hang] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.
   [Joo, Jungseock] Univ Calif Los Angeles, Dept Commun Studies, Los Angeles, CA 90095 USA.
   [Zhu, Song-Chun] Univ Calif Los Angeles, Dept Stat & Comp Sci, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles;
   University of California System; University of California Los Angeles;
   University of California System; University of California Los Angeles
RP Li, WX (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.
EM lwx@cs.ucla.edu; jjoo@commstds.ucla.edu; hangqi@cs.ucla.edu;
   sczhu@stat.ucla.edu
OI Joo, Jungseock/0000-0002-4707-8919
FU NSF CDI project [CNS 1028381]
FX This work was supported by the NSF CDI project CNS 1028381.
CR Aggarwal Charu C, 2012, A survey of text clustering algorithms, P163, DOI [10.1007/978-1-4614-3223-4, DOI 10.1007/978-1-4614-3223-46]
   Ahmed A., 2008, SDM, P219, DOI DOI 10.1137/1.9781611972788.20
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Allan J., 1998, P DARPA BROADC NEWS
   Allan J., 2002, INTRO TOPIC DETECTIO
   [Anonymous], 2005, Advances in Neural Information Processing Systems
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 1996, Technical Communication
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2012, 20 ACM INT C MULTIME
   [Anonymous], 2013, P 29 C 2013, DOI [DOI 10.5555/3023638.3023709, DOI 10.48550/ARXIV.1309.6874]
   [Anonymous], P 3 TEXT AN C
   [Anonymous], 2015, P 6 ACM MULTIMEDIA S
   [Anonymous], P 8 INT C LANG RES E
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2015, P C EMP METH NAT LAN
   Barbu A, 2005, IEEE T PATTERN ANAL, V27, P1239, DOI 10.1109/TPAMI.2005.161
   Bejan CA, 2014, COMPUT LINGUIST, V40, P311, DOI [10.1162/coli_a_00174, 10.1162/COLI_a_00174]
   Blei D.M., 2006, INT C MACHINE LEARNI, DOI [DOI 10.1145/1143844.1143859, 10.1145/1143844.1143859]
   Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Boyd-Graber JordanL., 2009, Advances in neural information processing systems, P185
   Cai D, 2011, IEEE T KNOWL DATA EN, V23, P902, DOI 10.1109/TKDE.2010.165
   Cai HY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P89, DOI 10.1145/2733373.2806236
   Chang J., 2009, AISTATS
   Chen T, 2015, AAAI CONF ARTIF INTE, P30
   Chu LY, 2016, IEEE T CIRC SYST VID, V26, P556, DOI 10.1109/TCSVT.2014.2347551
   Finkel Jenny Rose, 2005, ACL, P363
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Harriss Julian., 1981, The Complete Reporter: Fundamentals of News Gathering, Writing, and Editing, Complete with Exercises
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Hsu WH, 2006, IEEE IMAGE PROC, P141, DOI 10.1109/ICIP.2006.312379
   Ji Heng, 2011, ACL, P1148
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Joo J., 2012, Handbook of Perceptual Organization
   Joo J, 2015, IEEE I CONF COMP VIS, P3712, DOI 10.1109/ICCV.2015.423
   Joo J, 2014, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2014.35
   Kim D, 2011, LECT NOTES COMPUT SC, V6609, P163, DOI 10.1007/978-3-642-19437-5_13
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Hongzhi., 2013, Proceedings of the 21st ACM international conference on Multimedia, P449
   Li Hongzhi., 2013, Proceedings of the 21st ACM international conference on Multimedia, P357, DOI DOI 10.1145/2502081.2508118
   Makkonen J, 2004, INFORM RETRIEVAL, V7, P347, DOI 10.1023/B:INRT.0000011210.12953.86
   Mei Qiaozhu., 2005, KDD 05, P198, DOI DOI 10.1145/1081870.1081895
   Newman D., 2006, KDD 2006, P680
   Niu ZX, 2014, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2014.539
   Pavlovskaia M, 2015, LECT NOTES COMPUT SC, V8932, P421, DOI 10.1007/978-3-319-14612-6_31
   Putthividhya D, 2010, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2010.5540000
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Steinbach M., 2000, KDD WORKSH TEXT MIN, P525
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang Chong, 2011, P 17 ACM SIGKDD INT, P448, DOI [DOI 10.1145/2020408.2020480, 10.1145/2020408.2020480]
   Wu X, 2006, IEEE SIGNAL PROC MAG, V23, P59
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xu W., 2003, P 26 ANN INT ACM SIG, P267
   Yamron J.P., 1999, Proceedings of DARPA Broadcast News Workshop, P133
   Yao BZ, 2014, IEEE T PATTERN ANAL, V36, P436, DOI 10.1109/TPAMI.2013.144
   Yun Zhai, 2005, 13th Annual ACM International Conference on Multimedia, P2, DOI 10.1145/1101149.1101152
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
   Zhiwei Li, 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P106
   Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018
NR 66
TC 26
Z9 28
U1 2
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 367
EP 381
DI 10.1109/TMM.2016.2616279
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800012
OA Bronze
DA 2024-07-18
ER

PT J
AU Yan, Y
   Nie, FP
   Li, W
   Gao, CQ
   Yang, Y
   Xu, D
AF Yan, Yan
   Nie, Feiping
   Li, Wen
   Gao, Chenqiang
   Yang, Yi
   Xu, Dong
TI Image Classification by Cross-Media Active Learning With Privileged
   Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active learning; cross-media analysis; image classification; image-text
   joint modeling
AB In this paper, we propose a novel cross-media active learning algorithm to reduce the effort on labeling images for training. The Internet images are often associated with rich textual descriptions. Even though such textual information is not available in test images, it is still useful for learning robust classifiers. In light of this, we apply the recently proposed supervised learning paradigm, learning using privileged information, to the active learning task. Specifically, we train classifiers on both visual features and privileged information, and measure the uncertainty of unlabeled data by exploiting the learned classifiers and slacking function. Then, we propose to select unlabeled samples by jointly measuring the cross-media uncertainty and the visual diversity. Our method automatically learns the optimal tradeoff parameter between the two measurements, which in turn makes our algorithms particularly suitable for real-world applications. Extensive experiments demonstrate the effectiveness of our approach.
C1 [Yan, Yan; Yang, Yi] Univ Technol Sydney, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW 2007, Australia.
   [Nie, Feiping] Northwestern Polytech Univ, Ctr Opt Imagery Anal & Learning, Xian 710072, Peoples R China.
   [Li, Wen] ETH, Comp Vis Lab, CH-8092 Zurich, Switzerland.
   [Gao, Chenqiang] Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
   [Xu, Dong] Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia.
C3 University of Technology Sydney; Northwestern Polytechnical University;
   Swiss Federal Institutes of Technology Domain; ETH Zurich; Chongqing
   University of Posts & Telecommunications; University of Sydney
RP Yan, Y (corresponding author), Univ Technol Sydney, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW 2007, Australia.
EM yan.yan-3@student.uts.edu.au; feipingnie@gmail.com; liwenbnu@gmail.com;
   gaochenqiang@gmail.com; yi.yang@uts.edu.au; dong.xu@sydney.edu.au
RI yang, yang/GWB-9426-2022; Xu, Dong/A-3694-2011; yang,
   yang/HGT-7999-2022; yang, yang/GVT-5210-2022; Lang, Ming/HIK-0758-2022;
   Nie, Feiping/B-3039-2012; Yang, Yi/B-9273-2017; Yan, Yan/L-1864-2018
OI Yang, Yi/0000-0002-0512-880X; Yan, Yan/0000-0001-9108-6767; Nie,
   Feiping/0000-0002-0871-6519
FU Data to Decisions Cooperative Research Centre; ARC DECRA; National
   Natural Science Foundation of China [61571071]
FX This work was supported in part by the Data to Decisions Cooperative
   Research Centre (http://www.d2dcrc.com.au), in part by the ARC DECRA,
   and in part by the National Natural Science Foundation of China under
   Grant 61571071. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Winston Hsu.
CR Aggarwal CC, 2014, DATA CLASSIFICATION
   Balcan MF, 2007, LECT NOTES COMPUT SC, V4539, P35, DOI 10.1007/978-3-540-72927-3_5
   Bertsekas D. P., 1999, Nonlinear Program, V2nd
   Chang XJ, 2017, IEEE T CYBERNETICS, V47, P1180, DOI 10.1109/TCYB.2016.2539546
   Chen L, 2012, IEEE T MULTIMEDIA, V14, P1057, DOI 10.1109/TMM.2012.2187435
   Dasgupta S, 2008, P 25 INT C MACH LEAR, P208, DOI DOI 10.1145/1390156.1390183
   Delbos F, 2005, J CONVEX ANAL, V12, P45
   Duan LX, 2011, IEEE T IMAGE PROCESS, V20, P3280, DOI 10.1109/TIP.2011.2159227
   Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534
   Han YH, 2014, IEEE T MULTIMEDIA, V16, P1115, DOI 10.1109/TMM.2014.2306092
   Huang SJ, 2014, IEEE T PATTERN ANAL, V36, P1936, DOI 10.1109/TPAMI.2014.2307881
   Jain P, 2009, PROC CVPR IEEE, P762, DOI 10.1109/CVPRW.2009.5206651
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krapac J, 2010, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR.2010.5540092
   Li W, 2014, LECT NOTES COMPUT SC, V8693, P437, DOI 10.1007/978-3-319-10602-1_29
   Lindner A, 2015, IEEE T MULTIMEDIA, V17, P700, DOI 10.1109/TMM.2015.2410175
   Mac Aodha O, 2014, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2014.79
   Muslea I, 2006, J ARTIF INTELL RES, V27, P203, DOI 10.1613/jair.2005
   Nguyen Hieu T., 2004, INT C MACHINE LEARNI, P79
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Settles B, 2010, COMPUT SCI TECH REP
   Simonyan K., 2014, CORR
   Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243
   Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   WeiWang Zhi-Hua, 2008, P 25 INT C MACHINE L, P1152
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Wu Y, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P529, DOI 10.1109/ICME.2006.262442
   Xiaofei He, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P119
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
NR 33
TC 125
Z9 129
U1 0
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2494
EP 2502
DI 10.1109/TMM.2016.2602938
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200015
DA 2024-07-18
ER

PT J
AU Liu, YJ
   Ma, CX
   Zhao, GZ
   Fu, XL
   Wang, HG
   Dai, GZ
   Xie, LX
AF Liu, Yong-Jin
   Ma, Cuixia
   Zhao, Guozhen
   Fu, Xiaolan
   Wang, Hongan
   Dai, Guozhong
   Xie, Lexing
TI An Interactive SpiralTape Video Summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE User experience; user interaction; video content analysis; video
   summarization
AB A majority of video summarization systems use linear representations, such as rectangular storyboards and timelines at linear scales. In this paper, we propose a novel nonlinear dynamic representation called SpiralTape that summarizes a video in a smooth spiral pattern. SpiralTape provides an unusual and fresh activity suitable for stimulating environments such as science and technology museums, in which children or young individuals can have enjoyable experiences that create meaningful learning outcomes. In addition, SpiralTape provides an uninterrupted overall structure of video content and takes design principles including compactness, continuity, efficient overview, and interactivity into consideration. A working SpiralTape system was developed and deployed in pilot applications and exhibitions. Elaborate user studies with evaluation benchmarks on multiple metrics were conducted to compare SpiralTape with two representative linear video summarization methods and a state-of-the-art radial video visualization. The evaluation results demonstrate the effectiveness and natural interaction performance of SpiralTape.
C1 [Liu, Yong-Jin; Ma, Cuixia; Wang, Hongan; Dai, Guozhong] Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing 100864, Peoples R China.
   [Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing, Peoples R China.
   [Ma, Cuixia] Chinese Acad Sci, Inst Software, Beijing Key Lab Human Comp Interact, Beijing 100864, Peoples R China.
   [Zhao, Guozhen; Fu, Xiaolan] Chinese Acad Sci, Inst Psychol, State Key Lab Brain & Cognit Sci, Beijing 100864, Peoples R China.
   [Xie, Lexing] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 0200, Australia.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Tsinghua
   University; Chinese Academy of Sciences; Institute of Software, CAS;
   Chinese Academy of Sciences; Institute of Psychology, CAS; Australian
   National University
RP Liu, YJ (corresponding author), Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing 100864, Peoples R China.
EM liuyongjin@tsinghua.edu.cn; cuixia@iscas.ac.cn; zhaogz@psych.ac.cn;
   fuxl@psych.ac.cn; hongan@iscas.ac.cn; guozhong@iscas.ac.cn;
   lexing.xie@anu.edu.au
RI Liu, Yong/GWQ-6163-2022
OI Xie, Lexing/0000-0001-8319-0118; Zhao, Guozhen/0000-0003-4438-5320
FU Royal Society-Newton Advanced Fellowship; Natural Science Foundation of
   China [61322206, 61521002, 61232013, 61661130156, U1435220]; TNList
   Cross-Discipline Foundation
FX This work was supported in part by the Royal Society-Newton Advanced
   Fellowship, the Natural Science Foundation of China under Grant
   61322206, Grant 61521002, Grant 61232013, Grant 61661130156, and Grant
   U1435220, and in part by the TNList Cross-Discipline Foundation. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Zhen Wen. (Yong-Jin Liu and Cuixia
   Ma contributed equally to this work.)
CR [Anonymous], P 1 ACM SIGGRAPH C E
   Barnes C., 2010, P ACM SIGGRAPH
   Carlis J. V., 1998, 11th Annual Symposium on User Interface Software and Technology. UIST. Proceedings of the ACM Symposium, P29, DOI 10.1145/288392.288399
   Correa C. D., 2010, P ACM SIGGRAPH
   Draper GM, 2009, IEEE T VIS COMPUT GR, V15, P759, DOI 10.1109/TVCG.2009.23
   Fraser J, 1908, BRIT J PSYCHOL, V2, P307, DOI 10.1111/j.2044-8295.1908.tb00182.x
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gargi U, 2000, IEEE T CIRC SYST VID, V10, P1, DOI 10.1109/76.825852
   Graells Eduardo, 2012, P 2012 ACM INT C INT, P237, DOI DOI 10.1145/2166966.2167006
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Herranz L, 2012, IEEE T MULTIMEDIA, V14, P1290, DOI 10.1109/TMM.2012.2192917
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Kanade T., 1981, P 7 INT JOINT C ARTI, V1, P674, DOI DOI 10.5555/1623264.1623280
   Liang Y., 2016, IEEE T VIS IN PRESS
   Lin J., 2005, Information Visualization, V4, P61, DOI 10.1057/palgrave.ivs.9500089
   Liu YJ, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2645643
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Liu YJ, 2016, FRONT COMPUT SCI-CHI, V10, P216, DOI 10.1007/s11704-015-4450-1
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu SY, 2014, IEEE T MULTIMEDIA, V16, P1497, DOI 10.1109/TMM.2014.2319778
   Ma CX, 2012, IEEE T MULTIMEDIA, V14, P1153, DOI 10.1109/TMM.2012.2190389
   Mei T, 2009, VISUAL COMPUT, V25, P39, DOI 10.1007/s00371-008-0282-4
   Michael J., 2004, CONCEPTS PRINCIPLES
   Nguyen GP, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1324287.1324294
   Pritch Y, 2008, IEEE T PATTERN ANAL, V30, P1971, DOI 10.1109/TPAMI.2008.29
   Rother C, 2005, PROC CVPR IEEE, P589
   Smith MA, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P61, DOI 10.1109/CAIVD.1998.646034
   Tominski Christian, 2008, P ANN SIGRAD C SPEC, P53
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   Weber M, 2001, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2001, PROCEEDINGS, P7, DOI 10.1109/infvis.2001.963273
   Wu YD, 2015, J VISUAL-JAPAN, V18, P255, DOI 10.1007/s12650-014-0266-6
   Yang J, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P77, DOI 10.1109/INFVIS.2002.1173151
   Yu MJ, 2016, NEUROCOMPUTING, V173, P2041, DOI 10.1016/j.neucom.2015.09.046
NR 33
TC 10
Z9 12
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1269
EP 1282
DI 10.1109/TMM.2016.2557061
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600004
DA 2024-07-18
ER

PT J
AU Wang, JZ
   Wang, WM
   Wang, RG
   Gao, W
AF Wang, Jinzhuo
   Wang, Wenmin
   Wang, Ronggang
   Gao, Wen
TI CSPS: An Adaptive Pooling Method for Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Class-specific pooling shapes (CSPS); dictionary sensitivity; image
   classification; multi-shape matching kernel; representation compression
ID SCENE; REPRESENTATION; FEATURES
AB This paper proposes an adaptive approach to learn class-specific pooling shapes (CSPS) for image classification. Prevalent methods for spatial pooling are often conducted on predefined grids of images, which is an ad-hoc method and, thus, lacks generalization power across different categories. In contrast, our CSPS is designed in a data-driven fashion by generating plenty of candidates and selecting the optimal subset for each class. Specifically, we establish an overcomplete spatial shape set that preserves as many geometric patterns as possible. Then, the class-specific subset is selected by training a linear classifier with structured sparsity constraints and color distribution cues. To address the high computational cost and the risk of overfitting due to the overcomplete scheme, the image representations for CSPS are first compressed according to dictionary sensitivity and shape importance. These representations are finally fed to SVMs for the classification task. We demonstrate that CSPS can learn compact yet discriminative geometric information for different classes that carries more semantic meaning than other methods. Experimental results on four datasets demonstrate the benefits of the proposed method compared with other pooling schemes and illustrate its effectiveness on both object and scene images.
C1 [Wang, Jinzhuo] Peking Univ, Shenzhen Grad Sch, Sch Elect Engn & Comp Sci, Shenzhen 518055, Peoples R China.
   [Wang, Wenmin] Peking Univ, Dept Elect & Comp Engn, Shenzhen 518055, Peoples R China.
   [Wang, Ronggang] Peking Univ, Shenzhen Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
   [Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Peking University; Peking University; Peking University; Peking
   University
RP Wang, WM (corresponding author), Peking Univ, Dept Elect & Comp Engn, Shenzhen 518055, Peoples R China.
EM cr7or9@163.com; wangwm@ece.pku.edu.cn; rgwang@pkusz.edu.cn;
   wgao@pku.edu.cn
RI Wang, Wenmin/W-3511-2019
OI Wang, Wenmin/0000-0003-2664-4413; Wang, Jinzhuo/0000-0002-9464-4426
FU ShenZhen Peacock Plan
FX This work was supported by the ShenZhen Peacock Plan. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Balakrishnan Prabhakaran. (Corresponding author:
   Wenmin Wang.)
CR Altintakan UL, 2015, IEEE T MULTIMEDIA, V17, P323, DOI 10.1109/TMM.2014.2388312
   [Anonymous], 2009, Advances in neural information processing systems
   [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   [Anonymous], 2006, P IEEE C COMPUTER VI
   [Anonymous], P VIS REC CHALL WORK
   [Anonymous], 2011, P CVPR 2011 COL SPRI
   Baeza-Yates R., 1999, MODERN INFORM RETRIE, V463
   Bengio S., 2009, Advances in Neural Information Processing Systems, V22, P82
   Bergamo A, 2012, PROC CVPR IEEE, P3085, DOI 10.1109/CVPR.2012.6248040
   Bo LF, 2013, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2013.91
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Boureau Y. L., 2010, P 27 INT C MACH LEAR, P111
   Boureau YL, 2011, IEEE I CONF COMP VIS, P2651, DOI 10.1109/ICCV.2011.6126555
   BOUREAU YL, 2010, PROC CVPR IEEE, P2559, DOI DOI 10.1109/CVPR.2010.5539963
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J., 2012, Imagenet large scale visual recognition competition 2012 (ILSVRC2012)
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fanello SR, 2014, PROC CVPR IEEE, P851, DOI 10.1109/CVPR.2014.114
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Fulkerson B, 2008, LECT NOTES COMPUT SC, V5302, P179, DOI 10.1007/978-3-540-88682-2_15
   Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169
   Griffin G., 2007, USBCSD041366
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Jenatton R., 2010, Proceedings of the 27th International Conference on International Conference on Machine Learning, P487
   Jia YQ, 2012, PROC CVPR IEEE, P3370, DOI 10.1109/CVPR.2012.6248076
   Koenderink JJ, 1999, INT J COMPUT VISION, V31, P159, DOI 10.1023/A:1008065931878
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni N, 2011, PROC CVPR IEEE, P1609, DOI 10.1109/CVPR.2011.5995701
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lazebnik S, 2009, IEEE T PATTERN ANAL, V31, P1294, DOI 10.1109/TPAMI.2008.138
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li L.-j., 2010, NIPS
   Liu D, 2008, PROC CVPR IEEE, P480
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pandey M, 2011, IEEE I CONF COMP VIS, P1307, DOI 10.1109/ICCV.2011.6126383
   Parizi SN, 2012, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2012.6248001
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Pourian N, 2015, IEEE T MULTIMEDIA, V17, P616, DOI 10.1109/TMM.2015.2410734
   Qi GJ, 2010, IEEE T MULTIMEDIA, V12, P278, DOI 10.1109/TMM.2010.2046270
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Quelhas P, 2005, IEEE I CONF COMP VIS, P883
   Russakovsky O, 2012, LECT NOTES COMPUT SC, V7573, P1, DOI 10.1007/978-3-642-33709-3_1
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Sánchez J, 2012, PATTERN RECOGN LETT, V33, P2216, DOI 10.1016/j.patrec.2012.07.019
   Shao M, 2014, IEEE INT CON MULTI, DOI 10.1109/ICME.2014.6890269
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tishby N., 2000, ARXIVPHYSICS0004057
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang CJ, 2013, PATTERN RECOGN LETT, V34, P1046, DOI 10.1016/j.patrec.2013.02.013
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
NR 60
TC 22
Z9 23
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1000
EP 1010
DI 10.1109/TMM.2016.2544099
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100005
DA 2024-07-18
ER

PT J
AU Wu, O
   Zuo, HQ
   Hu, WM
   Li, B
AF Wu, Ou
   Zuo, Haiqiang
   Hu, Weiming
   Li, Bing
TI Multimodal Web Aesthetics Assessment Based on Structural SVM and
   Multitask Fusion Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetic features; fusion; local features; multitask learning; visual
   aesthetics; web pages
ID BACKGROUND COLOR; QUALITY; IMAGES; TEXT; EXPERIENCE; SEARCH
AB The overall visual attributes (e.g., aesthetics) of Web pages significantly influence user experience. A beautiful and well laid out Web page greatly facilitates user access and enhances the browsing experience. In this paper, a new method is proposed to learn an assessment model for the (visual) aesthetics of Web pages. First, multimodal features (structural, local visual, global visual, and functional) of a Web page that are known to significantly affect the aesthetics of a Web page are extracted to construct a feature vector. Second, the interuser disagreement of aesthetics is analyzed and novel aesthetic representations are obtained from the multiuser ratings of a page. A structural learning algorithm is proposed for the new aesthetic representations. Third, as a Web page's functional purpose also affects the perceived aesthetics, we divide Web pages into different types using functional features, and a soft multitask fusion learning strategy is introduced to train assessment models for pages with functional purposes. Experimental results show the effectiveness of our method: 1) the combination of structural, local, and global visual features outperforms existing state-of-the-art Web aesthetic features; 2) the proposed structural learning algorithm achieves good results for the new aesthetic representations; and 3) the proposed soft multitask fusion learning strategy improves the performances of aesthetics assessment models.
C1 [Wu, Ou; Hu, Weiming; Li, Bing] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Zuo, Haiqiang] China Univ Petr, Sch Engn, Qingdao 266580, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; China
   University of Petroleum
RP Wu, O; Hu, WM; Li, B (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Zuo, HQ (corresponding author), China Univ Petr, Sch Engn, Qingdao 266580, Peoples R China.
EM wuou@nlpr.ia.ac.cn; zhqupc@upc.edu.cn; wmhu@nlpr.ia.ac.cn;
   bli@nlpr.ia.ac.cn
RI Li, Bing/AAX-5919-2021
FU National Science Foundation China [61379098]
FX This work was supported by the National Science Foundation China under
   Grant 61379098. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Martha Larson.
CR [Anonymous], 2008, ICML
   [Anonymous], P SPIE
   [Anonymous], 2007, P 15 ACM INT C MULTI
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2003, VIPS VISION BASED PA
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   BOYLE C, 2001, COLOR HARMONY WEB GU
   Chen GY, 2008, APPLIED COMPUTING 2008, VOLS 1-3, P2353
   Chen J., 2001, P WWW 2001 US, P587
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Costa P., 2008, P 26 ANN ACM INT C D, P1071
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092
   Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109, DOI DOI 10.1145/1014052.1014067
   Hall RH, 2004, BEHAV INFORM TECHNOL, V23, P183, DOI 10.1080/01449290410001669932
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hassenzahl M, 2006, BEHAV INFORM TECHNOL, V25, P91, DOI 10.1080/01449290500330331
   Heer J., 2010, HUMAN FACTORS COMPUT, P203
   Hoffman R., 2004, P SAICSIT, P205
   Ivory M. Y., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P53, DOI 10.1145/365024.365035
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Koch R., 2014, 2014 IEEE VEHICLE PO, P1, DOI DOI 10.1109/VPPC.2014.7007135
   Kohlschtter C., 2008, PROCEEDING 17 ACM C, P1173, DOI DOI 10.1145/1458082.1458237
   Lavie T, 2004, INT J HUM-COMPUT ST, V60, P269, DOI 10.1016/j.ijhcs.2003.09.002
   Ling J, 2002, DISPLAYS, V23, P223, DOI 10.1016/S0141-9382(02)00041-0
   Liu J., 2009, P 25 C UNCERTAINTY A, P339, DOI DOI 10.5555/1795114.1795154
   Liu W, 2010, IEEE T KNOWL DATA EN, V22, P447, DOI 10.1109/TKDE.2009.109
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Michailidou E, 2008, SIGDOC'08: PROCEEDINGS OF THE 26TH ACM INTERNATIONAL CONFERENCE ON DESIGN OF COMMUNICATION, P215
   Mirdehghani M., 2009, P WORLD ACAD SCI ENG, V25, P124
   Moon P, 1944, J OPT SOC AM, V34, P46, DOI 10.1364/JOSA.34.000046
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Ngo DCL, 2003, INFORM SCIENCES, V152, P25, DOI 10.1016/S0020-0255(02)00404-8
   Ni BB, 2013, IEEE T MULTIMEDIA, V15, P1138, DOI 10.1109/TMM.2013.2241042
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   Nishiyama M, 2009, PROC CVPR IEEE, P1115, DOI 10.1109/CVPRW.2009.5206750
   Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490
   Osborne H., 1968, AESTHETICS ART HIST
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pandir M, 2006, INTERACT COMPUT, V18, P1351, DOI 10.1016/j.intcom.2006.03.007
   Park SE, 2004, INTERACT COMPUT, V16, P351, DOI 10.1016/j.intcom.2003.07.001
   Phillips C., 2009, USABILITY NEWS, V11, P1
   Purchase H. C., 2011, P 12 AUSTR US INT C, V117, P19
   Reinecke Katharina, 2013, P SIGCHI C HUM FACT, P2049, DOI [10.1145/2470654.2481281, DOI 10.1145/2470654.2481281, 10]
   Robins D, 2008, INFORM PROCESS MANAG, V44, P386, DOI 10.1016/j.ipm.2007.02.003
   San Pedro Jose., 2009, WWW, P771, DOI DOI 10.1145/1526709.1526813
   Schenkman BN, 2000, BEHAV INFORM TECHNOL, V19, P367, DOI 10.1080/014492900750000063
   Schmidt K. E., 2003, P ANN INT C IND ENG, P478
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Tarasewich P., 2001, Q J ELECT COMMERCE, V2, P67
   Thorlacius L, 2007, NORD REV, V28, P63, DOI 10.1515/nor-2017-0201
   Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77
   Tsochantaridis I., 2004, ICML, P104
   Tuch AN, 2009, INT J HUM-COMPUT ST, V67, P703, DOI 10.1016/j.ijhcs.2009.04.002
   Tullis T S., 1998, CHI 98 conference summary on Human factors in computing systems, P323
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   van Schaik P, 2003, DISPLAYS, V24, P187, DOI 10.1016/j.displa.2004.01.005
   Wu O., 2011, Proc. of WSDM, P337
   Wu O, 2013, ACM T WEB, V7, DOI 10.1145/2435215.2435216
   Wu O, 2011, IEEE I CONF COMP VIS, P225, DOI 10.1109/ICCV.2011.6126246
   Yang JF, 2011, SIAM J SCI COMPUT, V33, P250, DOI 10.1137/090777761
   Yao L, 2012, INT J COMPUT VISION, V96, P353, DOI 10.1007/s11263-011-0478-3
   Ye QX, 2005, IMAGE VISION COMPUT, V23, P565, DOI 10.1016/j.imavis.2005.01.004
   Yeh HH, 2013, IEEE T MULTIMEDIA, V15, P1944, DOI 10.1109/TMM.2013.2280250
   Zeng ZN, 2013, PROC CVPR IEEE, P708, DOI 10.1109/CVPR.2013.97
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zheng XJS, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1
NR 71
TC 14
Z9 15
U1 3
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1062
EP 1076
DI 10.1109/TMM.2016.2538722
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100010
DA 2024-07-18
ER

PT J
AU Roodaki, H
   Iravani, Z
   Hashemi, MR
   Shirmohammadi, S
AF Roodaki, Hoda
   Iravani, Zahra
   Hashemi, Mahmoud Reza
   Shirmohammadi, Shervin
TI A View-Level Rate Distortion Model for Multi-View/3D Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Inter-view disparity; intra-view disparity; multi-view video coding
   (MVC); rate control
ID MULTIVIEW VIDEO
AB Multi-view/3D video is currently available in games, entertainment, education, security, and surveillance applications. Since the amount of data in multi-view/3D increases proportionally with the number of cameras, and due to different bandwidth and playback capabilities of receivers, appropriate compression of multi-view/3D video to produce the correct bitrate while maintaining smooth video quality is crucial, a task that is mostly performed by the rate control module of the encoder. There are many existing rate control algorithms for single-view and multi-view video coding considering the specific features or aspects of these videos. In this paper, we introduce a novel view-level rate distortion (RD) model. We use a systematic methodology to derive this RD model by investigating the impact of multi-view/3D video characteristics on the bitrate of a compressed video. Our proposed RD model considers the concepts of intra-view and inter-view disparity as an effective feature of multi-view/3D video to estimate the overall bitrate of each view more accurately. Evaluation results indicate that our proposed view-level RD model outperforms existing linear models by a factor of 3 and can predict the rate of each view with relatively high precision and a low estimation error of 12% on average.
C1 [Roodaki, Hoda; Iravani, Zahra; Hashemi, Mahmoud Reza] Univ Tehran, Sch Elect & Comp Engn, Multimedia Proc Lab, Tehran 14395515, Iran.
   [Shirmohammadi, Shervin] Univ Tehran, Coll Engn, Sch Elect & Comp Engn, Multimedia Proc Lab, Tehran 14395515, Iran.
   [Shirmohammadi, Shervin] Univ Ottawa, Sch Elect Engn & Comp Sci, Distributed & Collaborat Virtual Environm Res Lab, Ottawa, ON K1N 6N5, Canada.
C3 University of Tehran; University of Tehran; University of Ottawa
RP Roodaki, H (corresponding author), Univ Tehran, Sch Elect & Comp Engn, Multimedia Proc Lab, Tehran 14395515, Iran.
EM h.roodaki@alumni.ut.ac.ir; z.iravani@ut.ac.ir; rhashemi@ut.ac.ir;
   sshirmohammadi@ut.ac.ir
RI Roodaki, Hoda/N-6891-2019; Shirmohammadi, Shervin/E-6945-2012; Hashemi,
   Mahmoud Reza/H-2172-2011
OI Shirmohammadi, Shervin/0000-0002-3973-4445; Hashemi, Mahmoud
   Reza/0000-0002-3518-9195
CR [Anonymous], 2008, MPEG2008W9974 ISOIEC
   [Anonymous], 2014, P INT C SIGN PROC CO
   [Anonymous], 2013, JCT3VC1001
   Cordina M., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P878, DOI 10.1109/ICME.2012.5
   Hu J, 2009, INT WORK QUAL MULTIM, P216, DOI 10.1109/QOMEX.2009.5246950
   Iravani Z, 2014, 2014 7TH INTERNATIONAL SYMPOSIUM ON TELECOMMUNICATIONS (IST), P301, DOI 10.1109/ISTEL.2014.7000718
   Lili Zhou, 2012, 2012 Eighth International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP), P246, DOI 10.1109/IIH-MSP.2012.66
   Liu F, 2011, CHINA COMMUN, V8, P83
   Liu YW, 2011, IEEE T BROADCAST, V57, P562, DOI 10.1109/TBC.2011.2105652
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Pei-Jun Lee, 2013, Journal of Display Technology, V9, P552, DOI 10.1109/JDT.2012.2237382
   Pei-Jun Lee, 2011, Proceedings of the 2011 International Conference on System Science and Engineering (ICSSE), P342, DOI 10.1109/ICSSE.2011.5961925
   Rezaei M, 2005, IEEE WRK SIG PRO SYS, P550, DOI 10.1109/SIPS.2005.1579928
   Roodaki H., 2013, P IEEE INT C MULT EX, P1
   Roodaki H, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2348816.2348823
   Seanae Park, 2009, 2009 IEEE 13th International Symposium on Consumer Electronics (ISCE), P115, DOI 10.1109/ISCE.2009.5157039
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Song HJ, 2004, IEEE T MULTIMEDIA, V6, P489, DOI 10.1109/TMM.2004.827488
   Tao Y, 2009, P SOPO, P1
   Tao Yan, 2011, 2011 4th International Congress on Image and Signal Processing (CISP 2011), P14, DOI 10.1109/CISP.2011.6100001
   Tao Yan, 2010, 2010 International Computer Symposium (ICS 2010), P34, DOI 10.1109/COMPSYM.2010.5685488
   Vizzotto B. B., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P485, DOI 10.1109/ICME.2012.69
   Vizzotto BB, 2013, IEEE T CIRC SYST VID, V23, P2090, DOI 10.1109/TCSVT.2013.2270400
   Yan T, 2009, WORLD SUMMIT ON GENETIC AND EVOLUTIONARY COMPUTATION (GEC 09), P1025
   Yi Liao, 2013, Journal of Theoretical and Applied Information Technology, V49, P959
   Yo-Sung Ho, 2007, 2007 14th International Workshop in Systems, Signals and Image Processing and 6th EURASIP Conference focused on Speech and Image Processing, Multimedia Communications and Services - EC-SIPMCS 2007, P5
   Zheng Q., 2012, LECT NOTES ELECT ENG, V107, P1089
NR 27
TC 9
Z9 10
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2016
VL 18
IS 1
BP 14
EP 24
DI 10.1109/TMM.2015.2500036
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CZ5JW
UT WOS:000367139700003
DA 2024-07-18
ER

PT J
AU Lu, YJ
   Yang, LJ
   Yang, KY
   Rui, Y
AF Lu, Yi-Jie
   Yang, Linjun
   Yang, Kuiyuan
   Rui, Yong
TI Mining Latent Attributes From Click-Through Logs for Image Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attribute; click-through log; matrix factorization; topic modeling
ID OBJECT CLASSES
AB Attribute-based image representation, which represents an image by projecting it into a space spanned by attributes, has attracted increasing attention from both computer vision and multimedia communities for its compactness and potential to bridge the semantic gap. While many works focus on learning attribute models and utilizing them in image recognition and retrieval, few touch on the problem of how to effectively construct a vocabulary of attributes, which is an essential part of effective attribute-based representation. Most existing approaches define the attribute vocabulary by human experts or through existing ontology, which is often limited in coverage of general concept space. In this paper, we propose automatically constructing the attribute vocabulary by mining latent topics from the click-through log of a commercial image search engine. These attributes are referred to as latent topic attributes (LTA), which take advantage of tens of millions of interactions between user submitted queries and images, thereby providing better coverage for the concept space than existing approaches. The mining of latent topics from the click log is formulated as a matrix factorization problem, and further improved by weighted terms-based matrix factorization to address the extreme sparsity of the click-through matrix. Both qualitative results of the mined LTA and quantitative results on the standard image recognition benchmark demonstrate the mined LTA's effectiveness.
C1 [Lu, Yi-Jie] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Yang, Linjun] Microsoft, Bellevue, WA 98009 USA.
   [Yang, Kuiyuan; Rui, Yong] Microsoft Res, Beijing 100080, Peoples R China.
C3 Microsoft; Microsoft Research Asia; Microsoft; Microsoft
RP Lu, YJ (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM iiedii@gmail.com; linjuny@microsoft.com; kuyang@microsoft.com;
   yongrui@microsoft.com
CR [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2007.383198
   [Anonymous], 2010, P NIPS
   [Anonymous], 2014, CORR
   [Anonymous], 2013, MSR BING IM RETR CHA
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Bergamo Alessandro., 2011, NIPS, P2088
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Farhadi A, 2010, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2010.5539924
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Ferrari V., 2007, P 20 INT C NEUR INF, P433
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fu YW, 2014, IEEE T PATTERN ANAL, V36, P303, DOI 10.1109/TPAMI.2013.128
   Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Mahajan D, 2011, IEEE I CONF COMP VIS, P1227, DOI 10.1109/ICCV.2011.6126373
   Mensink T, 2012, LECT NOTES COMPUT SC, V7573, P488, DOI 10.1007/978-3-642-33709-3_35
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Parikh D, 2011, PROC CVPR IEEE, P1681, DOI 10.1109/CVPR.2011.5995451
   Paterek A., 2007, P KDD CUP WORKSH, V2007, P5, DOI [DOI 10.1145/1557019.1557072, 10.1145/1557019.1557072]
   Rastegari M, 2012, LECT NOTES COMPUT SC, V7577, P876, DOI 10.1007/978-3-642-33783-3_63
   Rohrbach M, 2010, PROC CVPR IEEE, P910, DOI 10.1109/CVPR.2010.5540121
   Sarwar B, 2000, P KDD WORKSH WEB MIN
   Su Y, 2012, LECT NOTES COMPUT SC, V7585, P51, DOI 10.1007/978-3-642-33885-4_6
   Su Y, 2011, IEEE I CONF COMP VIS, P311, DOI 10.1109/ICCV.2011.6126257
   Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56
   Wang Y, 2010, LECT NOTES COMPUT SC, V6315, P155, DOI 10.1007/978-3-642-15555-0_12
   Yu FLX, 2013, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2013.105
   Yu XD, 2010, LECT NOTES COMPUT SC, V6315, P127
NR 37
TC 7
Z9 7
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1213
EP 1224
DI 10.1109/TMM.2015.2438712
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000008
DA 2024-07-18
ER

PT J
AU Zhou, Z
   Chen, K
   Zhang, JC
AF Zhou, Zhong
   Chen, Ke
   Zhang, Jingchang
TI Efficient 3-D Scene Prefetching From Learning User Access Patterns
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D scenes; networked virtual environment; prefetching; user access
   patterns
ID WALKTHROUGH; SIMILARITY; MANAGEMENT
AB Rendering large-scale 3-D scenes on a thin client is attracting increasing attention with the development of the mobile Internet. Efficient scene prefetching to provide timely data with a limited cache is one of the most critical issues for remote 3-D data scheduling in networked virtual environment applications. Existing prefetching schemes predict the future positions of each individual user based on user traces. In this paper, we investigate scene content sequences accessed by various users instead of user viewpoint traces and propose a user access pattern-based 3-D scene prefetching scheme. We make a relationship graph-based clustering to partition history user access sequences into several clusters and choose representative sequences from among these clusters as user access patterns. Then, these user access patterns are prioritized by their popularity and users' personal preference. Based on these access patterns, the proposed prefetching scheme predicts the scene contents that will most likely be visited in the future and delivers them to the client in advance. The experiment results demonstrate that our user access pattern-based prefetching approach achieves a high hit ratio and outperforms the prevailing prefetching schemes in terms of access latency and cache capacity.
C1 [Zhou, Zhong; Chen, Ke; Zhang, Jingchang] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University
RP Zhou, Z (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM zz@buaa.edu.cn; chenke19850113@gmail.com; jczhang@buaa.edu.cn
FU National 863 Program of China [2015AA016403]; Natural Science Foundation
   of China [61170188]
FX This work was supported by the National 863 Program of China under Grant
   2015AA016403 and by the Natural Science Foundation of China under Grant
   61170188. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Yonggang Wen.
CR Aljaafreh M, 2013, 2013 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P2393
   [Anonymous], P ACM INT 3D GRAPH
   [Anonymous], ACM CCS 10
   [Anonymous], 12781 IEEE
   [Anonymous], INT J COMPUT SCI ISS
   [Anonymous], P EUR
   Awad MA, 2012, IEEE T SYST MAN CY B, V42, P1131, DOI 10.1109/TSMCB.2012.2187441
   Bittner J, 2004, COMPUT GRAPH FORUM, V23, P615, DOI 10.1111/j.1467-8659.2004.00793.x
   Borges J, 2007, IEEE T KNOWL DATA EN, V19, P441, DOI 10.1109/TKDE.2007.1012
   CALVIN J, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P450, DOI 10.1109/VRAIS.1993.380745
   CARLSSON C, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P394, DOI 10.1109/VRAIS.1993.380753
   Cartharius K, 2005, BIOINFORMATICS, V21, P2933, DOI 10.1093/bioinformatics/bti473
   Chan A., 2005, ACM Transactions on Internet Technology, V5, P70, DOI 10.1145/1052934.1052937
   Chen J, 2013, PERS UBIQUIT COMPUT, V17, P1671, DOI 10.1007/s00779-012-0601-7
   Chim J, 2003, IEEE T MULTIMEDIA, V5, P503, DOI 10.1109/TMM.2003.819094
   Chim J. H. P., 1998, Proceedings ACM Multimedia 98, P171, DOI 10.1145/290747.290769
   Chim JHP, 1998, P IEEE INT FORUM RES, P66, DOI 10.1109/ADL.1998.670381
   Cohen-Or D, 2003, IEEE T VIS COMPUT GR, V9, P412, DOI 10.1109/TVCG.2003.1207447
   EAGLEN RH, 1985, AM J PHYS ANTHROPOL, V66, P307, DOI 10.1002/ajpa.1330660308
   Govindaraju N.K., 2003, P 2003 S INTERACTIVE, P103
   Guthe M., 2006, RENDERING TECHNIQUES, P207
   Hu SY, 2008, IEEE INFOCOM SER, P2047
   Hu SY, 2010, IEEE INTERNET COMPUT, V14, P54, DOI 10.1109/MIC.2009.98
   Hung SS, 2006, COMPUT ANIMAT VIRT W, V17, P469, DOI 10.1002/cav.149
   Iwata Tomoharu., 2007, Proceedings of the 16th international conference on World Wide Web, P1281
   Jiang S, 2014, VISUAL COMPUT, V30, P637, DOI 10.1007/s00371-014-0949-y
   Kucharavy D, 2007, TRIZ FUT C 2007 CURR, P81
   Leigh J, 1996, P IEEE VIRT REAL ANN, P253, DOI 10.1109/VRAIS.1996.490535
   Levenshtein V. I., 1966, SOV PHYS DOKL, V10, P707
   Li F.W., 2004, P ACM S VIRT REAL SO, P129
   Li FWB, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000493
   Li TY, 2004, VISUAL COMPUT, V20, P624, DOI 10.1007/s00371-004-0264-0
   Mattausch O, 2008, COMPUT GRAPH FORUM, V27, P221, DOI 10.1111/j.1467-8659.2008.01119.x
   Newman MEJ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066133
   Ng CM, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P100
   Park Sungju., 2001, VRST 01, P121
   Pazos F, 2001, PROTEIN ENG, V14, P609, DOI 10.1093/protein/14.9.609
   Rahimi H, 2011, IEEE INT CON MULTI
   Schmalstieg D, 1996, COMPUT GRAPH FORUM, V15, pC421, DOI 10.1111/1467-8659.1530421
   Song CM, 2010, SCIENCE, V327, P1018, DOI 10.1126/science.1177170
   STROHAL R, 1989, IMMUNOGENETICS, V30, P475, DOI 10.1007/BF02421180
   Teler E, 2001, COMPUT GRAPH FORUM, V20, pC17, DOI 10.1111/1467-8659.00494
   Varadhan G, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P69, DOI 10.1109/VISUAL.2002.1183759
   Wonka P, 2001, COMPUT GRAPH FORUM, V20, pC411, DOI 10.1111/1467-8659.00534
   Zheng Z, 2008, IEEE T VIS COMPUT GR, V14, P576, DOI 10.1109/TVCG.2007.70626
NR 45
TC 4
Z9 5
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 1081
EP 1095
DI 10.1109/TMM.2015.2430817
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300014
OA Bronze
DA 2024-07-18
ER

PT J
AU Geng, J
   Miao, ZJ
   Zhang, XP
AF Geng, Jie
   Miao, Zhenjiang
   Zhang, Xiao-Ping
TI Efficient Heuristic Methods for Multimodal Fusion and Concept Fusion in
   Video Concept Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Concept fusion; domain adaption; multimodal fusion; video concept
   indexing
ID SEMANTIC DIFFUSION; CONTEXT; ONTOLOGY; IMAGE
AB Semantic models are widely used to bridge the semantic gap between low-level features and high-level features in video concept indexing. Multimodal fusion and concept fusion are two commonly used approaches in building semantic models. In the previous work, domain adaptation is neglected in multimodal fusion, and many probability maximization based and unsupervised concept fusion methods are counterintuitive since they do not incorporate subjective human intuition. In this paper, we present a new two-stage semantic model combining the multimodal fusion and the concept fusion incorporating human heuristics. In the multimodal fusion model, we employ a new generic unsupervised method, namely, domain adaptive linear combination (DALC), to update the linear combination (LC) weights by incorporating the differences of element distributions between training and testing domains. In the concept fusion model, a novel mechanical node equilibrium (NE) model is developed by using forces to model the concept correlations to update the score of concepts represented by nodes. It is intuitive and can incorporate multiple kinds of correlations simultaneously to construct more sophisticated semantic structure. Compared to other state-of-the-art supervised and unsupervised methods, the new model can use either unsupervised or supervised factors to significantly improve the mean inferred average precision (MAP) performance on all datasets.
C1 [Geng, Jie; Miao, Zhenjiang] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Zhang, Xiao-Ping] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 Beijing Jiaotong University; Toronto Metropolitan University
RP Geng, J (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM 08112073@bjtu.edu.cn; zjmiao@bjtu.edu.cn; xzhang@ee.ryerson.ca
RI Zhang, Xiao-Ping (Steven)/B-1436-2016
OI Zhang, Xiao-Ping (Steven)/0000-0001-5241-0069
FU NSFC [6127327461370127]; 973 Program [2011CB302203]; National Key
   Technology R&D Program of China [2012BAH01F03, NSFB4123104, FRFCU
   2014JBZ004, Z131110001913143]; Tsinghua-Tencent Joint Laboratory for
   IIT; Natural Sciences and Engineering Research Council of Canada (NSERC)
   [RGPIN239031]
FX This work was supported by the NSFC under Grant 6127327461370127, the
   973 Program under Grant 2011CB302203, the National Key Technology R&D
   Program of China under Grant 2012BAH01F03, Grant NSFB4123104, Grant
   FRFCU 2014JBZ004, and Grant Z131110001913143, the Tsinghua-Tencent Joint
   Laboratory for IIT, and the Natural Sciences and Engineering Research
   Council of Canada (NSERC) under Grant RGPIN239031. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. K. Selcuk Candan.
CR [Anonymous], 2009, Proc. ACM International Confence on Multimedia
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], ACM MULTIMEDIA
   [Anonymous], 22220068 ADVENT COL
   [Anonymous], P INT WORKSH MULT IN
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Bertini M., 2005, 13th Annual ACM International Conference on Multimedia, P395, DOI 10.1145/1101149.1101235
   Chang S.-F., 2007, P INT WORKSHOP WORKS, P255, DOI DOI 10.1145/1290082.1290118
   Duan LX, 2009, PROC CVPR IEEE, P1375, DOI [10.1109/CVPRW.2009.5206747, 10.1109/CVPR.2009.5206747]
   Fan J, 2007, IEEE T MULTIMEDIA, V9, P939, DOI 10.1109/TMM.2007.900143
   Fox E. A., 1994, Second Text REtrieval Conference (TREC-2) (NIST-SP 500-215), P243
   GOLUB GH, 1979, IEEE T AUTOMAT CONTR, V24, P909, DOI 10.1109/TAC.1979.1102170
   Gu ZW, 2008, IEEE T MULTIMEDIA, V10, P1605, DOI 10.1109/TMM.2008.2007290
   Hoffman J, 2012, LECT NOTES COMPUT SC, V7573, P702, DOI 10.1007/978-3-642-33709-3_50
   Hollink L., 2005, 13th Annual ACM International Conference on Multimedia, P479, DOI 10.1145/1101149.1101256
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Iyengar G, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P329
   Iyengar G., 2003, P 11 ACM INT C MULTI, P255
   Jain A, 2005, PATTERN RECOGN, V38, P2270, DOI 10.1016/j.patcog.2005.01.012
   Jiang W, 2007, INT CONF ACOUST SPEE, P949
   Jiang Y.-G., 2008, 22320081 ADVENT COL
   Jiang YG, 2012, IEEE T IMAGE PROCESS, V21, P3080, DOI 10.1109/TIP.2012.2188038
   Jiang YG, 2009, IEEE I CONF COMP VIS, P1420, DOI 10.1109/ICCV.2009.5459295
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Kennedy L.S., 2007, Proceedings of the 6th ACM international conference on Image and video retrieval, P333
   Liu KH, 2008, IEEE T MULTIMEDIA, V10, P240, DOI 10.1109/TMM.2007.911826
   Lu ZW, 2011, IEEE T IMAGE PROCESS, V20, P1739, DOI 10.1109/TIP.2010.2103082
   Maedche A, 2001, IEEE INTELL SYST APP, V16, P72, DOI 10.1109/5254.920602
   Mylonas P, 2009, IEEE T MULTIMEDIA, V11, P229, DOI 10.1109/TMM.2008.2009681
   Naphade MR, 2002, IEEE T CIRC SYST VID, V12, P40
   Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Russell S., 2010, ARTIF INTELL, V3rd
   SCOTT DW, 1979, BIOMETRIKA, V66, P605, DOI 10.1093/biomet/66.3.605
   Smeaton AF, 2009, SIGNALS COMMUN TECHN, P151, DOI 10.1007/978-0-387-76569-3_6
   Smith JF, 2003, J PHASE EQUILIB, V24, P2, DOI 10.1361/105497103770330947
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Ulges A, 2010, COMPUT VIS IMAGE UND, V114, P429, DOI 10.1016/j.cviu.2009.08.002
   Wei XY, 2009, P ACM INT C IM VID R, P15
   Weng M.F., 2008, PROC ACM INT C MULTI, P71, DOI DOI 10.1145/1459359.1459370
   Weng MF, 2012, IEEE T PATTERN ANAL, V34, P1927, DOI 10.1109/TPAMI.2011.273
   Weng MF, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2168996.2169003
   Xu D, 2008, IEEE T PATTERN ANAL, V30, P1985, DOI 10.1109/TPAMI.2008.129
   Yan R., 2004, PROC ACM INT C MULTI, P548
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yang YH, 2009, IEEE T CIRC SYST VID, V19, P1880, DOI 10.1109/TCSVT.2009.2026978
   Yilmaz Emine, 2006, Proceedings of the 2006 ACM CIKM International Conference on Information and Knowledge Management, Arlington, Virginia, USA, November 6-11, 2006, P102, DOI [10.1145/1183614.1183633 (cit. on p. 34, DOI 10.1145/1183614.1183633(CIT.ONP.34]
NR 49
TC 16
Z9 16
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2015
VL 17
IS 4
BP 498
EP 511
DI 10.1109/TMM.2015.2398195
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QH
UT WOS:000351586300004
DA 2024-07-18
ER

PT J
AU Liu, Z
   Li, HQ
   Zhou, WG
   Hong, RC
   Tian, Q
AF Liu, Zhen
   Li, Houqiang
   Zhou, Wengang
   Hong, Richang
   Tian, Qi
TI Uniting Keypoints: Local Visual Information Fusion for Large-Scale Image
   Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Descriptor encoding; image search; local features; united keypoints
ID LEVEL SET METHOD; CONSISTENCY; RETRIEVAL
AB In this paper, we propose a novel approach to address the problem of the huge amount of local features for a large-scale database. First, in each image the local features are organized into dozens of groups by performing the standard - means clustering algorithm on their spatial positions. Second, a compact descriptor is generated to describe the visual information of each group of local features. Since, in each image, thousands of local features are reorganized into only dozens of groups and each group is described by a single descriptor, the total amount of descriptors in a large-scale database will be greatly reduced. Therefore, we can reduce the complexity of the searching procedure significantly. Further, the generated group descriptors are encoded into binary format to achieve the storage and computation efficiency. The experiments on two benchmark datasets, i. e., UKBench and Holidays, with the Flickr1M distractor database demonstrate the effectiveness of the proposed approach.
C1 [Liu, Zhen; Li, Houqiang; Zhou, Wengang] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Hong, Richang] Hefei Univ Technol, Dept Comp Sci, Hefei 230009, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Hefei University of Technology; University of Texas System;
   University of Texas at San Antonio (UTSA)
RP Liu, Z (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM liuzheng@mail.ustc.edu.cn; lihq@ustc.edu.cn; zhwg@ustc.edu.cn;
   hongrc.hfut@gmail.com; qi.tian@utsa.edu
RI Li, Houqiang Li/B-6259-2013
FU NSFC [61429201, 61325009, 61390514, 61472378, 61472116]; 973 Program
   [2015CB351803]; Fundamental Research Funds for the Central Universities
   [WK2100060014, WK2100060011]; Program for New Century Excellent Talents
   in University [NCET-13-0764]; ARO [W911NF-12-1-0057]; NEC Laboratories
   of America under the Faculty Research Awards
FX This work was supported in part by the NSFC under Contract 61429201. The
   work of H. Li was supported in part by the 973 Program under Contract
   2015CB351803 and by the NSFC under Contract 61325009 and Contract
   61390514. The work of W. Zhou was supported in part by the NSFC under
   Contract 61472378 and by the Fundamental Research Funds for the Central
   Universities under Contract WK2100060014 and Contract WK2100060011. The
   work of R. Hong was supported in part by the Program for New Century
   Excellent Talents in University under Contract NCET-13-0764 and by the
   NSFC under Contract 61472116. The work of Q. Tan was supported in part
   by the ARO under Grant W911NF-12-1-0057 and by the NEC Laboratories of
   America under the Faculty Research Awards. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Jing-Ming Guo.
CR [Anonymous], 2012, P 20 ACM MULTIMEDIA
   [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], 2013, P INT ACM C MULTIMED, DOI DOI 10.1145/2502081.2502093
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Avrithis Y, 2014, INT J COMPUT VISION, V107, P1, DOI 10.1007/s11263-013-0659-3
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Chu LY, 2013, IEEE T MULTIMEDIA, V15, P1982, DOI 10.1109/TMM.2013.2270455
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Gavves Efstratios., 2010, Proceedings of the International Conference on Multimedia, MM '10, P1123
   Guo J.-M., IEEE T CIRC IN PRESS
   Guo JM, 2015, IEEE T IMAGE PROCESS, V24, P1010, DOI 10.1109/TIP.2014.2372619
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   Hong RC, 2014, IEEE T CYBERNETICS, V44, P669, DOI 10.1109/TCYB.2013.2265601
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li X., 2012, P ACM INT C MULT RET, P41
   Liu Z., 2012, P ACM INT C MULTIMED, P199
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P2047, DOI 10.1109/TIP.2014.2312283
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P1606, DOI 10.1109/TIP.2014.2305072
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9
   Nister David, 2006, CVPR
   Philbin J., 2008, P CVPR, P1
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P858, DOI 10.1109/TMM.2012.2187181
   Wang S, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P587
   Xie LX, 2014, COMPUT VIS IMAGE UND, V124, P31, DOI 10.1016/j.cviu.2013.12.011
   Xie LX, 2014, IEEE T IMAGE PROCESS, V23, P1994, DOI 10.1109/TIP.2014.2310117
   Yang X, 2015, IEEE T IMAGE PROCESS, V24, P9, DOI 10.1109/TIP.2014.2372615
   Yang X, 2014, IEEE T IMAGE PROCESS, V23, P2854, DOI 10.1109/TIP.2014.2321506
   Zhang LN, 2013, CANCER GENE THER, V20, P1, DOI 10.1038/cgt.2012.84
   Zhang SL, 2013, IEEE I CONF COMP VIS, P1673, DOI 10.1109/ICCV.2013.210
   Zhang SL, 2013, IEEE T IMAGE PROCESS, V22, P2889, DOI 10.1109/TIP.2013.2251650
   Zhang X, 2012, PROC CVPR IEEE, P2058, DOI 10.1109/CVPR.2012.6247910
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zheng L, 2013, PROC CVPR IEEE, P1626, DOI 10.1109/CVPR.2013.213
   Zheng L, 2013, IEEE SIGNAL PROC LET, V20, P391, DOI 10.1109/LSP.2013.2249513
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
   Zhou WG, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422960
NR 46
TC 24
Z9 25
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2015
VL 17
IS 4
BP 538
EP 548
DI 10.1109/TMM.2015.2399851
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QH
UT WOS:000351586300007
DA 2024-07-18
ER

PT J
AU Qin, XM
   Shen, JB
   Mao, XY
   Li, XL
   Jia, YD
AF Qin, Xiameng
   Shen, Jianbing
   Mao, Xiaoyang
   Li, Xuelong
   Jia, Yunde
TI Structured-Patch Optimization for Dense Correspondence
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dense correspondence; features; match; optimization; structured patch
ID OPTICAL-FLOW; GRAPH; STEREO
AB This paper presents a new method to compute the dense correspondences between two images by using the energy optimization and the structured patches. In terms of the property of the sparse feature and the principle that nearest sub-scenes and neighbors are much more similar, we design a new energy optimization to guide the dense matching process and find the reliable correspondences. The sparse features are also employed to design a new structure to describe the patches. Both transformation and deformation with the structured patches are considered and incorporated into an energy optimization framework. Thus, our algorithm can match the objects robustly in complicated scenes. Finally, a local refinement technique is proposed to solve the perturbation of the matched patches. Experimental results demonstrate that our method outperforms the state-of-the-art matching algorithms.
C1 [Qin, Xiameng; Shen, Jianbing; Jia, Yunde] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Mao, Xiaoyang] Univ Yamanashi, Interdisciplinary Grad Sch Med & Engn, Kofu, Yamanashi 4008510, Japan.
   [Li, Xuelong] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Ctr Opt Imagery Anal & Learning, State Key Lab Transient Opt & Photon, Xian 710119, Peoples R China.
C3 Beijing Institute of Technology; University of Yamanashi; State Key
   Laboratory of Transient Optics & Photonics; Chinese Academy of Sciences;
   Xi'an Institute of Optics & Precision Mechanics, CAS
RP Shen, JB (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM shenjianbing@bit.edu.cn
RI Li, Xuelong/ABF-3381-2020; Li, Xuelong/Z-3785-2019; Shen,
   Jianbing/U-8796-2019; li, xiang/GWM-6319-2022
OI mao, xiaoyang/0000-0001-9531-3197; Li, Xuelong/0000-0002-0019-4197;
   Shen, Jianbing/0000-0002-4109-8353
FU National Basic Research Program of China (973 Program) [2013CB328805];
   National Natural Science Foundation of China [61272359, 61125106]; JSPS
   KAKENHI [25540045, 26240015, 26560006, 25280037]; Program for New
   Century Excellent Talents in University [NCET-11-0789]; Key Research
   Program of the Chinese Academy of Sciences [KGZD-EW-T03]; Specialized
   Fund for Joint Building Program of Beijing Municipal Education
   Commission; Grants-in-Aid for Scientific Research [25540045, 26560006,
   25280037, 26240015] Funding Source: KAKEN
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2013CB328805, the National Natural
   Science Foundation of China under Grant 61272359 and Grant 61125106,
   JSPS KAKENHI under Grant 25540045, Grant 26240015, Grant 26560006, and
   Grant 25280037, the Program for New Century Excellent Talents in
   University under Grant NCET-11-0789, the Key Research Program of the
   Chinese Academy of Sciences under Grant KGZD-EW-T03, and the Specialized
   Fund for Joint Building Program of Beijing Municipal Education
   Commission. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Ebroul Izquierdo.
   (Corresponding author: Jianbing Shen.)
CR Afonso MV, 2014, IEEE T MULTIMEDIA, V16, P1, DOI 10.1109/TMM.2013.2281023
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2007.383198
   [Anonymous], 2004, Workshop on Generative Model Based Vision
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Bagon S, 2008, LECT NOTES COMPUT SC, V5305, P30, DOI 10.1007/978-3-540-88693-8_3
   Barnes C., 2010, LECT NOTES COMPUT SC, V6313, P29
   Battiato S, 2010, IEEE T MULTIMEDIA, V12, P622, DOI 10.1109/TMM.2010.2060474
   Besse F, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.132
   Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14
   Boiman O, 2007, INT J COMPUT VISION, V74, P17, DOI 10.1007/s11263-006-0009-9
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43
   Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28
   Chen ZY, 2013, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2013.316
   Cho MS, 2013, IEEE I CONF COMP VIS, P25, DOI 10.1109/ICCV.2013.11
   Duchenne O, 2011, IEEE I CONF COMP VIS, P1792, DOI 10.1109/ICCV.2011.6126445
   Fusiello A, 1997, PROC CVPR IEEE, P858, DOI 10.1109/CVPR.1997.609428
   HaCohen Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964965
   Heise P, 2013, IEEE I CONF COMP VIS, P2360, DOI 10.1109/ICCV.2013.293
   Heo YS, 2011, IEEE T PATTERN ANAL, V33, P807, DOI 10.1109/TPAMI.2010.136
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hu J, 2012, LECT NOTES COMPUT SC, V7572, P499, DOI 10.1007/978-3-642-33718-5_36
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jianbing Shen, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3481, DOI 10.1109/CVPR.2011.5995507
   Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299
   Kim J, 2010, PROC CVPR IEEE, P2344, DOI 10.1109/CVPR.2010.5539923
   Korman S, 2013, PROC CVPR IEEE, P2331, DOI 10.1109/CVPR.2013.302
   Korman S, 2011, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2011.6126421
   Lee J, 2011, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2011.5995387
   Lee SY, 2013, IEEE T MULTIMEDIA, V15, P1719, DOI 10.1109/TMM.2013.2271747
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Olonetsky I, 2012, LECT NOTES COMPUT SC, V7575, P602, DOI 10.1007/978-3-642-33765-9_43
   Pece F., 2010, Proceedings 2010 Conference on Visual Media Production (CVMP 2010). 7th European Conference on Visual Media Production, P1, DOI 10.1109/CVMP.2010.8
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Shao L, 2014, IEEE T NEUR NET LEAR, V25, P1359, DOI 10.1109/TNNLS.2013.2293418
   Shao L, 2014, IEEE T CYBERNETICS, V44, P817, DOI 10.1109/TCYB.2013.2273174
   Shen JB, 2014, IEEE T CYBERNETICS, V44, P1579, DOI 10.1109/TCYB.2013.2290435
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Shen JB, 2013, IEEE T CYBERNETICS, V43, P1453, DOI 10.1109/TCYB.2013.2273270
   Shen JB, 2012, VISUAL COMPUT, V28, P463, DOI 10.1007/s00371-011-0642-3
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Simon I., 2007, Proc. CVPR, P1
   Tang J, 2014, PATTERN RECOGN, V47, P1469, DOI 10.1016/j.patcog.2013.09.017
   Tombari F., 2008, Computer Vision and Pattern Recognition, P1
   Xu L, 2012, IEEE T PATTERN ANAL, V34, P1744, DOI 10.1109/TPAMI.2011.236
   Zhang W, 2010, PROC CVPR IEEE, P530, DOI 10.1109/CVPR.2010.5540168
   Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667
NR 53
TC 7
Z9 8
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 295
EP 306
DI 10.1109/TMM.2015.2395078
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700003
DA 2024-07-18
ER

PT J
AU Bian, JW
   Yang, Y
   Zhang, HW
   Chua, TS
AF Bian, Jingwen
   Yang, Yang
   Zhang, Hanwang
   Chua, Tat-Seng
TI Multimedia Summarization for Social Events in Microblog Stream
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Microblog; multimedia summarization; social event
ID SEARCH
AB Microblogging services have revolutionized the way people exchange information. Confronted with the ever-increasing numbers of social events and the corresponding microblogs with multimedia contents, it is desirable to provide visualized summaries to help users to quickly grasp the essence of these social events for better understanding. While existing approaches mostly focus only on text-based summary, microblog summarization with multiple media types (e.g., text, image, and video) is scarcely explored. In this paper, we propose a multimedia social event summarization framework to automatically generate visualized summaries from the microblog stream of multiple media types. Specifically, the proposed framework comprises three stages, as follows. 1) A noise removal approach is first devised to eliminate potentially noisy images. An effective spectral filtering model is exploited to estimate the probability that an image is relevant to a given event. 2) A novel cross-media probabilistic model, termed Cross-Media-LDA (CMLDA), is proposed to jointly discover subevents from microblogs of multiple media types. The intrinsic correlations among these different media types are well explored and exploited for reinforcing the cross-media subevent discovery process. 3) Finally, based on the cross-media knowledge of all the discovered subevents, a multimedia microblog summary generation process is designed to jointly identify both representative textual and visual samples, which are further aggregated to form a holistic visualized summary. We conduct extensive experiments on two real-world microblog datasets to demonstrate the superiority of the proposed framework as compared to the state-of-the-art approaches.
C1 [Bian, Jingwen; Zhang, Hanwang; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Yang, Yang] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610051, Peoples R China.
C3 National University of Singapore; University of Electronic Science &
   Technology of China
RP Bian, JW (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
EM bian_jingwen@nus.edu.sg; dlyyang@gmail.com; hanwang@nus.edu.sg;
   dcscts@nus.edu.sg
RI yang, yang/GVT-5210-2022; Lang, Ming/HIK-0758-2022; yang,
   yang/HGT-7999-2022
OI Zhang, Hanwang/0000-0001-7374-8739
CR [Anonymous], 2007, IJCAI
   [Anonymous], 2014, P INT C MULT RETR
   [Anonymous], 2008, P 31 ANN INT ACM SIG, DOI DOI 10.1145/1390334.1390387
   [Anonymous], PROC CVPR IEEE
   Bian JW, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P537, DOI 10.1145/2600428.2609616
   Bian JW, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1807, DOI 10.1145/2505515.2505652
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chakrabarti D., 2011, ICWSM
   Chen Y, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P43
   Chua TS, 2012, IEEE MULTIMEDIA, V19, P81, DOI 10.1109/MMUL.2012.39
   Conroy J. M., 2001, SIGIR Forum, P406
   Erkan G., 2004, Proceedings of EMNLP, V4
   Goldstein J, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P121, DOI 10.1145/312624.312665
   Haghighi Aria, 2009, P HUMAN LANGUAGE TEC, P362, DOI DOI 10.3115/1620754.1620807
   Inouye D., 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P298, DOI 10.1109/PASSAT/SocialCom.2011.31
   Jones KS, 2007, INFORM PROCESS MANAG, V43, P1449, DOI 10.1016/j.ipm.2007.03.009
   Li PH, 2012, PHYSCS PROC, V35, P28, DOI 10.1016/j.phpro.2012.06.006
   Lin C., 2012, P 21 ACM INT C INF K, P175
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu W, 2011, PROC CVPR IEEE, P849, DOI 10.1109/CVPR.2011.5995315
   Mihalcea R, 2005, COMP VOL P C INCL PO
   Park S, 2007, LECT NOTES COMPUT SC, V4362, P761
   Radev DR, 2004, INFORM PROCESS MANAG, V40, P919, DOI 10.1016/j.ipm.2003.10.006
   Sakaki T., 2010, P 19 INT C WORLD WID, P851
   Sharifi Beaux, 2010, Proceedings of the 2010 IEEE Second International Conference on Social Computing (SocialCom 2010). the Second IEEE International Conference on Privacy, Security, Risk and Trust (PASSAT 2010), P49, DOI 10.1109/SocialCom.2010.17
   Sharifi B., 2010, HUMAN LANGUAGE TECHN, P685
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shi T, 2009, ANN STAT, V37, P3960, DOI 10.1214/09-AOS700
   Vanderwende L, 2007, INFORM PROCESS MANAG, V43, P1606, DOI 10.1016/j.ipm.2007.01.023
   Weng J., 2011, 5 INT AAAI C WEBLOGS
   Yan Rui, 2012, P 21 ACM INT C INFOR, P275, DOI DOI 10.1145/2396761.2396799
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Yang Y, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457456
   Yihong Gong, 2001, SIGIR Forum, P19
NR 34
TC 73
Z9 80
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2015
VL 17
IS 2
BP 216
EP 228
DI 10.1109/TMM.2014.2384912
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA AZ4RN
UT WOS:000348210500006
DA 2024-07-18
ER

PT J
AU Strong, G
   Gong, ML
AF Strong, Grant
   Gong, Minglun
TI Self-Sorting Map: An Efficient Algorithm for Presenting Multimedia Data
   in Structured Layouts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Algorithms; artificial neural networks; computational and artificial
   intelligence; computers and information processing; data visualization;
   neural networks; parallel algorithm; systems; man and cybernetics; user
   interfaces
ID DIMENSIONALITY REDUCTION; EXPLORATION; PROJECTION
AB This paper presents the Self-Sorting Map (SSM), a novel algorithm for organizing and presenting multimedia data. Given a set of data items and a dissimilarity measure between each pair of them, the SSM places each item into a unique cell of a structured layout, where the most related items are placed together and the unrelated ones are spread apart. The algorithm integrates ideas from dimension reduction, sorting, and data clustering algorithms. Instead of solving the continuous optimization problem that other dimension reduction approaches do, the SSM transforms it into a discrete labeling problem. As a result, it can organize a set of data into a structured layout without overlap, providing a simple and intuitive presentation. The algorithm is designed for sorting all data items in parallel, making it possible to arrange millions of items in seconds. Experiments on different types of data demonstrate the SSM's versatility in a variety of applications, ranging from positioning city names by proximities to presenting images according to visual similarities, to visualizing semantic relatedness between Wikipedia articles.
C1 [Strong, Grant; Gong, Minglun] Mem Univ Newfoundland, Dept Comp Sci, St John, NF A1B 3X5, Canada.
C3 Memorial University Newfoundland
RP Strong, G (corresponding author), Mem Univ Newfoundland, Dept Comp Sci, St John, NF A1B 3X5, Canada.
RI Gong, Minglun/AAU-3103-2020
OI Gong, Minglun/0000-0001-5820-5381
FU NSERC; Memorial University of Newfoundland
FX The authors would like to thank the anonymous reviewers for their
   constructive and valuable comments. Thanks also go to Roberto De Phino,
   M. C. F. de Oliveira, and A. de Andrade Lopes, for providing us with
   their HexBoard implementation. This research is supported by the NSERC
   and Memorial University of Newfoundland.
CR [Anonymous], 1995, SELF ORG MAPS
   [Anonymous], 2008, EFFECTIVE LOW COST M
   Borg I., 2005, Modern Multidimensional Scaling: Theory and Applications
   Chen TT, 2008, IEEE INT CONF INF VI, P415, DOI 10.1109/IV.2008.52
   Ciura M., 2001, Fundamentals of Computation Theory. 13th International Symposium, FCT 2001. Proceedings (Lecture Notes in Computer Science Vol.2138), P106
   de Pinho RD, 2010, MULTIMED TOOLS APPL, V50, P533, DOI 10.1007/s11042-010-0483-5
   Di Battista G., 1999, Graph Drawing: Algorithms for the Visualization of Graphs, V357
   Dwyer T, 2006, LECT NOTES COMPUT SC, V3843, P153
   Gomez-Nieto E., 2013, P SIBGR C GRAPH IM V
   Heesch D, 2008, MULTIMED TOOLS APPL, V40, P261, DOI 10.1007/s11042-008-0207-2
   Hoque E, 2013, INFORM PROCESS MANAG, V49, P1122, DOI 10.1016/j.ipm.2012.12.001
   Hoque E, 2011, ADV INTEL SOFT COMPU, V86, P73, DOI 10.1007/978-3-642-18029-3_8
   Hotton S., 2004, HISEE VER 1 0 0
   Jensen R. E., 2012, SELF SORTING MAP COL
   Joia P, 2011, IEEE T VIS COMPUT GR, V17, P2563, DOI 10.1109/TVCG.2011.220
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Kaski S, 2011, IEEE SIGNAL PROC MAG, V28, P100, DOI 10.1109/MSP.2010.940003
   Kohonen T, 2000, IEEE T NEURAL NETWOR, V11, P574, DOI 10.1109/72.846729
   MDSJ, 2009, MDSJ JAV LIB MULT SC
   Morrison A., 2003, Information Visualization, V2, P68, DOI 10.1057/palgrave.ivs.9500040
   Paulovich FV, 2011, COMPUT GRAPH FORUM, V30, P1091, DOI 10.1111/j.1467-8659.2011.01958.x
   Paulovich FV, 2012, COMPUT SCI ENG, V14, P74, DOI 10.1109/MCSE.2012.85
   Paulovich FV, 2010, IEEE T VIS COMPUT GR, V16, P1281, DOI 10.1109/TVCG.2010.207
   PINHO R, 2009, SAC 09, P1757, DOI DOI 10.1145/1529282.1529679
   Pinho R, 2009, INFORMATION VISUALIZATION, IV 2009, PROCEEDINGS, P32, DOI 10.1109/IV.2009.12
   POST F.H., 2002, DATA VISUALIZATION S
   Rodden K., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P190, DOI 10.1145/365024.365097
   SAMMON JW, 1969, IEEE T COMPUT, VC 18, P401, DOI 10.1109/T-C.1969.222678
   Strobelt H, 2012, COMPUT GRAPH FORUM, V31, P1135, DOI 10.1111/j.1467-8659.2012.03106.x
   Strong G., 2011, P GRAPHICS INTERFACE, P199
   Strong G., 2009, P INT C IM VID RETR, P1
   Strong G, 2010, LECT NOTES COMPUT SC, V6454, P481, DOI 10.1007/978-3-642-17274-8_47
   Strong G, 2010, LECT NOTES COMPUT SC, V6335, P424, DOI 10.1007/978-3-642-15470-6_44
   Strong G, 2008, LECT NOTES COMPUT SC, V5359, P390, DOI 10.1007/978-3-540-89646-3_38
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tikhonova A., 2008, P EUROGRAPHICS S PAR, P25
   vander Maaten L., 2009, J MACH LEARN RES, V10, P13, DOI [10.1080/13506280444000102, DOI 10.1080/13506280444000102]
   Venna J, 2010, J MACH LEARN RES, V11, P451
   Zhang J, 2009, IEEE T VIS COMPUT GR, V15, P1153, DOI 10.1109/TVCG.2009.202
NR 39
TC 23
Z9 24
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1045
EP 1058
DI 10.1109/TMM.2014.2306183
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800013
DA 2024-07-18
ER

PT J
AU Li, J
   Qian, XM
   Tang, YY
   Yang, LJ
   Mei, T
AF Li, Jing
   Qian, Xueming
   Tang, Yuan Yan
   Yang, Linjun
   Mei, Tao
TI GPS Estimation for Places of Interest From Social Users' Uploaded Photos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE BoW; GPS Estimation; Hierarchical Structure; Inverted File Structure;
   k-NN; Social Media; User
ID RETRIEVAL; FEATURES
AB Social media has become a very popular way for people to share their photos with friends. Because most of the social images are attached with GPS (geo-tags), a photo's GPS information can be estimated with the help of the large geo-tagged image set while using a visual searching based approach. This paper proposes an unsupervised image GPS location estimation approach with hierarchical global feature clustering and local feature refinement. It consists of two parts: an offline system and an online system. In the offline system, a hierarchical structure is constructed for a large-scale offline social image set with GPS information. Representative images are selected for each GPS location refined cluster, and an inverted file structure is proposed. In the online system, when given an input image, its GPS information can be estimated by hierarchical global clusters selection and local feature refinement in the online system. Both the computational cost and GPS estimation performance demonstrates the effectiveness of the proposed hierarchical structure and inverted file structure in our approach.
C1 [Li, Jing; Qian, Xueming] Xi An Jiao Tong Univ, SMILES LAB, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
   [Tang, Yuan Yan] FST Macau Univ, Macau, Peoples R China.
   [Yang, Linjun; Mei, Tao] Microsoft Res Asia, Beijing, Peoples R China.
C3 Xi'an Jiaotong University; University of Macau; Microsoft Research Asia;
   Microsoft
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, SMILES LAB, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
EM lijing.1@stu.xjtu.edu.cn; qianxm@mail.xjtu.edu.cn; yytang@umac.edu.cn;
   linjuny@microsoft.com; tmei@microsoft.com
RI Mei, Tao/GQZ-0596-2022; Qian, Xueming/E-9867-2015; Li, Jing/X-3424-2019
OI Mei, Tao/0000-0002-5990-7307; Li, Jing/0000-0003-4007-7697
CR [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], UUCS96006
   [Anonymous], P MEDIAEVAL 2010 WOR
   [Anonymous], P MEDIAEVAL 2012 WOR
   [Anonymous], P ACM INT C IM VID R
   [Anonymous], P MEDIAEVAL 2012 WOR
   [Anonymous], NEUROCOMPUTING
   [Anonymous], P MEDIAEVAL 2012 WOR
   [Anonymous], WORKING NOTES PLACIN
   [Anonymous], P MEDIAEVAL WORKSH W
   [Anonymous], ADV MULTIMEDIA MODEL
   [Anonymous], P MEDIAEVAL 2012 WOR
   [Anonymous], 2009, P ACM INT C IM VID R
   [Anonymous], P MEDIAEVAL 2010 WOR
   [Anonymous], 1995, STORAGE RETRIEVAL IM, DOI DOI 10.1117/12.205308
   Baatz G, 2012, INT J COMPUT VISION, V96, P315, DOI 10.1007/s11263-011-0458-7
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Hays J, 2008, PROC CVPR IEEE, P3436
   Ji RR, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037688
   Ji Rongrong., 2009, P 17 ASS COMP MACH I, P105, DOI DOI 10.1145/1631272.1631289
   Kalogerakis E, 2009, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2009.5459259
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li YP, 2009, IEEE I CONF COMP VIS, P1957, DOI 10.1109/ICCV.2009.5459432
   Liu GH, 2011, PATTERN RECOGN, V44, P2123, DOI 10.1016/j.patcog.2011.02.003
   Liu H., 2012, Proceedings_of_the_20th_ACM_ International_Conference_on_Multimedia, MM'12, P9
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Park M., 2010, Proceedings of the International Conference on Multimedia, Firenze, Italy, P631
   Qian X., 2012, Multimedia Tools and Applications, P1
   Qian XM, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P44, DOI 10.1109/ISM.2009.14
   Quack T., 2004, Proc. ACM Multimedia, P508
   Quack Till., 2008, P 2008 INT C CONTENT, P47
   Rui Y, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P815, DOI 10.1109/ICIP.1997.638621
   Simon I., 2007, IEEE11TH INT C COMPU, P1
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Wu CF, 2008, J NEUROGENET, V22, P1, DOI 10.1080/01677060801918016
   Xue Y, 2012, IEEE IMAGE PROC, P2873, DOI 10.1109/ICIP.2012.6467499
   Yang KY, 2010, LECT NOTES COMPUT SC, V5916, P174, DOI 10.1007/978-3-642-11301-7_20
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
NR 45
TC 53
Z9 55
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2058
EP 2071
DI 10.1109/TMM.2013.2280127
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900027
DA 2024-07-18
ER

PT J
AU Reju, VG
   Khong, AWH
   Bin Sulaiman, A
AF Reju, Vaninirappuputhenpurayil Gopalan
   Khong, Andy W. H.
   Bin Sulaiman, Amir
TI Localization of Taps on Solid Surfaces for Human-Computer Touch
   Interfaces
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human-computer interface; source localization on solids; tangible
   interfaces; TDOA estimation
ID PROPAGATION; LOCATION
AB Localization of impacts on solid surfaces is a challenging task due to dispersion where the velocity of wave propagation is frequency dependent. In this work, we develop a source localization algorithm on solids with applications to human-computer interface. We employ surface-mounted piezoelectric shock sensors that, in turn, allow us to convert existing flat surfaces to a low-cost touch interface. The algorithm estimates the time-differences-of-arrival between the signals via onset detection in the time-frequency domain. The proposed algorithm is suitable for vibration signals generated by a metal stylus and a finger. The validity of the algorithm is then verified on an aluminium and a glass plate surface.
C1 [Reju, Vaninirappuputhenpurayil Gopalan; Khong, Andy W. H.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Bin Sulaiman, Amir] Singapore Airlines, Singapore 486854, Singapore.
C3 Nanyang Technological University
RP Reju, VG (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM reju@ntu.edu.sg; AndyKhong@ntu.edu.sg; amir.su@gmail.com
RI Khong, Andy/A-5169-2011; Reju, Vaninirappuputhenpurayil
   Gopalan/C-3100-2017
OI Khong, Andy/0000-0002-0708-4791; Reju, Vaninirappuputhenpurayil
   Gopalan/0000-0002-6234-965X; Gopalan Reju,
   Vaninirappuputhenpurayil/0000-0003-2675-3935
FU Singapore National Research Foundation Interactive Digital Media RD
   Program [NRF2008IDM-IDM004-010]
FX Manuscript received November 29, 2012; accepted January 07, 2013. Date
   of publication May 22, 2013; date of current version September 13, 2013.
   This work was supported by the Singapore National Research Foundation
   Interactive Digital Media R&D Program, under research grant
   NRF2008IDM-IDM004-010. US Provisional patent application No.
   US61/507,717 was filed on July 14, 2011. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sheng-Wei (Kuan-Ta) Chen.
CR Akay M, 1998, IEEE Trans Inf Technol Biomed, V2, P282, DOI 10.1109/4233.737584
   [Anonymous], P IEEE INT C MULT FU
   [Anonymous], P VIRT C IPROMS
   Arun KR, 2011, IEEE T MULTIMEDIA, V13, P487, DOI 10.1109/TMM.2011.2123084
   Chen HY, 2012, IEEE T WIREL COMMUN, V11, P108, DOI 10.1109/TWC.2011.110811.101739
   CLEVELAND WS, 1988, J AM STAT ASSOC, V83, P596, DOI 10.2307/2289282
   Crevoisier A., 2005, P 2005 INT C NEW INT, P97
   Duda K, 2010, IEEE SIGNAL PROC MAG, V27, P124, DOI 10.1109/MSP.2010.938088
   Gaul L, 1998, MECH SYST SIGNAL PR, V12, P783, DOI 10.1006/mssp.1998.0163
   Ing RK, 2005, APPL PHYS LETT, V87, DOI 10.1063/1.2130720
   Jacobsen E, 2004, IEEE SIGNAL PROC MAG, V21, P110, DOI 10.1109/MSP.2004.1516381
   Jacobsen E, 2003, IEEE SIGNAL PROC MAG, V20, P74, DOI 10.1109/MSP.2003.1184347
   Jeong H, 2000, COMPOS STRUCT, V49, P443, DOI 10.1016/S0263-8223(00)00079-9
   Jeong H, 2000, IEEE T ULTRASON FERR, V47, P612, DOI 10.1109/58.842048
   KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830
   Last M, 2008, J MULTIVARIATE ANAL, V99, P191, DOI 10.1016/j.jmva.2007.06.010
   LEE HB, 1975, IEEE T AERO ELEC SYS, VAE11, P2, DOI 10.1109/TAES.1975.308023
   Lei Philip I. S., 2009, IT Professional, V11, P42, DOI 10.1109/MITP.2009.19
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164, DOI [10.1090/QAM/10666, DOI 10.1090/QAM/10666]
   Nichols SJV, 2007, COMPUTER, V40, P12, DOI 10.1109/MC.2007.286
   Pham DT, 2007, LECT NOTES COMPUT SC, V4551, P901
   Poletkin K, 2010, IEEE INT CON MULTI, P286, DOI 10.1109/ICME.2010.5582570
   Reju VG, 2010, IEEE T AUDIO SPEECH, V18, P101, DOI 10.1109/TASL.2009.2024380
   SACHSE W, 1978, J APPL PHYS, V49, P4320, DOI 10.1063/1.325484
   Scharnhorst K, 2001, ACTA APPL MATH, V69, P95, DOI 10.1023/A:1012692601098
   Shi QJ, 2010, IEEE T SIGNAL PROCES, V58, P3328, DOI 10.1109/TSP.2010.2045416
   Sulaiman A, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P82, DOI 10.1109/CW.2010.72
   Ventsel E., 2001, Appl Mech Rev., DOI DOI 10.1115/1.1483356
   Wang G, 2011, IEEE T WIREL COMMUN, V10, P1560, DOI 10.1109/TWC.2011.030311.101011
   Zheng J, 2007, SIGNAL PROCESS, V87, P3096, DOI 10.1016/j.sigpro.2007.06.014
   ZIOLA SM, 1991, J ACOUST SOC AM, V90, P2551, DOI 10.1121/1.402348
NR 31
TC 21
Z9 23
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1365
EP 1376
DI 10.1109/TMM.2013.2264656
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400013
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Chen, BC
   Chen, YY
   Kuo, YH
   Hsu, WH
AF Chen, Bor-Chun
   Chen, Yan-Ying
   Kuo, Yin-Hsi
   Hsu, Winston H.
TI Scalable Face Image Retrieval Using Attribute-Enhanced Sparse Codewords
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-based image retrieval; face image; human attributes
AB Photos with people (e.g., family, friends, celebrities, etc.) are the major interest of users. Thus, with the exponentially growing photos, large-scale content-based face image retrieval is an enabling technology for many emerging applications. In this work, we aim to utilize automatically detected human attributes that contain semantic cues of the face photos to improve content-based face retrieval by constructing semantic codewords for efficient large-scale face retrieval. By leveraging human attributes in a scalable and systematic framework, we propose two orthogonal methods named attribute-enhanced sparse coding and attribute-embedded inverted indexing to improve the face retrieval in the offline and online stages. We investigate the effectiveness of different attributes and vital factors essential for face retrieval. Experimenting on two public datasets, the results show that the proposed methods can achieve up to 43.5% relative improvement in MAP compared to the existing methods.
C1 [Chen, Bor-Chun; Chen, Yan-Ying; Hsu, Winston H.] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
   [Kuo, Yin-Hsi; Hsu, Winston H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Chen, BC (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
EM sirius42@cmlab.csie.ntu.edu.tw; yanying@cmlab.csie.ntu.edu.tw;
   kuonini@cmlab.csie.ntu.edu.tw; winston@csie.ntu.edu.tw
FU National Science Council of Taiwan [NSC 101-2628-E-002-027-MY2];
   National Taiwan University [AE00-00-05]
FX This work was supported in part by grants from the National Science
   Council of Taiwan, under Contracts NSC 101-2628-E-002-027-MY2, and
   Excellent Research Projects of National Taiwan University, AE00-00-05.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Cees Snoek.
CR Ahonen T., 2004, P EUR C COMP VIS
   [Anonymous], P INT C COMP VIS
   [Anonymous], P ICML
   [Anonymous], 2012, P IEEE C COMP VIS PA
   [Anonymous], P INT C COMP VIS
   [Anonymous], P ICML
   [Anonymous], 1999, P VLDB
   [Anonymous], 2006, PROC IEEE C COMPUTER
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P EUR C COMP VIS
   [Anonymous], P INT C COMP VIS
   [Anonymous], 2001, IEEE COMP SOC C COMP
   [Anonymous], P EUR C COMP VIS
   [Anonymous], 2004, ANN STAT
   [Anonymous], 2010, P IEEE C COMP VIS PA
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2009, P IEEE C COMP VIS PA
   Baluja S., 2007, INT J COMPUT VISION
   Chen B.-C., 2011, P ACM MULT
   Douze M., 2011, P IEEE C COMP VIS PA
   Gallagher AC, 2011, PROC CVPR IEEE
   Huang G.B., 2008, PROC WORKSHOP FACES
   Kumar N., 2011, P INT JOINT C BIOM
   Kumar N, 2011, IEEE T PATTERN ANAL, V33, P1962, DOI 10.1109/TPAMI.2011.48
   Kuo Y.-H., 2011, P IEEE C COMP VIS PA
   Lei Y.-H., 2011, P ACM MULT
   Lowe D., 2003, INT J COMPUT VISION
   Park U, 2010, IEEE T INF FOREN SEC, V5, P406, DOI 10.1109/TIFS.2010.2049842
   Raina Rajat., 2007, P ICML
   Siddiquie B., 2011, P IEEE C COMP VIS PA
   Wang D., 2011, P ACM MULT
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu L, 2010, IEEE T IMAGE PROCESS, V19, P1908, DOI 10.1109/TIP.2010.2045169
   Wu Z., 2010, P IEEE C COMP VIS PA
   Zobel J, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1132956.1132959
NR 35
TC 54
Z9 58
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1163
EP 1173
DI 10.1109/TMM.2013.2242460
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600018
DA 2024-07-18
ER

PT J
AU Yang, JC
   Luo, JB
   Yu, J
   Huang, TS
AF Yang, Jianchao
   Luo, Jiebo
   Yu, Jie
   Huang, Thomas S.
TI Photo Stream Alignment and Summarization for Collaborative Photo
   Collection and Sharing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer applications; distance learning; feature extraction; image
   analysis; image representation
ID CLASSIFICATION
AB With the popularity of digital cameras and camera phones, it is common for different people, who may or may not know each other, to attend the same event and take pictures and videos from different spatial or personal perspectives. Within the realm of social media, it is desirable to enable these people to select and share their pictures and videos in order to enrich memories and facilitate social networking. However, it is cumbersome to manually manage these photos from different cameras, of which the clocks settings are often not calibrated. In this paper, we propose automatic algorithms to address the above problems. First, we accurately align different photo streams or sequences from different photographers for the same event in chronological order on a common timeline, while respecting the time constraints within each photo stream. Given the preferred similarity measures (e. g., visual, and spatial similarities), our algorithm performs photo stream alignment via matching on a bipartite kernel sparse representation graph that forces the data connections to be sparse in an explicit fashion. Furthermore, we can produce a summary master stream from the aligned super stream of photos for efficient sharing by removing those redundant photos in the super stream while accounting for the temporal integrity. Based on a similar kernel sparse representation graph, our master stream summarization algorithm performs greedy backward selection to drop redundant photos without affecting the integrity of remaining photos for the entire event. We evaluate our algorithms on real-world personal online albums for 36 events and demonstrate its efficacy in automatically facilitating collaborative photo collection and sharing.
C1 [Yang, Jianchao; Huang, Thomas S.] Univ Illinois, Beckman Inst, Urbana, IL 61801 USA.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
   [Yu, Jie] GE Global Res, Niskayuna, NY 12309 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   University of Rochester; General Electric
RP Yang, JC (corresponding author), Univ Illinois, Beckman Inst, Urbana, IL 61801 USA.
EM jyang29@ifp.illinois.edu; jluo@cs.rochester.edu; jerry.j.yu@gmail.com;
   huang@ifp.illinois.edu
RI Luo, Jiebo/AAI-7549-2020; yan, shuicheng/HCH-9860-2022; yan,
   shuicheng/A-8531-2014
OI yan, shuicheng/0000-0003-4527-1018; yan, shuicheng/0000-0001-8906-3777;
   Luo, Jiebo/0000-0002-4516-9729
FU Kodak Research; U.S. Army Research Laboratory; U.S. Army Research Office
   [W911NF-09-1-0383]
FX This work was supported in part by Kodak Research and in part by U.S.
   Army Research Laboratory and U.S. Army Research Office under grant
   number W911NF-09-1-0383. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Chia-Wen Lin.
CR [Anonymous], P SIGCHI C HUM FACT
   [Anonymous], P CORR
   [Anonymous], PROC IEEE INT CONF C
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P INT C COMP VIS
   [Anonymous], P ACM INT C IM VID R
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], P INT C CONT BAS IM
   [Anonymous], P CVPR
   [Anonymous], P ACM INT C MULT
   [Anonymous], TECH REP
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764
   Elhamifar E., 2009, Proc. CVPR
   Gammeter S, 2009, IEEE I CONF COMP VIS, P614, DOI 10.1109/ICCV.2009.5459180
   Gao SH, 2010, LECT NOTES COMPUT SC, V6314, P1
   Hastie T, 1998, STAT SCI, V13, P54
   Huynh D.F., 2005, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Proc. CHI), P1937
   Li LJ, 2009, PROC CVPR IEEE, P2036, DOI 10.1109/CVPRW.2009.5206718
   Loui AC, 2003, IEEE T MULTIMEDIA, V5, P390, DOI 10.1109/TMM.2003.814723
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Simon I., 2007, PROC IEEE 11 INT C C, P1
   Wang XJ, 2010, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2010.5540046
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Zhang ST, 2010, PROC CVPR IEEE, P3312, DOI 10.1109/CVPR.2010.5540036
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 28
TC 16
Z9 19
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1642
EP 1651
DI 10.1109/TMM.2012.2198458
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400013
DA 2024-07-18
ER

PT J
AU Wang, SH
   Huang, QM
   Jiang, SQ
   Tian, Q
AF Wang, Shuhui
   Huang, Qingming
   Jiang, Shuqiang
   Tian, Qi
TI S<SUP>3</SUP>MKL: Scalable Semi-Supervised Multiple Kernel Learning for
   Real-World Image Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image categorization; multiple kernel learning; multiple kernel locality
   sensitive hashing; personalized image re-ranking; semi-supervised
   learning
ID GROUP LASSO
AB We study the visual learning models that could work efficiently with little ground-truth annotation and a mass of noisy unlabeled data for large scale Web image applications, following the subroutine of semi-supervised learning (SSL) that has been deeply investigated in various visual classification tasks. However, most previous SSL approaches are not able to incorporate multiple descriptions for enhancing the model capacity. Furthermore, sample selection on unlabeled data was not advocated in previous studies, which may lead to unpredictable risk brought by real-world noisy data corpse. We propose a learning strategy for solving these two problems. As a core contribution, we propose a scalable semi-supervised multiple kernel learning method ((SMKL)-M-3) to deal with the first problem. The aim is to minimize an overall objective function composed of log-likelihood empirical loss, conditional expectation consensus (CEC) on the unlabeled data and group LASSO regularization on model coefficients. We further adapt CEC into a group-wise formulation so as to better deal with the intrinsic visual property of real-world images. We propose a fast block coordinate gradient descent method with several acceleration techniques for model solution. Compared with previous approaches, our model better makes use of large scale unlabeled images with multiple feature representation with lower time complexity. Moreover, to address the issue of reducing the risk of using unlabeled data, we design a multiple kernel hashing scheme to identify the "informative" and "compact" unlabeled training data subset. Comprehensive experiments are conducted and the results show that the proposed learning framework provides promising power for real-world image applications, such as image categorization and personalized Web image re-ranking with very little user interaction.
C1 [Wang, Shuhui; Huang, Qingming; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Huang, Qingming] Chinese Acad Sci, Grad Univ, Beijing 100190, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; University of Texas System; University of Texas at San Antonio
   (UTSA)
RP Wang, SH (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM shwang@jdl.ac.cn; qmhuang@jdl.ac.cn; sqjiang@jdl.ac.cn;
   qitian@cs.utsa.edu
RI Huang, Qingming/GLR-3473-2022
OI Huang, Qingming/0000-0002-3025-7099
FU National Basic Research Program of China (973 Program) [2012CB316400];
   National Natural Science Foundation of China [61025011, 60833006,
   61070108]; NSF IIS [1052851]; Google; FXPAL Laboratory of America; NEC
   Laboratory of America; ARO [W911BF-12-1-0057]; Direct For Computer &
   Info Scie & Enginr; Div Of Information & Intelligent Systems [1052851]
   Funding Source: National Science Foundation
FX This work was supported in part by the National Basic Research Program
   of China (973 Program): 2012CB316400, in part by the National Natural
   Science Foundation of China: 61025011, 60833006, and 61070108. This work
   of Q. Tian was supported in part by NSF IIS 1052851, Faculty Research
   Awards by Google, FXPAL, NEC Laboratories of America, and ARO grant
   W911BF-12-1-0057, respectively. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Francesco G. B. De Natale.
CR [Anonymous], P CVPR
   [Anonymous], 1530 U WISC MAD
   [Anonymous], 2009, P CVPR
   [Anonymous], P ICML
   [Anonymous], P ICCV
   Bach F. R., 2004, P ICML
   Bach FR, 2008, J MACH LEARN RES, V9, P1179
   Bennett K. P., 1998, P NIPS
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Cao L., 2009, P ICCV
   Datar M., 2004, PROC 20 ANN S COMPUT, P253
   Duan L., 2010, P CVPR
   Everingham Mark, 2007, PASCAL VISUAL OBJECT
   Fan J., 2004, P ACM MULT
   Gonen M., 2008, P ICML
   Grandvalet Y., 2005, P NIPS
   Hino H., 2010, P ICMLA, P223
   Hino H, 2010, NEURAL COMPUT, V22, P2887, DOI 10.1162/NECO_a_00027
   HSIEH CJ, 2008, P ICML
   Huang J., 2009, ARXIV09012962
   Kulis B, 2009, IEEE T PATTERN ANAL, V31, P2143, DOI 10.1109/TPAMI.2009.151
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   MANN GS, 2007, P ICML
   McCallum A., 2007, 200760 U MASS
   Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x
   QI G, 2007, P ACM MULT
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Rosenberg D., 2009, IEEE SIGNAL PROCESS
   Saffari A., 2009, P CVPR
   SINDHWANI V, 2005, P ICML
   Sindhwani V., 2008, P ICML
   Singh A., 2008, P NIPS
   Snoek CG, 2005, P ACM MULT
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   TANG J, 2007, P ACM MULT
   Torralba A., 2004, P CVPR
   Torralba A., 2009, LABELME ONLINE IMAGE
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Tseng P., 2009, MATH PROGRAM B, V117, P1
   Tsuda K, 2005, BIOINFORMATICS, V21, P59, DOI 10.1093/bioinformatics/bti1110
   Vapnik V., 1999, NATURE STAT LEARNING
   Varma M., 2007, P ICCV
   Wang S., 2010, P ICPR
   Wang S., 2010, P ACM MULT
   Wu L., 2009, P ACM MULT
   Yan R., 2007, P ACM KDD
   Yan R., 2008, P CVPR
   Yang J., 2009, P ICCV
   ZHANG H, 2006, P CVPR
   Zhu X, 2003, ICML
NR 50
TC 31
Z9 34
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1259
EP 1274
DI 10.1109/TMM.2012.2193120
PN 2
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400012
DA 2024-07-18
ER

PT J
AU Sanchez-Cortes, D
   Aran, O
   Mast, MS
   Gatica-Perez, D
AF Sanchez-Cortes, Dairazalia
   Aran, Oya
   Mast, Marianne Schmid
   Gatica-Perez, Daniel
TI A Nonverbal Behavior Approach to Identify Emergent Leaders in Small
   Groups
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emergent leadership; nonverbal behavior
ID DOMINANCE; TIME
AB Identifying emergent leaders in organizations is a key issue in organizational behavioral research, and a new problem in social computing. This paper presents an analysis on how an emergent leader is perceived in newly formed, small groups, and then tackles the task of automatically inferring emergent leaders, using a variety of communicative nonverbal cues extracted from audio and video channels. The inference task uses rule-based and collective classification approaches with the combination of acoustic and visual features extracted from a new small group corpus specifically collected to analyze the emergent leadership phenomenon. Our results show that the emergent leader is perceived by his/her peers as an active and dominant person; that visual information augments acoustic information; and that adding relational information to the nonverbal cues improves the inference of each participant's leadership rankings in the group.
C1 [Sanchez-Cortes, Dairazalia; Aran, Oya] Idiap Res Inst, Martigny, Switzerland.
   [Sanchez-Cortes, Dairazalia; Gatica-Perez, Daniel] Swiss Fed Inst Technol EPFL, Idiap Res Inst & Maitre Enseignement & Rech Exter, Social Comp Grp, Lausanne, Switzerland.
   [Sanchez-Cortes, Dairazalia; Gatica-Perez, Daniel] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Mast, Marianne Schmid] Univ Neuchatel, CH-2000 Neuchatel, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne; University of Neuchatel
RP Sanchez-Cortes, D (corresponding author), Swiss Fed Inst Technol EPFL, Idiap Res Inst & Maitre Enseignement & Rech Exter, Social Comp Grp, Lausanne, Switzerland.
EM dscortes@idiap.ch; oya.aran@idiap.ch; marianne.schmid@unine.ch;
   gatica@idiap.ch
RI Aran, Oya/ABR-6400-2022
OI Aran, Oya/0000-0002-4679-9335; Sanchez Cortes,
   Dairazalia/0000-0003-2429-6152
FU Mexico's National Council for Science and Technology (CONACYT); EU;
   Swiss National Science Foundation
FX This work was supported in part by Mexico's National Council for Science
   and Technology (CONACYT) through a doctoral studies scholarship, the EU
   project NOVICOM, and Swiss National Science Foundation project SONVB.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sheng-Wei (Kuan-Ta) Chen.
CR AMBADY N, 1995, J PERS SOC PSYCHOL, V69, P518, DOI 10.1037/0022-3514.69.3.518
   Anderson C, 2009, J PERS SOC PSYCHOL, V96, P491, DOI 10.1037/a0014201
   [Anonymous], 2008, Proc. ACM Multimedia, DOI [10.1145/1459359.1459462, DOI 10.1145/1459359.1459462]
   [Anonymous], 2008, HONEST SIGNALS, DOI DOI 10.7551/MITPRESS/8022.001.0001
   [Anonymous], P INT
   [Anonymous], 1993, LEADERSHIP Q, DOI DOI 10.1016/1048-9843(93)90003-C
   Aran O., 2010, P ICPR AUR
   Aran O., 2008, THESIS BOGAZICI U IS
   Bachour K, 2010, IEEE T LEARN TECHNOL, V3, P203, DOI 10.1109/TLT.2010.18
   Baird J., 1977, Southern Communication Journal, P352, DOI DOI 10.1080/10417947709372361
   Bass B. M., 1990, Bass & Stogdill's handbook of leadership: Theory, research, and managerial applications, V3rd
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Bradski G., 2000, DOBBS J SOFTW TOOLS
   Carte TA, 2006, GROUP DECIS NEGOT, V15, P323, DOI 10.1007/s10726-006-9045-7
   DiMicco J. M., 2004, Computer Supported Cooperative Work Conference Proceedings, P614, DOI 10.1145/1031607.1031713
   Dong W, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P271
   Dunbar NE, 2005, J SOC PERS RELAT, V22, P207, DOI 10.1177/0265407505050944
   Favre S., 2008, Proc. ACM International Conference on Multimodal Interfaces, P29
   Gatica-Perez D., 2009, IMAGE VIS COMPUT, V1
   Gloor PA, 2006, INFORMATION VISUALIZATION-BOOK, P130
   Goodstein L. D., 1999, J BUSINESS PSYCHOL, V13
   GRAY AH, 1974, IEEE T ACOUST SPEECH, VAS22, P207, DOI 10.1109/TASSP.1974.1162572
   Hall JA, 2005, PSYCHOL BULL, V131, P898, DOI 10.1037/0033-2909.131.6.898
   Hersey P., 1977, Management of organizational behavior: Utilizing human resources, V3rd
   Hung H., 2007, P IACM INT C MULT AC
   Isard M., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P343, DOI 10.1007/BFb0015549
   Jayagopi D., 2009, P INT C MULT INT ICM
   Jayagopi D., 2009, P ICME JUN
   Jayagopi D., 2010, MOB UBIQ MULTIME DEC
   Jayagopi DB, 2010, IEEE T MULTIMEDIA, V12, P790, DOI 10.1109/TMM.2010.2065218
   Jayagopi DB, 2009, IEEE T AUDIO SPEECH, V17, P501, DOI 10.1109/TASL.2008.2008238
   Jensen D., 2004, P ACM SIGKDD INT C K
   John OP, 1990, HDB PERSONALITY THEO, P66
   Kickul J., 2000, J BUSINESS PSYCHOL, V15
   Kim T, 2008, CSCW: 2008 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, CONFERENCE PROCEEDINGS, P457
   Knapp M. L., 2008, NONVERBAL COMMUNICAT
   Lepri B., 2010, P INT C MULT INT ICM
   Madan A., 2011, P PERV
   Mast MS, 2002, HUM COMMUN RES, V28, P420, DOI 10.1111/j.1468-2958.2002.tb00814.x
   McCowan I., 2005, IEEE INT C MULT EXP, P1382, DOI DOI 10.1109/ICME.2005.1521688
   McDowell LK, 2009, J MACH LEARN RES, V10, P2777
   Neville J, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P609
   Pianesi F., 2008, P INT C MULT INT ICM
   Pianesi F, 2007, LANG RESOUR EVAL, V41, P409, DOI 10.1007/s10579-007-9060-6
   Raducanu B., MULTIMEDIA TOOLS APP, P1
   Raducanu B., 2009, P ICASSP APR
   RIENKS RJ, 2005, P WORKSH MACH LEARN
   Sanchez-Cortes D., 2010, P INT C MULT INT ICM
   STEIN RT, 1975, J PERS SOC PSYCHOL, V32, P125, DOI 10.1037/h0076842
   STEIN RT, 1979, J PERS SOC PSYCHOL, V37, P1993, DOI 10.1037/0022-3514.37.11.1993
   Sturm Janienke, 2007, P INT C MULT INT
   Talkin D., 1995, Speech coding and synthesis, V495, P518
   Temdee P, 2006, LECT NOTES COMPUT SC, V3942, P745, DOI 10.1007/11736639_92
   Varni G, 2010, IEEE T MULTIMEDIA, V12, P576, DOI 10.1109/TMM.2010.2052592
   WENTWORTH DK, 1984, SEX ROLES, V11, P513, DOI 10.1007/BF00287475
   Woolley AW, 2010, SCIENCE, V330, P686, DOI 10.1126/science.1193147
   Zancanaro M, 2006, P INT C MULT INT ICM
NR 57
TC 121
Z9 129
U1 3
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 816
EP 832
DI 10.1109/TMM.2011.2181941
PN 2
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700014
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Wang, YJ
   Guan, L
   Venetsanopoulos, AN
AF Wang, Yongjin
   Guan, Ling
   Venetsanopoulos, Anastasios N.
TI Kernel Cross-Modal Factor Analysis for Information Fusion With
   Application to Bimodal Emotion Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 11-15, 2011
CL Univ Ramon Llull, La Salle, Barcelona, SPAIN
SP IEEE, IEEE Signal Proc Soc, IEEE Circuits & Syst Soc, IEEE Comp Soc, IEEE Commun Soc
HO Univ Ramon Llull, La Salle
DE Cross-modal association; emotion recognition; information fusion; kernel
   machine technique
ID FACE; MODELS
AB In this paper, we investigate kernel based methods for multimodal information analysis and fusion. We introduce a novel approach, kernel cross-modal factor analysis, which identifies the optimal transformations that are capable of representing the coupled patterns between two different subsets of features by minimizing the Frobenius norm in the transformed domain. The kernel trick is utilized for modeling the nonlinear relationship between two multidimensional variables. We examine and compare with kernel canonical correlation analysis which finds projection directions that maximize the correlation between two modalities, and kernel matrix fusion which integrates the kernel matrices of respective modalities through algebraic operations. The performance of the introduced method is evaluated on an audiovisual based bimodal emotion recognition problem. We first perform feature extraction from the audio and visual channels respectively. The presented approaches are then utilized to analyze the cross-modal relationship between audio and visual features. A hidden Markov model is subsequently applied for characterizing the statistical dependence across successive time segments, and identifying the inherent temporal structure of the features in the transformed domain. The effectiveness of the proposed solution is demonstrated through extensive experimentation.
C1 [Wang, Yongjin] Hisense Co Ltd, State Key Lab Digital Multimedia Technol, Qingdao, Peoples R China.
   [Guan, Ling; Venetsanopoulos, Anastasios N.] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 Hisense; Toronto Metropolitan University
RP Wang, YJ (corresponding author), Hisense Co Ltd, State Key Lab Digital Multimedia Technol, Qingdao, Peoples R China.
EM wangyongjin@hisense.com; lguan@ee.ryerson.ca; tasvenet@ryerson.ca
RI Sun, Yuchen/JZD-1692-2024
CR Aldea E, 2007, LECT NOTES COMPUT SC, V4842, P307
   [Anonymous], 2003, P ACM INT C MULT ACM
   [Anonymous], 2008, P 2008 16 EUR SIGN P
   [Anonymous], P 3 INT C AN MOD FAC
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Baudat G, 2000, NEURAL COMPUT, V12, P2385, DOI 10.1162/089976600300014980
   Blaschko M. B., 2008, P IEEE CVPR, P1
   Bredin H, 2007, INT CONF ACOUST SPEE, P233
   Chan CH, 2010, LECT NOTES COMPUT SC, V6218, P718, DOI 10.1007/978-3-642-14980-1_71
   Correa NM, 2010, IEEE SIGNAL PROC MAG, V27, P39, DOI 10.1109/MSP.2010.936725
   De Bie T, 2007, BIOINFORMATICS, V23, pI125, DOI 10.1093/bioinformatics/btm187
   Garcia C, 1999, IEEE T MULTIMEDIA, V1, P264, DOI 10.1109/6046.784465
   Go HJ, 2003, SICE 2003 ANNUAL CONFERENCE, VOLS 1-3, P2890
   Han MJ, 2008, J COMPUT, V3, P39
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Joshi T, 2009, INT J BIOMETRICS, V1, P393, DOI 10.1504/IJBM.2009.027303
   Lai P., 2001, INT J NEURAL SYST, V10, P365
   Lanckriet GRG, 2003, PACIFIC SYMPOSIUM ON BIOCOMPUTING 2004, P300
   Li D., 2005, Marquette University, P1
   Ling Guan, 2010, International Journal of Multimedia Intelligence and Security, V1, P5, DOI 10.1504/IJMIS.2010.035969
   de Diego IM, 2010, PATTERN RECOGN LETT, V31, P837, DOI 10.1016/j.patrec.2009.12.030
   Martin O., 2006, 22 INT C DATA ENG WO, P8, DOI [DOI 10.1109/ICDEW.2006.145, 10.1109/ICDEW.2006.145]
   Metallinou A, 2008, IEEE INT SYM MULTIM, P250, DOI 10.1109/ISM.2008.40
   Müller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Ross A, 2003, PATTERN RECOGN LETT, V24, P2115, DOI 10.1016/S0167-8655(03)00079-5
   Salimi F, 2009, ICDIP 2009: INTERNATIONAL CONFERENCE ON DIGITAL IMAGE PROCESSING, PROCEEDINGS, P407, DOI 10.1109/ICDIP.2009.77
   Sargin ME, 2007, IEEE T MULTIMEDIA, V9, P1396, DOI 10.1109/TMM.2007.906583
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Song ML, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P877
   Sun QS, 2005, PATTERN RECOGN, V38, P2437, DOI 10.1016/j.patcog.2004.12.013
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Wang YJ, 2008, IEEE T MULTIMEDIA, V10, P936, DOI 10.1109/TMM.2008.927665
   Xu XN, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON AUTOMATION AND LOGISTICS, VOLS 1-6, P620, DOI 10.1109/ICAL.2007.4338638
   Xu XN, 2009, ITCS: 2009 INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY AND COMPUTER SCIENCE, PROCEEDINGS, VOL 2, PROCEEDINGS, P3, DOI 10.1109/ITCS.2009.139
   Yucheng Wang, 2008, 2008 9th International Conference on Signal Processing (ICSP 2008), P2092, DOI 10.1109/ICOSP.2008.4697557
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zeng ZH, 2008, IEEE T MULTIMEDIA, V10, P570, DOI 10.1109/TMM.2008.921737
   Zhao JO, 2009, PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS (ICIG 2009), P958, DOI 10.1109/ICIG.2009.149
NR 39
TC 106
Z9 111
U1 1
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 597
EP 607
DI 10.1109/TMM.2012.2189550
PN 1
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 943ZG
UT WOS:000304166300011
DA 2024-07-18
ER

PT J
AU Geng, B
   Li, YX
   Tao, DC
   Wang, M
   Zha, ZJ
   Xu, C
AF Geng, Bo
   Li, Yangxi
   Tao, Dacheng
   Wang, Meng
   Zha, Zheng-Jun
   Xu, Chao
TI Parallel Lasso for Large-Scale Video Concept Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Incomplete cholosky factorization; lasso; parallel learning; video
   concept detection
ID REGULARIZATION
AB Existing video concept detectors are generally built upon the kernel based machine learning techniques, e. g., support vector machines, regularized least squares, and logistic regression, just to name a few. However, in order to build robust detectors, the learning process suffers from the scalability issues including the high-dimensional multi-modality visual features and the large-scale keyframe examples. In this paper, we propose parallel lasso (Plasso) by introducing the parallel distributed computation to significantly improve the scalability of lasso (the regularized least squares). We apply the parallel incomplete Cholesky factorization to approximate the covariance statistics in the preprocess step, and the parallel primal-dual interior-point method with the Sherman-Morrison-Woodbury formula to optimize the model parameters. For a dataset with samples in a d-dimensional space, compared with lasso, Plasso significantly reduces complexities from the original O(d(3)) for computational time and O(d(2)) for storage space to O(h(2)d/m) and O(hd/m), respectively, if the system has processors and the reduced dimension is much smaller than the original dimension. Furthermore, we develop the kernel extension of the proposed linear algorithm with the sample reweighting schema, and we can achieve similar time and space complexity improvements [time complexity from O(n(3)) to O(h(2)n/m) and the space complexity from O(n(2)) to O(hn/m), for a dataset with training examples]. Experimental results on TRECVID video concept detection challenges suggest that the proposed method can obtain significant time and space savings for training effective detectors with limited communication overhead.
C1 [Geng, Bo; Li, Yangxi; Xu, Chao] Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China.
   [Tao, Dacheng] Univ Technol Sydney, Ctr Quantum Computat & Intelligent, Fac Engn & Informat Technol, Broadway, NSW 2007, Australia.
   [Wang, Meng] Hefei Univ Technol, Sch Comp & Informat, Hefei, Peoples R China.
   [Zha, Zheng-Jun] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
C3 Peking University; University of Technology Sydney; Hefei University of
   Technology; National University of Singapore
RP Geng, B (corresponding author), Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China.
EM bogeng@pku.edu.cn; liyangxi@cis.pku.edu.cn; dacheng.tao@uts.edu.au;
   eric.mengwang@gmail.com; zhazj@comp.nus.edu.sg; xuchao@cis.pku.edu.cn
RI cheng, cheng/JBR-8359-2023; Zha, Zheng-Jun/AAE-8408-2020; Wang,
   Meng/ITR-8699-2023; Zha, Zheng-Jun/AAF-8667-2020; Tao,
   Dacheng/A-5449-2012
OI Zha, Zheng-Jun/0000-0003-2510-8993; Tao, Dacheng/0000-0001-7225-5449
FU NBRPC [2011CB302400]; NSFC [60975014]; NSFB [4102024]
FX This work was supported by NBRPC 2011CB302400, NSFC 60975014, and NSFB
   4102024. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Shuicheng Yan.
CR [Anonymous], 2006, Advances in neural information processing systems
   [Anonymous], 2009, P 17 ACM INT C MULTI
   [Anonymous], 1998, STAT LEARNING THEORY
   Bi J., 2003, Journal of Machine Learning Research, V3, P1229, DOI 10.1162/153244303322753643
   Bo Geng, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2396, DOI 10.1109/CVPRW.2009.5206695
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chang E. Y., 2007, ADV NEURAL INF PROCE, V20
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619
   Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01
   Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712
   Geng B., 2008, P 1 ACM INT C MULT I, P443
   Geng B., IEEE T KNOW IN PRESS
   Geng B, 2010, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2010.5540003
   KERSHAW DS, 1978, J COMPUT PHYS, V26, P43, DOI 10.1016/0021-9991(78)90098-0
   Kim SJ, 2007, IEEE J-STSP, V1, P606, DOI 10.1109/JSTSP.2007.910971
   Koh KM, 2007, J MACH LEARN RES, V8, P1519
   Liu QS, 2008, NEUROCOMPUTING, V71, P1850, DOI 10.1016/j.neucom.2007.10.024
   Liu QS, 2009, COMPUT VIS IMAGE UND, V113, P317, DOI 10.1016/j.cviu.2008.11.004
   Liu ZY, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961198
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Mei T., 2008, P TRECV 2007 HIGH LE
   Naphade M.R., 2005, A light scale concept ontology for multimedia understanding for trecvid 2005
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Naphade MR, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P210
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Rong Yan, 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P324
   Rudinac S., 2010, P ACM INT C MULT ACM, P727
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Song YQ, 2008, LECT NOTES ARTIF INT, V5212, P374, DOI 10.1007/978-3-540-87481-2_25
   TIBSHIRANI, 1994, J R STAT SOC B, V58, P267
   Wang M, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P654, DOI 10.1145/1571941.1572062
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, COMPUT VIS IMAGE UND, V113, P384, DOI 10.1016/j.cviu.2008.08.003
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Yan Shuicheng., 2009, SOC IND APPL MATH P, P792, DOI [10.1137/1.9781611972795.68, DOI 10.1137/1.9781611972795.68]
   Yang P, 2009, IEEE I CONF COMP VIS, P1018, DOI 10.1109/ICCV.2009.5459371
   Yuan XT, 2010, PROC CVPR IEEE, P3493, DOI 10.1109/CVPR.2010.5539967
   Zha Z.-J., 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587384
NR 41
TC 16
Z9 18
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 55
EP 65
DI 10.1109/TMM.2011.2174781
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100006
DA 2024-07-18
ER

PT J
AU Zha, ZJ
   Wang, M
   Zheng, YT
   Yang, Y
   Hong, RC
   Chua, TS
AF Zha, Zheng-Jun
   Wang, Meng
   Zheng, Yan-Tao
   Yang, Yi
   Hong, Richang
   Chua, Tat-Seng
TI Interactive Video Indexing With Statistical Active Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active learning; optimum experimental design; video indexing
ID FRAMEWORK
AB Video indexing, also called video concept detection, has attracted increasing attentions from both academia and industry. To reduce human labeling cost, active learning has been introduced to video indexing recently. In this paper, we propose a novel active learning approach based on the optimum experimental design criteria in statistics. Different from existing optimum experimental design, our approach simultaneously exploits sample's local structure, and sample relevance, density, and diversity information, as well as makes use of labeled and unlabeled data. Specifically, we develop a local learning model to exploit the local structure of each sample. Our assumption is that for each sample, its label can be well estimated based on its neighbors. By globally aligning the local models from all the samples, we obtain a local learning regularizer, based on which a local learning regularized least square model is proposed. Finally, a unified sample selection approach is developed for interactive video indexing, which takes into account the sample relevance, density and diversity information, and sample efficacy in minimizing the parameter variance of the proposed local learning regularized least square model. We compare the performance between our approach and the state-of-the-art approaches on the TREC video retrieval evaluation (TRECVID) benchmark. We report superior performance from the proposed approach.
C1 [Zha, Zheng-Jun; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Wang, Meng; Hong, Richang] Hefei Univ Technol, Hefei, Anhui, Peoples R China.
   [Zheng, Yan-Tao] Inst Infocomm Res, Singapore, Singapore.
   [Yang, Yi] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld, Australia.
C3 National University of Singapore; Hefei University of Technology; Agency
   for Science Technology & Research (A*STAR); A*STAR - Institute for
   Infocomm Research (I2R); University of Queensland
RP Zha, ZJ (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
EM eric.mengwang@gmail.com
RI Zha, Zheng-Jun/AAE-8408-2020; Yang, Yi/B-9273-2017; yang,
   yang/HGT-7999-2022; Lang, Ming/HIK-0758-2022; Zha,
   Zheng-Jun/AAF-8667-2020; yang, yang/GWB-9426-2022; Brennan,
   Kathy/D-6118-2011; Wang, Meng/ITR-8699-2023; yang, yang/GVT-5210-2022
OI Zha, Zheng-Jun/0000-0003-2510-8993; Yang, Yi/0000-0002-0512-880X; 
FU A*Star [R-252-000-437-305]
FX This work was supported by A*Star Research Grant R-252-000-437-305. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Qingshan Liu.
CR [Anonymous], 2009, P 17 ACM INT C MULTI
   [Anonymous], 2004, P INT C MACH LEARN
   [Anonymous], P ACM INT C MULT CAL
   Atkinson AC, 2007, OPTIMUM EXPT DESIGNS
   Ayache S, 2007, SIGNAL PROCESS-IMAGE, V22, P692, DOI 10.1016/j.image.2007.05.010
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Brinker K., 2003, P INT C MACH LEARN
   Chapelle O., 2010, Semi-Supervised Learning, V1st
   Chen C., P 24 AAAI C ART INT, P254
   Chen M.-Y., 2005, P 20 NAT C ART INT W
   Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295
   Dagli CK, 2006, LECT NOTES COMPUT SC, V4071, P123
   Donmez P., 2007, P EUR C MACH LEARN
   Golub G.H., 2013, Matrix Computations, DOI DOI 10.56021/9781421407944
   Harville D.A., 1997, Matrix Algebra from a Statistician's Perspective
   Hastie T., 2009, The Elements of Statistical Learning
   He XF, 2009, PROC CVPR IEEE, P65, DOI 10.1109/CVPRW.2009.5206835
   He XF, 2010, IEEE T IMAGE PROCESS, V19, P254, DOI 10.1109/TIP.2009.2032342
   Huang TS, 2008, P IEEE, V96, P648, DOI 10.1109/JPROC.2008.916364
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Jin Rong., 2010, P ADV NEUR INF PROC
   Mei T., 2007, P TRECVID 2007 WORKS
   Ming-yu Chen, 2005, 13th Annual ACM International Conference on Multimedia, P902, DOI 10.1145/1101149.1101342
   Naphadea M., 2004, P INT C IM PROC
   Nguyen GP, 2007, IEEE T MULTIMEDIA, V9, P1404, DOI 10.1109/TMM.2007.906586
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Qi G.-J., 2007, P ACM INT C MULT
   Rosenfeld, 2005, SEMISUPERVISED LEARN
   Roy Nicholas, 2001, Toward optimal active learning through monte carlo estimation of error reduction, P441
   Shen JL, 2008, IEEE T CIRC SYST VID, V18, P1587, DOI 10.1109/TCSVT.2008.2005607
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Wang M., 2007, P INT C SEM COMP
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wu Mingrui, 2007, International Conference on Artificial Intelligence and Statistics, P628
   Xiaofei He, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P119
   Xinmei Tian, 2008, 2008 IEEE International Conference on Multimedia and Expo (ICME), P1509, DOI 10.1109/ICME.2008.4607733
   Yan R., 2003, P INT C COMP VIS
   Yang Yi., 2009, Proceedings of the 17th ACM international conference on Multimedia, P175
   Zha Z.-J., 2004, P INT C IM PROC
   Zha Z.-J., 2011, P INT C MULT RETR
   Zha ZJ, 2009, J VIS COMMUN IMAGE R, V20, P97, DOI 10.1016/j.jvcir.2008.11.009
   Zhang L., 2009, P 17 ACM INT C MULT, P45
NR 44
TC 101
Z9 110
U1 1
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 17
EP 27
DI 10.1109/TMM.2011.2174782
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100003
DA 2024-07-18
ER

PT J
AU Wang, WY
   Zhang, DM
   Zhang, YD
   Li, JT
   Gu, XG
AF Wang, Wenying
   Zhang, Dongming
   Zhang, Yongdong
   Li, Jintao
   Gu, Xiaoguang
TI Robust Spatial Matching for Object Retrieval and Its Parallel
   Implementation on GPU
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affine transformations; GPU; object retrieval; parallel computing
ID IMAGE
AB Spatial matching for object retrieval is often time-consuming and susceptible to viewpoint changes. To address this problem, we propose a novel spatial matching method that is robust to viewpoint changes and implement it on modern graphics processing unit (GPU) in parallel for real-time applications. Unlike previous spatial matching methods used in object retrieval, in which the affine transformation estimation is based on the gravity vector assumption, our method abandons this strong assumption by matching the affine covariant neighbors (ACNs) of corresponding local regions and estimating affine transformation from each single pair of corresponding local regions. Taking into account real-time applications, we implement the method on modern GPU in parallel to speed up the process. Computations are distributed evenly to threads with load balancing, and device memory accesses are optimized with bitmap-based parallel scan. Experimental results demonstrate that our method is more robust and more efficient than previous methods especially when the viewpoints are changed, and the parallel implementation on GPU obtains ten times speedup.
C1 [Wang, Wenying; Zhang, Dongming; Zhang, Yongdong; Li, Jintao; Gu, Xiaoguang] Chinese Acad Sci, Inst Comp Technol, Adv Comp Res Lab, Beijing 100190, Peoples R China.
   [Wang, Wenying] Nanjing Res Inst Elect Technol, Nanjing 210039, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Wang, WY (corresponding author), Chinese Acad Sci, Inst Comp Technol, Adv Comp Res Lab, Beijing 100190, Peoples R China.
EM xfcaswwy@163.com; dmzhang@ict.ac.cn; zhyd@ict.ac.cn; jtli@ict.ac.cn;
   xggu@ict.ac.cn
RI wang, wy/JDD-3603-2023
OI Zhang, Dongming/0000-0002-1237-7177
FU National Basic Research Program of China (973 Program) [2007CB311100];
   National High Technology and Research Development Program of China (863
   Program) [2007AA01Z416]; National Nature Science Foundation of China
   [60873165, 60802028]; Beijing New Star Project on Science Technology
   [2007B071]; Beijing Municipal Education Commission
FX Manuscript received October 29, 2010; revised April 05, 2011 and June
   03, 2011; accepted July 28, 2011. Date of publication August 15, 2011;
   date of current version November 18, 2011. This work was supported by
   the National Basic Research Program of China (973 Program,
   2007CB311100), National High Technology and Research Development Program
   of China (863 Program, 2007AA01Z416), National Nature Science Foundation
   of China (60873165, 60802028), Beijing New Star Project on Science &
   Technology (2007B071), and Co-building Program of Beijing Municipal
   Education Commission. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Chia-Wen Lin.
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2008, P CVPR
   [Anonymous], 2008, NVIDIA CUDA Programming Guide
   BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P2089, DOI 10.1109/TPAMI.2007.1126
   Chum O., 2003, DAGM S MAGD
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   Csurka Gabriella, 2004, ECCV WORKSH
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Fang Wenbin, 2008, PARALLEL DATA MINING
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   FLICKNER M, 1995, COMPUTER, V28, P23, DOI 10.1109/2.410146
   Harris M., 2008, OPTIMIZING PARALLEL
   Harris M., 2007, GPU GEMS, V3
   James F., 2004, P INT C AC SPEECH SI, P351
   Lampert C., 2009, P INT C COMP VIS
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Meng J., 2010, P INT C ACM MULT, P1147
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Nister David, 2006, CVPR
   Owens JD, 2007, COMPUT GRAPH FORUM, V26, P80, DOI 10.1111/j.1467-8659.2007.01012.x
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sinha S N., 2006, GPU BASED VIDEO FEAT
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sivic J, 2009, IEEE T PATTERN ANAL, V31, P591, DOI 10.1109/TPAMI.2008.111
   Terboven C., 2006, P EUR C COMP VIS
   Yu SX, 2004, IEEE T PATTERN ANAL, V26, P173, DOI 10.1109/TPAMI.2004.1262179
   Yuan J, 2007, PROC CVPR IEEE, P1930, DOI 10.1109/CVPR.2007.383222
   Zheng Q.-F., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, P77
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
NR 40
TC 8
Z9 9
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1308
EP 1318
DI 10.1109/TMM.2011.2165053
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400011
DA 2024-07-18
ER

PT J
AU Zhang, F
   Ma, L
   Li, SN
   Ngan, KN
AF Zhang, Fan
   Ma, Lin
   Li, Songnan
   Ngan, King Ngi
TI Practical Image Quality Metric Applied to Image Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality metric; perceptual coding
ID COSINE TRANSFORM; CONTRAST; COMPRESSION
AB Perceptual image coding requires an effective image quality metric, yet most of the existing metrics are complex and can hardly guide the compression effectively. This paper proposes a practical full-reference metric with consideration of the texture masking effect and contrast sensitivity function. The metric is capable of evaluating typical image impairments in real-world applications and can achieve the comparable performance as the state-of-the-art metrics on the publicly available subjectively-rated image databases. Due to its simplicity, the metric is embedded into JPEG image coding to ensure a better perceptual rate-distortion performance.
C1 [Zhang, Fan; Ma, Lin; Li, Songnan; Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Zhang, F (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
EM fan.zhang@technicolor.com; lma@ee.cuhk.edu.hk; snli@ee.cuhk.edu.hk;
   knngan@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014
OI Ngan, N/0000-0003-1946-3235
FU Chinese University of Hong Kong [1903003]
FX This work was supported in part by a grant from the Chinese University
   of Hong Kong under the Focused Investment Scheme (Project 1903003). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Ton Kalker.
CR Ahumada A. J.  Jr., 1992, Proceedings of the SPIE - The International Society for Optical Engineering, V1666, P365, DOI 10.1117/12.135982
   Altun HO, 2009, IEEE T IMAGE PROCESS, V18, P371, DOI 10.1109/TIP.2008.2008222
   [Anonymous], A57 DATABASE
   [Anonymous], 2009, NATURAL IMAGE STAT P
   [Anonymous], 2006, MODERN IMAGE QUALITY
   [Anonymous], P INT WORKSH VID PRO
   [Anonymous], 2005, Digital Video Quality: Vision Models and Metrics
   [Anonymous], 2010, Wireless Imaging Quality Database
   [Anonymous], MICT image quality evaluation database
   Autrusseau F., 2009, SUBJECTIVE QUALITY A
   Bex PJ, 2007, J VISION, V7, DOI 10.1167/7.12.1
   Chandler D.M., 2010, CSIQ IMAGE DATABASE
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   COX IJ, 2002, DIGITAL WATERMARKING, P216
   Fei Zhang, 2010, 2010 14th Biennial IEEE Conference on Electromagnetic Field Computation (CEFC 2010), DOI 10.1109/CEFC.2010.5481512
   Gao XB, 2009, IEEE T IMAGE PROCESS, V18, P1409, DOI 10.1109/TIP.2009.2018014
   Georgeson M.A., 1990, LIMITS VISION, P106
   ITU, 2002, ITU R REC BT 500 11
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Le Callet P., 2005, Subjective quality assessment irccyn/ivc database
   Lin WS, 2006, IEEE T IMAGE PROCESS, V15, P2513, DOI 10.1109/TIP.2006.877415
   Lin WS, 2005, IEEE T CIRC SYST VID, V15, P900, DOI 10.1109/TCSVT.2005.848345
   LUKAS FXJ, 1982, IEEE T COMMUN, V30, P1679, DOI 10.1109/TCOM.1982.1095616
   MULLEN KT, 1985, J PHYSIOL-LONDON, V359, P381, DOI 10.1113/jphysiol.1985.sp015591
   Narwaria M, 2010, IEEE T NEURAL NETWOR, V21, P515, DOI 10.1109/TNN.2010.2040192
   NGAN KN, 1989, IEEE T ACOUST SPEECH, V37, P1743, DOI 10.1109/29.46556
   NILL NB, 1985, IEEE T COMMUN, V33, P551, DOI 10.1109/TCOM.1985.1096337
   Ninassi A, 2007, IEEE IMAGE PROC, P733
   Pappas TN, 2007, IEEE COMMUN MAG, V45, P44, DOI 10.1109/MCOM.2007.284537
   Pappas TN, 2003, IEEE SIGNAL PROC MAG, V20, P14, DOI 10.1109/MSP.2003.1215228
   Ponomarenko N., 2008, Tampere image database
   Sendashonga M, 2006, IEEE IMAGE PROC, P385, DOI 10.1109/ICIP.2006.312474
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   TEO PC, 1994, IEEE IMAGE PROC, P982, DOI 10.1109/ICIP.1994.413502
   Tong HHY, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P428, DOI 10.1109/ICIP.1998.999032
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Voloshynovskiy S, 2000, LECT NOTES COMPUT SC, V1768, P211
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2007, IEEE IMAGE PROC, P685
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Watson A.B., 1993, SOC INFORM DISPLAY D, Vxxiv, P946
   WATSON AB, DCTUNE 2 0 PERCEPTUA
   Winkler S., 2005, DIGITAL VIDEO IMAGE
NR 45
TC 30
Z9 31
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 615
EP 624
DI 10.1109/TMM.2011.2134079
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300003
DA 2024-07-18
ER

PT J
AU Kang, XG
   Yang, R
   Huang, JW
AF Kang, Xiangui
   Yang, Rui
   Huang, Jiwu
TI Geometric Invariant Audio Watermarking Based on an LCM Feature
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio watermarking; geometric invariance; log coordinate mapping (LCM)
   feature; pitch shifting; time-scaling modification (TSM)
ID SCALE; ROBUST
AB The development of a geometric invariant audio watermarking scheme without degrading acoustical quality is challenging work. This paper proposes a multi-bit spread-spectrum audio watermarking scheme based on a geometric invariant log coordinate mapping (LCM) feature. The LCM feature is very robust to audio geometric distortions. The watermark is embedded in the LCM feature, but it is actually embedded in the Fourier coefficients which are mapped to the feature via LCM, so the embedding is actually performed in the DFT domain without interpolation, thus eliminating completely the severe distortion resulted from the non-uniform interpolation mapping. The watermarked audio achieves high auditory quality in both objective and subjective quality assessments. A mixed correlation between the LCM feature and a key-generated PN tracking sequence is proposed to align the log-coordinate mapping, thus synchronizing the watermark efficiently with only one FFT and one IFFT. Both the theoretical analysis and experimental results show that the proposed audio watermarking scheme is not only resilient against common signal processing operations, including low-pass filtering, MP3 recompression, echo addition, volume change, normalization, test functions in the Stirmark benchmark, and DA/AD conversion, but also has conquered the challenging audio geometric distortion and achieves the best robustness against simultaneous geometric distortions, such as pitch invariant time-scale modification (TSM) by +/- 20%, tempo invariant pitch shifting by +/- 20%, resample TSM with scaling factors between 75% and 140%, and random cropping by 95%. This is mainly contributed by the proposed geometric invariant LCM feature. To our best knowledge, audio watermarking based on LCM has not been reported before.
C1 [Kang, Xiangui; Yang, Rui; Huang, Jiwu] Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510275, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Kang, XG (corresponding author), Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510275, Guangdong, Peoples R China.
EM isskxg@mail.sysu.edu.cn
RI huang, jw/KVY-9917-2024; Kang, Xiangui/AAO-5527-2020
FU NSFC [60633030, 61070167]; 973 Program [2011CB302204]; NSF of Guangdong
   [9151027501000103]
FX This work was supported in part by NSFC (60633030, 61070167), in part by
   973 Program (2011CB302204), and in part by NSF of Guangdong
   (9151027501000103). A portion of this paper was partially presented at
   the International Conference on Digital Watermarking 2008, Busan, Korea,
   and won Best Student Paper award. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof. Ali
   N. Akansu.
CR [Anonymous], Sound Quality Assessment Material recordings for subjective tests
   [Anonymous], P ACM WORKSH MULT
   [Anonymous], 2000, Digital Watermarking
   ARFIB FKD, 2002, DAFX DIGITAL AUDIO E, P232
   ARNOLD M, 2009, P INFORM HIDING, P215
   Kim H. J., 2005, P PAC RIM WORKSH DIG, P1
   Kirovski D, 2003, IEEE T SIGNAL PROCES, V51, P1020, DOI 10.1109/TSP.2003.809384
   Li W, 2006, IEEE T MULTIMEDIA, V8, P60, DOI 10.1109/TMM.2005.861291
   Lin CY, 2001, IEEE T IMAGE PROCESS, V10, P767, DOI 10.1109/83.918569
   Mansour MF, 2005, IEEE T SPEECH AUDI P, V13, P432, DOI 10.1109/TSA.2005.845816
   Mansour MF, 2001, INT CONF ACOUST SPEE, P1353, DOI 10.1109/ICASSP.2001.941179
   O'Ruanaidh J., 1998, Signal Processing, V66, P303, DOI DOI 10.1016/S0165-1684(98)00012-7
   *SDMI, 2000, SDMI PHAS 2 SCREEN T
   Sencar HT, 2007, SIGNAL PROCESS, V87, P877, DOI 10.1016/j.sigpro.2006.08.012
   Steinebach M, 2001, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P49, DOI 10.1109/ITCC.2001.918764
   STEINEBACH M, 2001, P IEEE INT C INF TEC, P100
   SYLVAIN B, 2004, P MULT SEC WORKSH, P117
   Tachibana R, 2002, LECT NOTES COMPUT SC, V2532, P647
   Tachibana R, 2001, P SOC PHOTO-OPT INS, V4314, P104, DOI 10.1117/12.435390
   Wu SQ, 2005, IEEE T BROADCAST, V51, P69, DOI 10.1109/TBC.2004.838265
   Xiang SJ, 2007, IEEE T MULTIMEDIA, V9, P1357, DOI 10.1109/TMM.2007.906580
   Yang R, 2009, LECT NOTES COMPUT SC, V5450, P124, DOI 10.1007/978-3-642-04438-0_11
NR 22
TC 81
Z9 84
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 181
EP 190
DI 10.1109/TMM.2010.2098850
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800002
DA 2024-07-18
ER

PT J
AU Liu, YA
   Mei, T
AF Liu, Yuan
   Mei, Tao
TI Optimizing Visual Search Reranking via Pairwise Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Optimization; pairwise learning; search reranking; visual search
AB Visual search reranking is defined as reordering visual documents (images or video clips) based on the initial search results or some auxiliary knowledge to improve the search precision. Conventional approaches to visual search reranking empirically take the "classification performance" as the optimization objective, in which each visual document is determined relevant or not, followed by a process of increasing the order of relevant documents. In this paper, we first show that the classification performance fails to produce a globally optimal ranked list, and then we formulate reranking as an optimization problem, in which a ranked list is globally optimal only if any arbitrary two documents in the list are correctly ranked in terms of relevance. This is different from existing approaches which simply classify a document as "relevant" or not. To find the optimal ranked list, we convert the individual documents to "document pairs," each represented as a "ordinal relation." Then, we find the optimal document pairs which can maximally preserve the initial rank order while simultaneously keeping the consistency with the auxiliary knowledge mined from query examples and web resources as much as possible. We develop two pairwise reranking methods, difference pairwise reranking (DP-reranking) and exclusion pairwise reranking (EP-reranking), to obtain the relevant relation of each document pair. Finally, a round robin criterion is explored to recover the final ranked list. We conducted comprehensive experiments on an automatic video search task over TRECVID 2005-2007 benchmarks, and showed consistent improvements over text search baseline and other reranking approaches.
C1 [Liu, Yuan] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Mei, Tao] Microsoft Res Asia, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia
RP Liu, YA (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM yuanliu.ustc@gmail.com; tmei@microsoft.com
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307
CR [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2000, ADV LARGE MARGIN CLA
   [Anonymous], RC23612W0505104 IBM
   Burges C. J., 2005, P INT C MACH LEARN
   Cao Z., 2007, P INT C MACH LEARN
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Cossock D, 2008, IEEE T INFORM THEORY, V54, P5140, DOI 10.1109/TIT.2008.929939
   CREMEANT LB, 2003, P IEEE C DEC CONTR
   DUONG D, 2006, P IEEE INT JOINT C N
   Fergus R, 2005, IEEE I CONF COMP VIS, P1816
   GAO S, 2009, P IEEE INT C IM PROC
   Geng B., 2010, P IEEE C COMP VIS PA
   Hsu WinstonH., 2007, ACM MM
   Joachims T, 2002, KDD 2002
   Kennedy L., 2005, P ACM INT C MULT
   Kennedy L., 2007, P ACM INT C IM VID R
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li Xirong, 2007, P ACM INT C IM VID R
   LIN WH, 2003, P IEEE WIC INT C WEB, P13
   Liu Y., 2008, P ACM INT WORKSH MUL, P253
   LIU Y, 2009, P INT MULT MOD C
   Liu Y, 2010, IEEE T CIRC SYST VID, V20, P749, DOI 10.1109/TCSVT.2010.2045801
   Liu Y, 2009, IEEE T CIRC SYST VID, V19, P1841, DOI 10.1109/TCSVT.2009.2026951
   Liu YA, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P500, DOI 10.1145/1571941.1572027
   Mei T., 2007, TREC VID RETR EV ONL
   MOON T, 2010, P ACM INT C WEB SEAR
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   Neo S.-Y., 2006, P ACM INT C IM VID R
   Robertson S.-E., 1997, TR356 CAMBR U COMP L
   Snoek C.G.M., 2007, P IEEE INT C MULT EX
   Taylor M., 2008, P INT ACM C WEB SEAR
   Tian X., 2008, ACM INT C MULTIMEDIA, P131, DOI DOI 10.1145/1459359.1459378.ISBN
   WANG D, 2007, P ACM MULT
   XIA F, 2008, P INT C MACH LEARN
   Yan R, 2003, LECT NOTES COMPUT SC, V2728, P238
   YAN R, 2004, P ACM INT C MULT
   YAO T, 2010, P ACM INT C IM VID R
   YUE Y, 2007, P ACM SPEC INT GROUP
   Zha Z. J., 2009, P ACM INT C MULT
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu X.J, 2003, P 20 INT C MACH LEAR
NR 41
TC 23
Z9 24
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 280
EP 291
DI 10.1109/TMM.2010.2103931
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800010
DA 2024-07-18
ER

PT J
AU Fernandez, JC
   Taleb, T
   Guizani, M
   Kato, N
AF Fernandez, Juan Carlos
   Taleb, Tarik
   Guizani, Mohsen
   Kato, Nei
TI Bandwidth Aggregation-Aware Dynamic QoS Negotiation for Real-Time Video
   Streaming in Next-Generation Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bandwidth aggregation; mobile video streaming; packet scheduling; QoS
   negotiation; wireless networks
ID PERFORMANCE ANALYSIS; MOBILITY; FRAMEWORK; IP
AB In next generation wireless networks, Internet service providers (ISPs) are expected to offer services through several wireless technologies (e.g., WLAN, 3G, WiFi, and WiMAX). Thus, mobile computers equipped with multiple interfaces will be able to maintain simultaneous connections with different networks and increase their data communication rates by aggregating the bandwidth available at these networks. To guarantee quality-of-service (QoS) for these applications, this paper proposes a dynamic QoS negotiation scheme that allows users to dynamically negotiate the service levels required for their traffic and to reach them through one or more wireless interfaces. Such bandwidth aggregation (BAG) scheme implies transmission of data belonging to a single application via multiple paths with different characteristics, which may result in an out-of-order delivery of data packets to the receiver and introduce additional delays for packets reordering.
   The proposed QoS negotiation system aims to ensure the continuity of QoS perceived by mobile users while they are on the move between different access points, and also, a fair use of the network resources. The performance of the proposed dynamic QoS negotiation system is investigated and compared against other schemes. The obtained results demonstrate the outstanding performance of the proposed scheme as it enhances the scalability of the system and minimizes the reordering delay and the associated packet loss rate.
C1 [Fernandez, Juan Carlos; Kato, Nei] Tohoku Univ, GSIS, Sendai, Miyagi 9808579, Japan.
   [Taleb, Tarik] NEC Europe Ltd, Heidelberg, Germany.
   [Guizani, Mohsen] Kuwait Univ, Dept Informat Sci, Coll Women, Safat 13060, Kuwait.
C3 Tohoku University; NEC Corporation; Kuwait University
RP Fernandez, JC (corresponding author), Tohoku Univ, GSIS, Sendai, Miyagi 9808579, Japan.
EM carlos@it.ecei.tohoku.ac.jp; tarik.taleb@nw.neclab.eu;
   mguizani@ieee.org; kato@it.ecei.tohoku.ac.jp
RI KATO, NEI/T-5892-2019; Guizani, Mohsen/AAX-4534-2021; Taleb,
   Tarik/ABD-6339-2021
OI KATO, NEI/0000-0001-8769-302X; Guizani, Mohsen/0000-0002-8972-8094; 
CR Akyildiz IF, 2004, IEEE WIREL COMMUN, V11, P16, DOI 10.1109/MWC.2004.1325888
   Akyildiz IR, 2004, IEEE ACM T NETWORK, V12, P1021, DOI 10.1109/TNET.2004.838604
   [Anonymous], COMPUT COMMUN
   [Anonymous], 2000, 2960 RFC
   Banerjee N, 2003, IEEE WIREL COMMUN, V10, P54, DOI 10.1109/MWC.2003.1241101
   BARRY M, 2001, P IEEE INFOCOM 2001
   Chebrolu K, 2006, IEEE T MOBILE COMPUT, V5, P388, DOI 10.1109/TMC.2006.1599407
   CHEN JC, 2002, P ICC 2002 NEW YORK
   COLAS JA, 2005, CONNECTING AMBIENT N
   FERNANDEZ JC, 1927, INT J BUS DAT COMM N
   FERNANDEZ JC, 2007, P WCNC 2007 HONG KON
   Fracchia R, 2007, IEEE T MOBILE COMPUT, V6, P1130, DOI 10.1109/TMC.2007.1027
   Fu SJ, 2004, IEEE COMMUN MAG, V42, P64, DOI 10.1109/MCOM.2004.1284931
   Jain M, 2003, IEEE ACM T NETWORK, V11, P537, DOI 10.1109/TNET.2003.815304
   Jurca D, 2007, IEEE T MULTIMEDIA, V9, P1227, DOI 10.1109/TMM.2007.902852
   Kramer G, 2001, PHOTONIC NETW COMMUN, V3, P307, DOI 10.1023/A:1011463617631
   Liu M, 2008, IEEE T MOBILE COMPUT, V7, P846, DOI 10.1109/TMC.2007.70768
   MAGALHAES L, 2001, P 1 ACM WORKSH DAT C
   Mohanty S, 2007, IEEE T MOBILE COMPUT, V6, P731, DOI 10.1109/TMC.2007.1040
   Nguyen TMT, 2002, IEEE COMMUN MAG, V40, P158, DOI 10.1109/35.1000230
   Ramjee R, 2002, IEEE ACM T NETWORK, V10, P396, DOI 10.1109/TNET.2002.1012370
   Sarangan V, 2006, IEEE COMMUN MAG, V44, P151, DOI 10.1109/MCOM.2006.1607879
   TALEB T, 2007, P IEEE GLOB INF INFR
   TALEB T, 2008, P IEEE ICC 08 BEIJ C
   Taleb T, 2008, IEEE T VEH TECHNOL, V57, P3801, DOI 10.1109/TVT.2008.918727
   *TEQ CONS, 2001, SRNP SERV NEG PROT
   Wang X, 2000, IEEE J SEL AREA COMM, V18, P2514, DOI 10.1109/49.898734
   WARD C, 2002, P 3 INT C E COMM EC
   YU LMF, 2004, IEEE WIREL COMMUN, V11, P44
   2006, MPEG 4 H 263 VIDEO T
   CB LBNL VINT NETWORK
NR 31
TC 47
Z9 54
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1082
EP 1093
DI 10.1109/TMM.2009.2026086
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700005
DA 2024-07-18
ER

PT J
AU Nair, P
   Cavallaro, A
AF Nair, Prathap
   Cavallaro, Andrea
TI 3-D Face Detection, Landmark Localization, and Registration Using a
   Point Distribution Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face detection; face meshes; landmark localization; registration; shape
   model
ID ACTIVE SHAPE MODELS; REPRESENTATION SCHEME; 3D; RECOGNITION
AB We present an accurate and robust framework for detecting and segmenting faces, localizing landmarks, and achieving fine registration of face meshes based on the fitting of a facial model. This model is based on a 3-D Point Distribution Model (PDM) that is fitted without relying on texture, pose, or orientation information. Fitting is initialized using candidate locations on the mesh, which are extracted from low-level curvature-based feature maps. Face detection is performed by classifying the transformations between model points and candidate vertices based on the upper-bound of the deviation of the parameters from the mean model. Landmark localization is performed on the segmented face by finding the transformation that minimizes the deviation of the model from the mean shape. Face registration is obtained using prior anthropometric knowledge and the localized landmarks. The performance of face detection is evaluated on a database of faces and non-face objects where we achieve an accuracy of 99.6%. We also demonstrate face detection and segmentation on objects with different scale and pose. The robustness of landmark localization is evaluated with noisy data and by varying the number of shapes and model points used in the model learning phase. Finally, face registration is compared with the traditional Iterative Closest Point (ICP) method and evaluated through a face retrieval and recognition framework on the GavabDB dataset, where we achieve a recognition rate of 87.4 % and a retrieval rate of 83.9 %.
C1 [Nair, Prathap; Cavallaro, Andrea] Univ London, Multimedia & Vis Grp, London E1 4NS, England.
C3 University of London
RP Nair, P (corresponding author), Univ London, Multimedia & Vis Grp, London E1 4NS, England.
EM prathap.nair@elec.qmul.ac.uk; andrea.cavallaro@elec.qmul.ac.uk
FU Department of Computer Science; The State University of New York at
   Binghamton; The Research Foundation of State University of New York
FX The authors would like to thank Dr. L. Yin, Department of Computer
   Science, The State University of New York at Binghamton, and The
   Research Foundation of State University of New York, for providing the
   BU-3DFE database.
CR Achermann B, 2000, INT C PATT RECOG, P809, DOI 10.1109/ICPR.2000.906199
   [Anonymous], 2006, P IEEE COMP SOC C CO
   [Anonymous], 1997, THESIS CARNEGIE MELL
   [Anonymous], 2004, WORKSH BIOMN INT COS
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   AYYAGARI V, 2005, P IEEE C COMP VIS PA, P119
   Bergevin R, 1996, IEEE T PATTERN ANAL, V18, P540, DOI 10.1109/34.494643
   BES PJ, 1992, IEEE T PATTERN ANAL, V14, P239
   Boehnen C, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P135
   Buchaillard S, 2004, IEEE IMAGE PROC, P1077
   Chang KI, 2006, IEEE T PATTERN ANAL, V28, P1695, DOI 10.1109/TPAMI.2006.210
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   COLBRY D, 2006, P IEEE C COMP VIS PA, P118
   Colombo A, 2006, PATTERN RECOGN, V39, P444, DOI 10.1016/j.patcog.2005.09.009
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   de Bruijne M, 2003, LECT NOTES COMPUT SC, V2732, P136
   Dickens MM, 2002, FIFTH IEEE SOUTHWEST SYMPOSIUM ON IMAGE ANALYSIS AND INTERPRETATION, PROCEEDINGS, P248, DOI 10.1109/IAI.2002.999927
   Dorai C, 1997, IEEE T PATTERN ANAL, V19, P1115, DOI 10.1109/34.625113
   Fitzgibbon A., 2001, PROC BRIT MACHINE VI, P411
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Gordon G. G., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P808, DOI 10.1109/CVPR.1992.223253
   Hutton T., 2003, British Machine Vision Conference, P439
   Kakadiaris IA, 2007, IEEE T PATTERN ANAL, V29, P640, DOI 10.1109/TPAMI.2007.1017
   Lu XG, 2004, LECT NOTES COMPUT SC, V3072, P30
   Lu XG, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P585
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Mian AS, 2007, IEEE T PATTERN ANAL, V29, P1927, DOI 10.1109/TPAMI.2007.1105
   MORENO AB, 2003, P IR MACH VIS IM PRO
   MOSS JP, 1989, OPT LASER ENG, V10, P179, DOI 10.1016/0143-8166(89)90036-5
   NAIR P, 2007, P IEEE INT C IM PROC
   NAIR P, 2005, P EUR WORKSH INT KNO, P77
   Niese Robert, 2007, Journal of Multimedia, V2, P1, DOI 10.4304/jmm.2.5.1-12
   Pan G, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P193
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   Shi J, 2006, COMPUT VIS IMAGE UND, V102, P117, DOI 10.1016/j.cviu.2005.10.002
   Typke R, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1793, DOI 10.1109/ICME.2006.262900
   Wang YJ, 2002, PATTERN RECOGN LETT, V23, P1191, DOI 10.1016/S0167-8655(02)00066-1
   Wang YM, 2000, PROC CVPR IEEE, P644, DOI 10.1109/CVPR.2000.854933
   Xiao J, 2004, PROC CVPR IEEE, P535
   Yamany SM, 2002, IEEE T PATTERN ANAL, V24, P1105, DOI 10.1109/TPAMI.2002.1023806
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149
   Zhao Z, 2005, PROC SPIE, V5747, P303, DOI 10.1117/12.594736
NR 44
TC 94
Z9 112
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 611
EP 623
DI 10.1109/TMM.2009.2017629
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, B
   Cui, Y
   Lu, YS
   Xue, Y
AF Liu, Bo
   Cui, Yi
   Lu, Yansheng
   Xue, Yuan
TI Locality-Awareness in BitTorrent-Like P2P Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE BitTorrent; downloading; locality-aware; peer-to-peer; streaming
AB This paper presents the measurement study of locality-aware P2P solutions over real-world Internet autonomous systems (AS) topology. By using the accesses of nodes of PlanetLab testbed, we create a detailed AS-level map including the end-to-end path of all nodes, as well as the relationship of all involved ASes. Based on this map, we evaluate the performance of a set of locality-aware P2P solutions, including an optimal solution guaranteeing the minimum AS hop count, as well as modified BitTorrent system with locality-awareness built into its neighbor selection, peer choking/unchoking, and piece selection processes. Our findings suggest that locality-awareness can help existing P2P solution to significantly decrease load on Internet, and achieve shorter downloading time. By comparing the performance of different kinds of locality-aware and traditional BitTorrent systems, we also point out the necessity to tradeoff between the goals of optimizing AS-related performance and achieving fairness among peers such as intra-AS traffic and peer burden fairness.
C1 [Liu, Bo; Lu, Yansheng] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
   [Cui, Yi; Xue, Yuan] Vanderbilt Univ, Dept Comp Sci, Nashville, TN 37240 USA.
C3 Huazhong University of Science & Technology; Vanderbilt University
RP Liu, B (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
EM newpoo@smail.hust.edu.cn; yi.cui@vanderbilt.edu; lys@smail.hust.edu.cn;
   yuan.xue@vanderbilt.edu
FU NSF [0643489]; Vanderbilt Discovery grant; Microsoft Research
FX Manuscript received May 29, 2008; revised November 23, 2008. Current
   version published March 18, 2009. This work was supported in part by NSF
   award 0643489, in part by a Vanderbilt Discovery grant, and in part by a
   gift from Microsoft Research. Views and conclusions of this paper are
   those of authors, which should not be interpreted as representing the
   official policies, either expressed or implied, of the funding agencies.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Lexing Xie.
CR BHARAMBE A, 2005, P ACM SIGMETRIC
   BINDAL R, 2006, P ICDCS
   *BITTORRENT, BITTORRENT DNA
   CASTRO M, 2003, P ACM S OP SYST PRIN
   CHOE Y, 2007, ACM MULTIMEDIA, P117
   CHOFFNES DR, 2008, P SIGCOMM
   Chu Y., 2000, P ACM SIGMETRICS
   Cui Y, 2004, IEEE J SEL AREA COMM, V22, P91, DOI 10.1109/JSAC.2003.818799
   DANA C, 2005, P IEEE MMSP
   DIMITROPOULOS X, 2007, P ACM SIGCOMM
   DONGYU Q, 2004, P 2004 C APPL TECHN, P367
   Gao LX, 2001, IEEE ACM T NETWORK, V9, P733, DOI 10.1109/90.974527
   GKANTSIDIS C, 2006, P IPTPS
   Guo L., 2005, P IMC
   Huang C., 2007, P ACM SIGCOMM
   JIN S, 2002, P INT WORKSH NETW GR
   KARAGIANNIS T, 2005, P INT MEAS C
   KOSTIC D, 2003, P ACM S OP SYST PRIN
   LAURENT M, 2005, P 2005 ACM SIGMETRIC, V33, P2
   LI J, 2007, ACM SPRINGER MULTIME, P173
   Padmanabhan V.N., 2002, P 12 INT WORKSHOP NE, P177
   REN S, 2006, P ICDCS
   Spring N, 2004, IEEE ACM T NETWORK, V12, P2, DOI 10.1109/TNET.2003.822655
   Vlavianos A., 2006, P IEEE INFOCOM, P1
   Wu C, 2008, IEEE T PARALL DISTR, V19, P77, DOI 10.1109/TPDS.2007.1119
   XIE H, 2008, P SIGCOMM
   YANG X, 2004, P INFOCOM
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 28
TC 26
Z9 29
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 361
EP 371
DI 10.1109/TMM.2009.2012911
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300003
DA 2024-07-18
ER

PT J
AU Chen, HY
   Liu, KY
AF Chen, Herng-Yow
   Liu, Kuo-Yu
TI WMA: A Marking-Based Synchronized Multimedia Tutoring System for English
   Composition Studies
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; hypertext systems; multimedia computing; navigation;
   synchronization
AB This paper presents a system, Web-based Multimedia Annotation (WMA) system, for English as Foreign Language learning in writing skills. The whole correcting process, including the instructor's voice and navigation events (i.e., tele-pointer (cursor), highlight, pen strokes, markings and annotations), can be captured through our system for later access. We address the issues of exploring involved media correlation to benefit adaptable presentation in a synchronization manner from temporal, spatial and content domains. The proposed computed synchronization techniques include speech-event binding process in the temporal domain, tele-pointer movement interpolation and adaptable handwriting presentation in the spatial domain, and visualized annotation erasing in the content domain. The experimental results show that in the speech-event binding process 74% of speech access entries for accessible visualized annotations are found. The acceptable rate of human perception of tele-pointer movement is higher than 85% if time interval is selected carefully. The accuracy of visualized annotation erasing for content removal is about 71%. Our user study shows that students can devote their efforts to writing practice because they can better understand their own mistakes corrected by the instructors using this multimedia presentation.
C1 [Chen, Herng-Yow] Natl Chi Nan Univ, Dept Comp Sci & Informat Engn, Puli, Nantou, Taiwan.
   [Liu, Kuo-Yu] Providence Univ, Dept Comp Sci & Commun Engn, Taichung 43301, Taiwan.
C3 National Chi Nan University; Providence University - Taiwan
RP Chen, HY (corresponding author), Natl Chi Nan Univ, Dept Comp Sci & Informat Engn, Puli, Nantou, Taiwan.
EM hychen@csie.ncnu.edu.tw; kyliu@pu.edu.tw
OI Chen, Herng-Yow/0009-0006-3664-0706
CR Agius HW, 2001, MULTIMED TOOLS APPL, V15, P5, DOI 10.1023/A:1011386102507
   Allan J., 1998, P DARPA BROADC NEWS
   [Anonymous], SYNCHRONIZED MULTIME
   Blakowski G, 1996, IEEE J SEL AREA COMM, V14, P5, DOI 10.1109/49.481691
   Brotherton J. A., 2004, ACM Transactions on Computer-Human Interaction, V11, P121, DOI 10.1145/1005361.1005362
   Chen HY, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P887, DOI 10.1109/MMCS.1999.778605
   Chu WT, 2005, MULTIMEDIA SYST, V10, P183, DOI 10.1007/s00530-004-0150-7
   GINIGE A, 1995, IEEE MULTIMEDIA, V2, P24, DOI 10.1109/93.482293
   HARDMAN LM, THESIS U AMSTERDAM A
   KNOY T, 2000, EDITING WORKBOOK CHI
   Müller R, 2000, MULTIMEDIA SYST, V8, P158, DOI 10.1007/s005300000042
   MULLER R, 2002, P 13 INT C INF RES M, P567
   OWEN CB, 1998, PCSTR98335
   STIFELMAN L, 1997, THESIS MIT MEDIA LAB
NR 14
TC 2
Z9 2
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 324
EP 332
DI 10.1109/TMM.2008.2009721
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800012
DA 2024-07-18
ER

PT J
AU Li, Y
   Markopoulou, A
   Apostolopoulos, J
   Bambos, N
AF Li, Yan
   Markopoulou, Athina
   Apostolopoulos, John
   Bambos, Nicholas
TI Content-aware playout and packet scheduling for video streaming over
   wireless links
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive media playout; cross-layer optimization; multimedia delivery
   over wireless networks; network control; packet scheduling; video-aware
   adaptation and communication
AB Media streaming over wireless links is a challenging problem due to both the unreliable, time-varying nature of the wireless channel and the stringent delivery requirements of media traffic. In this paper, we use joint control of packet scheduling at the transmitter and content-aware playout at the receiver, so as to maximize the quality of media streaming over a wireless link. Our contributions are twofold. First, we formulate and study the problem of joint scheduling and playout control in the framework of Markov decision processes. Second, we propose a novel content-aware adaptive playout control, that takes into account the content of a video sequence, and in particular the motion characteristics of different scenes. We find that the joint scheduling and playout control can significantly improve the quality of the received video, at the expense of only a small amount of playout slowdown. Furthermore, the content-aware adaptive playout places the slowdown preferentially in the low-motion scenes, where its perceived effect is lower.
C1 [Li, Yan] Qualcomm, Campbell, CA 95008 USA.
   [Markopoulou, Athina] Univ Calif Irvine, Dept Elect Engn & Comp Sci, Irvine, CA 92697 USA.
   [Apostolopoulos, John] Hewlett Packard Labs, Streaming Media Grp, Palo Alto, CA 94304 USA.
   [Bambos, Nicholas] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Li, Yan] Texas Instruments Inc, San Jose, CA USA.
C3 Qualcomm; University of California System; University of California
   Irvine; Hewlett-Packard; Stanford University; Texas Instruments
RP Li, Y (corresponding author), Qualcomm, Campbell, CA 95008 USA.
EM liyan@stanfordalumni.org; athina@uci.edu; japos@hpi.hp.com;
   bambos@stanford.edu
OI Markopoulou, Athina/0000-0003-1803-8675; Bambos,
   Nicholas/0000-0001-9250-4553
CR [Anonymous], 2005, IEEE WIRELESS CO AUG
   [Anonymous], 1999, 80211 IEEE WG 11
   Barry RA, 2004, IEEE SIGNAL PROC MAG, V21, P59, DOI 10.1109/MSP.2004.1328089
   BERTSEKAS DP, 1995, DYNAMIC PROGRAMMING, V2
   Cabrera J, 2002, IEEE T CIRC SYST VID, V12, P496, DOI 10.1109/TCSVT.2002.800306
   Chakareski J, 2005, IEEE T CIRC SYST VID, V15, P1257, DOI 10.1109/TCSVT.2005.854227
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Farber N., 2000, Compressed Video over Networks
   GIROD B, 2002, P IWDC 2002 CAPR IT
   Hartwell J., 2004, P IEEE VEH TECHN C V
   *IEEE, 2005, P80211ED13 IEEE
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   Kalman M, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P189, DOI 10.1109/ICIP.2002.1038937
   KALMAN M, 2004, PACK VID WORKSH IRV
   KARANDE S, 2003, P IEEE ICME 2003 BAL
   LI Y, 2004, P IEEE PACK VID IRV
   LI Y, 2005, P IEEE MMSP 2005 SPE
   Li Y, 2006, IEEE T MULTIMEDIA, V8, P830, DOI 10.1109/TMM.2006.876236
   Li YM, 2004, Proceedings of 2004 Chinese Control and Decision Conference, P560
   Liang YJ, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P684
   Liang YJ, 2003, IEEE T MULTIMEDIA, V5, P532, DOI 10.1109/TMM.2003.819095
   MARKOPOULOU A, INFORM SUBJECTIVE TE
   MCDOUGALL J, 2003, P GLOBECOM 2003 SAN
   Moon SB, 1998, MULTIMEDIA SYST, V6, P17, DOI 10.1007/s005300050073
   RAMJEE R, 1994, IEEE INFOCOM SER, P680, DOI 10.1109/INFCOM.1994.337672
   Setton E, 2005, IEEE WIREL COMMUN, V12, P59, DOI 10.1109/MWC.2005.1497859
   SETTON E, 2004, P IEEE INT WORKSH MU
   Shankar S., 2004, P IEEE PACK VID 2004
   SU GM, 2006, IEEE NETWORK MAG MAR
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   Verhelst W., 1993, PROC IEEE INT C ACOU, V2, P554
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   2006, IEEE NETWORK MAG MAR
NR 33
TC 56
Z9 86
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 885
EP 895
DI 10.1109/TMM.2008.922860
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ehara, H
   Yoshida, K
AF Ehara, Hiroyuki
   Yoshida, Koji
TI Decoder initializing technique for improving frame-erasure resilience of
   a CELP speech codec
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE CELP; error resilience; frame erasure; packet loss; speech coding
ID LOSS-RECOVERY TECHNIQUES
AB The authors present and evaluate a technique for synchronizing the internal states of a code-excited-linear-prediction (CELP) encoder and decoder after the occurrence of frame erasure. The designed technique, called "duplicated transmission (DT);" uses some redundant information for realizing synchronization. The encoder performs encoding processes twice and sends two codes for each frame. One code is encoded by an encoder that is initialized. The code is used in cases where the previous frame is erased. An onset detector is combined with the DT technique to select the frames to which the DT should be applied. Subjective test results suggest that, by introducing DT selectively, the number of DT frames is reducible by about 80% without degrading the subjective quality. Results demonstrate that synchronization of the internal states is effective in cases of erasure of onset. The DT technique requires no additional algorithmic delay. For that reason, it would a better choice for particular applications for which the delay has a significant impact.
C1 [Ehara, Hiroyuki; Yoshida, Koji] Matsushita Elect Ind Co Ltd Panason, Next Generat Mobile Commun Dev Ctr, Yokosuka, Kanagawa 2390847, Japan.
C3 Panasonic
RP Ehara, H (corresponding author), Matsushita Elect Ind Co Ltd Panason, Next Generat Mobile Commun Dev Ctr, Yokosuka, Kanagawa 2390847, Japan.
EM ehara.hiroyuki@jp.panasonic.com; yoshida.koji@jp.panasonic.com
CR *3GPP, 2002, 3GTS26091V500200206
   *3GPP, 2002, 3GTS26090V500200206
   Chua TK, 2006, IEEE NETWORK, V20, P14, DOI 10.1109/MNET.2006.273116
   EHARA H, 2003, FORUM INFORM TECHNOL, P237
   MONTMINY C, 2000, INT C MULT EXP 2000, V1, P433
   Perkins C, 1998, IEEE NETWORK, V12, P40, DOI 10.1109/65.730750
   Schroeder M., 1985, IEEE International Conference on Acoustics, Speech, and Signal Processing, V10, P937
   Shah A, 1996, ELECTRON LETT, V32, P95, DOI 10.1049/el:19960032
   Wah BW, 2000, INTERNATIONAL SYMPOSIUM ON MULTIMEDIA SOFTWARE ENGINEERING, PROCEEDINGS, P17, DOI 10.1109/MMSE.2000.897185
NR 9
TC 5
Z9 6
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 549
EP 553
DI 10.1109/TMM.2008.917411
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100022
DA 2024-07-18
ER

PT J
AU Lu, L
   Hanjalic, A
AF Lu, Lie
   Hanjalic, Alan
TI Audio keywords discovery for text-like audio content analysis and
   retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio content mining; audio element; audio keywords; content-based audio
   analysis; key audio element; knowledge discovery
AB Inspired by classical text document analysis employing the concept of (key) words, this paper presents an unsupervised approach to discover (key) audio elements in general audio documents. The (key) audio elements can be considered the equivalents of the text (key) words, and enable content-based audio analysis and retrieval following the analogy to the proven text analysis theories and methods. Since general audio signals usually show complicated and strongly varying distribution and density in the feature space, we propose an iterative spectral clustering method with context-dependent scaling factors to decompose an audio data stream into audio elements. Using this clustering method, temporal signal segments with similar low-level features are grouped into natural clusters that we adopt as audio elements. To detect those audio elements that are most representative for the semantic content, that is, the key audio elements, two cases are considered. First, if only one audio document is available for analysis, a number of heuristic importance indicators are defined and employed to detect the key audio elements. For the case that multiple audio documents are available, more sophisticated measures for audio element importance, including expected term frequency (ETF), expected inverse document frequency (EIDF), expected term duration (ETD) and expected inverse document duration (EIDD), are proposed. Our experiments showed encouraging results regarding the quality of the obtained (key) audio elements and their potential applicability for content-based audio document analysis and retrieval.
C1 [Lu, Lie] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Hanjalic, Alan] Delft Univ Technol, Dept Mediamat, NL-2628 CD Delft, Netherlands.
C3 Microsoft Research Asia; Microsoft; Delft University of Technology
RP Lu, L (corresponding author), Microsoft Res Asia, Hai Dian Dist, Beijing 100080, Peoples R China.
EM llu@microsoft.com; A.Hanjalic@ewi.tudelft.nl
OI Hanjalic, Alan/0000-0002-5771-2549
CR Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Cai R, 2006, IEEE T AUDIO SPEECH, V14, P1026, DOI 10.1109/TSA.2005.857575
   Cai R, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P37
   CAI R, 2004, P IEEE ICASSP MONTR, V4, P345
   CAI R, 2005, P ACM MULT 05, P628
   Cheng W.-H., 2003, P 5 ACM SIGMM INT WO, P109, DOI DOI 10.1145/973264.973282
   Duda R., 1973, Pattern Classification and Scene Analysis
   Fanti C, 2004, ADV NEUR IN, V16, P1603
   LIU Z, 1998, J VLSI SIGNAL PR JUN
   Lu L, 2002, IEEE T SPEECH AUDI P, V10, P504, DOI 10.1109/TSA.2002.804546
   LU L, 2005, P 30 IEEE INT C AC S, V2, P1069
   LU L, 2006, P ICASSP06, V5, P17
   Moncrieff S., 2001, P IEEE INT C MULT EX, P989
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   RADHAKRISHNAN R, 2004, P 6 ACM SIGMM INT WO, P157
   Scott GuyL., 1990, BRIT MACHINE VISION, P103
   Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407
   Sundaram H., 2000, Proceedings ACM Multimedia 2000, P95, DOI 10.1145/354384.354440
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   Weiss Y., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P975, DOI 10.1109/ICCV.1999.790354
   Xu M, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P281
   Yu SX, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P313, DOI 10.1109/iccv.2003.1238361
NR 23
TC 21
Z9 26
U1 2
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 74
EP 85
DI 10.1109/TMM.2007.911304
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200008
DA 2024-07-18
ER

PT J
AU Shu, HY
   Chan, LP
AF Shu, Haiyan
   Chan, Lap-Pui
TI Intra/inter macroblock mode decision for error-resilient transcoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE channel distortion propagation; intra refreshment; rate-distortion
   optimization; video transcoding
ID VIDEO; CONCEALMENT
AB When transmitting the precoded bitstream over an error-prone network, error-resilient transcoding is adopted to convert the bitstream to a resilient format for robust delivery. Intra refreshment is an efficient tool to reduce the dependency between frames and stop the channel error propagation. In the conventional scheme, the rate-distortion optimized macroblock mode decision is employed to adaptively determine the coding mode of each macroblock. However, this scheme only considers the channel error propagated from the previous frames to the current frame. As opposed to this traditional algorithm, this paper proposes a method which considers consecutive two frames in a sequence, thus taking the error propagation to the following frame into account. This enhances the overall robustness of the transcoded bitstream against the packet loss. Considering the availability of the next frame information, two cases are discussed respectively. Experimental results show that the proposed methods present quality improvement when compared with the conventional rate-distortion optimized error-resilient coding scheme under different test environments, and the PSNR improvement can reach as high as 0.9 dB.
C1 [Shu, Haiyan; Chan, Lap-Pui] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Shu, HY (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
EM hyshu@ee.cuhk.edu.hk; elpchau@ntu.edu.sg
RI Chau, Lap-Pui/A-5149-2011
OI Chau, Lap-Pui/0000-0003-4932-0593
CR [Anonymous], IEEE J SEL AREAS COM
   Baccichet P, 2005, IEEE T CONSUM ELECTR, V51, P227, DOI 10.1109/TCE.2005.1405724
   Belfiore S, 2005, IEEE T MULTIMEDIA, V7, P316, DOI 10.1109/TMM.2005.843347
   Chen WHJ, 2001, IEEE T CIRC SYST VID, V11, P974, DOI 10.1109/76.937447
   Chiou HJ, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 3, PROCEEDINGS, P777
   Côté G, 2000, IEEE J SEL AREA COMM, V18, P952, DOI 10.1109/49.848249
   Dogan S, 2002, IEEE T CIRC SYST VID, V12, P453, DOI 10.1109/TCSVT.2002.800308
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   Xia MH, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 3, PROCEEDINGS, P945
   Xin J, 2005, P IEEE, V93, P84, DOI 10.1109/JPROC.2004.839620
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   2001, VIDEO CODING LOW BIT
NR 14
TC 11
Z9 14
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 97
EP 104
DI 10.1109/TMM.2007.911300
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200010
DA 2024-07-18
ER

PT J
AU Cheng, I
   Basu, A
AF Cheng, Irene
   Basu, Anup
TI Perceptually optimized 3-D transmission over wireless networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT ACM SIGGRAPH 2005 Conference
CY JUL 31-AUG 04, 2005
CL Los Angeles, CA
SP ACM SIGGRAPH
DE 3-D transmission; packet loss; perceptual quality
ID TEXTURE; ERROR
AB Many protocols optimized to transmissions over wireless networks have been proposed. However, one issue that has not been looked into is considering human perception in deciding a transmission strategy for three-dimensional (3-D) objects. Several factors, such as the number of vertices and the resolution of texture, can affect the display quality of 3-D objects. When the resources of a graphics system are not sufficient to render the ideal image, degradation is inevitable. It is therefore important to study how individual factors affect the overall quality, and how the degradation can be controlled given limited bandwidth resources and possibility of data loss. In this paper, the essential factors determining the display quality are reviewed. We provide an overview of our research on designing a 3-D perceptual quality metric integrating two important ones, resolution of texture and resolution of mesh, that control the transmission bandwidth requirements. A review of robust mesh transmission considering packet loss is presented, followed by a discussion of the difference of existing literature with our problem and approach. We then suggest alternative strategies for packet transmission of both 3-D texture and mesh. These strategies are then compared with respect to preserving 3-D perceptual quality under packet loss.
C1 Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
C3 University of Alberta
RP Cheng, I (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
EM lin@cs.ualberta.ca; anup@cs.ualberta.ca
CR AlRegib G, 2005, IEEE T MULTIMEDIA, V7, P766, DOI 10.1109/TMM.2005.850981
   Alregib G, 2005, ACM T GRAPHIC, V24, P182, DOI 10.1145/1061347.1061349
   ALREGIB G, 2002, P IEEE INT C IM PROC
   [Anonymous], IEEE VISUALIZATION
   [Anonymous], P SIGGRAPH
   AYANOGLU E, 1995, ACM BALTZER WIRELESS, V1, P47
   BAKRE A, 1995, INT CON DISTR COMP S, P136, DOI 10.1109/ICDCS.1995.500012
   BAKSHI BS, 1997, P INT C DISTR COMP S
   BALAKRISHNAN H, 1995, 1 ACM INT C MOB COMP
   Balakrishnan H, 1995, WIREL NETW, V1, P469, DOI 10.1007/BF01985757
   BALMELLI L, 2002, EUR 02 C P, P411
   BASU A, 1998, IEEE T SYST MAN CYBE
   Baudisch P, 2003, COMMUN ACM, V46, P60, DOI 10.1145/636772.636799
   BHANDARKAR S, 2004, IEEE T MOBILE CO FEB
   Bolin M. R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P299, DOI 10.1145/280814.280924
   BOULANGER IC, 2005, P SIGGRAPH
   BRODSKY D, 2000, GRAPHICS INTERFACE
   Brown K., 1997, Computer Communication Review, V27, P19, DOI 10.1145/269790.269794
   CACERES R, 1995, IEEE J SEL AREA COMM, V13, P850, DOI 10.1109/49.391749
   Chen Z., 2003, WEB3D, P161
   Cheng I, 2005, IEEE T CIRC SYST VID, V15, P1234, DOI 10.1109/TCSVT.2005.854234
   CHENG I, 2004, SIGGRAPH 2004 RES AB
   CHENG I, 2006, FRONTIERS CS CSE ED, P7
   CHENG I, 2006, IEEE P INT C MULT EX
   CHENG I, 2006, P EUROGRAPHICS
   CHENG I, 2005, P EUROGRAPHICS
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cignoni P, 1998, COMPUT GRAPH-UK, V22, P37, DOI 10.1016/S0097-8493(97)00082-4
   Cignoni P, 1997, VISUAL COMPUT, V13, P199, DOI 10.1007/s003710050099
   CIGNONI P, 1997, COMPUTERS GRAPHICS
   COHEN J, 1998, P SIGGRAPH
   COHENOR D, 1999, P SIGGRPAH
   De Floriani L., 2000, GeoInformatica, V4, P287, DOI 10.1023/A:1009805426595
   DEERING M, 1995, P 22 ANN C COMP GRAP, P13
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Elaarag H, 2002, ACM COMPUT SURV, V34, P357, DOI 10.1145/568522.568524
   FALL K, 1996, COMPUT COMMUN REV, P5
   Ferwerda J. A., 1997, Proc. ACM SIGGRAPH, P143
   Fitzek FHP, 2001, IEEE J SEL AREA COMM, V19, P2015, DOI 10.1109/49.957315
   FOWLER RJ, 1979, P SIGGRAPH 79, P199
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   GARLAND M, P IEEE VISUALIZATION, V98
   GELASCA ED, 2005, P IEEE INT C IM PROC
   Haeberli Paul., 1993, Texture Mapping as a Fundamental Drawing Primitive
   Heckbert PS., 1997, SURVEY POLYGONAL SUR
   Hoe J.C., 1996, P ACM SIGCOMM
   Hoppe H., 1996, P SIGGRAPH
   Hu JH, 2003, IEEE T WIREL COMMUN, V2, P205, DOI 10.1109/TWC.2003.808957
   KHO Y, 2003, ACM 14D
   KHODAKOVSKY A, 2000, P SIGGRAPH
   Lee KK, 2002, IEEE T VEH TECHNOL, V51, P1569, DOI 10.1109/TVT.2002.804843
   LEWINER T, 2004, P SIGGRAPH
   LIMB JO, 1979, IEEE T SYST MAN CYB, V9, P778, DOI 10.1109/TSMC.1979.4310129
   LINDSTROM P, 2000, ACM T GRAPHICS
   LINDSTROM P, 1995, TR9506
   LIU F, 2004, P SIGGRAPH
   Luebke D, 2001, SPRING EUROGRAP, P223
   LUEBKE D, 2002, LEVEL DETAIL 3 D GRA
   MANNOS JL, 1974, IEEE T INFORM THEORY, V31
   MCNAMARA A, 2000, VISUAL PERCEPTION RE
   NAGATA S, 1984, P SID
   O'Sullivan C, 2003, ACM T GRAPHIC, V22, P527, DOI 10.1145/882262.882303
   OSULLIVAN C, 2004, PERCEPTUALLY APAPTIV
   PAN Y, 2005, IEEE T MULTIMED  APR
   Pereira F., 2002, IMSC Press multimedia series
   Pojar Erik., 2003, Proceedings of the 2003 symposium on Interactive 3D graphics, P127
   Reddy M, 2001, IEEE COMPUT GRAPH, V21, P68, DOI 10.1109/38.946633
   Reddy M., 1997, THESIS U EDINBURGH
   Rogowitz BE, 2001, P SOC PHOTO-OPT INS, V4299, P340, DOI 10.1117/12.429504
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   Rushmeier H, 2000, P SOC PHOTO-OPT INS, V3959, P372, DOI 10.1117/12.387174
   SANDER P, 2001, P SIGGRAPH
   Schmitt F., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P710, DOI 10.1109/CVPR.1991.139793
   Shaffer E., 2001, IEEE VISUALIZATION
   Sinha P, 2002, WIREL NETW, V8, P301, DOI 10.1023/A:1013702428498
   Soucy M, 1996, VISUAL COMPUT, V12, P503, DOI 10.1007/s003710050082
   STEIN CS, 1989, P SOC PHOTO-OPT INS, V1977, P198
   TEO PC, 1994, P SOC PHOTO-OPT INS, V2179, P127, DOI 10.1117/12.172664
   TURK G, 1991, P SIGRAPH
   VANDEN CJ, 1996, P SOC PHOTO-OPT INS, P450
   Wang Feng., 2002, ACM MOBIHOC
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
   Watson B, 2001, COMP GRAPH, P213, DOI 10.1145/383259.383283
   WILLIAMS N, 2003, SIGGRAPH S INT 3 D G
   WU D, 2002, IEEE T WIREL COMMUN, V2, P630
   XIA J, 1997, IEEE T VISUAL CO JUN
   YAVATKAR R, 1994, P WORKSH MOB COMP SY, P146
   YU Y, 2003, IEEE T MULTIMEDIA, V466
   YU Y, 2000, SIGGRAPH SKETCH
   ZANUTTIGH P, 2005, P IEEE INT C IM PROC
   [No title captured]
NR 91
TC 15
Z9 19
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 386
EP 396
DI 10.1109/TMM.2006.886291
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900016
OA Green Published
DA 2024-07-18
ER

PT J
AU van der Schaar, M
   Turaga, DS
AF van der Schaar, Mihaela
   Turaga, Deepak S.
TI Cross-layer packetization and retransmission strategies for
   delay-sensitive wireless multimedia transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cross-layer wireless multimedia transmission; packetization strategies;
   retransmission; resilient transmission
ID VIDEO; PROTECTION; NETWORKS
AB Existing wireless networks provide dynamically varying resources with only limited support for the quality of service required by the bandwidth-intense, loss-tolerant, and delay-sensitive multimedia applications. This variability of resources does not significantly impact delay insensitive data transmission (e.g., file transfers), but has considerable consequences for multimedia applications. Recently, the research focus has been to adapt existing algorithms and protocols at the lower layers of the protocol stack to better support multimedia transmission applications, and conversely, to modify application layer solutions to cope with the varying wireless networks resources. In this paper, we show that significant improvements in wireless multimedia performance can be obtained by deploying a joint application-layer adaptive packetization and prioritized scheduling and MAC-layer retransmission strategy. We deploy a state-of-the-art wavelet coder for the compression of the video data that enables on-the-fly adaptation to changing channel conditions and inherent prioritization of the video bitstream. We pose the cross-layer problem as a distortion minimization given delay constraints and derive analytical solutions by modifying existing joint source-channel coding theory aimed at fulfilling rate, rather than delay, constraints. We also propose real-time algorithms that explicitly consider the available information about previously transmitted packets. The obtained results show significant improvements in terms of video quality as opposed to ad-hoc optimizations currently deployed, while the complexity associated with performing this optimization in real time, i.e., at transmission time, is limited.
C1 Univ Calif Los Angeles, Dept Elect & Comp Engn, Los Angeles, CA 90095 USA.
   IBM TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
C3 University of California System; University of California Los Angeles;
   International Business Machines (IBM)
RP van der Schaar, M (corresponding author), Univ Calif Los Angeles, Dept Elect & Comp Engn, Los Angeles, CA 90095 USA.
EM mvanderschaar@ece.ucdavis.edu; mihaela@ee.ucla.edu
CR [Anonymous], MSRTR200135
   Bianchi G, 2000, IEEE J SEL AREA COMM, V18, P535, DOI 10.1109/49.840210
   Chou P. A., 2000, Proceedings DCC 2000. Data Compression Conference, P440, DOI 10.1109/DCC.2000.838184
   Feideropoulou G, 2004, EURASIP J APPL SIG P, V2004, P1931, DOI 10.1155/S1110865704402327
   GIROD B, 2001, COMPRESSED VIDEO NET
   Li Q, 2004, IEEE T MULTIMEDIA, V6, P278, DOI 10.1109/TMM.2003.822792
   LI Q, 2002, 1SC29WG11M8944 ISOIE
   Majumdar A, 2002, IEEE T CIRC SYST VID, V12, P524, DOI 10.1109/TCSVT.2002.800315
   MCCANNE S, 1996, P ACM SIGCOMM 96 STA, P117
   Mohr AE, 2000, IEEE J SEL AREA COMM, V18, P819, DOI 10.1109/49.848236
   Puri R, 2001, SIGNAL PROCESS-IMAGE, V16, P745, DOI 10.1016/S0923-5965(01)00005-4
   Qiao D., 2002, IEEE T MOBILE COMPUT, V1, P580
   Rogers JK, 1998, IEEE SIGNAL PROC LET, V5, P105, DOI 10.1109/97.668942
   Rusert T, 2003, PROC SPIE, V5150, P682, DOI 10.1117/12.509881
   Secker A, 2003, IEEE T IMAGE PROCESS, V12, P1530, DOI 10.1109/TIP.2003.819433
   Setton E, 2005, IEEE WIREL COMMUN, V12, P59, DOI 10.1109/MWC.2005.1497859
   SINGER D, 2001, 1449612002 ISOIEC
   TAUBMAN D, 2004, M11441 MPEG
   Taubman D.S., 2002, JPEG 2000: Image Compression Fundamentals, Standards and Practice, DOI 10.1007/978-1-4615-0799-4
   Thie J, 2004, EURASIP J APPL SIG P, V2004, P207, DOI 10.1155/S1110865704308024
   THIE J, 2003, THESIS U NEW S WALES
   van der Schaar M, 2003, IEEE J SEL AREA COMM, V21, P1752, DOI 10.1109/JSAC.2003.815231
   WANG M, 2005, P ICASSP PHIL PA MAR
   Wenger S, 2003, IEEE T CIRC SYST VID, V13, P645, DOI 10.1109/TCSVT.2003.814966
   Wu XL, 2001, IEEE T MULTIMEDIA, V3, P132, DOI 10.1109/6046.909600
   Xu JZ, 2001, APPL COMPUT HARMON A, V10, P290, DOI 10.1006/acha.2000.0345
NR 26
TC 100
Z9 109
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 185
EP 197
DI 10.1109/TMM.2006.886384
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500018
DA 2024-07-18
ER

PT J
AU Peng, WH
   Chiang, TH
   Hang, HM
   Lee, CY
AF Peng, Wen-Hsiao
   Chiang, Tihao
   Hang, Hsueh-Ming
   Lee, Chen-Yi
TI A context adaptive bit-plane coder with maximum-likelihood-based
   stochastic bit-reshuffling technique for scalable video coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bit-plane coding; fine granularity scalability; scalable video coding
ID FINE GRANULARITY SCALABILITY
AB In this paper, we propose a context adaptive bit-plane coding (CABIC) with a stochastic bit reshuffling (SBR) scheme to deliver higher coding efficiency and better subjective quality for fine granular scalable (FGS) video coding. Traditional bit-plane coding in FGS algorithm suffers from poor coding efficiency and subjective quality. To improve coding efficiency, our CABIC constructs context models based on both the energy distribution in a block and the spatial correlations in the adjacent blocks. Moreover, it exploits the context across bit-planes to save side information. To improve subjective quality, our SBR reorders the coefficient bits by their estimated rate-distortion performance. Particularly, we model transform coefficients with Laplacian distributions and incorporate them into the context probability models for content-aware parameter estimation. Moreover, our SBR is implemented with a dynamic priority management that uses a low-complexity dynamic memory organization. Experimental results show that our CABIC improves the PSNR by 0.5 similar to 1.0 dB at medium and high bit rates. While maintaining similar or even higher coding efficiency, our SBR improves the subjective quality.
C1 Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu 30010, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Peng, WH (corresponding author), Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu 30010, Taiwan.
EM pawn@mail.si2lab.org; tchiang@mail.nctu.edu.tw; hmhang@mail.nctu.edu.tw;
   cylee@cc.nctu.edu.tw
RI Hang, Hsueh-Ming/K-7848-2012
OI PENG, WEN-HSIAO/0000-0002-4421-8031
CR CHEN Y, Patent No. 20030133499
   Cover Thomas A., 1991, ELEM INF THEORY, DOI 10.1002/047174882X
   Huang HC, 2002, IEEE T CIRC SYST VID, V12, P372, DOI 10.1109/TCSVT.2002.800314
   HUANG HC, 2004, P IEEE INT S CIRC SY
   LAURANCE NK, 1997, P IEEE INT C AC SPEE
   LI J, 2003, IEEE T IMAGE PROCESS, V9, P297
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   NISTER D, 1998, P IEEE INT C AC SPEE
   PENG WH, 2003, P IEEE INT S SIGN PR
   PENG WH, 2001, P IEEE INT C IM PROC
   PENG WH, 2004, P IEEE INT S CIRC SY
   REICHEL J, 2005, JTCISC29WG11
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   WU F, 2003, INT J IMAG SYST TECH, V13, P913
NR 14
TC 1
Z9 1
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 654
EP 667
DI 10.1109/TMM.2006.876302
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300002
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhai, Y
   Shah, M
AF Zhai, Yun
   Shah, Mubarak
TI Video scene segmentation using Markov chain Monte Carlo
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Markov chain Monte Carlo; video scene segmentation
ID IMAGE SEGMENTATION
AB Videos are composed of many shots that are caused by different camera operations, e.g., on/off operations and switching between cameras. One important goal in video analysis is to group the shots into temporal scenes, such that all the shots in a single scene are related to the same subject, which could be a particular physical setting, an ongoing action or a theme. In this paper, we present a general framework for temporal scene segmentation in various video domains. The proposed method is formulated in a statistical fashion and uses the Markov chain Monte Carlo (MCMC) technique to determine the boundaries between video scenes. In this approach, a set of arbitrary scene boundaries are initialized at random locations and are automatically updated using two types of updates: diffusion and jumps. Diffusion is the process of updating the boundaries between adjacent scenes. Jumps consist of two reversible operations: the merging of two scenes and the splitting of an existing scene. The posterior probability of the target distribution of the number of scenes and their corresponding boundary locations is computed based on the model priors and the data likelihood. The updates of the model parameters are controlled by the hypothesis ratio test in the MCMC process, and the samples are collected to generate the final scene boundaries. The major advantage of the proposed framework is two-fold: 1) it is able to find the weak boundaries as well as the strong boundaries, i.e., it does not rely on the fixed threshold; 2) it can be applied to different video domains. We have tested the proposed method on two video domains: home videos and feature films, and accurate results have been obtained.
C1 Univ Cent Florida, Sch Comp Sci, Orlando, FL 32826 USA.
C3 State University System of Florida; University of Central Florida
RP Zhai, Y (corresponding author), Univ Cent Florida, Sch Comp Sci, Orlando, FL 32826 USA.
EM yzhai@cs.ucf.edu; shah@cs.ucf.edu
RI Sahoo, Sarat Kumar/J-8765-2014
OI Sahoo, Sarat Kumar/0000-0001-5734-6844; Shah,
   Mubarak/0000-0001-6172-5572
CR Adams B, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P283, DOI 10.1109/ICIP.2000.899358
   AMIR A, 2003, TRECVID 2003
   ANER A, 2002, EUR C COMP VIS
   [Anonymous], EUR C COMP VIS
   Chaisorn L, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P73, DOI 10.1109/ICME.2002.1035721
   Dellaert F, 2003, MACH LEARN, V50, P45, DOI 10.1023/A:1020245811187
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711
   GRENANDER U, 1994, J ROY STAT SOC B, V56
   GUNSEL B, 1998, J ELECT IMAG, V7
   Han F, 2004, IEEE T PATTERN ANAL, V26, P1138, DOI 10.1109/TPAMI.2004.70
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.1093/biomet/57.1.97
   HOASHI K, 2004, TREC VID RETR EV FOR
   HSU W, 2004, INT C MULT EXP
   JAVED O, 2002, INT J COMPUT APPL
   KENDER JR, 1998, INT C COMP VIS PATT
   Li Y., 2003, VIDEO MINING
   LIENHART R, 1999, INT C MULT COMP SYST
   NGO CW, 2001, INT J COMPUT VIS
   PHILLIPS DB, 1995, MARKOV CHAIN MONTE C, pCH13
   RASHEED Z, 2003, INT C COMP VIS PATT
   SENEGAS J, 2002, EUR C COMP VIS
   SUNDARAM H, 2000, INT C MULT EXP
   Tavanapong W, 2004, IEEE T MULTIMEDIA, V6, P517, DOI 10.1109/tmm.2004.830810
   Tu ZW, 2002, IEEE T PATTERN ANAL, V24, P657, DOI 10.1109/34.1000239
   Yeung M, 1998, COMPUT VIS IMAGE UND, V71, P94, DOI 10.1006/cviu.1997.0628
   YEUNG MM, 1995, SPIE C MULT COMP NET
   ZHAI Y, 2005, INT C IM AN REC
   ZHAI Y, 2003, TREC VID RETR EV FOR
   ZHANG HJ, 1997, PATTERN RECOGNIT, V30
   Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343
NR 32
TC 70
Z9 81
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 686
EP 697
DI 10.1109/TMM.2006.876299
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300004
DA 2024-07-18
ER

PT J
AU Li, T
   Ogihara, M
AF Li, Tao
   Ogihara, Mitsunori
TI Toward intelligent music information retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE clustering; FFT; machine learning; music information retrieval; wavelet
ID GENRE
AB Efficient and intelligent music information retrieval is a very important topic of the 21st century. With the ultimate goal of building personal music information retrieval systems, this paper studies the problem of intelligent music information retrieval. Huron [10] points out that since the preeminent functions of music are social and psychological, the most useful characterization would be based on four types of information: genre, emotion, style, and similarity.
   This paper introduces Daubechies Wavelet Coefficient Histograms (DWCH) for music feature extraction for music information retrieval. The histograms are computed from the coefficients of the db(8) Daubechies wavelet filter applied to 3 s of music. A comparative study of sound features and classification algorithms on a dataset compiled by Tzanetakis shows that combining DWCH with timbral features (MFCC and FFT), with the use of multiclass extensions of support vector machine, achieves approximately 80% of accuracy, which is a significant improvement over the previously known result on this dataset. On another dataset the combination achieves 75% of accuracy.
   The paper also studies the issue of detecting emotion in music. Rating of two subjects in the three bipolar adjective pairs are used. The accuracy of around 70% was achieved in predicting emotional labeling in these adjective pairs. The paper also studies the problem of identifying groups of artists based on their lyrics and sound using a semi-supervised classification algorithm. Identification of artist groups based on the Similar Artist lists at All Music Guide is attempted. The semi-supervised learning algorithm resulted in nontrivial increases in the accuracy to more than 70%.
   Finally, the paper conducts a proof-of-concept experiment on similarity search using the feature set.
C1 Florida Int Univ, Sch Comp Sci, Miami, FL 33199 USA.
   Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
C3 State University System of Florida; Florida International University;
   University of Rochester
RP Li, T (corresponding author), Florida Int Univ, Sch Comp Sci, Miami, FL 33199 USA.
EM taoli@cs.flu.edu; ogihara@cs.rochester.edu
RI Ogihara, Mitsunori/AAB-8275-2020
OI Ogihara, Mitsunori/0000-0002-5690-7854
CR [Anonymous], P ACM SIGMOD INT C M
   [Anonymous], P ACM SIGKDD INT C K
   [Anonymous], 2003, Proceedings of the International Symposium on Music Information Retrieval
   Argamon Shlomo., 2003, Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, P475, DOI [10.1145/956750.956805, DOI 10.1145/956750.956805]
   Bill E., 1994, P 12 NATL C ARTIFICI, V1, P722
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Daubechies I., 1992, Ten lectures on wavelets, DOI [DOI 10.1137/1.9781611970104, 10.1137/1.9781611970104]
   Dowling W.J., 1986, MUSIC COGNITION
   ELLIS DP, 2002, P INT S MUS INF RETR, P170
   FARNSWORTH PR, 1958, SOCIAL PSYCHOL MUSIC
   FLANDRIN P, 1992, IEEE T INFORM THEORY, V38, P910, DOI 10.1109/18.119751
   FUNG G, 2001, 0106 U
   Hevner K, 1936, AM J PSYCHOL, V48, P246, DOI 10.2307/1415746
   Huron D., 2000, COGNITION, V14, P83
   KESSLERB, 1997, P 35 ANN M ASS COMP, P32
   LEMAN M, 2003, UNPUB ACOUSTICAL COM
   Li GH, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P885, DOI 10.1109/ICME.2000.871501
   Li T, 2005, KNOWL INF SYST, V7, P289, DOI 10.1007/s10115-004-0155-8
   Li T, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P143
   Li T, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P705
   LI T, 2003, P 26 ANN INT ACM SIG, P282
   Li T., 2002, ACM SIGKDD Explorations Newsletter, V4, P49, DOI [10.1145/772862.772870, DOI 10.1145/772862.772870]
   Li Tao., 2004, P 12 ANN ACM INT C M, P364
   Logan Beth, 2000, ISMIR, V270, P1
   Mandal MK, 1999, COMPUT VIS IMAGE UND, V75, P99, DOI 10.1006/cviu.1999.0766
   MITTON R, 1987, INFORM PROCESS MANAG, V23, P495, DOI 10.1016/0306-4573(87)90116-6
   Peretz I, 1998, COGNITION, V68, P111, DOI 10.1016/S0010-0277(98)00043-2
   Perrot D., 1999, SOC MUSIC PERCEPTION, P88
   PYE D, 2000, P 2000 IEEE INT C AC
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923
   Stamatatos E, 2000, COMPUT LINGUIST, V26, P471, DOI 10.1162/089120100750105920
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Tweedie FJ, 1998, COMPUT HUMANITIES, V32, P323, DOI 10.1023/A:1001749303137
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Vapnik V., 1998, STAT LEARNING THEORY, V3
   WEISS G, 2001, 44 MLTR
   Westphal M., 1998, P 1998 IEEE INT C AC
NR 38
TC 107
Z9 128
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 564
EP 574
DI 10.1109/TMM.2006.870730
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000013
DA 2024-07-18
ER

PT J
AU Atzori, L
   Lobina, ML
   Corona, M
AF Atzori, L
   Lobina, ML
   Corona, M
TI Playout buffering of speech packets based on a quality maximization
   approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bursty packet losses; IP telephony; playout buffering; speech quality
   evaluation
AB To combat jitter problems in voice streaming over packet networks, playout buffering algorithms are used at the receiver side. Most of the proposed solutions rely on two main operations: prediction of delay statistics for future packets; setting of the end-to-end delay so as to limit or avoid packet losses. In recent years, a new approach has been presented, which is based on using a quality model to evaluate the impact of both packet loss and delay on the voice quality. Such a model is used to find the buffer setting that maximizes the expected quality.
   In this paper, we present a playout buffering algorithm whose main contribution is the extension of the new quality-based approach to the case of voice communications affected by bursty packet losses. This work is motivated by two main considerations: most of IP telephony applications are characterized by bursty losses instead of random ones; the human perception of the speech quality is significantly affected by the temporal correlation of losses. To this purpose, we make use of the extensions proposed in the ETSI Tiphon for the ITU-T E-Model so as to incorporate the effects of loss burstiness on the perceived quality. The resulting playout algorithm estimates the characteristics of the loss process varying the end-to-end delay, weights the loss and the delay effects on the perceived quality, and maximizes the overall quality to find the optimal setting for the playout buffer. The experimental results prove the effectiveness of the proposed technique.
C1 Univ Cagliari, Dept Elect & Elect Engn, I-09123 Cagliari, Italy.
C3 University of Cagliari
RP Univ Cagliari, Dept Elect & Elect Engn, I-09123 Cagliari, Italy.
EM l.atzori@diee.unica.it; m.lobina@diee.unica.it; m.corona@diee.unica.it
OI Atzori, Luigi/0000-0003-1350-3574
CR [Anonymous], G107 ITUT
   Atzori L, 2004, IEEE SIGNAL PROC LET, V11, P382, DOI 10.1109/LSP.2003.822924
   CLARK A, T1A112001037 ETSI
   COLE R, 2001, ACM COMPUT COMMUN RE, V31
   DeLeon P, 1999, INT CONF ACOUST SPEE, P3097, DOI 10.1109/ICASSP.1999.757496
   Fujimoto K, 2002, GLOB TELECOMM CONF, P2451
   *ITU T, 2000, G114 ITUT
   ITU-T, 2001, G113 ITUT
   Jiang W., 2000, P NOSSDAV JUN
   RAMJEE R, 1994, P IEEE INFOCOM TOR O
   Sreenan CJ, 2000, IEEE T MULTIMEDIA, V2, P88, DOI 10.1109/6046.845013
   SUN L, 2004, P INT C COMM JUN
NR 12
TC 5
Z9 8
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 420
EP 426
DI 10.1109/TMM.2005.864348
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300022
DA 2024-07-18
ER

PT J
AU Yiu, WPK
   Wong, KFS
   Chan, SHG
   Wong, WC
   Zhang, Q
   Zhu, WW
   Zhang, YQ
AF Yiu, WPK
   Wong, KFS
   Chan, SHG
   Wong, WC
   Zhang, Q
   Zhu, WW
   Zhang, YQ
TI Lateral error recovery for media streaming in application-level
   multicast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE application-level multicast (ALM); error recovery; retransmission
   deadline; streaming applications
ID VIDEO MULTICAST; FEC
AB We consider media streaming using application-level multicast (ALM) where packet loss has to be recovered via retransmission in a timely manner. Since packets may be lost due to congestion, node failures, and join and leave dynamics, traditional "vertical" recovery approach where upstream nodes retransmit the lost packets is no longer effective. We therefore propose lateral error recovery (LER). In LER, hosts are divided into a number of planes, each of which forms an independent ALM tree. Since error correlation across planes is low, a node effectively recovers its error by "laterally" requesting retransmission from nearby nodes in other planes.
   We present analysis on the complexity and recovery delay on LER. Using Internet-like topologies, we show via simulations that LER is an effective error recovery mechanism. It achieves low overhead in terms of delivery delay (i.e., relative delay penalty) and physical link stress. As compared with traditional recovery schemes, LER attains much lower residual loss rate (i.e., loss rate after retransmission) under a certain deadline constraint. The performance can be substantially improved in the presence of some reliable proxies.
C1 Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   Intel Commun Technol China Lab, Beijing 100080, Peoples R China.
   Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Hong Kong University of Science & Technology; Intel Corporation;
   Microsoft Research Asia; Microsoft
RP Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM kenyiu@cs.ust.hk; cssrn@cs.ust.hk; gchan@cs.ust.hk; wwilliam@cs.ust.hk;
   qianzh@cs.ust.hk; wenwu.zhu@intel.com
RI Zhang, Qian/B-9058-2009
OI Zhang, Qian/0000-0001-9205-1881; Chan, Gary Shueng
   Han/0000-0003-4207-764X
CR Banerjee S., 2003, ACM SIGMETRICS 2003
   BANERJEE S, 2002, P ACM SIGCOMM PITTSB
   Cai JF, 2000, IEEE WCNC, P1243, DOI 10.1109/WCNC.2000.904809
   Calderon M, 1998, IEEE NETWORK, V12, P46, DOI 10.1109/65.690961
   Castro M, 2003, IEEE INFOCOM SER, P1510
   Castro M, 2002, IEEE J SEL AREA COMM, V20, P1489, DOI 10.1109/JSAC.2002.803069
   CHENG KL, 2005, P IEEE ICC SEOUL KOR
   Cheuk KWR, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1441, DOI 10.1109/ICC.2004.1312750
   Chu YH, 2002, IEEE J SEL AREA COMM, V20, P1456, DOI 10.1109/JSAC.2002.803066
   DABEK F, 2004, P ACM SIGCOMM PORTL
   El-Sayed A, 2003, IEEE NETWORK, V17, P46, DOI 10.1109/MNET.2003.1174177
   FORTUNE S, 1987, ALGORITHMICA, V2, P153, DOI 10.1007/BF01840357
   FRANCIS P, 2000, YOID YOUR OWN INTERN
   Frossard P, 2001, IEEE T IMAGE PROCESS, V10, P1815, DOI 10.1109/83.974566
   Gemmell J, 2003, IEEE NETWORK, V17, P16, DOI 10.1109/MNET.2003.1174173
   Grossglauser M, 1997, IEEE J SEL AREA COMM, V15, P422, DOI 10.1109/49.564139
   Huang SL, 2004, J COLD REG ENG, V18, P2, DOI 10.1061/(ASCE)0887-381X(2004)18:1(2)
   JIN X, 2005, P IEEE ICC SEOUL KOR
   Kasera SK, 2000, IEEE NETWORK, V14, P48, DOI 10.1109/65.819171
   KELLER P, 2000, P IEEE INFOCOM ISR M, V3, P1137
   KERMODE R, 1998, P ACM SIGCOMM 98 VAN, P278
   KIM DK, 2004, IEEE CONS COMM NETW
   Kim E, 2002, IEEE COMMUN LETT, V6, P464, DOI 10.1109/LCOMM.2002.803474
   Lacher MS, 2000, IEEE ACM T NETWORK, V8, P224, DOI 10.1109/90.842144
   Lee TWA, 2002, IEEE T CIRC SYST VID, V12, P1059, DOI 10.1109/TCSVT.2002.806816
   Lestayo T, 2001, ELECTRON LETT, V37, P1333, DOI 10.1049/el:20010913
   Li VOK, 2002, P IEEE, V90, P360, DOI 10.1109/5.993404
   Liebeherr J, 2002, IEEE J SEL AREA COMM, V20, P1472, DOI 10.1109/JSAC.2002.803067
   Liu CG, 1998, IEEE ACM T NETWORK, V6, P686, DOI 10.1109/90.748082
   NG TSE, 2002, P IEEE INFOCOM 2002
   NOGUCHI T, 2001, P ICC 2001, V8, P2348
   Nonnenmacher J, 1999, IEEE ACM T NETWORK, V7, P375, DOI 10.1109/90.779206
   Nonnenmacher J, 1998, IEEE ACM T NETWORK, V6, P349, DOI 10.1109/90.720869
   Oouchi H, 2005, 2005 SYMPOSIUM ON APPLICATIONS AND THE INTERNET, PROCEEDINGS, P311, DOI 10.1109/SAINT.2005.42
   PADMANABHAN VN, 2002, DIMACS WORKSH INT WW
   Pendarakis D., 2001, P 3 USENIX S INT TEC
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   SRIPANIDKULCHAI K, 2004, P INT MEAS C IMC TAO
   SRIPANIDKULCHAI K, 2004, P ACM SIGCOMM PORTL
   SUBRAMANIAN L, 2002, P 1 HOTNETS WORKSH O
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   Towsley D, 1997, IEEE J SEL AREA COMM, V15, P398, DOI 10.1109/49.564137
   VENKATA PAC, 2004, P 3 INT WORKSH PEER
   Whetten B, 2000, IEEE NETWORK, V14, P37, DOI 10.1109/65.819170
   Wong KFS, 2004, IEEE INFOCOM SER, P2708
   Wong WC, 2003, GLOB TELECOMM CONF, P2835
   Yiu WPK, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1451, DOI 10.1109/ICC.2004.1312752
   YIU WPK, 2005, P IEEE ICC SEOUL KOR
   Zegura E.W., 1996, P IEEE INFOCOM 1996
NR 49
TC 19
Z9 25
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 219
EP 232
DI 10.1109/TMM.2005.864268
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300004
DA 2024-07-18
ER

PT J
AU Röder, M
   Cardinal, J
   Hamzaoui, R
AF Röder, M
   Cardinal, J
   Hamzaoui, R
TI Branch and bound algorithms for rate-distortion optimized media
   streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT Data Compression Conference (DCC 2004)
CY MAR 23-25, 2004
CL Snowbird, UT
SP Brandeis Univ, IEEE Comp Soc
DE branch and bound algorithms; computational complexity; streaming media
AB We consider the problem of rate-distortion optimized streaming of packetized multimedia data over a single quality-of-service network using feedback and retransmissions. For a single data unit, we prove that the problem is NP-hard and provide efficient branch and bound algorithms that are much faster than the previously best solution based on dynamic programming. For a group of interdependent data units, we show how to compute optimal solutions with branch and bound algorithms. The branch and bound algorithms for a group of data units are much slower than the current state of the art, a heuristic technique known as sensitivity adaptation. However, in many real-world situations, they provide a significantly better rate-distortion performance.
C1 Univ Konstanz, Dept Comp & Informat Sci, D-7750 Constance, Germany.
   Univ Libre Bruxelles, Dept Comp Sci, Brussels, Belgium.
C3 University of Konstanz; Universite Libre de Bruxelles
RP Univ Konstanz, Dept Comp & Informat Sci, D-7750 Constance, Germany.
EM roeder@inf.uni-konstanz.de; jcardin@ulb.ac.be;
   hamzaoui@inf.uni-konstanz.de
CR [Anonymous], MSRTR200135
   [Anonymous], 1995, Algorithms for knapsack problems
   Chakareski J, 2004, IEEE DATA COMPR CONF, P202
   Chakareski J, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P645
   Chakareski J, 2002, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.2002.999943
   CHAKARESKI J, 2004, P IEEE ICIP 2004 SIN
   Garey M.R., 1979, COMPUTERS INTRACTABI
   KALMAN M, 2003, P IEEE ICIP 03 BARC, V3, P277
   PREPARATA FP, 1985, COMPUTATONAL GEOMETR
   RODER M, 2004, 195 KONST SCHRIFT MA
   Sehgal A, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P857, DOI 10.1109/ICME.2002.1035917
   SZEKLI R., 1995, Lecture Notes in Statistics, V97
   WIEGAND T, 2003, 1449610 AVC ISOIEC
   Wu DP, 2001, IEEE T CIRC SYST VID, V11, P282, DOI 10.1109/76.911156
NR 14
TC 10
Z9 12
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 170
EP 178
DI 10.1109/TMM.2005.861281
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000016
DA 2024-07-18
ER

PT J
AU da Fonseca, NLS
   Rubinsztejn, HKS
AF da Fonseca, NLS
   Rubinsztejn, HKS
TI Dimensioning the capacity of true video-on-demand servers
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE batching; erlang formula; interactive TV; piggy-backing
ID PIGGYBACKING
AB The need to reduce the huge bandwidth demand of video-on-demand (VoD) services has led to the conception of both multicast and broadcast based techniques for the deployment of such services on a large scale. Interactivity, a desirable feature for video services, includes the capacity to perform VCR operations. However, whenever a viewer requests the performance of a VCR operation, his/her video stream becomes unsynchronized with that of his/her multicast group, and a new channel must be allocated for the performance of this operation. The present paper introduces a novel approach for determining the number of video channels needed to support such interactivity. Moreover, it investigates the performance of. interactive systems with a pool of channels reserved for the support of VCR operations. Systems with both batching and piggybacking are analyzed. Results indicate that for a medium to high number of users performing VCR operations the number of channels required to achieve target levels of quality-of-service is lower for systems with no pool of reserved channels than it is for systems with such a pool.
C1 Univ Estadual Campinas, Inst Comp, BR-13084 Campinas, SP, Brazil.
   Pontificia Univ Catolica Rio de Janeiro, Dept Informat, Rio De Janeiro, Brazil.
C3 Universidade Estadual de Campinas; Pontificia Universidade Catolica do
   Rio de Janeiro
RP Univ Estadual Campinas, Inst Comp, BR-13084 Campinas, SP, Brazil.
EM nfonseca@ic.unicamp.br; hana@inf.puc-rio.br
RI Fonseca, Nelson/C-6981-2012
OI Fonseca, Nelson L. S. da/0000-0003-2046-602X
CR Aggarwal C., 1996, Performance Evaluation Review, V24, P200, DOI 10.1145/233008.233044
   Aggarwal CC, 2001, IEEE T COMPUT, V50, P97, DOI 10.1109/12.908987
   ALMEIDA JM, 2001, 11 INT WORKSH NETW O
   [Anonymous], 1975, QUEUEING SYSTEM
   Bradshaw MK, 2003, MULTIMEDIA SYST, V9, P78, DOI 10.1007/s00530-003-0079-2
   Branch P., 1999, 1999 IEEE International Conference on Communications (Cat. No. 99CH36311), P978, DOI 10.1109/ICC.1999.765419
   CAI Y, 1999, ACM SPIE C MULT COMP, P204
   CHEN YS, 1992, COMPUT MATH APPL, V24, P77, DOI 10.1016/0898-1221(92)90156-C
   DAFONSECA NLS, 2003, WILEY ENCY TELECOMMU, V1, P232
   DAFONSECA NLS, 2000, IEEE GLOB TEL C, P1334
   Dan A, 1996, MULTIMEDIA SYST, V4, P112, DOI 10.1007/s005300050016
   DAN A, 1995, J PARALLEL DISTR COM, V30, P168, DOI 10.1006/jpdc.1995.1135
   Dey-Sircar J. K., 1994, Proceedings ACM Multimedia '94, P25, DOI 10.1145/192593.192615
   Eager D, 2001, IEEE T KNOWL DATA EN, V13, P742, DOI 10.1109/69.956098
   Eager D, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P199, DOI 10.1145/319463.319601
   Eager DL, 2000, PERFORM EVALUATION, V42, P163, DOI 10.1016/S0166-5316(00)00029-8
   Façanha RD, 1999, MULTIMED TOOLS APPL, V8, P371, DOI 10.1023/A:1009626427236
   FONSECA NLS, 2002, IEEE T MULTIMEDIA, V4, P114
   Golubchik L, 1996, MULTIMEDIA SYST, V4, P140, DOI 10.1007/s005300050019
   GUO Y, 2003, 12 WORLD WID WEB C W
   Hu AL, 2001, IEEE INFOCOM SER, P508, DOI 10.1109/INFCOM.2001.916754
   HUA K, 1997, ACM SIGCOM SEP
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   Hua KA, 2004, P IEEE, V92, P1439, DOI 10.1109/JPROC.2004.832954
   Pâris JF, 1998, IEEE IC COMP COM NET, P690, DOI 10.1109/ICCCN.1998.998831
   RAPPAPORT TS, 1996, WIRELESS COMMUNICATI, P555
   SEN S, 1999, IEEE NOSSDAV 99 4UN
   Shachnai H, 1998, MULTIMEDIA SYST, V6, P382, DOI 10.1007/s005300050101
   TRAN D, 2002, ACM MULTIMEDIA   DEC, P247
NR 29
TC 6
Z9 7
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 932
EP 941
DI 10.1109/TMM.2005.854407
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900014
DA 2024-07-18
ER

PT J
AU Barni, M
   Bartolini, F
   Checcacci, N
AF Barni, M
   Bartolini, F
   Checcacci, N
TI Watermarking of MPEG-4 video objects
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE MPEG-4 watermarking; objects watermarking; video watermarking
AB The recent finalization of MPEG-4 will make this standard very attractive for a large range of applications such as video editing, Internet video distribution, wireless video communications. Some of these applications are likely to get great benefit from watermarking technology, since it can enable a number of innovative services, such as conditional access policies, data annotation, data labeling, content authentication, to be implemented at a low price. One of the key points of the MPEG-4 standard is the possibility to access and manipulate objects within a video sequence. Thus object watermarking has to be achieved in such a way that, while a video object is transferred from a sequence to another, it is still possible to correctly access the data embedded within the object itself. The algorithm proposed in this paper embeds a watermark in each video object by imposing a particular relationship between some predefined pairs of quantized discrete cosine transform (DCT) coefficients in the luminance blocks of pseudo-randomly selected macroblocks (MBs). Watermarks are equally embedded into intra and inter MBs Experimental results are presented validating the effectiveness of the proposed approach.
C1 Univ Siena, Dipartimento Ingn Informaz, I-53100 Siena, Italy.
   Univ Florence, Dipartimento Elettron & Telecomun, Florence, Italy.
C3 University of Siena; University of Florence
RP Barni, M (corresponding author), Univ Siena, Dipartimento Ingn Informaz, Via Laterina 8, I-53100 Siena, Italy.
EM barni@dii.unisi.it
CR Barni M, 1999, P SOC PHOTO-OPT INS, V3657, P31, DOI 10.1117/12.344689
   Barni M, 2001, IEEE T IMAGE PROCESS, V10, P755, DOI 10.1109/83.918568
   Deguillaume F, 1999, PROC SPIE, V3657, P113, DOI 10.1117/12.344662
   Dittmann J., 1998, Proceedings ACM Multimedia 98, P71, DOI 10.1145/290747.290757
   Hartung F, 1996, P SOC PHOTO-OPT INS, V2952, P205, DOI 10.1117/12.251278
   HARTUNG F, 1997, P IEEE ICASSP 97 MUN, V4, P2621
   Hernández JR, 2000, PROC SPIE, V3971, P24, DOI 10.1117/12.384985
   HSU C, 1997, P IEEE INT C DIG SIG, V1, P217
   HSU CT, 1996, P IEEE INT C IM PROC, V3, P223
   *ISO ISO IEC, 2000, N5229 ISO ISOIEC
   Kalker T, 1999, PROC SPIE, V3657, P103, DOI 10.1117/12.344661
   KALKER T, 1999, P IEEE INT C MULT CO
   KOCH E, 1995, P IEEE WORKSH NONL S
   Langelaar GC, 1999, P SOC PHOTO-OPT INS, V3657, P2, DOI 10.1117/12.344659
   Langelaar GC, 2001, IEEE T IMAGE PROCESS, V10, P148, DOI 10.1109/83.892451
   Lin S., 1983, Error Control Coding: Fundamentals and Applications
   Mobasseri BG, 1999, P SOC PHOTO-OPT INS, V3657, P96, DOI 10.1117/12.344660
   PIVA A, 2000, P ICIP 2001 IEEE INT
   PIVA A, 1998, P EMMSEC98 EUR MULT, P513
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Swanson MD, 1998, IEEE J SEL AREA COMM, V16, P540, DOI 10.1109/49.668976
   *TR, 2000, 180341 TR ISO ISOIEC
NR 22
TC 51
Z9 56
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 23
EP 32
DI 10.1109/TMM.2004.840594
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300003
DA 2024-07-18
ER

PT J
AU Ko, B
   Byun, H
AF Ko, B
   Byun, H
TI FRIP: A region-based image retrieval tool using automatic image
   segmentation and stepwise Boolean AND matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayes' theorem; Boolean AND matching; content-based image retrieval
   (CBIR); finding region in the pictures (FRIP)
AB In this paper, we present our region-based image retrieval tool, finding region in the picture (FRIP), that is able to accommodate, to the extent possible, region scaling, rotation, and translation. Our goal is to develop an effective retrieval system to overcome a few limitations associated with existing systems. To do this, we propose adaptive circular filters used for semantic image segmentation, which are based on both Bayes' theorem and texture distribution of image. In addition, to decrease the computational complexity without losing the accuracy of the search results, we extract optimal feature vectors from segmented regions and apply them to our stepwise Boolean AND matching scheme. The experimental results using real world images show that our system can indeed improve retrieval performance compared to other global property-based or region-of-interest-based image retrieval methods.
C1 Samsung Elect, Suwon 442600, South Korea.
   Yonsei Univ, Dept Comp Sci, Seoul 120749, South Korea.
C3 Samsung; Samsung Electronics; Yonsei University
RP Samsung Elect, Suwon 442600, South Korea.
EM byoungchul.ko@samsung.com; hrbyun@aipiri.yonsei.ac.kr
RI Byun, Hyeran/G-8146-2012
CR [Anonymous], IEEE COMPUT
   [Anonymous], 1995, STORAGE RETRIEVAL IM, DOI DOI 10.1117/12.205308
   Burrus S., 1998, Introduction to Wavelets and Wavelet Transformations: A Primer
   CARSON M, 1999, P INT C VIS INF SYS, P509
   CHANG CC, 1991, PATTERN RECOGN, V24, P1053, DOI 10.1016/0031-3203(91)90121-K
   DENG Y, 1999, P IEEE INT C COMP VI, V2, P447
   GONG Y, 1994, P IEEE INT C MULT CO, P121
   HAFNER J, 1995, IEEE T PATTERN ANAL, V17, P729, DOI 10.1109/34.391417
   Hua KA, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P225, DOI 10.1145/319463.319610
   HUANG J, 1997, P IEEE C COMP VIS PA, P369
   Ikonomakis N, 2000, IEEE IMAGE PROC, P537, DOI 10.1109/ICIP.2000.901014
   Jia Li, 2000, Proceedings ACM Multimedia 2000, P147
   KAN AH, 2000, P 6 EUR C COMP VIS 2
   Ko B, 2002, INT C PATT RECOG, P138, DOI 10.1109/ICPR.2002.1047418
   Ko B, 2001, PATTERN ANAL APPL, V4, P174, DOI 10.1007/s100440170015
   Ko BC, 2000, ELECTRON LETT, V36, P24, DOI 10.1049/el:20000047
   KO BC, 2002, P INT C IM VID RETR, V2303, P81
   Ma WY, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P568, DOI 10.1109/ICIP.1997.647976
   Moghaddam B, 1999, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES (CBAIVL'99) - PROCEEDINGS, P89, DOI 10.1109/IVL.1999.781130
   Moghaddamzadeh A, 1997, PATTERN RECOGN, V30, P867, DOI 10.1016/S0031-3203(96)00084-2
   Pauwels EJ, 1999, COMPUT VIS IMAGE UND, V75, P73, DOI 10.1006/cviu.1999.0763
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   SUNDARAM H, 1999, P SPIE STOR RETR I 7, V3656, P108
   SWAIN MJ, 1993, P SOC PHOTO-OPT INS, V1908, P95, DOI 10.1117/12.143659
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Tian Q, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P746, DOI 10.1109/ICIP.2000.899562
   Tieu K, 2000, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2000.855824
NR 28
TC 47
Z9 52
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 105
EP 113
DI 10.1109/TMM.2004.840603
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300011
DA 2024-07-18
ER

PT J
AU Ramkumar, M
   Akansu, AN
AF Ramkumar, M
   Akansu, AN
TI On the design of data hiding methods robust to lossy compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE data compression; data hiding; watermarking
AB The problem of efficient data hiding robust to lossy compression is divided into two subproblems. First is to maximize the resource-which is the permitted distortion of images. The second is the efficient use of the resource by means of sophisticated signaling techniques. We conclude that a good solution to the first problem is choosing magnitude discrete Fourier transform as the domain in which the message signal is embedded. For the signaling method we use a sophisticated signaling technique, using periodic functions to tile the space with signal constellations.
C1 Mississippi State Univ, Dept Comp Sci & Engn, Mississippi State, MS 39762 USA.
   New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
C3 Mississippi State University; New Jersey Institute of Technology
RP Mississippi State Univ, Dept Comp Sci & Engn, Mississippi State, MS 39762 USA.
EM ramkumar@cse.msstate.edu; ali@njit.edu
CR [Anonymous], 2004, Data hiding fundamentals and applications: content security in digital multimedia
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   CULTUC D, 1999, P SPIE SECURITY WATE, V3657, P252
   GINTER KL, 1999, Patent No. 5917912
   KUTER M, 1999, P EL IM 99 SEC WAT M, V3657, P226
   LEGGE GE, 1980, J OPT SOC AM, V70, P1458, DOI 10.1364/JOSA.70.001458
   OPPENHEIM AV, 1981, P IEEE, V69, P529, DOI 10.1109/PROC.1981.12022
   PEARLMAN WA, 1978, IEEE T INFORM THEORY, V24, P683, DOI 10.1109/TIT.1978.1055950
   RAMKUMAR M, 1999, P IEEE INT S CIRC SY, V3, P520
   RAMKUMAR M, 1999, P 33 ASILOMAR C SIGN
   RAMKUMAR M, 2000, P IEEE ICASSP 2000, V4, P1979
   RAMKUMAR M, 1999, P SPIE INT WORKSH VO
   Rongen PMJ, 1999, PROC SPIE, V3657, P273, DOI 10.1117/12.344676
   SWANSON MD, 1996, P IEEE INT C IM PROC, V3, P211
   TESCHER AG, 1978, TECH US CIPI PUB, V510
   TEWFIK AH, WHITE PAPER DATA EMB
   Wicker S. B., 1995, Error Control Systems for Digital Communication and Storage, Englewood Cliffs, V1st
NR 17
TC 5
Z9 7
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 947
EP 951
DI 10.1109/TMM.2004.837254
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200016
DA 2024-07-18
ER

PT J
AU Wu, KL
   Yu, PS
   Wolf, JL
AF Wu, KL
   Yu, PS
   Wolf, JL
TI Segmentation of multimedia streams for proxy caching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE media segmentation; multimedia streaming; proxy caching; pyramid
   segmentation; segmentation-based caching; video caching
ID VIDEO; SYSTEMS
AB Proxy caching of large multimedia objects on the edge of the Internet has become increasingly important for reducing network latency. For a large media object, such as a two-hour video, treating the whole media as a single object for caching is not appropriate. In this paper, we study three media segmentation approaches to proxy caching: fixed, pyramid, and skyscraper. Blocks of a media stream are grouped into various segments for cache management. The cache admission and replacement policies attach different caching priorities to individual segments, taking into account the access frequency of the media object and the segment distance from the start of the media. These caching policies give preferential treatment to the beginning segments. As such, most user requests can be quickly played back from the proxy servers without delay. Event-driven simulations are conducted to evaluate the segmentation approaches and compare them with whole media caching. The results show that: 1) compared with whole media caching, segmentation-based caching is more effective not only in increased byte-hit ratio but also in lowered fraction of requests that requires delayed start; 2) pyramid segmentation, where segment size increases exponentially, is the best segmentation approach; and 3) segmentation-based caching is especially advantageous when the cache size is limited, when the set of hot media objects changes over time, when the media file size is large, and when there are a large number of distinct media objects.
C1 IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
C3 International Business Machines (IBM)
RP IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
EM klwu@us.ibm.com; psyu@us.ibm.com; jlw@us.ibm.com
RI Yu, Philip S/A-2815-2012
CR ACHARYA S, 2000, P 10 WORKSH NETW OP
   [Anonymous], 1949, Human behaviour and the principle of least-effort
   CHAE Y, 2002, IEEE J SEL AREA COMM, V7, P1328
   CHEN MS, 1994, P ACM MULT, P391
   Chiu MYM, 1998, IEEE T IND ELECTRON, V45, P44, DOI 10.1109/41.661304
   EAGER DL, 1999, P MULT COMP NETW SAN
   Gruber S, 2000, COMPUT NETW, V33, P657, DOI 10.1016/S1389-1286(00)00058-X
   GUO Y, 2001, PREFIX CACHING ASSIS
   Hua K., 1997, PROC SIGCOMM, P89
   JIN S, 2002, P INT C DISTR COMP S
   KAMATH M, 1995, P INT C DAT SYST ADV
   Krishnamurthy B., 2001, WEB PROTOCOLS PRACTI
   MIAO Z, 1999, P 10 INT PACK VID WO
   REJAIE R, 1999, P 4 INT WEB CACH WOR
   Salehi JD, 1998, IEEE ACM T NETWORK, V6, P397, DOI [10.1109/90.720873, 10.1142/S0218213097000219]
   SEN S, 1999, P IEEE INFOCOM NEW Y
   Shim J, 1999, IEEE T KNOWL DATA EN, V11, P549, DOI 10.1109/69.790804
   Viswanathan S, 1996, MULTIMEDIA SYST, V4, P197, DOI 10.1007/s005300050023
   WANG V, 2001, UMCS2001005 DEP COMP
   Wang YW, 1998, IEEE INFOCOM SER, P660, DOI 10.1109/INFCOM.1998.665087
   Wolf JL, 1997, MULTIMEDIA SYST, V5, P358, DOI 10.1007/s005300050067
   Wu KL, 2000, COMPUT NETW, V33, P633, DOI 10.1016/S1389-1286(00)00042-6
   WU KL, 2001, P 10 INT C WORLD WID, P36
NR 23
TC 30
Z9 41
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2004
VL 6
IS 5
BP 770
EP 780
DI 10.1109/TMM.2004.834870
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 854XI
UT WOS:000223936800009
DA 2024-07-18
ER

PT J
AU Zhang, DS
   Nunamaker, JF
AF Zhang, DS
   Nunamaker, JF
TI A natural language approach to content-based video indexing and
   retrieval for interactive E-learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE learning by asking; natural language processing; video indexing and
   retrieval
ID ACTIVE RETRIEVAL; NEWS
AB As a powerful and expressive nontextual media that can capture and present information, instructional videos are extensively used in e-Learning (Web-based distance learning). Since each video may cover many subjects, it is critical for an e-Learning environment to have content-based video searching capabilities to meet diverse individual learning needs. In this paper, we present an interactive multimedia-based e-Learning environment that enables users to interact with it to obtain knowledge in the form of logically segmented video clips. We propose a natural language approach to content-based video indexing and retrieval to identify appropriate video clips that can address users, needs. The method integrates natural language processing, named entity extraction, frame-based indexing, and information retrieval techniques to explore knowledge-on-demand in a video-based interactive e-Learning environment. A preliminary evaluation shows that precision and recall of this approach are better than those of the traditional keyword based approach.
C1 Univ Maryland, Dept Informat Syst, Baltimore, MD 21250 USA.
   Univ Arizona, Ctr Management Informat, Tucson, AZ 85721 USA.
C3 University System of Maryland; University of Maryland Baltimore;
   University of Arizona
RP Univ Maryland, Dept Informat Syst, Baltimore, MD 21250 USA.
EM zhangd@umbc.edu; jnunamaker@cmi.arizona.edu
CR Amir A., 2001, 34 HAW INT C SYST SC, P1662
   Ardizzone E, 1997, MULTIMED TOOLS APPL, V4, P29, DOI 10.1023/A:1009630331620
   Bikel DM, 1999, MACH LEARN, V34, P211, DOI 10.1023/A:1007558221122
   Boykin S, 2000, COMMUN ACM, V43, P35, DOI 10.1145/328236.328143
   BROWN M, 1996, 4 ACM ITN MULT C BOS, P307
   BURKE R, 1995, EXPERT SYST APPL, V9, P361, DOI 10.1016/0957-4174(95)00001-P
   Burke R, 1996, KNOWL-BASED SYST, V9, P491, DOI 10.1016/S0950-7051(97)00002-6
   Burke RD, 1997, AI MAG, V18, P57
   Carswell L., 1997, Proceeding of the 2nd conference on Integrating Technology into computer science education, P1
   Carville S, 2000, INNOV EDUC TRAIN INT, V37, P42, DOI 10.1080/135580000362070
   Chang HS, 1999, IEEE T CIRC SYST VID, V9, P1269, DOI 10.1109/76.809161
   Chang YL, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P306, DOI 10.1109/MMCS.1996.534992
   CHINCHOR N, MUC7 NAMED ENTITY TA
   Chiu P, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1329, DOI 10.1109/ICME.2000.871011
   Cook WalterAnthony., 1989, Case Grammar Theory
   Dagtas S, 2000, IEEE T IMAGE PROCESS, V9, P88, DOI 10.1109/83.817601
   DIMITROVA N, 1999, P ACM C INF KNOWL MA, P113
   GRISHMAN R, 1995, 6 MESS UND C MUC6, P167
   Gunsel B, 1997, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.1997.609413
   HAMPAPUR A, 1998, MULTIMEDIA DATA MANA, pCH9
   JAVED O, 2000, IASTED INT C INT MUL
   Jiang HT, 1998, IEEE T KNOWL DATA EN, V10, P947, DOI 10.1109/69.738359
   JOHNSON C, 2000, INTELLIGENCE, P17
   KIM SY, 1999, IEEE REG 10 C TENC 9, P1506
   Kuwano H, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P759, DOI 10.1109/ICME.2000.871472
   Lienhart R., 1996, Proceedings ACM Multimedia 96, P11, DOI 10.1145/244130.244137
   LITKOWSKI KC, 2000, 9 TEXT RETR C GAITH, P83
   Marinelli D, 1998, 1998 IEEE 4TH WORKSHOP INTERACTIVE VOICE TECHNOLOGY FOR TELECOMMUNICATIONS APPLICATIONS - IVTTA '98, P43, DOI 10.1109/IVTTA.1998.727691
   MORALES C, 2001, 2001 INF RES MAN ASS
   PAPADIAS D, 1999, 22 ANN INT ACM SIGIR, P168
   Pimentel MD, 2001, INTERACT COMPUT, V13, P353, DOI 10.1016/S0953-5438(00)00042-4
   Salton G., 1971, SMART RETRIEVAL SYST
   SIMPSON H, 1992, USE VIDEOTELETRAININ
   Smoliar S. W., 1994, IEEE Multimedia, V1, P62, DOI 10.1109/93.311653
   SRIHARI R, 1998, 8 TEXT RETR C TREC8, P185
   Syed M.R., 2001, IEEE Transactions on Multimedia, V8, P18, DOI DOI 10.1109/MMUL.2001.939996
   Tsekeridou S, 2001, IEEE T CIRC SYST VID, V11, P522, DOI 10.1109/76.915358
   VOORHEES EM, 1999, 8 TEXT RETR C GAITH
   VOUTILAINEN A, 2000, CORPORA GALORE ANAL
   Wactlar HD, 1999, COMPUTER, V32, P66, DOI 10.1109/2.745722
   Wactlar HD, 2000, COMMUN ACM, V43, P42, DOI 10.1145/328236.328144
   WACTLAR HD, 2000, INFORMEDIA SEARCH SU
   Wei Li, 1996, Proceedings of the 1st ACM International Conference on Digital Libraries, P19, DOI 10.1145/226931.226936
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
   ZHONG D, 1997, 1997 IEEE INT S CIRC, P1492
NR 45
TC 36
Z9 45
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 450
EP 458
DI 10.1109/TMM.2004.827505
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200008
DA 2024-07-18
ER

PT J
AU Lu, CS
   Liao, HYM
AF Lu, CS
   Liao, HYM
TI Structural digital signature for image authentication: An incidental
   distortion resistant scheme
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT ACM Multimedia and Security Workshop
CY 2000
CL LOS ANGELES, CA
SP ACM
DE authentication; digital signature; fragility; robustness; wavelet
   transform
ID WATERMARKING; COMPRESSION
AB The existing digital data verification methods are able to detect regions that have been tampered with, but are too fragile to resist incidental manipulations. This paper proposes a new digital signature scheme which makes use of an image's contents (in the wavelet transform domain) to construct a structural digital signature (SDS) for image authentication. The characteristic of the SDS is that it can tolerate content-preserving modifications while detecting content-changing modifications. Many incidental manipulations, which were detected as malicious modifications in the previous digital signature verification or fragile watermarking schemes, can be bypassed in the proposed scheme. Performance analysis is conducted and experimental results show that the new scheme is indeed superb for image authentication.
C1 Acad Sinica, Inst Sci Informat, Taipei 115, Taiwan.
C3 Academia Sinica - Taiwan
RP Acad Sinica, Inst Sci Informat, Taipei 115, Taiwan.
EM lcs@iis.sinica.edu.tw; liao@iis.sinica.edu.tw
RI Liao, Hong-Yuan Mark/AAQ-5514-2021
CR [Anonymous], P IEEE DIG SIGN PROC
   [Anonymous], IEEE C IMAGE PROCESS
   [Anonymous], IEEE INT C MULT COMP
   [Anonymous], IEEE INT C IMAGE PRO
   Buccigrossi RW, 1999, IEEE T IMAGE PROCESS, V8, P1688, DOI 10.1109/83.806616
   FRIEDMAN GL, 1993, IEEE T CONSUM ELECTR, V39, P905, DOI 10.1109/30.267415
   Kundur D, 1999, P IEEE, V87, P1167, DOI 10.1109/5.771070
   Lin CY, 2001, IEEE T CIRC SYST VID, V11, P153, DOI 10.1109/76.905982
   Lin E., 1999, P MULTIMEDIA SECURIT, P25
   Lu CS, 2000, IEEE T MULTIMEDIA, V2, P209, DOI 10.1109/6046.890056
   Lu CS, 2003, IEEE T MULTIMEDIA, V5, P161, DOI 10.1109/TMM.2003.811621
   Lu CS, 1997, PATTERN RECOGN, V30, P729, DOI 10.1016/S0031-3203(96)00116-1
   Lu CS, 2001, IEEE T IMAGE PROCESS, V10, P1579, DOI 10.1109/83.951542
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   WALTON S, 1995, DR DOBBS J, V20, P18
   Wolfgang RB, 1999, PROC SPIE, V3657, P40, DOI 10.1117/12.344700
   Xie LH, 2001, IEEE T IMAGE PROCESS, V10, P1754, DOI 10.1109/83.967402
   Yu GJ, 2001, OPT ENG, V40, P1396, DOI 10.1117/1.1384885
NR 18
TC 258
Z9 322
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2003
VL 5
IS 2
BP 161
EP 173
DI 10.1109/TMM.2003.811621
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 695HB
UT WOS:000183824100002
OA Green Published
DA 2024-07-18
ER

PT J
AU De Vleeschouwer, C
   Delaigle, JF
   Macq, B
AF De Vleeschouwer, C
   Delaigle, JF
   Macq, B
TI Circular interpretation of bijective transformations in lossless
   watermarking for media asset management
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE data hiding; data management; image processing; watermarking
ID IMAGE; COMPRESSION
AB The need for reversible or lossless watermarking methods has recently been highlighted to associate subliminal management information with losslessly processed media and to enable their authentication. This paper first analyzes the specificity and the application scope of lossless watermarking methods. It explains why early attempts to achieve reversibility are not satisfactory. They are restricted to well-chosen images, strictly lossless context and/or suffer from Annoying visual artifacts. Circular interpretation of bijective transformations is proposed to implement a method that fulfills all quality and functionality requirements of lossless watermarking. Results of several bench tests demonstrate the validity of the approach.
C1 Univ Catholique Louvain, B-1348 Louvain, Belgium.
C3 Universite Catholique Louvain
RP De Vleeschouwer, C (corresponding author), Univ Catholique Louvain, B-1348 Louvain, Belgium.
CR [Anonymous], 1993, DIGITAL IMAGE COMPUT
   Augot D, 1999, P IEEE, V87, P1251, DOI 10.1109/5.771076
   Bender W, 1996, IBM SYST J, V35, P313, DOI 10.1147/sj.353.0313
   Boulgouris NV, 2001, IEEE T IMAGE PROCESS, V10, P1, DOI 10.1109/83.892438
   Caronni G., 1995, VERLAESSLICHE ITSYST, P251
   Carpentieri B, 2000, P IEEE, V88, P1797, DOI 10.1109/5.892715
   COATRIEUX G, 2000, WORKSH INT TEL INF S, P250
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   FRIDRICH J, 1998, P ICIP 98 CHIC IL OC, P409
   FRIDRICH J, 2001, P SPIE 2001 SEC WAT
   KOLODNER S, 1999, FILMLESS RADIOLOGY
   Kundur D, 1999, P IEEE, V87, P1167, DOI 10.1109/5.771070
   Langelaar GC, 2000, IEEE SIGNAL PROC MAG, V17, P20, DOI 10.1109/79.879337
   MACQ B, 2000, P EUSIPCO 2000 TAMP
   MINTZER F, 1997, D LIB MAG        DEC
   RIVEST R, 1992, RFC1321 DDN NETW INF
   ROOS P, 1988, IEEE T MED IMAGING, V7, P328, DOI 10.1109/42.14516
   SIMMONS GJ, 1992, CONT CRYPTOLOGY SCI, pCH6
   Yeung MM, 1998, J ELECTRON IMAGING, V7, P578, DOI 10.1117/1.482612
NR 19
TC 290
Z9 300
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017-2394 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2003
VL 5
IS 1
BP 97
EP 105
DI 10.1109/TMM.2003.809729
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 675HP
UT WOS:000182688200008
DA 2024-07-18
ER

PT J
AU Bao, YM
   Zhao, X
   Qian, DH
AF Bao, Yiming
   Zhao, Xu
   Qian, Dahong
TI FusePose: IMU-Vision Sensor Fusion in Kinematic Space for Parametric
   Human Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D pose estimation; human kinematic model; IMUs; sensor fusion
ID MOTION CAPTURE; VIDEO
AB Commercial motion-capture systems produce excell- ent in-studio reconstructions, but offer no comparable solution for acquisition in everyday environments. We present a system for acquiring motions almost anywhere. This wearable system gathers ultrasonic time-of-flight and inertial measurements with a set of inexpensive miniature sensors worn on the garment. After recording, the information is combined using an Extended Kalman Filter to reconstruct joint configurations of a body. Experimental results show that even motions that are traditionally difficult to acquire are recorded with ease within their natural settings. Although our prototype does not reliably recover the global transformation, we show that the resulting motions are visually similar to the original ones, and that the combined acoustic and intertial system reduces the drift commonly observed in purely inertial systems. Our final results suggest that this system could become a versatile input device for a variety of augmented-reality applications.
C1 [Bao, Yiming; Qian, Dahong] Shanghai Jiao Tong Univ, Sch Biomed Engn, Shanghai 200240, Peoples R China.
   [Zhao, Xu] Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Qian, DH (corresponding author), Shanghai Jiao Tong Univ, Sch Biomed Engn, Shanghai 200240, Peoples R China.; Zhao, X (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
EM yiming.bao@sjtu.edu.cn; zhaoxu@sjtu.edu.cn; dahong.qian@sjtu.edu.cn
RI Bao, Yiming/ABF-9043-2020
OI Bao, Yiming/0000-0002-9080-9406
FU NSFC [62176156]; Deepwise Healthcare Joint Research Lab, Shanghai Jiao
   Tong University
FX This work was supported in part by NSFC under Grant 62176156, and in
   part by the Deepwise Healthcare Joint Research Lab, Shanghai Jiao Tong
   University.
CR Arnab A, 2019, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2019.00351
   Bachmann E.R., 2001, PROC ACM S VIRTUAL R, P9
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Csiszar A, 2017, I C MECH MACH VIS PR, P372
   Dabral R, 2018, LECT NOTES COMPUT SC, V11213, P679, DOI 10.1007/978-3-030-01240-3_41
   Del Rosario MB, 2018, IEEE SENS J, V18, P9332, DOI 10.1109/JSEN.2018.2864989
   Fan TS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11437, DOI 10.1109/ICCV48922.2021.01126
   Foxlin E, 1996, P IEEE VIRT REAL ANN, P185, DOI 10.1109/VRAIS.1996.490527
   Gao Q, 2019, IEEE T IND ELECTRON, V66, P9663, DOI 10.1109/TIE.2019.2898624
   Gilbert A, 2019, INT J COMPUT VISION, V127, P381, DOI 10.1007/s11263-018-1118-y
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Hanyue Tu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P197, DOI 10.1007/978-3-030-58452-8_12
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Helten T, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P279, DOI 10.1109/3DV.2013.44
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Huang FY, 2020, IEEE WINT CONF APPL, P418, DOI [10.1109/wacv45572.2020.9093526, 10.1109/WACV45572.2020.9093526]
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kingma D. P., 2015, PROC INT CON LEA REP
   Kocabas M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11107, DOI 10.1109/ICCV48922.2021.01094
   Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500
   Li JF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11005, DOI 10.1109/ICCV48922.2021.01084
   Li JF, 2021, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR46437.2021.00339
   Li YJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11293, DOI 10.1109/ICCV48922.2021.01112
   Lin JH, 2021, PROC CVPR IEEE, P11881, DOI 10.1109/CVPR46437.2021.01171
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P494, DOI 10.1109/TPAMI.2019.2894422
   Liu YB, 2013, IEEE T PATTERN ANAL, V35, P2720, DOI 10.1109/TPAMI.2013.47
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Malleson C, 2020, INT J COMPUT VISION, V128, P1594, DOI 10.1007/s11263-019-01270-5
   Malleson C, 2017, INT CONF 3D VISION, P449, DOI 10.1109/3DV.2017.00058
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Navaratnam R., 2005, PROC BRIT MACH VIS, P1
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pavllo D, 2020, INT J COMPUT VISION, V128, P855, DOI 10.1007/s11263-019-01245-6
   Pons-Moll G, 2010, PROC CVPR IEEE, P663, DOI 10.1109/CVPR.2010.5540153
   Qiu HB, 2019, IEEE I CONF COMP VIS, P4341, DOI 10.1109/ICCV.2019.00444
   Roetenberg D, 2005, IEEE T NEUR SYS REH, V13, P395, DOI 10.1109/TNSRE.2005.847353
   Roetenberg D., 2009, XSENS MOTION TECHNO, V3
   Shi MY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3407659
   Trumble M, 2017, BRIT MACHINE VISION
   Trumble M, 2018, LECT NOTES COMPUT SC, V11214, P800, DOI 10.1007/978-3-030-01249-6_48
   Villegas R, 2018, PROC CVPR IEEE, P8639, DOI 10.1109/CVPR.2018.00901
   Vitali RV, 2021, IEEE SENS J, V21, P3561, DOI 10.1109/JSEN.2020.3026895
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Vlasic D, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276421, 10.1145/1239451.1239486]
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   von Marcard T, 2016, IEEE T PATTERN ANAL, V38, P1533, DOI 10.1109/TPAMI.2016.2522398
   Wang, 2020, P IEEE CVF C COMP VI, P7376
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Ye M, 2014, PROC CVPR IEEE, P2353, DOI 10.1109/CVPR.2014.301
   Yi XY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459786
   Zhang Z, 2020, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR42600.2020.00227
   Zheng ZR, 2018, LECT NOTES COMPUT SC, V11213, P389, DOI 10.1007/978-3-030-01240-3_24
NR 57
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7736
EP 7746
DI 10.1109/TMM.2022.3227472
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cai, DS
   Qian, SS
   Fang, Q
   Hu, J
   Ding, WK
   Xu, CS
AF Cai, Desheng
   Qian, Shengsheng
   Fang, Quan
   Hu, Jun
   Ding, Wenkui
   Xu, Changsheng
TI Heterogeneous Graph Contrastive Learning Network for Personalized
   Micro-Video Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graph contrastive learning; heterogeneous graph; multi-modal;
   micro-video recommendation
ID INFORMATION
AB Personalized micro-video recommendation has attracted a lot of research attention with the growing popularity of micro-video sharing platforms. Many efforts have been made to consider micro-video recommendation as a matching task and shown promising performance, while they only focus on simple features or multi-modal attribute information. Recently, Graph Neural Networks (GNNs) have been employed in many recommendation tasks and achieved impressive success. However, these GNN-based methods may suffer from the following limitations: (1) fail to capture the heterogeneity of nodes in user-video bipartite graphs; (2) ignore the non-local (global) semantic correlation information remained in heterogeneous graphs. In this paper, we present a novel approach, Heterogeneous Graph Contrastive Learning Network (HGCL), for personalized micro-video recommendation. To consider heterogeneity in user-video bipartite graphs, we first introduce a heterogeneous graph encoder network for a high-quality representation learning of users and micro-videos. Specifically, we design a random surfing model to generate node-type specific homogeneous graphs to preserve the heterogeneity. Then we propose a graph contrastive learning framework to achieve representation learning on each node-type specific homogeneous graph by maximizing the mutual information between local patches of a graph and the global representation of the entire graph. Finally, a type-crossing objective function is proposed to jointly integrate the node embeddings from different node types to facilitate high-quality representation learning. Experimental results on real-world datasets in the micro-video recommendation task validate the performance of our method, compared with state-of-the-art baseline algorithms.
C1 [Cai, Desheng] Hefei Univ Technol, Hefei 230009, Peoples R China.
   [Qian, Shengsheng; Fang, Quan; Hu, Jun; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, 100190, Peoples R China.
   [Qian, Shengsheng; Fang, Quan; Hu, Jun] Univ Chinese Acad Sci, Sch Artificial Intelligence, Natl Lab Pattern Recognit, Beijing 100049, Peoples R China.
   [Ding, Wenkui] Kwai Techonol Co Ltd, Beijing 100085, Peoples R China.
C3 Hefei University of Technology; Chinese Academy of Sciences; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, 100190, Peoples R China.
EM 457238187@qq.com; shengsheng.qian@nlpr.ia.ac.cn; qfang@nlpr.ia.ac.cn;
   hujunxianligong@gmail.com; dingwenkui@kuaishou.com; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
OI xu, chang sheng/0000-0001-8343-9665
FU National Natural Science Foundation of China [62036012, 61936005,
   61832002, 6207072426, 61720106006, 62072456, 61872199, 61872424]; Key
   Research Program of Frontier Sciences, CAS [QYZDJ-SSW-JSC039]; K.C.Wong
   Education Foundation; CCF-Tencent Open Fund; Open Research Projects of
   Zhejiang Lab [2021KE0AB05]; Tencent WeChat Rhino-Bird Focused Research
   Program
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants, 62036012, 61936005, 61832002,
   6207072426, 61720106006, 62072456, 61872199, and 61872424, in part the
   Key Research Program of Frontier Sciences, CAS under Grant
   QYZDJ-SSW-JSC039, in part by the K.C.Wong Education Foundation, in part
   by CCF-Tencent Open Fund, in part by Open Research Projects of Zhejiang
   Lab under Grant 2021KE0AB05, and in part by the Tencent WeChat
   Rhino-Bird Focused Research Program.& nbsp;
CR Belghazi MI, 2018, PR MACH LEARN RES, V80
   Cai DS, 2022, Arxiv, DOI arXiv:2111.10342
   Casanova Arantxa, 2018, INT C LEARNING REPRE
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen XS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1146, DOI 10.1145/3240508.3240617
   Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190
   Ferracani A, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P351, DOI 10.1145/2911996.2912066
   Guo HF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1725
   Hamilton WL, 2017, ADV NEUR IN, V30
   Han TT, 2017, IEEE T MULTIMEDIA, V19, P712, DOI 10.1109/TMM.2016.2631881
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   He XN, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P549, DOI 10.1145/2911451.2911489
   Hjelm R. D., 2019, PROC INT C LEARN REP, P1, DOI [DOI 10.48550/ARXIV.1808.06670, 10.48550/arXiv.1808.06670]
   Hu BB, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1531, DOI 10.1145/3219819.3219965
   Huang L, 2018, LECT NOTES COMPUT SC, V10735, P564, DOI 10.1007/978-3-319-77380-3_54
   Huang QH, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2598779
   Huang W., 2020, COMPUT RES REPOSITOR
   Huang YX, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P35, DOI 10.1145/2882903.2903743
   Khoshgoftaar T.M., 2009, ADV ARTIFICIAL INTEL, DOI [10.1155/2009/421425, DOI 10.1155/2009/421425]
   Kingma D. P., 2014, arXiv
   Kipf T. N., 2017, 8 INT C LEARN REPR, P1
   Koenigstein N., 2013, 7th ACM Conf. on Rec. Systems, P129, DOI DOI 10.1145/2507157.2507168
   Le Quoc V., 2014, P INT C MACH LEARN I
   LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36
   Liu S, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3020, DOI 10.1145/3308558.3313513
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma JW, 2018, MULTIMED TOOLS APPL, V77, P2991, DOI 10.1007/s11042-017-4827-2
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Qian SS, 2021, AAAI CONF ARTIF INTE, V35, P2440
   Qian SS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451215
   Ravanelli M, 2019, INTERSPEECH, P1153, DOI 10.21437/Interspeech.2019-2380
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Rendle S, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168771
   Sang L, 2021, IEEE T MULTIMEDIA, V23, P2019, DOI 10.1109/TMM.2020.3007330
   Shepstone SE, 2014, IEEE T MULTIMEDIA, V16, P1999, DOI 10.1109/TMM.2014.2337845
   Shi C, 2019, WORLD WIDE WEB, V22, P153, DOI 10.1007/s11280-018-0553-6
   Shi C, 2019, IEEE T KNOWL DATA EN, V31, P357, DOI 10.1109/TKDE.2018.2833443
   Shi C, 2016, KNOWL INF SYST, V49, P835, DOI 10.1007/s10115-016-0925-0
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Velickovic P., 2019, ARXIV180910341, P1
   Wang X, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2022, DOI 10.1145/3308558.3313562
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Wu M, 2020, IEEE DATA MINING, P681, DOI 10.1109/ICDM50108.2020.00077
   Wu M, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1457, DOI 10.1145/3366423.3380219
   Xu J, 2020, FOUND TRENDS INF RET, V14, P102, DOI 10.1561/1500000076
   Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890
   Zhang CX, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P793, DOI 10.1145/3292500.3330961
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zheng J., 2017, INT J DATA SCI ANAL, V3, P35, DOI [DOI 10.1007/S41060-016-0031-0, 10.1007/s41060-016-0031-0]
   Zhou P, 2016, IEEE T MULTIMEDIA, V18, P1217, DOI 10.1109/TMM.2016.2537216
NR 53
TC 14
Z9 14
U1 8
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2761
EP 2773
DI 10.1109/TMM.2022.3151026
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600024
DA 2024-07-18
ER

PT J
AU Choi, JY
   Lee, B
AF Choi, Jae Young
   Lee, Bumshik
TI Combining Deep Convolutional Neural Networks With Stochastic Ensemble
   Weight Optimization for Facial Expression Recognition in the Wild
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial expression recognition; ensemble deep learning; stochastic
   ensemble weight optimization; simulated annealing; energy function; deep
   ensemble generalization error
ID CLASSIFICATION; DIVERSITY
AB Although recent emotion recognition methods (based on facial expression cues) achieve excellent performance in controlled scenarios, the recognition of emotion in the wild remains a challenging problem because of occlusion, large head poses, illumination variations, etc. Recent advances in deep learning show that combining an ensemble of deep learning models can considerably outperform the approach of using only a single deep learning model for challenging recognition problems. This paper presents a novel ensemble deep learning method, "deep convolutional neural network (DCNN) ensemble classifier ", for improved facial expression recognition (FER) in the wild. Our proposed DCNN ensemble classifier is novel in terms of the following aspects: (1) the process of finding ensemble weights for combining DCNN decision outputs is formulated as a stochastic optimization problem (via simulated annealing) in which the energy to be minimized represents the generalized (test) classification error of the DCNN ensemble and (2) for the creation of DCNN ensemble members, we propose the combined use of different types of face representations and bagging (T. G. Dietterich, 2000), which is quite useful in increasing the diversity of the DCNN ensemble. Extensive and comparative experiments on three wild FER datasets, namely FER2013, SFEW2.0, and RAF-DB, show that the proposed DCNN ensemble classifier achieves competitive FER performances when compared with other recently developed methods-76.69%, 58.68%, and 87.13% of FER accuracy under the FER2013, SFEW2.0, and RAF-DB evaluation protocols, respectively.
C1 [Choi, Jae Young] Hankuk Univ Foreign Studies, Div Comp Engn, Yongin 17025, Gyeonggi Do, South Korea.
   [Lee, Bumshik] Chosun Univ, Informat & Commun Engn, Gwangju 61452, South Korea.
C3 Hankuk University Foreign Studies; Chosun University
RP Lee, B (corresponding author), Chosun Univ, Informat & Commun Engn, Gwangju 61452, South Korea.
EM jychoi@hufs.ac.kr; bslee@chosun.ac.kr
OI Lee, Bumshik/0000-0003-2482-1869
FU Hankuk University; National Research Foundation of Korea (NRF) -Korea
   government (MSIT) [2021R1A2C1092322]; Institute of Information &
   Communications Technology Planning & Evaluation (IITP)-Korea government
   (MSIT) [1711134404]
FX This work was supported in part by the Hankuk University of Foreign
   Studies Research Fund, in part by the National Research Foundation of
   Korea (NRF) grant funded by the Korea government (MSIT) under Grant
   2021R1A2C1092322, in part by Institute of Information & Communications
   Technology Planning & Evaluation (IITP) grant funded by the Korea
   government (MSIT) under Grant 1711134404.
CR Abbass H., 2001, Australasia-Japan Workshop on Intelligent and Evolutionary Systems, P45
   Acharya D, 2018, IEEE COMPUT SOC CONF, P480, DOI 10.1109/CVPRW.2018.00077
   Ali G, 2016, PATTERN RECOGN, V55, P14, DOI 10.1016/j.patcog.2016.01.032
   [Anonymous], 1993, Neural Networks for Speech and Image Processing
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Brown G, 2005, J MACH LEARN RES, V6, P1621
   Brown G., 2005, Information Fusion, V6, P5, DOI 10.1016/j.inffus.2004.04.004
   Brown G., 2004, THESIS BIRMINGHAM U
   Chen HH, 2010, IEEE T KNOWL DATA EN, V22, P1738, DOI 10.1109/TKDE.2010.26
   Cheng D, 2015, IEEE INT C SEMANT CO, P32, DOI 10.1109/ICOSC.2015.7050775
   Choi JY, 2012, IEEE T IMAGE PROCESS, V21, P1366, DOI 10.1109/TIP.2011.2168413
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Connie Tee, 2017, Multi-disciplinary Trends in Artificial Intelligence. 11th International Workshop, MIWAI 2017. Proceedings: LNAI 10607, P139, DOI 10.1007/978-3-319-69456-6_12
   De la Torre F, 2015, IEEE INT CONF AUTOMA
   Dhall A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2106, DOI 10.1109/ICCVW.2011.6130508
   Dietterich TG, 2000, MACH LEARN, V40, P139, DOI 10.1023/A:1007607513941
   Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1
   Ding H, 2017, IEEE INT CONF AUTOMA, P118, DOI 10.1109/FG.2017.23
   Duda R., 1973, Pattern Classification and Scene Analysis
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Gan YL, 2019, PATTERN RECOGN LETT, V125, P105, DOI 10.1016/j.patrec.2019.04.002
   Gehrig Tobias., 2013, P 2013 EMOTION RECOG, P9, DOI DOI 10.1145/2531923
   GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1
   Georgescu MI, 2019, IEEE ACCESS, V7, P64827, DOI 10.1109/ACCESS.2019.2917266
   Ghayoumi M., 2017, J. Commun. Comput, V14, P34
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Joo J, 2014, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2014.35
   Kahou SE, 2016, J MULTIMODAL USER IN, V10, P99, DOI 10.1007/s12193-015-0195-2
   Kim BK, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P427, DOI 10.1145/2818346.2830590
   Kim BK, 2016, J MULTIMODAL USER IN, V10, P173, DOI 10.1007/s12193-015-0209-0
   Kim YD, 2016, PROC CVPR IEEE, P5318, DOI 10.1109/CVPR.2016.574
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231
   Kuncheva LI, 2003, MACH LEARN, V51, P181, DOI 10.1023/A:1022859003006
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li DY, 2018, MULTIMED TOOLS APPL, V77, P15251, DOI 10.1007/s11042-017-5105-z
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Liu MY, 2015, NEUROCOMPUTING, V159, P126, DOI 10.1016/j.neucom.2015.02.011
   Liu XF, 2017, IEEE COMPUT SOC CONF, P522, DOI 10.1109/CVPRW.2017.79
   Lopes AT, 2017, PATTERN RECOGN, V61, P610, DOI 10.1016/j.patcog.2016.07.026
   Ly T. S., 2019, IEEE COMPUT SOC CONF, V92, P1
   Malli RC, 2016, IEEE COMPUT SOC CONF, P714, DOI 10.1109/CVPRW.2016.94
   Meng ZB, 2017, IEEE INT CONF AUTOMA, P558, DOI 10.1109/FG.2017.140
   Mollahosseini A, 2016, IEEE COMPUT SOC CONF, P1509, DOI 10.1109/CVPRW.2016.188
   Momin R., 2021, ARXIV
   Optiz D. W., 1996, P ADV NEUR INF PROC, P531
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Pentina A., 2016, ADV NEURAL INFORM PR, P3612
   Pramerdorfer C, 2016, Arxiv, DOI arXiv:1612.02903
   Ranawana R., 2009, INFORM SCIENCES, V179, P1298
   Ren Y, 2016, IEEE COMPUT INTELL M, V11, P41, DOI 10.1109/MCI.2015.2471235
   Rokach L, 2009, COMPUT STAT DATA AN, V53, P4046, DOI 10.1016/j.csda.2009.07.017
   RUTENBAR RA, 1989, IEEE CIRCUITS DEVICE, V5, P19, DOI 10.1109/101.17235
   Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sexton R. S., 1999, Journal of End User Computing, V11, P3
   Shao J, 2019, NEUROCOMPUTING, V355, P82, DOI 10.1016/j.neucom.2019.05.005
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Siqueira H, 2020, AAAI CONF ARTIF INTE, V34, P5800
   Tang YC, 2015, Arxiv, DOI arXiv:1306.0239
   Ueda N, 1996, IEEE IJCNN, P90, DOI 10.1109/ICNN.1996.548872
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang ZN, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107694
   Yang JL, 2017, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2017.554
   Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435
   Zafeiriou S., 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, P36
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeng JB, 2018, LECT NOTES COMPUT SC, V11217, P227, DOI 10.1007/978-3-030-01261-8_14
   Zhou Z.-H., 2012, ENSEMBLE METHODS FDN, DOI DOI 10.1201/B12207
NR 74
TC 5
Z9 5
U1 5
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 100
EP 111
DI 10.1109/TMM.2021.3121547
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400008
DA 2024-07-18
ER

PT J
AU Cui, LZ
   Ni, ER
   Zhou, YP
   Wang, Z
   Zhang, L
   Liu, JC
   Xu, YD
AF Cui, Laizhong
   Ni, Erchao
   Zhou, Yipeng
   Wang, Zhi
   Zhang, Lei
   Liu, Jiangchuan
   Xu, Yuedong
TI Towards Real-Time Video Caching at Edge Servers: A Cost-Aware Deep
   Q-Learning Solution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Reinforcement learning; deep Q-learning; edge cache; video popularity
ID MOBILE; NETWORKS
AB Given the rapid growth of user-generated videos, internet traffic has been heavily dominated by online video streaming. Caching videos on edge servers in close proximity to users has been an effective approach to reduce the backbone traffic and the request response time, as well as to improve the video quality on the user side. Video popularity, however, can be highly dynamic over time. The cost of cache replacement at edge servers, particularly that related to service interruption during replacement, is not yet well understood. This paper presents a novel lightweight video caching algorithm for edge servers, seeking to optimize the hit rate with real-time decisions and minimized cost. Inspired by recent advances in deep Q-learning, our DQN-based online video caching (DQN-OVC) makes effective use of the rich and readily available information from users and networks. We decompose the Q-value function as a product of the video value function and the action function, which significantly reduces the state space. We instantiate the action function for cost-aware caching decisions with low complexity so that the cached videos can be updated continuously and instantly with dynamic video popularity. We used video traces from Tencent, one of the largest online video providers in China, to evaluate the performance of our DQN-OVC and to compare it with state-of-the-art solutions. The results demonstrate that DQN-OVC significantly outperforms the baseline algorithms in the edge caching context.
C1 [Cui, Laizhong; Ni, Erchao; Zhang, Lei] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Cui, Laizhong; Ni, Erchao; Zhang, Lei] Shenzhen Univ, Guangdong Lab Artificial Intelligence & Cyber Econ, Shenzhen 518060, Peoples R China.
   [Zhou, Yipeng] Macquarie Univ, Fac Sci & Engn, Dept Comp, Sydney, NSW 2109, Australia.
   [Zhou, Yipeng] Peng Cheng Lab, Shenzhen 518000, Guangdong, Peoples R China.
   [Wang, Zhi] Tsinghua Shenzhen Int Grad Sch, Shenzhen 518000, Guangdong, Peoples R China.
   [Liu, Jiangchuan] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
   [Xu, Yuedong] Fudan Univ, Dept Elect Engn, Shanghai 200433, Peoples R China.
C3 Shenzhen University; Shenzhen University; Macquarie University; Peng
   Cheng Laboratory; Tsinghua Shenzhen International Graduate School; Simon
   Fraser University; Fudan University
RP Zhou, YP (corresponding author), Macquarie Univ, Fac Sci & Engn, Dept Comp, Sydney, NSW 2109, Australia.; Zhou, YP (corresponding author), Peng Cheng Lab, Shenzhen 518000, Guangdong, Peoples R China.
EM cuilz@szu.edu.cn; nierchao5@qq.com; yipeng.zhou@mq.edu.au;
   wangzhi@sz.tsinghua.edu.cn; leizhang@szu.edu.cn; jcliu@cs.sfu.ca;
   ydxu@fudan.edu.cn
RI Wang, Zhi/GZB-2713-2022; Zhang, Lei/N-7527-2015; Cui,
   Laizhong/AAX-9571-2020
OI Wang, Zhi/0000-0001-6952-8848; Wang, Zhi/0000-0002-5462-6178; Zhou,
   Yipeng/0000-0003-1533-0865
FU National Key R&D Program of China [2018YFB1800302, 2018YFB1800805];
   National Natural Science Foundation of China [2021A1515012633]; Shenzhen
   Science and Technology Program [RCYX20200714114645048,
   JCYJ20190808142207420, GJHZ20190822095416463, RCYX20200714114523079];
   Pearl River Young Scholars funding of Shenzhen University; PCL Future
   Greater-Bay Area Network Facilities for Large-scale Ex-periments and
   Applications [LZC0019]; Australia Research Council [DE180100950]
FX This work was supported in part by the National Key R&D Program of China
   under Grants 2018YFB1800302 and 2018YFB1800805, in part by the National
   Natural Science Foundation of China under Grants 61772345, 61772139,
   61902257, and 61872215, in part by the Natural Science Foundation of
   Guangdong Province under Grant 2021A1515012633, in part by Shenzhen
   Science and Technology Program under Grants RCYX20200714114645048,
   JCYJ20190808142207420, GJHZ20190822095416463, and RCYX20200714114523079,
   in part by the Pearl River Young Scholars funding of Shenzhen
   University, in part by the Project PCL Future Greater-Bay Area Network
   Facilities for Large-scale Ex-periments and Applications (LZC0019), and
   in part by Australia Research Council under Grant DE180100950. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Prof. Mea Wang.
CR Akhtar Z, 2019, PROCEEDINGS OF THE 15TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT '19), P305, DOI 10.1145/3359989.3365423
   Beckmann N, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P389
   Berger DS, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P483
   Chen LX, 2020, IEEE T MULTIMEDIA, V22, P432, DOI 10.1109/TMM.2019.2929004
   Cisco Systems, 2017, Tech. Rep.
   Cullen Cam, 2019, SANDVINE PRESSEMITTE
   Fedchenko Vladyslav, 2018, ACM SIGMETRICS Performance Evaluation Review, V46, P139, DOI 10.1145/3308897.3308958
   Goian HS, 2019, IEEE ACCESS, V7, P27699, DOI 10.1109/ACCESS.2019.2898734
   Guo YD, 2019, IEEE T MULTIMEDIA, V21, P2903, DOI 10.1109/TMM.2019.2912703
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar S, 2018, 12 IEEE INT C ADV NE
   Kwak J, 2018, IEEE T WIREL COMMUN, V17, P3030, DOI 10.1109/TWC.2018.2805893
   Liu D, 2016, IEEE COMMUN MAG, V54, P22, DOI 10.1109/MCOM.2016.7565183
   Maggi L, 2015, Arxiv, DOI arXiv:1512.03274
   Mao YY, 2017, IEEE COMMUN SURV TUT, V19, P2322, DOI 10.1109/COMST.2017.2745201
   Mehrabi A, 2018, IEEE ACCESS, V6, P52261, DOI 10.1109/ACCESS.2018.2870855
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mukherjee M, 2018, IEEE COMMUN SURV TUT, V20, P1826, DOI 10.1109/COMST.2018.2814571
   Narayanan A, 2018, P 2018 WORKSHOP NETW, P48, DOI DOI 10.1145/3229543.3229555
   Pang H., 2018, 2018 IEEE ACM 26 INT, P1
   Park S.-H., 2016, PROC IEEE 17 INT WOR, P1
   Peng MG, 2016, IEEE NETWORK, V30, P46, DOI 10.1109/MNET.2016.7513863
   Piao ZY, 2019, IEEE INTERNET THINGS, V6, P1010, DOI 10.1109/JIOT.2018.2866709
   Ren J, 2017, IEEE NETWORK, V31, P96, DOI 10.1109/MNET.2017.1700030
   Song ZY, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P529
   Sung J, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P547, DOI [10.1109/ICMLA.2016.0096, 10.1109/ICMLA.2016.85]
   Tran TX, 2017, IEEE COMMUN MAG, V55, P54, DOI 10.1109/MCOM.2017.1600863
   Wang S, 2017, IEEE ACCESS, V5, P6757, DOI 10.1109/ACCESS.2017.2685434
   Wang XF, 2014, IEEE COMMUN MAG, V52, P131, DOI 10.1109/MCOM.2014.6736753
   Yang P, 2019, IEEE T MULTIMEDIA, V21, P915, DOI 10.1109/TMM.2018.2870521
   Yao JJ, 2019, IEEE COMMUN SURV TUT, V21, P2525, DOI 10.1109/COMST.2019.2908280
   Ye Z, 2017, 2017 PROCEEDINGS OF THE 29TH INTERNATIONAL TELETRAFFIC CONGRESS (ITC 29), VOL 1, P205, DOI [10.1109/ITC.2017.19, 10.23919/ITC.2017.8064357]
   Yu W, 2018, IEEE ACCESS, V6, P6900, DOI 10.1109/ACCESS.2017.2778504
   Zhang M, 2015, IEEE COMMUN SURV TUT, V17, P1473, DOI 10.1109/COMST.2015.2420097
   Zhong C, 2020, IEEE T COGN COMMUN, V6, P48, DOI 10.1109/TCCN.2020.2968326
   Zhong C, 2018, 2018 52ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS), DOI 10.1109/ciss.2018.8362276
   Zhou P, 2019, IEEE T MULTIMEDIA, V21, P539, DOI 10.1109/TMM.2018.2885509
   Zhou YP, 2015, IEEE T MULTIMEDIA, V17, P1273, DOI 10.1109/TMM.2015.2447277
NR 38
TC 2
Z9 2
U1 3
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 302
EP 314
DI 10.1109/TMM.2021.3125803
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400021
DA 2024-07-18
ER

PT J
AU Ding, XF
   Zeng, TY
   Tang, J
   Che, ZP
   Peng, YX
AF Ding, Xiaofeng
   Zeng, Tieyong
   Tang, Jian
   Che, Zhengping
   Peng, Yaxin
TI SRRNet: A Semantic Representation Refinement Network for Image
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Segmentation; semantic representation; convolutional neural networks;
   semantic embedding; semantic attention
AB Semantic context has raised concerns in semantic segmentation. In most cases, it is applied to guide feature learning. Instead, this paper applies it to extract the semantic representation, which records the global feature information of each category with a memory tensor. Specifically, we propose a novel semantic representation (SR) module, which consists of semantic embedding (SE) and semantic attention (SA) blocks. The SE block adaptively embeds features into the semantic representation by calculating the memory similarity, and the SA block aggregates the embedded features with semantic attention. The main advantages of the SR module lie in three aspects: i) it enhances the representation ability of semantic context by employing global (cross-image) semantic information; ii) it improves the consistency of intraclass features by aggregating global features of the same categories; and iii) it can be extended to build a semantic representation refinement network (SRRNet) by iteratively applying the SR module across multiple scales, shrinking the semantic gap and enhancing the structural reasoning of the model. Extensive experiments demonstrate that our method significantly improves the segmentation results and achieves superior performance on the PASCAL VOC 2012, Cityscapes, and PASCAL Context datasets.
C1 [Ding, Xiaofeng; Peng, Yaxin] Shanghai Univ, Sch Sci, Dept Math, Shanghai 200444, Peoples R China.
   [Zeng, Tieyong] Chinese Univ Hong Kong, Dept Math, Hong Kong, Peoples R China.
   [Tang, Jian; Che, Zhengping] Midea Grp, Shanghai 201702, Peoples R China.
C3 Shanghai University; Chinese University of Hong Kong; Midea
RP Peng, YX (corresponding author), Shanghai Univ, Sch Sci, Dept Math, Shanghai 200444, Peoples R China.
EM dxfeng@shu.edu.cn; zeng@math.cuhk.edu.hk; tangjian22@midea.com;
   chezhengping@gmail.com; yaxin.peng@shu.edu.cn
RI Zeng, Tieyong/B-7147-2009; Che, Zhengping/U-2509-2019; ding,
   xiao/KAM-4458-2024
OI Che, Zhengping/0000-0001-6818-1125; ZENG, Tieyong/0000-0002-0688-202X;
   Ding, Xiaofeng/0000-0003-1261-5195
FU National Key R&D Program of China [2021YFE0203700]; National Natural
   Science Foundation of China [11771276]; "Shuguang Program" of Shanghai
   Education Development Foundation; Shanghai Municipal Education
   Commission [20SG40]; Program of Shanghai Academic Research Leader
   [20XD1421700]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021YFE0203700, in part by the National Natural Science
   Foundation of China under Grant 11771276, in part by "Shuguang Program"
   of Shanghai Education Development Foundation and Shanghai Municipal
   Education Commission under Grant 20SG40, and in part by the Program of
   Shanghai Academic Research Leader under Grant 20XD1421700. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Prof. M. Shamim Hossain. (Corresponding author: Yaxin
   Peng.)
CR Alonso I, 2020, IEEE T ROBOT, V36, P1340, DOI 10.1109/TRO.2020.2974099
   Amit Y, 2004, IEEE T PATTERN ANAL, V26, P1606, DOI 10.1109/TPAMI.2004.111
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Chan L, 2019, IEEE I CONF COMP VIS, P10661, DOI 10.1109/ICCV.2019.01076
   Chandra S, 2017, IEEE I CONF COMP VIS, P5113, DOI 10.1109/ICCV.2017.546
   Changqian Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12413, DOI 10.1109/CVPR42600.2020.01243
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng B, 2021, ADV NEUR IN, V34
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ding HH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15828, DOI 10.1109/ICCV48922.2021.01555
   Ding HH, 2019, IEEE I CONF COMP VIS, P6818, DOI 10.1109/ICCV.2019.00692
   Ding HH, 2019, PROC CVPR IEEE, P8877, DOI 10.1109/CVPR.2019.00909
   Ding HH, 2020, IEEE T IMAGE PROCESS, V29, P3520, DOI 10.1109/TIP.2019.2962685
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Ding XF, 2021, IEEE INT CONF COMP V, P3002, DOI 10.1109/ICCVW54120.2021.00335
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fleuret F, 2001, INT J COMPUT VISION, V41, P85, DOI 10.1023/A:1011113216584
   Fu J, 2019, IEEE I CONF COMP VIS, P6747, DOI 10.1109/ICCV.2019.00685
   Fu J, 2021, IEEE T NEUR NET LEAR, V32, P2547, DOI 10.1109/TNNLS.2020.3006524
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Geng QC, 2021, IEEE T IMAGE PROCESS, V30, P2436, DOI 10.1109/TIP.2020.3046921
   Gu ZX, 2021, IEEE T MULTIMEDIA, V23, P3738, DOI 10.1109/TMM.2020.3035231
   GUIASU S, 1985, MATH INTELL, V7, P42, DOI 10.1007/BF03023004
   Hao SJ, 2020, NEUROCOMPUTING, V406, P302, DOI 10.1016/j.neucom.2019.11.118
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XJ, 2022, IEEE T IMAGE PROCESS, V31, P2850, DOI 10.1109/TIP.2022.3162101
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu P, 2021, IEEE ROBOT AUTOM LET, V6, P263, DOI 10.1109/LRA.2020.3039744
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Li X., 2020, P IEEE CVF C COMP VI, P8950, DOI 10.1109/CVPR42600.2020.00897
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Liu MY, 2021, PROC CVPR IEEE, P9721, DOI 10.1109/CVPR46437.2021.00960
   Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma J, 2021, MED PHYS, V48, P1197, DOI 10.1002/mp.14676
   Minghao Yin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P191, DOI 10.1007/978-3-030-58555-6_12
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pang YW, 2019, IEEE I CONF COMP VIS, P4229, DOI 10.1109/ICCV.2019.00433
   Ping Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8815, DOI 10.1109/CVPR42600.2020.00884
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K., 2014, CORR
   Song Q., 2021, arXiv
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Valada Abhinav, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4644, DOI 10.1109/ICRA.2017.7989540
   Vaswani A, 2017, ADV NEUR IN, V30
   Vemulapalli R, 2016, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2016.351
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wanli Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P52, DOI 10.1007/978-3-030-58520-4_4
   Xie EZ, 2021, ADV NEUR IN, V34
   Xu JC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P601
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yin CX, 2022, IEEE T MULTIMEDIA, V24, P4183, DOI 10.1109/TMM.2021.3114541
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Yuan YH, 2021, INT J COMPUT VISION, V129, P2375, DOI 10.1007/s11263-021-01465-9
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zhang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6933, DOI 10.1109/ICCV48922.2021.00687
   Zhang F, 2019, IEEE I CONF COMP VIS, P6797, DOI 10.1109/ICCV.2019.00690
   Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang PP, 2019, PATTERN RECOGN, V88, P702, DOI 10.1016/j.patcog.2018.12.021
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou YZ, 2019, PROC CVPR IEEE, P4041, DOI 10.1109/CVPR.2019.00417
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
   Zilong Zhong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13062, DOI 10.1109/CVPR42600.2020.01308
NR 81
TC 0
Z9 0
U1 7
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5720
EP 5732
DI 10.1109/TMM.2022.3198314
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500006
DA 2024-07-18
ER

PT J
AU Gao, LL
   Zhao, Q
   Zhu, JC
   Su, ST
   Cheng, LC
   Zhao, L
AF Gao, Lianli
   Zhao, Qike
   Zhu, Junchen
   Su, Sitong
   Cheng, Lechao
   Zhao, Lei
TI From External to Internal: Structuring Image for Text-to-Image
   Attributes Manipulation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Natural languages; Image representation; Image color
   analysis; Generators; Generative adversarial networks; Visualization;
   GANs; image translation; text-to-image attribute manipulation
AB Manipulating visual attributes of an image through a natural language description, known as text-to-image attributes manipulation (T2AM), is a challenging task. However, existing approaches tend to search the whole image to manipulate the target instance indicated by a description, thus they often fail to locate and manipulate the accurate text-relevant regions, and even disturb the text-irrelevant contents, e.g. texture and background. Meanwhile, the model efficiency needs to be improved. To tackle the above issues, we introduce a novel yet simple GAN-based approach, namely Structuring Image for Manipulating (SIMGAN), to narrow down the optimization areas from external to internal. It consists of two major components: 1) External Structuring (ExST), a pretrained segmentation network, for recognizing and separating the target instances and background from an image; and 2) Internal Structuring (InST) for seeking out and editing the text-relevant attributes of the target instances based on the given description and masked hierarchical image representations from ExST. Specifically, the InST structures target instances from outline to detail by firstly drawing the sketch and colors underpainting of instances with an Outline-Oriented Structuring (OuST), and then enhancing the text-relevant attributes and elaborating on details with a Detail-Oriented Structuring (DeST). Extensive experiments on benchmark datasets demonstrate that our framework significantly outperforms state-of-the-art both quantitatively and qualitatively. Compared with the state-of-the-art method ManiGAN, our approach reduces the training time by 88%, while the inferring time is three times faster. In addition, our approach is easily extended to solve the instance-level image-to-image translation problem, and the results exhibit the versatility and effectiveness of our approach. This code is released in https://github.com/qikizh/SIMGAN.
C1 [Gao, Lianli; Zhao, Qike; Zhu, Junchen; Su, Sitong] Ctr Futher Media, Sch & Technol China, Chengdu 611731, Peoples R China.
   [Cheng, Lechao] Zhejiang Lab, Hangzhou 311121, Peoples R China.
   [Zhao, Lei] Sichuan Artificial Intelligence Res Inst, Yibin 644000, Peoples R China.
C3 Zhejiang Laboratory
RP Zhao, L (corresponding author), Sichuan Artificial Intelligence Res Inst, Yibin 644000, Peoples R China.
EM lianli.gao@uestc.edu.cn; kebound@foxmail.com; junchen.zhu@hotmail.com;
   sitongsu9796@gmail.com; chenglc@zhejianglab.com;
   zhaolei@std.uestc.edu.cn
OI Zhu, Junchen/0000-0002-3872-6689; Cheng, Lechao/0000-0002-7546-9052
FU Open Research Projects of Zhejiang Laboratory [2019KD0AD01/011]; Fok
   Ying-Tong Education Foundation [171106]; National Key Research and
   Development Program of China [2018AAA0102205]
FX This work was supported in part by the Open Research Projects of
   Zhejiang Laboratory under Grant 2019KD0AD01/011, in part by Fok
   Ying-Tong Education Foundation under Grant 171106, and in part by the
   National Key Research and Development Program of China under Grant
   2018AAA0102205.
CR Abdal R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447648
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Deng YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2719, DOI 10.1145/3394171.3414015
   Dhamo H, 2020, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR42600.2020.00526
   Donahue J, 2019, Advances in Neural Information Processing Systems
   Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608
   Dumoulin V.., 2017, P INT C LEARN REPR I
   El-Nouby A, 2019, IEEE I CONF COMP VIS, P10303, DOI 10.1109/ICCV.2019.01040
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Feng FX, 2022, IEEE T MULTIMEDIA, V24, P2112, DOI 10.1109/TMM.2021.3075997
   Gao LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3734, DOI 10.1145/3394171.3414027
   Gao LL, 2019, AAAI CONF ARTIF INTE, P8312
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gomez R, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3164, DOI 10.1145/3394171.3413785
   Haruyama T, 2021, IEEE IMAGE PROC, P2433, DOI 10.1109/ICIP42928.2021.9506601
   Hensel M, 2017, ADV NEUR IN, V30
   Hu ZY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3320, DOI 10.1145/3394171.3413853
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiadong Liang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P491, DOI 10.1007/978-3-030-58548-8_29
   Karayil Federico, 2021, P GERM C PATT REC, P361
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Li B., 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P7880
   Li B., 2020, Advances in Neural Information Processing Systems, V33, P22020, DOI [10.48550/arxiv.2010.12136, DOI 10.48550/ARXIV.2010.12136]
   Li W., 2019, PROC IEEECVF C COMPU, p12 166
   Lin Q, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1094, DOI 10.1145/3394171.3413982
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1357, DOI 10.1145/3394171.3413505
   Liu ZH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P322, DOI 10.1145/3394171.3413777
   Nam S, 2018, ADV NEUR IN, V31
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Pavllo D., 2020, P EUR C COMP VIS, P482
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Salimans T, 2016, ADV NEUR IN, V29
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su S., 2021, PROC IJCAI, P1004
   Su ST, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1784, DOI 10.1145/3474085.3475326
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan HC, 2022, IEEE T MULTIMEDIA, V24, P832, DOI 10.1109/TMM.2021.3060291
   Tao M, 2022, PROC CVPR IEEE, P16494, DOI 10.1109/CVPR52688.2022.01602
   Vaswani A, 2017, ADV NEUR IN, V30
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1, DOI 10.1145/3394171.3413891
   Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Yuheng Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8036, DOI 10.1109/CVPR42600.2020.00806
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang J, 2022, IEEE T CIRC SYST VID, V32, P5916, DOI 10.1109/TCSVT.2022.3164190
   Zhang LS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1302, DOI 10.1145/3394171.3414017
   Zhang YF, 2020, IEEE T MULTIMEDIA, V22, P1407, DOI 10.1109/TMM.2019.2943750
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 62
TC 3
Z9 3
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7248
EP 7261
DI 10.1109/TMM.2022.3219677
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000039
DA 2024-07-18
ER

PT J
AU Jiao, JY
   Tang, YM
   Lin, KY
   Gao, YP
   Ma, AJ
   Wang, YW
   Zheng, WS
AF Jiao, Jiayu
   Tang, Yu-Ming
   Lin, Kun-Yu
   Gao, Yipeng
   Ma, Andy J.
   Wang, Yaowei
   Zheng, Wei-Shi
TI DilateFormer: Multi-Scale Dilated Transformer for Visual Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Vision transformer
ID ATTENTION NETWORK; IMAGE
AB As a de facto solution, the vanilla Vision Transformers (ViTs) are encouraged to model long-range dependencies between arbitrary image patches while the global attended receptive field leads to quadratic computational cost. Another branch of Vision Transformers exploits local attention inspired by CNNs, which only models the interactions between patches in small neighborhoods. Although such a solution reduces the computational cost, it naturally suffers from small attended receptive fields, which may limit the performance. In this work, we explore effective Vision Transformers to pursue a preferable trade-off between the computational complexity and size of the attended receptive field. By analyzing the patch interaction of global attention in ViTs, we observe two key properties in the shallow layers, namely locality and sparsity, indicating the redundancy of global dependency modeling in shallow layers of ViTs. Accordingly, we propose Multi-Scale Dilated Attention (MSDA) to model local and sparse patch interaction within the sliding window. With a pyramid architecture, we construct a Multi-Scale Dilated Transformer (DilateFormer) by stacking MSDA blocks at low-level stages and global multi-head self-attention blocks at high-level stages. Our experiment results show that our DilateFormer achieves state-of-the-art performance on various vision tasks. On ImageNet-1 K classification task, DilateFormer achieves comparable performance with 70% fewer FLOPs compared with existing state-of-the-art models. Our DilateFormer-Base achieves 85.6% top-1 accuracy on ImageNet-1 K classification task, 53.5% box mAP/46.1% mask mAP on COCO object detection/instance segmentation task and 51.1% MS mIoU on ADE20 K semantic segmentation task. The code is available at https://isee-ai.cn/(similar to)jiaojiayu/DilteFormer.html.
C1 [Jiao, Jiayu; Tang, Yu-Ming; Lin, Kun-Yu; Gao, Yipeng; Ma, Andy J.; Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.
   [Zheng, Wei-Shi] Sun Yat Sen Univ, Key Lab Machine Intelligence & Adv Comp, Minist Educ, Guangzhou 510275, Peoples R China.
   [Wang, Yaowei] Pengcheng Lab, Shenzhen 518066, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University
RP Zheng, WS (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.
EM jiaojy6@mail2.sysu.edu.cn; tangym9@mail2.sysu.edu.cn;
   linky5@mail2.sysu.edu.cn; gaoyp23@mail2.sysu.edu.cn;
   majh8@mail.sysu.edu.cn; wangyw@pcl.ac.cn; wszheng@ieee.org
RI Lin, Kun-Yu/JDA-0444-2023; Tang, Yu-Ming/JEF-8770-2023
OI Lin, Kun-Yu/0000-0002-0013-3730; Jiao, Jiayu/0000-0003-0507-2620; Tang,
   Yu-Ming/0000-0001-5472-0079; Ma, Jinhua/0000-0002-0165-8416
FU National Natural Science Foundation of China
FX No Statement Available
CR Beltagy I, 2020, Arxiv, DOI arXiv:2004.05150
   Brown T., 2020, Advances in Neural Information Processing Systems, V33, P1877, DOI [DOI 10.48550/ARXIV.2005.14165, DOI 10.5555/3495724.3495883]
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P655, DOI 10.1109/TMM.2021.3057493
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen Y, 2022, PATTERN RECOGN LETT, V157, P90, DOI 10.1016/j.patrec.2022.03.020
   Chen YP, 2022, PROC CVPR IEEE, P5260, DOI 10.1109/CVPR52688.2022.00520
   Chu XX, 2021, ADV NEUR IN
   Chu XX, 2021, Arxiv, DOI [arXiv:2102.10882, DOI 10.48550/ARXIV.2102.10882]
   Cohen TS, 2016, PR MACH LEARN RES, V48
   Contributors M., 2020, MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark
   Dai XY, 2021, PROC CVPR IEEE, P7369, DOI 10.1109/CVPR46437.2021.00729
   Dai Z, 2021, ADV NEUR IN, V34
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong XY, 2022, PROC CVPR IEEE, P12114, DOI 10.1109/CVPR52688.2022.01181
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   El-Nouby A, 2021, ADV NEUR IN
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Gao P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3601, DOI 10.1109/ICCV48922.2021.00360
   Gildenblat J., 2021, PyTorch Library for CAM Methods
   Gong X, 2022, IEEE T MULTIMEDIA, V24, P217, DOI 10.1109/TMM.2021.3050082
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Guo RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7137, DOI 10.1109/ICCV48922.2021.00707
   Han K., 2021, Adv. Neural Inf. Process. Syst., V34, P15908, DOI DOI 10.48550/ARXIV.2103.00112
   Han Q., 2022, PROC INT C LEARN REP, P1
   Hassani A, 2022, Arxiv, DOI arXiv:2209.15001
   Hassani A, 2022, Arxiv, DOI [arXiv:2204.07143, DOI 10.48550/ARXIV.2204.07143]
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hoffer Elad, 2020, P IEEECVF C COMPUTER, P8129
   Huang ZL, 2021, Arxiv, DOI arXiv:2106.03650
   Jiang Z., 2021, P NIPS, P18590
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee Y, 2022, PROC CVPR IEEE, P7277, DOI 10.1109/CVPR52688.2022.00714
   Li G., 2021, PROC IEEE INT C MULT, P1
   Li JS, 2022, Arxiv, DOI arXiv:2207.05501
   Li JX, 2021, IEEE T MULTIMEDIA, V23, P1397, DOI 10.1109/TMM.2020.2997192
   Li KC, 2022, Arxiv, DOI arXiv:2201.09450
   Li YH, 2023, IEEE T PATTERN ANAL, V45, P1489, DOI 10.1109/TPAMI.2022.3164083
   Lin L, 2022, IEEE T MULTIMEDIA, V24, P1922, DOI 10.1109/TMM.2021.3074008
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HB, 2022, IEEE T MULTIMEDIA, V24, P2902, DOI 10.1109/TMM.2021.3090274
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2022, IEEE T MULTIMEDIA, V24, P2890, DOI 10.1109/TMM.2021.3090206
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Ma X, 2021, IEEE T MULTIMEDIA, V23, P3048, DOI 10.1109/TMM.2021.3068576
   Ma YJ, 2022, IEEE T MULTIMEDIA, V24, P261, DOI 10.1109/TMM.2021.3050059
   Peng YX, 2023, IEEE T MULTIMEDIA, V25, P2393, DOI 10.1109/TMM.2022.3146775
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Radford Alec, 2018, IMPROVING LANGUAGE U, DOI DOI 10.18653/V1/N18-1202
   Radosavovic Ilija, 2020, P IEEE CVF C COMP VI, P10428
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren SC, 2022, PROC CVPR IEEE, P10843, DOI 10.1109/CVPR52688.2022.01058
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K., 2014, CORR
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PR MACH LEARN RES, V97
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P32, DOI 10.1109/ICCV48922.2021.00010
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Tu ZZ, 2022, LECT NOTES COMPUT SC, V13684, P459, DOI 10.1007/978-3-031-20053-3_27
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang WX, 2021, Arxiv, DOI [arXiv:2108.00154, DOI 10.48550/ARXIV.2108.00154]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10013, DOI 10.1109/ICCV48922.2021.00988
   Xia RJ, 2022, IEEE T MULTIMEDIA, V24, P2648, DOI 10.1109/TMM.2021.3086758
   Xia X, 2022, Arxiv, DOI arXiv:2205.09579
   Xiao T., 2021, NEURIPS, V34, P30392
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xie P, 2022, IEEE T MULTIMEDIA, V24, P3908, DOI 10.1109/TMM.2021.3109665
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xie ZD, 2022, PROC CVPR IEEE, P9643, DOI 10.1109/CVPR52688.2022.00943
   Xu YF, 2021, 35 C NEURAL INFORM P
   Yan ZY, 2022, IEEE T MULTIMEDIA, V24, P2633, DOI 10.1109/TMM.2021.3086709
   Yang J., 2021, arXiv, DOI DOI 10.48550/ARXIV.2107.00641
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Yu F., 2015, ARXIV
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Yu Y., 2021, P ADV NEUR INF PROC, P12992
   Yuan K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P559, DOI 10.1109/ICCV48922.2021.00062
   Yuan L, 2023, IEEE T PATTERN ANAL, V45, P6575, DOI 10.1109/TPAMI.2022.3206108
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Yue XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P377, DOI 10.1109/ICCV48922.2021.00044
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhai XH, 2022, PROC CVPR IEEE, P12094, DOI 10.1109/CVPR52688.2022.01179
   Zhang CX, 2021, IEEE T MULTIMEDIA, V24, P3340, DOI 10.1109/TMM.2021.3096083
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang PC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2978, DOI 10.1109/ICCV48922.2021.00299
   Zhang Z., 2021, AAAI, V36, P3417, DOI DOI 10.1609/AAAI.V3613.20252
   Zhang ZZ, 2022, AAAI CONF ARTIF INTE, P3417
   Zhao BX, 2021, IEEE T MULTIMEDIA, V23, P1722, DOI 10.1109/TMM.2020.3002614
   Zheng Y., 2022, Comput. Intell. Neurosci.
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou DQ, 2021, Arxiv, DOI [arXiv:2103.11886, 10.48550/arXiv.2103.11886, DOI 10.48550/ARXIV.2103.11886]
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
   Zhu FR, 2021, IEEE INT CONF COMP V, P2667, DOI 10.1109/ICCVW54120.2021.00301
   Zuo YF, 2021, IEEE T MULTIMEDIA, V24, P3506, DOI 10.1109/TMM.2021.3100766
NR 114
TC 20
Z9 20
U1 36
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8906
EP 8919
DI 10.1109/TMM.2023.3243616
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000049
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jing, MM
   Meng, LC
   Li, JJ
   Zhu, L
   Shen, HT
AF Jing, Mengmeng
   Meng, Lichao
   Li, Jingjing
   Zhu, Lei
   Shen, Heng Tao
TI Adversarial Mixup Ratio Confusion for Unsupervised Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain adaptation; transfer learning
ID KERNEL
AB Multimedia applications often involve knowledge transfer across domains, e.g., from images to texts, where Unsupervised Domain Adaptation (UDA) can be used to reduce the domain shifts. Most of the UDA methods are based on adversarial learning. However, previous adversarial domain adaptation methods may suffer from three issues. First, although the features learned by previous methods could fool the domain classifier to make false classification predictions, they may not be domain-invariant. Second, the limited number of training samples make the latent space of features not smooth and continuous enough. Third, the target domain features may lack discriminability. In this paper, we propose a novel adversarial domain adaptation method named Adversarial Mixup Ratio Confusion (AMRC) to alleviate all the above issues. Specifically, we propose a new adversarial training pattern that uses mixup to generate multiple features with different mixup ratios, which represent different intermediate states between the source and target domain. Then, on one hand, we train an estimator to estimate the mixup ratio as accurately as possible. On the other hand, we train a generator to make the estimator be uncertain about the mixup ratio. In this way, our method could learn a continuous and domain-invariant latent space. Furthermore, we apply the intra-domain and cross-domain mixup regularizations to ensure the smoothness and continuity of the latent space, while making the classifier behave more linearly on in-between samples. At last, we exploit the sharpened pseudo-labels of the target samples for self-supervised learning to enhance the discriminability of the target features.The experimental results on 3 benchmarks verify the effectiveness of our method.
C1 [Jing, Mengmeng; Meng, Lichao; Li, Jingjing; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, 611731, Peoples R China.
   [Li, Jingjing] Inst Elect & Informat Engn UESTC, Guangdong 523808, Peoples R China.
   [Zhu, Lei] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
C3 University of Electronic Science & Technology of China; Shandong Normal
   University
RP Li, JJ (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, 611731, Peoples R China.; Li, JJ (corresponding author), Inst Elect & Informat Engn UESTC, Guangdong 523808, Peoples R China.
EM jingmeng1992@gmail.com; menglc1107@gmail.com; lijin117@yeah.net;
   leizhu0608@gmail.com; shenhengtao@hotmail.com
RI Li, Jingjing/T-6522-2019; Zhu, Lei/GQQ-1130-2022; Shen, Heng
   Tao/ABD-5331-2021; Jing, Mengmeng/JEO-6295-2023
OI Zhu, Lei/0000-0002-5348-7532; Jing, Mengmeng/0000-0002-0693-2197; Zhu,
   Lei/0000-0002-2993-7142; Meng, Lichao/0000-0002-9354-5258
FU National Natural Science Foundation of China [62176042, 62073059];
   Sichuan Science and Technology Program [2020YFG0080, 2020YFG0481];
   Guangdong Basic and Applied Basic Research Foundation [2021B1515140013]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62176042 and 62073059, in part by
   Sichuan Science and Technology Program under Grants 2020YFG0080 and
   2020YFG0481, and in part by Guangdong Basic and Applied Basic Research
   Foundation under Grant 2021B1515140013.
CR [Anonymous], 2020, IJCNN
   Arora S, 2017, PR MACH LEARN RES, V70
   Batanina E, 2019, INT CONF IMAG PROC
   Ben-David S., 2007, ADV NEURAL INFORM PR, V19
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Berthelot D, 2019, ADV NEUR IN, V32
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Chapelle O, 2005, P INT WORKSH ART INT, P57
   Chen MH, 2020, AAAI CONF ARTIF INTE, V34, P3521
   Chen XY, 2019, PR MACH LEARN RES, V97
   Crammer K, 2008, J MACH LEARN RES, V9, P1757
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   Damodaran BB, 2018, LECT NOTES COMPUT SC, V11208, P467, DOI 10.1007/978-3-030-01225-0_28
   Deng ZJ, 2019, IEEE I CONF COMP VIS, P9943, DOI 10.1109/ICCV.2019.01004
   Farnia F., 2020, P MACHINE LEARNING R, P3029
   French G., 2018, PROC INT C LEARN REP
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ghifary M, 2017, IEEE T PATTERN ANAL, V39, P1414, DOI 10.1109/TPAMI.2016.2599532
   Glorot X., 2011, P 28 INT C INT C MAC, P513
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Grandvalet Y., 2006, Semi-Supervised Learning, P151, DOI [DOI 10.7551/MITPRESS/9780262033589.003.0009, 10.7551/mitpress/9780262033589.001.0001]
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   Guo HY, 2019, AAAI CONF ARTIF INTE, P3714
   Guo HY, 2020, AAAI CONF ARTIF INTE, V34, P4044
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Jing MM, 2021, IEEE T CYBERNETICS, V51, P3390, DOI 10.1109/TCYB.2020.2974106
   Jing MM, 2021, AAAI CONF ARTIF INTE, V35, P8013
   Jing MM, 2020, NEURAL NETWORKS, V130, P39, DOI 10.1016/j.neunet.2020.06.016
   Kalantidis Yannis, 2020, P INT C NEUR INF PRO, P21798
   Kim J., 2020, ADV NEUR IN, V33, P14567
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Li JJ, 2022, IEEE T KNOWL DATA EN, V34, P5770, DOI 10.1109/TKDE.2021.3060473
   Li JJ, 2022, IEEE T PATTERN ANAL, V44, P8196, DOI 10.1109/TPAMI.2021.3109287
   Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050
   Li JJ, 2019, IEEE T IMAGE PROCESS, V28, P6103, DOI 10.1109/TIP.2019.2924174
   Li JJ, 2019, IEEE T NEUR NET LEAR, V30, P1381, DOI 10.1109/TNNLS.2018.2868854
   Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624
   Liang J, 2019, PROC CVPR IEEE, P2970, DOI 10.1109/CVPR.2019.00309
   Liu H, 2019, 36 INT C MACHINE LEA, V97
   Long M., 2017, PMLR, P2208
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Lu YW, 2022, IEEE T MULTIMEDIA, V24, P1871, DOI 10.1109/TMM.2021.3073258
   Luo YW, 2020, AAAI CONF ARTIF INTE, V34, P5029
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Mancini M., 2020, P EUR C COMP VIS, P466, DOI DOI 10.1007/978-3-030-58592
   Mao XD, 2019, Arxiv, DOI arXiv:1905.04215
   Mengxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13933, DOI 10.1109/CVPR42600.2020.01395
   Mozafari AS, 2016, PATTERN RECOGN, V56, P142, DOI 10.1016/j.patcog.2016.03.009
   Na J, 2021, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR46437.2021.00115
   Pan S. J., 2010, Cross-domain sentiment classi fication via spectral feature alignment, P751, DOI DOI 10.1145/1772690.1772767
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   Peng XC, 2017, Arxiv, DOI arXiv:1710.06924
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Pinheiro PO, 2018, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2018.00835
   Rizve Mamshad Nayeem, 2020, INT C LEARN REPR
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Shu Y, 2021, PROC CVPR IEEE, P9619, DOI 10.1109/CVPR46437.2021.00950
   Song SY, 2022, IEEE T MULTIMEDIA, V24, P128, DOI 10.1109/TMM.2020.3046868
   Song XL, 2021, IEEE T MULTIMEDIA, V24, P3229, DOI 10.1109/TMM.2021.3096014
   Sugiyama M, 2007, J MACH LEARN RES, V8, P985
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Verma V, 2019, PR MACH LEARN RES, V97
   Wang XM, 2019, AAAI CONF ARTIF INTE, P5345
   Xiao N, 2021, PROC CVPR IEEE, P15237, DOI 10.1109/CVPR46437.2021.01499
   Xu MH, 2020, AAAI CONF ARTIF INTE, V34, P6502
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417
   Yabin Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P781, DOI 10.1007/978-3-030-58548-8_45
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Ying Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P464, DOI 10.1007/978-3-030-58589-1_28
   Yuan Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P540, DOI 10.1007/978-3-030-58526-6_32
   Zhang BW, 2021, 35 C NEURAL INFORM P, V34
   Zhang C, 2022, IEEE T MULTIMEDIA, V24, P2246, DOI 10.1109/TMM.2021.3078141
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhang YB, 2019, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR.2019.00517
   Zhang ZC, 2018, LECT NOTES COMPUT SC, V11257, P356, DOI 10.1007/978-3-030-03335-4_31
   Zhao H., 2018, P ADV NEUR INF PROC, V31, P8568
NR 96
TC 15
Z9 15
U1 8
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2559
EP 2572
DI 10.1109/TMM.2022.3148592
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600008
DA 2024-07-18
ER

PT J
AU Li, SY
   Zhang, BB
   Fei, LK
   Zhao, SP
   Zhou, YC
AF Li, Shuyi
   Zhang, Bob
   Fei, Lunke
   Zhao, Shuping
   Zhou, Yicong
TI Learning Sparse and Discriminative Multimodal Feature Codes for Finger
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Biometrics (access control); Binary codes;
   Representation learning; Histograms; Face recognition; Codes; Finger
   recognition; sparse and discriminative feature; binary codes;
   inter-modality; intra-modality
ID FUSION; VEIN; REPRESENTATION; HALLUCINATION; ENSEMBLE; NETWORK; SCALE
AB Compared with uni-modal biometrics systems, multimodal biometrics systems using multiple sources of information for establishing an individual's identity have received considerable attention recently. However, most traditional multimodal biometrics techniques generally extract features from each modality independently, ignoring the implicit associations between different modalities. In addition, most existing work uses hand-crafted descriptors that are difficult to capture the latent semantic structure. This paper proposes to learn the sparse and discriminative multimodal feature codes (SDMFCs) for multimodal finger recognition, which simultaneously takes into account the specific and common information among inter-modality and intra-modality. Specifically, given the multimodal finger images, we first establish the local difference matrix to capture informative texture features in local patches. Then, we aim to jointly learn discriminative and compact binary codes by constraining the observations from multiple modalities. Finally, we develop a novel SDMFC-based multimodal finger recognition framework, which integrates the local histograms of each division block in the learned binary codes together for classification. Experimental results on three commonly used finger databases demonstrate the effectiveness and robustness of the proposed framework in multimodal biometrics tasks.
C1 [Li, Shuyi; Zhang, Bob] Univ Macau, Dept Comp & Informat Sci, PAMI Res Grp, Macau 999078, Peoples R China.
   [Fei, Lunke; Zhao, Shuping] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 University of Macau; Guangdong University of Technology; University of
   Macau
RP Zhang, BB (corresponding author), Univ Macau, Dept Comp & Informat Sci, PAMI Res Grp, Macau 999078, Peoples R China.
EM yb97443@um.edu.mo; bobzhang@um.edu.mo; flksxm@126.com;
   yb77458@um.edu.mo; yicongzhou@um.edu.mo
RI Zhang, Bob/HIR-3656-2022; Zhou, Yicong/A-8017-2009
OI Zhang, Bob/0000-0001-6512-0474; Zhou, Yicong/0000-0002-4487-6384; Li,
   Shuyi/0000-0001-6264-9006; Fei, Lunke/0000-0001-6072-7875; Zhang,
   Bob/0000-0003-2497-9519
FU University of Macau [MYRG2018-00053-FST]; National Natural Science
   Foundation of China [61602540, 62176066]
FX This work was supported in part by the University of Macau under Grant
   MYRG2018-00053-FST, in part by the National Natural Science Foundation
   of China under Grant 61602540, and in part by the National Natural
   Science Foundation of China under Grant 62176066.
CR Abdi A, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107634
   Asaari MSM, 2014, EXPERT SYST APPL, V41, P3367, DOI 10.1016/j.eswa.2013.11.033
   Bahrampour S, 2016, IEEE T IMAGE PROCESS, V25, P24, DOI 10.1109/TIP.2015.2496275
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Elmadany NE, 2019, IEEE T MULTIMEDIA, V21, P1317, DOI 10.1109/TMM.2018.2875510
   Fei LK, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107894
   Fei LK, 2021, IEEE T MULTIMEDIA, V23, P2930, DOI 10.1109/TMM.2020.3019701
   Fei LK, 2020, IEEE T INSTRUM MEAS, V69, P9743, DOI 10.1109/TIM.2020.3002463
   Fei LK, 2019, IEEE T IMAGE PROCESS, V28, P3808, DOI 10.1109/TIP.2019.2903307
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu BZ, 2021, IEEE T IMAGE PROCESS, V30, P472, DOI 10.1109/TIP.2020.3036770
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Jian MW, 2019, INFORM SCIENCES, V488, P181, DOI 10.1016/j.ins.2019.03.026
   Jian M, 2015, IEEE T CIRC SYST VID, V25, P1761, DOI 10.1109/TCSVT.2015.2400772
   Jian MW, 2014, SIGNAL PROCESS, V100, P9, DOI 10.1016/j.sigpro.2014.01.004
   Jian MW, 2013, PATTERN RECOGN, V46, P3091, DOI 10.1016/j.patcog.2013.03.020
   Kumar A, 2012, IEEE T IMAGE PROCESS, V21, P2228, DOI 10.1109/TIP.2011.2171697
   Li H., 2019, SENSORS-BASEL, V19
   Li SY, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2140, DOI 10.1109/ICASSP39728.2021.9413688
   Li SY, 2021, IEEE T INF FOREN SEC, V16, P3186, DOI 10.1109/TIFS.2021.3074315
   Li SY, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107704
   Li SY, 2021, INFORM SCIENCES, V547, P1170, DOI 10.1016/j.ins.2020.09.045
   Li SY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19092213
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Lu JW, 2018, IEEE T PATTERN ANAL, V40, P1979, DOI 10.1109/TPAMI.2017.2737538
   Lu JW, 2015, IEEE T PATTERN ANAL, V37, P2041, DOI 10.1109/TPAMI.2015.2408359
   Luo YT, 2016, PATTERN RECOGN, V50, P26, DOI 10.1016/j.patcog.2015.08.025
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Rida I, 2018, IEEE ACCESS, V6, P3241, DOI 10.1109/ACCESS.2017.2787666
   Shekhar S, 2014, IEEE T PATTERN ANAL, V36, P113, DOI 10.1109/TPAMI.2013.109
   Shen JL, 2021, INFORM SCIENCES, V569, P469, DOI 10.1016/j.ins.2020.11.026
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang D, 2019, IEEE T PATTERN ANAL, V41, P2466, DOI 10.1109/TPAMI.2018.2861000
   Wen J, 2019, IEEE T CIRC SYST VID, V29, P390, DOI 10.1109/TCSVT.2018.2799214
   Wu JD, 2011, EXPERT SYST APPL, V38, P5423, DOI 10.1016/j.eswa.2010.10.013
   Xie L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3133
   Yang CL, 2013, PATTERN RECOGN, V46, P948, DOI 10.1016/j.patcog.2012.07.011
   Yang JF, 2017, PATTERN RECOGN, V66, P34, DOI 10.1016/j.patcog.2017.01.008
   Yang JF, 2012, PATTERN RECOGN LETT, V33, P623, DOI 10.1016/j.patrec.2011.11.002
   Yang JF, 2010, LECT NOTES COMPUT SC, V5994, P374
   Yang WC, 2018, PATTERN RECOGN, V78, P242, DOI 10.1016/j.patcog.2018.01.026
   Yang WM, 2019, IEEE T INF FOREN SEC, V14, P2512, DOI 10.1109/TIFS.2019.2902819
   Yang WM, 2019, IEEE T INF FOREN SEC, V14, P90, DOI 10.1109/TIFS.2018.2844803
   Yang WM, 2014, INFORM SCIENCES, V268, P20, DOI 10.1016/j.ins.2013.10.010
   Yin YL, 2011, LECT NOTES COMPUT SC, V7098, P260, DOI 10.1007/978-3-642-25449-9_33
   Zeng SN, 2022, IEEE T CYBERNETICS, V52, P4935, DOI 10.1109/TCYB.2020.3025757
   Zhang HG, 2019, IEEE ACCESS, V7, P28607, DOI 10.1109/ACCESS.2019.2902133
   Zhang L, 2010, PATTERN RECOGN, V43, P2560, DOI 10.1016/j.patcog.2010.01.020
   Zhang XD, 2021, IEEE T MULTIMEDIA, V23, P611, DOI 10.1109/TMM.2020.2985526
NR 50
TC 9
Z9 9
U1 9
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 805
EP 815
DI 10.1109/TMM.2021.3132166
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900010
DA 2024-07-18
ER

PT J
AU Luo, F
   Chen, SX
   Chen, JJ
   Wu, ZX
   Jiang, YG
AF Luo, Fan
   Chen, Shaoxiang
   Chen, Jingjing
   Wu, Zuxuan
   Jiang, Yu-Gang
TI Self-Supervised Learning for Semi-Supervised Temporal Language Grounding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporal language grounding; Semi-supervised learning; Contrastive
   learning
ID LOCALIZATION
AB Given a text description, Temporal Language Grounding (TLG) aims to localize temporal boundaries of the segments that contain the specified semantics in an untrimmed video. TLG is inherently a challenging task, as it requires comprehensive understanding of both sentence semantics and video contents. Previous works either tackle this task in a fully-supervised setting that requires a large amount of temporal annotations or in a weakly-supervised setting that usually cannot achieve satisfactory performance. Since manual annotations are expensive, to cope with limited annotations, we tackle TLG in a semi-supervised way by incorporating self-supervised learning, and propose Self-Supervised Semi-Supervised Temporal Language Grounding (S(4)TLG). S(4)TLG consists of two parts: (1) A pseudo label generation module that adaptively produces instant pseudo labels for unlabeled samples based on predictions from a teacher model; (2) A self-supervised feature learning module with intermodal and intra-modal contrastive losses to learn video feature representations under the constraints of video content consistency and video-text alignment. We conduct extensive experiments on the ActivityNet-CD-OOD and Charades-CD-OOD datasets. The results demonstrate that our proposed S(4)TLG can achieve competitive performance compared to fully-supervised state-of-the-art methods while only requiring a small portion of temporal annotations.
C1 [Luo, Fan; Chen, Shaoxiang; Chen, Jingjing; Wu, Zuxuan; Jiang, Yu-Gang] Fudan Univ, Shanghai Collaborat Innovat Ctr Intelligent Visua, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 201203, Peoples R China.
C3 Fudan University
RP Chen, JJ (corresponding author), Fudan Univ, Shanghai Collaborat Innovat Ctr Intelligent Visua, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 201203, Peoples R China.
EM fluo20@fudan.edu.cn; sxchen13@fudan.edu.cn; chenjingjing@fudan.edu.cn;
   zxwu@fudan.edu.cn; ygj@fudan.edu.cn
RI WANG, YILUN/KFB-0627-2024; chen, huan/KEC-2019-2024
FU Shuguang Program; Shanghai Education Development Foundation;
   ShanghaiMunicipal Education Commission [18SG01]; Shanghai Science and
   Technology Program [21JC1400600]
FX The work of Yu-Gang Jiang was supported in part by Shuguang Program and
   in part by Shanghai Education Development Foundation and
   ShanghaiMunicipal Education Commission under Grant 18SG01. This work was
   supported by Shanghai Science and Technology Program under Grant
   21JC1400600.
CR Arazo E, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207304
   Behrmann N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9224, DOI 10.1109/ICCV48922.2021.00911
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Chen Ting, 2020, ADV NEURAL INFORM PR, V33, P22243
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen X, 2021, AUTOPHAGY, V17, P2054, DOI 10.1080/15548627.2020.1810918
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan X, 2018, ADV NEUR IN, V31
   Gao JL, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P3978
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao MF, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1481
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Gong G., 2020, PROC IEEE C COMPUT, P9819
   Han TD, 2019, IEEE INT CONF COMP V, P1483, DOI 10.1109/ICCVW.2019.00186
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Pham H, 2021, PROC CVPR IEEE, P11552, DOI 10.1109/CVPR46437.2021.01139
   Jeong J, 2019, ADV NEUR IN, V32
   Ji JW, 2019, IEEE I CONF COMP VIS, P7072, DOI 10.1109/ICCV.2019.00717
   Jiang B, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P217, DOI 10.1145/3323873.3325019
   Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082
   Kingma D. P., 2015, PRO INT CONF REP
   Laine S., 2017, PROC INT C LEA REP
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Li R., 2021, P IEEE CVF INT C COM, P2105
   Liu DZ, 2021, PROC CVPR IEEE, P11230, DOI 10.1109/CVPR46437.2021.01108
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu W., 2019, Neural Inf. Process. Syst., P536
   Minuk Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P156, DOI 10.1007/978-3-030-58604-1_10
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Otani M., 2020, PROC BRIT MACH VIS
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Patrick M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10540, DOI 10.1109/ICCV48922.2021.01039
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI [10.1109/WACV45572.2020.9093328, 10.1109/wacv45572.2020.9093328]
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Sohn K., 2020, ADV NEUR IN, V33
   Sohn K, 2020, Arxiv, DOI [arXiv:2005.04757, DOI 10.48550/ARXIV.2005.04757]
   Song Y., 2020, COR
   Tan R., 2019, COR
   Tang HY, 2022, IEEE T MULTIMEDIA, V24, P1338, DOI 10.1109/TMM.2021.3063631
   Tang YH, 2021, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR46437.2021.00315
   Tapaswi M, 2016, PROC CVPR IEEE, P4631, DOI 10.1109/CVPR.2016.501
   Tarvainen A, 2017, ADV NEUR IN, V30
   Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168
   Wang JP, 2021, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR46437.2021.01163
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Wang X, 2021, PROC CVPR IEEE, P1905, DOI 10.1109/CVPR46437.2021.00194
   Wang YC, 2021, IEEE T MULTIMEDIA, V24, P3276, DOI 10.1109/TMM.2021.3096087
   Watkinson J., 2012, THE MPEG HDB
   Wu J, 2020, AAAI CONF ARTIF INTE, V34, P12386
   Yang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1, DOI 10.1145/3404835.3462823
   Yuan Yitian, 2021, HUMA'21: Proceedings of the 2nd International Workshop on Human-centric Multimedia Analysis, P13, DOI 10.1145/3475723.3484247
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, CVPR
   Zhang H., 2021, Towards debiasing temporal sentence grounding in video
   Zhang H, 2022, IEEE T PATTERN ANAL, V44, P4252, DOI 10.1109/TPAMI.2021.3060449
   Zhang SY, 2022, IEEE T PATTERN ANAL, V44, P9073, DOI 10.1109/TPAMI.2021.3120745
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang Z., 2020, Advances in NIPS, P18123
   Zhang Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4098, DOI 10.1145/3394171.3413967
   Zhao Y, 2021, PROC CVPR IEEE, P4195, DOI 10.1109/CVPR46437.2021.00418
   Zhou H, 2021, PROC CVPR IEEE, P8441, DOI 10.1109/CVPR46437.2021.00834
   Zhou Q, 2021, PROC CVPR IEEE, P4079, DOI 10.1109/CVPR46437.2021.00407
NR 72
TC 3
Z9 3
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7747
EP 7757
DI 10.1109/TMM.2022.3228167
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sharma, R
   Somandepalli, K
   Narayanan, S
AF Sharma, Rahul
   Somandepalli, Krishna
   Narayanan, Shrikanth
TI Cross Modal Video Representations for Weakly Supervised Active Speaker
   Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal learning; weakly supervised learning; multiple instance
   learning; active speaker localization.
ID DATASET
AB An objective understanding of media depictions, such as inclusive portrayals of how much someone is heard and seen on screen such as in film and television, requires the machines to discern automatically who, when, how, and where someone is talking, and not. Speaker activity can be automatically discerned from the rich multimodal information present in the media content. This is however a challenging problem due to the vast variety and contextual variability in media content, and the lack of labeled data. In this work, we present a cross-modal neural network for learning visual representations, which have implicit information pertaining to the spatial location of a speaker in the visual frames. Avoiding the need for manual annotations for active speakers in visual frames, acquiring of which is very expensive, we present a weakly supervised system for the task of localizing active speakers in movie content. We use the learned cross-modal visual representations, and provide weak supervision from movie subtitles acting as a proxy for voice activity, thus requiring no manual annotations. Furthermore, we propose an audio-assisted post-processing formulation for the task of active speaker detection. We evaluate the performance of the proposed system on three benchmark datasets: i) AVA active speaker dataset, ii) Visual person clustering dataset, and iii) Columbia datset, and demonstrate the effectiveness of the cross-modal embeddings for localizing active speakers in comparison to fully supervised systems.
C1 [Sharma, Rahul; Narayanan, Shrikanth] Univ Southern Calif, Elect & Comp Engn, Los Angeles, CA 90047 USA.
   [Somandepalli, Krishna] USC, Los Angeles, CA 90007 USA.
   [Somandepalli, Krishna] Google Inc, New York, NY 10011 USA.
C3 University of Southern California; University of Southern California;
   Google Incorporated
RP Sharma, R (corresponding author), Univ Southern Calif, Elect & Comp Engn, Los Angeles, CA 90047 USA.
EM ra.rahulsharma.sh@gmail.com; somandep@usc.edu; shri@sipi.usc.edu
OI Somandepalli, Krishna/0000-0002-2845-1079
CR Afouras T., 2020, P EUR C COMP VIS, V12363, P208, DOI 10.1007/978-3-030- 58523-5-13
   Afouras T, 2022, PROC CVPR IEEE, P10565, DOI 10.1109/CVPR52688.2022.01032
   Alcazar J. L., 2020, IEEECVF C COMPUTER V, p12 465
   Alcázar JL, 2022, LECT NOTES COMPUT SC, V13697, P126, DOI 10.1007/978-3-031-19836-6_8
   Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Barzelay Z., 2007, P IEEE C COMP VIS PA, P1
   Bazzani L, 2016, IEEE WINT CONF APPL, DOI 10.1109/wacv.2016.7477688
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Bredin H, 2020, INT CONF ACOUST SPEE, P7124, DOI [10.1109/ICASSP40776.2020.9052974, 10.1109/icassp40776.2020.9052974]
   Brown A, 2021, IEEE INT CONF COMP V, P3177, DOI 10.1109/ICCVW54120.2021.00357
   Chakravarty P, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P312, DOI 10.1145/2993148.2993172
   Chakravarty P, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P87, DOI 10.1145/2818346.2820780
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   Everingham M., 2006, BMVC, V2, P6
   Fisher JW, 2001, ADV NEUR IN, V13, P772
   Gao Y, 2019, IEEE I CONF COMP VIS, P9833, DOI 10.1109/ICCV.2019.00993
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Hermans M., 2013, Advances in neural information processing systems, V26
   Hershey J, 2000, ADV NEUR IN, V12, P813
   Klemen J, 2012, NEUROSCI BIOBEHAV R, V36, P111, DOI 10.1016/j.neubiorev.2011.04.015
   Alcázar JL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P265, DOI 10.1109/ICCV48922.2021.00033
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Pardo A, 2022, LECT NOTES COMPUT SC, V13667, P668, DOI 10.1007/978-3-031-20071-7_39
   Roth J, 2020, INT CONF ACOUST SPEE, P4492, DOI [10.1109/icassp40776.2020.9053900, 10.1109/ICASSP40776.2020.9053900]
   Schmiedchen K, 2012, BRAIN RES, V1466, P99, DOI 10.1016/j.brainres.2012.05.015
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shahid M, 2021, IEEE WINT CONF APPL, P2331, DOI 10.1109/WACV48630.2021.00238
   Shahid M, 2019, LECT NOTES COMPUT SC, V11751, P48, DOI 10.1007/978-3-030-30642-7_5
   Shams L, 2010, PHYS LIFE REV, V7, P269, DOI 10.1016/j.plrev.2010.04.006
   Sharma R, 2019, IEEE IMAGE PROC, P2991, DOI [10.1109/icip.2019.8803248, 10.1109/ICIP.2019.8803248]
   Somandepalli K, 2021, P IEEE, V109, P891, DOI 10.1109/JPROC.2020.3047978
   Somandepalli K, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P418, DOI 10.1145/3242969.3243026
   Chung JS, 2019, Arxiv, DOI arXiv:1906.10555
   Song CG, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2020.102437
   Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326
   Wan F, 2019, PROC CVPR IEEE, P2194, DOI 10.1109/CVPR.2019.00230
   Wang D., 2020, P IEEECVF C COMPUTER, P12695, DOI DOI 10.1109/CVPR42600.2020.01271
   Wang XL, 2017, PROC CVPR IEEE, P3039, DOI 10.1109/CVPR.2017.324
   Wang Y, 2019, INT CONF ACOUST SPEE, P31, DOI 10.1109/ICASSP.2019.8682847
   Xu HM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3893, DOI 10.1145/3394171.3413581
   Zhang Y. -H., 2019, P ACTIVITYNET LARG S, P1
   Zhao H, 2019, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2019.00182
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
   Zhongzheng Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10595, DOI 10.1109/CVPR42600.2020.01061
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 49
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7825
EP 7836
DI 10.1109/TMM.2022.3229975
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ueno, S
   Fujihashi, T
   Koike-Akino, T
   Watanabe, T
AF Ueno, Soushi
   Fujihashi, Takuya
   Koike-Akino, Toshiaki
   Watanabe, Takashi
TI Point Cloud Soft Multicast for Untethered XR Users
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Point cloud compression; Quantization
   (signal); Wireless communication; Bandwidth; X reality; Compaction;
   Givens rotation; graph fourier transform; point cloud delivery;
   non-uniform quantization; XR devices
ID ATTRIBUTE COMPRESSION; MIMO; POWER; TRANSMISSION; DELIVERY
AB 3D point cloud data formats are used to express three-dimensional (3D) information using numerous points in a 3D space. A key challenge is the delivery of high-quality 3D point cloud for the users under a diverse channel quality and available bandwidth to share the same 3D space across multiple untethered extended reality (XR) users. The existing digital-based schemes suffer from two issues owing to the diversity: cliff and leveling-off effects. This paper proposes a novel soft multicasting scheme of point cloud data for untethered XR users. The key ideas of the proposed scheme are three-fold: 1) integration of graph signal processing and analog modulation to adaptively improve the 3D reconstruction quality according to the channel quality for all individual XR users, 2) integration of Givens rotation and non-uniform adaptive quantization to reduce metadata overhead for the graph Fourier transform, and 3) prioritized transmission of the metadata to realize adaptive quality improvement based on the bandwidth available for each XR user. This paper reveals that the proposed scheme prevents cliff and leveling-off effects even when the XR users experience different channel qualities. Furthermore, the proposed transmission exhibits better 3D reconstruction quality compared with the state-of-the-art graph-based delivery scheme in band-limited environments.
C1 [Ueno, Soushi; Fujihashi, Takuya; Watanabe, Takashi] Osaka Univ, Grad Sch Informat Sci, Osaka 5650871, Japan.
   [Koike-Akino, Toshiaki] Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.
C3 Osaka University
RP Ueno, S; Fujihashi, T (corresponding author), Osaka Univ, Grad Sch Informat Sci, Osaka 5650871, Japan.
EM ueno.soushi@ist.osaka-u.ac.jp; fujihashi.takuya@ist.osaka-u.ac.jp;
   koike@merl.com; watanabe@ist.osaka-u.ac.jp
OI Fujihashi, Takuya/0000-0002-6960-0122
FU JSPS KAKENHI [JP22H03582]; Grants-in-Aid for Scientific Research
   [22H03582] Funding Source: KAKEN
FX The work of Takuya Fujihashi was supported by JSPS KAKENHI under Grant
   JP22H03582.
CR Cui H, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2338279
   Di Lorenzo P, 2018, IEEE T SIGNAL PROCES, V66, P3584, DOI 10.1109/TSP.2018.2835384
   Fan XP, 2013, IEEE T CIRC SYST VID, V23, P1040, DOI 10.1109/TCSVT.2013.2249019
   Fujihashi T., 2018, PROC IEEE GLOB COMMU, P1
   Fujihashi T., 2021, arXiv
   Fujihashi T., 2020, PROC IEEE INT C COMM, P1
   Fujihashi T, 2022, IEEE T MULTIMEDIA, V24, P2179, DOI 10.1109/TMM.2021.3077772
   Fujihashi T, 2021, IEEE ICC, DOI 10.1109/ICC42927.2021.9500925
   Fujihashi T, 2019, IEEE T MULTIMEDIA, V21, P1000, DOI 10.1109/TMM.2018.2870074
   Fujihashi T, 2018, IEEE T MULTIMEDIA, V20, P473, DOI 10.1109/TMM.2017.2743984
   Fujihashi Takuya, 2019, P IEEE INT C COMM, P1
   github, Draco 3D data compression
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   Gu S, 2020, IEEE SIGNAL PROC LET, V27, P176, DOI 10.1109/LSP.2019.2963793
   Gui YQ, 2022, IEEE T MULTIMEDIA, V24, P33, DOI 10.1109/TMM.2020.3045294
   Han B, 2020, MOBICOM '20: PROCEEDINGS OF THE 26TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM 2020), P137, DOI 10.1145/3372224.3380888
   Jakubczak S., 2011, PROC MOBICOM, P289
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Liu XL, 2014, IEEE T MULTIMEDIA, V16, P2038, DOI 10.1109/TMM.2014.2331616
   Lu Yujun, 2020, P IEEE INT C COMM, P1
   Luo L, 2019, IEEE T MULTIMEDIA, V21, P2973, DOI 10.1109/TMM.2019.2919474
   Mekuria R, 2016, IEEE DATA COMPR CONF, P620, DOI 10.1109/DCC.2016.91
   Muller K., 2011, PROC IEEE INT C ROBO, P1
   Ortega A, 2018, P IEEE, V106, P808, DOI 10.1109/JPROC.2018.2820126
   Pavez E, 2020, IEEE IMAGE PROC, P2726, DOI 10.1109/ICIP40778.2020.9191183
   Pavez E, 2018, APSIPA TRANS SIGNAL, V7, DOI 10.1017/ATSIP.2018.15
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Roh JC, 2007, IEEE T WIREL COMMUN, V6, P282, DOI 10.1109/TWC.2007.05195
   Sadrabadi MA, 2006, IEEE T WIREL COMMUN, V5, P3335, DOI [10.1109/TWC.2006.256951, 10.1109/TWC.2006.04599]
   Shen FY, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675449
   Shen J, 2018, IEEE T MULTIMEDIA, V20, P2788, DOI 10.1109/TMM.2018.2811622
   Souto AL, 2020, IEEE IMAGE PROC, P2701, DOI 10.1109/ICIP40778.2020.9191205
   Tang XW, 2021, IEEE T MULTIMEDIA, V23, P2398, DOI 10.1109/TMM.2020.3011319
   Tang XW, 2020, IEEE T VEH TECHNOL, V69, P9896, DOI 10.1109/TVT.2020.3003478
   Duong CT, 2019, Arxiv, DOI arXiv:1911.08795
   Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506
   Ueno S., 2022, PROC IEEE INT C CONS, P1
   Wu J, 2016, IEEE T MULTIMEDIA, V18, P893, DOI 10.1109/TMM.2016.2535727
   Xu YQ, 2021, IEEE T CIRC SYST VID, V31, P1968, DOI 10.1109/TCSVT.2020.3015901
   Zhang C, 2014, IEEE IMAGE PROC, P2066, DOI 10.1109/ICIP.2014.7025414
   Zhao J, 2019, IEEE J EM SEL TOP C, V9, P58, DOI 10.1109/JETCAS.2019.2898750
NR 41
TC 0
Z9 0
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7185
EP 7195
DI 10.1109/TMM.2022.3218172
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000034
OA hybrid
DA 2024-07-18
ER

PT J
AU Wang, C
   Xu, DJ
   Wan, RJ
   He, B
   Shi, BX
   Duan, LY
AF Wang, Ce
   Xu, Dejia
   Wan, Renjie
   He, Bin
   Shi, Boxin
   Duan, Ling-Yu
TI Background Scene Recovery From an Image Looking Through Colored Glass
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image restoration; colored glass
ID BLIND SEPARATION; REFLECTION
AB Colored glass, which is commonly seen in modern city life, often degrades images taken through it with co-occurring reflection and color bias due to its optical property of simultaneous transmission, reflection, and wavelength-selective absorption. Recovering the clean background behind colored glass is inherently challenging due to the mutual interference of two degradations within a single mixture observation, and has barely been specifically considered by existing image restoration methods. In this paper, we aim at realizing faithful background scene recovery for an image taken in front of colored glass. We first analyze the formation model of mixed degradations caused by colored glass, and propose a cooperative framework to address the mutual interference problem, featuring a novel glass color invariant loss and progressive refinement. Besides, we propose a data synthesis strategy for network training. Experimental results on our newly collected real-world dataset show that our proposed method achieves state-of-the-art performance.
C1 [Wang, Ce; Shi, Boxin; Duan, Ling-Yu] Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Shi, Boxin; Duan, Ling-Yu] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
   [Xu, Dejia] Univ Texas Austin, Dept Elect & Comp Engn, Visual Informat Grp VITA, Austin, TX 78705 USA.
   [Xu, Dejia] Peking Univ, Beijing, Peoples R China.
   [Wan, Renjie] Hong Kong Baptist Univ, Hong Kong, Peoples R China.
   [He, Bin] Megvii Technol Co LTD, Beijing 100086, Peoples R China.
   [He, Bin] Peking Univ, Natl Engn Res Ctr Visual Technol, Beijing, Peoples R China.
C3 Peking University; Peng Cheng Laboratory; University of Texas System;
   University of Texas Austin; Peking University; Hong Kong Baptist
   University; Peking University
RP Duan, LY (corresponding author), Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing 100871, Peoples R China.
EM wce@pku.edu.cn; dejia@utexas.edu; wanpeoplejie@gmail.com;
   cs_hebin@pku.edu.cn; shiboxin@pku.edu.cn; lingyu@pku.edu.cn
RI Wan, Patrick/AAL-2841-2021
OI Wan, Patrick/0000-0002-0161-0367; Xu, Dejia/0000-0001-8474-3095
FU National Natural Science Foundation of China [62088102, 62136001,
   61872012]; Ng Teng Fong Charitable Foundation
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62088102, 62136001, and 61872012, and
   in part by the PKU-NTU Joint Research Institute (JRI) sponsored by a
   donation from the Ng Teng Fong Charitable Foundation.
CR Afifi M, 2020, PROC CVPR IEEE, P1394, DOI 10.1109/CVPR42600.2020.00147
   Afifi M, 2019, IEEE I CONF COMP VIS, P243, DOI 10.1109/ICCV.2019.00033
   Afifi M, 2019, PROC CVPR IEEE, P1535, DOI 10.1109/CVPR.2019.00163
   Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Arvanitopoulos N, 2017, PROC CVPR IEEE, P1752, DOI 10.1109/CVPR.2017.190
   Barnard K, 2000, LECT NOTES COMPUT SC, V1842, P390
   Barron JT, 2015, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2015.51
   Be'ery E, 2008, IEEE T IMAGE PROCESS, V17, P340, DOI 10.1109/TIP.2007.915548
   Chakrabarti A, 2012, IEEE T PATTERN ANAL, V34, P1509, DOI 10.1109/TPAMI.2011.252
   Chandramouli P, 2017, LECT NOTES COMPUT SC, V10113, P129, DOI 10.1007/978-3-319-54187-7_9
   Cheng DL, 2015, PROC CVPR IEEE, P1000, DOI 10.1109/CVPR.2015.7298702
   Deng ZJ, 2019, IEEE I CONF COMP VIS, P2453, DOI 10.1109/ICCV.2019.00254
   Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   Funt B, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P47
   Gai K, 2012, IEEE T PATTERN ANAL, V34, P19, DOI 10.1109/TPAMI.2011.87
   GILES CL, 1982, APPL PHYS LETT, V40, P210, DOI 10.1063/1.93043
   Guo J, 2018, LECT NOTES COMPUT SC, V11208, P282, DOI 10.1007/978-3-030-01225-0_17
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Joze HRV, 2012, COLOR IMAG CONF, P41
   Kim S, 2020, PROC CVPR IEEE, P5163, DOI 10.1109/CVPR42600.2020.00521
   Kong NJ, 2014, IEEE T PATTERN ANAL, V36, P209, DOI 10.1109/TPAMI.2013.45
   Lei CY, 2021, Arxiv, DOI arXiv:2108.03380
   Levin A, 2007, IEEE T PATTERN ANAL, V29, P1647, DOI 10.1109/TPAMI.2007.1106
   Li C, 2020, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR42600.2020.00362
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Ma DQ, 2019, IEEE I CONF COMP VIS, P2444, DOI 10.1109/ICCV.2019.00253
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Qian YL, 2019, PROC CVPR IEEE, P8054, DOI 10.1109/CVPR.2019.00825
   Qian YL, 2016, INT C PATT RECOG, P1899, DOI 10.1109/ICPR.2016.7899914
   Schaefer G, 2005, PROC CVPR IEEE, P148
   Shi W, 2016, LECT NOTES COMPUT SC, V9908, P371, DOI 10.1007/978-3-319-46493-0_23
   Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939
   Sigel Jr GH, 1977, TREATISE MATERIALS S, P5
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun C, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P466, DOI 10.1145/2964284.2967264
   Szeliski R, 2000, PROC CVPR IEEE, P246, DOI 10.1109/CVPR.2000.855826
   Tan R. T., 2008, P IEEE C COMP VIS PA, P1
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Wan RJ, 2021, INT J COMPUT VISION, V129, P385, DOI 10.1007/s11263-020-01372-5
   Wan RJ, 2020, PROC CVPR IEEE, P2395, DOI 10.1109/CVPR42600.2020.00247
   Wan RJ, 2018, PROC CVPR IEEE, P4777, DOI 10.1109/CVPR.2018.00502
   Wan RJ, 2018, IEEE T IMAGE PROCESS, V27, P2927, DOI 10.1109/TIP.2018.2808768
   Wan RJ, 2017, IEEE I CONF COMP VIS, P3942, DOI 10.1109/ICCV.2017.423
   Wan RJ, 2016, IEEE IMAGE PROC, P21, DOI 10.1109/ICIP.2016.7532311
   Wang Y, 2017, IEEE IMAGE PROC, P1382, DOI 10.1109/ICIP.2017.8296508
   Wei KX, 2019, PROC CVPR IEEE, P8170, DOI 10.1109/CVPR.2019.00837
   Wen Q, 2019, PROC CVPR IEEE, P3766, DOI 10.1109/CVPR.2019.00389
   Wu ZP, 2020, STRUCT MULTIDISCIP O, V62, P597, DOI 10.1007/s00158-020-02516-4
   Wu ZQ, 2022, IEEE T MULTIMEDIA, V24, P3782, DOI 10.1109/TMM.2021.3107688
   Xue TF, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766940
   Yang KF, 2015, PROC CVPR IEEE, P2254, DOI 10.1109/CVPR.2015.7298838
   Yano T, 2010, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2010.5540204
   Yu HL, 2020, AAAI CONF ARTIF INTE, V34, P12725
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XN, 2019, PROC CVPR IEEE, P3757, DOI 10.1109/CVPR.2019.00388
   Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zheng Q, 2021, PROC CVPR IEEE, P13390, DOI 10.1109/CVPR46437.2021.01319
   Zheng Q, 2020, PROC CVPR IEEE, P3019, DOI 10.1109/CVPR42600.2020.00309
NR 65
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2876
EP 2887
DI 10.1109/TMM.2022.3152390
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600033
DA 2024-07-18
ER

PT J
AU Wang, D
   Zhang, CP
   Wang, Q
   Tian, YM
   He, LH
   Zhao, L
AF Wang, Di
   Zhang, Caiping
   Wang, Quan
   Tian, Yumin
   He, Lihuo
   Zhao, Lin
TI Hierarchical Semantic Structure Preserving Hashing for Cross-Modal
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Codes; Binary codes; Representation learning; Correlation;
   Hash functions; Feature extraction; Cross-modal retrieval; deep hashing;
   semantic preserving; hierarchical learning
AB Cross-modal hashing has become a vital technique in cross-modal retrieval due to its fast query speed and low storage cost in recent years. Generally, most of the priors supervised cross-modal hashing methods are flat methods which are designed for non-hierarchical labeled data. They treat different categories independently and ignore the inter-category correlations. In practical applications, many instances are labeled with hierarchical categories. The hierarchical label structure provides rich information among different categories. To rationally take use of category correlations, hierarchical cross-modal hashing is proposed. However, existing methods intend to preserve instance-pairwise or class-pairwise similarities, which cannot fully explore the semantic correlations among different categories and make the learned hash codes less discriminative. In this paper, we propose a deep cross-modal hashing method named hierarchical semantic structure preserving hashing (HSSPH), which directly exploits the label hierarchy information to learn discriminative hash codes. Specifically, HSSPH learns a set of class-wise hash codes for each layer. By augmenting class-wise codes with labels, it generates layer-wise prototype codes which reflect the semantic structure of each layer. In order to enhance the discriminative ability of hash codes, HSSPH supervises the hash codes learning with both labels and semantic structures to preserve the hierarchical semantics. Besides, efficient optimization algorithms are developed to directly learn the discrete hash codes for each instance and each class. Extensive experiments on two benchmark datasets show the superiority of HSSPH over several state-of-the-art methods.
C1 [Wang, Di] Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
   [Wang, Di; Zhao, Lin] Nanjing Univ Sci & Technol, Jiangsu Key Lab Image & Video Understanding Social, Nanjing 210093, Jiangsu, Peoples R China.
   [Zhang, Caiping; Wang, Quan; Tian, Yumin] Xidian Univ, Sch Comp Sci & Technol, Key Labratory Smart Human Comp Interact & Wearable, Xian 710071, Peoples R China.
   [He, Lihuo] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Xidian University; Nanjing University of Science & Technology; Xidian
   University; Xidian University
RP Wang, Q (corresponding author), Xidian Univ, Sch Comp Sci & Technol, Key Labratory Smart Human Comp Interact & Wearable, Xian 710071, Peoples R China.; He, LH (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM wangdi@xidian.edu.cn; zhangcaiping@stu.xidian.edu.cn;
   Qwang@xidian.edu.cn; ymtian@mail.xidian.edu.cn; lhhe@mail.xidian.edu.cn;
   linzhao@njust.edu.cn
RI Zhang, Caiping/AAQ-5944-2020
OI , Lin/0000-0002-8756-2027; Wang, Quan/0000-0001-6913-8604; He,
   Lihuo/0000-0002-0555-3574; Wang, Di/0000-0001-8027-4287
FU National Natural Science Foundation of China [62072354, 61972302,
   62072355, 62172222, 61876146]; Scientific Research Project of Shaanxi
   Province, China [2019ZDLGY13-01]; Fundamental Research Funds for the
   Central Universities [JB210305]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072354, 61972302, 62072355,
   62172222,and 61876146, in part by the Scientific Research Project of
   Shaanxi Province, China under Grant 2019ZDLGY13-01, and in part by the
   Fundamental Research Funds for the Central Universities under Grant
   JB210305.
CR [Anonymous], 2013, Book Linear cross-modal hashing for efficient multimedia search, DOI DOI 10.1145/2502081.2502107
   Cao Y, 2017, AAAI CONF ARTIF INTE, P3974
   Carvalho T, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P866, DOI 10.1109/ICMLA.2017.00-47
   Chatfield K., 2014, PROC 25 BRIT MACH VI
   Cong Bai, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P525, DOI 10.1145/3372278.3390711
   Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Gui J, 2018, IEEE T PATTERN ANAL, V40, P490, DOI 10.1109/TPAMI.2017.2678475
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Jiang QY, 2019, IEEE T IMAGE PROCESS, V28, P3490, DOI 10.1109/TIP.2019.2897944
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Koutaki G., 2016, CORR
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Lin ZJ, 2017, IEEE T CYBERNETICS, V47, P4342, DOI 10.1109/TCYB.2016.2608906
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Lu X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P715, DOI 10.1145/3331184.3331217
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Mithun NC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1856, DOI 10.1145/3240508.3240712
   Rafailidis D, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P781, DOI 10.1145/2911451.2914710
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Singh A. P., 2008, P 14 ACM SIGKDD INT, P650
   Song XM, 2018, ACM/SIGIR PROCEEDINGS 2018, P5, DOI 10.1145/3209978.3209996
   Sun CC, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P725, DOI 10.1145/3331184.3331229
   Wang D, 2018, AAAI CONF ARTIF INTE, P7388
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Ye ZD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P852, DOI 10.1145/3240508.3240560
   Zhan YW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3386, DOI 10.1145/3394171.3413962
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
NR 37
TC 11
Z9 11
U1 2
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1217
EP 1229
DI 10.1109/TMM.2022.3140656
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100015
DA 2024-07-18
ER

PT J
AU Wang, MJ
   Cai, H
   Han, XF
   Zhou, J
   Gong, ML
AF Wang, Mingjie
   Cai, Hao
   Han, Xian-Feng
   Zhou, Jun
   Gong, Minglun
TI STNet: Scale Tree Network With Multi-Level Auxiliator for Crowd Counting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tree structure; scale enhancer; multi-level auxiliator; crowd counting
ID SEGMENTATION; PEOPLE
AB State-of-the-art approaches for crowd counting resort to deepneural networks to predict density maps. However, counting people in congested scenes remains a challenging task because the presence of drastic scale variation, density inconsistency, and complex background can seriously degrade their counting accuracy. To battle the ingrained issue of accuracy degradation, in this paper, we propose a novel and powerful network called Scale Tree Network (STNet) for accurate crowd counting. STNet consists of two key components: a Scale-Tree Diversity Enhancer and a Multi-level Auxiliator. Specifically, the Diversity Enhancer is designed to enrich scale diversity, which alleviates limitations of existing methods caused by insufficient level of scales. A novel tree structure is adopted to hierarchically parse coarse-to-fine crowd regions. Furthermore, a simple yet effective Multi-level Auxiliator is presented to aid in exploiting generalisable shared characteristics at multiple levels, allowing more accurate pixel-wise background cognition. The overall STNet is trained in an end-to-end manner, without the needs for manually tuning loss weights between the main and the auxiliary tasks. Extensive experiments on five challenging crowd datasets demonstrate the superiority of the proposed method.
C1 [Wang, Mingjie; Gong, Minglun] Univ Guelph, Sch Comp Sci, Guelph, ON N1G 2W1, Canada.
   [Cai, Hao] Mem Univ Newfoundland, Dept Comp Sci, St John, NF A1B 3V6, Canada.
   [Han, Xian-Feng] Southwest Univ, Chongqing 400715, Peoples R China.
   [Zhou, Jun] Dalian Maritime Univ, Dalian 116026, Peoples R China.
C3 University of Guelph; Memorial University Newfoundland; Southwest
   University - China; Dalian Maritime University
RP Gong, ML (corresponding author), Univ Guelph, Sch Comp Sci, Guelph, ON N1G 2W1, Canada.
EM mingjiew@mun.ca; hc1864@mun.ca; xianfenghan@swu.edu.cn;
   jun90@dlmu.edu.cn; minglun@uoguelph.ca
OI Gong, Minglun/0000-0001-5820-5381; Han, Xianfeng/0000-0002-4869-4537
FU NSERC; University of Guelph
FX ~This research is funded by NSERC Discovery grants and the University of
   Guelph.~~
CR Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Chen K, 2013, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2013.319
   Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998
   Gastaldi X., 2017, arXiv
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Jiang XH, 2021, IEEE T MULTIMEDIA, V23, P443, DOI 10.1109/TMM.2020.2980945
   Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li D., 2020, P IEEE CVF C COMP VI, P7642
   Li M, 2008, INT C PATT RECOG, P1998
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334
   Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Miao YQ, 2020, AAAI CONF ARTIF INTE, V34, P11765
   Oñoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38
   Paszke A., 2019, Adv. Neural Inf. Process. Syst., V32, P8024
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Shi MJ, 2019, PROC CVPR IEEE, P7271, DOI 10.1109/CVPR.2019.00745
   Sidla Oliver, 2006, 2006 IEEE INT C VID, P70
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Subburaman VB, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P470, DOI 10.1109/AVSS.2012.87
   Pham VQ, 2015, IEEE I CONF COMP VIS, P3253, DOI 10.1109/ICCV.2015.372
   Wan J., 2020, P ADV NEUR INF PROC, P3386
   Wang MJ, 2020, INT CONF ACOUST SPEE, P2008, DOI [10.1109/ICASSP40776.2020.9054238, 10.1109/icassp40776.2020.9054238]
   Wang MJ, 2021, NEUROCOMPUTING, V441, P128, DOI 10.1016/j.neucom.2021.01.112
   Wang Q, 2021, IEEE T PATTERN ANAL, V43, P2141, DOI 10.1109/TPAMI.2020.3013269
   Xiong HP, 2019, IEEE I CONF COMP VIS, P8361, DOI 10.1109/ICCV.2019.00845
   Xiyang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P241, DOI 10.1007/978-3-030-58586-0_15
   Xu CF, 2019, IEEE I CONF COMP VIS, P8381, DOI 10.1109/ICCV.2019.00847
   Yan ZY, 2023, IEEE T CIRC SYST VID, V33, P6544, DOI 10.1109/TCSVT.2021.3137593
   Yichen Shen, 2019, 2019 24th OptoElectronics and Communications Conference (OECC) and 2019 International Conference on Photonics in Switching and Computing (PSC), DOI 10.23919/PS.2019.8817791
   Yutao Hu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P747, DOI 10.1007/978-3-030-58542-6_45
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhao MM, 2019, PROC CVPR IEEE, P12728, DOI 10.1109/CVPR.2019.01302
   Zhao T, 2003, PROC CVPR IEEE, P459, DOI 10.1109/NSSMIC.2003.1352083
NR 46
TC 18
Z9 18
U1 6
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2074
EP 2084
DI 10.1109/TMM.2022.3142398
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100038
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xu, Q
   Wang, JH
   Jiang, B
   Luo, B
AF Xu, Qin
   Wang, Jiahui
   Jiang, Bo
   Luo, Bin
TI Fine-Grained Visual Classification via Internal Ensemble Learning
   Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Cross layer design; Transformers; Head; Ensemble
   learning; Visualization; Annotations; fine-grained visual
   classification; multi-head self-attention; vision transformer
AB Recently, vision transformers (ViTs) have been investigated in fine-grained visual recognition (FGVC) and are now considered state of the art. However, most ViT-based works ignore the different learning performances of the heads in the multi-head self-attention (MHSA) mechanism and its layers. To address these issues, in this paper, we propose a novel internal ensemble learning transformer (IELT) for FGVC. The proposed IELT involves three main modules: multi-head voting (MHV) module, cross-layer refinement (CLR) module, and dynamic selection (DS) module. To solve the problem of the inconsistent performances of multiple heads, we propose the MHV module, which considers all of the heads in each layer as weak learners and votes for tokens of discriminative regions as cross-layer feature based on the attention maps and spatial relationships. To effectively mine the cross-layer feature and suppress the noise, the CLR module is proposed, where the refined feature is extracted and the assist logits operation is developed for the final prediction. In addition, a newly designed DS module adjusts the token selection number at each layer by weighting their contributions of the refined feature. In this way, the idea of ensemble learning is combined with the ViT to improve fine-grained feature representation. The experiments demonstrate that our method achieves competitive results compared with the state of the art on five popular FGVC datasets.
C1 [Xu, Qin; Wang, Jiahui; Luo, Bin] Anhui Univ, Sch Comp Sci & Technol, Key Lab Intelligent Comp & Signal Proc, Anhui Prov Key Lab Multimodal Cognit Computat,Mini, Hefei 230601, Peoples R China.
   [Jiang, Bo] Anhui Univ, Sch Comp Sci & Technol, Informat Mat & Intelligent Sensing Lab Anhui Prov, Hefei 230026, Peoples R China.
   [Jiang, Bo] Inst Artificial Intelligence, Hefei Comprehens Natl Sci Ctr, Hefei, Peoples R China.
C3 Anhui University; Anhui University
RP Jiang, B (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Informat Mat & Intelligent Sensing Lab Anhui Prov, Hefei 230026, Peoples R China.; Jiang, B (corresponding author), Inst Artificial Intelligence, Hefei Comprehens Natl Sci Ctr, Hefei, Peoples R China.
EM xuqin2013@aliyun.com; e21301179@stu.ahu.edu.cn; zeyiabc@163.com;
   ahu_lb@163.com
RI Jiahui, Wang/HNI-4414-2023
OI Wang, Jiahui/0000-0002-6438-1506; Xu, Qin/0000-0002-9020-2006
FU National Natural Science Foundation of China
FX No Statement Available
CR Behera A, 2021, AAAI CONF ARTIF INTE, V35, P929
   Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655
   Brown T., 2020, Advances in Neural Information Processing Systems, V33, P1877, DOI [DOI 10.48550/ARXIV.2005.14165, DOI 10.5555/3495724.3495883]
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Du RY, 2022, IEEE T PATTERN ANAL, V44, P9521, DOI 10.1109/TPAMI.2021.3126668
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao Y, 2020, AAAI CONF ARTIF INTE, V34, P10818
   Gedraite ES, 2011, ELMAR PROC, P393
   Guo P, 2019, IEEE WINT CONF APPL, P1876, DOI 10.1109/WACV.2019.00204
   Han JW, 2022, IEEE T PATTERN ANAL, V44, P579, DOI 10.1109/TPAMI.2019.2933510
   He J, 2022, AAAI CONF ARTIF INTE, P852
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4239, DOI 10.1145/3474085.3475561
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Khosla A., 2011, P WORKSH COMP VIS PA, P806
   Kim K, 2021, IEEE COMPUT SOC CONF, P3065, DOI 10.1109/CVPRW53098.2021.00342
   Korsch D, 2019, LECT NOTES COMPUT SC, V11824, P62, DOI 10.1007/978-3-030-33676-9_5
   Lei Ba J., 2016, arXiv
   Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400
   Liu CB, 2020, AAAI CONF ARTIF INTE, V34, P11555
   Liu M, 2022, IEEE T IMAGE PROCESS, V31, P748, DOI 10.1109/TIP.2021.3135477
   Liu XD, 2022, NEUROCOMPUTING, V492, P137, DOI 10.1016/j.neucom.2022.04.037
   Luo W, 2019, IEEE I CONF COMP VIS, P8241, DOI 10.1109/ICCV.2019.00833
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Paszke A, 2019, ADV NEUR IN, V32
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Rao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1005, DOI 10.1109/ICCV48922.2021.00106
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Sagi O, 2018, WIRES DATA MIN KNOWL, V8, DOI 10.1002/widm.1249
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Song JW, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534004
   Sun GL, 2020, AAAI CONF ARTIF INTE, V34, P12047
   Tan M, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3492221
   Tan M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3209666
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P854, DOI 10.1109/ICCV48922.2021.00091
   Touvron Hugo, 2019, ADV NEURAL INFORM PR
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Vaswani A, 2017, ADV NEUR IN, V30
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang J., 2021, 32 BRIT MACH VIS C 2
   Wang M, 2023, IEEE T CIRC SYST VID, V33, P2798, DOI 10.1109/TCSVT.2022.3227737
   Wang WY, 2020, NEURAL COMPUT APPL, V32, P14613, DOI 10.1007/s00521-020-05148-3
   Wei XS, 2022, IEEE T PATTERN ANAL, V44, P8927, DOI 10.1109/TPAMI.2021.3126648
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xie LX, 2016, PROC CVPR IEEE, P270, DOI 10.1109/CVPR.2016.36
   Xu KR, 2023, IEEE T NEUR NET LEAR, V34, P3488, DOI 10.1109/TNNLS.2021.3112768
   Yang S., 2021, arXiv
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yu J, 2022, IEEE T PATTERN ANAL, V44, P563, DOI 10.1109/TPAMI.2019.2932058
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Zhang CY, 2023, IEEE T MULTIMEDIA, V25, P4691, DOI 10.1109/TMM.2022.3181439
   Zhang CJ, 2023, IEEE T MULTIMEDIA, V25, P6756, DOI 10.1109/TMM.2022.3214431
   Zhang LB, 2019, IEEE I CONF COMP VIS, P8330, DOI 10.1109/ICCV.2019.00842
   Zhang SJ, 2018, ENVIRON POLLUT, V241, P1027, DOI 10.1016/j.envpol.2018.06.016
   Zhang Y, 2022, INT CONF ACOUST SPEE, P3234, DOI 10.1109/ICASSP43922.2022.9747591
   Zhao YF, 2021, PROC CVPR IEEE, P15074, DOI 10.1109/CVPR46437.2021.01483
   Zhao YF, 2021, IEEE T IMAGE PROCESS, V30, P9470, DOI 10.1109/TIP.2021.3126490
   Zheng H., 2019, CVPR, P5012
   Zheng H., 2019, P NEURIPS, P4277
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhou DQ, 2021, Arxiv, DOI [arXiv:2103.11886, 10.48550/arXiv.2103.11886, DOI 10.48550/ARXIV.2103.11886]
   Zhu HW, 2022, PROC CVPR IEEE, P4682, DOI 10.1109/CVPR52688.2022.00465
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
NR 74
TC 15
Z9 15
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9015
EP 9028
DI 10.1109/TMM.2023.3244340
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000027
DA 2024-07-18
ER

PT J
AU Yue, HJ
   Cheng, YJ
   Mao, Y
   Cao, C
   Yang, JY
AF Yue, Huanjing
   Cheng, Yijia
   Mao, Yan
   Cao, Cong
   Yang, Jingyu
TI Recaptured Screen Image Demoireing in Raw Domain
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Cameras; Image restoration; Pipelines; Noise
   reduction; Training; Lenses; Raw image demoireing; class-specific
   learning; demoireing dataset
ID RESTORATION
AB Capturing screen content by smart-phone cameras has become a daily routine to record or share instant information from display screens for convenience. However, these recaptured screen images are often degraded by moire patterns and usually present color cast against the original screen source. We observe that performing demoireing in raw domain before feeding into the image signal processor (ISP) is more effective than demoireing in the sRGB domain as done in recent demoireing works. In this paper, we investigate the demoireing of raw images through a class-specific learning approach. To this end, we build the first well-aligned raw moire image dataset by pixel-wise alignment between the recaptured images and source ones. Noting that document images occupy a large portion of screen contents and have different properties from generic images, we propose a class-specific learning strategy for textual images and natural color images. In addition, to deal with moire patterns with various scales, a multi-scale encoder with multi-level feature fusion is proposed. The shared encoder enables us to extract rich representations for the two kinds of contents and the class-specific decoders benefit the specific content reconstruction by focusing on targeted representations. Experiment results demonstrate that our method achieves state-of-the-art demoireing performance. We have released the code and dataset in https://github.com/tju-chengyijia/RDNet
C1 [Yue, Huanjing; Cheng, Yijia; Mao, Yan; Cao, Cong; Yang, Jingyu] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Yang, JY (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM dayueer@tju.edu.cn; yijia_cheng@tju.edu.cn; maoyan@tju.edu.cn;
   caocong_123@tju.edu.cn; yjy@tju.edu.cn
RI Yang, jingyu/AAA-2088-2021
FU National Natural Science Foundation of China [62072331]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62072331.
CR Abdelhamed A, 2020, IEEE COMPUT SOC CONF, P2077, DOI 10.1109/CVPRW50498.2020.00256
   Abdelhamed A, 2019, IEEE COMPUT SOC CONF, P2197, DOI 10.1109/CVPRW.2019.00273
   Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Anaya J, 2018, J VIS COMMUN IMAGE R, V51, P144, DOI 10.1016/j.jvcir.2018.01.012
   Bin He, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P713, DOI 10.1007/978-3-030-58542-6_43
   Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129
   Cai JR, 2019, IEEE COMPUT SOC CONF, P2211, DOI 10.1109/CVPRW.2019.00274
   Chang M, 2022, IEEE T MULTIMEDIA, V24, P702, DOI 10.1109/TMM.2021.3058586
   Chen C, 2019, IEEE I CONF COMP VIS, P3184, DOI 10.1109/ICCV.2019.00328
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Cheng X, 2019, IEEE INT CONF COMP V, P3486, DOI 10.1109/ICCVW.2019.00432
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gharbi M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982399
   He B, 2019, IEEE I CONF COMP VIS, P2424, DOI 10.1109/ICCV.2019.00251
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ignatov A, 2019, IEEE INT CONF COMP V, P3584, DOI 10.1109/ICCVW.2019.00443
   Joze HRV, 2020, IEEE COMPUT SOC CONF, P2190, DOI 10.1109/CVPRW50498.2020.00267
   Khashabi D, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2359774
   Kim S., 2020, P IEEE C COMP VIS PA, P426
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liang CH, 2022, IEEE T MULTIMEDIA, V24, P61, DOI 10.1109/TMM.2020.3045303
   Liang ZT, 2021, IEEE T IMAGE PROCESS, V30, P2248, DOI 10.1109/TIP.2021.3051486
   Lin Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P86, DOI 10.1007/978-3-030-58601-0_6
   Liu B., 2018, arXiv
   Liu FL, 2015, 2015 VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Liu JM, 2019, IEEE COMPUT SOC CONF, P2070, DOI 10.1109/CVPRW.2019.00259
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lugmayr A, 2020, IEEE COMPUT SOC CONF, P2058, DOI 10.1109/CVPRW50498.2020.00255
   Ma JY, 2022, IEEE T MULTIMEDIA, V24, P3157, DOI 10.1109/TMM.2021.3094058
   Mei KF, 2019, IEEE INT CONF COMP V, P3441, DOI 10.1109/ICCVW.2019.00427
   Plötz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294
   Ratnasingam S, 2019, IEEE INT CONF COMP V, P3868, DOI 10.1109/ICCVW.2019.00480
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schwartz E, 2019, IEEE T IMAGE PROCESS, V28, P912, DOI 10.1109/TIP.2018.2872858
   Siddiqui H, 2010, IEEE T IMAGE PROCESS, V19, P746, DOI 10.1109/TIP.2009.2035238
   Sun B, 2014, IEEE T IMAGE PROCESS, V23, P3698, DOI 10.1109/TIP.2014.2332394
   Sun YJ, 2018, IEEE T IMAGE PROCESS, V27, P4160, DOI 10.1109/TIP.2018.2834737
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Xu DJ, 2020, IEEE COMPUT SOC CONF, P1943, DOI 10.1109/CVPRW50498.2020.00244
   Xu XY, 2019, PROC CVPR IEEE, P1723, DOI 10.1109/CVPR.2019.00182
   Yang J., 2017, P IEEE VIS COMM IM P, P1
   Yang JY, 2017, IEEE T IMAGE PROCESS, V26, P3528, DOI 10.1109/TIP.2017.2698920
   Yang QS, 2018, IEEE T MED IMAGING, V37, P1348, DOI 10.1109/TMI.2018.2827462
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yuan SX, 2020, IEEE COMPUT SOC CONF, P1882, DOI 10.1109/CVPRW50498.2020.00238
   Yuan SX, 2019, IEEE INT CONF COMP V, P3534, DOI 10.1109/ICCVW.2019.00438
   Yuanhao Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P36, DOI 10.1007/978-3-030-58526-6_3
   Yue HJ, 2021, NEUROCOMPUTING, V456, P352, DOI 10.1016/j.neucom.2021.05.099
   Yue HJ, 2021, IEEE T CIRC SYST VID, V31, P49, DOI 10.1109/TCSVT.2020.2969984
   Yue HJ, 2020, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR42600.2020.00237
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zamir SW, 2020, PROC CVPR IEEE, P2693, DOI 10.1109/CVPR42600.2020.00277
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XN, 2019, PROC CVPR IEEE, P3757, DOI 10.1109/CVPR.2019.00388
   Zheng BL, 2020, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR42600.2020.00369
NR 57
TC 2
Z9 2
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5589
EP 5600
DI 10.1109/TMM.2022.3198333
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300069
DA 2024-07-18
ER

PT J
AU Zhai, Q
   Yang, F
   Li, X
   Xie, GS
   Cheng, H
   Liu, ZC
AF Zhai, Qiang
   Yang, Fan
   Li, Xin
   Xie, Guo-Sen
   Cheng, Hong
   Liu, Zicheng
TI Co-Communication Graph Convolutional Network for Multi-View Crowd
   Counting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Task analysis; Cognition; Feature extraction; Three-dimensional
   displays; Synchronization; Representation learning; Crowd counting;
   multi-view representation learning; graph convolutional network
ID SET
AB We study and address the multi-view crowd counting (MVCC) problem which poses more realistic challenges than single-view crowd counting for better facilitating crowd management/public safety systems. Its major challenge lies in how to fully distill and aggregate useful, complementary information among multiple camera views to create powerful ground-plane representations for wide-area crowd analysis. In this paper, we present a graph-based, multi-view learning model called Co-Communication Graph Convolutional Network (CoCo-GCN) to jointly investigate intra-view contextual dependencies and inter-view complementary relations. More specifically, CoCo-GCN builds a view-agnostic graph interaction space for each camera view to conduct efficient contextual reasoning, and extends the intra-view reasoning by using a novel Graph Communication Layer (GCL) to also take between-graph (cross-view), complementary information into account. Moreover, CoCo-GCN uses a new Co-Memory Layer (CoML) to jointly coarsen the graphs and close the 'representational gap' among them for further exploiting the compositional nature of graphs and learning more consistent representations. Finally, these jointly learned features of multiple views can be easily fused to create ground-plane representations for wide-area crowd counting. Experiments show that the proposed CoCo-GCN achieves state-of-the-art results on three MVCC datasets, i.e., PETS2009, DukeMTMC, and City Street, significantly improving the scene-level accuracy over previous models.
C1 [Zhai, Qiang; Cheng, Hong] Univ Elect Sci & Technol China, Ctr Robot, Chengdu 611731, Peoples R China.
   [Yang, Fan] AIQ, Abu Dhabi 51133, U Arab Emirates.
   [Li, Xin; Xie, Guo-Sen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Liu, Zicheng] Microsoft AI Percept & Mixed Real, Redmond, WA 98101 USA.
C3 University of Electronic Science & Technology of China; Nanjing
   University of Science & Technology
RP Cheng, H (corresponding author), Univ Elect Sci & Technol China, Ctr Robot, Chengdu 611731, Peoples R China.
EM qiang.zh6@gmail.com; fanyang_uestc@hotmail.com; xinli_uestc@hotmail.com;
   gsxiehm@gmail.com; hcheng@uestc.edu.cn; zliu@microsoft.com
OI Liu, Zicheng/0000-0001-5894-7828; xie, guosen/0000-0002-5487-9845; Li,
   Xin/0000-0001-8047-9610; Zhai, Qiang/0000-0001-5328-675X; Yang,
   Fan/0000-0002-1157-8719
FU National Natural Science Foundation of China
FX No Statement Available
CR Ao Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P346, DOI 10.1007/978-3-030-58610-2_21
   Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465
   Bianchi FM, 2020, PR MACH LEARN RES, V119
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Dai F, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P64, DOI 10.1145/3460426.3463628
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dittrich F, 2017, Arxiv, DOI arXiv:1704.00326
   Ferryman J., 2009, P IEEE INT WORKSH PE, P1
   Gao GS, 2020, Arxiv, DOI [arXiv:2003.12783, DOI 10.48550/ARXIV.2003.12783]
   Gao JY, 2021, IEEE T CYBERNETICS, V51, P4822, DOI 10.1109/TCYB.2020.3034316
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P2162, DOI 10.1109/TMM.2020.3008035
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Jiang XH, 2021, IEEE T MULTIMEDIA, V23, P443, DOI 10.1109/TMM.2020.2980945
   Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629
   Kang D, 2019, IEEE T CIRC SYST VID, V29, P1408, DOI 10.1109/TCSVT.2018.2837153
   Khasahmadi A. H., 2020, P INT C LEARN REPR, P2400
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li JW, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P178, DOI 10.1109/AVSS.2012.54
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Li ZY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2952
   Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192
   Liang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P164, DOI 10.1007/978-3-030-58607-2_10
   Liu CC, 2019, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2019.00131
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu LB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1553, DOI 10.1145/3240508.3240681
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Luo A, 2022, AAAI CONF ARTIF INTE, P1890
   Luo A, 2020, AAAI CONF ARTIF INTE, V34, P11693
   Ma HD, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2089094.2089107
   Maddalena L, 2014, PATTERN RECOGN LETT, V36, P125, DOI 10.1016/j.patrec.2013.10.006
   Mohamed Abduallah, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14412, DOI 10.1109/CVPR42600.2020.01443
   Nadeem U, 2021, INFORM SCIENCES, V580, P578, DOI 10.1016/j.ins.2021.08.093
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Ojala T, 2000, LECT NOTES COMPUT SC, V1842, P404
   Pritzel A., 2017, P 34 INT C MACH LEAR, P2827
   Reddy MKK, 2022, IEEE T MULTIMEDIA, V24, P1008, DOI 10.1109/TMM.2021.3062481
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Rodriguez M, 2011, IEEE I CONF COMP VIS, P1235, DOI 10.1109/ICCV.2011.6126374
   Ryan D, 2014, PATTERN RECOGN LETT, V44, P98, DOI 10.1016/j.patrec.2013.10.002
   Semertzidis T, 2010, IET INTELL TRANSP SY, V4, P103, DOI 10.1049/iet-its.2008.0092
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Simonyan K., 2014, CORR
   Sindagi VA, 2018, PATTERN RECOGN LETT, V107, P3, DOI 10.1016/j.patrec.2017.07.007
   Studiawan Hudan, 2020, Advanced Information Networking and Applications. Proceedings of the 33rd International Conference on Advanced Information Networking and Applications (AINA-2019). Advances in Intelligent Systems and Computing (AISC 926), P914, DOI 10.1007/978-3-030-15032-7_77
   Tang NC, 2015, IEEE T IMAGE PROCESS, V24, P80, DOI 10.1109/TIP.2014.2363445
   Tianyi Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P34, DOI 10.1007/978-3-030-58520-4_3
   Varior R. R., 2019, Scale-aware attention network for crowd counting
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan J, 2019, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2019.00416
   Wang BL, 2020, AAAI CONF ARTIF INTE, V34, P6110
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xia W, 2021, IEEE T MULTIMEDIA, V24, P3182, DOI 10.1109/TMM.2021.3094296
   Xiyang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P241, DOI 10.1007/978-3-030-58586-0_15
   Yan Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P242, DOI 10.1007/978-3-030-58555-6_15
   Yang H, 2020, PROC CVPR IEEE, P3802, DOI 10.1109/CVPR42600.2020.00386
   Yaobin Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7728, DOI 10.1109/CVPR42600.2020.00775
   Yifan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P1, DOI 10.1007/978-3-030-58598-3_1
   Ying R, 2018, ADV NEUR IN, V31
   Yutao Hu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P747, DOI 10.1007/978-3-030-58542-6_45
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhang A. B., 2022, IEEE Trans. Neural Netw.Learn. Syst., P1
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang H., 2022, IEEE Trans. Knowl. DataEng., DOI [10.1109/TKDE.2022.3144352.[41]D., DOI 10.1109/TKDE.2022.3144352.[41]D]
   Zhang L, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4277, DOI 10.1145/3474085.3475567
   Zhang Q, 2022, INT J COMPUT VISION, V130, P1938, DOI 10.1007/s11263-022-01626-4
   Zhang Q, 2021, PROC CVPR IEEE, P557, DOI 10.1109/CVPR46437.2021.00062
   Zhang Q, 2020, AAAI CONF ARTIF INTE, V34, P12837
   Zhang Q, 2019, PROC CVPR IEEE, P8289, DOI 10.1109/CVPR.2019.00849
   Zhang SH, 2017, IEEE I CONF COMP VIS, P3687, DOI 10.1109/ICCV.2017.396
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhang Z, 2021, IEEE T NEUR NET LEAR, V32, P4514, DOI 10.1109/TNNLS.2020.3018790
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao MM, 2019, PROC CVPR IEEE, P12728, DOI 10.1109/CVPR.2019.01302
   Zheng Y. Li, 2021, IEEE INT C MULT EXP, P1
   Zhou J., 2020, OPEN, V1, P57
NR 85
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5813
EP 5825
DI 10.1109/TMM.2022.3199555
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300001
DA 2024-07-18
ER

PT J
AU Zhang, HM
   Qian, F
   Zhang, B
   Du, WL
   Qian, JJ
   Yang, J
AF Zhang, Hengmin
   Qian, Feng
   Zhang, Bob
   Du, Wenli
   Qian, Jianjun
   Yang, Jian
TI Incorporating Linear Regression Problems Into an Adaptive Framework With
   Feasible Optimizations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Linear regression; Optimization; Loss measurement; Adaptation models;
   Matrices; Sparse matrices; Convergence; Linear regression; convex vector
   and matrix norm; iteratively re-weighted algorithm; convergence analysis
ID THRESHOLDING ALGORITHM; FACE RECOGNITION; SPLITTING METHOD; CONVERGENCE;
   SPARSITY
AB Accompanied with the increasing popularity of linear regression approaches, most of the existing minimization problems are related with several convex measurements, e.g., l(1)/l(2) /l(2,1) norm of a vector and L-1/L-2,L-1/Frobenius/nuclear norm of amatrix, where the regularized function and the loss function are usually studied for two objective terms case by case, respectively. To address this issue, this work combines these linear regression problems into a unified expression framework by employing an adaptive and flexible function, in which we need to choose different variable elements and adjust an inner parameter, properly. Besides this, they are equipped with some corresponding relationships and their interesting properties. Intuitively speaking, the proposed framework can generalize several traditional linear regression formulations and even more complex ones into an extended representation. For further optimizations, an iteratively re-weighted penalty solution (IRwPS) is devised without any inner loops, making the iteration programming easy to perform. Meanwhile, the theoretical results are provided for guaranteeing that themathematical convergence analysis is solid andmeaningful. Finally, by performing real-world applications in supervised, unsupervised, and semi-supervised tasks, numerical experiments are conducted to validate the theoretical properties and the superiority over some of the state-of-the-art.
C1 [Zhang, Hengmin; Qian, Feng; Du, Wenli] East China Univ Sci & Technol, Sch Informat Sci & Engn, Key Lab Smart Mfg Energy Chem Proc, Minist Educ, Shanghai 200237, Peoples R China.
   [Zhang, Hengmin; Zhang, Bob] Univ Macau, Dept Comp & Informat Sci, Pattern Recognit & Machine Intelligence PAMI Res, Fac Sci & Technol, Macau 999078, Peoples R China.
   [Qian, Feng; Du, Wenli] Tongji Univ, Shanghai Inst Intelligent Sci & Technol, Shanghai 200092, Peoples R China.
   [Qian, Jianjun] PCA Lab, Guangzhou, Peoples R China.
   [Qian, Jianjun; Yang, Jian] Nanjing Univ Sci & Technol, Key Lab Intelligent Percept & Syst High Dimens In, Minist Educ, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
C3 East China University of Science & Technology; University of Macau;
   Tongji University; Nanjing University of Science & Technology
RP Qian, F; Du, WL (corresponding author), East China Univ Sci & Technol, Sch Informat Sci & Engn, Key Lab Smart Mfg Energy Chem Proc, Minist Educ, Shanghai 200237, Peoples R China.
EM zhanghengmin@126.com; fqian@ecust.edu.cn; bobzhang@um.edu.mo;
   wldu@ecust.edu.cn; csjqian@njust.edu.cn; csjyang@njust.edu.cn
RI Zhang, Bob/ABD-5926-2021; WANG, SHIHAO/KHC-8263-2024; li,
   xiaomin/KCX-9845-2024
OI Zhang, Bob/0000-0003-2497-9519; Qian, Jianjun/0000-0002-0968-8556; Du,
   Wenli/0000-0002-2676-6341
FU National Natural Science Foundation of China for the Distinguished Young
   Scholars [61725301]; General Program [61906067, 62176124, 61876083];
   Youth Program [61906067, 62176124, 61876083]; China Postdoctoral Science
   Foundation [2019M651415, 2020T130191]; UM Macao Talent Programme
   [UMMTP-2020-01]
FX This work was supported in part by the National Natural Science
   Foundation of China for the Distinguished Young Scholars under Grant
   61725301, and in part General and Youth Programs under Grants 61906067,
   62176124, and 61876083, and in part by the China Postdoctoral Science
   Foundation under Grants 2019M651415 and 2020T130191, and in part by UM
   Macao Talent Programme under Grant UMMTP-2020-01.
CR [Anonymous], [No title captured]
   Asuncion A., 2007, Uci machine learning repository
   Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bouwmans T, 2017, COMPUT SCI REV, V23, P1, DOI 10.1016/j.cosrev.2016.11.001
   Boyd S., 2011, FDN TRENDS MACH LEAR, P1, DOI DOI 10.1561/2200000016
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Cai SJ, 2016, PROC CVPR IEEE, P2950, DOI 10.1109/CVPR.2016.322
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candès EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Candès EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Chen YD, 2018, IEEE T MULTIMEDIA, V20, P3212, DOI 10.1109/TMM.2018.2834867
   Chen Yudong, 2011, Proceedings of the 28th International Conference on Machine Learning, P873
   DING C, 2013, A NEW ROBUST FUNCTIO
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Fazel M, 2001, P AMER CONTR CONF, P4734, DOI 10.1109/ACC.2001.945730
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   He R, 2011, NEURAL COMPUT, V23, P2074, DOI 10.1162/NECO_a_00155
   Jing PG, 2020, IEEE T MULTIMEDIA, V22, P1555, DOI 10.1109/TMM.2019.2944749
   Jing PG, 2019, IEEE T CIRC SYST VID, V29, P1296, DOI 10.1109/TCSVT.2018.2832095
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Lin TY, 2015, SIAM J OPTIMIZ, V25, P1478, DOI 10.1137/140971178
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   LIN Z, 2010, THE AUGMENTED LAGRAN
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Lu CY, 2016, IEEE T IMAGE PROCESS, V25, P829, DOI 10.1109/TIP.2015.2511584
   Lu CY, 2013, IEEE I CONF COMP VIS, P1345, DOI 10.1109/ICCV.2013.170
   Lu CY, 2012, PROCEEDINGS OF INTERNATIONAL CONFERENCE ON COMPUTER VISION IN REMOTE SENSING, P34
   Luo L, 2017, IEEE T NEUR NET LEAR, V28, P2168, DOI 10.1109/TNNLS.2016.2573644
   Luo L, 2016, IEEE T IMAGE PROCESS, V25, P5757, DOI 10.1109/TIP.2016.2612885
   Luo L, 2015, PATTERN RECOGN, V48, P3811, DOI 10.1016/j.patcog.2015.06.012
   Nie F., 2013, P 23 INT JOINT C ART, P1565
   Peng X, 2018, IEEE T NEUR NET LEAR, V29, P218, DOI 10.1109/TNNLS.2016.2608834
   Qian M., 2015, NEUR NETW IJCNN 2015, P1
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Srihari SN, 1997, PROC INT CONF DOC, P892, DOI 10.1109/ICDAR.1997.620640
   Toh KC, 2010, PAC J OPTIM, V6, P615
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yan JJ, 2016, IEEE T MULTIMEDIA, V18, P1319, DOI 10.1109/TMM.2016.2557721
   Yang J, 2017, IEEE T PATTERN ANAL, V39, P156, DOI 10.1109/TPAMI.2016.2535218
   Yang L, 2017, SIAM J IMAGING SCI, V10, P74, DOI 10.1137/15M1027528
   Zhang HM, 2019, IEEE T NEUR NET LEAR, V30, P2916, DOI 10.1109/TNNLS.2019.2900572
   Zhang HM, 2019, IEEE T NEUR NET LEAR, V30, P2825, DOI 10.1109/TNNLS.2018.2885699
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang R, 2020, IEEE T FUZZY SYST, V28, P2814, DOI 10.1109/TFUZZ.2019.2945232
   Zhang SB, 2016, AAAI CONF ARTIF INTE, P2330
   Zhang Z, 2015, IEEE ACCESS, V3, P490, DOI 10.1109/ACCESS.2015.2430359
   Zhou ZH, 2010, IEEE INT SYMP INFO, P1518, DOI 10.1109/ISIT.2010.5513535
NR 52
TC 15
Z9 15
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4041
EP 4051
DI 10.1109/TMM.2022.3171088
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500037
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Pu, LJ
   Lin, T
   Yan, JY
AF Zhang, Yuan
   Pu, Lingjun
   Lin, Tao
   Yan, Jinyao
TI QoE-Oriented Mobile Virtual Reality Game in Distributed Edge Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Games; Rendering (computer graphics); Delays; Quality of experience;
   Visualization; Servers; Convex functions; ADMM; mobile virtual reality;
   QoE; rendering level
ID SERVICE PLACEMENT
AB Mobile edge computing is a promising framework for mobile virtual reality (VR) game. Although there are several existing studies on the edge assisted mobile VR game system, they lack the consideration of provisioning services with satisfactory QoE to a large number of users. In this paper, we consider the problem of providing QoE-oriented edge assisted mobile VR game as a service to multiple users, with a comprehensive QoE concern of both visual and delay aspects. Due to the unique features of mobile VR game, the problem is formulated into a Mixed Integer Quadratically Constrained Quadratic Programming (MIQCQP) problem. We show that the problem is NP-hard with object placement decision and rendering level selection decision quadratically coupling together. To solve this problem, we propose the Alternating Directions Method of Multipliers (ADMM) algorithm which can iteratively decouple the quadratic terms and reform the problem into the efficiently solvable MIQCQP-1 (i.e., MIQCQP with one constraint) problem. Trace driven simulation shows that our algorithm fits the edge assisted mobile VR game scenario well with fast computation time (at least 4 orders of magnitude less computation time compared to Gurobi solver) and good performance (at least 18% of user visual QoE improvement compared to other mobile VR scheme).
C1 [Zhang, Yuan; Lin, Tao] Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China.
   [Yan, Jinyao] Commun Univ China, Key Lab Media Audio & Video, Beijing 100024, Peoples R China.
   [Pu, Lingjun] Nankai Univ, Inst Syst & Networks, Coll Comp Sci, Tianjin Key Lab NDST & TMCC, Tianjin 300071, Peoples R China.
C3 Communication University of China; Communication University of China;
   Nankai University
RP Yan, JY (corresponding author), Commun Univ China, Key Lab Media Audio & Video, Beijing 100024, Peoples R China.
EM yuanzhang@cuc.edu.cn; pulingjun@nankai.edu.cn; lintao@cuc.edu.cn;
   jyan@cuc.edu.cn
OI Lin, Tao/0000-0003-1170-636X
FU National Key Research and Development Program of China
FX No Statement Available
CR An XM, 2022, IEEE INTERNET THINGS, V9, P16546, DOI 10.1109/JIOT.2022.3150976
   [Anonymous], 2017, Robo Recall
   [Anonymous], 2022, Virtual reality (VR) market-Growth, trends, COVID-19 impact, and forecasts (2021-2026)
   [Anonymous], 2017, Several opensource VR game on github
   [Anonymous], 2022, Virtual reality market size, share and trends analysis report by technology
   [Anonymous], 2017, Playerunknown's Battlegrounds
   Batmaz AU, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P167, DOI 10.1109/VR.2018.8446217
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Bozkir E, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P597, DOI 10.1109/VR50410.2021.00085
   Buchbinder Niv, 2014, P 25 ANN ACM SIAM S, P436
   Chakareski J, 2023, IEEE T MULTIMEDIA, V25, P5941, DOI 10.1109/TMM.2022.3201397
   Chen J., 2021, P IEEE C COMP COMM, P1
   Chen JG, 2022, INT CON DISTR COMP S, P1018, DOI 10.1109/ICDCS54860.2022.00102
   Chen Y, 2022, IEEE CONF COMPUT, DOI 10.1109/INFOCOMWKSHPS54753.2022.9797972
   Cheon M, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P78, DOI 10.1109/ISM.2014.33
   Chopra L, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2379, DOI 10.1145/3442381.3450070
   Fei ZS, 2020, IEEE J-STSP, V14, P78, DOI 10.1109/JSTSP.2019.2956631
   Han R, 2022, IEEE INFOCOM SER, P880, DOI 10.1109/INFOCOM48880.2022.9796792
   Hong MY, 2016, SIAM J OPTIMIZ, V26, P337, DOI 10.1137/140990309
   Huang KJ, 2016, IEEE T SIGNAL PROCES, V64, P5297, DOI 10.1109/TSP.2016.2593681
   Huang L, 2022, INT CON DISTR COMP S, P403, DOI 10.1109/ICDCS54860.2022.00046
   Islam MA, 2018, PROC CVPR IEEE, P7142, DOI 10.1109/CVPR.2018.00746
   Jiayi Meng, 2020, ASPLOS '20: Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, P923, DOI 10.1145/3373376.3378516
   Kämäräinen T, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1181, DOI 10.1145/3240508.3240620
   Keighrey C, 2021, IEEE T MULTIMEDIA, V23, P333, DOI 10.1109/TMM.2020.2982046
   Lai ZQ, 2020, IEEE T MOBILE COMPUT, V19, P1586, DOI 10.1109/TMC.2019.2913364
   Liang Y, 2021, IEEE T MOBILE COMPUT, V20, P1064, DOI 10.1109/TMC.2019.2952097
   Liu LY, 2018, MOBISYS'18: PROCEEDINGS OF THE 16TH ACM INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P68, DOI 10.1145/3210240.3210313
   Liu X, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P943
   Lu HL, 2022, INT CON DISTR COMP S, P526, DOI 10.1109/ICDCS54860.2022.00057
   Park J, 2017, Arxiv, DOI arXiv:1703.07870
   Raake A., 2017, 2017 9 INT C QUALITY, P1, DOI DOI 10.1109/QOMEX.2017.7965631
   Saleem U, 2021, IEEE T WIREL COMMUN, V20, P360, DOI 10.1109/TWC.2020.3024538
   Seung-Hwan Lim, 2012, Performance Evaluation Review, V40, P271, DOI 10.1145/2318857.2254790
   Shi JX, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3124, DOI 10.1145/3503161.3547750
   Shi Shu., 2019, Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services, P130
   Sun LY, 2023, IEEE T MULTIMEDIA, V25, P2636, DOI 10.1109/TMM.2022.3149642
   Sun Y, 2022, IEEE INFOCOM SER, P790, DOI 10.1109/INFOCOM48880.2022.9796947
   Tan Z., 2019, ACM SIGMETRICS Perform. Eval. Rev., V46, P74
   Wang L, 2021, IEEE ACM T NETWORK, V29, P34, DOI 10.1109/TNET.2020.3025985
   Wang L, 2018, IEEE INFOCOM SER, P468, DOI 10.1109/INFOCOM.2018.8486411
   Wang SB, 2022, PROCEEDINGS OF THE 2022 THE 28TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, ACM MOBICOM 2022, P542, DOI 10.1145/3495243.3517018
   Wolf D, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P347, DOI 10.1109/VR.2018.8448289
   Zhang A, 2022, PROCEEDINGS OF THE 19TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '22), P137
   Zhang HD, 2023, IEEE T MULTIMEDIA, V25, P4225, DOI 10.1109/TMM.2022.3172550
   Zhang J, 2020, INT CONF UTIL CLOUD, P402, DOI 10.1109/UCC48980.2020.00063
   Zhang Q, 2023, IEEE T MULTIMEDIA, V25, P5386, DOI 10.1109/TMM.2022.3190697
   Zhang WX, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2232, DOI 10.1145/3442381.3449829
   Zhang X, 2021, PROCEEDINGS OF THE 18TH USENIX SYMPOSIUM ON NETWORKED SYSTEM DESIGN AND IMPLEMENTATION, P303
   Zhang Y, 2019, IEEE J SEL AREA COMM, V37, P1881, DOI 10.1109/JSAC.2019.2927071
NR 50
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9132
EP 9146
DI 10.1109/TMM.2023.3247182
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200037
DA 2024-07-18
ER

PT J
AU Zhao, YB
   Zhang, H
   Gao, Z
   Gao, WJ
   Wang, M
   Chen, SY
AF Zhao, Yibo
   Zhang, Hua
   Gao, Zan
   Gao, Wenjie
   Wang, Meng
   Chen, Shengyong
TI A Novel Action Saliency and Context-Aware Network for Weakly-Supervised
   Temporal Action Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action saliency and context-aware network; temporal saliency and context
   module channel self-attention module; hybrid attention mechanism;
   weakly-supervised temporal action localization
AB Temporal action localization is a challenging task in computer vision, and it tries to find the start time and the end time of the actions and predict their categories. However, compared to temporal action localization, weakly supervised temporal action localization (WTAL) is a more challenging task due to its poor annotations. With only video-level annotation, some background frames, similar to actions, would be classified as actions and produce inaccurate results. In addition, the two-stream fusion problem, ignored previously, also needs to be further considered. To resolve these issues, we propose a novel action saliency and context-aware network (ASCN) for WTAL tasks. Specifically, the temporal saliency and context module is designed to enhance the global saliency and context information of the RGB and flow features to suppress the backgrounds and enhance the actions. In addition, a hybrid attention mechanism using frame differences and two-stream attention is designed to model the local action context information and further enlarge the scores of the potential action regions and suppress the background regions. Finally, to obtain two-stream consistency and solve the fusion problem, we use the similarity loss and a channel self-attention module to adaptively fuse the enhanced RGB and flow features. Extensive experiments demonstrate that ASCN can outperform all of the SOTA WTAL methods on THUMOS14 dataset and ActivityNet1.3 dataset with an average mAP that can reach 37.2% on THUMOS14 dataset and attains an average mAP of 26.3% on ActivityNet1.3 dataset. On ActivityNet1.2 dataset, ASCN can also obtain comparable results.
C1 [Zhao, Yibo; Zhang, Hua; Gao, Zan; Gao, Wenjie; Chen, Shengyong] Tianjin Univ Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China.
   [Gao, Zan] Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Jinan 250014, Peoples R China.
   [Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 300384, Peoples R China.
C3 Tianjin University of Technology; Qilu University of Technology; Hefei
   University of Technology
RP Gao, Z (corresponding author), Tianjin Univ Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China.; Gao, Z (corresponding author), Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Jinan 250014, Peoples R China.
EM zybtjut@163.com; hzhang62@163.com; zangaonsh4522@gmail.com;
   gaowenjie0911@163.com; eric.mengwang@gmail.com; sy@ieee.org
RI Zhao, yibo/GWV-0812-2022; Chen, S./H-3083-2011
OI Chen, S.Y./0000-0002-6705-3831; Zhao, Yibo/0000-0003-4187-1980; ,
   zan/0000-0003-2182-5741
FU National Natural Science Foundation of China
FX No Statement Available
CR Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cao M, 2022, IEEE T IMAGE PROCESS, V31, P5203, DOI 10.1109/TIP.2022.3193752
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen XD, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3547892
   Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao JY, 2022, PROC CVPR IEEE, P19967, DOI 10.1109/CVPR52688.2022.01937
   Gao Z, 2022, IEEE T NEUR NET LEAR, V33, P1147, DOI 10.1109/TNNLS.2020.3041018
   Gao Z, 2021, IEEE T IMAGE PROCESS, V30, P767, DOI 10.1109/TIP.2020.3038372
   Hong FT, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1591, DOI 10.1145/3474085.3475298
   Huang LJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7982, DOI 10.1109/ICCV48922.2021.00790
   Huang LJ, 2022, IEEE T IMAGE PROCESS, V31, P1504, DOI 10.1109/TIP.2021.3137649
   Islam A, 2021, AAAI CONF ARTIF INTE, V35, P1637
   Ji Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P853, DOI 10.1145/3474085.3475261
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Kingma D., 2014, ICLR, DOI DOI 10.1063/1.4902458
   Lee P, 2021, AAAI CONF ARTIF INTE, V35, P1854
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P330, DOI 10.1109/TPAMI.2020.3011148
   Lin CM, 2021, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR46437.2021.00333
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu XL, 2022, IEEE T IMAGE PROCESS, V31, P5427, DOI 10.1109/TIP.2022.3195321
   Liu Y, 2021, PROC CVPR IEEE, P6172, DOI 10.1109/CVPR46437.2021.00611
   Liu ZY, 2021, AAAI CONF ARTIF INTE, V35, P2233
   Luo W, 2021, PROC CVPR IEEE, P9964, DOI 10.1109/CVPR46437.2021.00984
   Narayan S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13588, DOI 10.1109/ICCV48922.2021.01335
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Passalis N, 2018, LECT NOTES COMPUT SC, V11215, P283, DOI 10.1007/978-3-030-01252-6_17
   Pathak D, 2015, IEEE I CONF COMP VIS, P1796, DOI 10.1109/ICCV.2015.209
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Pramono RRA, 2022, IEEE T MULTIMEDIA, V24, P625, DOI 10.1109/TMM.2021.3056892
   Qu SQ, 2021, Arxiv, DOI arXiv:2104.02967
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen YH, 2018, IEEE T NEUR NET LEAR, V29, P5960, DOI 10.1109/TNNLS.2018.2816021
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Su R, 2021, IEEE T IMAGE PROCESS, V30, P2103, DOI 10.1109/TIP.2020.3044218
   Sun C, 2022, IEEE T MULTIMEDIA, V24, P274, DOI 10.1109/TMM.2021.3050067
   Vaezi Joze Hamid Reza, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13286, DOI 10.1109/CVPR42600.2020.01330
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Xia K., 2022, P IEEE CVF C COMP VI, P13884
   Xing YL, 2021, CAAI T INTELL TECHNO, V6, P80, DOI 10.1049/cit2.12014
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Yang JY, 2021, IEEE T MULTIMEDIA, V23, P883, DOI 10.1109/TMM.2020.2990082
   Yang L, 2022, IEEE T PATTERN ANAL, V44, P9814, DOI 10.1109/TPAMI.2021.3132058
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Yang WF, 2021, PROC CVPR IEEE, P53, DOI 10.1109/CVPR46437.2021.00012
   Yuan Y., 2019, P 7 INT C LEARN REPR
   Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhai YH, 2022, IEEE T MULTIMEDIA, V24, P1857, DOI 10.1109/TMM.2021.3073235
   Zhang C, 2021, PROC CVPR IEEE, P16005, DOI 10.1109/CVPR46437.2021.01575
   Zhang JX, 2022, CAAI T INTELL TECHNO, V7, P46, DOI 10.1049/cit2.12012
   Zhang XY, 2023, IEEE T NEUR NET LEAR, V34, P1852, DOI 10.1109/TNNLS.2019.2962815
   Zhao YB, 2022, IEEE T IMAGE PROCESS, V31, P4746, DOI 10.1109/TIP.2022.3182866
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zheng ZH, 2022, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR52688.2022.00919
NR 65
TC 3
Z9 3
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8253
EP 8266
DI 10.1109/TMM.2023.3234362
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000047
DA 2024-07-18
ER

PT J
AU Zhu, J
   Zhang, QW
   Fei, LK
   Cai, RC
   Xie, Y
   Sheng, B
   Yang, XK
AF Zhu, Jian
   Zhang, Qingwu
   Fei, Lunke
   Cai, Ruichu
   Xie, Yuan
   Sheng, Bin
   Yang, Xiaokang
TI FFFN: Frame-By-Frame Feedback Fusion Network for Video Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Superresolution; Feature extraction; Convolution;
   Task analysis; Three-dimensional displays; Spatiotemporal phenomena;
   Video super-resolution; Feedback mechanism; Frame fusion; Dual-path
   reconstruction
ID IMAGE SUPERRESOLUTION; ACCURATE
AB Video super-resolution (VSR) is a fundamental and challenging task in computer vision. Many of the existing VSR works focus on how to effectively align neighboring frames to better incorporate temporal information, while little work is devoted to the important subsequent step of inter-frame information fusion, and the existing methods on frame fusion have shortcomings such as not being able to make full use of spatio-temporal information. In this work, we propose a Frame-by-frame Feedback Fusion Network (FFFN) for VSR tasks. By applying the feedback learning mechanism commonly existing in the human cognitive system to the frame fusion stage, FFFN can refine low-level representation of the fused frames with high-level information in a coarse-to-fine manner. Specifically, after the neighboring frames are aligned, we first rearrange them from near to far according to the distance from the reference frame in the temporal space, and then feed them one-by-one into a proposed recurrent structure called Feedback Fusion Module (FFM), which is then able to iteratively generate high-level representation of the fused frames with several Feature Refinement Groups (FRGs) and feedback connections. Finally, we design a Dual-path Residual Reconstruction Module (DRRM) to reconstruct the final high-resolution image. The proposed FFFN comes with a strong frame fusion and reconstruction ability, and extensive experiments on several benchmark data sets show that it achieves favorable performance against state-of-the-art methods.
C1 [Zhu, Jian; Zhang, Qingwu; Fei, Lunke; Cai, Ruichu] Guangdong Univ Technol GDUT, Sch Comp Sci, Guangzhou 510006, Peoples R China.
   [Xie, Yuan] East China Normal Univ ECNU, Sch Comp Sci & Technol, Shanghai 200240, Peoples R China.
   [Sheng, Bin] Shanghai Jiao Tong Univ SJTU, Dept Comp Sci & Engn CSE, Shanghai 200240, Peoples R China.
   [Yang, Xiaokang] Shanghai Jiao Tong Univ SJTU, Dept Elect Engn EE, Shanghai 200240, Peoples R China.
C3 East China Normal University; Shanghai Jiao Tong University; Shanghai
   Jiao Tong University
RP Fei, LK (corresponding author), Guangdong Univ Technol GDUT, Sch Comp Sci, Guangzhou 510006, Peoples R China.; Sheng, B (corresponding author), Shanghai Jiao Tong Univ SJTU, Dept Comp Sci & Engn CSE, Shanghai 200240, Peoples R China.
EM dr.zhuj@gmail.com; qing.wu.zhang@qq.com; flksxm@126.com;
   cairuichu@gmail.com; yxie@cs.ecnu.edu.cn; shengbin@cs.sjtu.edu.cn;
   xkyang@sjtu.edu.cn
OI Zhu, Jian/0000-0002-2551-2024; Sheng, Bin/0000-0001-8678-2784; xie,
   yuan/0000-0001-6945-7437; Fei, Lunke/0000-0001-6072-7875
FU National Natural Science Foundation of China (NSFC) [62237001]; National
   Key Research and Development Program of China [2021ZD0111501]; NSFC
   [6212200101, 62272298, 62176066, 61976052]; Guangzhou Science and
   Technology Plan [202002030110, 202007040005]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) for Key Program under Grant 62237001, in part
   by the National Key Research and Development Program of China under
   Grant 2021ZD0111501, in part by the NSFC for Excellent Young Scholars
   under Grant 6212200101, in part by the NSFC for General Program under
   Grants 62272298, 62176066, and 61976052, and in part by Guangzhou
   Science and Technology Plan under Grants 202002030110 and 202007040005.
CR Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338
   Cao JZ, 2023, Arxiv, DOI [arXiv:2106.06847, DOI 10.48550/ARXIV.2106.06847]
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Chu XX, 2021, INT C PATT RECOG, P59, DOI 10.1109/ICPR48806.2021.9413080
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Fuoli D, 2019, IEEE INT CONF COMP V, P3476, DOI 10.1109/ICCVW.2019.00431
   Gilbert CD, 2007, NEURON, V54, P677, DOI 10.1016/j.neuron.2007.05.019
   Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170
   Gupta A, 2017, IEEE I CONF COMP VIS, P4087, DOI 10.1109/ICCV.2017.438
   Han W, 2018, PROC CVPR IEEE, P1654, DOI 10.1109/CVPR.2018.00178
   Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Huang Y, 2015, ADV NEUR IN, V28
   Hupé JM, 1998, NATURE, V394, P784, DOI 10.1038/29537
   Isobe Takashi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P645, DOI 10.1007/978-3-030-58610-2_38
   Jia X., 2016, PROC EUR C COMPUT VI, P1
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim SY, 2019, IEEE IMAGE PROC, P2831, DOI [10.1109/icip.2019.8803297, 10.1109/ICIP.2019.8803297]
   Kingma D. P., 2014, arXiv
   Lai W., 2018, PROC EUR C COMPUT VI, P1
   Li Wenbo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P335, DOI 10.1007/978-3-030-58607-2_20
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liu C, 2014, IEEE T PATTERN ANAL, V36, P346, DOI 10.1109/TPAMI.2013.127
   Liu D, 2018, IEEE T IMAGE PROCESS, V27, P3432, DOI 10.1109/TIP.2018.2820807
   Liu HY, 2021, AAAI CONF ARTIF INTE, V35, P2127
   Nilsson D, 2018, PROC CVPR IEEE, P6819, DOI 10.1109/CVPR.2018.00713
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Sam DB, 2018, AAAI CONF ARTIF INTE, P7323
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szeto R., 2021, PROC IEEECVF WINTER, P3080
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang LG, 2021, PROC CVPR IEEE, P4915, DOI 10.1109/CVPR46437.2021.00488
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wei YX, 2021, PROC CVPR IEEE, P13380, DOI 10.1109/CVPR46437.2021.01318
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Ying XY, 2020, IEEE SIGNAL PROC LET, V27, P1500, DOI 10.1109/LSP.2020.3013518
   Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196
   Zhang DY, 2021, IEEE T MULTIMEDIA, V23, P2172, DOI 10.1109/TMM.2020.3008041
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 56
TC 3
Z9 3
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6821
EP 6835
DI 10.1109/TMM.2022.3214776
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000009
DA 2024-07-18
ER

PT J
AU Zhu, T
   Li, LD
   Yang, JF
   Zhao, SC
   Liu, HT
   Qian, JS
AF Zhu, Tong
   Li, Leida
   Yang, Jufeng
   Zhao, Sicheng
   Liu, Hantao
   Qian, Jiansheng
TI Multimodal Sentiment Analysis With Image-Text Interaction Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Index Terms-Image-text interaction; multimodal sentiment analysis;
   region-word alignment
ID LANGUAGE; CONTEXT
AB More and more users are getting used to posting images and text on social networks to share their emotions or opinions. Accordingly, multimodal sentiment analysis has become a research topic of increasing interest in recent years. Typically, there exist affective regions that evoke human sentiment in an image, which are usually manifested by corresponding words in people's comments. Similarly, people also tend to portray the affective regions of an image when composing image descriptions. As a result, the relationship between image affective regions and the associated text is of great significance for multimodal sentiment analysis. However, most of the existing multimodal sentiment analysis approaches simply concatenate features from image and text, which could not fully explore the interaction between them, leading to suboptimal results. Motivated by this observation, we propose a new image-text interaction network (ITIN) to investigate the relationship between affective image regions and text for multimodal sentiment analysis. Specifically, we introduce a cross-modal alignment module to capture region-word correspondence, based on which multimodal features are fused through an adaptive cross-modal gating module. Moreover, considering the complementary role of context information on sentiment analysis, we integrate the individual-modal contextual feature representations for achieving more reliable prediction. Extensive experimental results and comparisons on public datasets demonstrate that the proposed model is superior to the state-of-the-art methods.
C1 [Zhu, Tong; Qian, Jiansheng] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
   [Li, Leida] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Yang, Jufeng] Nankai Univ, Sch Comp Sci & Control Engn, Tianjin 300350, Peoples R China.
   [Zhao, Sicheng] Columbia Univ, Dept Radiol, New York, NY 10027 USA.
   [Liu, Hantao] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF243AA, Wales.
C3 China University of Mining & Technology; Xidian University; Nankai
   University; Columbia University; Cardiff University
RP Li, LD (corresponding author), Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
EM zhutong@cumt.edu.cn; ldli@xidian.edu.cn; yangjufeng@nankai.edu.cn;
   schzhao@gmail.com; Hantao.Liu@cs.cardiff.ac.uk; qianjsh@cumt.edu.cn
OI Zhu, Tong/0000-0002-3082-7848
FU National Natural Science Foundation of China [62171340, 61771473,
   61991451]; Key Project of Shaanxi Provincial Department of Education
   (Collaborative Innovation Center) [20JY024]; Six Talent Peaks High-level
   Talents in Jiangsu Province [XYDXX-063]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62171340, 61771473, and 61991451, in
   part by the Key Project of Shaanxi Provincial Department of Education
   (Collaborative Innovation Center) under Grant 20JY024, and in part by
   the Six Talent Peaks High-level Talents in Jiangsu Province under Grant
   XYDXX-063. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Mohammed Daoudi.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   [Anonymous], 2015, P INT C LEARN REPR
   Arevalo J., 2017, Gated multimodal units for information fusion
   Barbosa L., 2010, Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING '10, P36, DOI DOI 10.1145/3167132.3167324
   Barrett LF, 2007, TRENDS COGN SCI, V11, P327, DOI 10.1016/j.tics.2007.06.003
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Cai GY, 2015, LECT NOTES ARTIF INT, V9362, P159, DOI 10.1007/978-3-319-25207-0_14
   Cambria E, 2013, IEEE INTELL SYST, V28, P15, DOI 10.1109/MIS.2013.30
   Castellano G, 2008, LECT NOTES COMPUT SC, V4868, P92, DOI 10.1007/978-3-540-85099-1_8
   Chen T, 2014, Arxiv, DOI arXiv:1410.8586
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Gao Y, 2016, IEEE T MULTIMEDIA, V18, P2115, DOI 10.1109/TMM.2016.2581483
   Guo WY, 2021, IEEE T MULTIMEDIA, V23, P1785, DOI 10.1109/TMM.2020.3003648
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu M., 2004, P 10 ACM SIGKDD INT, P168, DOI DOI 10.1145/1014052.1014073
   Huang FR, 2019, KNOWL-BASED SYST, V167, P26, DOI 10.1016/j.knosys.2019.01.019
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Maas Andrew, 2011, P 49 ANN M ASS COMP
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Majumder N, 2018, KNOWL-BASED SYST, V161, P124, DOI 10.1016/j.knosys.2018.07.041
   Niu T., 2016, MULTIMEDIA MODELING, P15, DOI [DOI 10.1007/978-3-319-27674-82, 10.1007/978-3-319-27674-8_2]
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79, DOI 10.3115/1118693.1118704
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Paszke Adam, 2017, NIPS W
   Poria S, 2017, IEEE DATA MINING, P1033, DOI 10.1109/ICDM.2017.134
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Truong QT, 2019, AAAI CONF ARTIF INTE, P305
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Taboada M, 2011, COMPUT LINGUIST, V37, P267, DOI 10.1162/COLI_a_00049
   Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556
   Tang Duyu, 2015, P 2015 C EMPIRICAL M, P1422
   Wang M., 2014, P INT C INT MULT COM, P76, DOI [10.1145/2632856.2632912, DOI 10.1145/2632856.2632912]
   Wang Y., 2016, P 2016 C EMPIRICAL M, P606, DOI 10.18653/v1/D16-1058
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Xu N, 2019, AAAI CONF ARTIF INTE, P371
   Xu N, 2018, ACM/SIGIR PROCEEDINGS 2018, P929, DOI 10.1145/3209978.3210093
   Xu N, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2399, DOI 10.1145/3132847.3133142
   Xu N, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS (ISI), P152, DOI 10.1109/ISI.2017.8004895
   Yang JF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3266
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Yang Z., 2016, P 2016 C N AM CHAPT, P1480, DOI [DOI 10.18653/V1/N16-1174, 10.18653/v1/n16-1174]
   You QZ, 2017, AAAI CONF ARTIF INTE, P231
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Yu YH, 2016, ALGORITHMS, V9, DOI 10.3390/a9020041
   Yuan J., 2013, P 2 INT WORKSH ISS S, P1
   Yue L, 2019, KNOWL INF SYST, V60, P617, DOI 10.1007/s10115-018-1236-4
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1025, DOI 10.1145/2647868.2655035
   Zhao ZY, 2019, INFORM PROCESS MANAG, V56, DOI 10.1016/j.ipm.2019.102097
   Zhu HC, 2020, NEURAL PROCESS LETT, V51, P2105, DOI 10.1007/s11063-019-09987-7
NR 59
TC 43
Z9 43
U1 111
U2 145
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3375
EP 3385
DI 10.1109/TMM.2022.3160060
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200033
OA Green Accepted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zuo, YK
   Yao, HT
   Zhuang, LS
   Xu, CS
AF Zuo, Yukun
   Yao, Hantao
   Zhuang, Liansheng
   Xu, Changsheng
TI Dual Structural Knowledge Interaction for Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Manifolds; Feature extraction; Adaptation models; Representation
   learning; Task analysis; Semisupervised learning; Pattern recognition;
   Domain adaptation; structural knowledge; dual structural knowledge
   interaction
ID ALIGNMENT
AB Domain adaptation aims to transfer knowledge from a label-rich source domain to an unlabeled target domain. A common strategy is to assign pseudo-labels to unlabeled target samples for performing representation learning. However, most existing methods only apply the source-guided classifier to generate the source-biased pseudo-labels for self-training, leading to biased target representations. Moreover, the generated pseudo-labels ignore the manifold assumption that neighboring samples are likely to have the same labels. To address the above problem, we formulate a novel structural knowledge to assign target-oriented and manifold-guided pseudo-labels for unlabeled target samples. The structural knowledge consists of cluster-based knowledge and locality-based knowledge. The cluster-based knowledge denotes the label consistency between the target samples and the non-parametric target cluster centers, making the pseudo-labels target-oriented. The locality-based knowledge constrains the target sample and its neighbors to satisfy the manifold assumption. As the neighbors contain the source and target samples, the source and target locality-based knowledge are utilized to boost the descriptions. With the structural knowledge, we propose a novel Dual Structural Knowledge Interaction (DSKI) framework for domain adaptation. For generating aligned and discriminative features, knowledge consistency constraint and instance mutual constraint are proposed in DSKI. Evaluations on three benchmarks demonstrate the effectiveness of the Dual Structural Knowledge Interaction, e.g., 74.9%, 87.7%, and 90.8% for Office-Home, VisDa-2017, and Office-31, respectively.
C1 [Zuo, Yukun; Zhuang, Liansheng] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Yao, Hantao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence, Beijing 100190, Peoples R China.
   [Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Institute of Automation, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence, Beijing 100190, Peoples R China.
EM zykpy@mail.ustc.edu.cn; yao@nlpr.ia.ac.cn; lszhuang@ustc.edu.cn;
   csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
OI Zuo, Yukun/0000-0002-2883-8572; Yao, Hantao/0000-0001-8125-2864; xu,
   chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China
FX No Statement Available
CR [Anonymous], 2019, P ICML
   Arazo E, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207304
   Berthelot D, 2019, ADV NEUR IN, V32
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753
   Chen CQ, 2019, PROC CVPR IEEE, P627, DOI 10.1109/CVPR.2019.00072
   Chen ST, 2020, IEEE T IMAGE PROCESS, V29, P8264, DOI 10.1109/TIP.2020.3013167
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P1042, DOI 10.1109/TMM.2021.3106095
   Courty N, 2017, ADV NEUR IN, V30
   Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Cui S., 2020, ADV NEURAL INFORM PR, V33, P7571
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   Deng ZJ, 2019, IEEE I CONF COMP VIS, P9943, DOI 10.1109/ICCV.2019.01004
   Ding N, 2022, PROC CVPR IEEE, P7202, DOI 10.1109/CVPR52688.2022.00707
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8917, DOI 10.1109/ICCV48922.2021.00881
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han T., 2020, Advances in Neural Information Processing Systems, V33, P9972, DOI DOI 10.48550/ARXIV.2010.05517
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hui Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8722, DOI 10.1109/CVPR42600.2020.00875
   Ji X, 2019, IEEE I CONF COMP VIS, P9864, DOI 10.1109/ICCV.2019.00996
   Jing LL, 2022, INT J OCCUP SAF ERGO, V28, P842, DOI [10.1080/10803548.2020.1835234, 10.1109/TPAMI.2020.2991050]
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Lee Jaeho, 2018, Advances in Neural Information Processing Systems
   Li C.-L., 2020, Advances in neural information processing systems, V33, P596
   Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050
   Li S, 2021, AAAI CONF ARTIF INTE, V35, P8455
   Liang J., 2020, INT C MACH LEARN, P6028, DOI DOI 10.48550/ARXIV.2002.08546
   Liang J, 2021, PROC CVPR IEEE, P16627, DOI 10.1109/CVPR46437.2021.01636
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Lu YW, 2022, IEEE T MULTIMEDIA, V24, P1871, DOI 10.1109/TMM.2021.3073258
   Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261
   Ma XH, 2019, PROC CVPR IEEE, P8258, DOI 10.1109/CVPR.2019.00846
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Mancini M, 2019, PROC CVPR IEEE, P6561, DOI 10.1109/CVPR.2019.00673
   Mancini M, 2021, IEEE T PATTERN ANAL, V43, P485, DOI 10.1109/TPAMI.2019.2933829
   Na J, 2021, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR46437.2021.00115
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   Peng XC, 2017, Arxiv, DOI arXiv:1710.06924
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Saito Kuniaki, 2018, P INT C LEARN REPR
   Saito Kuniaki, 2021, P IEEE CVF INT C COM, P9184
   Shen J, 2018, AAAI CONF ARTIF INTE, P4058
   Shi WW, 2018, LECT NOTES COMPUT SC, V11209, P311, DOI 10.1007/978-3-030-01228-1_19
   Shuhao Cui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12452, DOI 10.1109/CVPR42600.2020.01247
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Sun J, 2021, NEUROCOMPUTING, V454, P152, DOI 10.1016/j.neucom.2021.04.098
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tarvainen A, 2017, ADV NEUR IN, V30
   Tian Y., 2020, NeurIPS, V33, P6827
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Tzeng E, 2014, Arxiv, DOI [arXiv:1412.3474, 10.48550/arXiv.1412.3474, DOI 10.48550/ARXIV.1412.3474]
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P1, DOI 10.1007/978-3-540-71050-9
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Wu JL, 2019, IEEE INT CON MULTI, P886, DOI 10.1109/ICME.2019.00157
   Xie SA, 2018, PR MACH LEARN RES, V80
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yang SQ, 2021, ADV NEUR IN, V34
   Ying Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P464, DOI 10.1007/978-3-030-58589-1_28
   Yuan Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P540, DOI 10.1007/978-3-030-58526-6_32
   Zellinger Werner, 2017, ARXIV170208811
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang WC, 2021, IEEE T PATTERN ANAL, V43, P2047, DOI 10.1109/TPAMI.2019.2962476
   Zhihe Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9108, DOI 10.1109/CVPR42600.2020.00913
   Zhu Fei, 2021, Advances in Neural Information Processing Systems, V34, P2
   Zuo YK, 2022, IEEE T MULTIMEDIA, V24, P1020, DOI 10.1109/TMM.2021.3097495
NR 78
TC 2
Z9 2
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9057
EP 9070
DI 10.1109/TMM.2023.3245420
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200032
DA 2024-07-18
ER

PT J
AU Ahmmed, A
   Murshed, M
   Paul, M
   Taubman, D
AF Ahmmed, Ashek
   Murshed, Manzur
   Paul, Manoranjan
   Taubman, David
TI A Commonality Modeling Framework for Enhanced Video Coding Leveraging on
   the Cuboidal Partitioning Based Representation of Frames
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video coding; Encoding; Video sequences; Partitioning algorithms;
   Standards; Rate-distortion; Optimization; Video coding; commonality;
   HEVC
ID MOTION COMPENSATION
AB Video coding algorithms attempt to minimize the significant commonality that exists within a video sequence. Each new video coding standard contains tools that can perform this task more efficiently compared to its predecessors. Modern video coding systems are block-based wherein commonality modeling is carried out only from the perspective of the block that need be coded next. In this work, we argue for a commonality modeling approach that can provide a seamless blending between global and local homogeneity information. For this purpose, at first the frame that need be coded, is recursively partitioned into rectangular regions based on the homogeneity information of the entire frame. After that each obtained rectangular region's feature descriptor is taken to be the average value of all the pixels' intensities encompassing the region. In this way, the proposed approach generates a coarse representation of the current frame by minimizing both global and local commonality. This coarse frame is computationally simple and has a compact representation. It attempts to preserve important structural properties of the current frame which can be viewed subjectively as well as from improved rate-distortion performance of a reference scalable HEVC coder that employs the coarse frame as a reference frame for encoding the current frame.
C1 [Ahmmed, Ashek; Paul, Manoranjan] Charles Sturt Univ, Sch Comp & Math, Bathurst, NSW 2795, Australia.
   [Murshed, Manzur] Federat Univ, Sch Sci Engn & Informat Technol, Mt Helen, Vic 3350, Australia.
   [Taubman, David] Univ New South Wales, Sch Elect Engn & Telecommun, Sydney, NSW 2052, Australia.
C3 Charles Sturt University; Federation University Australia; University of
   New South Wales Sydney
RP Paul, M (corresponding author), Charles Sturt Univ, Sch Comp & Math, Bathurst, NSW 2795, Australia.
EM aahmmed@csu.edu.au; manzur.murshed@federation.edu.au; mpaul@csu.edu.au;
   d.taubman@unsw.edu.au
RI Paul, Manoranjan/AAD-4047-2021
OI Paul, Manoranjan/0000-0001-6870-5056; Taubman,
   David/0000-0002-8458-6402; Murshed, Manzur/0000-0001-7079-9717
FU Australian Research Council (ARC) [DP190102574]
FX Thisworkwas supported in part by Australian Research Council (ARC)
   Discovery Project Grant DP190102574.The associate editor coordinating
   the reviewof this manuscript and approving it for publication was Prof.
   Yongdong Zhang.
CR Afsana F., 2021, IEEE T CIRCUITS SYST, P1
   Ahmmed A, 2021, IEEE IMAGE PROC, P2074, DOI 10.1109/ICIP42928.2021.9506150
   Ahmmed A, 2021, IEEE IMAGE PROC, P2159, DOI 10.1109/ICIP42928.2021.9506333
   Ahmmed A, 2020, INT CONF ACOUST SPEE, P2188, DOI [10.1109/icassp40776.2020.9053851, 10.1109/ICASSP40776.2020.9053851]
   Ahmmed A, 2018, PICT COD SYMP, P238, DOI 10.1109/PCS.2018.8456251
   Ahmmed A, 2016, PICT COD SYMP
   Ahmmed A, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P55, DOI 10.1109/PCS.2015.7170046
   Ahmmed A, 2013, PICT COD SYMP, P177, DOI 10.1109/PCS.2013.6737712
   Ahmmed A, 2013, IEEE INT WORKSH MULT, P58, DOI 10.1109/MMSP.2013.6659264
   Alummed A, 2016, IEEE DATA COMPR CONF, P579, DOI 10.1109/DCC.2016.93
   [Anonymous], SHM REFERENCE SOFTWA
   [Anonymous], 2010, The H.264 Advanced Video Compression Standard
   Bjontegaard G., 2001, Document VCEG-M33
   Bossen F., 2012, JOINT COLLABORATIVE
   Boyce J., 2018, JVET-J1010: JVET common test conditions and software reference configurations
   British Broadcasting Corporation (B.B.C.), 2013, JOINT COLL TEAM VID
   CHAN MH, 1990, IEE PROC-I, V137, P205, DOI 10.1049/ip-i-2.1990.0029
   Cormen T.H., 2009, INTRO ALGORITHMS
   Kim IK, 2012, IEEE T CIRC SYST VID, V22, P1697, DOI 10.1109/TCSVT.2012.2223011
   Kim JH, 2008, IEEE IMAGE PROC, P2452, DOI 10.1109/ICIP.2008.4712289
   Ma SW, 2007, PROC SPIE, V6508, DOI 10.1117/12.707582
   Milani S, 2011, IEEE IMAGE PROC, P1649, DOI 10.1109/ICIP.2011.6115769
   Murshed M., 2018, 2018 DIGITAL IMAGE C, P1
   Murshed M, 2017, 2017 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING - TECHNIQUES AND APPLICATIONS (DICTA), P884
   Paul M, 2011, IEEE T CIRC SYST VID, V21, P1242, DOI 10.1109/TCSVT.2011.2138750
   Paul M, 2009, IEEE T MULTIMEDIA, V11, P581, DOI 10.1109/TMM.2009.2017610
   Shahriyar S, 2020, IEEE T CIRC SYST VID, V30, P835, DOI 10.1109/TCSVT.2019.2897403
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zupancic I, 2016, IEEE T MULTIMEDIA, V18, P1677, DOI 10.1109/TMM.2016.2579505
NR 30
TC 3
Z9 3
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4446
EP 4457
DI 10.1109/TMM.2021.3117397
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 6A4RT
UT WOS:000880645000006
DA 2024-07-18
ER

PT J
AU Dong, XC
   Shen, LQ
   Yu, M
   Yang, H
AF Dong, Xinchao
   Shen, Liquan
   Yu, Mei
   Yang, Hao
TI Fast Intra Mode Decision Algorithm for Versatile Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fast coding algorithm; intra mode decision; new coding techniques;
   statistical learning; versatile video coding
ID CU SIZE DECISION; PREDICTION
AB To achieve higher coding efficiency, the latest Versatile Video Coding (VVC) standard adopts a series of new intra coding techniques, including the quadtree plus multi-type tree (QTMT), intra sub-partitions (ISP) and intra block copy (IBC). However, this makes the intra codingmore complicated, asVVCneeds to traverse all prediction modes and partition types of QTMT to find the optimal combination. In this paper, we propose a fast algorithm for VVCfromtwo aspects ofmode selection and prediction terminating to reduce coding complexity. For the mode selection, adaptive mode pruning (AMP) is proposed to remove non-promisingmodes. First, since the newly introduced modes (IBC and ISP) are not effective for all blocks, learning-based classifiers are designed to remove them intelligently. Second, for normal modes, an ensemble decision strategy is proposed to sort the candidate modes and increase the probability of being the optimal mode for the first few candidates; thus, we can remove redundant candidates more efficiently. In terms of prediction terminating, we find that different optimal modes of current depth level lead to different termination probabilities of remaining intra predictions. Therefore, modedependent termination (MDT) is proposed to select an appropriate model through the optimal mode and terminate unnecessary intra predictions of remaining depth levels. The proposed algorithm is implemented on VVC test model, and simulation results show that it can achieve 51%similar to 53% time savings with only 0.93%similar to 1.08% BDBR increases.
C1 [Dong, Xinchao; Yang, Hao] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Shen, Liquan] Shanghai Univ, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commu, Shanghai 200444, Peoples R China.
   [Yu, Mei] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
C3 Shanghai University; Shanghai University; Ningbo University
RP Shen, LQ (corresponding author), Shanghai Univ, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commu, Shanghai 200444, Peoples R China.
EM 19916542520@163.com; jsslq@163.com; yumei@nbu.edu.cn;
   aidoneus@shu.edu.cn
RI ; Shen, Liquan/D-4832-2012
OI Yu, Mei/0000-0003-3583-1587; Shen, Liquan/0000-0002-2148-6279
FU National Natural Science Foundation of China [61931022, 61671282]; Open
   Fund of Key Laboratory of Advanced Display, and System Applications of
   Ministry of Education (Shanghai University); Shanghai Science, and
   Technology Innovation Plan [18010500200]; Shanghai Shuguang Program
   [17SG37]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61931022 and 61671282, in part by the
   Open Fund of Key Laboratory of Advanced Display, and System Applications
   of Ministry of Education (Shanghai University), in part by the Shanghai
   Science, and Technology Innovation Plan under Grant 18010500200, and in
   part by the Shanghai Shuguang Program under Grant 17SG37. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. SanjeevMehrotra. (Corresponding author: Liquan
   Shen.)
CR Amestoy T, 2020, IEEE T IMAGE PROCESS, V29, P1313, DOI 10.1109/TIP.2019.2938670
   Bjontegaard G, 2001, VCEGM33
   Bossen, 2019, JVETN1010
   Bossen F., 2019, JVETN0003
   Bross B., 2018, P JOINT VIDEO EXPL T
   Chen J., 2019, PROC IEEE VIS COMMUN, P1
   Correa G, 2015, IEEE T CIRC SYST VID, V25, P660, DOI 10.1109/TCSVT.2014.2363753
   De-Luxán-Hernández S, 2019, IEEE IMAGE PROC, P1203, DOI [10.1109/ICIP.2019.8803777, 10.1109/icip.2019.8803777]
   Fu T, 2019, IEEE INT CON MULTI, P55, DOI 10.1109/ICME.2019.00018
   Gao LF, 2015, IEEE INT SYMP CIRC S, P517, DOI 10.1109/ISCAS.2015.7168684
   Hu N, 2015, IEEE T CIRC SYST VID, V25, P1521, DOI 10.1109/TCSVT.2015.2395772
   Hu Q, 2016, IEEE T MULTIMEDIA, V18, P379, DOI 10.1109/TMM.2015.2512799
   Jamali M, 2019, IEEE T BROADCAST, V65, P109, DOI 10.1109/TBC.2018.2847464
   Jamali M, 2015, IEEE DATA COMPR CONF, P43, DOI 10.1109/DCC.2015.21
   Laude T, 2016, PICT COD SYMP
   Lei JJ, 2017, IEEE T BROADCAST, V63, P48, DOI 10.1109/TBC.2016.2623241
   Lei M, 2019, IEEE IMAGE PROC, P4120, DOI [10.1109/ICIP.2019.8803421, 10.1109/icip.2019.8803421]
   Li Y, 2017, IEEE T MULTIMEDIA, V19, P1431, DOI 10.1109/TMM.2017.2669863
   Min B, 2015, IEEE T CIRC SYST VID, V25, P892, DOI 10.1109/TCSVT.2014.2363739
   Moon TK, 1996, IEEE SIGNAL PROC MAG, V13, P47, DOI 10.1109/79.543975
   Park SH, 2019, IEEE ACCESS, V7, P172597, DOI 10.1109/ACCESS.2019.2956196
   Piao Y., 2013, JCTVCC207
   Roobaert D, 2006, STUD FUZZ SOFT COMP, V207, P463
   Ryu S, 2018, IEEE T IMAGE PROCESS, V27, P5525, DOI 10.1109/TIP.2018.2857404
   SAFAVIAN SR, 1991, IEEE T SYST MAN CYB, V21, P660, DOI 10.1109/21.97458
   Shen LQ, 2013, IEEE T CONSUM ELECTR, V59, P207, DOI 10.1109/TCE.2013.6490261
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tan HL, 2016, IEEE T BROADCAST, V62, P128, DOI 10.1109/TBC.2015.2505406
   Tang N, 2019, 2019 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS 2019), P361, DOI [10.1109/apccas47518.2019.8953076, 10.1109/APCCAS47518.2019.8953076]
   Tariq J, 2018, J VIS COMMUN IMAGE R, V51, P1, DOI 10.1016/j.jvcir.2017.12.008
   Tsang SH, 2019, IEEE T MULTIMEDIA, V21, P269, DOI 10.1109/TMM.2018.2856078
   Wang LL, 2013, IEEE T CIRC SYST VID, V23, P1686, DOI 10.1109/TCSVT.2013.2255398
   Wang Z, 2017, IEEE DATA COMPR CONF, P23, DOI 10.1109/DCC.2017.70
   Wei Jiang, 2012, 2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet), P1836, DOI 10.1109/CECNet.2012.6201851
   Wieckowski A, 2019, IEEE IMAGE PROC, P4130, DOI [10.1109/icip.2019.8803533, 10.1109/ICIP.2019.8803533]
   Yang H, 2020, IEEE T CIRC SYST VID, V30, P1668, DOI 10.1109/TCSVT.2019.2904198
   Zhang H, 2014, IEEE T CIRC SYST VID, V24, P660, DOI 10.1109/TCSVT.2013.2290578
   Zhang T, 2017, IEEE T CIRC SYST VID, V27, P1714, DOI 10.1109/TCSVT.2016.2556518
   Zhang YF, 2019, IEEE DATA COMPR CONF, P241, DOI [10.1109/ICAICA.2019.8873494, 10.1109/DCC.2019.00032]
   Zhao L., 2018, JVET document, JVET-J0065
   Zuo X., 2018, JVETJ0042
NR 42
TC 61
Z9 61
U1 5
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 400
EP 414
DI 10.1109/TMM.2021.3052348
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300031
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Gui, YQ
   Lu, HC
   Wu, F
   Chen, CW
AF Gui, Yongqiang
   Lu, Hancheng
   Wu, Feng
   Chen, Chang Wen
TI LensCast: Robust Wireless Video Transmission Over MmWave MIMO With Lens
   Antenna Array
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lenses; Antenna arrays; MIMO communication; Resource management;
   Millimeter wave communication; Transmitting antennas; Radio frequency;
   Lens antenna array; millimeter wave (mmwave); multiple-input
   multiple-output (MIMO); resource allocation; video transmission
ID MILLIMETER-WAVE MIMO; CHANNEL ESTIMATION; BEAMSPACE MIMO; ANALOG;
   SYSTEMS; COMMUNICATION; SELECTION; DESIGN
AB In this paper, we present LensCast, a novel cross-layer video transmission framework for wireless networks, which seamlessly integrates millimeter wave (mmWave) lens multiple-input multiple-output (MIMO) with robust video transmission. LensCast is designed to exploit the video content diversity at the application layer, together with the spatial path diversity of lens antenna array at the physical layer, to achieve graceful video transmission performance under varying channel conditions. In LensCast, a transmission distortion minimization problem is formulated with the consideration of video chunk scheduling, path matching and power allocation, which is an intractable mixed integer non-linear programming (MINLP) problem. The solution of this MINLP problem is converted into resource allocation (i.e., joint path matching and power allocation) plus chunk scheduling. First, resource allocation is investigated with given chunk scheduling results. By analyzing the optimality of the resource allocation problem, a winner-takes-all assignment is obtained to guide resource allocation. After that, a greedy water-filling algorithm is proposed as a near-optimal solution. Second, we propose a low-complexity chunk scheduling algorithm to schedule chunks for each transmission. Simulation results demonstrate that the proposed LensCast achieves an improved performance in terms of both peak signal-to-noise ratio and visual quality comparing with reference schemes.
C1 [Gui, Yongqiang; Lu, Hancheng; Wu, Feng] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Chen, Chang Wen] SUNY Buffalo, Buffalo, NY 14228 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; State University of New York (SUNY) System; State University
   of New York (SUNY) Buffalo
RP Lu, HC (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM yongqgui@mail.ustc.edu.cn; hclu@ustc.edu.cn; fengwu@ustc.edu.cn;
   chencw@buffalo.edu
RI Lu, Hancheng/AAC-6388-2019; Wu, Feng/KCY-3017-2024
OI Lu, Hancheng/0000-0001-8302-4996; Chen, Chang Wen/0000-0002-6720-234X;
   Gui, Yongqiang/0000-0002-4527-4051
FU National Science Foundation of China [61 631 017, 61 771 445, 91 538
   203]; Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Science Foundation of
   China under Grants 61 631 017, 61 771 445, and 91 538 203 and in part by
   the Fundamental Research Funds for the Central Universities.
CR Akdeniz MR, 2014, IEEE J SEL AREA COMM, V32, P1164, DOI 10.1109/JSAC.2014.2328154
   [Anonymous], 2017, 38901 TR 3GPP
   [Anonymous], 2016, 2016 IEEE Global Communications Conference (GLOBECOM)
   [Anonymous], 2019, CISCOVISUAL NETWORKI
   Björnson E, 2019, IEEE WIREL COMMUN, V26, P100, DOI 10.1109/MWC.2018.1800140
   Boyd S., 2004, CONVEX OPTIMIZATION
   Brady J, 2013, IEEE T ANTENN PROPAG, V61, P3814, DOI 10.1109/TAP.2013.2254442
   Cui H, 2016, IEEE T CIRC SYST VID, V26, P992, DOI 10.1109/TCSVT.2015.2430651
   Cui H, 2014, IEEE INFOCOM SER, P73, DOI 10.1109/INFOCOM.2014.6847926
   El Ayach O, 2014, IEEE T WIREL COMMUN, V13, P1499, DOI 10.1109/TWC.2014.011714.130846
   Fan XP, 2015, IEEE T CIRC SYST VID, V25, P1801, DOI 10.1109/TCSVT.2015.2402831
   Fan XP, 2013, IEEE T CIRC SYST VID, V23, P1040, DOI 10.1109/TCSVT.2013.2249019
   Forenza A, 2007, IEEE T VEH TECHNOL, V56, P1924, DOI 10.1109/TVT.2007.897212
   Gao XY, 2019, IEEE T SIGNAL PROCES, V67, P4809, DOI 10.1109/TSP.2019.2931202
   Ghandi MM, 2006, J VIS COMMUN IMAGE R, V17, P451, DOI 10.1016/j.jvcir.2005.05.005
   Gui YQ, 2020, IEEE T CIRC SYST VID, V30, P3181, DOI 10.1109/TCSVT.2019.2935127
   Guo RB, 2018, IEEE J-STSP, V12, P313, DOI 10.1109/JSTSP.2018.2822052
   Han SF, 2015, IEEE COMMUN MAG, V53, P186, DOI 10.1109/MCOM.2015.7010533
   He CF, 2018, IEEE T IMAGE PROCESS, V27, P3599, DOI 10.1109/TIP.2018.2818019
   He DL, 2017, IEEE T MULTIMEDIA, V19, P1894, DOI 10.1109/TMM.2017.2686703
   Huang W, 2018, IEEE T WIREL COMMUN, V17, P6575, DOI 10.1109/TWC.2018.2860963
   Jakubczak S, 2010, ACM SIGCOMM COMP COM, V40, P449, DOI 10.1145/1851275.1851257
   Jang B, 2019, IEEE COMMUN SURV TUT, V21, P508, DOI 10.1109/COMST.2018.2867935
   Liu D, 2018, IEEE T MULTIMEDIA, V20, P3097, DOI 10.1109/TMM.2018.2823903
   Liu XL, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P233
   RAMCHANDRAN K, 1993, IEEE J SEL AREA COMM, V11, P6, DOI 10.1109/49.210540
   Sayeed A., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1196, DOI 10.1109/ALLERTON.2010.5707050
   Sayeed A, 2017, P 1 ACM WORKSH MILL, P35
   Sayeed AM, 2002, IEEE T SIGNAL PROCES, V50, P2563, DOI 10.1109/TSP.2002.803324
   Shen J, 2018, IEEE T MULTIMEDIA, V20, P2788, DOI 10.1109/TMM.2018.2811622
   Skoglund M, 2006, IEEE T INFORM THEORY, V52, P3757, DOI 10.1109/TIT.2006.878212
   Sun S, 2014, IEEE COMMUN MAG, V52, P110, DOI 10.1109/MCOM.2014.6979962
   Tan B, 2017, IEEE T MULTIMEDIA, V19, P2293, DOI 10.1109/TMM.2017.2733303
   Wang BC, 2017, IEEE J SEL AREA COMM, V35, P2370, DOI 10.1109/JSAC.2017.2725878
   Wang X, 2011, IEEE T INFORM THEORY, V57, P4359, DOI 10.1109/TIT.2011.2145770
   Wei ZQ, 2019, IEEE T COMMUN, V67, P1705, DOI 10.1109/TCOMM.2018.2879930
   Wu JY, 2016, IEEE J SEL AREA COMM, V34, P2231, DOI 10.1109/JSAC.2016.2577178
   Wymeersch H, 2009, P IEEE, V97, P427, DOI 10.1109/JPROC.2008.2008853
   Xiong RQ, 2016, IEEE T IMAGE PROCESS, V25, P1820, DOI 10.1109/TIP.2016.2535288
   Yang J, 2018, IEEE T COMMUN, V66, P4767, DOI 10.1109/TCOMM.2018.2805359
   Yang L, 2018, IEEE T VEH TECHNOL, V67, P3239, DOI 10.1109/TVT.2017.2779828
   Zeng Y, 2018, IEEE T WIREL COMMUN, V17, P2800, DOI 10.1109/TWC.2018.2803180
   Zeng Y, 2016, IEEE T COMMUN, V64, P1557, DOI 10.1109/TCOMM.2016.2533490
   Zhang X, 2019, IEEE WIREL COMMUN, V26, P178, DOI 10.1109/MWC.2019.1800440
   Zhao H, 2019, IEEE WIREL COMMUN LE, V8, P1171, DOI 10.1109/LWC.2019.2910530
NR 45
TC 4
Z9 4
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 33
EP 48
DI 10.1109/TMM.2020.3045294
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300003
DA 2024-07-18
ER

PT J
AU Huang, JL
   Liao, J
   Kwong, S
AF Huang, Jialu
   Liao, Jing
   Kwong, Sam
TI Unsupervised Image-to-Image Translation via Pre-Trained StyleGAN2
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Gallium nitride; Data models; Training data; Image
   reconstruction; Task analysis; Superresolution; GAN; machine learning;
   neural network
AB Image-to-Image (I2I) translation is an emerging topic in academia, and it also has been applied in real-world industry for tasks like image synthesis, super-resolution, and colorization. Traditional I2I translation methods usually train data in two or more domains together. This requires lots of computation resources. The results are of lower quality, and contain more artifacts. The training process could be unstable when the data in different domains are not balanced, and modal collapse is more likely to happen. In this paper, we propose a new I2I translation method that generates a new model in the target domain via a series of model transformations on a pre-trained StyleGAN2 model in the source domain. After that, we develop an inversion method to achieve the conversion between an image and its latent vector. By feeding the latent vector into the generated model, we can perform I2I translation between the source domain and target domain. Both qualitative and quantitative evaluations were conducted to verify that the proposed method can achieve better performance in terms of image quality, diversity and semantic similarity to the input and reference images compared to state-of-the-art works.
C1 [Huang, Jialu; Liao, Jing; Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong Shenzhen Res Inst, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong; Shenzhen Research Institute, City
   University of Hong Kong
RP Kwong, S (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM jialhuang8-c@my.cityu.edu.hk; jingliao@cityu.edu.hk; cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261; LIAO, Jing/0000-0001-7014-5377; HUANG,
   Jialu/0000-0002-0516-3736
FU Key Project of Science and Technology Innovation 2030; Ministry of
   Science and Technology of China [2018AAA0101301]; National Natural
   Science Foundation of China [61672443]; HongKong GRF -RGC General
   Research Fund [9042816 (CityU 11209819), 9042958 (CityU 11203820)]; Hong
   Kong Research Grants Council (RGC) Early Career Scheme [9048148 (CityU
   21209119)]; CityU of Hong Kong under APRC [9610488]
FX This work was supported in part by the Key Project of Science and
   Technology Innovation 2030, in part by the Ministry of Science and
   Technology of China under Grant 2018AAA0101301, in part by the National
   Natural Science Foundation of China underGrant 61672443, in part by
   HongKong GRF -RGC General Research Fund 9042816 (CityU 11209819) and
   9042958 (CityU 11203820), in part by the Hong Kong Research Grants
   Council (RGC) Early Career Scheme under Grant Y9048148 (CityU 21209119),
   and in part by the CityU of Hong Kong under APRC under Grant 9610488.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Xiaochun Cao.
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Adler Doron., TOON YOURS
   [Anonymous], 2019, DANBOORU2018 ALARGE
   Banerjee I, 2018, COMPUT MED IMAG GRAP, V65, P167, DOI 10.1016/j.compmedimag.2017.05.002
   Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Creswell A, 2019, IEEE T NEUR NET LEAR, V30, P1967, DOI 10.1109/TNNLS.2018.2875194
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fernandes K, 2017, LECT NOTES COMPUT SC, V10255, P243, DOI 10.1007/978-3-319-58838-4_27
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Gu XL, 2021, IEEE T MULTIMEDIA, V23, P2361, DOI 10.1109/TMM.2020.3009500
   Guo YH, 2019, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2019.00494
   Han XJ, 2020, IEEE T MULTIMEDIA, V22, P1619, DOI 10.1109/TMM.2019.2945197
   Hensel M, 2017, ADV NEUR IN, V30
   Hu JL, 2017, IMAGE VISION COMPUT, V60, P48, DOI 10.1016/j.imavis.2016.08.007
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huang Z, 2016, NEUROCOMPUTING, V218, P448, DOI 10.1016/j.neucom.2016.09.018
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Karras T., 2019, P IEEECVF C COMPUTER, P8110
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Liang D, 2019, AAAI CONF ARTIF INTE, P8698
   Lipton Zachary C., 2017, P 5 INT C LEARN REPR
   Liu MY, 2017, ADV NEUR IN, V30
   Luo JY, 2017, LECT NOTES COMPUT SC, V10635, P207, DOI 10.1007/978-3-319-70096-0_22
   Ma F., 2018, Advances in Neural Information Processing Systems, P9628
   Ma L, 2019, P 7 INT C LEARN REPR
   McCloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P109, DOI [10.1016/S0079-7421(08)60536-8, DOI 10.1016/S0079-7421(08)60536-8]
   Mejjati YA, 2018, ADV NEUR IN, V31
   Mo S., 2002, ARXIV PREPRINT ARXIV, V10964, P2020
   Perarnau G., 2016, NIPS WORKSH ADV TRAI
   Pinkney Justin NM, 2020, NEUR MACH LEARN CREA
   Richardson E., 2020, ARXIV200800951
   Romero A, 2019, IEEE INT CONF COMP V, P3285, DOI 10.1109/ICCVW.2019.00410
   Royer A., 2020, Domain Adaptation for Visual Understanding, P33
   Shao SY, 2019, IEEE T IND INFORM, V15, P2446, DOI 10.1109/TII.2018.2864759
   Shen Yujun, 2021, P IEEE C COMP VIS PA
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang M, 2019, PROC CVPR IEEE, P1495, DOI 10.1109/CVPR.2019.00159
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wang Q, 2019, IEEE T IMAGE PROCESS, V28, P4376, DOI 10.1109/TIP.2019.2910667
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang YJ, 2018, AAAI CONF ARTIF INTE, P5553
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wulfmeier M., 2017, C ROBOT LEARNING, P281
   XWang Anna, 2018, P 1 ACM SIGCAS C COM, P1
   You S., 2019, ARXIV PREPRINT ARXIV
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 60
TC 29
Z9 32
U1 4
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1435
EP 1448
DI 10.1109/TMM.2021.3065230
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, H
   Fang, S
   Zhang, ZL
   Li, DTC
   Lin, K
   Wang, JZ
AF Liu, Hai
   Fang, Shuai
   Zhang, Zhaoli
   Li, Duantengchuan
   Lin, Ke
   Wang, Jiazhang
TI MFDNet: Collaborative Poses Perception and Matrix Fisher Distribution
   for Head Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Head; Pose estimation; Three-dimensional displays; Measurement; Feature
   extraction; Uncertainty; Solid modeling; Head pose estimation; Triplet
   loss; Rotation matrix; Matrix Fisher distribution; Metric learning
ID NETWORK
AB Head pose estimation suffers from several problems, including low pose tolerance under different disturbances and ambiguity arising from common head pose representation. In this study, a robust three-branch model with triplet module and matrix Fisher distribution module is proposed to address these problems. Based on metric learning, the triplet module employs triplet architecture and triplet loss. It is implemented to maximize the distance between embeddings with different pose pairs and minimize the distance between embeddings with same pose pairs. It can learn a highly discriminate and robust embedding related to head pose. Moreover, the rotation matrix instead of Euler angle and unit quaternion is utilized to represent head pose. An exponential probability density model based on the rotation matrix (referred to as the matrix Fisher distribution) is developed to model head rotation uncertainty. The matrix Fisher distribution can further analyze the head pose, and its maximum likelihood obtained using singular value decomposition provides enhanced accuracy. Extensive experiments executed over AFLW2000 and BIWI datasets demonstrate that the proposed model achieves state-of-the-art performance in comparison with traditional methods.
C1 [Liu, Hai; Zhang, Zhaoli; Li, Duantengchuan] Cent China Normal Univ, Natl Engn Res Ctr E Learning, Wuhan 430079, Peoples R China.
   [Fang, Shuai] Cent China Normal Univ, Natl Engn Lab Educ Big Data, Wuhan 430079, Peoples R China.
   [Lin, Ke] Harbin Inst Technol, Control Sci & Engn, Shenzhen 150001, Peoples R China.
   [Wang, Jiazhang] Northwestern Univ, Evanston, IL 60208 USA.
C3 Central China Normal University; Central China Normal University; Harbin
   Institute of Technology; Northwestern University
RP Fang, S (corresponding author), Cent China Normal Univ, Natl Engn Lab Educ Big Data, Wuhan 430079, Peoples R China.
EM hailiu0204@mail.ccnu.edu.cn; fibreggs@gmail.com;
   zl.zhang@mail.ccnu.edu.cn; dtclee1222@gmail.com;
   sheldon.chris.lin@gmail.com; jiazhang.wang@u.northwestern.edu
RI Zhang, Zhaoli/IWD-8445-2023; Li, Duantengchuan/IZP-8522-2023
OI Zhang, Zhaoli/0000-0002-0844-0719; Li,
   Duantengchuan/0000-0003-2902-7365; Fang, Shuai/0000-0002-1132-5391
FU National Natural Science Foundation of China [62011530436, 62077020,
   62005092, 61875068]; Fundamental Research Funds for the Central
   Universities [CCNU20ZT017, CCNU2020ZN008]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62011530436, 62077020, 62005092, and
   61875068, and the Fundamental Research Funds for the Central
   Universities under Grants CCNU20ZT017 and CCNU2020ZN008. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Sebastian Knorr
CR [Anonymous], 2003, Statistics on Special Manifolds
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong X, 2020, IEEE T PATTERN ANAL, V1, P99
   Dong XY, 2022, IEEE T PATTERN ANAL, V44, P3634, DOI 10.1109/TPAMI.2021.3054824
   Dong XY, 2018, PROC CVPR IEEE, P360, DOI 10.1109/CVPR.2018.00045
   DOWNS TD, 1972, BIOMETRIKA, V59, P665, DOI 10.2307/2334817
   Fanelli Gabriele, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P101, DOI 10.1007/978-3-642-23123-0_11
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   Gu JW, 2017, PROC CVPR IEEE, P1531, DOI 10.1109/CVPR.2017.167
   Hermans Alexander, 2017, ARXIV170307737
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Hsu HW, 2019, IEEE T MULTIMEDIA, V21, P1035, DOI 10.1109/TMM.2018.2866770
   Huang B, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.11.005
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kaya M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11091066
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   KHATRI CG, 1977, J ROY STAT SOC B MET, V39, P95
   Koestinger M., 2011, IEEE INT C COMP VIS, P2144, DOI DOI 10.1109/ICCVW.2011.6130513
   Kumar A, 2017, IEEE INT CONF AUTOMA, P258, DOI 10.1109/FG.2017.149
   Lee J, 2010, IEEE T MULTIMEDIA, V12, P552, DOI 10.1109/TMM.2010.2051874
   Lee T, 2018, IEEE T AUTOMAT CONTR, V63, P3377, DOI 10.1109/TAC.2018.2797162
   Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001
   Mardia K. V., 1999, DIRECTIONAL STAT, DOI DOI 10.1002/9780470316979
   Martin Manuel, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P641, DOI 10.1109/3DV.2014.54
   Meyer GP, 2015, IEEE I CONF COMP VIS, P3649, DOI 10.1109/ICCV.2015.416
   Mohlin David, 2020, ADV NEURAL INFORM PR, V33, P4884
   Mukherjee SS, 2015, IEEE T MULTIMEDIA, V17, P2094, DOI 10.1109/TMM.2015.2482819
   Otsuka K, 2018, IEEE T MULTIMEDIA, V20, P1432, DOI 10.1109/TMM.2017.2771396
   Park S, 2018, LECT NOTES COMPUT SC, V11217, P741, DOI 10.1007/978-3-030-01261-8_44
   Prokudin S, 2018, LECT NOTES COMPUT SC, V11213, P542, DOI 10.1007/978-3-030-01240-3_33
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Ruiz N, 2018, IEEE COMPUT SOC CONF, P2155, DOI 10.1109/CVPRW.2018.00281
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Saxena A, 2009, INT CONF ACOUST SPEE, P713, DOI 10.1109/ICASSP.2009.4959683
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Stiefelhagen R., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P2422
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang WX, 2020, P AMER CONTR CONF, P4429
   Yang TY, 2019, PROC CVPR IEEE, P1087, DOI 10.1109/CVPR.2019.00118
   Ying C., 2019, PROC INT C MACH LEAR, P7105
   Yuan H, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107316
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12789
   Zhang Y, 2020, NEUROCOMPUTING, V407, P259, DOI 10.1016/j.neucom.2020.05.010
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 52
TC 6
Z9 6
U1 5
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2449
EP 2460
DI 10.1109/TMM.2021.3081873
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600017
DA 2024-07-18
ER

PT J
AU Pang, YX
   Lin, JX
   Qin, T
   Chen, ZB
AF Pang, Yingxue
   Lin, Jianxin
   Qin, Tao
   Chen, Zhibo
TI Image-to-Image Translation: Methods and Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Data models; Generative adversarial networks;
   Measurement; Generators; PSNR; Analytical models; Image-to-image
   translation; two-domain I2I; multi-domain I2I; supervised methods;
   unsupersived methods; semi-supervised methods; few-shot methods
ID GENERATIVE ADVERSARIAL NETWORKS; SEMI; GAN
AB Image-to-image translation (I2I) aims to transfer images from a source domain to a target domain while preserving the content representations. I2I has drawn increasing attention and made tremendous progress in recent years because of its wide range of applications in many computer vision and image processing problems, such as image synthesis, segmentation, style transfer, restoration, and pose estimation. In this paper, we provide an overview of the I2I works developed in recent years. We will analyze the key techniques of the existing I2I works and clarify the main progress the community has made. Additionally, we will elaborate on the effect of I2I on the research and industry community and point out remaining challenges in related fields.
C1 [Pang, Yingxue; Chen, Zhibo] Univ Sci & Technol China, Dept Elect Engineer & Informat Sci, Hefei 230026, Anhui, Peoples R China.
   [Lin, Jianxin] Hunan Univ, Sch Comp Sci & Elect Engn, Changsha 410000, Peoples R China.
   [Qin, Tao] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Hunan University; Microsoft Research Asia; Microsoft
RP Chen, ZB (corresponding author), Univ Sci & Technol China, Dept Elect Engineer & Informat Sci, Hefei 230026, Anhui, Peoples R China.
EM pangyx@mail.ustc.edu.cn; linjx@mail.ustc.edu.cn; taoqin@microsoft.com;
   chenzhibo@ustc.edu.cn
RI Liu, Lin/IQR-4766-2023
OI Qin, Tao/0000-0002-9095-0776
FU NSFC [U1908209, 61632001, 62021001]; National Key Research and
   Development Program of China [2018AAA0101400]
FX This work was supported in part by NSFC under Grants U1908209, 61632001,
   and 62021001 and in part by the National Key Research and Development
   Program of China under Grant 2018AAA0101400.
CR Abdelhamed A, 2019, IEEE I CONF COMP VIS, P3165, DOI 10.1109/ICCV.2019.00326
   AlBahar B, 2019, IEEE I CONF COMP VIS, P9015, DOI 10.1109/ICCV.2019.00911
   Alharbi Y, 2019, PROC CVPR IEEE, P1458, DOI 10.1109/CVPR.2019.00155
   Almahairi A., 2018, PR MACH LEARN RES, V80, P195
   Amodio M, 2019, PROC CVPR IEEE, P8975, DOI 10.1109/CVPR.2019.00919
   [Anonymous], 2018, ARXIV180700734
   Arar M., 2020, P IEEE CVF C COMP VI, P13410, DOI 10.1109/CVPR42600.2020
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Armanious K, 2019, EUR SIGNAL PR CONF, DOI 10.23919/eusipco.2019.8902799
   Armanious K, 2020, COMPUT MED IMAG GRAP, V79, DOI 10.1016/j.compmedimag.2019.101684
   Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870
   Bansal A., 2017, ARXIV170805349
   Bansal A, 2018, LECT NOTES COMPUT SC, V11209, P122, DOI 10.1007/978-3-030-01228-1_8
   Benaim S., 2018, ADV NEUR IN
   Benaim S, 2017, ADV NEUR IN, V30
   Berthelot D, 2017, Arxiv, DOI arXiv:1703.10717
   Berthelot D, 2019, ADV NEUR IN, V32
   Bhattacharjee D, 2020, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR42600.2020.00484
   Binkowski Mikolaj, 2018, INT C LEARNING REPRE
   Cao J, 2018, ARXIV
   Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012
   Chang S, 2019, IEEE I CONF COMP VIS, P4802, DOI 10.1109/ICCV.2019.00490
   Chen HT, 2020, AAAI CONF ARTIF INTE, V34, P3585
   Chen R., 2020, PROC IEEECVF C COMPU, P8165, DOI DOI 10.1109/CVPR42600.2020.00819
   Chen WL, 2018, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR.2018.00981
   Chen X, 2018, PR MACH LEARN RES, V80
   Chen X, 2016, ADV NEUR IN, V29
   Chen XY, 2018, LECT NOTES COMPUT SC, V11206, P167, DOI 10.1007/978-3-030-01216-8_11
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Chen YF, 2019, ASIAPAC SIGN INFO PR, P1122, DOI [10.1109/APSIPAASC47483.2019.9023296, 10.1109/apsipaasc47483.2019.9023296]
   Chen YC, 2020, PROC CVPR IEEE, P5273, DOI 10.1109/CVPR42600.2020.00532
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Cho W, 2019, PROC CVPR IEEE, P10631, DOI 10.1109/CVPR.2019.01089
   Cho YG, 2020, INT J CONTROL AUTOM, V18, P605, DOI 10.1007/s12555-019-0689-x
   Cho Y, 2019, IFAC PAPERSONLINE, V52, P82, DOI 10.1016/j.ifacol.2019.12.287
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Chu HM, 2018, LECT NOTES COMPUT SC, V11206, P409, DOI 10.1007/978-3-030-01216-8_25
   Cohen T, 2019, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2019.00187
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   de Stoutz Etienne, 2018, P EUR C COMP VIS
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Denton E. L., 2017, ADV NEURAL INFORM PR, P4414
   Ding L, 2022, IEEE T IMAGE PROCESS, V31, P678, DOI 10.1109/TIP.2021.3134455
   Dinh L, 2015, Arxiv, DOI [arXiv:1410.8516, 10.48550/arXiv.1410.8516]
   Donahue J., 2017, ICLR, P1, DOI DOI 10.48550/ARXIV.1605.09782
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Duan B, 2021, INT C PATT RECOG, P1336, DOI 10.1109/ICPR48806.2021.9412890
   Dudhane A, 2019, IEEE COMPUT SOC CONF, P2014, DOI 10.1109/CVPRW.2019.00253
   Dumoulin V., 2017, PROC 5 INT C LEARN R, V1050
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Engelhardt S, 2018, LECT NOTES COMPUT SC, V11070, P747, DOI 10.1007/978-3-030-00928-1_84
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Fu H, 2019, PROC CVPR IEEE, P2422, DOI [10.1109/CVPR.2019.00253, 10.1109/cvpr.2019.00253]
   Gamrian S., 2019, PMLR
   Gao YM, 2020, AAAI CONF ARTIF INTE, V34, P646
   Germain M, 2015, PR MACH LEARN RES, V37, P881
   Ghosh Partha, 2019, ARXIV190312436
   Gokaslan A, 2018, LECT NOTES COMPUT SC, V11216, P662, DOI 10.1007/978-3-030-01258-8_40
   Gonzalezgarcia A., 2018, ADV NEURAL INFORM PR, P1287
   Goodfellow I., 2016, arXiv
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo X, 2020, NEUROCOMPUTING, V394, P127, DOI 10.1016/j.neucom.2019.01.115
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365
   He TY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2484
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hicsonmez S, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103886
   Higgins I., 2017, PROC 5 INT C LEARN R, P1
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hsin-Ying Lee, 2020, International Journal of Computer Vision, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Hsin-Yu Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P573, DOI 10.1007/978-3-030-58598-3_34
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Hui L, 2018, INT C PATT RECOG, P2044, DOI 10.1109/ICPR.2018.8545169
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jichao Zhang, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P1588, DOI 10.1145/3394171.3413981
   Jie Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P404, DOI 10.1007/978-3-030-58529-7_24
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaji S, 2019, RADIOL PHYS TECHNOL, V12, P235, DOI 10.1007/s12194-019-00520-y
   Katzir Oren, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P673, DOI 10.1007/978-3-030-58536-5_40
   Kazemi H, 2018, ADV NEUR IN, V31
   Kim H, 2018, PR MACH LEARN RES, V80
   Kim J., 2019, INT C LEARN REPR
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P, 2015, International Conference on Learning Representations
   Kingma D. P., 2013, STAT-US
   Kingma DP, 2014, ADV NEUR IN, V27
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lample Guillaume, 2017, P ANN C NEUR INF PRO, P5967
   Larochelle H., 2011, P 14 INT C ARTIFICIA, P29
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   LeCun Y., 2006, TUTORIAL ENERGY BASE, P1
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lee D, 2019, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2019.00259
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584
   Li B., 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P7880
   Li MJ, 2018, LECT NOTES COMPUT SC, V11213, P186, DOI 10.1007/978-3-030-01240-3_12
   Li R, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2020.107343
   Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173
   Li S, 2020, P IEEE CVF C COMP VI, P13158
   Li X, ARXIV
   Li Y., 2020, Adv. Neural Inf. Process. Syst., V33, P15885, DOI 10.48550/arXiv.2012.02780
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P5881, DOI 10.1109/TIP.2019.2922854
   Li ZY, 2021, IEEE T MULTIMEDIA, V23, P2694, DOI 10.1109/TMM.2020.3015015
   Liang J, 2021, PROC CVPR IEEE, P9387, DOI 10.1109/CVPR46437.2021.00927
   Liming Jiang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P206, DOI 10.1007/978-3-030-58580-8_13
   Lin J, 2019, ARXIV
   Lin J., 2020, EUROPEAN C COMPUTER, P18
   Lin JX, 2020, AAAI CONF ARTIF INTE, V34, P11507
   Lin JX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2980
   Lin JX, 2021, IEEE T PATTERN ANAL, V43, P1254, DOI 10.1109/TPAMI.2019.2950198
   Lin JX, 2018, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2018.00579
   Liu AH, 2018, ADV NEUR IN, V31
   Liu H., 2018, PROC EUR C COMPUT VI
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu MY, 2017, ADV NEUR IN, V30
   Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600
   Liu Y, arXiv
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI DOI 10.48550/ARXIV.1411.4038
   Ma L., 2018, INT C LEARN REPR
   Ma LQ, 2017, ADV NEUR IN, V30
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Ma S, 2018, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2018.00593
   Madam NT, 2018, LECT NOTES COMPUT SC, V11214, P358, DOI 10.1007/978-3-030-01249-6_22
   Manakov I, 2019, LECT NOTES COMPUT SC, V11795, P3, DOI 10.1007/978-3-030-33391-1_1
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mejjati YA, 2018, ADV NEUR IN, V31
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Miyato Takeru, 2018, P ICLR
   Mo S., 2018, Instagan: Instance-aware image-to-image translation
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   Mustafa Aamir, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P599, DOI 10.1007/978-3-030-58523-5_35
   Naeem M. F., 2020, INT C MACH LEARN, V119, P7176
   Nalisnick E., 2016, NIPS WORKSHOP BAYESI, V2, P131
   Navigli R, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1459352.1459355
   Ojha U., 2021, PROC IEEECVF C COMPU, p10 743
   Oussidi A, 2018, 2018 INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS AND COMPUTER VISION (ISCV2018)
   PAL SK, 1992, IEEE T NEURAL NETWOR, V3, P683, DOI 10.1109/72.159058
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Park Taesung, 2020, Advances in Neural Information Processing Systems, V33, P7198
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Pesko M, 2019, FUND INFORM, V168, P311, DOI 10.3233/FI-2019-1834
   Press O., 2018, EMERGING DISENTANGLE
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Qi GJ, 2022, IEEE T PATTERN ANAL, V44, P2045, DOI 10.1109/TPAMI.2020.3029801
   Radford A., 2015, ARXIV
   Rasmus Antti, 2015, PROC 28 INT C NEURAL, P3546
   Regmi K, 2018, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR.2018.00369
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Saito Kuniaki, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P382, DOI 10.1007/978-3-030-58580-8_23
   Salakhutdinov R., 2007, P 24 INT C MACH LEAR, P791, DOI DOI 10.1145/1273496.1273596
   Salakhutdinov R., 2009, AISTATS
   Salimans T, 2016, ADV NEUR IN, V29
   Schmarje L, 2021, IEEE ACCESS, V9, P82146, DOI 10.1109/ACCESS.2021.3084358
   Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175
   Shaham TR, 2021, PROC CVPR IEEE, P14877, DOI 10.1109/CVPR46437.2021.01464
   Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467
   Shen ZQ, 2019, PROC CVPR IEEE, P3678, DOI 10.1109/CVPR.2019.00380
   Shi M, 2011, BIOINFORMATICS, V27, P3017, DOI 10.1093/bioinformatics/btr502
   Shi YC, 2019, PROC CVPR IEEE, P6512, DOI 10.1109/CVPR.2019.00668
   Shocher A., 2020, CVPR
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Siddiquee MMR, 2019, IEEE I CONF COMP VIS, P191, DOI [10.1109/ICCV.2019.00028, 10.1109/iccv.2019.00028]
   Snell J, 2017, ADV NEUR IN, V30
   Song LX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P627, DOI 10.1145/3240508.3240612
   Song YH, 2018, LECT NOTES COMPUT SC, V11206, P3, DOI [10.1007/978-3-030-01216-8_1, 10.1109/APCAP.2017.8420330]
   Su H, PROC AAAI C ARTIF IN, V35, P2611
   Suárez PL, 2017, IEEE COMPUT SOC CONF, P212, DOI 10.1109/CVPRW.2017.32
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Taigman Y., 2017, INT C LEARN REPR ICL, P1
   Tang H., 2020, P BMVC, P12
   Tang H., 2020, P IEEE CVPR, P7870
   Tang H., 2019, P 14 IEEE INT C AUT, P1
   Tang H, 2020, Arxiv, DOI arXiv:2003.13898
   Tang H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2052, DOI 10.1145/3343031.3350980
   Tang H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P774, DOI 10.1145/3240508.3240704
   Tang H, 2019, PROC CVPR IEEE, P2412, DOI 10.1109/CVPR.2019.00252
   Tang H, 2019, IEEE IMAGE PROC, P4449, DOI [10.1109/icip.2019.8803654, 10.1109/ICIP.2019.8803654]
   Tang Hao, 2020, ACM MM, P1994
   Tao M, 2022, Arxiv, DOI arXiv:2008.05865
   Tieleman T., 2012, Technical report, V4, P26
   Tolstikhin I., 2018, 6 INT C LEARN REPR I
   Tomczak Jakub M., 2018, P MACHINE LEARNING R
   Tomei M, 2019, PROC CVPR IEEE, P5842, DOI 10.1109/CVPR.2019.00600
   Touvron H, 2020, Arxiv, DOI arXiv:2008.05763
   Tschannen M, 2018, ADV NEUR IN, V31
   Tung HYF, 2017, IEEE I CONF COMP VIS, P4364, DOI 10.1109/ICCV.2017.467
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   van den Oord A, 2016, ADV NEUR IN, V29
   van den Oord A, 2016, PR MACH LEARN RES, V48
   van der Ouderaa TFA, 2019, PROC CVPR IEEE, P4715, DOI 10.1109/CVPR.2019.00485
   Wang C, 2018, P EUR C COMP VIS ECC, P770
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang W, 2018, PROC CVPR IEEE, P7083, DOI 10.1109/CVPR.2018.00740
   Wang YX, 2020, PROC CVPR IEEE, P4452, DOI 10.1109/CVPR42600.2020.00451
   Wang YX, 2018, LECT NOTES COMPUT SC, V11210, P220, DOI 10.1007/978-3-030-01231-1_14
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu PW, 2019, IEEE I CONF COMP VIS, P5913, DOI 10.1109/ICCV.2019.00601
   Wu RZ, 2019, IEEE I CONF COMP VIS, P5922, DOI 10.1109/ICCV.2019.00602
   Wu WY, 2019, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2019.00820
   Xinrui Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8087, DOI 10.1109/CVPR42600.2020.00811
   Xu JG, 2015, IETE TECH REV, V32, P131, DOI 10.1080/02564602.2014.987328
   Yan YC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P199, DOI 10.1145/3123266.3123277
   Yang QY, 2018, Arxiv, DOI arXiv:1801.06940
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32
   Yu Xiaoming, 2019, P ADV NEUR INF PROC, P2990
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhang B, 2019, PROC CVPR IEEE, P8044, DOI 10.1109/CVPR.2019.00824
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang LZ, 2020, PROC CVPR IEEE, P5476, DOI 10.1109/CVPR42600.2020.00552
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zhang R., 2018, INT C LEARN REPR NEW
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang RX, 2018, ADV NEUR IN, V31
   Zhang YB, 2020, IEEE T IMAGE PROCESS, V29, P1101, DOI 10.1109/TIP.2019.2938347
   Zhao B, 2018, LECT NOTES COMPUT SC, V11218, P157, DOI 10.1007/978-3-030-01264-9_10
   Zhao Junbo, 2017, ICLR
   Zhao L, 2020, PROC CVPR IEEE, P5740, DOI 10.1109/CVPR42600.2020.00578
   Zhao Y., 2020, ECCV, P800
   Zheng CX, 2021, PROC CVPR IEEE, P16402, DOI 10.1109/CVPR46437.2021.01614
   Zheng RB, 2019, AAAI CONF ARTIF INTE, P825
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZQ, 2019, NEUROCOMPUTING, V355, P71, DOI 10.1016/j.neucom.2019.04.032
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhongyou Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9360, DOI 10.1109/CVPR42600.2020.00938
   Zhou X., 2021, CVPR, P11465
   Zhu HY, 2019, AAAI CONF ARTIF INTE, P9332
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 256
TC 88
Z9 91
U1 23
U2 55
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3859
EP 3881
DI 10.1109/TMM.2021.3109419
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400015
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Phaphuangwittayakul, A
   Guo, Y
   Ying, FL
AF Phaphuangwittayakul, Aniwat
   Guo, Yi
   Ying, Fangli
TI Fast Adaptive Meta-Learning for Few-Shot Image Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Training; Image synthesis; Adaptation models; Generative
   adversarial networks; Generators; Data models; Meta-learning; few-shot
   image generation; generative adversarial network; unsupervised learning
ID ADVERSARIAL; NETWORK
AB Generative Adversarial Networks (GANs) are capable of effectively synthesising new realistic images and estimating the potential distribution of samples utilising adversarial learning. Nevertheless, conventional GANs require a large amount of training data samples to produce plausible results. Inspired by the capacity for humans to quickly learn new concepts from a small number of examples, several meta-learning approaches for the few-shot datasets are presented. However, most of meta-learning algorithms are designed to tackle few-shot classification and reinforcement learning tasks. Moreover, the existing meta-learning models for image generation are complex, thereby affecting the length of training time required. Fast Adaptive Meta-Learning (FAML) based on GAN and the encoder network is proposed in this study for few-shot image generation. This model demonstrates the capability to generate new realistic images from previously unseen target classes with only a small number of examples required. With 10 times faster convergence, FAML requires only one-fourth of the trainable parameters in comparison baseline models by training a simpler network with conditional feature vectors from the encoder, while increasing the number of generator iterations. The visualisation results are demonstrated in the paper. This model is able to improve few-shot image generation with the lowest FID score, highest IS, and comparable LPIPS to MNIST, Omniglot, VGG-Faces, and miniImageNet datasets. The source code is available on https://github.com/phaphuang/FAML.
C1 [Phaphuangwittayakul, Aniwat; Guo, Yi; Ying, Fangli] East China Univ Sci & Technol, Dept Comp Sci & Engn, Shanghai 200237, Peoples R China.
   [Guo, Yi] Natl Engn Lab Big Data Distribut & Exchange Techn, Business Intelligence & Visualisat Res Ctr, Shanghai 200436, Peoples R China.
   [Guo, Yi] Shanghai Engn Res Ctr Big Data & Internet Audienc, Shanghai 200272, Peoples R China.
   [Ying, Fangli] East China Univ Sci & Technol, State Key Lab Bioreactor Engn, Shanghai 200237, Peoples R China.
C3 East China University of Science & Technology; East China University of
   Science & Technology
RP Guo, Y (corresponding author), East China Univ Sci & Technol, Dept Comp Sci & Engn, Shanghai 200237, Peoples R China.; Guo, Y (corresponding author), Natl Engn Lab Big Data Distribut & Exchange Techn, Business Intelligence & Visualisat Res Ctr, Shanghai 200436, Peoples R China.; Guo, Y (corresponding author), Shanghai Engn Res Ctr Big Data & Internet Audienc, Shanghai 200272, Peoples R China.
EM aniwat.pha@gmail.com; guoyi@ecust.edu.cn; yfangli@ecust.edu.cn
RI Ying, Fangli/ABE-3342-2021; Phaphuangwittayakul, Aniwat/ABV-7858-2022
OI Ying, Fangli/0000-0001-8390-3229; Phaphuangwittayakul,
   Aniwat/0000-0002-2289-3116
FU National Key Research and Development Program of China [2018YFC0807105];
   Science and Technology Committee of Shanghai Municipality (STCSM)
   [17DZ1101003, 18511106602, 18DZ2252300]; State Key Laboratory of
   Bioreactor Engineering, East China University of Science and Technology,
   Shanghai, China; International College of Digital Innovation (ICDI),
   Chiang Mai University, Thailand
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFC0807105, in part by the
   Science and Technology Committee of Shanghai Municipality (STCSM) under
   Grants 17DZ1101003, 18511106602, and 18DZ2252300, in part by the Open
   Funding Project of the State Key Laboratory of Bioreactor Engineering,
   East China University of Science and Technology, Shanghai, China, and in
   part by the International College of Digital Innovation (ICDI), Chiang
   Mai University, Thailand. (Aniwat Phaphuangwittayakul and Fangli Ying
   contributed equally to this work.)
CR Antoniou A., 2018, INT C LEARN REPR, P1
   Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Clouatre L., 2019, ABS190102199 CORR
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Durall R, 2019, INT CONF IMAG VIS, DOI 10.1109/ivcnz48456.2019.8961034
   Finn C, 2017, PR MACH LEARN RES, V70
   Gao Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356574
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   Hensel M, 2017, ADV NEUR IN, V30
   Hsu K., 2019, P INT C LEARN REPR, P1
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Jamal M. A., 2018, ABS180507722 CORR, V1805
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Jolicoeur-Martineau A., 2019, P INT C LEARN REPR
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D. P., 2014, arXiv
   KOCH G., ICML DEEP LEARN WORK, V2
   Lake B., 2011, P ANN M COGNITIVE SC, P1
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lemke C, 2015, ARTIF INTELL REV, V44, P117, DOI 10.1007/s10462-013-9406-y
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li Z., 2017, ABS170709835 CORR
   Lin XM, 2019, MULTIMED TOOLS APPL, V78, P783, DOI 10.1007/s11042-017-5457-4
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lucic Mario, 2019, P INT C MACH LEARN
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Miyato T., 2018, INT C LEARN REPR FFE
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Qi GJ, 2020, INT J COMPUT VISION, V128, P1118, DOI 10.1007/s11263-019-01265-2
   Qi GJ, 2018, PROC CVPR IEEE, P1517, DOI 10.1109/CVPR.2018.00164
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Radford A., 2016, INT C LEARN REPR NOV
   Ronneberger O., INTERNATIONAL, P234
   Saatci Y., 2017, P ANN C NEUR INF PRO, P3622
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Salimans T, 2016, ADV NEUR IN, V29
   Santoro A, 2016, PR MACH LEARN RES, V48
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Snell J, 2017, ADV NEUR IN, V30
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Viazovetskyi Yuri, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P170, DOI 10.1007/978-3-030-58542-6_11
   Vinyals O., 2016, P ADV NEUR INF PROC, V29, P3637
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Wang ZW, 2018, PROC CVPR IEEE, P7939, DOI 10.1109/CVPR.2018.00828
   Wu Y, 2020, J VIS COMMUN IMAGE R, V67, DOI 10.1016/j.jvcir.2020.102761
   Xia XJ, 2019, IEEE T MULTIMEDIA, V21, P1359, DOI 10.1109/TMM.2018.2879750
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang RX, 2018, ADV NEUR IN, V31
   Zhao YR, 2018, LECT NOTES COMPUT SC, V11213, P508, DOI 10.1007/978-3-030-01240-3_31
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu Jun-Yan, 2018, Advances in Neural Information Processing Systems
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
   Zou YX, 2020, IEEE T MULTIMEDIA, V22, P3166, DOI 10.1109/TMM.2020.2972128
NR 63
TC 14
Z9 14
U1 11
U2 82
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2205
EP 2217
DI 10.1109/TMM.2021.3077729
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200033
DA 2024-07-18
ER

PT J
AU Song, YQ
   Chen, SZ
   Jin, Q
   Luo, W
   Xie, J
   Huang, F
AF Song, Yuqing
   Chen, Shizhe
   Jin, Qin
   Luo, Wei
   Xie, Jun
   Huang, Fei
TI Enhancing Neural Machine Translation With Dual-Side Multimodal Awareness
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Machine translation; Task analysis; Training; Decoding;
   Semantics; Degradation; Multimodal machine translation; graph;
   relationship; multimodal rewards; reinforcement learning
AB Multimodal machine translation (MMT) aims to translate a sentence in the source language into the target language with the context of an associated image. According to where the visual information is employed, previous approaches can be categorized into two types: directly injecting the visual information at the input side or exploiting it as a visual constraint at the objective side. In this work, we propose an IO-MMT model which exploits the visual assistance in dual sides to fully exploit the visual information for multimodal machine translation. It contains a relation-aware multimodal transformer to simultaneously exploit the objects and their spatial relationships in the image with a graph at the input-side and a novel visual assistance structure to further improve visual consistency of the translation at the objective-side. Experimental results under both normal setting and input degradation settings on the Multi30k benchmark dataset show that combining the visual assistance in dual sides consistently outperforms single-side MMT models and achieves the state-of-the-art results on EN-DE and EN-FR translation tasks. We will release the codes and models at https://github.com/syuqings/MMT.
C1 [Song, Yuqing; Jin, Qin] Renmin Univ China, Beijing 100872, Peoples R China.
   [Chen, Shizhe] INRIA, F-75012 Paris, France.
   [Luo, Wei; Xie, Jun; Huang, Fei] Alibaba DAMO Acad, Beijing 100872, Peoples R China.
C3 Renmin University of China; Inria
RP Jin, Q (corresponding author), Renmin Univ China, Beijing 100872, Peoples R China.
EM syuqing@ruc.edu.cn; cshizhe@gmail.com; qjin@ruc.edu.cn;
   muzhuo.lw@alibaba-inc.com; qingjing.xj@alibaba-inc.com;
   f.huang@alibaba-inc.com
OI Chen, Shizhe/0000-0002-7313-9703; Song, Yuqing/0000-0002-0246-4330;
   Huang, Fei/0000-0002-3709-5053; Jin, Qin/0000-0001-6486-6020
FU National Natural Science Foundation of China [61772535, 62072462];
   BeijingNatural Science Foundation [4192028]; National Key RAMP;D Program
   of China [2020AAA0108600]
FX This work was supported in part by National Natural Science Foundation
   of China underGrants 61772535& 62072462, in part by BeijingNatural
   Science Foundation under Grant 4192028, and part by the National Key R&D
   Program of China under Grant 2020AAA0108600.
CR [Anonymous], P BMVC2018 C
   Biten A. F., 2019, PROC IEEE C COMPUT V, p12 466
   Bojar Ond.rej, 2017, P SEC C MACH TRANSL, P489, DOI DOI 10.18653/V1/W17-4755
   Bugliarello E, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P1618
   Caglayan O, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4159
   Caglayan Ozan, 2016, ARXIV160903976
   Calixto I., 2017, P 6 WORKSHOP VISION, P31, DOI 10.18653/v1/W17-2004
   Calixto I., 2017, P 2017 C EMP METH NA, P992, DOI 10.18653/v1/D17-1105
   Calixto I, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6392
   Calixto I, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1913, DOI 10.18653/v1/P17-1175
   Che WB, 2020, IEEE T MULTIMEDIA, V22, P2307, DOI 10.1109/TMM.2019.2954750
   Chen SZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4932
   Chen SZ, 2019, AAAI CONF ARTIF INTE, P8207
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Clark J. H., 2011, P 49 ANN M ASS COMP, P176
   Delbrouck J.-B., 2017, P 2017 C EMP METH NA, P910
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Desmond Elliott, 2017, P 8 INT JOINT C NATU, P130
   Duvenaudt D, 2015, ADV NEUR IN, V28
   Elliott D., 2016, P 5 WORKSH VIS LANG, P70, DOI [10.18653/v1/w16-3210, DOI 10.18653/V1/W16-3210]
   Elliott Desmond, 2017, P 2 C MACH TRANSL CO, P215, DOI DOI 10.18653/V1/W17-4718
   Feng YS, 2013, IEEE T PATTERN ANAL, V35, P797, DOI 10.1109/TPAMI.2012.118
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huan Lin, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P1320, DOI 10.1145/3394171.3413715
   Huang P.-Y., 2020, P ACL
   Ive J, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6525
   Kingma D.P., 2014, ARXIV14126980
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Libovicky J, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P196, DOI 10.18653/v1/P17-2031
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Matusov E., 2017, P 15 C EUROPEAN CHAP, P637, DOI DOI 10.18653/V1/E17-2101
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Popovic Maja, 2015, P 10 WORKSHOP STAT M, P392
   Ranzato M, 2016, 4 INT C LEARN REPR I
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715
   Sigurdsson G. A., 2020, PROC IEEE C COMPUT V, p10 847
   Snover Matthew., 2006, P 7 C ASS MACHINE TR, P223
   Su Y., 2019, PROC IEEE C COMPUT V, p10 482
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Toutanova K, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P252, DOI 10.3115/1073445.1073478
   Vaswani A, 2017, ADV NEUR IN, V30
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang PC, 2020, AAAI CONF ARTIF INTE, V34, P9418
   Yang X., 2020, COMPUTER VISION ECCV, P1
   Yao SW, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4346
   Yin YJ, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3025
   Zhang MS, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1151
   Zhang Z., 2020, INT C LEARN REPR
   Zhou M., 2018, Empirical Methods in Natural Language Processing, P3643, DOI 10.18653/v1/D18-1400
NR 54
TC 2
Z9 2
U1 4
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3013
EP 3024
DI 10.1109/TMM.2021.3092187
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000026
DA 2024-07-18
ER

PT J
AU Ye, F
   Huang, CQ
   Cao, JK
   Li, MS
   Zhang, Y
   Lu, CW
AF Ye, Fei
   Huang, Chaoqin
   Cao, Jinkun
   Li, Maosen
   Zhang, Ya
   Lu, Cewu
TI Attribute Restoration Framework for Anomaly Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image restoration; Anomaly detection; Feature extraction; Semantics;
   Task analysis; Training; Image reconstruction; Anomaly detection;
   attribute restoration framework; semantic feature embedding
AB With the recent advances in deep neural networks, anomaly detection in multimedia has received much attention in the computer vision community. While reconstruction-based methods have recently shown great promise for anomaly detection, the information equivalence among input and supervision for reconstruction tasks can not effectively force the network to learn semantic feature embeddings. We here propose to break this equivalence by erasing selected attributes from the original data and reformulate it as a restoration task, where the normal and the anomalous data are expected to be distinguishable based on restoration errors. Through forcing the network to restore the original image, the semantic feature embeddings related to the erased attributes are learned by the network. During testing phases, because anomalous data are restored with the attribute learned from the normal data, the restoration error is expected to be large. Extensive experiments have demonstrated that the proposed method significantly outperforms several state-of-the-arts on multiple benchmark datasets, especially on ImageNet, increasing the AUROC of the top-performing baseline by 10.1%. We also evaluate our method on a real-world anomaly detection dataset MVTec AD.
C1 [Ye, Fei; Huang, Chaoqin; Li, Maosen; Zhang, Ya] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
   [Cao, Jinkun] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.
   [Lu, Cewu] Shanghai Jiao Tong Univ, Qing Yuan Res Inst, Shanghai, Peoples R China.
   [Lu, Cewu] Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, AI Inst, Shanghai, Peoples R China.
   [Lu, Cewu] Shanghai Qi Zhi Inst, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai
   Jiao Tong University; Shanghai Jiao Tong University
RP Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
EM yf3310@sjtu.edu.cn; huangchaoqin@sjtu.edu.cn; caojinkun@sjtu.edu.cn;
   maosen_li@sjtu.edu.cn; ya_zhang@sjtu.edu.cn; lucewu@sjtu.edu.cn
RI Zhang, Ya/Y-8255-2019; Huang, Chaoqin/JCF-0974-2023; Huang,
   Chaoqin/HNI-0155-2023
OI Zhang, Ya/0000-0002-5390-9053; Huang, Chaoqin/0000-0001-6314-4472; 
FU National Key Research and Development Program of China [2020YFB1406801];
   111 plan [BP0719010]; STCSM [18DZ2270700]; State Key Laboratory of UHD
   Video, and Audio Production, and Presentation
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB1406801, in part by the
   111 plan (BP0719010) and STCSM (18DZ2270700), and in part by the State
   Key Laboratory of UHD Video, and Audio Production, and Presentation. The
   associate editor coordinating the review of this manuscript and
   approving it for publicationwas Prof. Ferdous Sohel. (Fei Ye and
   ChaoqinHuang are contributed equally to this work.)
CR Akçay S, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8851808
   Akcay S, 2019, LECT NOTES COMPUT SC, V11363, P622, DOI 10.1007/978-3-030-20893-6_39
   [Anonymous], 2015, SPEC LECT IE
   Bergmann P, 2020, PROC CVPR IEEE, P4182, DOI 10.1109/CVPR42600.2020.00424
   Bergmann P, 2019, PROC CVPR IEEE, P9584, DOI 10.1109/CVPR.2019.00982
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cao V, 2016, LECT NOTES COMPUT SC, V9921, P717, DOI 10.1007/978-3-319-45823-6_67
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Deecke L., 2018, JOINT EUR C MACH LEA
   Denton E, 2016, ARXIV161106430
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Eskin E., 2000, Proc. 17th International Conf. on Machine Learning, P255, DOI DOI 10.1109/ICCSA.2008.70
   Gidaris S., 2019, P INT C LEARN REPR
   Golan I, 2018, 32 C NEURAL INFORM P, V31
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks Dan, 2019, ARXIV190312261
   Huang GL, 2017, IEEE ICC
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jenni S, 2018, PROC CVPR IEEE, P2733, DOI 10.1109/CVPR.2018.00289
   Kingma D. P., 2013, ARXIV13126114
   Kiran BR, 2018, J IMAGING, V4, DOI 10.3390/jimaging4020036
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Lee KH, 2018, ECO-EFFIC IND SCI, V33, P1, DOI 10.1007/978-3-319-70899-7_1
   Long Y, 2017, IEEE WINT CONF APPL, P944, DOI 10.1109/WACV.2017.110
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Perera P, 2019, PROC CVPR IEEE, P2893, DOI 10.1109/CVPR.2019.00301
   Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882
   Rahmani M, 2017, IEEE T SIGNAL PROCES, V65, P6260, DOI 10.1109/TSP.2017.2749215
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruff L, 2018, PR MACH LEARN RES, V80
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sabokrou M, 2016, ELECTRON LETT, V52, P1122, DOI 10.1049/el.2016.0440
   Sabokrou M, 2021, IEEE T NEUR NET LEAR, V32, P675, DOI 10.1109/TNNLS.2020.2979049
   Sabokrou M, 2019, LECT NOTES COMPUT SC, V11366, P488, DOI 10.1007/978-3-030-20876-9_31
   Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356
   Sabokrou M, 2018, COMPUT VIS IMAGE UND, V172, P88, DOI 10.1016/j.cviu.2018.02.006
   Sabokrou M, 2017, IEEE T IMAGE PROCESS, V26, P1992, DOI 10.1109/TIP.2017.2670780
   Sakurada M., 2014, P MLSDA 2014 2 WORKS, P4, DOI DOI 10.1145/2689746.2689747
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Schölkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang S., 2019, P ADV NERUAL INF PRO, P5960
   Wang XH, 2020, KNOWL-BASED SYST, V190, DOI 10.1016/j.knosys.2019.105187
   Xia Y, 2015, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2015.177
   Xiao H., 2017, ARXIV170807747
   Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156
   Xu K, 2020, IEEE T MULTIMEDIA, V22, P394, DOI 10.1109/TMM.2019.2929931
   Xu K, 2018, IEEE T MULTIMEDIA, V20, P1062, DOI 10.1109/TMM.2018.2818942
   Yamanishi K., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P320, DOI 10.1145/347090.347160
   Zenati H, 2018, ARXIV
   Zhai SF, 2016, PR MACH LEARN RES, V48
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zong B., 2018, P 6 INT C LEARN REPR
NR 61
TC 82
Z9 83
U1 10
U2 72
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 116
EP 127
DI 10.1109/TMM.2020.3046884
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300009
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zhang, ZY
   Li, MY
   Xie, HN
   Yu, J
   Liu, TL
   Chen, CW
AF Zhang, Zhaoyu
   Li, Mengyan
   Xie, Haonian
   Yu, Jun
   Liu, Tongliang
   Chen, Chang Wen
TI TWGAN: Twin Discriminator Generative Adversarial Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE GAN; non-saturating loss; saturating loss; TWGAN; training instability
AB Generative Adversarial Networks (GAN) has become more and more popular these years. However, it is difficult to train and suffers from the training instability problem. To tackle this difficulty, this paper proposes a novel approach. Our idea is intuitive but proven to be very useful. In essence, it combines saturating loss and non-saturating loss into the loss function. Thus it will exploit the complementary statistical properties from two kinds of loss functions to effectively improve the training stability. We term our method twin discriminator Generative Adversarial Networks (TWGAN), which, unlike GAN, has a generator and a twin discriminator. The twin discriminator consists of two discriminators with identical architecture and both of them aim to distinguish whether the samples are from real data or fake data. We develop theoretical analysis to show that, given the optimal discriminators, optimizing the generator of TWGAN reduces to minimizing the Kullback-Leibler (KL) divergence between the distribution of generated data (P-g) and the distribution of real data (P(d)ata), hence effectively addressing the training instability problem. Extensive experiments on MNIST, Fashion MNIST, CIFAR-10/100 and STL-10 datasets demonstrate that the competitive performance of our TWGAN in generating good quality and diverse samples over baselines. The obtained highest inception score (IS) and lowest Fr e chet Inception Distance (FID), compared with other state-of-the-art GANs, show the superiority of our TWGAN.
C1 [Zhang, Zhaoyu; Li, Mengyan; Xie, Haonian; Yu, Jun] Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
   [Liu, Tongliang] Univ Sydney, Trustworthy Machine Learning Lab, Darlington, NSW 2008, Australia.
   [Chen, Chang Wen] SUNY Buffalo, Comp Sci & Engn, Buffalo, NY 14260 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Sydney; State University of New York (SUNY)
   System; State University of New York (SUNY) Buffalo
RP Yu, J (corresponding author), Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
EM zzy95@mail.ustc.edu.cn; limmy@mail.ustc.edu.cn; xie233@mail.ustc.edu.cn;
   harryjun@ustc.edu.cn; tliang.liu@sydney.edu.au; chencw@buffalo.edu
RI Liu, Tongliang/AAA-1506-2021
OI Liu, Tongliang/0000-0002-9640-6472; Zhang, Zhaoyu/0000-0003-4303-2806;
   Chen, Chang Wen/0000-0002-6720-234X
FU National Natural Science Foundation of China [U1736123, 61572450]; USTC
   Research Funds of the Double First-Class Initiative [YD2350002001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1736123 and 61572450 and in part USTC
   Research Funds of the Double First-Class Initiative under Grant
   YD2350002001.
CR [Anonymous], 2019, ARXIV190202934
   [Anonymous], 2016, P ADV NEUR INF PROC
   Arjovsky, 2017, ARXIV170104862
   Arjovsky M., 2017, ARXIV170107875
   Bellemare M. G., 2017, arXiv
   Berthelot David, 2017, CoRR
   Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877
   Ceulemans B, 2018, IEEE T MULTIMEDIA, V20, P2235, DOI 10.1109/TMM.2018.2802646
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Cui RP, 2020, IEEE T MULTIMEDIA, V22, P2551, DOI 10.1109/TMM.2019.2960700
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jenni S, 2019, PROC CVPR IEEE, P12137, DOI 10.1109/CVPR.2019.01242
   Jolicoeur-Martineau A., 2018, The relativistic discriminator: A key element missing from standard GAN
   Karras T, 2018, P INT C LEARN REPR I
   Kingma D. P., 2014, arXiv
   Krizhevsky A., 2014, The cifar-10 dataset
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Miyato T, 2018, INT C LEARN REPR
   Tran NT, 2018, LECT NOTES COMPUT SC, V11218, P387, DOI 10.1007/978-3-030-01264-9_23
   NGUYEN T., 2017, ADV NEURAL INF PROCE, V30, P2667
   Odena A, 2017, PR MACH LEARN RES, V70
   Park SW, 2019, PROC CVPR IEEE, P4287, DOI 10.1109/CVPR.2019.00442
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Radford A., 2015, ARXIV
   Sabour S, 2017, ADV NEUR IN, V30
   Salimans T, 2016, ADV NEUR IN, V29
   Salimans Tim, 2018, P INT C LEARN REPR
   Siarohin A., 2019, P INT C LEARN REPR
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   Toni L, 2016, IEEE T MULTIMEDIA, V18, P852, DOI 10.1109/TMM.2016.2537207
   van den Oord A, 2016, PR MACH LEARN RES, V48
   Wang Dilin., 2016, Learning to draw samples: With application to amortized MLE for generative adversarial learning
   Warde-Farley D., 2017, International Conference on Learning Representations (ICLR)
   Wu JQ, 2019, PROC CVPR IEEE, P3708, DOI 10.1109/CVPR.2019.00383
   Xiao H., 2017, arXiv
   Yeh CH, 2015, IEEE T MULTIMEDIA, V17, P1508, DOI 10.1109/TMM.2015.2449659
   Zhang ZY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P674, DOI 10.1145/3343031.3351026
   Zhou Zhiming, 2018, P INT C LEARN REPR
NR 45
TC 5
Z9 5
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 677
EP 688
DI 10.1109/TMM.2021.3057989
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100013
DA 2024-07-18
ER

PT J
AU Li, PG
   Sun, X
   Yu, HF
   Tian, Y
   Yao, FL
   Xu, GL
AF Li, Peiguang
   Sun, Xian
   Yu, Hongfeng
   Tian, Yu
   Yao, Fanglong
   Xu, Guangluan
TI Entity-Oriented Multi-Modal Alignment and Fusion Network for Fake News
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Task analysis; Heuristic algorithms;
   Social networking (online); Semantics; Sun; Capsule network; cross-modal
   interaction; fake news detection; multi-modal
AB The development of social media enables fake news to be expressed in a multi-modal form, which is disseminated on various social platforms and brings harmful social impacts. To handle this challenge, the fake news detection task was proposed to examine whether false information is contained in multi-modal news. Existing methods exploit various approaches with cross-modal interaction and fusion, which have proven to be effective in detecting common fake news. However, although the description of multi-modal news is narrated around entities, the previously developed methods pay less attention to this characteristic. They do not explore its benefits to the detection task and underperform with respect to the detection of fake news that requires entity-centric comparisons. To make up for this omission, we explore a novel paradigm to detect fake news by aligning and fusing multi-modal entities and propose the Entity-oriented Multi-modal Alignment and Fusion network (EMAF). Our work adopts entity-centric cross-modal interaction, which can reserve semantic integrity and capture the details of multi-modal entities. Specifically, we design an Alignment module with the improved dynamic routing algorithm and introduce a Fusion module based on the comparison, the former aligns and captures the important entities and the latter compares and aggregates entity-centric features. Comparative experiments conducted on multiple public datasets, including Weibo, Twitter, and Reddit, reveal the superiority of the proposed EMAF method, and extensive analytical experiments demonstrate the effectiveness of our proposed modules.
C1 [Li, Peiguang; Sun, Xian; Yu, Hongfeng; Tian, Yu; Yao, Fanglong; Xu, Guangluan] Chinese Acad Sci, Aerosp Informat Res Inst, Beijing 100190, Peoples R China.
   [Li, Peiguang; Sun, Xian; Tian, Yu; Yao, Fanglong] Chinese Acad Sci, Univ Chinese Acad Sci, Aerosp Informat Res Inst, Key Lab Network Informat Syst Technol NIST, Beijing 100190, Peoples R China.
   [Li, Peiguang; Sun, Xian; Tian, Yu; Yao, Fanglong] Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing 100190, Peoples R China.
   [Yu, Hongfeng; Xu, Guangluan] Chinese Acad Sci, Aerosp Informat Res Inst, Key Lab Network Informat Syst Technol NIST, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Aerospace Information Research Institute,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Aerospace Information Research Institute, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences; Aerospace Information Research Institute,
   CAS
RP Sun, X (corresponding author), Chinese Acad Sci, Aerosp Informat Res Inst, Beijing 100190, Peoples R China.
EM lipeiguang17@mails.ucas.ac.cn; sunxian@aircas.ac.cn; hfyu@mail.ie.ac.cn;
   tianyu181@mails.ucas.edu.cn; yaofanglong17@mails.ucas.ac.cn;
   gluaww@rnailie.ac.cn
OI Li, Peiguang/0000-0002-4838-7828; Tian, Yu/0000-0002-8235-6507; Yao,
   Fanglong/0000-0003-4187-9755; Sun, Xian/0000-0002-0038-9816
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Augenstein I, 2019, PROC EMNLPIJCNLP, P4677
   Bahdanau D., 2014, 3 INT C LEARN REPR
   Boididou C., 2015, WORKING NOTES PROC M, V1436
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Castillo C., 2011, P 20 INT C WORLD WID, P675, DOI DOI 10.1145/1963405.1963500
   Che W., 2020, N LTP OPEN SOURCE NE, P11616
   Chen Q, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1657, DOI 10.18653/v1/P17-1152
   Chen T, 2018, LECT NOTES ARTIF INT, V11154, P40, DOI 10.1007/978-3-030-04503-6_4
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Devlin J, 2018, N AM ASS COMP LING
   Hinton G.E., 2018, INT C LEARN REPR
   Islam MN, 2018, COGENT FOOD AGR, V4, DOI 10.1080/23311932.2018.1477232
   Jin ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P795, DOI 10.1145/3123266.3123454
   Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078
   Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552
   Kingma D. P, 2015, International Conference on Learning Representations
   Kingma D. P., 2014, AUTOENCODING VARIATI, P3581
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kumar S, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P591, DOI 10.1145/2872427.2883085
   Lee N, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1133
   Ma J., 2016, P 25 INT JOINT C ART, P3818
   Ma J, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2561
   Nakamura K, 2020, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2020), P6149
   Nie YX, 2019, AAAI CONF ARTIF INTE, P6859
   Parikh AP., 2016, EMNLP
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sabour S, 2017, ADV NEUR IN, V30
   Santosh TYSS, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P2218
   Simonyan K., 2014, CORR
   Singhal S, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P39, DOI [10.1109/BigMM.2019.00-44, 10.1109/BigMM.2019.00018]
   Tacchini Eugenio, 2017, ARXIV170407506, P1, DOI [10.48550/arXiv.1704.07506, DOI 10.1257/JEP.31.2.211]
   Thorne James, 2018, P 2018 C N AM CHAPT, P809, DOI DOI 10.18653/V1/N18-1074
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang YQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P849, DOI 10.1145/3219819.3219903
   Zhong Wanjun, 2020, P 58 ANN M ASS COMPU, P6170
   Zhou J, 2020, AI OPEN, V1, P57, DOI 10.1016/j.aiopen.2021.01.001
   Zhou J, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P892
   Zhou XY, 2020, LECT NOTES ARTIF INT, V12085, P354, DOI 10.1007/978-3-030-47436-2_27
NR 40
TC 30
Z9 31
U1 9
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 26
PY 2021
VL 24
BP 3455
EP 3468
DI 10.1109/TMM.2021.3098988
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NF
UT WOS:000824706400002
DA 2024-07-18
ER

PT J
AU Zhou, DW
   Wang, NN
   Peng, CL
   Yu, Y
   Yang, X
   Gao, XB
AF Zhou, Dawei
   Wang, Nannan
   Peng, Chunlei
   Yu, Yi
   Yang, Xi
   Gao, Xinbo
TI Towards Multi-Domain Face Synthesis Via Domain-Invariant Representations
   and Multi-Level Feature Parts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Feature extraction; Data mining; Training; Semantics;
   Faces; Image reconstruction; Domain-invariant representations; Face
   synthesis; multi-domain; multi-level features
ID CORTEX
AB Cross-domain face synthesis plays a positive role in the real world. It is challenging to synthesize high-quality faces across multiple domains based on limited paired data because the multiple mappings between different domains may interfere with each other. Cognitive science investigates that the brain can recognize the same person with multiple different expressions by extracting invariant information on the face and we humans perceive instances by decomposing them into parts. Motivated by these cognition, we propose a unified semi-supervised framework for multi-domain face synthesis by extracting a domain-invariant representation and exploiting parts of multi-level features. Specifically, realized by adversarial training with additional ability to utilize domain-specific information, a encoder is trained to remove domain-specific information and extract the domain-invariant representation from multiple inputs. Then, we utilize the multi-level feature parts extracted from inputs and reconstructed faces via a pre-trained recognition model to ensure that the domain-invariant representation contains enough useful semantic information. we also utilize the feature parts extracted from inputs and limited paired data to compose pseudo features in target domain for supervising the synthesis, which makes our framework suitable for large amounts of unpaired training data. By exploiting this framework, we can achieve face synthesis between multiple domains using some paired data together with a large training database without ground truth target faces. Experimental results demonstrate our framework achieves great performances on qualitative and quantitative evaluations under both artificial and uncontrolled environments, and our framework has competitive performances in single translation compared with specialized methods for translation between two specific domains.
C1 [Zhou, Dawei; Wang, Nannan; Yang, Xi] Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
   [Peng, Chunlei] Xidian Univ, Sch Cyber Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
   [Yu, Yi] Natl Inst Informat, Digital Content & Media Sci Div, Tokyo, Japan.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Xidian University; Xidian University; Research Organization of
   Information & Systems (ROIS); National Institute of Informatics (NII) -
   Japan; Chongqing University of Posts & Telecommunications
RP Wang, NN (corresponding author), Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
EM dwzhou.xidian@gmail.com; nnwang@xidian.edu.cn; clpeng@xidian.edu.cn;
   yiyu@nii.ac.jp; yangx@xidian.edu.cn; gaoxb@cqupt.edu.cn
RI Zhou, Dawei/JOJ-5062-2023
OI Zhou, Dawei/0000-0002-0694-3603; Wang, Nannan/0000-0002-4695-6134
FU National Key Research and Development Program of China [2018AAA0103202];
   National Natural Science Foundation of China [61922066, 61876142,
   62036007, 61772402, 62050175, 61976166]; Fundamental Research Funds for
   the Central Universities; Guangxi Natural Science Foundation Program
   [2021GXNSFDA075011]; Innovation Fund of Xidian University; Xidian
   University Intellifusion Joint Innovation Laboratory of Artificial
   Intelligence
FX This work was supported in part by the NationalKey Research and
   Development Program of China under Grant 2018AAA0103202, in part by the
   National Natural Science Foundation of China under Grants 61922066,
   61876142, 62036007, 61772402, 62050175, and 61976166, in part by
   theXidian University Intellifusion Joint Innovation Laboratory of
   Artificial Intelligence, in part by the Fundamental Research Funds for
   the Central Universities, in part by Guangxi Natural Science Foundation
   Program under Grant 2021GXNSFDA075011, and in part by the Innovation
   Fund of Xidian University.
CR [Anonymous], 2017, ARXIV171200899
   Cao B, 2019, IEEE T NEUR NET LEAR, V30, P1731, DOI 10.1109/TNNLS.2018.2872675
   Cao C, 2019, IEEE T MULTIMEDIA, V21, P2750, DOI 10.1109/TMM.2019.2911457
   Chen CF, 2019, LECT NOTES COMPUT SC, V11361, P216, DOI 10.1007/978-3-030-20887-5_14
   Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9
   Chen X, 2016, ADV NEUR IN, V29
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gatys L., 2015, NIPS
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu X., 2020, IEEE T MULTIMEDIA, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosseini-Asl E, 2016, IEEE T NEUR NET LEAR, V27, P2486, DOI 10.1109/TNNLS.2015.2479223
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kanwisher N, 1997, J NEUROSCI, V17, P4302
   Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403
   Kaur P, 2019, IEEE WINT CONF APPL, P2097, DOI 10.1109/WACV.2019.00227
   Kingma D. P., 2013, ARXIV13126114
   Kulkarni TD, 2015, ADV NEUR IN, V28
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li HL, 2011, IEEE T MULTIMEDIA, V13, P1230, DOI 10.1109/TMM.2011.2168814
   Li Z., 2020, IEEE Transactions on Multimedia, P1
   Liu AH, 2018, ADV NEUR IN, V31
   Logothetis NK, 1996, ANNU REV NEUROSCI, V19, P577, DOI 10.1146/annurev.ne.19.030196.003045
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Martinez A., 2007, COMPUTER VISION CTR, V3, P5
   Messer K., 1999, 2 INT C AUD VID BAS, V964, P965
   MISHKIN M, 1982, BEHAV BRAIN RES, V6, P57, DOI 10.1016/0166-4328(82)90081-X
   Odena A, 2017, PR MACH LEARN RES, V70
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang XO, 2004, IEEE T CIRC SYST VID, V14, P50, DOI 10.1109/TCSVT.2003.818353
   Tang XO, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P687, DOI 10.1109/ICCV.2003.1238414
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Ulyanov Dmitry, 2016, arXiv
   Wang LD, 2018, IEEE INT CONF AUTOMA, P83, DOI 10.1109/FG.2018.00022
   Wang N., 2017, ARXIV170101911, V2
   Wang NN, 2018, PATTERN RECOGN, V76, P215, DOI 10.1016/j.patcog.2017.11.008
   Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Zhang LL, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P627, DOI 10.1145/2671188.2749321
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang W, 2011, PROC CVPR IEEE, P513, DOI 10.1109/CVPR.2011.5995324
   Zhang WC, 2021, IEEE T PATTERN ANAL, V43, P2047, DOI 10.1109/TPAMI.2019.2962476
   Zhou H, 2012, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2012.6247788
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 52
TC 11
Z9 11
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 26
PY 2021
VL 24
BP 3469
EP 3479
DI 10.1109/TMM.2021.3099297
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NF
UT WOS:000824706400003
DA 2024-07-18
ER

PT J
AU Bai, C
   Li, HK
   Zhang, JL
   Huang, L
   Zhang, L
AF Bai, Cong
   Li, Hongkai
   Zhang, Jinglin
   Huang, Ling
   Zhang, Lu
TI Unsupervised Adversarial Instance-Level Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Training; Generators; Generative adversarial networks;
   Feature extraction; Gallium nitride; Task analysis; Generative
   adversarial training; human intelligence simulation; instance level
   image retrieval; unsupervised training
ID CONVOLUTIONAL FEATURES
AB With the wide use of visual sensors in the Internet of Things (IoT) in the past decades, huge amounts of images are captured in people's daily lives, which poses challenges to traditional deep-learning-based image retrieval frameworks. Most such frameworks need a large amount of annotated training data, which are expensive. Moreover, machines still lack human intelligence, as illustrated by the fact that they pay less attention to the interesting regions that humans generally focus on when searching for images. Hence, this paper proposes a novel unsupervised framework that focuses on the instance object in the image and integrates human intelligence into the deep-learning-based image retrieval. This framework is called adversarial instance-level image retrieval (AILIR). We incorporate adversarial training and an attention mechanism into this framework that considers human intelligence with artificial intelligence. The generator and discriminator are redesigned to guarantee that the generator retrieves similar images while the discriminator selects unmatched images and creates an adversarial reward for the generator. A minimax game is conducted by the adversarial reward retrieval mechanism until the discriminator is unable to judge whether the image sequence retrieved matches the query. Comparison and ablation experiments on four benchmark datasets prove that the proposed adversarial training framework indeed improves instance retrieval and outperforms the state-of-the-art methods focused on instance retrieval.
C1 [Bai, Cong; Li, Hongkai; Huang, Ling] Zhejiang Univ Technol, Coll Comp Sci, Hangzhou 310023, Peoples R China.
   [Bai, Cong; Li, Hongkai; Huang, Ling] Key Lab Visual Media Intelligent Proc Technol Zhe, Hangzhou, Peoples R China.
   [Zhang, Jinglin] Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.
   [Zhang, Lu] Univ Rennes, CNRS, INSA Rennes, IETR UMR 6164, F-35000 Rennes, France.
C3 Zhejiang University of Technology; Hebei University of Technology;
   Centre National de la Recherche Scientifique (CNRS); CNRS - Institute
   for Engineering & Systems Sciences (INSIS); Institut National des
   Sciences Appliquees de Rennes; Universite de Rennes
RP Zhang, JL (corresponding author), Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.
EM congbai@zjut.edu.cn; hongkail@zjut.edu.cn; jinglin.zhang37@gmail.com;
   iweisskohl@gmail.com; lu.ge@insa-rennes.fr
RI Bai, Cong/T-9188-2019
OI Bai, Cong/0000-0002-6177-3862; zhang, lu/0000-0002-8859-5453
FU National Natural Science Foundation of China [61976192, U1908210];
   Zhejiang Provincial Natural Science Foundation of China [LR21F020002]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61976192 and U1908210, and in part by
   the Zhejiang Provincial Natural Science Foundation of China under Grant
   LR21F020002.
CR Amato G, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2019.102100
   [Anonymous], 2016, PROC INT C LEARN REP
   Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bai C, 2020, J VIS COMMUN IMAGE R, V71, DOI 10.1016/j.jvcir.2020.102835
   Bai C, 2018, NEUROCOMPUTING, V303, P60, DOI 10.1016/j.neucom.2018.04.034
   Belagiannis V, 2017, IEEE INT CONF AUTOMA, P468, DOI 10.1109/FG.2017.64
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen JN, 2020, THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2020), P171, DOI 10.1109/MIPR49039.2020.00042
   Chen ZQ, 2018, IEEE IMAGE PROC, P1982, DOI 10.1109/ICIP.2018.8451486
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Cong Bai, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P525, DOI 10.1145/3372278.3390711
   Dizaji KG, 2018, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR.2018.00386
   FRIEDMAN A, 1979, J EXP PSYCHOL GEN, V108, P316, DOI 10.1037/0096-3445.108.3.316
   Goodfellow IJ, 2014, ADV NEUR IN, P2672, DOI DOI 10.1145/3422622
   Guo LT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1267, DOI 10.1145/3123266.3127939
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hu PF, 2020, IEEE INTERNET THINGS, V7, P5934, DOI 10.1109/JIOT.2019.2954528
   Huang L, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P97, DOI 10.1109/MIPR.2019.00025
   Huang X, 2020, IEEE T CYBERNETICS, V50, P1047, DOI 10.1109/TCYB.2018.2879846
   Jimenez A., 2017, BMVC
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Kim J., 2018, INT J ADV ROBOT SYST, V15, DOI [10.1177/1729881418792170, DOI 10.1177/1729881418817974]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li HK, 2020, LECT NOTES COMPUT SC, V11961, P381, DOI 10.1007/978-3-030-37731-1_31
   Li X, 2019, NEUROCOMPUTING, V359, P219, DOI 10.1016/j.neucom.2019.06.008
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1109/PLASMA.2013.6634954, 10.1017/S1368980013002176]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lv Y, 2018, LECT NOTES COMPUT SC, V10705, P239, DOI 10.1007/978-3-319-73600-6_21
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Ning HS, 2019, IEEE INTERNET THINGS, V6, P6811, DOI 10.1109/JIOT.2019.2911564
   Pang SM, 2018, PATTERN RECOGN, V83, P150, DOI 10.1016/j.patcog.2018.05.010
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Shi FF, 2020, IEEE NETWORK, V34, P8, DOI 10.1109/MNET.011.2000009
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song JK, 2018, AAAI CONF ARTIF INTE, P394
   Hoang T, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1600, DOI 10.1145/3123266.3123417
   Wan J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P157, DOI 10.1145/2647868.2654948
   Wang GA, 2018, LECT NOTES COMPUT SC, V11219, P491, DOI 10.1007/978-3-030-01267-0_29
   Wang W., 2021, IEEE T MULTIMEDIA, DOI [10.1109/TMM.2020.3011288, DOI 10.1109/TMM.2020.3011288]
   Xu J, 2018, AAAI CONF ARTIF INTE, P7436
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P2347, DOI 10.1109/TMM.2020.3009476
   Yelamarthi SK, 2018, LECT NOTES COMPUT SC, V11208, P316, DOI 10.1007/978-3-030-01225-0_19
   Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang JL, 2020, IEEE INTERNET THINGS, V7, P9702, DOI 10.1109/JIOT.2020.2991578
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhang XL, 2018, NEUROCOMPUTING, V322, P38, DOI 10.1016/j.neucom.2018.09.047
   Zhao RL, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2020.102306
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zhu JH, 2019, J VIS COMMUN IMAGE R, V62, P368, DOI 10.1016/j.jvcir.2019.06.006
NR 53
TC 27
Z9 27
U1 2
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2199
EP 2207
DI 10.1109/TMM.2021.3065578
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Guo, WY
   Zhang, Y
   Cai, XR
   Meng, L
   Yang, JF
   Yuan, XJ
AF Guo, Wenya
   Zhang, Ying
   Cai, Xiangrui
   Meng, Lei
   Yang, Jufeng
   Yuan, Xiaojie
TI LD-MAN: Layout-Driven Multimodal Attention Network for Online News
   Sentiment Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sentiment analysis; Visualization; Layout; Feature extraction;
   Analytical models; Neural networks; Image recognition; Multimodal
   sentiment recognition; online news; attention mechanism; article layout
ID CLASSIFICATION; EXTRACTION; FUSION; MODEL
AB The prevailing use of both images and text to express opinions on the web leads to the need for multimodal sentiment recognition. Some commonly used social media data containing short text and few images, such as tweets and product reviews, have been well studied. However, it is still challenging to predict the readers' sentiment after reading online news articles, since news articles often have more complicated structures, e.g., longer text and more images. To address this problem, we propose a layout-driven multimodal attention network (LD-MAN) to recognize news sentiment in an end-to-end manner. Rather than modeling text and images individually, LD-MAN uses the layout of online news to align images with the corresponding text. Specifically, it exploits a set of distance-based coefficients to model the image locations and measure the contextual relationship between images and text. LD-MAN then learns the affective representations of the articles from the aligned text and images using a multimodal attention mechanism. Considering the lack of relevant datasets in this field, we collect two multimodal online news datasets, containing a total of 14,566 articles with 56,260 images and 251,202 words. Experimental results demonstrate that the proposed method performs favorably compared with state-of-the-art approaches. We will release all the codes, models and datasets to the community.
C1 [Guo, Wenya; Zhang, Ying; Yang, Jufeng; Yuan, Xiaojie] Nankai Univ, Tianjin Key Lab Network & Data Secur Technol, Coll Comp Sci, Tianjin 300350, Peoples R China.
   [Cai, Xiangrui] Nankai Univ, Coll Cyber Sci, Tianjin 300350, Peoples R China.
   [Meng, Lei] Natl Univ Singapore, Sch Comp, NUS Tsinghua Southampton Ctr Extreme Search NExT, Singapore 117417, Singapore.
C3 Nankai University; Nankai University; National University of Singapore
RP Zhang, Y (corresponding author), Nankai Univ, Tianjin Key Lab Network & Data Secur Technol, Coll Comp Sci, Tianjin 300350, Peoples R China.
EM guowenya@dbis.nankai.edu.cn; yingzhang@nankai.edu.cn;
   caixr@nankai.edu.cn; lmeng@nus.edu.sg; yangjufeng@nankai.edu.cn;
   yuanxj@nankai.edu.cn
FU Major Project for New Generation of AI Grant [2018AAA0100403]; NSFC
   [61876094, U1933114, U1836109, U1903128, U1936206]; Natural Science
   Foundation of Tianjin, China [18JCYBJC15400, 18ZXZNGX00110]
FX This work was supported in part by the Major Project for New Generation
   of AI Grant under Grant 2018AAA0100403, in part by NSFC under Grants
   61876094, U1933114, U1836109, U1903128, and U1936206, in part by Natural
   Science Foundation of Tianjin, China under Grants 18JCYBJC15400 and
   18ZXZNGX00110.
CR Angelidis S., 2018, Transactions of the Association for Computational Linguistics, V6, P17, DOI DOI 10.1162/TACLA00002
   [Anonymous], 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   [Anonymous], 2013, P 13 ACM MULT C
   [Anonymous], 2015, P 1 INT WORKSH AFF S, DOI 10.1145/2813524.2813530
   [Anonymous], 2012, Mining Text Data, DOI [10.1007/978-1-4614-3223-4_13, DOI 10.1007/978-1-4614-3223-4_13, DOI 10.1007/978-1-4614-3223-413]
   Arevalo J., 2017, Gated multimodal units for information fusion
   Cai YT, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2506
   Cambria E, 2013, PROCEEDINGS OF THE 2013 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE FOR HUMAN-LIKE INTELLIGENCE (CIHLI), P108, DOI 10.1109/CIHLI.2013.6613272
   Cambria E, 2013, IEEE INTELL SYST, V28, P15, DOI 10.1109/MIS.2013.30
   Campos V, 2017, IMAGE VISION COMPUT, V65, P15, DOI 10.1016/j.imavis.2017.01.011
   Chang S.-F., 2016, P ACM INT C MULT, P684
   Chaturvedi I, 2019, PATTERN RECOGN LETT, V125, P264, DOI 10.1016/j.patrec.2019.04.024
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Chen YX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P117, DOI 10.1145/3240508.3240533
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Dou S., 2020, J PHYS C SERIES, V1453
   Dragoni M, 2018, IEEE INTELL SYST, V33, P77, DOI 10.1109/MIS.2018.033001419
   Du CD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P108, DOI 10.1145/3240508.3240528
   Groen Y, 2015, J AUTISM DEV DISORD, V45, P2848, DOI 10.1007/s10803-015-2448-z
   Tran HN, 2018, MEMET COMPUT, V10, P3, DOI 10.1007/s12293-017-0228-3
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang FR, 2019, KNOWL-BASED SYST, V167, P26, DOI 10.1016/j.knosys.2019.01.019
   Iyyer M, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1681
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Jia XY, 2019, PROC CVPR IEEE, P9833, DOI 10.1109/CVPR.2019.01007
   Johnson R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P562, DOI 10.18653/v1/P17-1052
   Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068
   Kankanhalli M. S., 2019, P IEEE C COMP VIS PA, P1140
   Lee J, 2019, IEEE I CONF COMP VIS, P10142, DOI 10.1109/ICCV.2019.01024
   Letarte G., 2018, P EMNLP WORKSH BLACK, P267, DOI DOI 10.18653/V1/W18-5429
   Li Hongzhi., 2013, Proceedings of the 21st ACM international conference on Multimedia, P449
   Li JY, 2016, SOFT COMPUT, V20, P3411, DOI 10.1007/s00500-015-1812-4
   Li X, 2019, AAAI CONF ARTIF INTE, P6714
   Lin Kevin Hsin-Yih, 2008, Wl 2008. 2008 IEEE/WIC/ACM International Conference on Web Intelligence. IAT 2008. 2008 IEEE/WIC/ACM International Conference on Intelligent Agent Technology. Wl-IAT Workshop 2008 2008 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology Workshops, P220, DOI 10.1109/WIIAT.2008.197
   Liu P, 2016, LECT NOTES COMPUT SC, V10042, P3, DOI 10.1007/978-3-319-48743-4_1
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Luo L, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4244
   Majumder N, 2019, IEEE INTELL SYST, V34, P38, DOI 10.1109/MIS.2019.2904691
   Morency L.-P., 2011, P 13 INT C MULT INT, P108
   Niu T., 2016, MULTIMEDIA MODELING, P15, DOI [DOI 10.1007/978-3-319-27674-82, 10.1007/978-3-319-27674-8_2]
   Panda R, 2018, LECT NOTES COMPUT SC, V11206, P594, DOI 10.1007/978-3-030-01216-8_36
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79, DOI 10.3115/1118693.1118704
   Paszke A, 2019, ADV NEUR IN, V32
   Perez-Rosas V., 2013, P ANN M ASS COMP LIN, P973
   Poria S, 2018, IEEE INTELL SYST, V33, P17, DOI 10.1109/MIS.2018.2882362
   Truong QT, 2019, AAAI CONF ARTIF INTE, P305
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   Rao YH, 2014, WORLD WIDE WEB, V17, P723, DOI 10.1007/s11280-013-0221-9
   Rao YH, 2014, INFORM SCIENCES, V266, P90, DOI 10.1016/j.ins.2013.12.059
   Rieis J., 2015, P INT AAAI C WEB SOC, V9, P357
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Simonyan K., 2014, 14091556 ARXIV
   Stein RA, 2019, INFORM SCIENCES, V471, P216, DOI 10.1016/j.ins.2018.09.001
   Tang DY, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1014
   Verma S, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3627
   Wang HH, 2017, IEEE INT CON MULTI, P949, DOI 10.1109/ICME.2017.8019301
   Weichselbraun A, 2017, IEEE INTELL SYST, V32, P80, DOI 10.1109/MIS.2017.57
   Wöllmer M, 2013, IEEE INTELL SYST, V28, P46, DOI 10.1109/MIS.2013.34
   Xu J, 2019, KNOWL-BASED SYST, V178, P61, DOI 10.1016/j.knosys.2019.04.018
   Xu J, 2019, APPL SOFT COMPUT, V80, P387, DOI 10.1016/j.asoc.2019.04.010
   Xu N., 2019, AAAI, P371
   Xu N, 2018, ACM/SIGIR PROCEEDINGS 2018, P929, DOI 10.1145/3209978.3210093
   Xu N, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2399, DOI 10.1145/3132847.3133142
   Yang JF, 2018, PROC CVPR IEEE, P7584, DOI 10.1109/CVPR.2018.00791
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yang QJ, 2019, IEEE INTELL SYST, V34, P43, DOI 10.1109/MIS.2019.2899142
   Yang Z., 2016, P 2016 C N AM CHAPT, P1480, DOI [DOI 10.18653/V1/N16-1174, 10.18653/v1/n16-1174]
   Yao XX, 2019, IEEE I CONF COMP VIS, P1140, DOI 10.1109/ICCV.2019.00123
   You QZ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1008, DOI 10.1145/2964284.2964288
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zhan C, 2019, IEEE I CONF COMP VIS, P1151, DOI 10.1109/ICCV.2019.00124
   Zhang KP, 2014, NEURAL NETWORKS, V58, P60, DOI 10.1016/j.neunet.2014.04.005
   Zhao SC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P192, DOI 10.1145/3343031.3351062
   Zhao ZY, 2019, INFORM PROCESS MANAG, V56, DOI 10.1016/j.ipm.2019.102097
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhu SY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P471
   Zhu XG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3595
   Zhu XL, 2019, LECT NOTES COMPUT SC, V11295, P264, DOI 10.1007/978-3-030-05710-7_22
NR 81
TC 25
Z9 27
U1 7
U2 49
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1785
EP 1798
DI 10.1109/TMM.2020.3003648
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300024
DA 2024-07-18
ER

PT J
AU Jia, CM
   Luo, FL
   Zhang, XF
   Wang, SQ
   Wang, SS
   Ma, SW
AF Jia, Chuanmin
   Luo, Falei
   Zhang, Xinfeng
   Wang, Shiqi
   Wang, Shanshe
   Ma, Siwei
TI Fast Non-Local Adaptive In-Loop Filter Optimization on GPU
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graphics processing units; Video coding; Standards; Encoding; Image
   reconstruction; Optimization; Image coding; Video coding; NSS; adaptive
   in-loop filter; NALF; GPU
ID MOTION ESTIMATION; VIDEO
AB The non-local adaptive in-loop filter (NALF) for video coding has achieved significant coding gain by exploiting image non-local self-similarity (NSS) to efficiently reduce the compression artifacts. However, the intensive computation of NALF hinders its practical deployment in video standardizations. In this paper, we propose a fast NALF optimization algorithm in parallel-computing framework by leveraging the massive parallel execution resources of GPU. First, the computational complexity of original NALF is analyzed in depth, then the pipelines of computational-intensive modules are re-designed to adapt to the general-purpose GPU with more parallel-friendly consideration. Specifically, we speed up the NALF by optimizing thread allocation to maximize the parallelism degree and elaborately designing the GPU block dimension to avoid access conflict. The group-level and pixel-level parallelization for collaboratively filtering and patch matching modules are designed respectively. To reduce the cost in data transmission, the whole filtering process is implemented on GPU by taking the advantage of low data dependency in NALF. Extensive experimental results show that the proposed fast NALF optimization using GPU architecture achieves high-speeed processing while maintaining the significant coding performance of original NALF, which shows the potential of NALF in the future video coding standard.
C1 [Jia, Chuanmin; Luo, Falei; Wang, Shanshe; Ma, Siwei] Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
   [Zhang, Xinfeng] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Kwoloon, Hong Kong, Peoples R China.
C3 Peking University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; City University of Hong Kong
RP Ma, SW (corresponding author), Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
EM cmjia@pku.edu.cn; falei.luo@vipl.ict.ac.cn; zhangxinf07@gmail.com;
   shiqwang@cityu.edu.hk; sswang@pku.edu.cn; swma@pku.edu.cn
RI Zhang, Xinfeng/X-8148-2019
OI Zhang, Xinfeng/0000-0002-7517-3868; Jia, Chuanmin/0000-0002-7418-6245
FU National Key Research and Development Project [2019YFF0302703];
   Guangdong Key Research and Development Project [2019B010133001]
FX This work was supported in part by National Key Research and Development
   Project (2019YFF0302703), in part by Guangdong Key Research and
   Development Project (2019B010133001), and in part by High-performance
   Computing Platform of Peking University.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Cheung NM, 2010, IEEE SIGNAL PROC MAG, V27, P79, DOI 10.1109/MSP.2009.935416
   Cho S, 2015, IEEE T MULTIMEDIA, V17, P778, DOI 10.1109/TMM.2015.2418995
   Côté G, 1998, IEEE T CIRC SYST VID, V8, P849, DOI 10.1109/76.735381
   de Souza DF, 2017, IEEE T MULTIMEDIA, V19, P459, DOI 10.1109/TMM.2016.2625261
   Duan YZ, 2014, IEEE T MULTIMEDIA, V16, P1915, DOI 10.1109/TMM.2014.2337834
   Fan L, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P423, DOI 10.1109/ICME.2004.1394219
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Garland M, 2008, IEEE MICRO, V28, P13, DOI 10.1109/MM.2008.57
   Gray J, 2003, NY TIMES BK REV, P9
   Huang K., 2009, P SPIE INT SOC OPTIC, V7497
   Jia CF, 2018, INT J PARALLEL PROG, V46, P674, DOI 10.1007/s10766-017-0520-3
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karczewicz M., 2016, P PICT COD S PCS, P1
   LEI SM, 1994, IEEE T CIRC SYST VID, V4, P425, DOI 10.1109/76.313137
   List P, 2003, IEEE T CIRC SYST VID, V13, P614, DOI 10.1109/TCSVT.2003.815175
   Luo FL, 2019, IEEE T MULTIMEDIA, V21, P851, DOI 10.1109/TMM.2018.2867260
   Ma SW, 2016, IEEE MULTIMEDIA, V23, P16, DOI 10.1109/MMUL.2016.16
   Ma SW, 2015, IEEE SIGNAL PROC MAG, V32, P172, DOI 10.1109/MSP.2014.2371951
   McGaffin MG, 2015, IEEE T IMAGE PROCESS, V24, P1273, DOI 10.1109/TIP.2015.2400813
   Meng XW, 2018, PICT COD SYMP, P233, DOI 10.1109/PCS.2018.8456299
   Momcilovic S, 2014, IEEE T MULTIMEDIA, V16, P108, DOI 10.1109/TMM.2013.2284892
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   NVIDIA CUDA, 2007, TECH REP
   Shahid MU, 2015, IEEE T CIRC SYST VID, V25, P701, DOI 10.1109/TCSVT.2014.2351111
   Stone JE, 2010, COMPUT SCI ENG, V12, P66, DOI 10.1109/MCSE.2010.69
   Ström J, 2017, IEEE IMAGE PROC, P11, DOI 10.1109/ICIP.2017.8296233
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tsai CY, 2013, IEEE J-STSP, V7, P934, DOI 10.1109/JSTSP.2013.2271974
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Yuan XT, 2013, J MACH LEARN RES, V14, P899
   Zhang J, 2016, IEEE DATA COMPR CONF, P91, DOI 10.1109/DCC.2016.105
   Zhang XF, 2017, IEEE T CIRC SYST VID, V27, P2177, DOI 10.1109/TCSVT.2016.2581618
NR 35
TC 4
Z9 4
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 39
EP 51
DI 10.1109/TMM.2020.2981185
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600004
DA 2024-07-18
ER

PT J
AU Lee, J
   Vigier, T
   Le Callet, P
   Lee, JS
AF Lee, Junghyuk
   Vigier, Toinon
   Le Callet, Patrick
   Lee, Jong-Seok
TI Wide Color Gamut Image Content Characterization: Method, Evaluation, and
   Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Quality of experience; Measurement; Standards; Toy
   manufacturing industry; Visualization; Color; Wide color gamut; color
   gamut mapping; content characterization; content selection; quality of
   experience
ID QUALITY ASSESSMENT; DIFFERENCE; FRAMEWORK
AB In this paper, we propose a novel framework to characterize a wide color gamut image content based on perceived quality due to the processes that change color gamut, and demonstrate two practical use cases where the framework can be applied. We first introduce the main framework and implementation details. Then, we provide analysis for understanding of existing wide color gamut datasets with quantitative characterization criteria on their characteristics, where four criteria, i.e., coverage, total coverage, uniformity, and total uniformity, are proposed. Finally, the framework is applied to content selection in a gamut mapping evaluation scenario in order to enhance reliability and robustness of the evaluation results. As a result, the framework fulfils content characterization for studies where quality of experience of wide color gamut stimuli is involved.
C1 [Lee, Junghyuk; Lee, Jong-Seok] Yonsei Univ, Sch Integrated Technol, Incheon 21983, South Korea.
   [Vigier, Toinon] Univ Nantes, LS2N, F-44300 Nantes, France.
   [Le Callet, Patrick] Univ Nantes, IRCCyN Lab, F-44300 Nantes, France.
C3 Yonsei University; Nantes Universite; Nantes Universite
RP Lee, JS (corresponding author), Yonsei Univ, Sch Integrated Technol, Incheon 21983, South Korea.
EM junghyuklee@yonsei.ac.kr; Toinon.Vigier@univ-nantes.fr;
   jong-seok.lee@yonsei.ac.kr
RI Lee, Jong-Seok/AAF-5197-2020
OI Lee, Jong-Seok/0000-0001-5255-4425; Lee, Junghyuk/0000-0002-6164-0728;
   Le callet, Patrick/0000-0002-2143-7063
FU MSIT (Ministry of Science, and ICT), Korea, under the "ICT Consilience
   Creative Program" [IITP-2019-2017-0-01015]; International Research &
   Development Program of National Research Foundation of Korea (NRF) - the
   Korea government (MSIT) [NRF-2016K1A3A1A21005710]; Science and
   Technology Amicable Research (STAR) Program - Partenariats Hubert Curien
   (PHC); Campus France [PHC-STAR 36664WK]
FX This research was supported by the MSIT (Ministry of Science, and ICT),
   Korea, under the "ICT Consilience Creative Program"
   (IITP-2019-2017-0-01015) supervised by the IITP (Institute for
   Information and communications Technology Promotion), the
   InternationalResearch & Development Program of the National Research
   Foundation of Korea (NRF) funded by theKorea government (MSIT)
   (NRF-2016K1A3A1A21005710), and the Science and Technology Amicable
   Research (STAR) Program funded by the Partenariats Hubert Curien (PHC)
   and the Campus France (PHC-STAR 36664WK). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xiaoqing Zhu. A preliminary version of this work was
   presented at the International Conference on Image Processing (ICIP) in
   2018 [1].
CR Alsam A, 2012, LECT NOTES COMPUT SC, V7431, P556, DOI 10.1007/978-3-642-33179-4_53
   [Anonymous], 2012, Recommendation BT.500-13
   [Anonymous], 2006, PROC COLOR IMAG C
   [Anonymous], 2004, 1562004 CIE
   [Anonymous], 2015, ITURBT20202
   [Anonymous], 2015, RECOMMENDATION ITU R
   [Anonymous], 2004, J149 ITUT
   [Anonymous], 2006, P 2 INT WORKSH VID P
   Chinnock C, 2016, 1261752O
   Chou CH, 2007, IEEE IC COMP COM NET, P1154
   CIE (Commission Internationale de l'Eclairage), 1978, E-1.3.1 1971/TC-1$3, V15
   Darcy D., 2016, P IS T INT S HUM VIS, P1
   Dugay F, 2008, COLOR RES APPL, V33, P470, DOI 10.1002/col.20443
   Ebner F, 1997, COLOR RES APPL, V22, P402, DOI 10.1002/(SICI)1520-6378(199712)22:6<402::AID-COL7>3.0.CO;2-Z
   Fairchild MD, 2004, J ELECTRON IMAGING, V13, P126, DOI 10.1117/1.1635368
   Fan ZW, 2017, IEEE T MULTIMEDIA, V19, P2720, DOI 10.1109/TMM.2017.2711860
   Farup I, 2007, IEEE T IMAGE PROCESS, V16, P2423, DOI 10.1109/TIP.2007.904946
   Freitas PG, 2018, IEEE T MULTIMEDIA, V20, P3353, DOI 10.1109/TMM.2018.2839529
   Froehlich J, 2014, PROC SPIE, V9023, DOI 10.1117/12.2040003
   Fu X., 2019, arXiv
   Gatta C., 2017, P IS T SPIE EL IM, P12
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Hardeberg JY, 2008, COLOR TECHNOL, V124, P243, DOI 10.1111/j.1478-4408.2008.00148.x
   Hasler D, 2003, P SOC PHOTO-OPT INS, V5007, P87, DOI 10.1117/12.477378
   Hong GW, 2002, PROC SPIE, V4421, P618, DOI 10.1117/12.464761
   Katoh N, 1999, J ELECTRON IMAGING, V8, P365, DOI 10.1117/1.482705
   Kolås O, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P207
   Krasula L, 2017, IEEE J-STSP, V11, P64, DOI 10.1109/JSTSP.2016.2637168
   Krasula L, 2014, PROC SPIE, V9217, DOI 10.1117/12.2075270
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Lee JS, 2014, IEEE T MULTIMEDIA, V16, P564, DOI 10.1109/TMM.2013.2292590
   Lee J, 2018, IEEE IMAGE PROC, P709, DOI 10.1109/ICIP.2018.8451497
   Lissner I, 2013, IEEE T IMAGE PROCESS, V22, P435, DOI 10.1109/TIP.2012.2216279
   Mackin A, 2019, IEEE T MULTIMEDIA, V21, P1499, DOI 10.1109/TMM.2018.2880603
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Morovic J, 2003, ELEVENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING - SYSTEMS, TECHNOLOGIES, APPLICATIONS, P282
   Morovic J, 1997, FIFTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS, AND APPLICATIONS, P44
   Murch G. M., 1989, P EUR SEM ADV COMP G, P19
   Narwaria Manish, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P13, DOI 10.1109/QoMEX.2014.6982279
   Ortiz-Jaramillo B., 2016, Proc. 8th Int. Conf. on Qual. Multim. Exp, P1
   Paudyal P, 2017, INT WORK QUAL MULTIM
   Pedersen M, 2009, LECT NOTES COMPUT SC, V5646, P81, DOI 10.1007/978-3-642-03265-3_9
   Pinson MH, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-50
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Rajashekar U, 2009, IEEE IMAGE PROC, P2213, DOI 10.1109/ICIP.2009.5413889
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Smith T., 1932, Transactions of the Optical Society, V33, P73
   STONE MC, 1988, ACM T GRAPHIC, V7, P249, DOI 10.1145/46165.48045
   Toet A, 2003, DISPLAYS, V24, P197, DOI 10.1016/j.displa.2004.01.006
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Zamir SW, 2017, IEEE T IMAGE PROCESS, V26, P1595, DOI 10.1109/TIP.2017.2661404
   Winkler S, 2012, IEEE J-STSP, V6, P616, DOI 10.1109/JSTSP.2012.2215007
   Zamir SW, 2014, IEEE J-STSP, V8, P490, DOI 10.1109/JSTSP.2014.2313182
   Zhang F, 2018, IEEE T MULTIMEDIA, V20, P2620, DOI 10.1109/TMM.2018.2817070
   Zhang X., 1996, 1996 SID International Symposium. Digest of Technical Papers. First Edition, P731
   Zhang X., J SOC INF DISPLAY, V5, P61
   Zolliker P., 2006, P C COL GRAPH IM VIS, V2006, P257
   Zolliker P, 2007, IEEE T IMAGE PROCESS, V16, P664, DOI 10.1109/TIP.2006.891346
NR 60
TC 1
Z9 2
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3817
EP 3827
DI 10.1109/TMM.2020.3032026
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100030
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Li, LD
   Li, YX
   Wu, JJ
   Ma, L
   Fang, YM
AF Li, Leida
   Li, Yixuan
   Wu, Jinjian
   Ma, Lin
   Fang, Yuming
TI Quality Evaluation for Image Retargeting With Instance Semantics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Feature extraction; Distortion; Measurement; Degradation;
   Image segmentation; Data mining; Image retargeting quality assessment;
   semantics; instance segmentation; semantics-based weighting
ID COLOR
AB To meet the ever-increasing demand for devices with diversified displays, image retargeting has become a prevalent technique for adaptive image resizing. In practice, the retargeting operation inevitably causes impairments in the images; thus, image retargeting quality assessment (IRQA) is urgently needed and, can be used to guide algorithm optimization, selection and design. Unlike traditional image quality assessment, image retargeting introduces geometric distortions, which typically affect high-level image semantics. With this motivation, this paper presents a quality evaluation model for image retargeting based on INstance SEMantics (INSEM). Considering that the human visual system (HVS) perceives images highly dependent on apprehensible areas and that impairments in image retargeting mainly degrade the salient instances, an image instance is utilized as the basic semantic unit, and a top-down method is devised to extract instance-level semantic features for IRQA. In addition, taking into account the influence of semantic categories on the perception of retargeting quality, we further propose Semantic-based self-adaptive pooling (SSAP) to integrate instance-based semantic features. Finally, global features are incorporated to generate quality scores that are more consistent with people's perceptions. Extensive experiments and comparisons of three public databases, in terms of both intradatabase and cross-database settings, demonstrate the superiority of the proposed metric over state-of-the-art methods.
C1 [Li, Leida; Li, Yixuan] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Jiangsu, Peoples R China.
   [Li, Leida; Wu, Jinjian] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Ma, Lin] Meituan Dianping Grp, Beijing 100102, Peoples R China.
   [Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Management, Nanchang 330032, Jiangxi, Peoples R China.
C3 China University of Mining & Technology; Xidian University; Jiangxi
   University of Finance & Economics
RP Li, YX (corresponding author), China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Jiangsu, Peoples R China.
EM reader1104@hotmail.com; ts18060134p31@cumt.edu.cn;
   jinjian.wu@mail.xidian.edu.cn; forest.linma@gmail.com;
   fa0001ng@e.ntu.edu.sg
RI Li, Li/AEM-3636-2022; li, li/HII-4157-2022; Wu, Jinjian/GQH-0222-2022
OI Li, Yixuan/0000-0003-4725-7586
FU National Natural Science Foundation of China [61771473, 61991451,
   61379143]; National Key R&D Program of China [2018AAA0100601]; Key
   Project of Shaanxi Provincial Department of Education [20JY024]; Science
   and Technology Plan of Xi'an [20191122015KYPT011JC013]; Natural Science
   Foundation of Jiangsu Province [BK20181354]; Six Talent Peaks High-level
   Talents in Jiangsu Province [XYDXX-063]
FX Manuscript received April 1, 2020; revised July 24, 2020; accepted
   August 5, 2020. Date of publication August 14, 2020; date of current
   version August 24, 2021. This work was supported in part by the National
   Natural Science Foundation of China under Grants 61771473, 61991451, and
   61379143, in part by the National Key R&D Program of China under Grant
   2018AAA0100601, in part by the Key Project of Shaanxi Provincial
   Department of Education under Grant 20JY024, in part by the Science and
   Technology Plan of Xi'an under Grant 20191122015KYPT011JC013, in part by
   the Natural Science Foundation of Jiangsu Province under Grant
   BK20181354, and in part by the Six Talent Peaks High-level Talents in
   Jiangsu Province under Grant XYDXX-063. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Joao M Ascenso. (Corresponding author: Yixuan Li.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2016, ELECT IMAGING
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Barrow H. G., 1977, P IMAGE UNDERSTANDIN, P659
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Castillo S., 2011, ACM SIGGRAPH S APPLP, P7
   Chen ZB, 2017, IEEE T IMAGE PROCESS, V26, P5138, DOI 10.1109/TIP.2017.2736422
   Dong WM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618471
   Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11
   Fang YM, 2014, IEEE J EM SEL TOP C, V4, P95, DOI 10.1109/JETCAS.2014.2298919
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Fei-Fei L, 2007, J VISION, V7, DOI 10.1167/7.1.10
   Flusser J., 2009, Moments and Moment Invariants in Pattern Recognition
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hsu CC, 2014, IEEE J-STSP, V8, P377, DOI 10.1109/JSTSP.2014.2311884
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang B, 2020, IEEE T CYBERNETICS, V50, P87, DOI 10.1109/TCYB.2018.2864158
   Jiang QP, 2018, IEEE T CYBERNETICS, V48, P1276, DOI 10.1109/TCYB.2017.2690452
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   Kasutani E, 2001, IEEE IMAGE PROC, P674, DOI 10.1109/ICIP.2001.959135
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lei JJ, 2017, IEEE T MULTIMEDIA, V19, P1442, DOI 10.1109/TMM.2017.2660440
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Liang Y, 2017, IEEE T VIS COMPUT GR, V23, P1099, DOI 10.1109/TVCG.2016.2517641
   Liu AM, 2015, SIGNAL PROCESS-IMAGE, V39, P444, DOI 10.1016/j.image.2015.08.001
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma L, 2016, IEEE T MULTIMEDIA, V18, P2228, DOI 10.1109/TMM.2016.2614187
   Ma L, 2012, IEEE J-STSP, V6, P626, DOI 10.1109/JSTSP.2012.2211996
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Mukundan R, 2001, IEEE T IMAGE PROCESS, V10, P1357, DOI 10.1109/83.941859
   Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Schölkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565
   Shamir Ariel, 2009, ACM SIG-GRAPH ASIA 2009 Courses, P11
   Siahaan E, 2016, IEEE INT SYM MULTIM, P307, DOI [10.1109/ISM.2016.0067, 10.1109/ISM.2016.54]
   Simakov D., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587842
   Tan WM, 2020, IEEE T MULTIMEDIA, V22, P1730, DOI 10.1109/TMM.2019.2959925
   Tang F, 2020, IEEE T MULTIMEDIA, V22, P641, DOI 10.1109/TMM.2019.2932620
   Thung KH, 2012, PATTERN RECOGN, V45, P2193, DOI 10.1016/j.patcog.2011.12.001
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan B, 2020, IEEE T MULTIMEDIA, V22, P676, DOI 10.1109/TMM.2019.2932566
   Yang JC, 2020, IEEE T MULTIMEDIA, V22, P2635, DOI 10.1109/TMM.2019.2961209
   Yang JC, 2019, IEEE T IMAGE PROCESS, V28, P1314, DOI 10.1109/TIP.2018.2878283
   Zhang JY, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P257, DOI 10.1145/2647868.2654922
   Zhang P, 2015, PROC CVPR IEEE, P2394, DOI 10.1109/CVPR.2015.7298853
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P451, DOI 10.1109/TIP.2017.2761556
   Zhang YB, 2016, IEEE T IMAGE PROCESS, V25, P4286, DOI 10.1109/TIP.2016.2585884
   Zhang YC, 2017, IEEE T IMAGE PROCESS, V26, P5980, DOI 10.1109/TIP.2017.2746260
NR 57
TC 9
Z9 10
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2757
EP 2769
DI 10.1109/TMM.2020.3016124
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600017
DA 2024-07-18
ER

PT J
AU Liu, Q
   Yuan, H
   Hou, JH
   Hamzaoui, R
   Su, HL
AF Liu, Qi
   Yuan, Hui
   Hou, Junhui
   Hamzaoui, Raouf
   Su, Honglei
TI Model-Based Joint Bit Allocation Between Geometry and Color for
   Video-Based 3D Point Cloud Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit allocation; distortion-quantization (D-Q) model; point cloud
   compression; rate-distortion optimization (RDO); rate-quantization (R-Q)
   model
AB In video-based 3D point cloud compression, the quality of the reconstructed 3D point cloud depends on both the geometry, and color distortions. Finding an optimal allocation of the total bitrate between the geometry coder, and the color coder is a challenging task due to the large number of possible solutions. To solve this bit allocation problem, we first propose analytical distortion, and rate models for the geometry, and color information. Using these models, we formulate the joint bit allocation problem as a constrained convex optimization problem, and solve it with an interior point method. Experimental results show that the rate-distortion performance of the proposed solution is close to that obtained with exhaustive search but at only 0.66% of its time complexity.
C1 [Liu, Qi] Shandong Univ, Sch Informat Sci & Engn, Qingdao 266237, Peoples R China.
   [Liu, Qi; Yuan, Hui] Shandong Univ, Shenzhen Res Inst, Shenzhen 518057, Peoples R China.
   [Yuan, Hui] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
   [Hou, Junhui] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Hamzaoui, Raouf] De Montfort Univ, Sch Engn & Sustainable Dev, Leicester LE1 9BH, Leics, England.
   [Su, Honglei] Qingdao Univ, Sch Elect Informat, Qingdao 266071, Peoples R China.
C3 Shandong University; Shandong University; Shandong University; City
   University of Hong Kong; De Montfort University; Qingdao University
RP Yuan, H (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
EM sdqi.lu@gmail.com; yuanhui0325@gmail.com; jh.hou@cityu.edu.hk;
   rhamzaoui@dmu.ac.uk; suhonglei@qdu.edu.cn
RI Yuan, Hui/HDO-3699-2022
OI Yuan, Hui/0000-0001-5212-3393
FU National Natural Science Foundation of China [61571274, 61871342];
   Shandong Provincial Key Research, and Development Plan [2017CXGC1504];
   Open Project Program of State Key Laboratory of Virtual Reality
   Technology, and Systems, Beihang University [VRLAB2019B03]; Shenzhen
   Science, and Technology Research, and Development Funds
   [JCYJ20170818103244664]; Hong Kong Research Grants Council [9042955,
   CityU 11202320]; Shandong Provincial Natural Science Foundation, China
   [ZR2018PF002]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61571274 and 61871342, in part by the
   Shandong Provincial Key Research, and Development Plan under Grant
   2017CXGC1504, in part by Open Project Program of State Key Laboratory of
   Virtual Reality Technology, and Systems, Beihang University, under Grant
   VRLAB2019B03, in part by Shenzhen Science, and Technology Research, and
   Development Funds underGrant JCYJ20170818103244664, in part by theHong
   Kong Research Grants Council under Grants 9042955 and (CityU 11202320),
   and in part by the Shandong Provincial Natural Science Foundation, China
   under Grant ZR2018PF002. The associate editor coordinating the review of
   this manuscript and approving it for publication was Professor Marco
   Grangetto.
CR 3DG, 2020, ISOIECJTC1SC29WG11MP
   3DG, 2019, JTC1SC29WG11 ISOIEC
   Anis A, 2016, INT CONF ACOUST SPEE, P6360, DOI 10.1109/ICASSP.2016.7472901
   Bjotegaard G., 2001, VCEGM33
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cen F, 2014, 2014 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND MULTIMEDIA APPLICATIONS (SIGMAP), P54
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chou P. A., 2018, ISOIECJTC1SC29WG11MP
   Chou PA, 2016, IEEE IMAGE PROC, P1524, DOI 10.1109/ICIP.2016.7532613
   Cohen RA, 2016, IEEE DATA COMPR CONF, P141, DOI 10.1109/DCC.2016.67
   Cohen RA, 2016, IEEE IMAGE PROC, P1374, DOI 10.1109/ICIP.2016.7532583
   Cui Y, 2013, IEEE T PATTERN ANAL, V35, P1039, DOI 10.1109/TPAMI.2012.190
   David F., 2019, ISOIECJTC1SC2WG11MPE
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3886, DOI 10.1109/TIP.2017.2707807
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3507, DOI 10.1109/TIP.2017.2699922
   de Queiroz RL, 2016, IEEE T IMAGE PROCESS, V25, P3947, DOI 10.1109/TIP.2016.2575005
   Fang L, 2014, IEEE T IMAGE PROCESS, V23, P185, DOI 10.1109/TIP.2013.2287608
   Gu S, 2020, IEEE SIGNAL PROC LET, V27, P176, DOI 10.1109/LSP.2019.2963793
   Gu S, 2020, IEEE T IMAGE PROCESS, V29, P796, DOI 10.1109/TIP.2019.2936738
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Hou JH, 2019, IEEE SIGNAL PROC LET, V26, P1847, DOI 10.1109/LSP.2019.2949724
   Hou JH, 2017, INT CONF ACOUST SPEE, P2926, DOI 10.1109/ICASSP.2017.7952692
   Hou JH, 2015, IEEE T CIRC SYST VID, V25, P51, DOI 10.1109/TCSVT.2014.2329376
   Hou JH, 2014, IEEE T CIRC SYST VID, V24, P1541, DOI 10.1109/TCSVT.2014.2313890
   Hou JH, 2013, IEEE T CIRC SYST VID, V23, P1537, DOI 10.1109/TCSVT.2013.2248971
   Idele I., 1979, J ROY STAT SOC D-STA, V28, P209
   JCT-VC, 2020, HEVC TEST MOD
   Khaled M, 2018, ISOIECJTC1SC29WG11MP
   Khaled M., 2018, ISOIECJTC1SC29WG11MP
   Li L, 2020, IEEE T IMAGE PROCESS, V29, P289, DOI 10.1109/TIP.2019.2931621
   Liu H, 2020, IEEE T BROADCAST, V66, P701, DOI 10.1109/TBC.2019.2957652
   Liu Q, 2018, ASIAPAC SIGN INFO PR, P1981, DOI 10.23919/APSIPA.2018.8659653
   Mekuria R., 2016, ISOIECJTC1SC29WG1MPE
   Mekuria R, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   MPEG, 2017, TEST CODE PCC 3DG
   MPEGC, 2020, MPEG POINT CLOUD DAT
   Oropallo William, 2018, Computer-Aided Design and Applications, V15, P90, DOI 10.1080/16864360.2017.1353732
   Park Y, 2008, INT SYM MIX AUGMENT, P117, DOI 10.1109/ISMAR.2008.4637336
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Sandri GP, 2019, IEEE SIGNAL PROC LET, V26, P1369, DOI 10.1109/LSP.2019.2931425
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Sebastian S., 2019, ISOIECJTC1SC29WG11MP
   Shao YT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1199, DOI 10.1145/3240508.3240696
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506
   Tu CX, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1712, DOI 10.1109/ITSC.2016.7795789
   Vlachos E, 2018, IEEE T MULTIMEDIA, V20, P3276, DOI 10.1109/TMM.2018.2839911
   Xia JZ, 2012, IEEE T CIRC SYST VID, V22, P77, DOI 10.1109/TCSVT.2011.2158337
   Yuan H, 2011, IEEE T CIRC SYST VID, V21, P485, DOI 10.1109/TCSVT.2011.2125610
   Zhang C, 2014, IEEE IMAGE PROC, P2066, DOI 10.1109/ICIP.2014.7025414
NR 52
TC 27
Z9 27
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3278
EP 3291
DI 10.1109/TMM.2020.3023294
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000026
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lu, K
   Zhang, LH
AF Lu, Kun
   Zhang, Lihong
TI TBEFN: A Two-Branch Exposure-Fusion Network for Low-Light Image
   Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lighting; Noise reduction; Image enhancement; Estimation; Image
   reconstruction; Image color analysis; Visualization; Blind low-light
   image enhancement; exposure-fusion network; highly adaptable; transfer
   function estimation
ID QUALITY ASSESSMENT; DECOMPOSITION; RETINEX
AB Images obtained under low-light conditions are usually accompanied by varied and highly unpredictable degradation. The uncertainty of the imaging environment makes the enhancement even more challenging. In this paper, we present a two-branch exposure-fusion network to tackle the problem of blind low-light image enhancement. In the first part of the paper, we provide a basic insight into the degradation mechanism of low-light images, and propose a quick and effective enhancement strategy by estimating the transfer function for varied illumination levels. To further deal with the challenge brought about by the blindness of input images, a novel generation-and-fusion strategy is then introduced, where the enhancements for slightly and heavily distorted images are carried out respectively in the two enhancing branches, followed by a self-adaptive attention unit to perform the final fusion. Moreover, a two-stage denoising strategy is also proposed to ensure effective noise reduction in a data-driven manner. To evaluate the performance of the proposed method, three commonly used datasets are adopted for quantitative evaluation and six for visual evaluation, where our method outperforms many of the existing state-of-the-art ones, showing great effectiveness and potential.
C1 [Lu, Kun; Zhang, Lihong] Shanxi Univ, Coll Phys & Elect Engn, Taiyuan 030006, Peoples R China.
C3 Shanxi University
RP Zhang, LH (corresponding author), Shanxi Univ, Coll Phys & Elect Engn, Taiyuan 030006, Peoples R China.
EM lukun199@gmail.com; lhzhang@sxu.edu.cn
OI lu, kun/0000-0003-4698-9769
CR Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong XC, 2011, INT C PAR DISTRIB SY, P9, DOI 10.1109/ICPADS.2011.115
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He RJ, 2012, 2012 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING TECHNIQUES AND APPLICATIONS (DICTA)
   Hensel M, 2017, ADV NEUR IN, V30
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Ignatov A, 2018, IEEE COMPUT SOC CONF, P804, DOI 10.1109/CVPRW.2018.00112
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Jia XX, 2018, NEUROCOMPUTING, V322, P216, DOI 10.1016/j.neucom.2018.09.064
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   King DB, 2015, ACS SYM SER, V1214, P1
   Kinoshita Y, 2019, IEEE T IMAGE PROCESS, V28, P4101, DOI 10.1109/TIP.2019.2906501
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Lee S, 2018, IEEE ACCESS, V6, P49913, DOI 10.1109/ACCESS.2018.2868246
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Liu D., 2019, ENLIGHTENGAN DEEP LI
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lv F., 2018, P BMVC, V220, P4
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Pisano ED, 1998, J DIGIT IMAGING, V11, P193, DOI 10.1007/BF03178082
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Ren J, 2013, IEEE T IMAGE PROCESS, V22, P1454, DOI 10.1109/TIP.2012.2231690
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen L., 2017, ARXIV171102488
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang YF, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2922106
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Yan QS, 2019, IEEE WINT CONF APPL, P41, DOI 10.1109/WACV.2019.00012
   Ying Z., 2017, ARXIV PREPRINT ARXIV
   Ying ZQ, 2017, IEEE INT CONF COMP V, P3015, DOI 10.1109/ICCVW.2017.356
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
NR 51
TC 92
Z9 101
U1 5
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4093
EP 4105
DI 10.1109/TMM.2020.3037526
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900014
DA 2024-07-18
ER

PT J
AU Lu, Y
   Cha, JH
   Youm, SK
   Jung, SW
AF Lu, Yucheng
   Cha, Jin-Hyuck
   Youm, Se-Kyoung
   Jung, Seung-Won
TI Parametric Shape Estimation of Human Body Under Wide Clothing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Clothing; Three-dimensional displays; Two dimensional displays;
   Biological system modeling; Pose estimation; Silhouette confidence;
   convolutional neural network; human shape estimation; synthetic dataset
ID HIP RATIO; POSE
AB The shape of the human body plays an important role in many applications, such as those involving personal healthcare and virtual clothing try-ons. However, accurate body shape measurements typically require the user to be wearing a minimal amount of clothing, which is not practical in many situations. To resolve this issue using deep learning techniques, we need a paired dataset of ground-truth naked human body shapes and their corresponding color images with clothes. As it is practically impossible to collect enough of this kind of data from real-world environments to train a deep neural network, in this paper, we present the Synthetic dataset of Human Avatars under wiDE gaRment (SHADER). The SHADER dataset consists of 300,000 paired ground-truth naked and dressed images of 1,500 synthetic humans with different body shapes, poses, garments, skin tones, and backgrounds. To take full advantage of SHADER, we propose a novel silhouette confidence measure and show that our silhouette confidence prediction network can help improve the performance of state-of-the-art shape estimation networks for human bodies under clothing. The experimental results demonstrate the effectiveness of the proposed approach. The code and dataset are available at https://github.com/YCL92/SHADER.
C1 [Lu, Yucheng; Cha, Jin-Hyuck] Dongguk Univ, Dept Multimedia Engn, Seoul 04620, South Korea.
   [Youm, Se-Kyoung] Dongguk Univ, Dept Ind & Syst Engn, South, Seoul 04620, South Korea.
   [Jung, Seung-Won] Korea Univ, Dept Elect Engn, Seoul 02841, South Korea.
C3 Dongguk University; Dongguk University; Korea University
RP Jung, SW (corresponding author), Korea Univ, Dept Elect Engn, Seoul 02841, South Korea.
EM yucheng.l@outlook.com; ckwlsgur20@gmail.com; skyoum@dgu.edu;
   swjung83@korea.ac.kr
RI Lu, Yucheng/GQR-2077-2022
OI Jung, Seung-Won/0000-0002-0319-4467; Lu, Yucheng/0000-0003-2990-5252
FU Samsung Research Funding& Incubation Center of Samsung Electronics
   [SRFC-IT1801-11]
FX Manuscript received June 15, 2020; revised September 8, 2020; accepted
   October 5, 2020. Date of publication October 9, 2020; date of current
   version October 19, 2021. Thisworkwas supported by the Samsung Research
   Funding& Incubation Center of Samsung Electronics under Project No.
   SRFC-IT1801-11. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Wen-Huang Cheng.
   (Corresponding author: Seung-Won Jung).
CR Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2018, ARXIV180608485
   Autodesk, 2019, 3DS MAX SOFTW
   Autodesk, 2019, MAYA SOFTW
   Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552
   Blackwell S, 2002, AFRLHEWPTR20020169
   Blender Foundation, 2019, BLEND SOFTW
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303
   Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388
   Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58
   CLO Virtual Fashion LLC, 2019, MD STOR ONL 3D GARM
   CLO Virtual Fashion LLC, 2019, CLO3D SOFTW
   Dabral R, 2018, LECT NOTES COMPUT SC, V11213, P679, DOI 10.1007/978-3-030-01240-3_41
   Elsayed EF, 2008, AM J KIDNEY DIS, V52, P49, DOI 10.1053/j.ajkd.2008.04.002
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Fan XC, 2015, PROC CVPR IEEE, P1347, DOI 10.1109/CVPR.2015.7298740
   Ferrari V, 2009, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2009.5206495
   Guan P, 2009, IEEE I CONF COMP VIS, P1381, DOI 10.1109/iccv.2009.5459300
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Gundogdu E, 2019, IEEE I CONF COMP VIS, P8738, DOI 10.1109/ICCV.2019.00883
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton, 2012, NEURAL NETWORKS MACH
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Johnson S, 2011, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR.2011.5995318
   Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   KIMMEL R, 1993, P SOC PHOTO-OPT INS, V2031, P259, DOI 10.1117/12.146631
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Ladicky L, 2013, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2013.459
   Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500
   Li C, 2018, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2018.00548
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Mishra G, 2018, IEEE WINT CONF APPL, P390, DOI 10.1109/WACV.2018.00049
   Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343
   Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062
   Pavlakos G, 2019, IEEE I CONF COMP VIS, P803, DOI 10.1109/ICCV.2019.00089
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Pishchulin L, 2011, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2011.5995574
   Pumarola A, 2019, IEEE I CONF COMP VIS, P2242, DOI [10.1109/ICCV.2019.00233, 10.1109/ICCV.2019.2019.00233]
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Robinette K., 2002, Civilian American and European surface anthropometry resource (CAESAR), Final report
   Rogez G, 2020, IEEE T PATTERN ANAL, V42, P1146, DOI 10.1109/TPAMI.2019.2892985
   Rong Y, 2019, IEEE I CONF COMP VIS, P5339, DOI 10.1109/ICCV.2019.00544
   Rucklidge W., 1996, HAUSDORFF DISTANCE
   Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471
   Sekhavat YA, 2017, IEEE T MULTIMEDIA, V19, P1041, DOI 10.1109/TMM.2016.2639380
   Sigal L., 2008, ADV NEURAL INFORM PR, P1337
   Slabaugh G. G, 1999, COMPUTING EULER ANGL, V6, P39
   Song D, 2016, COMPUT GRAPH FORUM, V35, P147, DOI 10.1111/cgf.13012
   Tan I. B. Vince, 2017, P BRIT MACHINE VISIO
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Tompson J, 2014, ADV NEUR IN, V27
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492
   Wang YB, 2019, PROC CVPR IEEE, P9146, DOI 10.1109/CVPR.2019.00937
   Wei P, 2019, IEEE T MULTIMEDIA, V21, P2195, DOI 10.1109/TMM.2019.2897902
   Welborn TA, 2003, MED J AUSTRALIA, V179, P580, DOI 10.5694/j.1326-5377.2003.tb05704.x
   Xu YL, 2019, IEEE I CONF COMP VIS, P7759, DOI 10.1109/ICCV.2019.00785
   Yang JL, 2016, LECT NOTES COMPUT SC, V9908, P439, DOI 10.1007/978-3-319-46493-0_27
   Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551
   Yu T, 2018, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR.2018.00761
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51
NR 68
TC 6
Z9 6
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3657
EP 3669
DI 10.1109/TMM.2020.3029941
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100018
DA 2024-07-18
ER

PT J
AU Lv, K
   Sheng, H
   Xiong, Z
   Li, W
   Zheng, L
AF Lv, Kai
   Sheng, Hao
   Xiong, Zhang
   Li, Wei
   Zheng, Liang
TI Improving Driver Gaze Prediction With Reinforced Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Computational modeling; Vehicles; Predictive models;
   Hidden Markov models; Semantics; Computer architecture; Gaze prediction;
   driver attention; reinforcement learning; video processing; deep
   learning
ID SYSTEM
AB We consider the task of driver gaze prediction: estimating where the location of the focus of a driver should be, based on a raw video of the outside environment. In practice, we output a probability map that gives the normalized probability of each point in a given scene being the object of the driver attention. Most existing methods (i.e., Coarse-to-Fine and Multi-branch) take an image or a video as input and directly output the fixation map. While successful, these methods can often produce highly scattered predictions, rendering them unreliable for real-world usage. Motivated by this observation, we propose the reinforced attention (RA) model as a regulatory mechanism to increase prediction density. Our method is built directly on top of existing methods, making it complementary to current approaches. Specifically, we first use Multi-branch to obtain an initial fixation map. Then, RA is trained using deep reinforcement learning to learn a location prediction policy, producing a reinforced attention. Finally, in order to obtain the final gaze prediction result, we combine the fixation map and the reinforced attention by a mask-guided multiplication. Experimental results show that our framework improves the accuracy of gaze prediction, and provides state-of-the-art performance on the DR(eye)VE dataset.
C1 [Lv, Kai; Sheng, Hao] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
   [Lv, Kai; Sheng, Hao] Beihang Univ, Beijing Adv Innovat Ctr Big Data & Brain Comp, Beijing 100191, Peoples R China.
   [Lv, Kai; Sheng, Hao] Beihang Univ, Beihang Hangzhou Inst Innovat Yuhang, Hangzhou 311121, Peoples R China.
   [Xiong, Zhang; Li, Wei] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Zheng, Liang] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 2601, Australia.
C3 Beihang University; Beihang University; Beihang University; Beihang
   University; Australian National University
RP Sheng, H (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
EM lvkai@buaa.edu.cn; shenghao@huaa.edu.cn; xiongz@buaa.edu.cn;
   liwei@nlsde.buaa.edu.cn; liang.zheng@anu.edu.au
OI Lv, Kai/0000-0001-6533-5176; Zheng, Liang/0000-0002-1464-9500
FU National Key R&D Program of China [2019YFB2101600]; National Natural
   Science Foundation of China [61861166002, 61872025, 61635002]; Science
   andTechnology Development Fund, Macau SAR [0001/2018/AFJ]; Fundamental
   Research Funds for the Central Universities; Open Fund of the State Key
   Laboratory of Software Development Environment [SKLSDE2019ZX-04];
   Australian Research Council - Australian Government [DE200101283];
   Australian Research Council [DE200101283] Funding Source: Australian
   Research Council
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2019YFB2101600, in part by the National Natural Science
   Foundation of China under Grants 61861166002, 61872025, and 61635002, in
   part by the Science andTechnology Development Fund, Macau SAR (File
   no.0001/2018/AFJ), and in part by the Fundamental Research Funds for the
   Central Universities and the Open Fund of the State Key Laboratory of
   Software Development Environment under Grant SKLSDE2019ZX-04. The work
   of L. Zheng was supported by the Australian Research Council Discovery
   Early Career Award (DE200101283) funded by the Australian Government.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2016, IEEE COMPUT SOC CONF, DOI DOI 10.1109/CVPRW.2016.14
   [Anonymous], 2016, ARXIV161108215
   Bazzani L., 2017, INT C LEARN REPR
   Bremond R., 2014, P T RES ARENA
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Chen L, 2011, IEEE INT VEH SYM, P908, DOI 10.1109/IVS.2011.5940543
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   Davies SJC, 2009, IEEE T MULTIMEDIA, V11, P39, DOI 10.1109/TMM.2008.2008916
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang J., 2019, DADA LARGE SCALE BEN
   Feng YL, 2013, IEEE T MULTIMEDIA, V15, P1865, DOI 10.1109/TMM.2013.2272918
   Fischer T, 2018, LECT NOTES COMPUT SC, V11214, P339, DOI 10.1007/978-3-030-01249-6_21
   Habenicht S, 2011, IEEE INT VEH SYM, P375, DOI 10.1109/IVS.2011.5940417
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Helmer T, 2015, IEEE INT C INTELL TR, P2019, DOI 10.1109/ITSC.2015.327
   Huang MX, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2546, DOI 10.1145/3025453.3025794
   Kellnhofer P, 2019, IEEE I CONF COMP VIS, P6911, DOI 10.1109/ICCV.2019.00701
   Krafka K, 2016, PROC CVPR IEEE, P2176, DOI 10.1109/CVPR.2016.239
   Lee KH, 2015, IEEE T MULTIMEDIA, V17, P1429, DOI 10.1109/TMM.2015.2455418
   Li NX, 2013, IEEE T MULTIMEDIA, V15, P1213, DOI 10.1109/TMM.2013.2241416
   Lv K, 2020, IEEE T IMAGE PROCESS, V29, P5163, DOI 10.1109/TIP.2020.2980130
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Mathe S, 2015, IEEE T PATTERN ANAL, V37, P1408, DOI 10.1109/TPAMI.2014.2366154
   Minut S., 2001, Proceedings of the Fifth International Conference on Autonomous Agents, P457, DOI 10.1145/375735.376414
   Mnih V, 2016, PR MACH LEARN RES, V48
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Palazzi A, 2019, IEEE T PATTERN ANAL, V41, P1720, DOI 10.1109/TPAMI.2018.2845370
   Pugeault N, 2015, IEEE T VEH TECHNOL, V64, P5424, DOI 10.1109/TVT.2015.2487826
   Rehder T, 2019, IEEE T INTELL VEHICL, V4, P265, DOI 10.1109/TIV.2019.2904386
   Sheng H, 2020, IEEE INTERNET THINGS, V7, P9611, DOI 10.1109/JIOT.2020.2980549
   Simon L, 2009, IEEE INT VEH SYM, P48, DOI 10.1109/IVS.2009.5164251
   Song G, 2018, PROC CVPR IEEE, P1760, DOI 10.1109/CVPR.2018.00189
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Takanobu R, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4403
   Uzkent B., 2020, P IEEE CVF C COMP VI, P12345
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wang ZY, 2016, PR MACH LEARN RES, V48
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhang XC, 2015, PROC CVPR IEEE, P4511, DOI 10.1109/CVPR.2015.7299081
   Zhang YT, 2016, IEEE T MULTIMEDIA, V18, P1604, DOI 10.1109/TMM.2016.2568138
   Zhong S.-h., 2013, AAAI Conference on Artificial Intelligence, P1063
NR 52
TC 18
Z9 18
U1 4
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4198
EP 4207
DI 10.1109/TMM.2020.3038311
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900022
OA hybrid
DA 2024-07-18
ER

PT J
AU Tsai, YY
AF Tsai, Yuan-Yu
TI Separable Reversible Data Hiding for Encrypted Three-Dimensional Models
   Based on Spatial Subdivision and Space Encoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Solid modeling; Three-dimensional displays; Data models; Encryption;
   Encoding; Distortion; Controllable distortion; encrypted
   three-dimensional (3D) models; separable reversible data hiding; space
   encoding; spatial subdivision
ID IMAGE; DIFFERENCE
AB Reversible data hiding for encrypted media not only preserves the privacy of the media content but also can convey additional information during message transmission. Some studies advocate separability, that is, the message can be correctly extracted irrespective of whether the encrypted media has been decrypted. However, current research has focused on encrypted images. Urgent research is required on encrypted three-dimensional (3D) models. This paper proposes a separable reversible data hiding method based on spatial subdivision and space encoding for encrypted 3D models. A bounding volume is first constructed using the vertices with boundary values in the processing model. Each vertex coordinate value is then converted into a ratio (between 0 and 1) of the distance between the vertex and minimum boundary point to the side length of the bounding volume. The owner of the 3D model then uses a secret key to encrypt all ratios except those of the boundary vertices to obtain an encrypted 3D model of the same size as the original model. The spatial subdivision technique and a subdivision threshold are subsequently used to divide the bounding volume into a series of blocks and simultaneously control the vertex distortion. The secret message is embedded in the encrypted vertex by using the space encoding method with an embedding threshold. Experimental results indicate that the proposed algorithm enables high privacy, performs separable reversible data hiding, and has low computational complexity, high embedding capacity, and controllable distortion.
C1 [Tsai, Yuan-Yu] Asia Univ, Dept M Commerce & Multimedia Applicat, Taichung 41354, Taiwan.
   [Tsai, Yuan-Yu] China Med Univ, China Med Univ Hosp, Dept Med Res, Taichung 404, Taiwan.
C3 Asia University Taiwan; China Medical University Taiwan; China Medical
   University Hospital - Taiwan
RP Tsai, YY (corresponding author), Asia Univ, Dept M Commerce & Multimedia Applicat, Taichung 41354, Taiwan.; Tsai, YY (corresponding author), China Med Univ, China Med Univ Hosp, Dept Med Res, Taichung 404, Taiwan.
EM yytsai@asia.edu.tw
OI Tsai, Yuan-Yu/0000-0001-7904-8637
FU Ministry of Science and Technology of Taiwan [MOST 107-2221-E-468-017,
   MOST 108-2221-E-468-022]
FX This work was supported by the Ministry of Science and Technology of
   Taiwan under the grant numbers MOST 107-2221-E-468-017 and MOST
   108-2221-E-468-022.
CR [Anonymous], 2017, IEEE C INF COMM TECH
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Chang JC, 2017, SIGNAL PROCESS, V133, P135, DOI 10.1016/j.sigpro.2016.11.003
   Chen YC, 2014, J VIS COMMUN IMAGE R, V25, P1164, DOI 10.1016/j.jvcir.2014.04.003
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Kumar K, 2016, IEEE CONF CLOUD COMP, P95, DOI [10.1109/CCEM.2016.025, 10.1109/CCEM.2016.24]
   Li L, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12030347
   Li M, 2017, SIGNAL PROCESS, V130, P190, DOI 10.1016/j.sigpro.2016.07.002
   Liao X, 2015, J VIS COMMUN IMAGE R, V28, P21, DOI 10.1016/j.jvcir.2014.12.007
   Qamar S, 2018, PROCEEDINGS OF THE 2018 SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS), P1814, DOI 10.1109/ICCONS.2018.8663159
   Qin C, 2015, J VIS COMMUN IMAGE R, V31, P154, DOI 10.1016/j.jvcir.2015.06.009
   Shah M, 2018, ARAB J SCI ENG, V43, P8145, DOI 10.1007/s13369-018-3354-4
   Sharma S., 2017, P 2 INT C COMP VIS I P 2 INT C COMP VIS I, P51
   Shiu CW, 2015, SIGNAL PROCESS-IMAGE, V39, P226, DOI 10.1016/j.image.2015.09.014
   Wang JX, 2019, IEEE ACCESS, V7, P35564, DOI 10.1109/ACCESS.2019.2903079
   Wu CP, 2005, IEEE T MULTIMEDIA, V7, P828, DOI 10.1109/TMM.2005.854469
   Wu XT, 2018, SIGNAL PROCESS, V143, P269, DOI 10.1016/j.sigpro.2017.09.017
   Wu XT, 2014, SIGNAL PROCESS, V104, P387, DOI 10.1016/j.sigpro.2014.04.032
   Yang HJ, 2007, IEEE T MULTIMEDIA, V9, P475, DOI 10.1109/TMM.2006.887990
   Yin Z., 2019, ARXIV190802473V2
   Yin ZX, 2017, MULTIMED TOOLS APPL, V76, P3899, DOI 10.1007/s11042-016-4049-z
   Zhang XP, 2016, IEEE T CIRC SYST VID, V26, P1622, DOI 10.1109/TCSVT.2015.2433194
   Zhang XP, 2014, J VIS COMMUN IMAGE R, V25, P322, DOI 10.1016/j.jvcir.2013.11.001
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
NR 26
TC 16
Z9 18
U1 4
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2286
EP 2296
DI 10.1109/TMM.2020.3009492
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800010
DA 2024-07-18
ER

PT J
AU Wang, YM
   Cai, ZC
   He, WG
AF Wang, Yaomin
   Cai, Zhanchuan
   He, Wenguang
TI High Capacity Reversible Data Hiding in Encrypted Image Based on
   Intra-Block Lossless Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encryption; Ciphers; Complexity theory; Streaming media; Correlation;
   Data mining; Information security; image encryption; reversible data
   hiding; block selection
ID EXPANSION; SCHEME
AB The cover image is generally encrypted by a stream cipher in existing reversible data hiding in encrypted image (RDHEI) methods. As pixel correlation is seriously damaged, more than one pixel should be employed to carry one bit such that the quite limited capacity is achieved. To overcome this issue, a new RDHEI method with high capacity, that preserves pixel correlation and exploits it to vacate embedding room, is proposed in this paper. First, we propose a block-level encryption scheme which combines block-level stream cipher and block-level permutation, and all blocks are classified into usable blocks (UBs) and unusable blocks (NUBs) by preserving the correlation of pixels in blocks. Then, UB is reconstructed to vacate room for data embedding, because the pixels in blocks share the same most significant bits (MSBs). To ensure reversibility, the number of NUBs between current UB and the previous one is also embedded along with additional data, and the blocks are rearranged in a reversible way such that UBs are always in front of NUBs. Experimental results show that not only the embedding capacity is significantly improved but also the hidden data can be losslessly extracted, and the cover image can be perfectly recovered.
C1 [Wang, Yaomin; Cai, Zhanchuan; He, Wenguang] Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
   [Wang, Yaomin; He, Wenguang] Guangdong Med Univ, Sch Biomed Engn, Zhanjiang 524023, Guangdong, Peoples R China.
C3 Macau University of Science & Technology; Guangdong Medical University
RP Cai, ZC (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
EM 249668530@qq.com; zccai@must.edu.mo; 56207403@qq.com
OI , Yaomin/0000-0003-4696-0907; he, wenguang/0000-0003-1051-389X
FU National Basic Research Program of China (973 Program) [2011CB302400];
   Science and Technology Development Fund of Macau [0038/2020/A,
   0012/2018/A1, 0069/2018/A2]; Open Project Program of State Key
   Laboratory of Virtual Reality Technology and Systems, Beihang University
   [VRLAB2019C02]; Open Fund of the State Key Laboratory of Remote Sensing
   Science [OFSLRSS201901]; Open Project Program of the State Key Lab of
   CAD & CG of Zhejiang University [A1910]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2011CB302400, in part by the Science
   and Technology Development Fund of Macau under Grants 0038/2020/A,
   0012/2018/A1, and 0069/2018/A2, in part by the Open Project Program of
   State Key Laboratory of Virtual Reality Technology and Systems, Beihang
   University under Grant VRLAB2019C02, in part by the Open Fund of the
   State Key Laboratory of Remote Sensing Science under Grant
   OFSLRSS201901, and in part by the Open Project Program of the State Key
   Lab of CAD & CG of Zhejiang University under Grant A1910. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Sanjeev Mehrotra.
CR Bhardwaj R., PATTERN RECOGN LETT
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chang CC, 2009, PATTERN RECOGN, V42, P1597, DOI 10.1016/j.patcog.2008.11.040
   Coltuc D, 2011, IEEE T INF FOREN SEC, V6, P873, DOI 10.1109/TIFS.2011.2145372
   Dragoi IC, 2014, IEEE T IMAGE PROCESS, V23, P1779, DOI 10.1109/TIP.2014.2307482
   Gui XL, 2014, SIGNAL PROCESS, V98, P370, DOI 10.1016/j.sigpro.2013.12.005
   Hong W, 2017, MULTIMED TOOLS APPL, V76, P5441, DOI 10.1007/s11042-016-4032-8
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Hou DD, 2018, SIGNAL PROCESS, V148, P41, DOI 10.1016/j.sigpro.2018.02.002
   Hu YJ, 2008, IEEE T MULTIMEDIA, V10, P1500, DOI 10.1109/TMM.2008.2007341
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Khanam FTZ, 2016, INT CONF UBIQ FUTUR, P869, DOI 10.1109/ICUFN.2016.7537160
   Li XL, 2015, IEEE T INF FOREN SEC, V10, P2016, DOI 10.1109/TIFS.2015.2444354
   Liao X, 2015, J VIS COMMUN IMAGE R, V28, P21, DOI 10.1016/j.jvcir.2014.12.007
   Lin CC, 2015, INFORM SCIENCES, V293, P314, DOI 10.1016/j.ins.2014.08.057
   Lu T. C., 2017, Multimedia Tools Appl., V76, P1
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Ou B, 2016, J VIS COMMUN IMAGE R, V39, P12, DOI 10.1016/j.jvcir.2016.05.005
   Ou B, 2013, J SYST SOFTWARE, V86, P2700, DOI 10.1016/j.jss.2013.05.077
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Qin C, 2013, IEEE T CIRC SYST VID, V23, P1109, DOI 10.1109/TCSVT.2012.2224052
   Sahu A.K., J KING SAUD U COMPUT
   Nguyen TS, 2016, SIGNAL PROCESS-IMAGE, V44, P84, DOI 10.1016/j.image.2016.03.010
   Wang JX, 2020, IEEE T CIRC SYST VID, V30, P2313, DOI 10.1109/TCSVT.2019.2915584
   Wang XF, 2017, MULTIMED TOOLS APPL, V76, P6127, DOI 10.1007/s11042-016-3288-3
   Weng SW, 2019, INFORM SCIENCES, V489, P136, DOI 10.1016/j.ins.2019.03.032
   Wu XT, 2014, SIGNAL PROCESS, V104, P387, DOI 10.1016/j.sigpro.2014.04.032
   Yang Y, 2016, DIGIT SIGNAL PROCESS, V52, P13, DOI 10.1016/j.dsp.2016.02.006
   Ying QC, 2019, IEEE ACCESS, V7, P46506, DOI 10.1109/ACCESS.2019.2909560
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
NR 33
TC 37
Z9 40
U1 4
U2 51
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1466
EP 1473
DI 10.1109/TMM.2020.2999187
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300001
DA 2024-07-18
ER

PT J
AU Zhao, PS
   Xie, LX
   Zhang, Y
   Tian, Q
AF Zhao, Peisen
   Xie, Lingxi
   Zhang, Ya
   Tian, Qi
TI Universal-to-Specific Framework for Complex Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; Three-dimensional displays; Task analysis; Feature
   extraction; Manganese; Solid modeling; Action recognition; feature
   representation; neural networks
AB Video-based action recognition has recently attracted much attention in the field of computer vision. To solve more complex recognition tasks, it has become necessary to distinguish different levels of interclass variations. Inspired by a common flowchart based on the human decision-making process that first narrows down the probable classes, and then applies a "rethinking" process for finer-level recognition, we propose an effective universal-to-specific (U2S) framework for complex action recognition. The U2S framework is composed of three subnetworks: a universal network, a category-specific network, and a mask network. The universal network first learns universal feature representations. The mask network then generates attention masks for confusing classes through category regularization based on the output of the universal network. The mask is further used to guide the category-specific network for class-specific feature representations. The entire framework is optimized in an end-to-end manner. Experiments on a variety of benchmark datasets, e.g., the Something-Something, UCF101, and HMDB51 datasets, demonstrate the effectiveness of the U2S framework; i.e., U2S can focus on discriminative spatiotemporal regions for confusing categories. We further visualize the relationship between different classes, showing that U2S indeed improves the discriminability of learned features. Moreover, the proposed U2S model is a general framework, and may adopt any base recognition network.
C1 [Zhao, Peisen; Zhang, Ya] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
   [Xie, Lingxi] Huawei Noahs Ark Lab, Shenzhen 518129, Guangdong, Peoples R China.
   [Tian, Qi] Huawei Noahs Ark Lab, Comp Vis, Shenzhen 518129, Guangdong, Peoples R China.
C3 Shanghai Jiao Tong University; Huawei Technologies; Huawei Technologies
RP Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
EM pszhao@sjtu.edu.cn; 198808xc@gmail.com; ya_zhang@sjtu.edu.cn;
   qitian@cs.utsa.edu
RI Zhang, Ya/Y-8255-2019
OI Zhang, Ya/0000-0002-5390-9053
FU National Key Research and Development Program of China [2019YFB1804304];
   SHEITC [2018-RGZN-02046]; 111 plan [BP0719010]; STCSM [18DZ2270700];
   State Key Laboratory of UHD Video and Audio Production and Presentation
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2019YFB1804304, in part by
   SHEITC under Grant 2018-RGZN-02046, in part by 111 plan under Grant
   BP0719010, in part by STCSM under Grant 18DZ2270700, and in part by the
   State Key Laboratory of UHD Video and Audio Production and Presentation.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. David Crandall.
CR Agethen S, 2020, IEEE T MULTIMEDIA, V22, P819, DOI 10.1109/TMM.2019.2932564
   [Anonymous], 2017, P GERM C PATT REC
   [Anonymous], 2018, IEEE T IMAGE PROCESS, DOI DOI 10.1109/TIP.2018.2791180
   [Anonymous], 2017, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen Peihao, 2019, IEEE T MULTIMEDIA
   Chuang Gan, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P5589, DOI 10.1109/CVPR.2018.00586
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan LJ, 2018, PROC CVPR IEEE, P6016, DOI 10.1109/CVPR.2018.00630
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang DA, 2018, PROC CVPR IEEE, P7366, DOI 10.1109/CVPR.2018.00769
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Huang WB, 2019, IEEE T IMAGE PROCESS, V28, P1773, DOI 10.1109/TIP.2018.2877936
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Kay W., 2017, CORR ABS170506950
   Khosla A., 2011, P CVPR WORKSH FIN GR
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee S., 2018, ARXIV180710037
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu MY, 2018, IEEE T MULTIMEDIA, V20, P1932, DOI 10.1109/TMM.2017.2786868
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Martinez B, 2019, IEEE I CONF COMP VIS, P5481, DOI 10.1109/ICCV.2019.00558
   Miech A., 2017, ARXIV PREPRINT ARXIV
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Shi XJ, 2015, ADV NEUR IN, V28
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soomro K., 2012, ARXIV12120402CS
   Sun L, 2017, IEEE I CONF COMP VIS, P2166, DOI 10.1109/ICCV.2017.236
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, P ECCV
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XL, 2016, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2016.291
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wei X-S, 2016, ARXIV PREPRINT ARXIV
   Wu CY, 2018, PROC CVPR IEEE, P6026, DOI 10.1109/CVPR.2018.00631
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xie Saining, 2017, ARXIV171204851, V1, P5
   Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zeng RH, 2019, IEEE T IMAGE PROCESS, V28, P5797, DOI 10.1109/TIP.2019.2922108
   Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297
   Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129
   Zhang SW, 2018, IEEE T MULTIMEDIA, V20, P769, DOI 10.1109/TMM.2017.2758524
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhao Yue., 2018, Advances in Neural Information Processing Systems, P2204
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zhou B., 2017, CoRR
   Zhu GM, 2019, IEEE T MULTIMEDIA, V21, P1011, DOI 10.1109/TMM.2018.2869278
   Zhu Y, 2018, AS C COMP VIS, P363, DOI DOI 10.1007/978-3-030-20893-6
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 69
TC 5
Z9 5
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3441
EP 3453
DI 10.1109/TMM.2020.3025665
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zheng, CQ
   Zhu, L
   Cheng, ZY
   Li, JJ
   Liu, AA
AF Zheng, Chaoqun
   Zhu, Lei
   Cheng, Zhiyong
   Li, Jingjing
   Liu, An-An
TI Adaptive Partial Multi-View Hashing for Efficient Social Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Training; Semantics; Feature extraction; Kernel;
   Adaptation models; Binary codes; Social image retrieval; partial
   multi-view hashing; discrete optimization; adaptive weights
ID BINARY-CODES; SCALE
AB Social networks allow users to actively upload images and descriptive tags, which has led to an explosive growth in the number of social images. Multi-view hashing is an efficient technique for supporting large-scale social image retrieval because of its desirable capabilities of encoding multi-view features into compact binary hash codes with extremely low storage costs and fast retrieval speeds. However, existing methods require multi-view features to be fully paired at both the offline model training and online query stages. This requirement cannot be easily satisfied for social image retrieval, where social images that lack descriptive tags are common in social networks. In this paper, we propose an Unsupervised Adaptive Partial Multi-view Hashing (UAPMH) method to handle the partial-view hashing problem for efficient social image retrieval. Specifically, the shared and view-specific latent representations of fully paired and partial-view images, respectively, are learned separately by an adaptive partial multi-view matrix factorization module within the identical semantic space. In particular, instead of adopting simple fixed view combination weights, we develop a parameter-free weight learning scheme to adaptively learn the weights to capture the view variations and the discriminative capabilities of different views. With such a design, our model can sufficiently exploit the available partial-view samples with separate hash code learning and effectively preserve the latent relations of images and tags in hash codes with semantic space sharing. Moreover, to avoid relaxing errors and improve the learning efficiency, binary hash codes are directly learned in a fast mode with simple and efficient operations. Finally, we extend UAPMH to the supervised learning paradigm as Supervised Adaptive Partial Multi-view Hashing (SAPMH) with the supervision of pair-wise semantic labels to further enhance the discriminative capability of hash codes. The experiments demonstrate the state-of-the-art performance of the proposed approaches on public social image retrieval datasets. Our source codes and testing datasets can be obtained at https://github.com/ChaoqunZheng/APMH.
C1 [Zheng, Chaoqun; Zhu, Lei] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
   [Cheng, Zhiyong] Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Jinan 250014, Peoples R China.
   [Li, Jingjing] Univ Elect Sci & Technol China, Chengdu 611731, Peoples R China.
   [Liu, An-An] Tianjin Univ, Sch Elect Informat Engn, Tianjin 300072, Peoples R China.
C3 Shandong Normal University; Qilu University of Technology; University of
   Electronic Science & Technology of China; Tianjin University
RP Zhu, L (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
EM cqzhengWORK@163.com; leizhu0608@gmail.com; jason.zy.cheng@gmail.com;
   lijin1l7@yeah.net; anan0422@gmail.com
RI Li, Jing/GYU-5036-2022; li, jy/HTT-1535-2023; LI, Jing/HNB-5575-2023;
   Zhu, Lei/GQQ-1130-2022; li, jian/IAQ-2794-2023
OI Zhu, Lei/0000-0002-5348-7532; Zhu, Lei/0000-0002-2993-7142; Zheng,
   Chaoqun/0000-0002-6799-5212
FU National Natural Science Foundation of China [61802236, 61772322,
   U1836216]; Natural Science Foundation of Shandong, China [ZR2019QF002];
   Major Fundamental Research Project of Shandong, China [ZR2019ZD03];
   Youth Innovation Project of Shandong Universities, China [2019KJN040];
   Taishan Scholar Project of Shandong, China [ts20190924]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61802236, 61772322, and U1836216, in
   part by the Natural Science Foundation of Shandong, China, under Grant
   ZR2019QF002, in part by the Major Fundamental Research Project of
   Shandong, China, under Grant ZR2019ZD03, in part by the Youth Innovation
   Project of Shandong Universities, China, under Grant 2019KJN040, and in
   part by the Taishan Scholar Project of Shandong, China, under Grant
   ts20190924.
CR Chen ZX, 2018, PROC CVPR IEEE, P6838, DOI 10.1109/CVPR.2018.00715
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Guo J, 2020, IEEE T IMAGE PROCESS, V29, P1344, DOI 10.1109/TIP.2019.2941858
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang QY, 2019, IEEE T IMAGE PROCESS, V28, P3490, DOI 10.1109/TIP.2019.2897944
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kang Y, 2012, IEEE DATA MINING, P930, DOI 10.1109/ICDM.2012.24
   Li N, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2397
   Liu H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1589, DOI 10.1145/3240508.3240684
   Liu L, 2015, IEEE T IMAGE PROCESS, V24, P956, DOI 10.1109/TIP.2015.2390975
   Liu XL, 2014, PATTERN RECOGN, V47, P748, DOI 10.1016/j.patcog.2013.08.022
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Lu X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P715, DOI 10.1145/3331184.3331217
   Mandal D, 2019, IEEE T IMAGE PROCESS, V28, P102, DOI 10.1109/TIP.2018.2863040
   Qiu Q, 2018, LECT NOTES COMPUT SC, V11206, P442, DOI 10.1007/978-3-030-01216-8_27
   ROSALESMACEDO HA, 1994, J OPER RES SOC, V45, P846, DOI 10.2307/2584294
   Sang JT, 2016, MULTIMEDIA SYST, V22, P1, DOI 10.1007/s00530-015-0482-5
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen XB, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3178119
   Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441
   Shen XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2733373.2806342
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Tang JH, 2018, IEEE T CIRC SYST VID, V28, P2730, DOI 10.1109/TCSVT.2017.2715227
   Tang JH, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P483, DOI 10.1145/2671188.2749307
   Wang DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2291
   Wang QF, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3904
   Wang Y, 2017, IEEE T IMAGE PROCESS, V26, P1393, DOI 10.1109/TIP.2017.2655449
   Wang Y, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P999, DOI 10.1145/2766462.2767825
   Wang YF, 2018, IEEE T SUSTAIN ENERG, V9, P1627, DOI 10.1109/TSTE.2018.2801625
   Wu L, 2019, IEEE T IMAGE PROCESS, V28, P1602, DOI 10.1109/TIP.2018.2878970
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yang EK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1064
   Yang R, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P180, DOI 10.1145/3078971.3078981
   Zhang CH, 2017, IEEE T IMAGE PROCESS, V26, P2604, DOI 10.1109/TIP.2017.2675205
   Zhang D, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P225
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zhu L, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3365841
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
   Zhu L, 2018, IEEE T NEUR NET LEAR, V29, P5264, DOI 10.1109/TNNLS.2018.2797248
   Zhu L, 2017, IEEE T MULTIMEDIA, V19, P2066, DOI 10.1109/TMM.2017.2729025
NR 43
TC 24
Z9 26
U1 5
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4079
EP 4092
DI 10.1109/TMM.2020.3037456
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900013
DA 2024-07-18
ER

PT J
AU Qiu, HQ
   Li, HL
   Wu, QB
   Meng, FM
   Xu, LF
   Ngan, KN
   Shi, HC
AF Qiu, Heqian
   Li, Hongliang
   Wu, Qingbo
   Meng, Fanman
   Xu, Linfeng
   Ngan, King Ngi
   Shi, Hengcan
TI Hierarchical Context Features Embedding for Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Object detection; Logic gates; Image segmentation;
   Convolution; Semantics; Decoding; Segmentation features; object
   detection; hierarchical context embedding module; gated encoder-decoder
   network
AB Pixel-level segmentation has been widely used to improve object detection. Most of the existing methods refine detection features by adding the constraint of the segmentation branch or by simply embedding high-level segmentation features into detection features within the local receptive field. However, noisy segmentation features are unavoidable in real-word applications and can easily cause false positives. To address this problem, we propose a novel hierarchical context embedding module to effectively embed segmentation features into detection features. The idea of this module is to capture hierarchical context information that includes local objects or parts and nonlocal context features by learning multiple attention maps, and subsequently utilize interdependencies between features to recalibrate noisy segmentation features. Furthermore, we use this module in the proposed gated encoder-decoder network that adaptively aggregates feature maps of different resolutions based on the gate mechanism so that we can embed multiscale segmentation feature maps into detection features for more accurate detection of objects of all sizes. Experimental results demonstrate the effectiveness of the proposed method on the Pascal VOC 2012Seg dataset, the Pascal VOC dataset and the MS COCO dataset.
C1 [Qiu, Heqian; Li, Hongliang; Wu, Qingbo; Meng, Fanman; Xu, Linfeng; Ngan, King Ngi; Shi, Hengcan] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Li, HL (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM hqqiu@std.uestc.edu.cn; hlli@uestc.edu.cn; qbwu@uestc.edu.cn;
   fmmeng@std.uestc.edu.cn; lfxu@uestc.edu.cn; knngan@uestc.edu.cnLife;
   shihc@std.uestc.edu.cn
RI Ngan, N/E-8240-2014; Wu, Qingbo/AAF-6872-2019; Xu, Linfeng/HME-1913-2023
OI Ngan, N/0000-0003-1946-3235; Wu, Qingbo/0000-0003-2936-6340; Xu,
   Linfeng/0000-0002-9934-0958; Qiu, Heqian/0000-0002-0963-0311; Li,
   Hongliang/0000-0002-7481-095X
FU National Natural Science Foundation of China [61831005, 61525102]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61831005 and 61525102. The associate
   editor coordinating the review of this manuscript and approving it for
   publicationwas Dr. Zhu Liu.
CR [Anonymous], IEEE I CONF COMP VIS
   [Anonymous], IEEE I CONF COMP VIS
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], IEEE I CONF COMP VIS
   [Anonymous], IEEE T MULTIMEDIA
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], IEEE I CONF COMP VIS
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2018, P EUR C COMP VIS
   [Anonymous], INT J COMPUT VISION
   [Anonymous], 2018, ADV NEUR IN, DOI DOI 10.5555/3327757.3327944
   [Anonymous], PROC CVPR IEEE
   [Anonymous], PROC CVPR IEEE
   [Anonymous], INT J COMPUT VISION
   [Anonymous], P 2016 IEEE INT C
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2018, PALG S RACE ETHN IND, DOI DOI 10.1007/978-1-349-95807-8
   [Anonymous], PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2018.00745, DOI 10.1109/TPAMI.2019.2913372]
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Brazil G, 2017, IEEE I CONF COMP VIS, P4960, DOI 10.1109/ICCV.2017.530
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Z, 2018, LECT NOTES COMPUT SC, V11212, P74, DOI 10.1007/978-3-030-01237-3_5
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Dauphin YN, 2017, PR MACH LEARN RES, V70
   Dvornik N, 2017, IEEE I CONF COMP VIS, P4174, DOI 10.1109/ICCV.2017.447
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fidler S, 2013, PROC CVPR IEEE, P3294, DOI 10.1109/CVPR.2013.423
   Garg S, 2019, IEEE T MULTIMEDIA, V21, P566, DOI 10.1109/TMM.2019.2893549
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342
   Krizhevsky A., 2012, ADV NEURAL INF PROCE, V25, P1097
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma L, 2017, IEEE T MULTIMEDIA, V19, P2545, DOI 10.1109/TMM.2017.2703089
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren S., 2017, IEEETransactionsonPatternAnalysisandMachineIntelligence, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Sutskever I, 2014, ADV NEUR IN, V27
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Tian YL, 2015, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2015.7299143
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Zhang ZS, 2018, PROC CVPR IEEE, P5813, DOI 10.1109/CVPR.2018.00609
   Zhao XY, 2018, PROC CVPR IEEE, P4061, DOI 10.1109/CVPR.2018.00427
   Zhou K, 2016, DESTECH TRANS COMP
   Zhu Y, 2015, PROC CVPR IEEE, P4703, DOI 10.1109/CVPR.2015.7299102
NR 64
TC 30
Z9 30
U1 2
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3039
EP 3050
DI 10.1109/TMM.2020.2971175
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700002
DA 2024-07-18
ER

PT J
AU Wang, W
   Alameda-Pineda, X
   Xu, D
   Ricci, E
   Sebe, N
AF Wang, Wei
   Alameda-Pineda, Xavier
   Xu, Dan
   Ricci, Elisa
   Sebe, Nicu
TI Learning How to Smile: Expression Video Generation With Conditional
   Adversarial Recurrent Nets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face; Generators; Generative adversarial networks; Manifolds; Solid
   modeling; Three-dimensional displays; Visualization; Video generation;
   gated recurrent unit; smile
AB While several research studies have focused on analyzing human behavior and, in particular, emotional signals from visual data, the problem of synthesizing face video sequences with specific attributes (e.g. age, facial expressions) received much less attention. This paper proposes a novel deep generative model able to produce face videos from a given image of a neutral face and a label indicating a specific facial expression, e.g. spontaneous smile. Our framework consists of two main building blocks: an image generator and a frame sequence generator. The image generator is implemented as a deep neural model which combines generative adversarial networks and variational auto-encoders, while the sequence generator is a label-conditioned recurrent neural network. In the proposed framework, given as input a neural face and a label, the sequence generator outputs a set of hidden representations with smooth transitions corresponding to video frames. Then, the image generator is used to decode the hidden representations into the actual face images. To impose that the net generates videos consistent with the given label, a novel identity adversarial loss is proposed. Our experimental results demonstrate the effectiveness of the framework and the advantage of introducing an adversarial component into recurrent models for face video generation.
C1 [Wang, Wei] Ecole Polytech Fed Lausanne, CVLAB, CH-1015 Lausanne, Switzerland.
   [Alameda-Pineda, Xavier] Inria Ctr Rech Grenoble Rhone Alpes, Percept Team, F-38334 Grenoble, France.
   [Xu, Dan] Univ Oxford, VGG Grp, Oxford OX1 3PJ, England.
   [Ricci, Elisa; Sebe, Nicu] Univ Trento, I-38123 Trento, Italy.
   [Ricci, Elisa] Fdn Bruno Kessler, I-38123 Trento, Italy.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; University of Oxford; University of Trento;
   Fondazione Bruno Kessler
RP Wang, W (corresponding author), Ecole Polytech Fed Lausanne, CVLAB, CH-1015 Lausanne, Switzerland.
EM wei.wang@epfl.ch; xavier.alameda-pineda@inria.fr; dan.xu@unitn.it;
   eliricci@fbk.eu; sebe@disi.unitn.it
RI Xu, Dan/KPA-7396-2024; Wang, Wei/AAK-5521-2021; Sebe,
   Niculae/KEC-2000-2024; Ricci, Elisa/IYS-6532-2023
OI Xu, Dan/0000-0003-4602-3550; Wang, Wei/0000-0002-5477-1017; Sebe,
   Niculae/0000-0002-6597-7248; Ricci, Elisa/0000-0002-0228-1147;
   Alameda-Pineda, Xavier/0000-0002-5354-1084
FU IDEX of Universite Grenoble Alpes; Agence Nationale de la Recherche on
   the project ML3RI
FX X. Alameda-Pineda acknowledges the support from the IDEX of Universite
   Grenoble Alpes and from the Agence Nationale de la Recherche on the
   project ML3RI.
CR [Anonymous], 2016, PROC INT C MULTIMEDI
   [Anonymous], 2018, 32 AAAI C ART INT
   [Anonymous], 2016, ARXIV
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Arjovsky M, 2017, arXiv preprint arXiv:1701.07875
   Baltrusaitis T, 2016, IEEE WINT CONF APPL
   Baltrusaitis T, 2015, IEEE INT CONF AUTOMA
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Calixto I, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1913, DOI 10.18653/v1/P17-1175
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Dibeklioglu H, 2012, LECT NOTES COMPUT SC, V7574, P525, DOI 10.1007/978-3-642-33712-3_38
   Ertugrul IÖ, 2017, IEEE INT CONF AUTOMA, P33, DOI 10.1109/FG.2017.14
   Gonçalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021
   Gong B., 2009, Proceedings of the 17th ACM International Conference on Multimedia, P569
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goyal P, 2017, IEEE I CONF COMP VIS, P5104, DOI 10.1109/ICCV.2017.545
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Hou XX, 2017, IEEE WINT CONF APPL, P1133, DOI 10.1109/WACV.2017.131
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kalchbrenner Nal, 2017, INT C MACHINE LEARNI, P1771
   Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335
   Kulkarni TD, 2015, ADV NEUR IN, V28
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li M., 2016, ARXIV161005586
   Liu XM, 2009, IEEE T PATTERN ANAL, V31, P1941, DOI 10.1109/TPAMI.2008.238
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mavadati M, 2016, IEEE COMPUT SOC CONF, P1452, DOI 10.1109/CVPRW.2016.182
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   Oh J., 2015, P NEURIPS, P2863
   Park SY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P911, DOI 10.1145/2733373.2806362
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Qi G.-J., 2019, INT C DIG SIGN PROC, V27, P1
   Radford A, 2016, 4 INT C LEARNING REP
   Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308
   Salimans T, 2015, PR MACH LEARN RES, V37, P1218
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sonderby CK, 2016, ADV NEUR IN, V29
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   Vondrick C, 2016, 30 C NEURAL INFORM P, V29
   Wagner A, 2012, IEEE T PATTERN ANAL, V34, P372, DOI 10.1109/TPAMI.2011.112
   Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51
   Wen T.-H., 2015, P 2015 C EMP METH NA, P1711, DOI DOI 10.18653/V1/D15-1199
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Yu LT, 2017, AAAI CONF ARTIF INTE, P2852
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   Zhou YP, 2016, LECT NOTES COMPUT SC, V9912, P262, DOI 10.1007/978-3-319-46484-8_16
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
NR 64
TC 9
Z9 9
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2808
EP 2819
DI 10.1109/TMM.2019.2963621
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900005
DA 2024-07-18
ER

PT J
AU Yang, JC
   Zhao, Y
   Jiang, B
   Lu, W
   Gao, XB
AF Yang, Jiachen
   Zhao, Yang
   Jiang, Bin
   Lu, Wen
   Gao, Xinbo
TI No-Reference Quality Evaluation of Stereoscopic Video Based on
   Spatio-Temporal Texture
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stereoscopic video quality; key-frame sequences; movement intensity;
   motion information; spatio-temporal texture; local binary patterns from
   three orthogonal planes (LBP-TOP)
ID PREDICTION; VISIBILITY; REGRESSION
AB Due to the wide application of stereoscopic display technology, stereoscopic video quality assessment (SVQA) is facing great challenges, but worthwhile. Stereoscopic videos contain a great deal of information, which involves not only the spatial domain but also the spatio-temporal domain. Motion in stereoscopic video plays a critical role in quality perception, while the existing SVQA methods rarely refer to motion factors, and the performance of these methods is restrained. In this article, a novel SVQA based on motion perception is introduced and its performance is superior to that of existing excellent methods. Particularly, to appropriately reduce the amount of data processing, we extract the key-frame sequences according to the influence of movement intensity on binocular visual quality perception. The binocular summation and difference operations are implemented on extracted sequences, and then spatial texture and spatio-temporal texture statistic measurement are extracted simultaneously with local binary patterns from three orthogonal planes (LBP-TOP). Experiments are implemented on two publicly available databases and the results demonstrate the effectiveness and robustness of our algorithm for various categories of distortion stereoscopic video pairs.
C1 [Yang, Jiachen; Zhao, Yang; Jiang, Bin] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Lu, Wen] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Gao, Xinbo] Xidian Univ, State Key Lab Integrated Serv Networks, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Tianjin University; Xidian University; Xidian University
RP Zhao, Y (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM yangjiachen@tju.edu.cn; yangzhao321@tju.edu.cn; jiangbin@tju.edu.cn;
   luwen@mail.xidian.edu.cn; xbgao@mail.xidian.edu.cn
RI Yang, Jiachen/ABH-5032-2020; , Yang/AAT-4192-2021
OI Yang, Jiachen/0000-0003-2558-552X; , Yang/0000-0002-7302-7787
FU National Natural Science Foundation of China [61871283]; Foundation of
   Pre-Research on Equipment of China [61400010304]; Major Civil-Military
   Integration Project in Tianjin, China [18ZXJMTG00170]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61871283, in part by the Foundation of
   Pre-Research on Equipment of China under Grant 61400010304, and in part
   by the Major Civil-Military Integration Project in Tianjin, China, under
   Grant 18ZXJMTG00170.
CR [Anonymous], 2023, 10 INT C QUALITYMULT, DOI DOI 10.1109/QOMEX.2018.8463426
   Appina B, 2018, IEEE SIGNAL PROC LET, V25, P823, DOI 10.1109/LSP.2018.2829107
   Chen CG, 2010, PROC IEEE INT SYMP, P1321, DOI 10.1109/ISIE.2010.5637112
   Chen Y, 2018, IEEE ACCESS, V6, P60456, DOI 10.1109/ACCESS.2018.2875951
   Chen ZB, 2018, IEEE T IMAGE PROCESS, V27, P721, DOI 10.1109/TIP.2017.2766780
   Chou CH, 1996, IEEE T CIRC SYST VID, V6, P143, DOI 10.1109/76.488822
   Ding Y, 2018, APPL OPTICS, V57, P2610, DOI 10.1364/AO.57.002610
   Gao F, 2018, PATTERN RECOGN, V81, P432, DOI 10.1016/j.patcog.2018.04.016
   Gao F, 2016, SIGNAL PROCESS, V124, P210, DOI 10.1016/j.sigpro.2015.08.012
   Gu J, 2018, IEEE T MULTIMEDIA, V20, P1140, DOI 10.1109/TMM.2017.2761993
   Guo YM, 2013, IEEE T IMAGE PROCESS, V22, P3879, DOI 10.1109/TIP.2013.2263148
   Hong WH, 2018, IEEE SIGNAL PROC LET, V25, P214, DOI 10.1109/LSP.2017.2780285
   JAYANT N, 1993, P IEEE, V81, P1385, DOI 10.1109/5.241504
   Jiang GY, 2018, J VIS COMMUN IMAGE R, V50, P247, DOI 10.1016/j.jvcir.2017.12.001
   Kingdom FAA, 2012, CURR BIOL, V22, pR22, DOI 10.1016/j.cub.2011.11.048
   Krüger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   LIMB JO, 1978, IEEE T COMMUN, V26, P573, DOI 10.1109/TCOM.1978.1094114
   Liu LX, 2019, IEEE T MULTIMEDIA, V21, P2305, DOI 10.1109/TMM.2019.2900941
   Liu TJ, 2019, IEEE ACCESS, V7, P8058, DOI 10.1109/ACCESS.2018.2890304
   Liu TJ, 2018, IEEE T IMAGE PROCESS, V27, P1138, DOI 10.1109/TIP.2017.2771422
   Liu XK, 2015, IEEE T IMAGE PROCESS, V24, P4847, DOI 10.1109/TIP.2015.2469140
   Liu YT, 2020, IEEE T CIRC SYST VID, V30, P929, DOI 10.1109/TCSVT.2019.2900472
   May KA, 2016, CURR BIOL, V26, P1571, DOI 10.1016/j.cub.2016.04.037
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2013, SIGNAL PROCESS-IMAGE, V28, P870, DOI 10.1016/j.image.2012.08.004
   Orban GA, 2008, PHYSIOL REV, V88, P59, DOI 10.1152/physrev.00008.2007
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Qi F, 2016, SIGNAL IMAGE VIDEO P, V10, P737, DOI 10.1007/s11760-015-0802-4
   Qi F, 2013, IEEE IMAGE PROC, P34, DOI 10.1109/ICIP.2013.6738008
   Qian YT, 2013, IEEE T GEOSCI REMOTE, V51, P2276, DOI 10.1109/TGRS.2012.2209657
   Saad MA, 2011, IEEE IMAGE PROC
   Shao F, 2018, IEEE T MULTIMEDIA, V20, P659, DOI 10.1109/TMM.2017.2748460
   Urvoy M, 2012, INT WORK QUAL MULTIM, P109, DOI 10.1109/QoMEX.2012.6263847
   Wang JH, 2017, IEEE T IMAGE PROCESS, V26, P1330, DOI 10.1109/TIP.2017.2651387
   Wang JH, 2015, IEEE T IMAGE PROCESS, V24, P3400, DOI 10.1109/TIP.2015.2446942
   Wang YF, 2019, NEUROCOMPUTING, V332, P298, DOI 10.1016/j.neucom.2018.12.029
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei H, 2011, NAT COMMUN, V2, DOI 10.1038/ncomms1388
   Wu QB, 2018, IEEE T BROADCAST, V64, P367, DOI 10.1109/TBC.2017.2786023
   Yan QS, 2019, IEEE T IMAGE PROCESS, V28, P2200, DOI 10.1109/TIP.2018.2883741
   Yang JC, 2018, IEEE T BROADCAST, V64, P341, DOI 10.1109/TBC.2018.2789583
   Yang JC, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0145800
   Yu M, 2016, J VIS COMMUN IMAGE R, V38, P246, DOI 10.1016/j.jvcir.2016.03.010
   Zhang Y, 2020, IEEE T NEUR NET LEAR, V31, P2716, DOI 10.1109/TNNLS.2018.2890310
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhou WJ, 2017, IEEE T BROADCAST, V63, P404, DOI 10.1109/TBC.2016.2638620
NR 49
TC 16
Z9 16
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2635
EP 2644
DI 10.1109/TMM.2019.2961209
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000010
DA 2024-07-18
ER

PT J
AU Zhang, LM
   Ju, XM
   Yao, YY
   Liu, ZG
AF Zhang, Luming
   Ju, Xiaoming
   Yao, Yiyang
   Liu, Zhenguang
TI Massive-Scale Genre Communities Learning Using a Noise-Tolerant Deep
   Architecture
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Massive-scale; aesthetic community; contaminated; stable templates;
   aggregation network
ID SCENE
AB Accurately categorizing million-scale Internet users (e.g., Flickr or Google Picasa) into multiple communities based on their genre tastes is an indispensable techniquein machine learning and multimedia. It can facilitate a series of applications, such as fashion recommendation and 3D non-realistic photo rendering. Conventional methods cannot handle this task appropriately because of the inherent contaminated image labels, which are produced by auxiliary image label predictors. In this article, we propose a noise-tolerant deep architecture which optimally encodes stable templates,(1) discovered from a collection of images with contaminated semantic labels. Specifically, we first construct a semantic space by encoding image labels using manifold embedding. Afterward, we observe that in the semantic space, the distribution of superpixels from images with the same label remains stable, regardless of the noises from image labels. According to this observation, a probabilistic generative model (Hidden Stable Analysis) is proposed to learn the stable templates toward each image label. To globally represent the composition of a user's images, a deep aggregation network is developed which statistically concatenates the CNN features learned from all its generated stable templates. Subsequently, an affinity graph is built, in which the genre difference among users is determined by their deep features. Finally, we employ a dense subgraph discovery technique which effectively mines the communities toward various genre tastes. Experiments on a million-scale image set (>1.4 million) compiled from Flickr have demonstrated the effectiveness of our method. Additionally, empirical study on the 33 SIFT-flow categories have shown that the detected stable templates maintain almost unchanged under nearly 32% contaminated image labels.
C1 [Zhang, Luming; Yao, Yiyang] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
   [Ju, Xiaoming] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200241, Peoples R China.
   [Liu, Zhenguang] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
C3 Zhejiang University; East China Normal University; National University
   of Singapore
RP Ju, XM (corresponding author), East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200241, Peoples R China.
EM zglumg@gmail.com; xmju@sei.ecnu.edu.cn; yiyangyao1778@126.com;
   liuzhenguang2008@gmail.com
RI Lei, Ming/JAD-1050-2023; zhang, lu/GRO-2969-2022
FU National Key R&D Program of China [2018YFB1403600]
FX This work was supported by the National Key R&D Program of China under
   Grant 2018YFB1403600. The associate editor coordinating the reviewof
   this manuscript and approving it for publication was Dr. Marco Bertini.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ahn YY, 2010, NATURE, V466, P761, DOI 10.1038/nature09182
   Airoldi EM, 2008, J MACH LEARN RES, V9, P1981
   [Anonymous], 2010, P 24 AAAI C ART INT
   [Anonymous], 2009, 2009 INT C INF MAN, DOI DOI 10.1109/ICIII.2009.59
   Balasubramanyan R., 2011, P SIAM INT C DAT MIN
   Cai D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P714
   Chen Y.-W., 2005, SPE EUR FORM DAM C 2
   Cheng B., 2010, P INT C ACM MULT, P291
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Frank M, 2012, J MACH LEARN RES, V13, P459
   Gregory S, 2008, LECT NOTES ARTIF INT, V5211, P408, DOI 10.1007/978-3-540-87479-9_45
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Hong RC, 2016, IEEE T MULTIMEDIA, V18, P1555, DOI 10.1109/TMM.2016.2567071
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P1124, DOI 10.1109/TIP.2016.2514499
   JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588
   Kimura A., 2013, P INT C ACM MULT, P565
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuhn H. W., 1956, NAVAL RES LOGIST Q, V24, P253
   Lancichinetti A, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.046110
   Lee YJ, 2012, IEEE T PATTERN ANAL, V34, P346, DOI 10.1109/TPAMI.2011.122
   Leskovec J., 2010, P INT C WORLD WID WE, P631
   Li YX, 2012, IEEE T MULTIMEDIA, V14, P1618, DOI 10.1109/TMM.2012.2199292
   Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Lu ZW, 2017, IEEE T PATTERN ANAL, V39, P486, DOI 10.1109/TPAMI.2016.2552172
   MacKay D. J. C., 2002, INFORM THEORY INFERE
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Negoescu RA, 2010, IEEE T MULTIMEDIA, V12, P399, DOI 10.1109/TMM.2010.2050649
   Ni BB, 2013, IEEE T MULTIMEDIA, V15, P1138, DOI 10.1109/TMM.2013.2241042
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Palla G, 2005, NATURE, V435, P814, DOI 10.1038/nature03607
   Papadimitriou S, 2008, LECT NOTES ARTIF INT, V5212, P170, DOI 10.1007/978-3-540-87481-2_12
   Rasiwasia N, 2012, IEEE T PATTERN ANAL, V34, P902, DOI 10.1109/TPAMI.2011.175
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Yang J, 2012, IEEE DATA MINING, P1170, DOI 10.1109/ICDM.2012.139
   Yang TB, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P927
   Yoshida T, 2010, IEEE IMAGE PROC, P349, DOI 10.1109/ICIP.2010.5651018
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, P2235, DOI 10.1109/TIP.2014.2311658
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
   Zhang YZ, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P997
NR 43
TC 0
Z9 0
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2467
EP 2478
DI 10.1109/TMM.2019.2955240
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200020
DA 2024-07-18
ER

PT J
AU Xu, BJ
   Li, JN
   Wong, YK
   Zhao, Q
   Kankanhalli, MS
AF Xu, Bingjie
   Li, Junnan
   Wong, Yongkang
   Zhao, Qi
   Kankanhalli, Mohan S.
TI Interact as You Intend: Intention-Driven Human-Object Interaction
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Object detection; Image recognition;
   Feature extraction; Computational modeling; Semantics; Human-Object
   Interactions (HOIs); Intention-Driven Analysis; Visual Relationships
ID EYE; MOVEMENTS; HEAD
AB The recent advances in instance-level detection tasks lay strong foundation for genuine comprehension of the visual scenes. However, the ability to fully comprehend a social scene is still in its preliminary stage. In this work, we focus on detecting human-object interactions (HOIs) in social scene images, which is demanding in terms of research and increasingly useful for practical applications. To undertake social tasks interacting with objects, humans direct their attention and move their body based on their intention. Based on this observation, we provide a unique computational perspective to explore human intention in HOI detection. Specifically, the proposed human intention-driven HOI detection (iHOI) framework models human pose with the relative distances from body joints to the object instances. It also utilizes human gaze to guide the attended contextual regions in a weakly-supervised setting. In addition, we propose a hard negative sampling strategy to address the problem of mis-grouping. We perform extensive experiments on two benchmark datasets, namely V-COCO and HICO-DET. The efficacy of each proposed component has also been validated.
C1 [Xu, Bingjie; Li, Junnan] Natl Univ Singapore, Grad Sch Integrat Sci & Engn, Singapore 119077, Singapore.
   [Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Zhao, Qi] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
C3 National University of Singapore; National University of Singapore;
   University of Minnesota System; University of Minnesota Twin Cities
RP Xu, BJ (corresponding author), Natl Univ Singapore, Grad Sch Integrat Sci & Engn, Singapore 119077, Singapore.
EM bingjiexu@u.nus.edu; e0008178@u.nus.edu; yongkang.wong@nus.edu.sg;
   qzhao@cs.umn.edu; mohan@comp.nus.edu.sg
RI zhao, qi/KGK-3760-2024; Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015; Wong,
   Yongkang/0000-0002-1239-4428
FU National Research Foundation; PrimeMinister's Office, Singapore under
   its Strategic Capability Research Centres Funding Initiative
FX This work was supported by the National Research Foundation,
   PrimeMinister's Office, Singapore under its Strategic Capability
   Research Centres Funding Initiative. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Federica Battisti.
CR Bordes A., 2013, P 26 INT C NEURAL IN, P2787
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chao YW, 2018, IEEE WINT CONF APPL, P381, DOI 10.1109/WACV.2018.00048
   Chao YW, 2015, IEEE I CONF COMP VIS, P1017, DOI 10.1109/ICCV.2015.122
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Fan LF, 2018, PROC CVPR IEEE, P6460, DOI 10.1109/CVPR.2018.00676
   Fathi A, 2012, LECT NOTES COMPUT SC, V7572, P314, DOI 10.1007/978-3-642-33718-5_23
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick Ross, 2018, Detectron
   Gkioxari G, 2018, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2018.00872
   Gkioxari G, 2015, IEEE I CONF COMP VIS, P1080, DOI 10.1109/ICCV.2015.129
   Gorji S, 2017, PROC CVPR IEEE, P3472, DOI 10.1109/CVPR.2017.370
   Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83
   Gupta Saurabh, 2015, ARXIV150504474, P5
   Hayes Bradley, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P6586, DOI 10.1109/ICRA.2017.7989778
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu JF, 2013, IEEE I CONF COMP VIS, P3144, DOI 10.1109/ICCV.2013.390
   Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325
   Kato K, 2018, LECT NOTES COMPUT SC, V11218, P247, DOI 10.1007/978-3-030-01264-9_15
   Krishna R, 2018, PROC CVPR IEEE, P6867, DOI 10.1109/CVPR.2018.00718
   Land MF, 2001, VISION RES, V41, P3559, DOI 10.1016/S0042-6989(01)00102-X
   Li JN, 2017, IEEE I CONF COMP VIS, P2669, DOI 10.1109/ICCV.2017.289
   Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142
   Li YL, 2018, INT C NUMER SIMUL, P41, DOI 10.1109/NUSOD.2018.8570220
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Malle BF, 1997, J EXP SOC PSYCHOL, V33, P101, DOI 10.1006/jesp.1996.1314
   Mukherjee SS, 2015, IEEE T MULTIMEDIA, V17, P2094, DOI 10.1109/TMM.2015.2482819
   Pelz J, 2001, EXP BRAIN RES, V139, P266, DOI 10.1007/s002210100745
   Ping Wei, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6801, DOI 10.1109/CVPR.2018.00711
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Recasens A., 2015, P ADV NEUR INF PROC, P199, DOI DOI 10.1038/SCIENTIFICAMERICAN0700-38
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen LY, 2018, IEEE WINT CONF APPL, P1568, DOI 10.1109/WACV.2018.00181
   van Boxtel JJA, 2010, FRONT PSYCHOL, V1, DOI 10.3389/fpsyg.2010.00217
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang L, 2017, IEEE T MULTIMEDIA, V19, P646, DOI 10.1109/TMM.2016.2617079
   Wang S, 2015, NEURON, V88, P604, DOI 10.1016/j.neuron.2015.09.042
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang XS, 2018, IEEE T MULTIMEDIA, V20, P2100, DOI 10.1109/TMM.2017.2788210
   Yao BP, 2012, IEEE T PATTERN ANAL, V34, P1691, DOI 10.1109/TPAMI.2012.67
   Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhuang BH, 2018, AAAI CONF ARTIF INTE, P7631
NR 45
TC 54
Z9 60
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1423
EP 1432
DI 10.1109/TMM.2019.2943753
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chai, XL
   Shao, F
   Jiang, QP
   Ho, YS
AF Chai, Xiongli
   Shao, Feng
   Jiang, Qiuping
   Ho, Yo-Sung
TI MSTGAR: Multioperator-Based Stereoscopic Thumbnail Generation With
   Arbitrary Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stereo image processing; Image resolution; Energy resolution;
   Visualization; Three-dimensional displays; Two dimensional displays;
   Strain; Stereoscopic thumbnails; image warping; image cropping; image
   resolution; depth adaptation
ID VISUAL-ATTENTION; IMAGE
AB At present, thumbnail generation for 2D images has been extensively studied, but the research in thumbnail generation for stereoscopic images is still relatively lacking. This paper presents a novel thumbnail generation technology for stereoscopic images based on multioperator with the following innovations: 1) The warping technique is used to retarget a stereopair into six-scale resolutions with different contexts, and the disparity is uniformly adjusted to a certain value based on just noticeable depth difference (JNDD) model, which overcomes the issues that 3D perception in stereoscopic thumbnail is uncontrollable and the sense of depth disappears in low-resolution stereoscopic images. 2) The six-scale images are cropped via cropping network, and are optimized to a target resolution based on the designed image visual representation energy. As a result, our method has better visual effect than state-of-the-art methods in generating thumbnail for stereoscopic display.
C1 [Chai, Xiongli; Shao, Feng; Jiang, Qiuping] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol, Sch Informat & Commun, Gwangju 500712, South Korea.
C3 Ningbo University; Gwangju Institute of Science & Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM 747866472@qq.com; shaofeng@nbu.edu.cn; jiangqiuping@nbu.edu.cn;
   hoyo@gist.ac.kr
RI Jiang, Qiuping/AAL-8273-2020
OI Chai, Xiongli/0000-0002-4245-5391; HO, YO-SUNG/0000-0002-7220-1034
FU Natural Science Foundation of China [61622109, 61901236]; Zhejiang
   Natural Science Foundation of China [R18F010008]; K. C. Wong Magna Fund
   in Ningbo University
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61622109 and 61901236, in part by the Zhejiang
   Natural Science Foundation of China under Grant R18F010008, and in part
   by K. C. Wong Magna Fund in Ningbo University. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Marco Carli.
CR Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Chai XL, 2019, IEEE ACCESS, V7, P25239, DOI 10.1109/ACCESS.2019.2896918
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Chen JS, 2016, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2016.61
   Cheng HG, 2018, POSTGRAD MED, V130, P568, DOI 10.1080/00325481.2018.1495541
   De Silva DVSX, 2011, IEEE J-STSP, V5, P335, DOI 10.1109/JSTSP.2011.2108113
   Dekel T, 2013, IEEE T PATTERN ANAL, V35, P2513, DOI 10.1109/TPAMI.2013.46
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Esmaeili SA, 2017, PROC CVPR IEEE, P4178, DOI 10.1109/CVPR.2017.445
   Furuta R, 2018, IEEE T CIRC SYST VID, V28, P1087, DOI 10.1109/TCSVT.2016.2620563
   Guo GJ, 2018, IEEE T MULTIMEDIA, V20, P2073, DOI 10.1109/TMM.2018.2794262
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hsu CC, 2014, IEEE J-STSP, V8, P377, DOI 10.1109/JSTSP.2014.2311884
   Huang JW, 2015, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2015.37
   Islam MB, 2018, IEEE T MULTIMEDIA, V20, P2964, DOI 10.1109/TMM.2018.2820324
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Ju R, 2015, SIGNAL PROCESS-IMAGE, V38, P115, DOI 10.1016/j.image.2015.07.002
   Judd T, 2011, J VISION, V11, DOI 10.1167/11.4.14
   Kim Y, 2019, IEEE T VIS COMPUT GR, V25, P3202, DOI 10.1109/TVCG.2018.2866106
   Konrad R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073594
   Lei JJ, 2017, IEEE T MULTIMEDIA, V19, P1442, DOI 10.1109/TMM.2017.2660440
   Li B, 2015, IEEE T IMAGE PROCESS, V24, P2811, DOI 10.1109/TIP.2015.2431441
   Li DB, 2018, PROC CVPR IEEE, P8193, DOI 10.1109/CVPR.2018.00855
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Niu YZ, 2012, IEEE T MULTIMEDIA, V14, P783, DOI 10.1109/TMM.2012.2186122
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shao F, 2017, IEEE T IMAGE PROCESS, V26, P4790, DOI 10.1109/TIP.2017.2721546
   Shao F, 2016, J DISP TECHNOL, V12, P22, DOI 10.1109/JDT.2015.2446973
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song Y, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1047, DOI 10.1145/3240508.3240623
   Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x
   Sun J, 2013, INT J COMPUT VISION, V104, P135, DOI 10.1007/s11263-013-0618-z
   Wang JQ, 2016, IEEE T CIRC SYST VID, V26, P2079, DOI 10.1109/TCSVT.2015.2493500
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2017, IEEE T VIS COMPUT GR, V23, P2014, DOI 10.1109/TVCG.2016.2600594
   Wu LF, 2018, NEUROCOMPUTING, V286, P198, DOI 10.1016/j.neucom.2018.01.058
   Yan CG, 2019, IEEE T MULTIMEDIA, V21, P2675, DOI 10.1109/TMM.2019.2903448
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yan JZ, 2015, INT J COMPUT VISION, V114, P74, DOI 10.1007/s11263-015-0801-5
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   Zhang YB, 2016, IEEE T IMAGE PROCESS, V25, P4286, DOI 10.1109/TIP.2016.2585884
NR 48
TC 8
Z9 9
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1208
EP 1219
DI 10.1109/TMM.2019.2939707
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200008
DA 2024-07-18
ER

PT J
AU Li, CY
   Guo, CL
   Guo, JC
   Han, P
   Fu, HZ
   Cong, RM
AF Li, Chongyi
   Guo, Chunle
   Guo, Jichang
   Han, Ping
   Fu, Huazhu
   Cong, Runmin
TI PDR-Net: Perception-Inspired Single Image Dehazing Network With
   Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Atmospheric modeling; Image color analysis; Visualization; Training
   data; Loss measurement; Image reconstruction; Task analysis; Image
   dehazing; perceptual loss; end-to-end; image refinement
ID VISIBILITY; WEATHER
AB During recent years, we have witnessed a rapid development of wireless network technologies which have revolutionized the way people take and share multimedia content. However, images captured in the outdoor scenes usually suffer from limited visibility due to suspended atmospheric particles, which directly affects the quality of photos. Despite the recent progress of image dehazing methods, the visual quality of dehazed results still needs further improvement. In this paper, we propose a deep convolutional neural network (CNN) for single image dehazing called PDR-Net, which includes a perception-inspired haze removal subnetwork that reconstructs the latent dehazed image and a refinement subnetwork that further enhances the contrast and color properties of the dehazed result by joint multi-term loss optimization. Compared to the previous methods, our method combines the advantages of existing indoor and outdoor image dehazing training data, which makes the proposed PDR-Net generalized to various hazy images and effective for improving the visual quality of the dehazed results. Extensive experiments demonstrate that the proposed method achieves comparable and even better performance on both real and synthetic images in qualitative and quantitative metrics. Additionally, the potential usage of our method in high-level vision tasks is discussed.
C1 [Li, Chongyi; Guo, Chunle; Guo, Jichang] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Li, Chongyi] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Han, Ping] Civil Aviat Univ China, Coll Elect Informat & Automat, Tianjin 300300, Peoples R China.
   [Fu, Huazhu] Inception Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Cong, Runmin] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Cong, Runmin] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
C3 Tianjin University; City University of Hong Kong; Civil Aviation
   University of China; Beijing Jiaotong University; Beijing Jiaotong
   University
RP Guo, CL (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM lichongyi25@gmail.com; guochunle@tju.edu.cn; jcguo@tju.edu.cn;
   hanpingcauc@163.com; hzfu@ieee.org; rmcong@bjtu.edu.cn
RI Guo, Jichang/GQY-5798-2022; Wang, Meng/ITR-8699-2023; Fu,
   Huazhu/A-1411-2014
OI Guo, Jichang/0000-0003-3130-1685; Fu, Huazhu/0000-0002-9702-5524
FU National Natural Science Foundation of China [61771334, 61571442];
   Fundamental Research Funds for the Central Universities [2019RC039];
   National Key Research and Development Program of China [2016YFB0502405]
FX This work was supported in part by the National Natural Science
   Foundation of China underGrant 61771334 andGrant 61571442, in part by
   the Fundamental Research Funds for the Central Universities under Grant
   2019RC039, and in part by the National Key Research and Development
   Program of China underGrant 2016YFB0502405.
CR Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   [Anonymous], 2019, ARXIV190105495
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2018, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2018.2807593
   [Anonymous], P IEEE C COMP VIS RE
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham Mark, 2007, PASCAL VISUAL OBJECT
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo CL, 2019, IEEE T IMAGE PROCESS, V28, P2545, DOI 10.1109/TIP.2018.2887029
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ignatov A, 2018, IEEE COMPUT SOC CONF, P804, DOI 10.1109/CVPRW.2018.00112
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Koschmieder H., 1924, THEORIE HORIZONTALEN
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li CY, 2019, IEEE T GEOSCI REMOTE, V57, P9156, DOI 10.1109/TGRS.2019.2925070
   Li CY, 2018, IEEE ACCESS, V6, P24877, DOI 10.1109/ACCESS.2018.2818882
   Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010
   Li F, 2018, IEEE T MULTIMEDIA, V20, P1154, DOI 10.1109/TMM.2017.2764329
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Li Y, 2017, COMPUT VIS IMAGE UND, V165, P1, DOI 10.1016/j.cviu.2017.09.003
   Liu Q, 2018, IEEE T IMAGE PROCESS, V27, P5178, DOI 10.1109/TIP.2018.2849928
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sohn K, 2015, ADV NEUR IN, V28
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Tan KK, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P788, DOI 10.1109/ICIP.2000.899827
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang D.M., 2018, IEEE International Symposium on Inertial Sensors and Systems, Moltrasio, P1
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   Yuan F, 2018, IEEE T IMAGE PROCESS, V27, P4395, DOI 10.1109/TIP.2018.2837900
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 55
TC 92
Z9 101
U1 5
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 704
EP 716
DI 10.1109/TMM.2019.2933334
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700011
DA 2024-07-18
ER

EF