FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Azad, SA
   Murshed, M
AF Azad, Salahuddin A.
   Murshed, Manzur
TI An Adaptive Borrow-and-Return Model for Broadcasting Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Borrow-and-return; client waiting time; fast broadcasting;
   video-on-demand
ID SCHEME
AB Yang et al. proposed the concept of borrow-and-return (BR) to leverage the unused server bandwidth when a group of popular videos being broadcast with the FSFC (first segment on the first channel) broadcasting schemes in order to improve the mean waiting time (MWT) of the viewers with the help of additional receiving bandwidth available at the high-end clients. The BR model borrows the bandwidth of the videos with no new-coming viewers during a timeslot to speed up the transmission of the first segments of some of the remaining videos. In this paper, we first address the relative advantage issue among various possible BR schemes by developing a parametric generic BR (GBR) scheme controlled externally by independent borrow parameters. Later, we propose a new BR (NBR) model by incorporating an efficient transmission strategy to reduce the MWT further. Finally, an optimal NBR scheme is developed by augmenting with the optimal borrow parameters, which significantly outperforms the existing and new BR schemes in terms of overall MWT.
C1 [Azad, Salahuddin A.; Murshed, Manzur] Monash Univ, Gippsland Sch Informat Technol, Churchill, Vic 3842, Australia.
C3 Federation University Australia; Monash University
RP Azad, SA (corresponding author), Monash Univ, Gippsland Sch Informat Technol, Churchill, Vic 3842, Australia.
EM salahuddin.azad@infotech.monash.edu.au;
   manzur.murshed@infotech.monash.edu.au
OI Murshed, Manzur/0000-0001-7079-9717; Azad,
   Salahuddin/0000-0002-6377-7912
CR AGGARWAL C, 1996, P ACM SIGMETRICS C M
   Aggarwal CC, 2001, IEEE T COMPUT, V50, P97, DOI 10.1109/12.908987
   Aggarwal CC, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P253, DOI 10.1109/MMCS.1996.534983
   [Anonymous], 2000, Introduction to probability models
   Cai Y., 2003, P 9 INT C DISTR MULT, P72
   EAGER D, 1998, P 4 INT WORKSH MULT
   Jhun L., 1997, IEEE T BROADCAST, V43, P268
   JIN H, 2003, P IEEE INT C MULT EX, V2, P305
   Juhn LS, 1998, IEEE T BROADCAST, V44, P100, DOI 10.1109/11.713059
   Lee JYB, 2005, IEEE T MULTIMEDIA, V7, P366, DOI 10.1109/TMM.2005.843356
   Liu Y, 2007, IET COMMUN, V1, P15, DOI 10.1049/iet-com:20050685
   Paris JF, 2005, IEEE IPCCC, P167
   Paris JF, 1998, SIXTH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P127, DOI 10.1109/MASCOT.1998.693685
   Sheu JP, 2004, IEEE T BROADCAST, V50, P120, DOI 10.1109/TBC.2004.828754
   Tseng YC, 2002, IEEE T COMMUN, V50, P1348, DOI 10.1109/TCOMM.2002.801466
   Viswanathan S, 1996, MULTIMEDIA SYST, V4, P197, DOI 10.1007/s005300050023
   Yang HC, 2005, CONSUM COMM NETWORK, P122
   Yang MH, 2003, IEEE T BROADCAST, V49, P162, DOI 10.1109/TBC.2003.813439
   Yoshihisa T, 2006, IEEE T BROADCAST, V52, P1, DOI 10.1109/TBC.2005.859235
   [No title captured]
NR 20
TC 0
Z9 0
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 707
EP 715
DI 10.1109/TMM.2009.2017617
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900012
DA 2024-07-18
ER

PT J
AU Lee, CH
   Shih, JL
   Yu, KM
   Lin, HS
AF Lee, Chang-Hsing
   Shih, Jau-Ling
   Yu, Kun-Ming
   Lin, Hwai-San
TI Automatic Music Genre Classification Based on Modulation Spectral
   Analysis of Spectral and Cepstral Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mel-frequency cepstral coefficients; modulation spectral analysis; music
   genre classification; normalized audio spectrum envelope; octave-based
   spectral contrast
AB In this paper, we will propose an automatic music genre classification approach based on long-term modulation spectral analysis of spectral (OSC and MPEG-7 NASE) as well as cepstral (MFCC) features. Modulation spectral analysis of every feature value will generate a corresponding modulation spectrum and all the modulation spectra can be collected to form a modulation spectrogram which exhibits the time-varying or rhythmic information of music signals. Each modulation spectrum is then decomposed into several logarithmically-spaced modulation subbands. The modulation spectral contrast (MSC) and modulation spectral valley (MSV) are then computed from each modulation subband. Effective and compact features are generated from statistical aggregations of the MSCs and MSVs of all modulation subbands. An information fusion approach which integrates both feature level fusion method and decision level combination method is employed to improve the classification accuracy. Experiments conducted on two different music datasets have shown that our proposed approach can achieve higher classification accuracy than other approaches with the same experimental setup.
C1 [Lee, Chang-Hsing; Shih, Jau-Ling; Yu, Kun-Ming; Lin, Hwai-San] Chung Hua Univ, Dept Comp Sci & Informat Engn, Hsinchu 300, Taiwan.
C3 Chung Hua University
RP Lee, CH (corresponding author), Chung Hua Univ, Dept Comp Sci & Informat Engn, Hsinchu 300, Taiwan.
EM chlee@chu.edu.tw; sjl@chu.edu.tw; yu@chu.edu.tw; m09502029@chu.edu.tw
RI liu, jiajia/ISS-0316-2023
OI Lee, Chang-Hsing/0000-0002-5761-421X
FU National Science Council [NSC-96-2221-E-216-043]
FX Manuscript received June 30, 2008; revised January 23. 2009. First
   published April 28, 2009; current version published May 15, 2009. This
   work was supported in part by the National Science Council of R.O.C.
   under contract NSC-96-2221-E-216-043. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Gerald Schuller.
CR Atlas L, 2003, EURASIP J APPL SIG P, V2003, P668, DOI 10.1155/S1110865703305013
   Aucouturier JJ, 2003, J NEW MUSIC RES, V32, P83, DOI 10.1076/jnmr.32.1.83.16801
   Bagci U, 2007, IEEE SIGNAL PROC LET, V14, P521, DOI 10.1109/LSP.2006.891320
   Bergstra J, 2006, MACH LEARN, V65, P473, DOI 10.1007/s10994-006-9019-7
   Duda R., 1973, Pattern Classification and Scene Analysis
   Ewert SD, 2000, J ACOUST SOC AM, V108, P1181, DOI 10.1121/1.1288665
   Grimaldi M., 2003, P 5 ACM SIGMM INT WO, P102, DOI [DOI 10.1145/973264.973281, 10.1145/973264.973281]
   Jiang DN, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P113, DOI 10.1109/ICME.2002.1035731
   Kanedera N, 1999, SPEECH COMMUN, V28, P43, DOI 10.1016/S0167-6393(99)00002-3
   Kim HG, 2005, MPEG-7 AUDIO AND BEYOND: AUDIO CONTENT INDEXING AND RETRIEVAL, P1, DOI 10.1002/0470093366
   Kim YK, 2004, EUR RADIOL, V14, P5, DOI 10.1007/s00330-003-2115-1
   Kingsbury BED, 1998, SPEECH COMMUN, V25, P117, DOI 10.1016/S0167-6393(98)00032-6
   Kinnunen T., 2006, P IEEE INT C AC SPEE, V1, P14
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   KOLLMEIER B, 1994, J ACOUST SOC AM, V95, P1593, DOI 10.1121/1.408546
   Lambrou T, 1998, INT CONF ACOUST SPEE, P3621, DOI 10.1109/ICASSP.1998.679665
   Lee CH, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P204, DOI 10.1109/INOW.2007.4302953
   Li T, 2006, IEEE T MULTIMEDIA, V8, P564, DOI 10.1109/TMM.2006.870730
   LIDY T, 2005, P 6 INT C MUS INF RE, P34
   LIN C, 2005, THEOR APPL GENET, V5, P1
   Lipp OV, 2004, EMOTION, V4, P233, DOI 10.1037/1528-3542.4.3.233
   MCKINNEY M, 2003, P INT S MUS INF RETR, P151
   Meng A, 2007, IEEE T AUDIO SPEECH, V15, P1654, DOI 10.1109/TASL.2007.899293
   Mörchen F, 2006, IEEE T AUDIO SPEECH, V14, P81, DOI 10.1109/TSA.2005.860352
   PANAGAKIS I, 2008, P INT S MUS INF RETR, P583
   PERROT D, 1999, P SOC MUS PERC COGN, P89
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Sethares WA, 2005, IEEE T SPEECH AUDI P, V13, P275, DOI 10.1109/TSA.2004.841053
   Song YY, 2008, M&SOM-MANUF SERV OP, V10, P1, DOI 10.1287/msom.1060.0140
   Sukittanon S, 2004, IEEE T SIGNAL PROCES, V52, P3023, DOI 10.1109/TSP.2004.833861
   Tyagi V., 2003, P WORKSH AUT SPEECH
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   TZANETAKIS G, 2002, P INT C MUS INF RETR
   Umapathy K, 2005, IEEE T MULTIMEDIA, V7, P308, DOI 10.1109/TMM.2005.843363
   WEST K, 2004, P INT C MUS INF RETR
   Xu CS, 2005, IEEE T SPEECH AUDI P, V13, P441, DOI 10.1109/TSA.2004.840939
NR 36
TC 95
Z9 106
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 670
EP 682
DI 10.1109/TMM.2009.2017635
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900009
DA 2024-07-18
ER

PT J
AU Awrangjeb, M
   Lu, GJ
AF Awrangjeb, Mohammad
   Lu, Guojun
TI Robust Image Corner Detection Based on the Chord-to-Point Distance
   Accumulation Technique
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Chord-to-point distance accumulation; corner detection; curvature
   scale-space
ID CURVATURE SCALE-SPACE
AB Many contour-based image corner detectors are based on the curvature scale-space (CSS). We identify the weaknesses of the CSS-based detectors. First, the "curvature" itself by its "definition" is very much sensitive to the local variation and noise on the curve, unless an appropriate smoothing is carried out beforehand. In addition, the calculation of curvature involves derivatives of up to second order, which may cause instability and errors in the result. Second, the Gaussian smoothing causes changes to the curve and it is difficult to select an appropriate smoothing-scale, resulting in poor performance of the CSS corner detection technique. We propose a complete corner detection technique based on the chord-to-point distance accumulation (CPDA) for the discrete curvature estimation. The CPDA discrete curvature estimation technique is less sensitive to the local variation and noise on the curve. Moreover, it does not have the undesirable effect of the Gaussian smoothing. We provide a comprehensive performance study. Our experiments showed that the proposed technique performs better than the existing CSS-based and other related methods in terms of both average repeatability and localization error.
C1 [Awrangjeb, Mohammad; Lu, Guojun] Monash Univ, Gippsland Sch Informat Technol, Churchill, Vic 3842, Australia.
C3 Federation University Australia; Monash University
RP Awrangjeb, M (corresponding author), Monash Univ, Gippsland Sch Informat Technol, Churchill, Vic 3842, Australia.
EM Awrangjeb@infotech.monash.edu.au; Guojun.Lu@infotech.monash.edu.au
RI Awrangjeb, Mohammad/N-3387-2016
OI Awrangjeb, Mohammad/0000-0002-2711-4329; Lu, Guojun/0000-0003-2523-7576
CR [Anonymous], 1986, COMPUTATIONAL APPROA, DOI DOI 10.1109/TPAMI.1986.4767851
   [Anonymous], IMAGE DATABASE CORNE
   Awrangieb M, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1483
   Awrangieb M, 2007, INT CONF ACOUST SPEE, P1233
   Awrangjeb M, 2006, IEEE IMAGE PROC, P1961, DOI 10.1109/ICIP.2006.313109
   Awrangjeb M, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P435, DOI 10.1109/MMSP.2006.285346
   CAO G, 2004, P 2004 AM CONTR C, V3, P2196
   Han JH, 2001, PATTERN RECOGN LETT, V22, P1133, DOI 10.1016/S0167-8655(01)00063-0
   He XC, 2004, INT C PATT RECOG, P791, DOI 10.1109/ICPR.2004.1334377
   Mokhtarian F., 2001, Proceedings of 12th Scandinavian Conference on Image Analysis, P145
   Mokhtarian F, 2001, PATTERN ANAL APPL, V4, P1, DOI 10.1007/PL00010984
   Mokhtarian F, 1998, IEEE T PATTERN ANAL, V20, P1376, DOI 10.1109/34.735812
   PETITCOLAS FAP, 2007, PHOTO DATABASE
   PHILLIPS TY, 1987, PATTERN RECOGN LETT, V5, P285, DOI 10.1016/0167-8655(87)90059-6
   RATTARANGSI A, 1992, IEEE T PATTERN ANAL, V14, P430, DOI 10.1109/34.126805
   Ray BK, 2003, PATTERN RECOGN, V36, P703, DOI 10.1016/S0031-3203(02)00084-5
   ROSENFELD A, 1973, IEEE T COMPUT, VC 22, P875, DOI 10.1109/TC.1973.5009188
   Seo JS, 2006, IEEE T SIGNAL PROCES, V54, P1537, DOI 10.1109/TSP.2006.870581
   *US SIP, 2007, US SIP IM DAT
   Zhang XH, 2007, PATTERN RECOGN LETT, V28, P545, DOI 10.1016/j.patrec.2006.10.006
   Zhong BJ, 2007, IEEE T PATTERN ANAL, V29, P508, DOI 10.1109/TPAMI.2007.50
NR 21
TC 116
Z9 160
U1 2
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1059
EP 1072
DI 10.1109/TMM.2008.2001384
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600010
OA Green Published
DA 2024-07-18
ER

PT J
AU Raja, A
   Azad, RMA
   Flanagan, C
   Ryan, C
AF Raja, Adil
   Azad, R. Muhammad Atif
   Flanagan, Colin
   Ryan, Conor
TI A Methodology for Deriving VoIP Equipment Impairment Factors for a Mixed
   NB/WB Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE E-Model; I-e,I-WB,I-eff; genetic programming; PESQ-WB; symbolic
   regression
ID QUALITY
AB This paper proposes a novel approach to quantifying the quality degradation of Voice over IP (VoIP) telephony in the presence of codec and network-related impairments. This approach differs from the baisc ITU-T E-Model for VoIP quality estimation [1] in that it addresses mixed narrowband/wideband scenarios. It makes novel use of instrumental models and symbolic regression via Genetic Programming (GP) to enable the evolution of degradation models from a modest set of initial parameters. Here, a two-step approach has been used. First, values of impairment factors are derived using WB-PESQ as a reference model. Secondly, a GP based symbolic regression approach has been utilized to automatically evolve the functional form of equipment impairment factors from a set of variables. Very few a priori assumptions are made about the model structure. The effectiveness of the approach is demonstrated by a number of generated models which compare favorably with WB-PESQ and outperform the traditional E-Model in terms of prediction accuracy when compared using WB-PESQ. A significant advantage of the approach is that new models are easily generated to account for continuing evolution of the VoIP standards.
C1 [Raja, Adil; Flanagan, Colin] Univ Limerick, Dept Comp Sci & Informat Syst, Limerick, Ireland.
   [Azad, R. Muhammad Atif; Ryan, Conor] Univ Limerick, Dept Elect & Comp Engn, Limerick, Ireland.
C3 University of Limerick; University of Limerick
RP Raja, A (corresponding author), Univ Limerick, Dept Comp Sci & Informat Syst, Limerick, Ireland.
EM adil.raja@ul.ie; colin.flanagan@ul.ie; conor.ryan@ul.ie
RI Azad, Raja Muhammad Atif/AAF-9916-2020
OI Azad, Raja Muhammad Atif/0000-0002-4013-5415; Ryan,
   Conor/0000-0002-7002-5815; PPCT, Leo/0000-0003-0897-4830
CR [Anonymous], 1996, Rec. ITU-T P.800
   [Anonymous], 1998, ITU T P S23
   [Anonymous], G729 ITUT
   [Anonymous], G107 ITUT
   [Anonymous], 1997, MACH LEARN
   [Anonymous], 2001, P 3 ANN C GENETIC EV, DOI 10.5555/2955239.2955258
   Banzhaf W., 1998, GENETIC PROGRAMMING
   Barriac V., 2004, P WORKSH WID SPEECH
   Chu W.C., 2003, SPEECH CODING ALGORI
   CLARK AD, 2001, 2 IP TEL WORKSH NEW
   Cole RG, 2001, ACM SIGCOMM COMP COM, V31, P9, DOI 10.1145/505666.505669
   *ETSI EN, 301704V721 ETSI EN
   Gustafson S, 2005, IEEE C EVOL COMPUTAT, P912
   HOENE C, 2005, INT J COMMUN SYST, V99, P1
   ITU, 2017, P.862.2
   *ITU T, 1996, G7231 ITUT
   *ITU T, 2001, P833 ITUT
   *ITU T, 2003, P8001 ITUT
   *ITU T, 2006, G113 ITUT
   *ITU T, 2003, G7222 ITUT
   *ITU T, 2005, G191 ITUT
   *ITU T, 2005, G1050 ITUT
   *ITU T, 2002, P834 ITU T
   *ITU T, 1988, G722 ITUT
   *ITU T, 2005, G7221 ITUT
   Janssen J, 2002, IEEE INTERNET COMPUT, V6, P48, DOI 10.1109/MIC.2002.1003131
   Jiang W., 2000, P NOSSDAV JUN
   Keijzer M., 2004, Genetic Programming and Evolvable Machines, V5, P259, DOI 10.1023/B:GENP.0000030195.77571.f9
   Koza J.R., 1992, GENETIC PROGRAMMING, VVolume 1
   LINGFEN, 2002, IEEE INT C COMM ICC, V4, P2573
   LUKE S, 2002, GECCO 2002, P829
   Möller S, 2006, IEEE T AUDIO SPEECH, V14, P1969, DOI 10.1109/TASL.2006.883262
   Mohamed S., 2004, MEASUREMENT SPEECH A
   MORIOKA C, 2004, IEEE INT C AC SPEECH
   Mugambi EM, 2004, KNOWL-BASED SYST, V17, P81, DOI 10.1016/j.knosys.2004.03.003
   O'Neill M, 2001, IEEE T EVOLUT COMPUT, V5, P349, DOI 10.1109/4235.942529
   Pennock S, 2002, MEASUREMENT SPEECH A
   Perkins C, 1998, IEEE NETWORK, V12, P40, DOI 10.1109/65.730750
   Raake A., 2006, Speech Quality of VoIP: Assessment and Prediction
   RAJA A, 2006, 1 INT C BIOINSP MOD, V4, P2573
   Raja A, 2007, LECT NOTES COMPUT SC, V4445, P217
   ROSENBERG JD, 2001, CUCS01601
   SANNECK H, 2000, SPIE ACM SIGMM MULT
   Sun L. E, 2002, MEASUREMENT SPEECH A
   Sun LF, 2006, IEEE T MULTIMEDIA, V8, P809, DOI 10.1109/TMM.2006.876279
NR 45
TC 10
Z9 11
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1046
EP 1058
DI 10.1109/TMM.2008.2001359
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, KC
   Guest, CC
   El-Maleh, K
   Das, PK
AF Yang, Kai-Chieh
   Guest, Clark C.
   El-Maleh, Khaled
   Das, Pankaj K.
TI Perceptual temporal quality metric for compressed video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE objective video quality assessment; perceptual video quality metric;
   subjective video temporal quality study; video temporal quality metric
AB This paper presents a metric to quantify frame loss according to the impact on perceived temporal quality. This metric particularly aims at measuring the temporal quality degradation caused by both regular and irregular frame loss. Experimental results with subjective viewing demonstrate high performance on prediction of perceptual temporal quality.
C1 Univ Calif San Diego, Dept Elect & Elect Engn, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Yang, KC (corresponding author), Univ Calif San Diego, Dept Elect & Elect Engn, La Jolla, CA 92093 USA.
EM kcyang@ucsd.edu; clark@ece.ucsd.edu; kelmaleh@qualcomm.com;
   das@cwc.ucsd.edu
CR Alparone L, 1996, INT CONF ACOUST SPEE, P2267, DOI 10.1109/ICASSP.1996.545874
   Camperi M, 1998, J COMPUT NEUROSCI, V5, P383, DOI 10.1023/A:1008837311948
   Caviedes J, 2004, SIGNAL PROCESS-IMAGE, V19, P147, DOI 10.1016/j.image.2003.08.002
   Claypool M., 1999, ACM INT C MULTIMEDIA, P115
   DANE G, 2004, P AS C SIGN SYST COM, V2, P1731
   Eng HY, 2005, PSYCHON B REV, V12, P1127, DOI 10.3758/BF03206454
   FEGHALI R, 2005, P IEEE INT C AC SPEE, P137
   Huang AM, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P395, DOI 10.1109/MMSP.2006.285338
   Iacovoni G., 2005, Proc. IEEE International Conference on Multimedia and Expo, P1452
   KARUNASEKERA SA, 1995, IEEE T IMAGE PROCESS, V4, P713, DOI 10.1109/83.388074
   Kato S, 2005, SEVENTH IASTED INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING, P442
   KATO S, 2005, P 1 INT WORKSH IM ME, P103
   Lu ZK, 2005, PROC SPIE, V5666, P554, DOI 10.1117/12.596845
   Marichal X., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P386, DOI 10.1109/ICIP.1999.822923
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   MENG JH, 1995, P SOC PHOTO-OPT INS, V2419, P14, DOI 10.1117/12.206359
   MONTENOVO M, 2006, P 3 INT WORKSH VID P
   Pastrana-Vidal RR, 2004, P SOC PHOTO-OPT INS, V5292, P182, DOI 10.1117/12.525746
   PASTRANAVIDAL R, 2006, P 3 INT WORKSH VID P
   WATANABE K, 2006, P SPIE EL IM IM QUAL
   Wu H.R., 1996, Proc. of Picture Coding Symposium, V1, P23
   Yang KC, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P777, DOI 10.1109/ICME.2006.262950
   YUEN M, 2005, CODING ARTIFACTS VIS
   2002, BT50011 IRR
NR 24
TC 58
Z9 72
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1528
EP 1535
DI 10.1109/TMM.2007.906576
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400017
DA 2024-07-18
ER

PT J
AU Argyriou, V
   Vlachos, T
AF Argyriou, Vasileios
   Vlachos, Theodore
TI Quad-tree motion estimation in the frequency domain using gradient
   correlation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE gradient motion estimation; phase correlation; quad-tree decomposition
AB We propose a new motion estimation scheme particularly suitable for broad cast-quality digital video applications due to its performance-complexity characteristics. The proposed scheme is based on the principle of gradient correlation and its computational efficiency is due to the fact that it operates in the frequency domain. The scheme involves the quad-tree decomposition of a frame thus providing a better level of adaptation to scene contents compared to fixed block size approaches. Quad-tree decompositions are obtained by using the motion compensated prediction error to control the partition of a parent block to four children quadrants. The partition criterion is applied iteratively until a target number of motion vectors or a target level of motion compensated prediction error is achieved or, ultimately, until no more than a single motion component can be identified. The partition criterion also guarantees a monotonic decrease of the motion compensated prediction error with an increasing number of iterations making our scheme suitable for progressive transmission and embedded coding applications. Our results show that our scheme outperforms fixed block size phase correlation as well as quad-tree motion estimation based on phase correlation in terms of rate-distortion characteristics yielding smoother motion vector fields as well as more compressible motion-compensated prediction residuals.
C1 Univ Surrey, Sch Elect & Phys Sci, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 University of Surrey
RP Argyriou, V (corresponding author), Univ Surrey, Sch Elect & Phys Sci, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
EM v.argyriou@imperial.ac.uk; t.vlachos@surrey.ac.uk
CR Abdou IE, 1998, P SOC PHOTO-OPT INS, V3653, P371, DOI 10.1117/12.334685
   AKSAY A, 2000, TIME SCALE TIME FREQ
   [Anonymous], P BRIT MACH VIS C
   Argyriou V, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P1433
   Argyriou V, 2005, IEE P-VIS IMAGE SIGN, V152, P107, DOI 10.1049/ip-vis:20051073
   ARGYRIOU V, 2004, P IEEE ICASSP, V3, P329
   BANHAM MR, 1994, IEEE T IMAGE PROCESS, V3, P652, DOI 10.1109/83.334979
   CAI W, 2004, P IEEE INT C AC SPEE, V2, P977
   Cai WT, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P49
   CHEONG CK, 1993, IEEE C AC SPEECH SIG, V5, P217
   Christmas WJ, 2000, IEEE T IMAGE PROCESS, V9, P1817, DOI 10.1109/83.869192
   CORDELL PJ, 1992, IEE PROC-I, V139, P575, DOI 10.1049/ip-i-2.1992.0077
   Jensen K, 1993, IEEE T CIRC SYST VID, V3, P99, DOI 10.1109/76.212716
   LEE J, 1995, P IEEE INT C IM PROC, V3, P480
   Li X, 2003, ITCC 2003: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: COMPUTERS AND COMMUNICATIONS, PROCEEDINGS, P488, DOI 10.1109/ITCC.2003.1197578
   NICOLAS H, 1992, P ICASSP, V3, P265
   Packwood RA, 1997, IEE CONF PUBL, P56, DOI 10.1049/cp:19970853
   Park HW, 2000, IEEE T IMAGE PROCESS, V9, P577, DOI 10.1109/83.841935
   PEARSON JJ, P SPIE, V119
   Schuster GM, 1998, IEEE T IMAGE PROCESS, V7, P1505, DOI 10.1109/83.725359
   SEFERIDIS V, 1994, P I ELECT ENG, V141
   STROBACH P, 1990, IEEE T COMMUN, V38, P477, DOI 10.1109/26.52659
   SULLIVAN GJ, 1994, IEEE T IMAGE PROCESS, V3, P327, DOI 10.1109/83.287030
   THOMAS GA, 1987, BBC RES DEPT
   TREDWELL S, 1999, P IEE EUR WORKSH DIS
   Wong KY, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P398, DOI 10.1109/ISIMP.2004.1434084
   ZAN J, 2007, IEEE T CIRCUITS SYST, V16, P439
NR 27
TC 15
Z9 19
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1147
EP 1154
DI 10.1109/TMM.2007.898926
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000006
DA 2024-07-18
ER

PT J
AU Chuang, HC
   Huang, CY
   Chiang, TH
AF Chuang, Hsiao-Chiang
   Huang, ChingYao
   Chiang, Tihao
TI Content-aware adaptive media playout controls for wireless video
   streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive media playout (AMP); buffers; cdma2000 1 x -RTT; scalable video
   streaming
AB Video streaming is one of the killer applications for cellular communications. The MPEG-4 fine-granularity scalability video coding technique can adapt to bandwidth variation and random packet errors. In this paper, to explore the impacts of cellular channel characteristics on the tolerance of buffer performance and quality of service, a novel statistical model-based adaptive media playout (AMP) is proposed by utilizing the statistical assumptions of both arrival and departure processes for a better decision on the dynamic threshold adjustment an frame-rate adjustment. Based on third-generation cellular transmission environment, simulation results will demonstrate that as compared to other AMP schemes, the proposed AMP control provides better visual quality with lower complexity.
C1 Natl Chiao Tung Univ, Dept Inst Elect Engn, Hsinchu 30050, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Chuang, HC (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM chuangh@purdue.edu; cyhuang@mail.nctu.edu.tw; tchiang@mail.nctu.edu.tw
CR Chuang HC, 2004, VTC2004-FALL: 2004 IEEE 60TH VEHICULAR TECHNOLOGY CONFERENCE, VOLS 1-7, P2582
   CHUANG HC, 2005, P IEEE ISCAS 05, V4, P3267
   Fujimoto K, 2004, TELECOMMUN SYST, V25, P259, DOI 10.1023/B:TELS.0000014784.20034.74
   Girod B, 2002, IEEE IMAGE PROC, P9
   Huang J, 2003, IEEE T CIRC SYST VID, V13, P973, DOI 10.1109/TCSVT.2003.816516
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   KALMAN M, 2002, P IEEE INT S CIRC SY, V1, P45
   Laoutaris N, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-10, CONFERENCE RECORD, P969, DOI 10.1109/ICC.2001.937381
   Lee SH, 2003, IEEE T KNOWL DATA EN, V15, P1535, DOI 10.1109/TKDE.2003.1245291
   Liang C. H., 2004, P IEEE ISCAS 04, V3, P749
   Stockhammer T, 2004, IEEE T MULTIMEDIA, V6, P268, DOI 10.1109/TMM.2003.822795
   WANG CN, 2004, ISOIECJTCISC269WG11M
   Wu DP, 2000, P IEEE, V88, P1855, DOI 10.1109/5.899055
   Yang YH, 2006, IEEE INT SYM MULTIM, P415
   Yuang MC, 1996, 1996 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS - CONVERGING TECHNOLOGIES FOR TOMORROW'S APPLICATIONS, VOLS. 1-3, P1365, DOI 10.1109/ICC.1996.533632
NR 15
TC 33
Z9 37
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1273
EP 1283
DI 10.1109/TMM.2007.902884
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000016
DA 2024-07-18
ER

PT J
AU Dao, MS
   De Natale, FGB
   Massa, A
AF Dao, Minh-Son
   De Natale, Francesco G. B.
   Massa, Andrea
TI Edge potential functions (EPF) and genetic algorithms (GA) for
   edge-based matching of visual objects
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE shape matching; image retrieval; edge potential function
ID SHAPE-ANALYSIS; RECOGNITION; RETRIEVAL
AB Edges are known to be a semantically rich representation of the contents of a digital image. Nevertheless, their use in practical applications is sometimes limited by computation and complexity constraints. In this paper, a new approach is presented that addresses the problem of matching visual objects in digital images by combining the concept of edge potential functions (EPF) with a powerful matching tool based on genetic algorithms (GAs). EPFs can be easily calculated starting from an edge map and provide a kind of attractive pattern for a matching contour, which is conveniently exploited by GAs. Several tests were performed in the framework of different image matching applications. The results achieved clearly outline the potential of the proposed method as compared to state of the art methodologies.
C1 Graphitech, Trento, Italy.
   Univ Trent, Dept Informat & Commun Technol, I-38050 Trento, Italy.
C3 Fondazione Bruno Kessler; FBK-ICT - Center for Information &
   Communication Technology; University of Trento
RP Dao, MS (corresponding author), Graphitech, Trento, Italy.
EM dao.minhson@graphitech.it; denatale@ing.unitm.it;
   andrea.massa@ing.unitn.it
RI Dao, Minh-Son/S-5984-2019; Massa, Andrea/F-3276-2013
OI Massa, Andrea/0000-0002-8429-8937
CR Akleman E, 1999, SHAPE MODELING INTERNATIONAL '99 - INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P72, DOI 10.1109/SMA.1999.749326
   [Anonymous], 1998, Statistical shape analysis: Wiley series in probability and statistics". In
   [Anonymous], 2015, Electromagnetic Theory
   [Anonymous], 1999, Shape and Shape Theory
   Barrow H. G., 1977, P IMAGE UNDERSTANDIN, P659
   Beveridge J. R., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P18, DOI 10.1109/ICPR.1990.118058
   BIEDERMAN I, 1986, P 2 WORKSH HUM MACH, P13
   Bookstein F. L., 1986, STAT SCI, V1, P181, DOI [DOI 10.1214/SS/1177013696, 10.1214/ss/1177013696]
   BORGEFORS G, 1988, IEEE T PATTERN ANAL, V10, P849, DOI 10.1109/34.9107
   BORGEFORS G, 1986, COMPUT VISION GRAPH, V34, P344, DOI 10.1016/S0734-189X(86)80047-0
   CARNE TK, 1990, P LOND MATH SOC, V61, P407
   Chang SF, 1997, IEEE SIGNAL PROC MAG, V14, P45, DOI 10.1109/79.598595
   Chowdhury MI, 2000, 2000 CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING, CONFERENCE PROCEEDINGS, VOLS 1 AND 2, P312, DOI 10.1109/CCECE.2000.849720
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   DRYDEN I, 2000, P IMA WORKSH IM AN H, P729
   Goldberg David E, 1989, GENETIC ALGORITHMS S
   Grenander U., 1994, General Pattern Theory: A Mathematical Study of Regular Structures
   GUDIVADA VN, 1995, COMPUTER, V28, P18, DOI 10.1109/2.410145
   Holland I.H., 1975, ADAPTATION NATURAL A
   Huttenlocher D. P., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P705, DOI 10.1109/CVPR.1993.341019
   HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073
   Inglada J, 2001, INT GEOSCI REMOTE SE, P2313, DOI 10.1109/IGARSS.2001.977986
   Johnson JM, 1997, IEEE ANTENN PROPAG M, V39, P7, DOI 10.1109/74.632992
   Jong K.A. D., 1975, An Analysis of the Behavior of a Class of Genetic Adaptive Systems
   KAWAGUCHI T, 1998, P IEEE INT C PATT RE, V1, P233
   KHATIB O, 1986, INT J ROBOT RES, V5, P90, DOI 10.1177/027836498600500106
   Li BH, 2003, IEEE SYS MAN CYBERN, P729
   Loncaric S, 1998, PATTERN RECOGN, V31, P983, DOI 10.1016/S0031-2023(97)00122-2
   Ma WY, 2000, IEEE T IMAGE PROCESS, V9, P1375, DOI 10.1109/83.855433
   MOKHTARIAN F, 1995, IEEE T PATTERN ANAL, V17, P539, DOI 10.1109/34.391387
   NAGATA TKR, 1999, P IEEE INT C IM PROC, V2, P710
   NASWANI N, 2003, P IEEE INT C AC SPEE
   Oller G, 2002, INT GEOSCI REMOTE SE, P2495
   Olson C. F., 1995, Proceedings International Symposium on Computer Vision (Cat. No.95TB100006), P91, DOI 10.1109/ISCV.1995.476983
   Olson CF, 2002, IEEE T PATTERN ANAL, V24, P853, DOI 10.1109/TPAMI.2002.1008392
   Olson CF, 1997, IEEE T IMAGE PROCESS, V6, P103, DOI 10.1109/83.552100
   Pastorino M, 2000, IEEE T INSTRUM MEAS, V49, P573, DOI 10.1109/19.850397
   Posner M., 1989, FDN COGNITIVE SCI
   SCOTT G, 2003, P ROY SOC LOND APR B, V244, P21
   Sim DG, 2001, IEEE T IMAGE PROCESS, V10, P475, DOI 10.1109/83.908541
   SMALL C, 1996, SPRINGER SERIES STAT, VO
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Srivastava A, 2005, IEEE T PATTERN ANAL, V27, P590, DOI 10.1109/TPAMI.2005.86
   Tsang PWM, 2003, IEE P-VIS IMAGE SIGN, V150, P107, DOI 10.1049/ip-vis:20030158
   Veltkamp RC, 2001, ADV PTRN RECOGNIT, P87
   Wang J, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P45, DOI 10.1109/ICIP.1998.727117
   Wang Y., 1996, P IEEE INT C PATT RE, P740
   Xu YW, 2000, IEEE IMAGE PROC, P73, DOI 10.1109/ICIP.2000.900895
   Yin PY, 1999, PATTERN RECOGN LETT, V20, P731, DOI 10.1016/S0167-8655(99)00037-9
   Zhang LH, 2003, PATTERN RECOGN LETT, V24, P9, DOI 10.1016/S0167-8655(02)00160-5
NR 50
TC 15
Z9 19
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 120
EP 135
DI 10.1109/TMM.2006.886371
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500012
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Lin, JW
   Chang, RI
   Ho, JM
   Lai, F
AF Lin, Jeng-Wei
   Chang, Ray-I
   Ho, Jan-Ming
   Lai, Feipei
TI FOS: A funnel-based approach for optimal online traffic smoothing of
   live video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE live video; multimedia streaming; online delivery; traffic smoothing
AB Traffic smoothing is an efficient means to reduce the bandwidth requirement for transmitting a variable-bit-rate video stream. Several traffic-smoothing algorithms have been presented to offline compute the transmission schedule for a prerecorded video. For live video applications, Sen et al present a sliding-window algorithm, referred to as SLWIN(k), to online compute the transmission schedule on the fly. SLWIN(k) looks ahead W video frames to compute the transmission schedule for the next k frametimes, where k <= w. Note that W is upper bounded by the initial delay of the transmission. The time complexity of SLWIN(k) is O(W * N/k) for an N frame live video. In this paper, we present an O(N) online traffic-smoothing algorithm and two variants, denoted as FOS, FOS1 and FOS2, respectively. Note that O(N) is a trivial lower bound of the time complexity of the traffic-smoothing problem. Thus, the proposed algorithm is optimal. We compare the performance of our algorithms with SLWIN(k) based on several benchmark video clips. Experiment results show that FOS2, which adopts the aggressive workahead heuristic, further reduces the bandwidth requirement and better utilizes the client buffer for real-time interactive applications in which the initial delays are small.
C1 Tunghai Univ, Dept Informat Management, Taichung 40704, Taiwan.
   Natl Taiwan Univ, Dept Engn Sci & Ocean Engn, Taipei 10764, Taiwan.
   Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10764, Taiwan.
   Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
C3 Tunghai University; National Taiwan University; Academia Sinica -
   Taiwan; National Taiwan University; National Taiwan University
RP Lin, JW (corresponding author), Tunghai Univ, Dept Informat Management, Taichung 40704, Taiwan.
EM jwlin@thu.edu.tw; rayichang@ntu.edu.tw; hoho@iis.sinica.edu.tw;
   flai@ntu.edu.tw
RI Ho, Jan-Ming/E-6895-2013; Chang, Ray-I/A-6197-2009
OI Chang, Ray-I/0000-0002-8737-7227
CR ADS A, 1996, P IEEE INFOCOM MAR, P1476
   AMIR E, 1995, P ACM MULT SAN FRANC
   [Anonymous], **NON-TRADITIONAL**
   CAO G, 1999, P 8 IEEE INT C COMP, P502
   Carle G, 1997, IEEE NETWORK, V11, P24, DOI 10.1109/65.642357
   Chandan R, 1997, PROPERTIES MILK ITS, P1
   Chang RI, 1999, IEEE INFOCOM SER, P447, DOI 10.1109/INFCOM.1999.751377
   CHANG RI, 1997, SPIE VVDC, P382
   CROSBY S, 1996, P IEE UK TEL S MANCH
   FENG W, 1999, P IEEE INFOCOM, P58
   FENG W, 1995, COMPUT COMMUN    OCT, P709
   FENG W, 1995, IS T SPIE MMCN, P234
   GARRETT M, 1994, P ACM SIGCOMM LOND U
   KRUNZ M, 1997, P ACM SIGMETRICS 97, P192
   LEE DT, 1984, NETWORKS, V14, P393, DOI 10.1002/net.3230140304
   MCMANUS JM, 1997, SPIE VOIC VID DAT CO
   MCMANUS JM, 1996, P IEEE INFOCOM SAN F
   REXFORD J, 1997, P INT WORKSH NETW OP, P249
   Salehi JD, 1998, IEEE ACM T NETWORK, V6, P397, DOI [10.1109/90.720873, 10.1142/S0218213097000219]
   Sen S, 2000, IEEE T MULTIMEDIA, V2, P37, DOI 10.1109/6046.825793
NR 20
TC 11
Z9 16
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 996
EP 1004
DI 10.1109/TMM.2006.879868
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400011
OA Green Published
DA 2024-07-18
ER

PT J
AU Wu, YD
AF Wu, Yongdong
TI Nonlinear collusion attack on a watermarking scheme for buyer
   authentication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE authentication watermarking; collusion attack
AB This paper presents an adaptive collusion attack on a buyer authentication watermarking scheme. To accomplish this attack, the traitors (i.e., dishonest buyers) select the pixels of their watermarked images generated from the same original image and average the selected pixels so as to remove the watermark information. Additionally, the forged image is of higher quality than any watermarked image. Both theoretical and experimental results demonstrate that our attack is very effective.
C1 Inst Infocomm Res, Singapore 119613, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R)
RP Wu, YD (corresponding author), Inst Infocomm Res, Singapore 119613, Singapore.
EM wydong@i2r.a-star.edu.sg
OI Wu, Yongdong/0000-0002-0850-724X
CR Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Das TK, 2002, LECT NOTES COMPUT SC, V2513, P184
   Delaigle JF, 1996, PROC SPIE, V2659, P99, DOI 10.1117/12.235449
   Ergun F, 1999, LECT NOTES COMPUT SC, V1592, P140
   Mukherjee DP, 2004, IEEE T MULTIMEDIA, V6, P1, DOI 10.1109/TMM.2003.819759
   Pitas I, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P215, DOI 10.1109/ICIP.1996.560422
   Swanson MD, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P211, DOI 10.1109/ICIP.1996.560421
   Trappe W, 2003, IEEE T SIGNAL PROCES, V51, P1069, DOI 10.1109/TSP.2003.809378
   Wolfgang RB, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P219, DOI 10.1109/ICIP.1996.560423
   Wu M, 2004, IEEE SIGNAL PROC MAG, V21, P15
   WU Y, 2005, IEEE INT C AC SPEECH
   Wu YD, 2003, LECT NOTES COMPUT SC, V2836, P238
   ZHAO H, 2003, IEEE INT C AC SPEECH
NR 13
TC 5
Z9 5
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 626
EP 629
DI 10.1109/TMM.2006.870721
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000019
DA 2024-07-18
ER

PT J
AU Zhu, Y
   Kankanhalli, MS
AF Zhu, Yongwei
   Kankanhalli, Mohan S.
TI Precise pitch profile feature extraction from musical audio for key
   detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio processing; key finding; music analysis; music information
   retrieval; pitch extraction; scale estimation
ID TEMPO; BEAT
AB The majority of pieces of music, including classical and popular music, are composed using music scales, such as keys. The key or the scale information of a piece provides important clues on its high level musical content, like harmonic and melodic context. Automatic key detection from music data can be useful for music classification, retrieval or further content analysis. Many researchers have addressed key finding from symbolically encoded music (MIDI); however, works for key detection in musical audio is still limited. Techniques; for key detection from musical audio mainly consist of two steps:: pitch extraction and key detection. The pitch feature typically characterizes the weights of presence of particular pitch classes in the music audio. In the existing approaches to pitch extraction, little consideration has been taken on pitch mistuning and interference of noisy percussion sounds in the audio signals, which inevitably affects the accuracy of key detection. In this paper, we present a novel technique of precise pitch profile feature extraction, which deals with pitch mistuning and noisy percussive sounds. The extracted pitch profile feature can characterize the pitch content in the signal more accurately than the previous techniques, thus lead to a higher key detection accuracy. Experiments based on classical and popular music data were conducted. The results showed that the proposed method has higher key detection accuracy than previous methods, especially for popular music with a lot of noisy drum sounds.
C1 Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
   Inst Infocomm Res, Singapore 119613, Singapore.
C3 National University of Singapore; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Zhu, Y (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
EM ywzhu@i2r.a-star.edu.sg; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
CR ABDALLAH SA, 2004, P ISMIR 04       OCT
   [Anonymous], 1990, COGNITIVE FDN MUSICA
   Bartsch MA, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P15, DOI 10.1109/ASPAA.2001.969531
   BROWN JC, 1991, J ACOUST SOC AM, V89, P425, DOI 10.1121/1.400476
   CHEW E, THESIS MIT CAMBRIDGE
   CHEW E, P 23 ANN M COGN SCI
   Davy M., 2003, BAYESIAN STAT 7
   Dixon S, 2001, J NEW MUSIC RES, V30, P39, DOI 10.1076/jnmr.30.1.39.7119
   Ghias A., 1995, PROC ACM MULTIMEDIA, P231
   GOMEZ E, 2004, P AES 25 INT C LOND
   GOMEZ E, 2004, P ISMIR 04       OCT
   GOTO M, 2003, P ICASSP 2003    APR, pV437
   GOTO M, 1996, P 2 INT C MULT SYST, P103
   GOTO M, 2001, P WORKSHOP CONSISTEN
   IZMIRLI O, THESIS MIDDLE E TU A
   IZMIRLI O, J NEW MUSIC RES, V25, P276
   KLAPURI A, 2000, P COSTG6 C GIG AUD E
   KLAPURI AP, 2003, P CAMBR MUS PROC C C
   PAUWS S, 2004, P ISMIR 04       OCT
   PURWINS H, 2004, AES 116 C BERL GERM
   PURWINS H, INT JOINT C NEUR NET, V6, P270
   Scheirer ED, 1998, J ACOUST SOC AM, V103, P588, DOI 10.1121/1.421129
   SCHROTER T, 2000, P ISMIR 2000
   SHEH A, ISMIR 2003
   SHENOY A, 2004, P INT C MULT EXP TAI
   Smaragdis P, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P177, DOI 10.1109/ASPAA.2003.1285860
   TAKEUCHI AH, 1994, PERCEPT PSYCHOPHYS, V56, P335, DOI 10.3758/BF03209767
   Tolonen T, 2000, IEEE T SPEECH AUDI P, V8, P708, DOI 10.1109/89.876309
   TZANETAKIS G, 2001, P ISMIR 2001     OCT
   UKKONEN E, 2003, P ISMIR 2003     OCT
   YLIHARJA O, 1999, P IEEE EURASIP WORKS
   ZHU Y, 2003, P ACM MULTIMEDIA NOV
   ZHU Y, 2005, 11 INT MULT MOD C ME
NR 33
TC 27
Z9 29
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 575
EP 584
DI 10.1109/TMM.2006.870727
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000014
DA 2024-07-18
ER

PT J
AU Lie, WN
   Lin, GS
AF Lie, WN
   Lin, GS
TI A feature-based classification technique for blind image steganalysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE data embedding; steganalysis; steganography
ID DIGITAL IMAGES; WATERMARKING; SCHEME; CAPACITY
AB In contrast to steganography, steganalysis is focused on detecting (the main goal of this research), tracking, extracting, and modifying secret messages transmitted through a covert channel. In this paper, a feature classification technique, based on the analysis of two statistical properties in the spatial and DCT domains, is proposed to blindly (i.e., without knowledge of the steganographic schemes) to determine the existence of hidden messages in an image. To be effective in class separation, the nonlinear neural classifier was adopted. For evaluation, a database composed of 2088 plain and stego images (generated by using six different embedding schemes) was established. Based on this database, extensive experiments were conducted to prove the feasibility and diversity of our proposed system, It was found that the proposed system consists of: 1) a 90%+ positive-detection rate; 2) not limited to the detection of a particular steganographic scheme; 3) capable of detecting stego images with an embedding rate as low as 0.01 bpp; and 4) considering the test of plain images incurred low-pass filtering, sharpening, and JPEG compression.
C1 Natl Chung Cheng Univ, Dept Elect Engn, Chiayi 621, Taiwan.
   Da Yeh Univ, Dept Comp Sci & Informat Engn, Changhua 515, Taiwan.
C3 National Chung Cheng University; Da Yeh University
RP Natl Chung Cheng Univ, Dept Elect Engn, Chiayi 621, Taiwan.
EM wnlie@ee.ccu.edu.tw; khlin@mail.dyu.edu.tw
RI Lie, Wen-Nung/AFP-1266-2022
CR [Anonymous], The Prisoners' Problem and the Subliminal Channel, DOI 10.1007/978-1-4684-4730-95
   [Anonymous], 2000, PROC ACM WORKSHOPS M
   [Anonymous], 1999, P IEEE INT C IM PROC
   Avcibas I, 2003, IEEE T IMAGE PROCESS, V12, P221, DOI 10.1109/TIP.2002.807363
   Barni M, 2000, IEEE T IMAGE PROCESS, V9, P1450, DOI 10.1109/83.855442
   Barni M, 2004, IEEE SIGNAL PROC MAG, V21, P28, DOI 10.1109/MSP.2004.1276109
   Bender W., 1996, IBM SYST J, V35, DOI DOI 10.1147/SJ.353.0313
   Chandramouli R, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P1019, DOI 10.1109/ICIP.2001.958299
   Chandramouli R, 2002, PROC SPIE, V4675, P14, DOI 10.1117/12.465273
   Chen TS, 1998, IEEE T IMAGE PROCESS, V7, P1485, DOI 10.1109/83.718488
   Cheng Q, 2001, IEEE T MULTIMEDIA, V3, P273, DOI 10.1109/6046.944472
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Cox IJ, 1999, P IEEE, V87, P1127, DOI 10.1109/5.771068
   Duda R. O., 2001, PATTERN CLASSIFICATI
   Eggers JJ, 2003, IEEE T SIGNAL PROCES, V51, P1003, DOI 10.1109/TSP.2003.809366
   Fridrich J, 2002, PROC SPIE, V4675, P1, DOI 10.1117/12.465263
   HARMSEN JJ, 2003, P SOC PHOTO-OPT INS, P21
   Hernandez JR, 1998, IEEE J SEL AREA COMM, V16, P510, DOI 10.1109/49.668974
   Huang JW, 1998, ELECTRON LETT, V34, P748, DOI 10.1049/el:19980545
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   Johnson NF, 1998, LECT NOTES COMPUT SC, V1525, P273
   JOSHI RL, 1995, IEEE SIGNAL PROC LET, V2, P81, DOI 10.1109/97.386283
   Katzenbeisser SC, 2000, ART H COMP SCI LIBR, P17
   Kim YS, 1999, ELECTRON LETT, V35, P466, DOI 10.1049/el:19990327
   Kong XW, 2002, LECT NOTES COMPUT SC, V2532, P434
   Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177
   Lee YK, 2000, IEE P-VIS IMAGE SIGN, V147, P288, DOI 10.1049/ip-vis:20000341
   LIE WN, 2000, P IEEE INT S CIRC SY, P1228
   Looney CarlGrant., 1997, Pattern recognition using neural networks: theory and algorithms for engineers and scientists
   MAES M, 1998, P INF HID, P290
   Manikopoulos C, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P355
   Marvel LM, 1999, IEEE T IMAGE PROCESS, V8, P1075, DOI 10.1109/83.777088
   Moulin P, 2003, IEEE T INFORM THEORY, V49, P563, DOI 10.1109/TIT.2002.808134
   Nikolaidis N, 1998, SIGNAL PROCESS, V66, P385, DOI 10.1016/S0165-1684(98)00017-6
   Ogihara T., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P675, DOI 10.1109/ICPR.1996.546908
   Papoulis A., 1991, Probability, Random Variables and Stochastic Processes
   Pérez-González F, 2003, IEEE T SIGNAL PROCES, V51, P960, DOI 10.1109/TSP.2003.809368
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   Podilchuk CI, 1998, IEEE J SEL AREA COMM, V16, P525, DOI 10.1109/49.668975
   Poor H. Vincent, 1994, An introduction to signal detection and estimation
   Voloshynovskiy S, 2002, PROC SPIE, V4675, P57, DOI 10.1117/12.465324
   Wolfgang RB, 1999, P IEEE, V87, P1108, DOI 10.1109/5.771067
NR 42
TC 82
Z9 107
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1007
EP 1020
DI 10.1109/TMM.2005.858377
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200002
DA 2024-07-18
ER

PT J
AU Cucchiara, R
   Piccardi, M
   Prati, A
AF Cucchiara, R
   Piccardi, M
   Prati, A
TI Neighbor cache prefetching for multimedia image and video processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cache memories; image processing; multimedia; neighbor prefetching;
   prefetching
ID PERFORMANCE
AB Cache performance is strongly influenced by the type of locality embodied in programs. In particular, multimedia programs handling images and videos are characterized by a bidimensional spatial locality, which is not adequately exploited by standard caches. In this paper we propose novel cache prefetching techniques for image data, called neighbor prefetching, able to improve exploitation of bidimensional spatial locality. A performance comparison is provided against other assessed prefetching techniques on a multimedia workload (with MPEG-2 and MPEG-4 decoding, image processing, and visual object segmentation), including a detailed evaluation of both the miss rate and the memory access time. Results prove that neighbor prefetching achieves a significant reduction in the time due to delayed memory cycles (more than 97% on MPEG-4 with respect to 75% of the second performing technique). This reduction leads to a substantial speedup on the overall memory access time (up to 140% for MPEG-4). Performance has been measured with the PRIMA trace-driven simulator, specifically devised to support cache prefetching.
C1 Univ Modena & Reggio Emilia, Dipartimento Ingn Informaz, I-41100 Modena, Italy.
   Univ Technol Sydney, Dept Comp Syst, Fac IT, Sydney, NSW 2007, Australia.
C3 Universita di Modena e Reggio Emilia; University of Technology Sydney
RP Univ Modena & Reggio Emilia, Dipartimento Ingn Informaz, I-41100 Modena, Italy.
EM cucchiara.rita@unimore.it; prati.andrea@unimore.it;
   massimo@it.uts.edu.au
RI Piccardi, Massimo/AAY-1323-2020; Prati, Andrea/B-7440-2014; Cucchiara,
   Rita/L-3006-2015
OI Piccardi, Massimo/0000-0001-9250-6604; Prati,
   Andrea/0000-0002-1211-529X; Cucchiara, Rita/0000-0002-2239-283X
CR [Anonymous], P INT C SUP
   Apprill M., 1997, IEEE POTENTIALS, V16, P11, DOI [10.1109/45.645823, DOI 10.1109/45.645823]
   Baglietto P, 1996, P IEEE, V84, P917, DOI 10.1109/5.503295
   CHEN TF, 1995, IEEE T COMPUT, V44, P609, DOI 10.1109/12.381947
   CHEN TF, 1996, P 21 INT S COMP ARCH, P223
   Cucchiara R, 1998, P INT C HIGH PERFORM, P466, DOI 10.1109/HIPC.1998.738023
   Cucchiara R, 2003, MULTIMED TOOLS APPL, V20, P159, DOI 10.1023/A:1023687722225
   Cucchiara R, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P345, DOI 10.1109/MMCS.1999.779228
   CUCCIARA R, 2001, P IEEE INT PERF COMP
   EICKEMEYER RJ, 1993, IBM J RES DEV, V37, P547, DOI 10.1147/rd.374.0547
   Freeman H., 1961, IRE T ELECTRON COMPU, V10, P260, DOI [DOI 10.1109/TEC.1961.5219197, 10.1109/TEC.1961.5219197]
   FU JWC, 1991, P 18 ANN INT S COMP, P54
   Hennessy J. L., 2012, Computer Architecture A Quantitative Approach, V5th
   IRLAM G, 1991, SPA APARC ANAL TOOL
   *ISO IEC DIS, 144962 ISOIEC DIS
   Joseph D, 1999, IEEE T COMPUT, V48, P121, DOI 10.1109/12.752653
   JOUPPI NP, 1990, 17TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, P364, DOI 10.1109/ISCA.1990.134547
   KANEKO T, 1985, IEEE T COMMUN, V33, P697, DOI 10.1109/TCOM.1985.1096361
   Katsaggelos AK, 1998, P IEEE, V86, P1126, DOI 10.1109/5.687833
   KOPLOWITZ J, 1981, IEEE T PATTERN ANAL, V3, P180, DOI 10.1109/TPAMI.1981.4767075
   Kuroda I, 1998, P IEEE, V86, P1203, DOI 10.1109/5.687835
   Lee CH, 1997, INT SYMP MICROARCH, P330, DOI 10.1109/MICRO.1997.645830
   LEGALL D, 1991, COMMUN ACM, V34, P46, DOI 10.1145/103085.103090
   Manku GS, 1997, FOURTH INTERNATIONAL CONFERENCE ON HIGH-PERFORMANCE COMPUTING, PROCEEDINGS, P100, DOI 10.1109/HIPC.1997.634478
   MILUTINOVIC V, 1996, P SCIZZL SANT CLAR C, V5
   Park GH, 1997, HIGH PERFORMANCE COMPUTING ON THE INFORMATION SUPERHIGHWAY - HPC ASIA '97, PROCEEDINGS, P712, DOI 10.1109/HPC.1997.592238
   PIMENTEL A, 1999, P IEEE INT PERF COMP, P525
   Ranganathan P, 1999, CONF PROC INT SYMP C, P124, DOI [10.1145/307338.300990, 10.1109/ISCA.1999.765945]
   SMITH AJ, 1982, COMPUT SURV, V14, P473, DOI 10.1145/356887.356892
   Tse J, 1998, IEEE T COMPUT, V47, P509, DOI 10.1109/12.677225
   Wu Z, 1998, 1998 IEEE WORKSHOP ON SIGNAL PROCESSING SYSTEMS-SIPS 98, P23, DOI 10.1109/SIPS.1998.715765
   ZUCKER DB, 1995, IEEE T CIRCUITS SYST, V10, P782
   Zucker DF, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P236, DOI 10.1109/MMCS.1996.534981
NR 33
TC 8
Z9 9
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2004
VL 6
IS 4
BP 539
EP 552
DI 10.1109/tmm.2004.830806
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 838RC
UT WOS:000222729800003
OA Green Published
DA 2024-07-18
ER

PT J
AU Ma, WH
   Du, DHC
AF Ma, WH
   Du, DHC
TI Design a progressive video caching policy for video proxy servers
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE caching policy; progressive video caching; proxy server; two-constraint
   multiple-choice knapsack problem
AB Proxy servers have been used to cache web objects to alleviate the load of the web servers and to reduce network congestion on the Internet. In this paper, a central video server is connected to a proxy server via wide area networks (WANs) and the proxy server can reach many clients via local area networks (LANs). We assume a video can be either entirely or partially cached in the proxy to reduce WAN bandwidth consumption. Since the storage space and the sustained disk I/O bandwidth are limited resources in the proxy, how to efficiently utilize these resources to maximize the WAN bandwidth reduction is an important issue. We design a progressive video caching policy in which each video can be cached at several levels corresponding to cached data sizes and required WAN bandwidths. For a video, the proxy server determines to cache a smaller amount of data at a lower level or to gradually accumulate more data to reach a higher level. The proposed progressive caching policy allows the proxy to adjust caching amount for each video based on its resource condition and the user access pattern. We investigate the scenarios in which the access pattern is priorly known or unknown and the effectiveness of the caching policy is evaluated.
C1 Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
C3 University of Minnesota System; University of Minnesota Twin Cities
RP Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
EM wma@cs.umn.edu; du@cs.umn.edu
CR Abrams M., 1995, P 4 INT WORLD WID WE, P119
   Aggarwal C, 1999, IEEE T KNOWL DATA EN, V11, P94, DOI 10.1109/69.755618
   [Anonymous], 1995, Algorithms for knapsack problems
   Belhe U, 1997, IEEE T SYST MAN CY A, V27, P105, DOI 10.1109/3468.553229
   Breslau L., 1999, INFOCOM 99
   Cao P, 1997, PROCEEDINGS OF THE USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P193
   Dan A., 1994, Proceedings ACM Multimedia '94, P15, DOI 10.1145/192593.192614
   Dan A, 1997, MULTIMED TOOLS APPL, V4, P279, DOI 10.1023/A:1009637022889
   DUDZINSKI K, 1987, EUR J OPER RES, V28, P3, DOI 10.1016/0377-2217(87)90165-2
   FENG W, 1997, P IEEE INFOCOM KOB J, P58
   Foong AP, 1999, IEEE INTERNET COMPUT, V3, P27, DOI 10.1109/4236.793455
   LOULOU R, 1979, OPER RES, V27, P1101, DOI 10.1287/opre.27.6.1101
   MA W, 2001, PROXY ASSISTED VIDEO
   Ma WH, 2002, IEEE T MULTIMEDIA, V4, P539, DOI 10.1109/TMM.2002.806536
   McManus JM, 1996, IEEE J SEL AREA COMM, V14, P1087, DOI 10.1109/49.508280
   MIAO Z, 1999, 9 INT PACKET WORKSH
   Moser M, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P86, DOI 10.1109/MMCS.1996.534959
   Moser M, 1997, IEICE T FUND ELECTR, VE80A, P582
   Pitkow J, 1994, P 2 INT WWW C, P1039
   REJAIE R, 2000, INFOCOM 2000 MAR
   REXFORD J, 1999, GLOB INT S DEC
   RIZZO L, UCL CS RS
   SALEHI JD, 1996, ACM SIGMETRICS, P222
   SEN S, 1999, INFOCOM 99 MAR
   Sen S., 1997, SPIE S VOIC VID DAT
   TEWARI R, 1998, P SPIE ACM C MULT CO
   WAGN Y, 1998, INFOCOM 98 APR
   WOOSTER R, 1997, 6 WWW C SANT CLAR CA
   [No title captured]
NR 29
TC 17
Z9 20
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2004
VL 6
IS 4
BP 599
EP 610
DI 10.1109/TMM.2004.830819
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 838RC
UT WOS:000222729800008
DA 2024-07-18
ER

PT J
AU August, NJ
   Ha, DS
AF August, NJ
   Ha, DS
TI Low power design of DCT and IDCT for low bit rate video codecs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE discrete cosine transform (DCT); H.263; inverse discrete cosine
   transform (IDCT); low power; video codec
ID DCT/IDCT PROCESSOR; ALGORITHM; ARCHITECTURE; EFFICIENT
AB This paper examines low power design techniques for discrete cosine transform (DCT) and inverse discrete cosine transform (IDCT) circuits applicable for low bit rate wireless video systems. The techniques include skipping DCT computation of low energy macroblocks, skipping IDCT computation of blocks with all coefficients equal to zero, using lower precision constant multipliers, gating the clock, and reducing transitions in the data path. The proposed DCT and IDCT circuits reduce power dissipation by, on average, 94% over baseline reference circuits.
C1 Virginia Polytech Inst & State Univ, Bradley Dept Elect & Comp Engn, VTVT, Blacksburg, VA 24061 USA.
C3 Virginia Polytechnic Institute & State University
RP Virginia Polytech Inst & State Univ, Bradley Dept Elect & Comp Engn, VTVT, Blacksburg, VA 24061 USA.
EM nateaugu@vt.edu; ha@vt.edu
OI Ha, Dong Sam/0000-0003-4838-1773
CR [Anonymous], 1999, document P.910, DOI 11.1002/1000/4751
   [Anonymous], 1998, Video coding for low bitrate communication
   Chang TS, 2000, IEEE T CIRC SYST VID, V10, P439, DOI 10.1109/76.836290
   CHEN J, 1999, P 1999 IEEE INT S CI, V1, P153
   Chen J, 2000, IEEE T MULTIMEDIA, V2, P111, DOI 10.1109/6046.845015
   Chen LG, 1998, PROCEEDINGS OF THE ASP-DAC '98 - ASIA AND SOUTH PACIFIC DESIGN AUTOMATION CONFERENCE 1998 WITH EDA TECHNO FAIR '98, P145, DOI 10.1109/ASPDAC.1998.669434
   CHEN WH, 1977, IEEE T COMMUN, V25, P1004, DOI 10.1109/TCOM.1977.1093941
   Chiang JS, 1996, NINTH ANNUAL IEEE INTERNATIONAL ASIC CONFERENCE AND EXHIBIT, PROCEEDINGS, P219, DOI 10.1109/ASIC.1996.551997
   CHO NI, 1991, IEEE T CIRCUITS SYST, V38, P297, DOI 10.1109/31.101322
   EROL B, 1998, 32 AS C SIGN SYST CO, V1, P462
   FEIG E, 1992, IEEE T SIGNAL PROCES, V40, P2174, DOI 10.1109/78.157218
   Hsiao SF, 1999, IEEE T CONSUM ELECTR, V45, P515, DOI 10.1109/30.793535
   Hsiao SF, 1999, INT CONF ACOUST SPEE, P3517, DOI 10.1109/ICASSP.1999.757601
   Huang YM, 1998, IEEE T CONSUM ELECTR, V44, P376, DOI 10.1109/30.681953
   JANG YF, 1994, IEEE T CONSUM ELECTR, V40, P703, DOI 10.1109/30.320861
   KIM HB, 1999, THESIS VIRGINIA POLY
   KIM IK, 1999, P 6 INT C VLSI CAD O, P557
   Kim K, 1999, IEEE T CONSUM ELECTR, V45, P62, DOI 10.1109/30.754418
   KIM K, 1999, P 1 IEEE AS PAC C AS, P135
   KUHLMANN M, 1998, 32 AS C SIGN SYST CO, V2, P1214
   LEE BG, 1984, IEEE T ACOUST SPEECH, V32, P1243
   Lee YP, 1997, IEEE T CIRC SYST VID, V7, P459
   Li J, 1996, NINTH ANNUAL IEEE INTERNATIONAL ASIC CONFERENCE AND EXHIBIT, PROCEEDINGS, P309, DOI 10.1109/ASIC.1996.552017
   MADISETTI A, 1995, IEEE T CIRC SYST VID, V5, P158, DOI 10.1109/76.388064
   Pao IM, 1999, IEEE T CIRC SYST VID, V9, P608, DOI 10.1109/76.767126
   SCHIMPFLE CV, 1997, 31 AS C SIGN SYST CO, V1, P729
   SCOPA E, 1995, P 1995 ICASSP, V5, P3271
   SHIN KW, 1994, P IEEE INT S CIRC SY, V4, P47
   Srinivasan V, 1996, IEEE T CIRC SYST VID, V6, P87, DOI 10.1109/76.486423
   Xanthopoulos T, 1999, IEEE J SOLID-ST CIRC, V34, P693, DOI 10.1109/4.760381
   Xanthopoulos T, 2000, IEEE J SOLID-ST CIRC, V35, P740, DOI 10.1109/4.841502
   Yu A, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P21, DOI 10.1145/266180.266326
   1998, ITURBT50010
NR 33
TC 31
Z9 35
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 414
EP 422
DI 10.1109/TMM.2004.827491
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200004
OA Green Published
DA 2024-07-18
ER

PT J
AU Dumitrescu, S
   Wu, XL
   Wang, Z
AF Dumitrescu, S
   Wu, XL
   Wang, Z
TI Globally optimal uneven error-protected packetization of scalable code
   streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE joint source-channel coding; multimedia streaming; optimization; uneven
   error protection
AB In this paper, we present a family of new algorithms for rate-fidelity optimal packetization of scalable source bit streams with uneven error protection. In the most general setting where no assumption is made on the probability function of packet loss or on the rate-fidelity function of the scalable code stream, one of our algorithms can find the globally optimal solution to the problem in O((NL2)-L-2) time, compared to a previously obtained O((NL2)-L-3) complexity, where N is the number of packets and L is the packet payload size. If the rate-fidelity function of the input is convex, the time complexity can be reduced to O(NL2) for a class of erasure channels, including channels for which the probability function of losing n packets is monotonically decreasing in n and independent erasure channels with packet erasure rate no larger than N/2(N + 1). Furthermore, our O (N L-2) algorithm for the convex case can be modified to find an approximation solution for the general case. All of our algorithms do away with the expediency of fractional bit allocation, a limitation of some existing algorithms.
C1 McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
C3 McMaster University
RP McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
EM sorina@mail.ece.mcmaster.ca; xwu@mail.ece.mcmaster.ca;
   zwang@grads.ece.mcmaster.ca
CR AGGARWAL A, 1987, ALGORITHMICA, V2, P195, DOI 10.1007/BF01840359
   Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   [Anonymous], 1997, Pattern matching algorithms
   Chande V, 1999, IEEE DATA COMPR CONF, P52, DOI 10.1109/DCC.1999.755654
   DAVIS G, 1996, SPIE C WAV APPL DIG, P376
   Horn U, 1999, SIGNAL PROCESS-IMAGE, V15, P77, DOI 10.1016/S0923-5965(99)00025-9
   Mohr AE, 2000, IEEE IMAGE PROC, P367, DOI 10.1109/ICIP.2000.900971
   Mohr AE, 1999, IEEE DATA COMPR CONF, P92, DOI 10.1109/DCC.1999.755658
   Puri R., 1999, Proceedings of the 33rd Asilomar Confe. Signals, Systems, V1, P342
   Sachs DG, 2000, PROC SPIE, V3974, P300, DOI 10.1117/12.382963
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Sherwood PG, 1997, IEEE DATA COMPR CONF, P72, DOI 10.1109/DCC.1997.581971
   Stankovic V, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P865, DOI 10.1109/ICME.2002.1035919
   STANKOVIC V, 2002, P IEEE INT C IM PROC, V2, P165
   STANKOVIC V, 2001, P MMSP 01 IEEE WORKS
   STOCKHAMMER T, 2001, P 11 INT PACK VID WO
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
NR 17
TC 50
Z9 58
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 230
EP 239
DI 10.1109/TMM.2003.822793
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400002
DA 2024-07-18
ER

PT J
AU Reibman, AR
   Vaishampayan, VA
   Sermadevi, Y
AF Reibman, AR
   Vaishampayan, VA
   Sermadevi, Y
TI Quality monitoring of video over a packet network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE quality-of-service; video packet loss; video quality
AB We consider monitoring the quality of compressed video transmitted over a packet network from the perspective of a network service provider. Our focus is on no-reference methods, which do not access the original signal, and on evaluating the impact of packet losses on quality. We present three methods to estimate mean squared error (MSE) due to packet losses directly from the video bitstream. NoParse uses only network-level measurements (like packet loss rate), QuickParse extracts the spatio-temporal extent of the impact of the loss, and FullParse extracts sequence-specific information including spatio-temporal activity and the effects of error propagation. Our simulation results with MPEG-2 video subjected to Transport Packet losses illustrate the performance possible using the three methods.
C1 AT&T Labs Res, Florham Pk, NJ 07932 USA.
C3 AT&T
RP AT&T Labs Res, Florham Pk, NJ 07932 USA.
EM amy@research.att.com; swamy@ece.cornell.edu
OI Vaishampayan, Vinay/0000-0001-7781-0990; Reibman,
   Amy/0000-0003-1859-1091
CR Draper N. R., 2014, Applied regression analysis
   HE Z, 2002, INT WORKSH PACK VID
   HEMAMI S, 2002, P ICIP SEP, P721
   KNEE M, INT BROADC CONV
   LIANG YJ, 2003, ICASSP 03 HONG KONG
   REIBMAN AR, 2002, AS C SIGN SYST COMP
   Snyder CW, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, pA457
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   TURAGA D, 2002, P ICIP SEP
   WOLF S, 1998, IN SERVICE PERFORMAN
   Wu DP, 2000, IEEE J SEL AREA COMM, V18, P977, DOI 10.1109/49.848251
   Yu ZH, 2002, P IEEE, V90, P154, DOI 10.1109/5.982412
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
NR 13
TC 91
Z9 112
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 327
EP 334
DI 10.1109/TMM.2003.822785
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400011
DA 2024-07-18
ER

PT J
AU Adjeroh, DA
   Lee, MC
AF Adjeroh, DA
   Lee, MC
TI Scene-adaptive transform domain video partitioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive video partitioning; scene classification; video indexing; video
   quality; video scene complexity
ID IMAGE; CLASSIFICATION; MODEL
AB An adaptive mechanism for video partitioning in the transform domain is proposed. Different quantitative measures for motion complexity and activity levels in a scene are defined, based on which a video scene can be consistently categorised into identifiable classes. Further, the video quality, as measured by the mean square error (MSE) is related to certain parameters used in video partitioning. Adaptability is realized by tailoring the parameters of the video partitioning algorithm to the specific characteristics of the video scene, as embodied in the video scene class and the video quality. Experimental results are included.
C1 W Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA.
   Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 West Virginia University; Chinese University of Hong Kong
RP W Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA.
EM don@csee.wvu.edu; mclee@cse.cuhk.edu.hk
CR Adjeroh DA, 1997, J VIS COMMUN IMAGE R, V8, P182, DOI 10.1006/jvci.1997.0349
   Aghagolzadeh S, 1991, IEEE T CIRC SYST VID, V1, P308, DOI 10.1109/76.120770
   Ahanger G, 1996, J VIS COMMUN IMAGE R, V7, P28, DOI 10.1006/jvci.1996.0004
   Arman F., 1994, MULTIMEDIA SYST, V1, P211, DOI DOI 10.1007/BF01268945
   Bouthemy P, 1999, IEEE T CIRC SYST VID, V9, P1030, DOI 10.1109/76.795057
   CHEN WH, 1977, IEEE T COMMUN, V25, P1285, DOI 10.1109/TCOM.1977.1093763
   Clarke R.J., 1985, TRANSFORM CODING IMA
   Courtney JD, 1997, PATTERN RECOGN, V30, P607, DOI 10.1016/S0031-3203(96)00107-0
   Dawood AM, 1999, IEEE T MULTIMEDIA, V1, P77, DOI 10.1109/6046.748173
   DIMITROVA N, 1995, ACM T INFORM SYST, V13, P408, DOI 10.1145/211430.211433
   DUFAUX F, 1995, P IEEE, V83, P858, DOI 10.1109/5.387089
   GRIMLETT JI, 1975, IEEE T COMMUN, V23, P785
   HASKELL BG, 1979, IMAGE TRANSMISSION T, P189
   LEGALL D, 1991, COMMUN ACM, V34, P46, DOI 10.1145/103085.103090
   Mandal MK, 1999, IMAGE VISION COMPUT, V17, P513, DOI 10.1016/S0262-8856(98)00143-7
   MESTER R, 1992, IEEE J SEL AREA COMM, V10, P913, DOI 10.1109/49.138996
   MUSMANN HG, 1985, P IEEE, V73, P523, DOI 10.1109/PROC.1985.13183
   Nagasaka A., 1995, Visual Database Systems II, P113
   NAKAJIMA H, 1994, ELECTRON COMM JPN 3, V77, P12
   Netravali A.N., 1995, DIGITAL PICTURES REP, V2nd
   NGAN KN, 1989, IEEE T ACOUST SPEECH, V37, P1743, DOI 10.1109/29.46556
   OHTA N, 1994, PACKET VIDEO MODELIN
   PEARSON DE, 1972, PICTURE BANDWIDTH CO, P472
   Pennebaker W. B., 1993, JPEG: Still image data compression standard
   PETHEL NV, 1997, PATTERN RECOGN, V30, P583
   Polana R., 1994, Journal of Visual Communication and Image Representation, V5, P172, DOI 10.1006/jvci.1994.1016
   PRATT W.K., 1991, DIGITAL IMAGE PROCES, V2
   Puri A, 1991, IEEE T CIRC SYST VID, V1, P351, DOI 10.1109/76.120774
   RAN XN, 1995, IEEE T IMAGE PROCESS, V4, P401, DOI 10.1109/83.370671
   SEYLER AJ, 1965, IEEE T INFORM THEORY, V11, P31, DOI 10.1109/TIT.1965.1053735
   Sudhir G, 1996, J VIS COMMUN IMAGE R, V7, P354, DOI 10.1006/jvci.1996.0031
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   ZHANG HJ, 1995, MULTIMED TOOLS APPL, V1, P91
   [No title captured]
NR 34
TC 9
Z9 11
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 58
EP 69
DI 10.1109/TMM.2003.819578
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200005
DA 2024-07-18
ER

PT J
AU Gevers, T
   Stokman, H
AF Gevers, T
   Stokman, H
TI Classifying color edges in video into shadow-geometry, highlight, or
   material transitions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive edge thresholding; color edge detection; color invariance; edge
   classification; noise propagation; video
ID REFLECTION
AB We aim at using color information to classify the physical nature of edges in video. To achieve physics-based edge classification, we first propose a novel approach to color edge detection by automatic noise-adaptive thresholding derived from sensor noise analysis. Then, we present a taxonomy on color edge types. As a result, a parameter-free edge classifier is obtained labeling color transitions into one of the following types: 1) shadow-geometry, 2) highlight edges, and 3) material edges. The proposed method is empirically verified on images showing complex real world scenes.
C1 Univ Amsterdam, Inst Informat, NL-1098 SJ Amsterdam, Netherlands.
C3 University of Amsterdam
RP Univ Amsterdam, Inst Informat, Kruislaan 403, NL-1098 SJ Amsterdam, Netherlands.
EM gevers@science.uva.nl
CR [Anonymous], 1982, An Introduction to Error Analysis
   [Anonymous], INT J PATTERN RECOGN
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Gevers T, 1999, PATTERN RECOGN, V32, P453, DOI 10.1016/S0031-3203(98)00036-3
   SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409
   Stauder J, 1999, IEEE T MULTIMEDIA, V1, P65, DOI 10.1109/6046.748172
   Tsang WH, 1997, PATTERN RECOGN LETT, V18, P165, DOI 10.1016/S0167-8655(96)00125-0
   Zhang W, 1997, INT J COMPUT VISION, V24, P219, DOI 10.1023/A:1007923307644
NR 8
TC 58
Z9 66
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2003
VL 5
IS 2
BP 237
EP 243
DI 10.1109/TMM.2003.811620
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 695HB
UT WOS:000183824100008
DA 2024-07-18
ER

PT J
AU Anand, D
   Togou, MA
   Muntean, GM
AF Anand, Devanshu
   Togou, Mohammed Amine
   Muntean, Gabriel-Miro
TI A Machine Learning Solution for Video Delivery to Mitigate Co-Tier
   Interference in 5G HetNets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HetNets; COVID-19; interference; machine learning; QoS; statistical
   visualization
ID CLUSTERING-ALGORITHM; RESOURCE-ALLOCATION; FEMTOCELL; MANAGEMENT;
   NETWORKS
AB The exponential demand for multimedia services is one reason behind the substantial growth of mobile data traffic. Video traffic patterns have significantly changed in the past two years due to the coronavirus disease (COVID-19). The worldwide pandemic has caused many individuals to work from home and use various online video platforms (e.g., Zoom, Google Meet, and Microsoft Teams). As a result, overloaded macrocells are unable to ensure high Quality of Experience (QoE) to all users. Heterogeneous Networks (HetNets) consisting of small cells (femtocells) and macrocells are a promising solution to mitigate this problem. A critical challenge with the deployment of femtocells in HetNets is the interference management between Macro Base Stations (MBSs), Femto Base Stations (FBSs), and between FBS and FBS. Indeed, the dynamic deployment of femtocells can lead to co-tier interference. With the rolling out of the 5G mobile network, it becomes imperative for mobile operators to maintain network capacity and manage different types of interference. Machine Learning (ML) is considered a promising solution to many challenges in 5G HetNets. In this paper, we propose a Machine Learning Interference Classification and Offloading Scheme (MLICOS) to address the problem of co-tier interference between femtocells for video delivery. Two versions of MLICOS, namely, MLICOS1 and MLICOS2, are proposed. The former uses conventional ML classifiers while the latter employs advanced ML algorithms. Both versions of MLICOS are compared with the classic Proportional Fair (PF) scheduling algorithm, Variable Radius and Proportional Fair scheduling (VR + PF) algorithm, and a Cognitive Approach (CA). The ML models are assessed based on the prediction accuracy, precision, recall and F-measure. Simulation results show that MLICOS outperforms the other schemes by providing the highest throughput and the lowest delay and packet loss ratio. A statistical analysis was also carried out to depict the degree of interference faced by users when different schemes are employed.
C1 [Anand, Devanshu; Togou, Mohammed Amine; Muntean, Gabriel-Miro] Dublin City Univ, Sch Elect Engn, Dublin, Ireland.
C3 Dublin City University
RP Anand, D (corresponding author), Dublin City Univ, Sch Elect Engn, Dublin, Ireland.
EM devanshu.anand2@mail.dcu.ie; mohammedamine.togou@dcu.ie;
   gabriel.muntean@dcu.ie
OI Anand, Devanshu/0000-0001-8211-9192; Muntean,
   Gabriel-Miro/0000-0002-9332-4770
FU Science Foundation Ireland [18/CRT/6183, 12/RC/2289_P2]
FX This work was supported by Science Foundation Ireland under Grants
   18/CRT/6183 to ML-LABS Centre for Research Training and 12/RC/2289_P2 to
   Insight Centre for Data Analytics.
CR Agarwal B, 2021, IEEE INT CONF COMM, DOI 10.1109/ICCWorkshops50388.2021.9473705
   Agarwal B, 2020, C LOCAL COMPUT NETW, P497, DOI 10.1109/LCN48667.2020.9314834
   Alomari M. S., 2016, Indian J. Sci. Technol., V9, P1
   Alotaibi AA, 2019, COMPUT COMMUN, V134, P163, DOI 10.1016/j.comcom.2018.07.013
   Alotaibi S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21144843
   Anand D, 2021, IEEE INT SYM BROADB, DOI 10.1109/BMSB53066.2021.9547176
   [Anonymous], 2020, Cisco Annual Internet Report-Cisco Annual Internet Report (2018-2023) White Paper
   Balasubramanian B, 2021, IEEE INTERNET COMPUT, V25, P7, DOI 10.1109/MIC.2021.3062487
   Berstad TJD, 2018, IEEE INT SYM MULTIM, P1, DOI 10.1109/ISM.2018.00009
   Cano Lengua M. A., 2020, P IEEE ENG INT RES C, P1
   Chen G, 2013, IEEE GLOB COMM CONF, P4717, DOI 10.1109/GLOCOMW.2013.6855696
   Chen MZ, 2019, IEEE COMMUN SURV TUT, V21, P3039, DOI 10.1109/COMST.2019.2926625
   Chen Y., 2013, Mathematical modelling of end-to-end packet delay in multi-hop wireless networks and their applications to QoS provisioning
   Dai JY, 2016, DIGIT COMMUN NETW, V2, P175, DOI 10.1016/j.dcan.2016.10.002
   Farhan N, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22041598
   Fath N, 2016, ASIA-PAC CONF COMMUN, P281, DOI 10.1109/APCC.2016.7581446
   Gür G, 2010, IEEE WIREL COMMUN, V17, P62, DOI 10.1109/MWC.2010.5547923
   Hassan T., 2019, Sensors, V19
   Kamal MA, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21196588
   Kamruzzaman M, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22031063
   Ladipo OI, 2019, COGENT ENG, V6, DOI 10.1080/23311916.2019.1691358
   Larsen LMP, 2019, IEEE COMMUN SURV TUT, V21, P146, DOI 10.1109/COMST.2018.2868805
   Liu FF, 2020, IEEE ACCESS, V8, P72548, DOI 10.1109/ACCESS.2020.2987360
   Medar R, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTING, COMMUNICATION, CONTROL AND AUTOMATION (ICCUBEA)
   O-RAN Alliance, 2022, About us
   Osama M, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10141723
   Owoyele O., 2021, arXiv
   Parker BM, 2015, QUEUEING SYST, V79, P365, DOI 10.1007/s11134-014-9421-y
   Pyun S.-Y., 2018, Mobile Inf. Syst., V2018, P1
   Roshan M., 2018, EAI Endorsed Trans. Mobile Commun. Appl., V4
   Samal S. R., 2018, J. Mobile Multimedia, V14, P273
   Sathya V, 2013, IEEE CONF WIREL MOB, P553, DOI 10.1109/WiMOB.2013.6673412
   Schulz P, 2021, IEEE T MULTIMEDIA, V23, P911, DOI 10.1109/TMM.2020.2990078
   Shgluof I, 2017, IEEE ACCESS, V5, P9032, DOI 10.1109/ACCESS.2017.2695518
   Soleimani B, 2020, IEEE T COMMUN, V68, P1746, DOI 10.1109/TCOMM.2018.2881464
   Susanto M, 2017, 2017 15TH INTERNATIONAL CONFERENCE ON QUALITY IN RESEARCH (QIR) - INTERNATIONAL SYMPOSIUM ON ELECTRICAL AND COMPUTER ENGINEERING, P479, DOI 10.1109/QIR.2017.8168533
   Teng DY, 2017, PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P622, DOI 10.1109/CompComm.2017.8322619
   Tian R, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18082548
   Wahab A, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10103662
   Wang L.-C., 2017, P 26 WIR OPT COMM C, P1
   Wang W, 2013, IEEE COMMUN MAG, V51, P37, DOI 10.1109/MCOM.2013.6525593
   Yang G, 2020, IEEE ACCESS, V8, P78726, DOI 10.1109/ACCESS.2020.2989502
   Yang G, 2018, ETRI J, V40, P227, DOI 10.4218/etrij.2017-0084
   Yang L, 2020, NEUROCOMPUTING, V415, P295, DOI 10.1016/j.neucom.2020.07.061
   Zhang HB, 2016, IEEE ACCESS, V4, P8643, DOI 10.1109/ACCESS.2016.2635938
   Zhang L., 2010, IEEE 72 VEH TECHN C, P1
   Zhong H, 2021, IEEE T MULTIMEDIA, V23, P982, DOI 10.1109/TMM.2020.2991539
   Zhou YP, 2018, IEEE T MULTIMEDIA, V20, P2153, DOI 10.1109/TMM.2017.2781364
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
NR 49
TC 2
Z9 2
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5117
EP 5129
DI 10.1109/TMM.2022.3187607
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300036
OA hybrid
DA 2024-07-18
ER

PT J
AU Bai, Y
   Jiao, JL
   Lou, YH
   Wu, SS
   Liu, J
   Feng, XT
   Duan, LY
AF Bai, Yan
   Jiao, Jile
   Lou, Yihang
   Wu, Shengsen
   Liu, Jun
   Feng, Xuetao
   Duan, Ling-Yu
TI Dual-Tuning: Joint Prototype Transfer and Structure Regularization for
   Compatible Feature Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compatible feature learning; prototype transfer; structure
   regularization
AB Visual retrieval system faces frequent model update and deployment. It is a heavy workload to re-extract features of the whole database every time. Feature compatibility enables the learned new visual features to be directly compared with the old features stored in the database. In this way, when updating the deployed model, we can bypass the inflexible and time-consuming feature re-extraction process. However, the old feature space that needs to be compatible is not ideal and faces outlier samples. Besides, the new and old models may be supervised by different losses, which will further causes distribution discrepancy problem between these two feature spaces. In this article, we propose a global optimization Dual-Tuning method to obtain feature compatibility against different networks and losses. A feature-level prototype loss is proposed to explicitly align two types of embedding features, by transferring global prototype information. Furthermore, we design a component-level mutual structural regularization to implicitly optimize the feature intrinsic structure. Experiments are conducted on six datasets, including person ReID datasets, face recognition datasets, and million-scale ImageNet and Place365. Experimental results demonstrate that our Dual-Tuning is able to obtain feature compatibility without sacrificing performance.
C1 [Bai, Yan; Duan, Ling-Yu] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Bai, Yan; Lou, Yihang; Wu, Shengsen; Duan, Ling-Yu] Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Jiao, Jile; Feng, Xuetao] Zhejiang Lianhe Technol Co Ltd, Alibaba Grp, Beijing 100016, Peoples R China.
   [Liu, Jun] Singapore Univ Technol & Design, Informat Syst Technol & Design Pillar, Singapore 487372, Singapore.
C3 Peng Cheng Laboratory; Peking University; Alibaba Group; Singapore
   University of Technology & Design
RP Duan, LY (corresponding author), Peng Cheng Lab, Shenzhen 518055, Peoples R China.
EM yanbai@pku.edu.cn; jile.jjl@alibaba-inc.com; yihanglou@pku.edu.cn;
   sswu@pku.edu.cn; jun_liu@sutd.edu.sg; xuetao.fxtg@alibaba-inc.com;
   lingyu@pku.edu.cn
OI Liu, Jun/0000-0002-4365-4165
FU National Natural Science Foundation of China [62088102]; PKU-NTU Joint
   Research Institute (JRI) from the Ng Teng Fong Charitable Foundation
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62088102 and in part by PKU-NTU Joint
   Research Institute (JRI) sponsored by the Donation from the Ng Teng Fong
   Charitable Foundation.
CR Anbang Yao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P294, DOI 10.1007/978-3-030-58555-6_18
   [Anonymous], 2018, PROC EUR C COMPUT VI
   [Anonymous], 2009, Advances in neural information processing systems
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Budnik M, 2021, PROC CVPR IEEE, P8224, DOI 10.1109/CVPR46437.2021.00813
   De Lange M, 2022, IEEE T PATTERN ANAL, V44, P3366, DOI 10.1109/TPAMI.2021.3057446
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Duggal R, 2021, PROC CVPR IEEE, P10718, DOI 10.1109/CVPR46437.2021.01058
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao Z, 2021, IEEE T MULTIMEDIA, V23, P3332, DOI 10.1109/TMM.2020.3023784
   Ge Y., 2020, P NIPS, V33, P11309
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hinton G., 2015, COMPUT SCI, V2
   Hu Jie, 2019, PROC IEEECVF C COMPU, P3004
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lou Y., 2019, IEEECONF COMPUT VIS, P3235
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Mallya A, 2018, LECT NOTES COMPUT SC, V11208, P72, DOI 10.1007/978-3-030-01225-0_5
   Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83
   Minghao Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12352, DOI 10.1109/CVPR42600.2020.01237
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   Qi C, 2017, IEEE IMAGE PROC, P2851, DOI 10.1109/ICIP.2017.8296803
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Ren PZ, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3447582
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen J, 2018, AAAI CONF ARTIF INTE, P4058
   Shen YT, 2020, PROC CVPR IEEE, P6367, DOI 10.1109/CVPR42600.2020.00640
   Shin H, 2017, ADV NEUR IN, V30
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Tang Z, 2019, IEEE I CONF COMP VIS, P211, DOI 10.1109/ICCV.2019.00030
   Wang Chien-Yi, 2020, arXiv
   Wang F, 2018, LECT NOTES COMPUT SC, V11213, P780, DOI 10.1007/978-3-030-01240-3_47
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang ZJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2104, DOI 10.1145/3394171.3413662
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yu L, 2019, PROC CVPR IEEE, P2902, DOI 10.1109/CVPR.2019.00302
   Zagoruyko S, 2017, Arxiv, DOI arXiv:1612.03928
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang JT, 2020, IEEE WINT CONF APPL, P1120, DOI [10.1109/WACV45572.2020.9093365, 10.1109/wacv45572.2020.9093365]
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong JC, 2020, Arxiv, DOI arXiv:2011.06182
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
NR 58
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7287
EP 7298
DI 10.1109/TMM.2022.3219680
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000042
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Bi, HY
   Xu, CH
   Shi, C
   Liu, GZ
   Li, YT
   Zhang, HH
   Qu, J
AF Bi, Hengyue
   Xu, Canhui
   Shi, Cao
   Liu, Guozhu
   Li, Yuteng
   Zhang, Honghong
   Qu, Jing
TI SRRV: A Novel Document Object Detector Based on Spatial-Related Relation
   and Vision
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Object detection; Proposals; Visualization;
   Cognition; Layout; Task analysis; Document object detection;
   spatial-related relation; graph convolutional Network; feature
   representation; document layout analysis
ID VIDEO SALIENCY DETECTION; NETWORK
AB Document object detection is a challenging task due to layout complexity and object diversity. Most of existing methods mainly focus on vision information, neglecting representative inherent spatial-related relationship among document objects. To capture structural information and contextual dependencies, we propose a novel document object detector based on spatial-related relation and vision (SRRV). It consists of three parts: vision feature extraction network, relation feature aggregation network and result refinement network. Vision feature extraction network enhances information propagation of hierarchical feature pyramid by adopting feature augmentation paths. Then, relation feature aggregation network combines graph construction module and graph learning module. Specifically, graph construction module calculates spatial information from geometric attributes of region proposals to encode relation information, while graph learning module stacks Graph Convolutional Network (GCN) layers to aggregate relation information at global scale. Both the vision and relation features are fed into result refinement network for feature fusion and relational reasoning. Experiments on the PubLayNet, POD and Article Regions datasets demonstrate that spatial relation information improves the performance with better accuracy and more precise bounding box prediction.
C1 [Bi, Hengyue; Xu, Canhui; Shi, Cao; Liu, Guozhu; Li, Yuteng; Zhang, Honghong; Qu, Jing] Qingdao Univ Sci & Technol, Sch Informat Sci & Technol, Qingdao 266061, Peoples R China.
C3 Qingdao University of Science & Technology
RP Liu, GZ (corresponding author), Qingdao Univ Sci & Technol, Sch Informat Sci & Technol, Qingdao 266061, Peoples R China.
EM bihengyue@qq.com; ccxu09@yeah.net; caoshi@yeah.net; lgz_0228@163.com;
   2420225023@qq.com; may_i_zhh@163.com; 969058112@qq.com
OI Bi, Hengyue/0000-0002-1578-3576
FU National Natural Science Foundation of China [61806107, 61702135];
   Shandong Key Laboratory of Wisdom Mine Information Technology; Opening
   Project of State Key Laboratory of Digital Publishing Technology
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61806107 and 61702135, in part by the
   Shandong Key Laboratory of Wisdom Mine Information Technology, and in
   part by the Opening Project of State Key Laboratory of Digital
   Publishing Technology.
CR Agarwal M, 2021, INT C PATT RECOG, P9491, DOI 10.1109/ICPR48806.2021.9411922
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.325
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P2350, DOI 10.1109/TIP.2021.3052069
   Chen CLZ, 2018, IEEE T MULTIMEDIA, V20, P3324, DOI 10.1109/TMM.2018.2839523
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   CHEN J, 2020, P IEEE CVF C COMPUT, V3, P392
   Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756
   Deng J., 2014, EUR C COMP VIS, P48
   Gao LC, 2017, PROC INT CONF DOC, P1417, DOI 10.1109/ICDAR.2017.231
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Herranz L, 2017, IEEE T MULTIMEDIA, V19, P430, DOI 10.1109/TMM.2016.2614861
   Jiang SQ, 2020, IEEE T IMAGE PROCESS, V29, P265, DOI 10.1109/TIP.2019.2929447
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kipf TN, 2017, INT C LEARN REPR
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lafferty John, 2001, INT C MACH LEARN ICM
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li K, 2020, P IEEECVF C COMPUTER, P12915
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li XH, 2018, INT C PATT RECOG, P3627, DOI 10.1109/ICPR.2018.8546073
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1153, DOI 10.1109/TMM.2019.2940851
   Li Z, 2020, P IEEE CVF WINT C AP, P1295
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Xiaojing, 2019, Graph Convolution for Multimodal Information Extraction from Visually Rich Documents
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Saha Ranajit, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P51, DOI 10.1109/ICDAR.2019.00018
   Schreiber S, 2017, PROC INT CONF DOC, P1162, DOI 10.1109/ICDAR.2017.192
   SHINYAMA Y, 2018, PDFMINER PYTHON PDF
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soto CX, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3464
   Tao X, 2014, COMPUT ELECTR ENG, V40, P1363, DOI 10.1016/j.compeleceng.2014.01.005
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu CH, 2021, J CENT SOUTH UNIV, V28, P1765, DOI 10.1007/s11771-021-4731-9
   Xu CH, 2021, IEEE ACCESS, V9, P143448, DOI 10.1109/ACCESS.2021.3121152
   Xu H, 2019, PROC CVPR IEEE, P9290, DOI 10.1109/CVPR.2019.00952
   Xu YH, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1192, DOI 10.1145/3394486.3403172
   Xu Zhong, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1015, DOI 10.1109/ICDAR.2019.00166
   Yang X, 2017, PROC CVPR IEEE, P4342, DOI 10.1109/CVPR.2017.462
   YEPES AJ, 2021, ICDAR 2021 COMPETITI
   Yi XH, 2017, PROC INT CONF DOC, P230, DOI 10.1109/ICDAR.2017.46
   YOUN J, 2019, P 2019 IEEE 90 VEH T, P1
   Zhang P, 2021, LECT NOTES COMPUT SC, V12821, P115, DOI 10.1007/978-3-030-86549-8_8
   Zhang Shi-Xue, 2020, IEEE C COMPUTER VISI, P9699
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
NR 52
TC 5
Z9 5
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3788
EP 3798
DI 10.1109/TMM.2022.3165717
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500019
DA 2024-07-18
ER

PT J
AU Chen, XQ
   Xu, XX
   Chen, JH
   Zhang, ZZ
   Takiguchi, T
   Hancock, ER
AF Chen, Xunquan
   Xu, Xuexin
   Chen, Jinhui
   Zhang, Zhizhong
   Takiguchi, Tetsuya
   Hancock, Edwin R.
TI Speaker-Independent Emotional Voice Conversion via Disentangled
   Representations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotional voice conversion; disentangled representation learning;
   adversarial learning; mutual information; speaker-independent
ID AUGMENTATION; NETWORKS; STARGAN; TIME
AB Emotional Voice Conversion (EVC) technology aims to transfer emotional state in speech while keeping the linguistic information and speaker identity unchanged. Prior studies on EVC have been limited to perform the conversion for a specific speaker or a predefined set of multiple speakers seen in the training stage. When encountering arbitrary speakers that may be unseen during training (outside the set of speakers used in training), existing EVC methods have limited conversion capabilities. However, converting the emotion of arbitrary speakers, even those unseen during the training procedure, in one model is much more challenging and much more attractive in real-world scenarios. To address this problem, in this study, we propose SIEVC, a novel speaker-independent emotional voice conversion framework for arbitrary speakers via disentangled representation learning. The proposed method employs the autoencoder framework to disentangle the emotion information and emotion-independent information of each input speech into separated representation spaces. To achieve better disentanglement, we incorporate mutual information minimization into the training process. In addition, adversarial training is applied to enhance the quality of the generated audio signals. Finally, speaker-independent EVC for arbitrary speakers could be achieved by only replacing the emotion representations of source speech with the target ones. The experimental results demonstrate that the proposed EVC model outperforms the baseline models in terms of objective and subjective evaluation for both seen and unseen speakers.
C1 [Chen, Xunquan; Takiguchi, Tetsuya] Kobe Univ, Grad Sch Syst Informat, Kobe 6578501, Japan.
   [Xu, Xuexin; Zhang, Zhizhong] Xiamen Univ, Xiamen 361005, Peoples R China.
   [Chen, Jinhui] Prefectural Univ Hiroshima, Hiroshima 7348558, Japan.
   [Hancock, Edwin R.] Univ York, Dept Comp Sci, York YO10 5GH, England.
C3 Kobe University; Xiamen University; University of York - UK
RP Chen, JH (corresponding author), Prefectural Univ Hiroshima, Hiroshima 7348558, Japan.
EM cxq0720@hotmail.com; xxxin555@stu.xmu.edu.cn; chen@rieb.kobe-u.ac.jp;
   zhihong@xmu.edu.cn; takigu@kobe-u.ac.jp; edwin.hancock@york.ac.uk
RI Hancock, Edwin/C-6071-2008
OI Hancock, Edwin/0000-0003-4496-2028
FU JSPS KAKENHI; Grants-in-Aid for Scientific Research [21H00906] Funding
   Source: KAKEN
FX No Statement Available
CR Aihara R., 2012, American Journal of Signal Processing, V2, P134
   Aihara R, 2014, ASIAPAC SIGN INFO PR
   Belghazi MI, 2018, PR MACH LEARN RES, V80
   Cao YX, 2020, INTERSPEECH, P3406, DOI 10.21437/Interspeech.2020-1647
   Chen MY, 2018, IEEE SIGNAL PROC LET, V25, P1440, DOI 10.1109/LSP.2018.2860246
   Cheng PY, 2020, PR MACH LEARN RES, V119
   Choi H, 2021, IEEE ACCESS, V9, P42674, DOI 10.1109/ACCESS.2021.3065460
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Chou JC, 2019, INTERSPEECH, P664, DOI 10.21437/Interspeech.2019-2663
   Chou JC, 2018, INTERSPEECH, P501
   DONSKER MD, 1975, COMMUN PUR APPL MATH, V28, P1, DOI 10.1002/cpa.3160280102
   Gao J, 2019, INTERSPEECH, P2858, DOI 10.21437/Interspeech.2019-2878
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu TY, 2020, INT CONF ACOUST SPEE, P3267, DOI [10.1109/icassp40776.2020.9054591, 10.1109/ICASSP40776.2020.9054591]
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Kameoka H, 2018, IEEE W SP LANG TECH, P266, DOI 10.1109/SLT.2018.8639535
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Krivokapic J, 2013, LAB PHONOL, V4, P39, DOI 10.1515/lp-2013-0003
   KUBICHEK RF, 1993, IEEE PACIF, P125, DOI 10.1109/PACRIM.1993.407206
   Kumar R., 2019, P NEURIPS, P14910
   Lee S.-H., 2021, P NEUR INF PROC SYST, V34, P294
   Liu LJ, 2018, INTERSPEECH, P1983
   Liu R, 2020, INT CONF ACOUST SPEE, P6274, DOI [10.1109/icassp40776.2020.9054681, 10.1109/ICASSP40776.2020.9054681]
   Liu SG, 2022, IEEE T MULTIMEDIA, V24, P1299, DOI 10.1109/TMM.2021.3063605
   Luo Z., 2016, P IEEE ACIS 15 INT C, P1
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Meng H, 2019, IEEE ACCESS, V7, P125868, DOI 10.1109/ACCESS.2019.2938007
   Morise M, 2016, IEICE T INF SYST, VE99D, P1877, DOI 10.1587/transinf.2015EDP7457
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P3793, DOI 10.1109/TMM.2020.3032037
   Ning HL, 2022, IEEE T MULTIMEDIA, V24, P1763, DOI 10.1109/TMM.2021.3071243
   Oord A. V. D., 2018, P ADV NEUR INF PROC, P1
   Peng X., 2019, INT C MACH LEARN, P5102
   Qian KZ, 2019, PR MACH LEARN RES, V97
   Raitio T, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P254
   Rizos G, 2020, INT CONF ACOUST SPEE, P3502, DOI [10.1109/ICASSP40776.2020.9054579, 10.1109/icassp40776.2020.9054579]
   Sanchez Eduardo Hugo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P205, DOI 10.1007/978-3-030-58542-6_13
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Veaux C., 2016, CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit
   Virtusio JJ, 2021, IEEE T MULTIMEDIA, V23, P2245, DOI 10.1109/TMM.2021.3087026
   Wang DS, 2021, INTERSPEECH, P1344, DOI 10.21437/Interspeech.2021-283
   Wang YX, 2017, INTERSPEECH, P4006, DOI 10.21437/Interspeech.2017-1452
   Xin DT, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P6608, DOI 10.1109/ICASSP39728.2021.9414226
   Xu H, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3056645
   Yuan S., 2021, P INT C LEARN REPR
   Zhang T, 2022, IEEE T AFFECT COMPUT, V13, P379, DOI 10.1109/TAFFC.2019.2937768
   Zhou K., 2020, P OD 2020 SPEAK LANG, P230
   Zhou K, 2021, INTERSPEECH, P811, DOI 10.21437/Interspeech.2021-781
   Zhou K, 2020, INTERSPEECH, P3416, DOI 10.21437/Interspeech.2020-2014
   Zhou K, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P920, DOI 10.1109/ICASSP39728.2021.9413391
   Zhou SP, 2021, AAAI CONF ARTIF INTE, V35, P6039
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 57
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7480
EP 7493
DI 10.1109/TMM.2022.3222646
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300002
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Gao, JN
   Kong, DH
   Wang, SF
   Li, JH
   Yin, BC
AF Gao, Junna
   Kong, Dehui
   Wang, Shaofan
   Li, Jinghua
   Yin, Baocai
TI DASI: Learning Domain Adaptive Shape Impression for 3D Object
   Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D reconstruction; 3D refinement; deep learning; domain adaptation;
   transformer
ID FACE RECONSTRUCTION; IMAGE
AB Previous 3D object reconstruction methods from 2D images involve two issues: the lack of in-depth exploration of the prior knowledge of 3D shapes, and the difficulty of dealing with the serious occluded parts. Inspired by human's perception on real-world objects which is composed of an overall impression (known as shape impression) and an enhanced cognition, we propose a deep network (denoted by DASI) to learn the Domain Adaptive Shape Impression for 3D reconstruction from arbitrary view images. DASI consists of two modules: shape reconstruction module and shape refinement module. The former module reconstructs a coarse volume by learning a domain adaptive shape impression as embedding in image-based reconstruction. We first leverage 3D objects to learn a shape impression being associated with prior knowledge of 3D objects. To attain consensus on shape impression from 2D images, we regard the 3D shape and the 2D image as two different domains. By adapting the two domains, the shape impression learned from 3D objects is transferred to 2D images and guides the images-based reconstruction. The latter module refines the objects by modeling the whole 3D volume to local 3D patches and exploring their intrinsic geometry relationships. Quantitative and qualitative experimental results on two benchmark datasets demonstrate that DASI outperforms several state-of-the-arts for 3D reconstruction from single and multi-view 2D images.
C1 [Gao, Junna; Kong, Dehui; Wang, Shaofan; Li, Jinghua; Yin, Baocai] Beijing Univ Technol, Beijing Inst Artificial Intelligence, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Kong, DH (corresponding author), Beijing Univ Technol, Beijing Inst Artificial Intelligence, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM jngao@emails.bjut.edu.cn; kdh@bjut.edu.cn; wangshaofan@bjut.edu.cn;
   lijinghua@bjut.edu.cn; ybc@bjut.edu.cn
OI WANG, SHAOFAN/0000-0002-3045-624X
FU National Natural Science Foundation of China [2021ZD0111900, 62172022,
   U21B2038, U19B2039, 61876012]; Beijing Natural Science Foundation
   [4202003]
FX The work was supported in part by the National Natural Science
   Foundation of China under Grants 2021ZD0111900, 62172022, U21B2038,
   U19B2039, and 61876012, and in part by Beijing Natural Science
   Foundation under Grant 4202003.
CR Agudo A, 2019, IEEE T MULTIMEDIA, V21, P821, DOI 10.1109/TMM.2018.2870081
   Baevski A., 2018, arXiv
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Broadhurst A, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P388, DOI 10.1109/ICCV.2001.937544
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Di XH, 2016, LECT NOTES COMPUT SC, V9915, P251, DOI 10.1007/978-3-319-49409-8_21
   Dibra E, 2017, PROC CVPR IEEE, P5504, DOI 10.1109/CVPR.2017.584
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Ferrari C, 2017, IEEE T MULTIMEDIA, V19, P2666, DOI 10.1109/TMM.2017.2707341
   Fuentes-Pacheco J, 2015, ARTIF INTELL REV, V43, P55, DOI 10.1007/s10462-012-9365-8
   Ghifary M, 2014, LECT NOTES ARTIF INT, V8862, P898, DOI 10.1007/978-3-319-13560-1_76
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Gwak J, 2017, INT CONF 3D VISION, P263, DOI 10.1109/3DV.2017.00038
   Häming K, 2010, KYBERNETIKA, V46, P926
   Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298
   Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253
   Ji P, 2017, IEEE I CONF COMP VIS, P929, DOI 10.1109/ICCV.2017.106
   Jiang L, 2018, IEEE T IMAGE PROCESS, V27, P4756, DOI 10.1109/TIP.2018.2845697
   Kar A, 2017, ADV NEUR IN, V30
   Kingma D. P., 2014, arXiv
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Kolen JF, 2009, A Field Guide to Dynamical Recurrent Networks, DOI [DOI 10.1109/9780470544037.CH14, 10.1109/9780470544037]
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Liu SB, 2010, PROC CVPR IEEE, P1530, DOI 10.1109/CVPR.2010.5539790
   Long MS, 2017, PR MACH LEARN RES, V70
   Lou JW, 2020, IEEE T MULTIMEDIA, V22, P730, DOI 10.1109/TMM.2019.2933338
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lun ZL, 2017, INT CONF 3D VISION, P67, DOI 10.1109/3DV.2017.00018
   Mandikal P, 2019, Arxiv, DOI arXiv:1807.07796
   Martin E, 2018, Arxiv, DOI arXiv:1709.04057
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Özyesil O, 2017, ACTA NUMER, V26, P305, DOI 10.1017/S096249291700006X
   Paschalidou D, 2018, PROC CVPR IEEE, P3897, DOI 10.1109/CVPR.2018.00410
   Rezende D.J., 2016, ADV NEURAL INFORM PR, P4996
   Richter SR, 2018, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR.2018.00207
   Richter SR, 2015, PROC CVPR IEEE, P1128, DOI 10.1109/CVPR.2015.7298716
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20
   Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962
   Tulsiani S, 2018, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR.2018.00306
   Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30
   Wang D., 2021, ICCV, P5722
   Wang M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P961, DOI 10.1145/3123266.3123340
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang Q, 2019, Arxiv, DOI [arXiv:1906.01787, DOI 10.48550/ARXIV.1906.01787]
   Wang W., 2017, P IEEE INT C COMP VI, P2298, DOI DOI 10.48550/ARXIV.1711.06375
   WITKIN AP, 1981, ARTIF INTELL, V17, P17, DOI 10.1016/0004-3702(81)90019-9
   Wu J., 2017, PROC ADVNEURAL INF P, V30, P153
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu PF, 2017, IEEE T MULTIMEDIA, V19, P266, DOI 10.1109/TMM.2016.2612761
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie HZ, 2019, IEEE I CONF COMP VIS, P2690, DOI 10.1109/ICCV.2019.00278
   Xie HZ, 2020, INT J COMPUT VISION, V128, P2919, DOI 10.1007/s11263-020-01347-6
   Yagubbayli F, 2022, Arxiv, DOI arXiv:2106.12102
   Yang B, 2020, INT J COMPUT VISION, V128, P53, DOI 10.1007/s11263-019-01217-w
   Yang B, 2017, IEEE INT CONF COMP V, P679, DOI 10.1109/ICCVW.2017.86
   Yang GD, 2018, LECT NOTES COMPUT SC, V11219, P90, DOI 10.1007/978-3-030-01267-0_6
   Yao Y, 2020, PROC CVPR IEEE, P528, DOI 10.1109/CVPR42600.2020.00061
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   Zhao MH, 2021, NEUROCOMPUTING, V430, P94, DOI 10.1016/j.neucom.2020.10.097
NR 67
TC 2
Z9 2
U1 14
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5248
EP 5262
DI 10.1109/TMM.2022.3189247
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300045
DA 2024-07-18
ER

PT J
AU Jia, J
   Gao, ZP
   Zhu, DD
   Min, XK
   Hu, MH
   Zhai, GT
AF Jia, Jun
   Gao, Zhongpai
   Zhu, Dandan
   Min, Xiongkuo
   Hu, Menghan
   Zhai, Guangtao
TI RIVIE: Robust Inherent Video Information Embedding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Watermarking; Decoding; Steganography; Distortion; Motion
   pictures; Training; Data hiding; display-camera communication; temporal
   consistency; 3D rendering; adversarial training
ID CONVOLUTIONAL NEURAL-NETWORK; IMAGE WATERMARKING; RESILIENT;
   STEGANOGRAPHY
AB Imagine an interesting situation when watching a movie, we can scan the screen using our smartphones to get some extra information about this movie such as the cast, the release date, the movie's homepage, etc. Our prospect is a world where each video contains invisible information that can be delivered to us through mobile devices with cameras. This paper proposes the first deep learning-based information hiding method for videos to achieve information transmission from screens to cameras. Compared with hiding information in single images, the methods for videos need to maintain visual quality in both spatial and temporal domains. Furthermore, the training of video models builds on a large video dataset, which needs much more computational resources than training models for images. To reduce the computational complexity, we propose to simulate data on-the-fly to generate simulated sequences from single images. Then, we use the simulated data to train a spatio-temporal generator that hides information in videos while maintaining visual quality. During training, a temporal loss function based on the simulated data is exploited to ensure the temporal consistency of generated videos. After embedding, we use a decoder to recover the hidden information. To simulate the imaging pipeline from screens to cameras in the real world, we insert a distortion network between the generator and decoder. The distortion network is based on differentiable 3D rendering to cover possible distortions introduced in the procedure of camera imaging. Experimental results show that the hidden information in videos can be extracted by cameras without impacting the visual quality. Our work can be applied to many fields, such as advertisement, entertainment, and education.
C1 [Jia, Jun; Min, Xiongkuo; Zhai, Guangtao] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
   [Gao, Zhongpai] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelli gence, Shanghai 200240, Peoples R China.
   [Gao, Zhongpai] United Imaging Intelligence, Cambridge, MA USA.
   [Zhu, Dandan] East China Normal Univ, Inst AI Educ, Shanghai 200241, Peoples R China.
   [Hu, Menghan] East China Normal Univ, Sch Commun & Elect Engn, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200241, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; East China
   Normal University; East China Normal University
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
EM jiajun0302@sjtu.edu.cn; gaozhongpai@sjtu.edu.cn; ddzhu@dhu.edu.cn;
   minxiongkuo@sjtu.edu.cn; mhhu@ce.ecnu.edu.cn; zhaiguangtao@sjtu.edu.cn
RI Hu, Menghan/AAK-7153-2021; Zhai, Guangtao/X-5949-2019; Gao,
   Zhongpai/ABA-9234-2021
OI Hu, Menghan/0000-0002-8557-8930; Zhai, Guangtao/0000-0001-8165-9322;
   Gao, Zhongpai/0000-0003-4344-4501
FU National Nature Science Foundation of China [61831015, 61901259,
   61901260, 62001289, 62225112, 62271312]; National Key R&D Program of
   China [2021YFE0206700]; China Postdoctoral Science Foundation
   [BX2019208]; Shanghai Municipal Science and Technology Major Project
   [2021SHZDZX0102]; Shanghai Pujiang Program [22PJ1407400]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grants 61831015, 61901259, 61901260, 62001289,
   62225112, and 62271312, in part by the National Key R&D Program of China
   under Grant 2021YFE0206700, in part by China Postdoctoral Science
   Foundation under Grant BX2019208, in part by Shanghai Municipal Science
   and Technology Major Project under Grant 2021SHZDZX0102, and in part by
   Shanghai Pujiang Program under Grant 22PJ1407400.
CR Zhang KA, 2019, Arxiv, DOI arXiv:1909.01285
   Zhang KA, 2019, Arxiv, DOI arXiv:1901.03892
   [Anonymous], 1760, Photometria sive de mensura et gradibus luminis, colorum et umbrae
   Buades A, 2020, IEEE T CIRC SYST VID, V30, P1960, DOI 10.1109/TCSVT.2019.2911877
   Chu WC, 2003, IEEE T MULTIMEDIA, V5, P34, DOI 10.1109/TMM.2003.808816
   Ehret T, 2019, PROC CVPR IEEE, P11361, DOI 10.1109/CVPR.2019.01163
   Eilertsen G, 2019, PROC CVPR IEEE, P11168, DOI 10.1109/CVPR.2019.01143
   Fang H, 2022, IEEE T MULTIMEDIA, V24, P955, DOI 10.1109/TMM.2021.3061801
   Fang H, 2021, IEEE T CIRC SYST VID, V31, P1436, DOI 10.1109/TCSVT.2020.3009349
   Fang H, 2020, IEEE T CIRC SYST VID, V30, P4075, DOI 10.1109/TCSVT.2019.2953720
   Fang H, 2019, IEEE T INF FOREN SEC, V14, P1403, DOI 10.1109/TIFS.2018.2878541
   Gao ZP, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1047, DOI 10.1145/2733373.2806398
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Isobe Takashi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8005, DOI 10.1109/CVPR42600.2020.00803
   Jia J, 2022, IEEE T CYBERNETICS, V52, P7094, DOI 10.1109/TCYB.2020.3037208
   Johnson NF, 1998, COMPUTER, V31, P26, DOI 10.1109/MC.1998.4655281
   Karybali IG, 2006, IEEE T INF FOREN SEC, V1, P256, DOI 10.1109/TIFS.2006.873652
   Kim Y, 2019, IEEE T CIRC SYST VID, V29, P2521, DOI 10.1109/TCSVT.2018.2864321
   Koppal S. J., 2014, Computer Vision: A Reference Guide, P441, DOI [DOI 10.1007/978-0-387-31439-6_534, 10.1007/978-0-387-31439-6_534]
   Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11
   Lei CY, 2019, PROC CVPR IEEE, P3748, DOI 10.1109/CVPR.2019.00387
   Li S, 2019, PROC CVPR IEEE, P10514, DOI 10.1109/CVPR.2019.01077
   Liu J, 2020, IEEE ACCESS, V8, P60575, DOI 10.1109/ACCESS.2020.2983175
   Lu G, 2020, IEEE T IMAGE PROCESS, V29, P1725, DOI 10.1109/TIP.2019.2943214
   Ma ZH, 2021, IEEE T CIRC SYST VID, V31, P4826, DOI 10.1109/TCSVT.2021.3055255
   Marvel LM, 1999, IEEE T IMAGE PROCESS, V8, P1075, DOI 10.1109/83.777088
   Mukherjee DP, 2004, IEEE T MULTIMEDIA, V6, P1, DOI 10.1109/TMM.2003.819759
   Pan JS, 2020, PROC CVPR IEEE, P3040, DOI 10.1109/CVPR42600.2020.00311
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3
   Shen W, 2020, PROC CVPR IEEE, P5113, DOI 10.1109/CVPR42600.2020.00516
   Shi HC, 2018, LECT NOTES COMPUT SC, V10735, P534, DOI 10.1007/978-3-319-77380-3_51
   Shi XJ, 2015, ADV NEUR IN, V28
   Shin R., 2017, P NIPS 2017 WORKSH M, V1, P8
   Tancik M, 2020, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR42600.2020.00219
   Tassano M, 2020, PROC CVPR IEEE, P1351, DOI 10.1109/CVPR42600.2020.00143
   Tian HW, 2013, IEEE T CYBERNETICS, V43, P2190, DOI 10.1109/TCYB.2013.2245415
   Valentin Julien, 2019, TensorFlow Graphics: Computer Graphics Meets Deep Learning
   Volkhonskiy D., 2019, PROC 12 INT C MACH V, P991
   Wang WJ, 2020, AAAI CONF ARTIF INTE, V34, P12233
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wengrowski E., 2017, PROC IEEE INT C COMP, P1
   Wengrowski E, 2019, PROC CVPR IEEE, P1515, DOI 10.1109/CVPR.2019.00161
   Xiyang Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13545, DOI 10.1109/CVPR42600.2020.01356
   Xu Y, 2019, IEEE I CONF COMP VIS, P7042, DOI 10.1109/ICCV.2019.00714
   Yang WH, 2020, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR42600.2020.00179
   Yang WH, 2019, PROC CVPR IEEE, P1661, DOI 10.1109/CVPR.2019.00176
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Yi P, 2020, IEEE T CIRC SYST VID, V30, P2503, DOI 10.1109/TCSVT.2019.2925844
   Yue HJ, 2020, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR42600.2020.00237
   Zhang B, 2019, PROC CVPR IEEE, P8044, DOI 10.1109/CVPR.2019.00824
   Zhang DY, 2021, IEEE T CIRC SYST VID, V31, P3954, DOI 10.1109/TCSVT.2020.3044451
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XQ, 2022, IEEE T CIRC SYST VID, V32, P1986, DOI 10.1109/TCSVT.2021.3093928
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 55
TC 3
Z9 3
U1 4
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7364
EP 7377
DI 10.1109/TMM.2022.3221894
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000048
DA 2024-07-18
ER

PT J
AU Jin, Z
   Huang, JJ
   Wang, WJ
   Xiong, AL
   Tan, XJ
AF Jin, Zhi
   Huang, Junjia
   Wang, Wenjin
   Xiong, Aolin
   Tan, Xiaojun
TI Estimating Human Weight From a Single Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual estimation of BMI; 2-D body imaging; Deep learning features;
   Anthropometric features; 2-Dimage-to-BMI dataset
ID BODY-MASS INDEX; HEART-RATE
AB Body weight, as one of the biometric traits, has been studied in both the forensic and medical domains. However, estimating weight directly from 2-D images is particularly challenging since visual inspection is rather sensitive to the distance between the subject and camera, even for frontal view images. In this case, the widely used body mass index (BMI), which is associated with body height and weight, can be employed as a measure of weight to indicate health conditions. Previous works on the estimation of BMI have predominantly focused on using multiple 2-D images, 3-D images, or facial images; however, these cues are not always available. To address this issue, we explore the feasibility of obtaining BMI from a single 2-D body image with the dual-branch regression framework proposed in this work. More specifically, the framework comprises an anthropometric feature computation branch and a deep learning-based feature extraction branch. One aggregation layer maps all the features to an estimated BMI value. In addition, a new public 2-D image-to-BMI dataset, which contains 4189 images (1477 males and 2712 females) from approximately 3000 subjects with attributes including gender, age, height, and weight, was collected and released to facilitate the study. Extensive experiments confirm that the proposed framework combining anthropometric features and deep features outperforms the single-type feature approaches to BMI estimation in most cases.
C1 [Jin, Zhi; Huang, Junjia; Xiong, Aolin; Tan, Xiaojun] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou 518107, Guangdong, Peoples R China.
   [Jin, Zhi] Sun Yat sen Univ, Sch Intelligent Syst Engn, Shenzhen Campus, Guangzhou 518107, Guangdong, Peoples R China.
   [Jin, Zhi] Guangdong Prov Key Lab Fire Sci & Technol, Guangzhou 510006, Peoples R China.
   [Wang, Wenjin] Southern Univ Sci & Technol, Shenzhen 518055, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Southern University of
   Science & Technology
RP Tan, XJ (corresponding author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou 518107, Guangdong, Peoples R China.
EM jinzh26@mail.sysu.edu.cn; huangjj77@mail2.sysu.edu.cn; wwang@tue.nl;
   xiongaolin@mail2.sysu.edu.cn; tanxj@mail.sysu.edu.cn
RI Jin, Zhi/AAB-2440-2022; TAN, XJ/JHS-5357-2023
OI Jin, Zhi/0000-0001-9670-7366; Tan, Xiaojun/0000-0003-0137-9270
FU Key-Area Research and Development Program of Guangdong Province
   [2020B090921003]; National Natural Science Foundation of China
   [62071500]; Shenzhen Science and Technology Program
   [GXWD20201231165807008, 2021A26]
FX This work was supported in part by Key-Area Research and Development
   Program of Guangdong Province under Grant 2020B090921003, in part by the
   National Natural Science Foundation of China under Grant 62071500, and
   in part by the Shenzhen Science and Technology Program under Grants
   GXWD20201231165807008 and 2021A26.& nbsp;
CR Awad M., 2015, Neural Information Processing-Letters and Reviews, P67, DOI [DOI 10.1007/978-1-4302-5990-94, 10.1007/978-1-4302-5990-9_4, DOI 10.1007/978-1-4302-5990-9_4]
   Blaak E, 2001, CURR OPIN CLIN NUTR, V4, P499, DOI 10.1097/00075197-200111000-00006
   Blewitt ME, 2008, NAT GENET, V40, P663, DOI 10.1038/ng.142
   BUSHBY KMD, 1992, ARCH DIS CHILD, V67, P1286, DOI 10.1136/adc.67.10.1286
   Clever HM, 2018, IEEE INT C INT ROBOT, P54, DOI 10.1109/IROS.2018.8593545
   Dantcheva A, 2018, INT C PATT RECOG, P3555, DOI 10.1109/ICPR.2018.8546159
   Dantcheva A, 2016, IEEE T INF FOREN SEC, V11, P441, DOI 10.1109/TIFS.2015.2480381
   Dantcheva A, 2011, MULTIMED TOOLS APPL, V51, P739, DOI 10.1007/s11042-010-0635-7
   Davoodnia V, 2020, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-02210-9
   Eknoyan G, 2008, NEPHROL DIAL TRANSPL, V23, P47, DOI 10.1093/ndt/gfm517
   He K., 2017, IEEE C COMP VIS PATT, P2961, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Junjia, 2021, 2021 IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME51207.2021.9428234
   Jiang M, 2019, IEEE T INF FOREN SEC, V14, P2676, DOI 10.1109/TIFS.2019.2904840
   Johnson Clifford L, 2013, Vital Health Stat 2, P1
   Kingma D. P., 2014, arXiv
   Kocabey E., 2017, Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social Media, V11, P572
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee BJ, 2014, BMC COMPLEM ALTERN M, V14, DOI 10.1186/1472-6882-14-248
   Li P., IN PRESS
   Molarius A, 1998, INT J OBESITY, V22, P719, DOI 10.1038/sj.ijo.0800660
   Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1
   Nahavandi D, 2017, 2017 IEEE INTERNATIONAL SYMPOSIUM ON SYSTEMS ENGINEERING (ISSE 2017), P19
   Nguyen TV, 2014, J COMPUT SCI TECH-CH, V29, P777, DOI 10.1007/s11390-014-1467-0
   Parkhi OM, 2015, Proceedings of the British Machine Vision Conference, DOI DOI 10.5244/C.29.41
   Pfitzner C., 2016, P SPIE, V9784, P524
   Pfitzner C, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051311
   Pouyan MB, 2017, 2017 IEEE EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL & HEALTH INFORMATICS (BHI), P65, DOI 10.1109/BHI.2017.7897206
   Qiu Y, 2019, IEEE T MULTIMEDIA, V21, P1778, DOI 10.1109/TMM.2018.2883866
   RAMSEY PH, 1989, J EDUC STAT, V14, P245, DOI 10.2307/1165017
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Spearman C, 2010, INT J EPIDEMIOL, V39, P1137, DOI 10.1093/ije/dyq191
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI [arXiv:1807.11626, 10.1109/CVPR.2019.00293]
   Vazquez G, 2007, EPIDEMIOL REV, V29, P115, DOI 10.1093/epirev/mxm008
   Velardo C., 2012, 2012 IEEE International Conference on Emerging Signal Processing Applications, P67, DOI 10.1109/ESPA.2012.6152447
   Velardo C., 2010, Biometrics: Theory Applications and Systems (BTAS), 2010 Fourth IEEE International Conference on, P1, DOI DOI 10.1109/BTAS.2010.5634540
   Velardo C, 2012, EUR SIGNAL PR CONF, P1980
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wen LY, 2013, IMAGE VISION COMPUT, V31, P392, DOI 10.1016/j.imavis.2013.03.001
   WHO Consultation, 2000, WHO TECH REP SER, V894, P1
   Williams CKI, 1996, ADV NEUR IN, V8, P514
   Wu Y., 2019, Detectron 2
   Yang C, 2017, IEEE T MULTIMEDIA, V19, P1625, DOI 10.1109/TMM.2017.2672198
   Zhang SH, 2019, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2019.00098
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
NR 49
TC 4
Z9 4
U1 3
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2515
EP 2527
DI 10.1109/TMM.2022.3147945
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600005
DA 2024-07-18
ER

PT J
AU Lee, S
   Ryu, C
   Park, E
AF Lee, SangEun
   Ryu, Chaeeun
   Park, Eunil
TI OSANet: Object Semantic Attention Network for Visual Sentiment Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Feature extraction; Sentiment analysis; Task
   analysis; Emotion recognition; Image color analysis; Visual sentiment
   analysis; emotional inference; neural network; semantic information;
   salient objects
AB Visual sentiment analysis aims to predict human emotional responses to visual stimuli. It has attracted considerable attention owing to the increasing popularity of online image sharing. Most researchers have focused on improving emotion recognition using holistic and local information derived from given images. Relatively less attention has been paid to the semantic information of objects in images, which influences human emotional responses to the images. Therefore, we propose a novel object semantic attention network (OSANet) that attempts to unravel the semantic information of objects in images that contribute to emotion detection. The OSANet combines both global representation and semantic information of objects to predict the emotion elicited by a given image. First, the holistic features that represent the entire image are extracted using convolutional blocks. Subsequently, the object-level semantic information is obtained from pre-trained word embedding and then weighted according to the relative importance of the object using the attention mechanism. Notably, a new loss function to address the subjectivity of sentiment analysis is introduced, which improves the performance of the emotion detection task. Extensive experiments on three image emotion datasets demonstrated the superiority and interpretability of the OSANet. The results show that the OSANet outperforms extant image emotion detection frameworks.
C1 [Lee, SangEun; Park, Eunil] Sungkyunkwan Univ, Dept Appl Artificial Intelligence, Seoul 03063, South Korea.
   [Ryu, Chaeeun] Sungkyunkwan Univ, Dept Comp Educ, Seoul 03063, South Korea.
C3 Sungkyunkwan University (SKKU); Sungkyunkwan University (SKKU)
RP Park, E (corresponding author), Sungkyunkwan Univ, Dept Appl Artificial Intelligence, Seoul 03063, South Korea.
EM sange1104@g.skku.edu; superbunny38@gmail.com; eunilpark@skku.edu
RI Lee, SangEun/JXN-7992-2024
OI Lee, SangEun/0000-0002-3828-6601
FU Gyeonggi Province [2022-001]; National Research Foundation of Korea -
   Korean Government [2021R1A4A3022102, NRF-2020R1C1C1004324]
FX This work was supported in part by Gyeonggi Province through Gyeonggi-do
   Researcher-Centered R & D Support Project under Grant 2022-001 and in
   part by the National Research Foundation of Korea Funded by the Korean
   Government under Grants 2021R1A4A3022102 and NRF-2020R1C1C1004324
CR Barbieri F, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4766
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   BOVIK AC, 1990, IEEE T PATTERN ANAL, V12, P55, DOI 10.1109/34.41384
   Chandrasekaran G., 2021, PROC IBEROAMERICAN K, P260
   Chen T, 2014, Arxiv, DOI arXiv:1410.8586
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P1031, DOI 10.1109/TMM.2015.2430819
   Gelli F, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P907, DOI 10.1145/2733373.2806361
   Geusebroek Jan -Mark, 2006, BMVC, P1029
   Guo LT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P765, DOI 10.1145/3343031.3350943
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Katsurai M, 2016, INT CONF ACOUST SPEE, P2837, DOI 10.1109/ICASSP.2016.7472195
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee S, 2022, KNOWL-BASED SYST, V242, DOI 10.1016/j.knosys.2022.108437
   Li T., 2021, PROC IEEE INT C MULT, P1
   Lu Xin, 2012, Proc ACM Int Conf Multimed, V2012, P229, DOI 10.1145/2393347.2393384
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Mao LC, 2019, IEEE ACCESS, V7, P172231, DOI 10.1109/ACCESS.2019.2956508
   Marín-Morales J, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185163
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Oh S, 2022, INF TECHNOL TOUR, V24, P109, DOI 10.1007/s40558-022-00222-z
   Ortis A, 2020, IET IMAGE PROCESS, V14, P1440, DOI 10.1049/iet-ipr.2019.1270
   Ou HC, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21062136
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Rao TR, 2020, NEURAL PROCESS LETT, V51, P2043, DOI 10.1007/s11063-019-10033-9
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruan SL, 2024, IEEE T MULTIMEDIA, V26, P4097, DOI 10.1109/TMM.2021.3118208
   Siersdorfer S., 2010, ACM MM, P715
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Solli M, 2009, LECT NOTES COMPUT SC, V5702, P573, DOI 10.1007/978-3-642-03767-2_70
   Song KK, 2018, NEUROCOMPUTING, V312, P218, DOI 10.1016/j.neucom.2018.05.104
   Sun M, 2016, IEEE INT CON MULTI
   Wu LF, 2022, LECT NOTES COMPUT SC, V13188, P501, DOI 10.1007/978-3-031-02375-0_37
   Wu LF, 2020, NEURAL PROCESS LETT, V51, P2063, DOI 10.1007/s11063-019-10027-7
   Yamamoto T, 2021, IEICE T INF SYST, VE104D, P1691, DOI 10.1587/transinf.2020EDP7218
   Yang JY, 2021, IEEE T IMAGE PROCESS, V30, P8686, DOI 10.1109/TIP.2021.3118983
   Yang JY, 2021, IEEE T IMAGE PROCESS, V30, P7432, DOI 10.1109/TIP.2021.3106813
   Yang JF, 2018, AAAI CONF ARTIF INTE, P491
   Yang JF, 2018, PROC CVPR IEEE, P7584, DOI 10.1109/CVPR.2018.00791
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yang Y, 2014, AAAI CONF ARTIF INTE, P306
   Yanulevskaya V, 2008, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2008.4711701
   You QZ, 2017, AAAI CONF ARTIF INTE, P231
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Zhang W, 2020, IEEE T MULTIMEDIA, V22, P515, DOI 10.1109/TMM.2019.2928998
   Zhao JQ, 2018, IEEE ACCESS, V6, P23253, DOI 10.1109/ACCESS.2017.2776930
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1025, DOI 10.1145/2647868.2655035
   Zhao Sicheng, 2016, P 24 ACM INT C MULT, P1385, DOI DOI 10.1145/2964284.2964289
   Zisad SN, 2021, ALGORITHMS, V14, DOI 10.3390/a14070213
NR 53
TC 5
Z9 5
U1 4
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7139
EP 7148
DI 10.1109/TMM.2022.3217414
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000031
DA 2024-07-18
ER

PT J
AU Li, CC
   Li, J
   Xie, YG
   Nie, JY
   Yang, T
   Lu, ZY
AF Li, Congcong
   Li, Jing
   Xie, Yuguang
   Nie, Jiayang
   Yang, Tao
   Lu, Zhaoyang
TI Calibration-Free Cross-Camera Target Association Using Interaction
   Spatiotemporal Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Calibration free; cross-camera target association; object-object
   interaction; overlapping multi-camera system; spatiotemporal consistency
ID TRACKING; REIDENTIFICATION
AB In this paper, we propose a novel calibration-free cross-camera target association algorithm that aims to relate local visual data of the same object across cameras with overlapping FOVs. Unlike other methods using object's own characteristics, our approach makes full use of the interactions between objects and explores their spatiotemporal consistency in projection transformation to associate cameras. It has wider applicability in deployed overlapping multi-camera systems with unknown or rarely available calibration data, especially if there is a large perspective gap between cameras. Specifically, we first extract trajectory intersection which is one of the typical object-object interactive behaviors from each camera for feature vector construction. Then, based on the consistency of object-object interactions, we propose a multi-camera spatiotemporal alignment method via wide-domain cross-correlation analysis. It realizes time synchronization and spatial calibration of the multi-camera system simultaneously. After that, we introduce a cross-camera target association approach using aligned object-object interactions. The local data of the same target are successfully associated across cameras without any additional calibration. Extensive experimental evaluations on different databases verify the effectiveness and robustness of our proposed method.
C1 [Li, Congcong; Li, Jing; Xie, Yuguang; Nie, Jiayang; Lu, Zhaoyang] Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
   [Yang, Tao] Northwestern Polytech Univ, Sch Comp Sci, Natl Engn Lab Integrated Aerosp Ground Ocean Big, SAIIP, Xian 710129, Peoples R China.
C3 Xidian University; Northwestern Polytechnical University
RP Li, J (corresponding author), Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
EM ccli@stu.xidian.edu.cn; jinglixd@mail.xidian.edu.cn;
   ygxie@stu.xidian.edu.cn; jiayangnie@stu.xidian.edu.cn;
   tyang@nwpu.edu.cn; zhylu@xidian.edu.cn
FU National Natural Science Foundation of China [62172315, 62073262,
   61672429]; Key Research and Development Program of Shaanxi
   [S2021-YF-ZDCXL-ZDLGY-0127]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172315, 62073262, and 61672429, in
   part by the Key Research and Development Program of Shaanxi under Grant
   S2021-YF-ZDCXL-ZDLGY-0127, the funding of Huawei.
CR Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Borgia A, 2018, IEEE T IMAGE PROCESS, V27, P5338, DOI 10.1109/TIP.2018.2851098
   Chen WH, 2017, IEEE T CIRC SYST VID, V27, P2367, DOI 10.1109/TCSVT.2016.2589619
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Eshel R., 2008, CVPR, P1
   Ferryman J., 2009, P IEEE 12 INT WORKSH, P1
   Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174
   Gan YY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P282, DOI 10.1145/3474085.3475177
   Gong X, 2022, IEEE T MULTIMEDIA, V24, P217, DOI 10.1109/TMM.2021.3050082
   He YH, 2020, IEEE T IMAGE PROCESS, V29, P5191, DOI 10.1109/TIP.2020.2980070
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu WM, 2006, IEEE T PATTERN ANAL, V28, P663, DOI 10.1109/TPAMI.2006.80
   Jocher G., 2020, YOLOv5
   Li B, 2021, COMPUT VIS IMAGE UND, V210, DOI 10.1016/j.cviu.2021.103249
   Li Y.L., 2021, P IEEECVF C COMPUTER, P4103
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Sharma A, 2020, IMAGE VISION COMPUT, V103, DOI 10.1016/j.imavis.2020.104022
   Sheng H, 2021, IEEE INTERNET THINGS, V8, P3189, DOI 10.1109/JIOT.2020.3015239
   Shi YX, 2021, IEEE T MULTIMEDIA, V23, P4376, DOI 10.1109/TMM.2020.3042068
   Srivastava S, 2011, INT CONF ACOUST SPEE, P1821
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Tesfaye YT, 2019, INT J COMPUT VISION, V127, P1303, DOI 10.1007/s11263-019-01180-6
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Xu YL, 2017, AAAI CONF ARTIF INTE, P4299
   Xu YL, 2016, PROC CVPR IEEE, P4256, DOI 10.1109/CVPR.2016.461
   Yang KS, 2021, IEEE COMPUT SOC CONF, P3978, DOI 10.1109/CVPRW53098.2021.00449
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   You SS, 2021, IEEE T CIRC SYST VID, V31, P3105, DOI 10.1109/TCSVT.2020.3036467
   Zhang RH, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107260
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
NR 40
TC 3
Z9 3
U1 4
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6105
EP 6120
DI 10.1109/TMM.2022.3205407
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500033
DA 2024-07-18
ER

PT J
AU Li, X
   Wang, JL
   Li, X
   Lu, Y
AF Li, Xiang
   Wang, Jinglu
   Li, Xiao
   Lu, Yan
TI Video Instance Segmentation by Instance Flow Assembly
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video instance segmentation; bottom-up method; instance flow
AB Instance segmentation is a challenging task aiming at classifying and segmenting all object instances of specific classes. While two-stage box-based methods achieve top performances in the image domain, they cannot easily extend their superiority into the video domain. This is because they usually deal with features or images cropped from the detected bounding boxes without alignment, failing to capture pixel-level temporal consistency. We embrace the observation that bottom-up methods dealing with box-free features could offer accurate spacial correlations across frames, which can be fully utilized for object and pixel level tracking. We first propose our bottom-up framework equipped with a temporal context fusion module to better encode inter-frame correlations. Intra-frame cues for semantic segmentation and object localization are simultaneously extracted and reconstructed by corresponding decoders after a shared backbone. For efficient and robust tracking among instances, we introduce an instance-level correspondence across adjacent frames, which is represented by a center-to-center flow, termed as instance flow, to assemble messy dense temporal correspondences. Experiments demonstrate that the proposed method outperforms the state-of-the-art online methods (taking image-level input) on the challenging Youtube-VIS dataset (Yang et al., 2019).
C1 [Li, Xiang] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
   [Wang, Jinglu; Li, Xiao; Lu, Yan] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Carnegie Mellon University; Microsoft Research Asia; Microsoft
RP Wang, JL (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM xangl9867@gmail.com; jinglu.cs@gmail.com; pableetoli@gmail.com;
   yanlu@microsoft.com
RI Wang, Jinglu/JVN-3859-2024
OI Wang, Jinglu/0000-0002-3222-6579
CR Athar Ali, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P158, DOI 10.1007/978-3-030-58621-8_10
   Bertasius Gedas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9736, DOI 10.1109/CVPR42600.2020.00976
   Bowen Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12472, DOI 10.1109/CVPR42600.2020.01249
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130
   Cheng HK, 2021, PROC CVPR IEEE, P5555, DOI 10.1109/CVPR46437.2021.00551
   Cheng JC, 2017, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2017.81
   Cong RM, 2019, IEEE T IMAGE PROCESS, V28, P4819, DOI 10.1109/TIP.2019.2910377
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Fang HS, 2019, IEEE I CONF COMP VIS, P682, DOI 10.1109/ICCV.2019.00077
   Ge Wenbin, 2021, CVPR, P16836
   He K., 2017, IEEE C COMP VIS PATT, P2961, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu L, 2021, PROC CVPR IEEE, P4142, DOI 10.1109/CVPR46437.2021.00413
   Huang PL, 2022, IEEE-CAA J AUTOMATIC, V9, P339, DOI 10.1109/JAS.2021.1004210
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Huiyu Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P108, DOI 10.1007/978-3-030-58548-8_7
   Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228
   Jiale Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P1, DOI 10.1007/978-3-030-58568-6_1
   Kingma D. P., 2014, arXiv
   Li X, 2022, AAAI CONF ARTIF INTE, P1429
   Li X, 2022, Arxiv, DOI arXiv:2207.05580
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Liang-Chieh Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P695, DOI 10.1007/978-3-030-58545-7_40
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P2228, DOI 10.1109/TPAMI.2020.3040258
   Oh SW, 2022, IEEE T PATTERN ANAL, V44, P442, DOI 10.1109/TPAMI.2020.3008917
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Qiao SY, 2020, Arxiv, DOI [arXiv:2012.05258, DOI 10.48550/ARXIV.2012.05258, 10.48550/arXiv.2012.05258]
   Seong H., 2020, EUR C COMP VIS, P629
   Seoung Wug Oh, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P9225, DOI 10.1109/ICCV.2019.00932
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tokmakov P, 2017, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2017.480
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wang HY, 2021, Arxiv, DOI arXiv:2012.00759
   Wang XL, 2020, Arxiv, DOI [arXiv:2003.10152, 10.48550/arXiv.2003.10152]
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Wang ZQ, 2019, IEEE I CONF COMP VIS, P3977, DOI 10.1109/ICCV.2019.00408
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Xiankai Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P661, DOI 10.1007/978-3-030-58580-8_39
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Yang LJ, 2019, IEEE I CONF COMP VIS, P5187, DOI 10.1109/ICCV.2019.00529
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Yang TJ, 2019, Arxiv, DOI arXiv:1902.05093
   Yu Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P735, DOI 10.1007/978-3-030-58607-2_43
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P475, DOI 10.1109/TPAMI.2018.2881114
   Zhao WB, 2021, PROC CVPR IEEE, P16821, DOI 10.1109/CVPR46437.2021.01655
   Zhou TF, 2020, IEEE T IMAGE PROCESS, V29, P8326, DOI 10.1109/TIP.2020.3013162
   Zongxin Yang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P332, DOI 10.1007/978-3-030-58558-7_20
NR 55
TC 2
Z9 2
U1 4
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7469
EP 7479
DI 10.1109/TMM.2022.3222643
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000054
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, GS
   Yue, HJ
   Wu, JM
   Yang, JY
AF Liu, Gaosheng
   Yue, Huanjing
   Wu, Jiamin
   Yang, Jingyu
TI Intra-Inter View Interaction Network for Light Field Image
   Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Spatial resolution; Feature extraction; Correlation; Finite element
   analysis; Three-dimensional displays; Superresolution; Image
   reconstruction; Light field image; super-resolution; LF-IINet
ID DEPTH
AB Light field (LF) cameras, which can record real-word scenes from multiple viewpoints in a single shot, are widely used in 3D reconstruction, re-focusing, and virtual reality etc. However, the inherent trade-off between spatial resolution and angular resolution of LF images hinders their applications for scenarios requiring high resolutions. In this paper, we propose a novel intra-inter view interaction network for LF image super-resolution, termed as LF-IINet, to exploit the correlations among all views and simultaneously preserve the parallax structure of LF views. The proposed LF-IINet consists of two parallel branches. Specifically, the top branch extracts global inter-view information, and the bottom branch first independently maps each view to deep representations and then models the correlations among all intra-view features via proposed multi-view context block (MCB). The two branches interact with each other by proposed inter-assist-intra feature updating module (IntraFUM, where the intra feature are updated with the assistance of the inter feature) and intra-assist-inter feature updating module (InterFUM, where the inter feature are updated with the assistance of the intra feature). In this way, our LF-IINet incorporates rich angular and spatial information for LF image super-resolution. Extensive comparison with state-of-the-art methods demonstrates that our method achieves superior performance visually and quantitatively. Furthermore, quantitative results also show that our method is effective for LF images with either small or large disparities.
C1 [Liu, Gaosheng; Yue, Huanjing; Yang, Jingyu] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Wu, Jiamin] Tsinghua Univ, Dept Automat, Beijing 10084, Peoples R China.
C3 Tianjin University; Tsinghua University
RP Yang, JY (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM gaoshengliu@tju.edu.cn; dayueer@tju.edu.cn; wujiamin@tsinghua.edu.cn;
   yjy@tju.edu.cn
RI Wu, Jiamin/ABD-5325-2021; YANG, JINGYU (Gracy)/AAD-3341-2021
OI Wu, Jiamin/0000-0003-3479-1026; Liu, Gaosheng/0000-0003-3386-3390
FU National Natural Science Foundation of China [62072331, 62071272,
   61771339]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072331, 62071272, and 61771339.
CR Alain M, 2018, IEEE IMAGE PROC, P2501, DOI 10.1109/ICIP.2018.8451162
   [Anonymous], 2014, Master thesis, DOI [10.1109/BTAS.2014.6996295, DOI 10.1145/2593069.2593124]
   Bishop TE, 2012, IEEE T PATTERN ANAL, V34, P972, DOI 10.1109/TPAMI.2011.168
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chuchvara A, 2020, IEEE T IMAGE PROCESS, V29, P2492, DOI 10.1109/TIP.2019.2959233
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Farrugia RA, 2017, IEEE J-STSP, V11, P1058, DOI 10.1109/JSTSP.2017.2747127
   Ghassab VK, 2020, IEEE T MULTIMEDIA, V22, P1447, DOI 10.1109/TMM.2019.2946094
   He ZW, 2020, IEEE T MULTIMEDIA, V22, P1042, DOI 10.1109/TMM.2019.2937688
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   HUANG FC, 2015, ACM SIGGRAPH 2015 EM, P1, DOI DOI 10.1145/2782782.2792493
   Jin J., 2020, PROC AAAI C ARTIF IN, p11 141
   Jin J, 2020, PROC CVPR IEEE, P2257, DOI 10.1109/CVPR42600.2020.00233
   Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Le Pendu M, 2018, IEEE T IMAGE PROCESS, V27, P1981, DOI 10.1109/TIP.2018.2791864
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Lu Z., 2019, OPT EXPRESS, V27, p18 131
   Mishiba K, 2020, IEEE T IMAGE PROCESS, V29, P4232, DOI 10.1109/TIP.2020.2970814
   Mitra K., 2012, P IEEE COMP SOC C CO, P22
   Polatkan G, 2015, IEEE T PATTERN ANAL, V37, P346, DOI 10.1109/TPAMI.2014.2321404
   Rerabek M., 2016, P 8 INT C QUAL MULT
   Rossi M, 2018, IEEE T IMAGE PROCESS, V27, P4207, DOI 10.1109/TIP.2018.2828983
   Shi JL, 2020, PROC CVPR IEEE, P2552, DOI 10.1109/CVPR42600.2020.00263
   Shi JL, 2019, IEEE T IMAGE PROCESS, V28, P5867, DOI 10.1109/TIP.2019.2923323
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shin C, 2018, PROC CVPR IEEE, P4748, DOI 10.1109/CVPR.2018.00499
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Vaish V, 2008, The (New) Stanford Light Field Archive
   Wang LG, 2019, PROC CVPR IEEE, P12242, DOI 10.1109/CVPR.2019.01253
   Wang TT, 2019, IEEE I CONF COMP VIS, P8837, DOI 10.1109/ICCV.2019.00893
   Wang YQ, 2021, IEEE T IMAGE PROCESS, V30, P1057, DOI 10.1109/TIP.2020.3042059
   Wang YL, 2018, IEEE T IMAGE PROCESS, V27, P4274, DOI 10.1109/TIP.2018.2834819
   Wanner S., 2013, INT S VIS MOD VIS, P225
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Wu JM, 2021, CELL, V184, P3318, DOI 10.1016/j.cell.2021.04.029
   Wu JM, 2016, SCI REP-UK, V6, DOI 10.1038/srep24624
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yeung HWF, 2019, IEEE T IMAGE PROCESS, V28, P2319, DOI 10.1109/TIP.2018.2885236
   Yingqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P290, DOI 10.1007/978-3-030-58592-1_18
   Yoon Y, 2017, IEEE SIGNAL PROC LET, V24, P848, DOI 10.1109/LSP.2017.2669333
   Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17
   Yu JY, 2017, IEEE MULTIMEDIA, V24, P104, DOI 10.1109/MMUL.2017.24
   Yuan Y, 2018, IEEE SIGNAL PROC LET, V25, P1359, DOI 10.1109/LSP.2018.2856619
   Zhang S., 2019, PROC IEEE C COMPUT V, p11 038
   Zhang S, 2021, IEEE T COMPUT IMAG, V7, P799, DOI 10.1109/TCI.2021.3099636
   Zhang S, 2021, IEEE T IMAGE PROCESS, V30, P5956, DOI 10.1109/TIP.2021.3079805
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 53
TC 24
Z9 24
U1 8
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 256
EP 266
DI 10.1109/TMM.2021.3124385
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400018
DA 2024-07-18
ER

PT J
AU Liu, JY
   He, HC
   Liu, MZ
   Li, JJ
   Lu, K
AF Liu, Jieyan
   He, Hongcai
   Liu, Mingzhu
   Li, Jingjing
   Lu, Ke
TI Manifold Regularized Joint Transfer for Open Set Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transfer learning; domain adaptation; structural risk minimization;
   graph embedding
ID GENERAL FRAMEWORK
AB Unsupervised Domain Adaptation (UDA) aims to leverage knowledge of a well-labeled source domain to learn an effective classifier for an unlabeled target domain. However, a common scenario in real-world applications is that the target domain contains unknown categories that are not observed in the source domain. This setting is termed as open set domain adaptation (OSDA). Most existing approaches of OSDA can only classify known classes well but fail to recognize unknown samples effectively. In this paper, we propose an effective method, named manifold regularized joint transfer (MRJT), for OSDA. MRJT learns new feature representations by simultaneously reducing distribution discrepancy between domains, increasing compactness of within-class, discriminating different known classes, and distinguishing the unknown from the known. The learned new features are projected onto reproducing kernel Hilbert space. In this space, a weighted structural risk minimization method is integrated with manifold regularization to utilize geometric information sufficiently to learn an effective classifier. Extensive experimental results on four real-world datasets verify the superiority of our method. It can not only classify known samples into the right known classes but also recognize unknown samples effectively.
C1 [Liu, Jieyan; He, Hongcai; Li, Jingjing; Lu, Ke] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Liu, Mingzhu] Univ Michigan, Ann Arbor, MI 48105 USA.
C3 University of Electronic Science & Technology of China; University of
   Michigan System; University of Michigan
RP Li, JJ (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM liujy@uestc.edu.cn; 1020084364@qq.com; mingzliu@umich.edu;
   lijin117@yeah.net; kel@uestc.edu.cn
RI Li, Jingjing/T-6522-2019; Liu, Mingzhu/AAZ-1332-2020
OI Liu, Mingzhu/0000-0003-2973-5361; Liu, Mingzhu/0000-0002-9905-2673
FU National Natural Science Foundation of China
FX No Statement Available
CR Belkin M, 2002, ADV NEUR IN, V14, P585
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bucci Silvia, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P422, DOI 10.1007/978-3-030-58517-4_25
   Busto PP, 2020, IEEE T PATTERN ANAL, V42, P413, DOI 10.1109/TPAMI.2018.2880750
   Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Chen YM, 2020, IEEE T IMAGE PROCESS, V29, P199, DOI 10.1109/TIP.2019.2928630
   Coppersmith D., 1987, PROC 19 ANN ACM S TH, P1
   Fang Z, 2021, IEEE T NEUR NET LEAR, V32, P4309, DOI 10.1109/TNNLS.2020.3017213
   Jia YQ, 2009, IEEE T NEURAL NETWOR, V20, P729, DOI 10.1109/TNN.2009.2015760
   Jing MM, 2021, IEEE T CYBERNETICS, V51, P3390, DOI 10.1109/TCYB.2020.2974106
   Jing MM, 2021, AAAI CONF ARTIF INTE, V35, P8013
   Ke Mei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P415, DOI 10.1007/978-3-030-58574-7_25
   Li J., 2016, P 25 INT JOINT C ART, P1697
   Li JJ, 2022, IEEE T KNOWL DATA EN, V34, P5770, DOI 10.1109/TKDE.2021.3060473
   Li JJ, 2022, IEEE T PATTERN ANAL, V44, P8196, DOI 10.1109/TPAMI.2021.3109287
   Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050
   Li JJ, 2019, IEEE T IMAGE PROCESS, V28, P6103, DOI 10.1109/TIP.2019.2924174
   Li JJ, 2019, IEEE T CYBERNETICS, V49, P2144, DOI 10.1109/TCYB.2018.2820174
   Li JJ, 2017, IEEE T CYBERNETICS, V47, P3516, DOI 10.1109/TCYB.2016.2565898
   Li P, 2015, KNOWL INF SYST, V43, P103, DOI 10.1007/s10115-013-0715-x
   Li S, 2018, IEEE T IMAGE PROCESS, V27, P4260, DOI 10.1109/TIP.2018.2839528
   Liu H, 2019, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2019.00304
   Liu JY, 2018, NEUROCOMPUTING, V275, P247, DOI 10.1016/j.neucom.2017.06.051
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1805, DOI 10.1109/TKDE.2013.97
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Luo YD, 2022, Arxiv, DOI [arXiv:2202.06174, DOI arXiv:2202.06174.v1]
   Mendes PR, 2017, MACH LEARN, V106, P359, DOI 10.1007/s10994-016-5610-8
   Rangwani H, 2022, PR MACH LEARN RES
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10
   Schölkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27
   Sharma A, 2021, PROC CVPR IEEE, P5357, DOI 10.1109/CVPR46437.2021.00532
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Shi J, 2014, NEURAL PROCESS LETT, V40, P211, DOI 10.1007/s11063-013-9323-8
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Snell J, 2017, ADV NEUR IN, V30
   Vapnik V.N., 1998, STAT LEARNING THEORY
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang S, 2022, IEEE T CYBERNETICS, V52, P7464, DOI 10.1109/TCYB.2020.3040763
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13864, DOI 10.1109/CVPR42600.2020.01388
   You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhang W., 2021, IEEE Trans. Ind. Informat., V18, P8077
   Zhao X, 2023, APPL INTELL, V53, P7862, DOI 10.1007/s10489-022-03805-9
   Zhong L, 2023, IEEE T NEUR NET LEAR, V34, P3859, DOI 10.1109/TNNLS.2021.3119965
   Zhou JL, 2022, IEEE ACM T COMPUT BI, V19, P2605, DOI 10.1109/TCBB.2021.3066331
NR 48
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9356
EP 9369
DI 10.1109/TMM.2023.3251094
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200005
DA 2024-07-18
ER

PT J
AU Liu, SC
   Ma, X
AF Liu, Shaocan
   Ma, Xin
TI Attention-Driven Appearance-Motion Fusion Network for Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; appearance-motion fusion; cross-modality attention;
   2D-single-convnet
ID REPRESENTATION; VIDEOS
AB Recent years have witnessed the popularity of using a two-stream architecture and attention mechanism for action recognition with videos. However, it is time-consuming to train two separate convolutional neural networks (ConvNets), especially with the complex attention mechanism. In this article, we present a novel architecture, termed as Appearance-Motion Fusion Network (AMFNet), to learn efficient and robust action representation from RGB and optical flow data in an end-to-end manner. AMFNet is constructed by connecting a convolutional neural network with an appearance-motion fusion block (AMFB), whose goal is to incorporate appearance and motion streams into a unified framework driven by a cross-modality attention (CMA) mechanism. More specifically, the CMA only relies on optical flow data, which consists of a Key-Frame Adaptive Selection Module (KFASM) and an Optical-Flow-Driven Spatial Attention Module (OFDSAM). The former aims to adaptively identify the discriminative key frames from a sequence, while the latter is able to guide our networks to focus on the action-relevant regions of each frame. We explore two schemes for appearance and motion streams fusion in AMFB from hierarchical and comprehensive levels. The proposed AMFNet is extensively evaluated on five action recognition data sets, including HMDB-51, UCF-101, JHMDB, Penn and Kinetics-400. Compared to the state-of-the-art methods operated at RGB and optical flow, the experimental results validate that our AMFNet achieves a comparable performance with a pure 2D-Single-ConvNet design.
C1 [Liu, Shaocan; Ma, Xin] Shandong Univ, Ctr Robot, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
C3 Shandong University
RP Ma, X (corresponding author), Shandong Univ, Ctr Robot, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
EM liushaocan@mail.sdu.edu.cn; maxin@sdu.edu.cn
FU National Key Research and Development Program of China [2018YFB1305803];
   Basic Research Key Development Program of Shandong Province
   [ZR2019ZD07]; National Natural Science Foundation of China [U21A20486];
   Science and Technology Innovation [2020AAA0108903]; Key Research Project
   of Inclusive Robots of National Natural Science Foundation of China
   [91948201]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFB1305803, in part by the
   Basic Research Key Development Program of Shandong Province under Grant
   ZR2019ZD07, in part by the National Natural Science Foundation of China
   under Grant U21A20486, in part by the Science and Technology Innovation
   2030-"New Generation Artificial Intelligence" major project under Grant
   2020AAA0108903, and in part by the Key Research Project of Inclusive
   Robots of National Natural Science Foundation of China under Grant
   91948201. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Jianguo Zhang.&
   nbsp;
CR Afouras T., 2020, P EUR C COMP VIS, V12363, P208, DOI 10.1007/978-3-030- 58523-5-13
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cheng J, 2022, IEEE T CIRC SYST VID, V32, P1498, DOI 10.1109/TCSVT.2021.3076165
   Cherian A, 2017, PROC CVPR IEEE, P1581, DOI 10.1109/CVPR.2017.172
   Chéron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368
   Couprie C., 2013, P 1 INT C LEARN REPR, P1
   Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
   Fan LF, 2019, IEEE I CONF COMP VIS, P5723, DOI 10.1109/ICCV.2019.00582
   Fan LJ, 2018, PROC CVPR IEEE, P6016, DOI 10.1109/CVPR.2018.00630
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Girdhar R., 2017, NIPS, P33
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Gong Xinyu, 2021, P IEEE CVF INT C COM, P8033
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Koniusz P, 2022, IEEE T PATTERN ANAL, V44, P648, DOI 10.1109/TPAMI.2021.3107160
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Kumawat S, 2022, IEEE T PATTERN ANAL, V44, P4839, DOI 10.1109/TPAMI.2021.3076522
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li HH, 2021, IEEE T IMAGE PROCESS, V30, P7926, DOI 10.1109/TIP.2021.3112008
   Li HY, 2019, LECT NOTES COMPUT SC, V11296, P365, DOI 10.1007/978-3-030-05716-9_30
   Li JN, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107356
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li Y., 2019, PROC IEEECVF C COMPU, P56
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu L, 2013, PATTERN RECOGN, V46, P1810, DOI 10.1016/j.patcog.2012.10.004
   Long X, 2018, AAAI CONF ARTIF INTE, P7202
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Meng LL, 2019, IEEE INT CONF COMP V, P1513, DOI 10.1109/ICCVW.2019.00189
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Piergiovanni AJ, 2019, PROC CVPR IEEE, P9937, DOI 10.1109/CVPR.2019.01018
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shou Z, 2019, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2019.00136
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370
   Song SJ, 2020, IEEE T IMAGE PROCESS, V29, P3957, DOI 10.1109/TIP.2020.2967577
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wu D, 2016, IEEE T PATTERN ANAL, V38, P1583, DOI 10.1109/TPAMI.2016.2537340
   Xie SN, 2018, Arxiv, DOI [arXiv:1712.04851, DOI 10.48550/ARXIV.1712.04851]
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zaremba W, 2015, Arxiv, DOI arXiv:1409.2329
   Zhang C, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P500, DOI 10.1145/3343031.3350876
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2518, DOI 10.1109/TMM.2019.2907453
   Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280
   Zhao H, 2019, IEEE I CONF COMP VIS, P8667, DOI 10.1109/ICCV.2019.00876
   Zhao H, 2019, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2019.00182
   Zhao Yue., 2018, Advances in Neural Information Processing Systems, P2204
   Zheng ZX, 2023, IEEE T NEUR NET LEAR, V34, P1304, DOI 10.1109/TNNLS.2021.3105184
   Zhou TF, 2022, IEEE T PATTERN ANAL, V44, P2827, DOI 10.1109/TPAMI.2021.3049156
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
   Zhu SJ, 2020, Arxiv, DOI arXiv:2011.12384
   Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316
NR 91
TC 4
Z9 4
U1 5
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2573
EP 2584
DI 10.1109/TMM.2022.3148588
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600009
DA 2024-07-18
ER

PT J
AU Ma, LY
   Huang, KJ
   Wei, DX
   Ming, ZY
   Shen, HB
AF Ma, Liyuan
   Huang, Kejie
   Wei, Dongxu
   Ming, Zhaoyan
   Shen, Haibin
TI FDA-GAN: Flow-Based Dual Attention GAN for Human Pose Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image synthesis; Task analysis; Strain; Solid modeling;
   Three-dimensional displays; Generative adversarial networks; Feature
   extraction; Generative adversarial networks (GANs); image synthesis;
   pose transfer
AB Human pose transfer aims at transferring the appearance of the source person to the target pose. Existing methods utilizing flow-based warping for non-rigid human image generation have achieved great success. However, they fail to preserve the appearance details in synthesized images since the spatial correlation between the source and target is not fully exploited. To this end, we propose the Flow-based Dual Attention GAN (FDA-GAN) to apply occlusion- and deformation-aware feature fusion for higher generation quality. Specifically, deformable local attention and flow similarity attention, constituting the dual attention mechanism, can derive the output features responsible for deformable- and occlusion-aware fusion, respectively. Besides, to maintain the pose and global position consistency in transferring, we design a pose normalization network for learning adaptive normalization from the target pose to the source person. Both qualitative and quantitative results show that our method outperforms state-of-the-art models in public iPER and DeepFashion datasets.
C1 [Ma, Liyuan; Huang, Kejie; Wei, Dongxu; Shen, Haibin] Zhejiang Univ, Dept Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
   [Ming, Zhaoyan] Zhejiang Univ, Inst Comp Innovat, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Huang, KJ (corresponding author), Zhejiang Univ, Dept Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
EM mlyarthur@zju.edu.cn; huangkejie@zju.edu.cn; tracywei@zju.edu.cn;
   mingzhaoyan@gmail.com; shen_hb@zju.edu.cn
RI , 黄科杰/J-5919-2019; Huang, Kejie/E-7511-2018
OI Huang, Kejie/0000-0003-3722-9979; Ma, Liyuan/0000-0002-9492-5324
FU National Natural Science Foundation of China [U19B2043]
FX This work was supported by the National Natural Science Foundation of
   China under Grant U19B2043.
CR Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Dong H., 2018, NeurIPS, P474
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu XW, 2021, IEEE T CYBERNETICS, V51, P5352, DOI 10.1109/TCYB.2020.2967462
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   Hensel M, 2017, ADV NEUR IN, V30
   Horiuchi Y., 2019, P MVA 2019 16 INT C, P1
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Knoche M., 2020, P IEEE CVF C COMP VI, P1044
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Lassner C, 2017, IEEE I CONF COMP VIS, P853, DOI 10.1109/ICCV.2017.98
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li K, 2020, IEEE T IMAGE PROCESS, V29, P9584, DOI 10.1109/TIP.2020.3029455
   Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381
   Liu S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P82, DOI 10.1145/3123266.3123431
   Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lu JJ, 2022, IEEE T MULTIMEDIA, V24, P558, DOI 10.1109/TMM.2021.3054973
   Ma LQ, 2017, ADV NEUR IN, V30
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Odena A, 2017, PR MACH LEARN RES, V70
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Ren YR, 2020, IEEE T IMAGE PROCESS, V29, P8622, DOI 10.1109/TIP.2020.3018224
   Salimans T, 2016, Arxiv, DOI [arXiv:1606.03498, DOI 10.48550/ARXIV.1606.03498]
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Tang JL, 2021, Arxiv, DOI arXiv:2102.02972
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Wang LD, 2018, IEEE INT CONF AUTOMA, P83, DOI 10.1109/FG.2018.00022
   Wang NN, 2018, PATTERN RECOGN LETT, V107, P59, DOI 10.1016/j.patrec.2017.06.012
   Wang T.-C., 2019, Advances in Neural Information Processing Systems
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei D., 2019, ARXIV
   Wei DX, 2020, Arxiv, DOI arXiv:2012.08976
   Yang LB, 2021, IEEE T IMAGE PROCESS, V30, P2422, DOI 10.1109/TIP.2021.3052364
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Zhang J., 2019, OPTOELECTRON IMAG MU, V11187, P188
   Zhang JS, 2021, PROC CVPR IEEE, P7978, DOI 10.1109/CVPR46437.2021.00789
   Zhang JS, 2020, COMPUT GRAPH FORUM, V39, P325, DOI 10.1111/cgf.14148
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng HT, 2021, IEEE T IMAGE PROCESS, V30, P1898, DOI 10.1109/TIP.2020.3031108
   Zhou YP, 2019, IEEE INT CONF COMP V, P1208, DOI 10.1109/ICCVW.2019.00153
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 57
TC 8
Z9 8
U1 5
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 930
EP 941
DI 10.1109/TMM.2021.3134157
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA A1OS1
UT WOS:000952899000001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Mao, JZ
   Yao, YZ
   Sun, ZR
   Huang, XG
   Shen, FM
   Shen, HT
AF Mao, Junzhu
   Yao, Yazhou
   Sun, Zeren
   Huang, Xingguo
   Shen, Fumin
   Shen, Heng-Tao
TI Attention Map Guided Transformer Pruning for Occluded Person
   Re-Identification on Edge Device
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Entropy; Transformers; Computational modeling; Indexes; Head;
   Computational efficiency; Task analysis; Vision transformer; occluded
   person re-identification; token pruning; model compression
ID NETWORK
AB Due to its significant capability of modeling long-range dependencies, vision transformer (ViT) has achieved promising success in both holistic and occluded person re-identification (Re-ID) tasks. However, the inherent problems of transformers such as the huge computational cost and memory footprint are still two unsolved issues that will block the deployment of ViT based person Re-ID models on resource-limited edge devices. Our goal is to reduce both the inference complexity and model size without sacrificing the comparable accuracy on person Re-ID, especially for tasks with occlusion. To this end, we propose a novel attention map guided (AMG) transformer pruning method, which removes both redundant tokens and heads with the guidance of the attention map in a hardware-friendly way. We first calculate the entropy in the key dimension and sum it up for the whole map, and the corresponding head parameters of maps with high entropy will be removed for model size reduction. Then we combine the similarity and first-order gradients of key tokens along the query dimension for token importance estimation and remove redundant key and value tokens to further reduce the inference complexity. Comprehensive experiments on Occluded DukeMTMC and Market-1501 demonstrate the effectiveness of our proposals. For example, our proposed pruning strategy on ViT-Base enjoys 29.4% FLOPs savings with 0.2% drop on Rank-1 and 0.4% improvement on mAP, respectively.
C1 [Mao, Junzhu; Yao, Yazhou; Sun, Zeren] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Huang, Xingguo] Jilin Univ, Coll Instrumentat & Elect Engn, Changchun 130061, Peoples R China.
   [Shen, Fumin; Shen, Heng-Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 Nanjing University of Science & Technology; Jilin University; University
   of Electronic Science & Technology of China
RP Huang, XG (corresponding author), Jilin Univ, Coll Instrumentat & Elect Engn, Changchun 130061, Peoples R China.
EM maojunzhu@njust.edu.cn; yazhou.yao@njust.edu.cn; zerens@njust.edu.cn;
   xingguohuang@jlu.edu.cn; fumin.shen@gmail.com; shenhengtao@hotmail.com
RI Huang, Xingguo/AAA-2441-2019; Shen, Fumin/R-2121-2016; Shen, Heng
   Tao/ABD-5331-2021
OI Huang, Xingguo/0000-0001-9719-6297; Mao, Junzhu/0009-0005-0831-9272;
   Yao, Yazhou/0000-0002-0337-9410; Sun, Zeren/0000-0001-6262-5338
FU National Natural Science Foundation of China [62102182, 62202227];
   Natural Science Foundation of Jiangsu Province [BK20210327, BK20220938];
   China Postdoctoral Science Foundation [2022M721628]; Jiangsu Funding
   Program for Excellent Postdoctoral Talent [2022ZB271]
FX Manuscript received 30 July 2022; revised 19 January 2023, 18 February
   2023, and 24 February 2023; accepted 19 March 2023. Date of publication
   6 April 2023; date of current version 8 May 2023. This work was
   supported in part by the National Natural Science Foundation of China
   under Grants 62102182 and 62202227, in part by the Natural Science
   Foundation of Jiangsu Province under Grants BK20210327 and BK20220938,
   in part by the China Postdoctoral Science Foundation under Grant
   2022M721628, and in part by the Jiangsu Funding Program for Excellent
   Postdoctoral Talent under Grant 2022ZB271. The guest editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Jun Wu. (Corresponding author: Xingguo Huang.)
CR Camara F, 2021, IEEE T INTELL TRANSP, V22, P6131, DOI 10.1109/TITS.2020.3006768
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Chen T., 2020, ADV NEURAL INFORM PR, V33, P15834, DOI DOI 10.48550/ARXIV.2007.12223
   Chen TL, 2021, Advances in Neural Information Processing Systems, V34
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Gaikwad B, 2022, COMPUT ELECTR ENG, V99, DOI 10.1016/j.compeleceng.2022.107824
   Graf, 2017, ARXIV160808710, P1, DOI DOI 10.48550/ARXIV.1608.08710
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   He S., 2021, P IEEECVF INT C COMP, P15013, DOI DOI 10.1109/ICCV48922.2021.01474
   Heo B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11916, DOI 10.1109/ICCV48922.2021.01172
   Huang H., 2020, IEEE INT CON MULTI, P1, DOI DOI 10.1109/ICME46284.2020.9102789
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liao Y.-L., 2021, ARXIV210900642
   Lingxiao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P357, DOI 10.1007/978-3-030-58604-1_22
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Molchanov P., 2017, INT C LEARN REPR ICL, P1, DOI DOI 10.1002/9781118786352.WBIEG1156
   Molchanov P, 2019, PROC CVPR IEEE, P11256, DOI 10.1109/CVPR.2019.01152
   Pei GS, 2022, LECT NOTES COMPUT SC, V13694, P596, DOI 10.1007/978-3-031-19830-4_34
   Rao YM, 2021, 35 C NEURAL INFORM P, V34
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun ZR, 2022, PROC CVPR IEEE, P5301, DOI 10.1109/CVPR52688.2022.00524
   Tan B, 2017, IEEE INTERNET THINGS, V4, P945, DOI 10.1109/JIOT.2017.2680407
   Tang Y, 2022, PROC IEEE C COMPUT V, p12 165
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang HL, 2017, IEEE T MULTIMEDIA, V19, P908, DOI 10.1109/TMM.2016.2645398
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang Z., 2022, P IEEE CVF C COMP VI, P4754
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Xu YF, 2022, AAAI CONF ARTIF INTE, P2964
   Yao YZ, 2021, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR46437.2021.00515
   Yao YZ, 2021, PROC CVPR IEEE, P2623, DOI 10.1109/CVPR46437.2021.00265
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yu F, 2022, AAAI CONF ARTIF INTE, P3143
   Yu S., 2022, P INT C LEARNING REP
   Yu SJ, 2021, Arxiv, DOI arXiv:2105.07345
   Zhao CR, 2021, IEEE T IMAGE PROCESS, V30, P7776, DOI 10.1109/TIP.2021.3109508
   Zhao CR, 2021, IEEE T IMAGE PROCESS, V30, P4212, DOI 10.1109/TIP.2021.3070182
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhu MJ, 2021, Arxiv, DOI arXiv:2104.08500
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 49
TC 5
Z9 5
U1 6
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1592
EP 1599
DI 10.1109/TMM.2023.3265159
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000019
DA 2024-07-18
ER

PT J
AU Mao, WD
   Yang, S
   Shi, HH
   Liu, JY
   Wang, ZF
AF Mao, Wendong
   Yang, Shuai
   Shi, Huihong
   Liu, Jiaying
   Wang, Zhongfeng
TI Intelligent Typography: Artistic Text Style Transfer for Complex Texture
   and Structure
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Prototypes; Generative adversarial networks;
   Visualization; Strain; Real-time systems; Training; Complex reference
   style; controllable style transfer; generative adversarial network; text
   style transfer; unsupervised learning
ID SCALE-SPACE
AB Text style transfer is an important task to render artistic texts from a reference image or style, and is widely desired in many visual creations. Previous works have brought some efficient methods for text style transfer, which facilitate users to design various artistic texts automatically. However, these works mainly focus on relatively simple text effects, and do not perform well on complex reference styles. In this paper, we propose a coarse-to-fine framework to generate exquisite texts with complex texture and structure in an unsupervised way, achieving real-time control of style scales (i.e., text stylistic degree or deformation degree). The key idea is to decouple the overall task into two steps, prototype generation and detail refinement, and explore delicate networks for each step to imitate the features at different levels. Based on this idea, in the first step, we present a novel pro-gen GAN to generate prototypes of artistic texts using the reference style, and develop a deformable module to empower the pro-gen GAN to continuously characterize the multi-scale shape features without network retraining. Furthermore, we propose a mix-attention training scheme for text style transfer, which can avoid artifacts and retain a clear text background. In the second step, we introduce two optimized networks for detail refinements. Experimental results show that the proposed method can synthesize exquisite stylized texts with complex reference styles, and surpass the state of the arts in texture reconstruction, contour imitation, and text image quality drastically.
C1 [Mao, Wendong; Shi, Huihong; Wang, Zhongfeng] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210008, Peoples R China.
   [Yang, Shuai; Liu, Jiaying] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100080, Peoples R China.
C3 Nanjing University; Peking University
RP Wang, ZF (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210008, Peoples R China.; Liu, JY (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100080, Peoples R China.
EM wdmao@smail.nju.edu.cn; williamyang@pku.edu.cn; shihh@smail.nju.edu.cn;
   liujiaying@pku.edu.cn; zfwang@nju.edu.cn
OI Shi, Huihong/0000-0002-7845-0154; Wang, Zhongfeng/0000-0002-7227-4786
FU National Natural Science Foundation of China [62174084, 62104097];
   High-Level Personnel Project of Jiangsu Province [JSSCBS20210034]; Key
   Research Plan of Jiangsu Province of China [BE2019003-4]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62174084 and 62104097, in part by the
   High-Level Personnel Project of Jiangsu Province under Grant
   JSSCBS20210034, and in part by the Key Research Plan of Jiangsu Province
   of China under Grant BE2019003-4. TheAssociate Editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Catherine Zhao.
CR Atarsaikhan G, 2017, PROC INT CONF DOC, P51, DOI 10.1109/ICDAR.2017.328
   Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789
   BABAUD J, 1986, IEEE T PATTERN ANAL, V8, P26, DOI 10.1109/TPAMI.1986.4767749
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Fruhstuck A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322993
   Gao Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356574
   Gatys LA, 2015, ADV NEUR IN, V28
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang SH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3721
   Jing YC, 2018, LECT NOTES COMPUT SC, V11217, P244, DOI 10.1007/978-3-030-01261-8_15
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kotovenko D, 2019, IEEE I CONF COMP VIS, P4421, DOI 10.1109/ICCV.2019.00452
   Kotovenko D, 2019, PROC CVPR IEEE, P10024, DOI 10.1109/CVPR.2019.01027
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li W, 2020, AAAI CONF ARTIF INTE, V34, P1717
   Liang-Chieh C., 2015, PROC INT C LEARN REP
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Meyer S., 2018, P BRIT MACH VIS C NE
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang WJ, 2019, PROC CVPR IEEE, P5882, DOI 10.1109/CVPR.2019.00604
   Wang X, 2017, PROC CVPR IEEE, P7178, DOI 10.1109/CVPR.2017.759
   Yang S, 2022, IEEE T PATTERN ANAL, V44, P3807, DOI 10.1109/TPAMI.2021.3055211
   Yang S, 2019, IEEE I CONF COMP VIS, P4441, DOI 10.1109/ICCV.2019.00454
   Yang S, 2021, IEEE T PATTERN ANAL, V43, P3709, DOI 10.1109/TPAMI.2020.2983697
   Yang S, 2019, AAAI CONF ARTIF INTE, P1238
   Yang S, 2018, COMPUT VIS IMAGE UND, V174, P43, DOI 10.1016/j.cviu.2018.07.004
   Yang S, 2019, IEEE T IMAGE PROCESS, V28, P952, DOI 10.1109/TIP.2018.2873064
   Yang S, 2017, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR.2017.308
   Yu F., 2015, ARXIV
   Zhou Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201285
   Zhu A, 2020, IEEE T IMAGE PROCESS, V29, P6932, DOI 10.1109/TIP.2020.2995062
NR 38
TC 2
Z9 2
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6485
EP 6498
DI 10.1109/TMM.2022.3209870
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500060
DA 2024-07-18
ER

PT J
AU Qu, YD
   Xie, HT
   Fang, SC
   Wang, YX
   Zhang, YD
AF Qu, Yadong
   Xie, Hongtao
   Fang, Shancheng
   Wang, Yuxin
   Zhang, Yongdong
TI ADNet: Rethinking the Shrunk Polygon-Based Approach in Scene Text
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Shape; Costs; Convolution; Adaptive systems; Text recognition;
   Synthetic aperture sonar; Scene text detection; shrunk polygon; aspect
   ratio; adaptive dilation factor
AB To localize text regions and separate close instances, the shrunk polygon is widely used in recent scene text detection methods. However, there exist two problems: 1) Existing methods fail to consider the aspect ratio sensitive problem when reconstructing the text instance from shrunk polygon. 2) Texts with extreme aspect ratios will lead to the fracture of shrunk polygons. To handle these two problems, in this paper, we propose a novel Adaptive Dilation Network (ADNet) to focus on the reconstruction process from shrunk polygon, which aims to provide a tight and complete text representation. Firstly, instead of using a fixed dilation factor, ADNet uses an aspect ratio-wise dilation factor to reconstruct the text region from shrunk polygon for each text instance. Such an instance-wise dilation factor considers the scale correlation between the original and shrunk polygon, and thus can guide an adaptive text region reconstruction for texts with large aspect ratio variance. Secondly, to deal with the fracture of detection results, a new Efficient Spatial Relationship Module (ESRM) is devised to capture long-range dependencies with low computation cost. ESRM uses a novel Weighted Pooling to reduce the resolution of feature maps without much information loss. Compared with the existing methods, ADNet further explores the potential of shrunk polygon-based approaches and obtains excellent detection results at an impressive speed. Extensive experiments on several datasets (Total-Text, CTW1500, MSRA-TD500 and ICDAR2015) verify the superiority of our method.
C1 [Qu, Yadong; Xie, Hongtao; Fang, Shancheng; Wang, Yuxin; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Xie, HT (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
EM qqqyd@mail.ustc.edu.cn; htxie@ustc.edu.cn; fangsc@ustc.edu.cn;
   wangyx58@mail.ustc.edu.cn; zhyd73@ustc.edu.cn
OI Wang, Yuxin/0000-0002-0228-6220
FU National Nature Science Foundation of China [62121002, 62022076,
   U1936210]; Youth Innovation Promotion Association Chinese Academy of
   Sciences [Y2021122]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grants 62121002, 62022076, and U1936210, and
   in part by the Youth Innovation Promotion Association Chinese Academy of
   Sciences under Grant Y2021122.
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Dai PW, 2021, PROC CVPR IEEE, P7389, DOI 10.1109/CVPR46437.2021.00731
   Dai PW, 2022, IEEE T MULTIMEDIA, V24, P1883, DOI 10.1109/TMM.2021.3073575
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He K., 2017, IEEE C COMP VIS PATT, P2961, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Liao MH, 2023, IEEE T PATTERN ANAL, V45, P919, DOI 10.1109/TPAMI.2022.3155612
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YL, 2020, IEEE T IMAGE PROCESS, V29, P2918, DOI 10.1109/TIP.2019.2954218
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI DOI 10.48550/ARXIV.1411.4038
   Long SB, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01369-0
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5
   Minghui Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P706, DOI 10.1007/978-3-030-58621-8_41
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   VATTI BR, 1992, COMMUN ACM, V35, P56, DOI 10.1145/129902.129906
   Wang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P111, DOI 10.1145/3394171.3413819
   Wang H, 2020, AAAI CONF ARTIF INTE, V34, P12160
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P1316, DOI 10.1109/TMM.2020.2995290
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Yuliang L, 2017, Arxiv, DOI arXiv:1712.02170
   Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang S, 2021, IEEE T MULTIMEDIA, V23, P454, DOI 10.1109/TMM.2020.2978630
   Zhang SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1285, DOI 10.1109/ICCV48922.2021.00134
   Zhang Shi-Xue, 2020, IEEE C COMPUTER VISI, P9699
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314
   Zhu YX, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107336
NR 58
TC 0
Z9 0
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6983
EP 6996
DI 10.1109/TMM.2022.3216729
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000020
DA 2024-07-18
ER

PT J
AU Song, WL
   Zhang, L
   Gao, XB
AF Song, Wenli
   Zhang, Lei
   Gao, Xinbo
TI Compound Projection Learning for Bridging Seen and Unseen Objects
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Zero-shot learning; knowledge transfer; projection learning
ID SHOT; CLASSIFICATION
AB Zero-shot Learning (ZSL) aims to transfer knowledge from seen image categories to unseen ones by leveraging semantic information. It is generally assumed that the seen and unseen objects share a common semantic space. Most of existing ZSL methods focus on how to connect the visual space and the semantic space. However, since there are some visual distribution differences between seen and unseen objects, the projection function learned by those seen classes is biased when transferring knowledge to unseen classes. We argue that, although the unseen objects are class-agnostic, the visual distribution information of unseen samples can be generated by exploiting semantic features. In this paper, we propose a Compound Projection Learning (CPL) model to transfer knowledge from seen to unseen objects by exploiting the information of both seen and class-agnostic samples. With the projected semantic representation by CPL, effective constraints such as projection loss and semantic reconstruction loss can be explored for seen and unseen objects, respectively, such that the semantic ambiguity across seen and unseen objects is reduced. Additionally, we utilize a similarity network to further explore the inter-class relationship by employing the labels and the similarities between seen and unseen classes. Extensive experiments on ZSL benchmark datasets show the effectiveness of our proposed approach.
C1 [Song, Wenli; Zhang, Lei] Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400044, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Chongqing University; Chongqing University of Posts & Telecommunications
RP Zhang, L (corresponding author), Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400044, Peoples R China.
EM swl@cqu.edu.cn; leizhang@cqu.edu.cn; gaoxb@cqupt.edu.cn
OI Zhang, Lei/0000-0002-5305-8543
FU National Key R&D Program of China [2021YFB3100800]; Chongqing Natural
   Science Fund [cstc2021jcyj-jqX0023]; CCF Hikvision Open Fund; National
   Natural Science Fundation of China [62036007, 62050175, 61771079];
   CAAI-Huawei Mind Spore Open Fund; Beijing Academy of Artificial
   Intelligence (BAAI)
FX This work was supported in part by the NationalKey R & D Program of
   China under Grant 2021YFB3100800, in part by the Chongqing Natural
   Science Fund under Grant cstc2021jcyj-jqX0023, in part bythe CCF
   Hikvision Open Fund, National Natural Science Fundation of Chinaunder
   Grants 62036007, 62050175, and 61771079, in part by the
   CAAI-HuaweiMindSpore Open Fund, and in part by the .
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   Arjovsky M, 2017, P 34 INT C MACH LEAR, P214, DOI DOI 10.48550/ARXIV.1701.07875
   Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Huynh D, 2020, PROC CVPR IEEE, P4482, DOI 10.1109/CVPR42600.2020.00454
   Deutsch S, 2017, PROC CVPR IEEE, P5292, DOI 10.1109/CVPR.2017.562
   Ding ZM, 2019, PROC CVPR IEEE, P6184, DOI 10.1109/CVPR.2019.00635
   Ding ZM, 2017, PROC CVPR IEEE, P6005, DOI 10.1109/CVPR.2017.636
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Frome A., 2013, P ADV NEUR INF PROC
   Fu YW, 2014, LECT NOTES COMPUT SC, V8690, P584, DOI 10.1007/978-3-319-10605-2_38
   Fu ZY, 2015, PROC CVPR IEEE, P2635, DOI 10.1109/CVPR.2015.7298879
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Guo JC, 2021, IEEE T MULTIMEDIA, V23, P524, DOI 10.1109/TMM.2020.2984091
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089
   Jiang HJ, 2019, IEEE I CONF COMP VIS, P9764, DOI 10.1109/ICCV.2019.00986
   Jiang HJ, 2017, IEEE I CONF COMP VIS, P4233, DOI 10.1109/ICCV.2017.453
   Kingma D. P., 2014, arXiv
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li J, 2020, IEEE T IMAGE PROCESS, V29, P5817, DOI 10.1109/TIP.2020.2986892
   Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758
   Li X, 2017, NEUROCOMPUTING, V238, P76, DOI 10.1016/j.neucom.2017.01.038
   Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779
   Li YN, 2017, PROC CVPR IEEE, P5207, DOI 10.1109/CVPR.2017.553
   Liu L, 2020, AAAI CONF ARTIF INTE, V34, P4868
   Liu Shaoteng, 2020, P IEEE CVF C COMP VI, P9273
   Liu SC, 2018, ADV NEUR IN, V31
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Lu LH, 2020, IEEE T MULTIMEDIA, V22, P524, DOI 10.1109/TMM.2019.2930344
   Meng M, 2019, IEEE T IMAGE PROCESS, V28, P1824, DOI 10.1109/TIP.2018.2881926
   Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220
   Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29
   Ni J., 2019, Advances in neural information processing systems, P6146
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Norouzi M., 2014, P INT C LEARN REPR
   Pandey A, 2020, IEEE WINT CONF APPL, P2529, DOI 10.1109/WACV45572.2020.9093402
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Romera-Paredes Bernardino, 2015, ICML
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Samuel D, 2021, IEEE WINT CONF APPL, P286, DOI 10.1109/WACV48630.2021.00033
   Satorras V.G., 2018, ICLR
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9
   Snell J, 2017, ADV NEUR IN, V30
   Socher R., 2013, P 2013 C EMP METH NA, V2013, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791
   Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Verma VK, 2020, AAAI CONF ARTIF INTE, V34, P6062
   Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xie GS, 2022, IEEE T NEUR NET LEAR, V33, P2903, DOI 10.1109/TNNLS.2020.3046924
   Zhang F., 2019, INT C MACHINE LEARNI, P7434
   Zhang H., 2017, MULTISTYLE GENERATIV
   Zhang HG, 2019, PROC CVPR IEEE, P2765, DOI 10.1109/CVPR.2019.00288
   Zhang HG, 2018, PROC CVPR IEEE, P7670, DOI 10.1109/CVPR.2018.00800
   Zhang J, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3291124
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
   Zhang ZM, 2016, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2016.649
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
   Zongyan Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12862, DOI 10.1109/CVPR42600.2020.01288
NR 75
TC 4
Z9 4
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2127
EP 2139
DI 10.1109/TMM.2022.3142958
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100041
DA 2024-07-18
ER

PT J
AU Wang, B
   Liu, CS
   Chang, FL
   Wang, WQ
   Li, NJ
AF Wang, Bin
   Liu, Chunsheng
   Chang, Faliang
   Wang, Wenqian
   Li, Nanjun
TI AE-Net:Adjoint Enhancement Network for Efficient Action Recognition in
   Video Understanding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; video understanding; temporal modeling; motion
   feature
AB Action recognition in video understanding is a challenging task, largely because of the complexity and difficulty in temporal modeling, making it suffer from motion information loss and misalignment of temporal attention in spatial dimensions. To overcome these difficulties, we propose a novel temporal modeling method called Adjoint Enhancement Network (AE-Net), which can fully explore clues of motion and time in the long-range structure. The AE-Net mainly consists of two new modules: the Initial Adjoint Enhancement Module (IAE-Module), which deals with shallow features; and the Global Adjoint Enhancement Module (GAE-Module), which deals with global features. With a novel mechanism of parallel spatio-temporal convolution and difference fusion, the IAE-Module is to enhance the degree of motion transformation in shallow network features, exciting the potential of motion flow and avoiding motion information loss. The GAE-Module is proposed to improve the local temporal representation in long-range structures by feeding the enhanced feature differences into a spatial cascade module with residuals to resolve the misalignment of temporal attention in the spatial dimension.The experimental results show that our AE-Net can achieve state-of-the-art results in Something-Something V1, UCF-101 and HMDB-51 datasets.
C1 [Wang, Bin; Liu, Chunsheng; Chang, Faliang; Wang, Wenqian; Li, Nanjun] Shandong Univ, Sch Control Sci & Engn, Jinan 250012, Peoples R China.
C3 Shandong University
RP Chang, FL (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250012, Peoples R China.
EM sducvwangb@gmail.com; liuchunsheng@sdu.edu.cn; flchang@sdu.edu.cn;
   wqwang@mail.sdu.edu.cn; sdulnj@gmail.com
OI Wang, Wenqian/0000-0003-0285-9786
FU National Key Ramp;D Program of China
FX No Statement Available
CR Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heeseung Kwon, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P345, DOI 10.1007/978-3-030-58517-4_21
   Huang WB, 2019, IEEE T IMAGE PROCESS, V28, P1773, DOI 10.1109/TIP.2018.2877936
   Jelodar AB, 2019, IEEE T MULTIMEDIA, V21, P1813, DOI 10.1109/TMM.2018.2885228
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lee MG, 2018, LECT NOTES COMPUT SC, V11214, P392, DOI 10.1007/978-3-030-01249-6_24
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13688, DOI 10.1109/ICCV48922.2021.01345
   Liu ZY, 2020, AAAI CONF ARTIF INTE, V34, P11669
   Martinez B, 2019, IEEE I CONF COMP VIS, P5481, DOI 10.1109/ICCV.2019.00558
   Simonyan K., 2015, P NEUR INF PROC SYST, V27, P568
   Soomro K., 2012, CoRR, V2
   Stroud JC, 2020, IEEE WINT CONF APPL, P614, DOI 10.1109/wacv45572.2020.9093274
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YB, 2017, PROC CVPR IEEE, P2097, DOI 10.1109/CVPR.2017.226
   Wang Z., 2021, IEEE Trans. Multimedia, V22, P2126
   Wang ZW, 2021, PROC CVPR IEEE, P13209, DOI 10.1109/CVPR46437.2021.01301
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Yang JY, 2021, IEEE T MULTIMEDIA, V23, P883, DOI 10.1109/TMM.2020.2990082
   Zhao Y, 2018, PROC CVPR IEEE, P6566, DOI 10.1109/CVPR.2018.00687
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
NR 42
TC 1
Z9 1
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5458
EP 5468
DI 10.1109/TMM.2022.3193057
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD7R0
UT WOS:001116593700008
DA 2024-07-18
ER

PT J
AU Wang, ST
   Zhu, JC
   Yin, YQ
   Wang, DJ
   Cheng, TCE
   Wang, YZ
AF Wang, Sutong
   Zhu, Jiacheng
   Yin, Yunqiang
   Wang, Dujuan
   Cheng, T. C. Edwin
   Wang, Yanzhang
TI Interpretable Multi-Modal Stacking-Based Ensemble Learning Method for
   Real Estate Appraisal
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Real estate appraisal; multi-modal convolutional neural network;
   interior images; geographical locations
ID FUSION; SYSTEM
AB With the development of online real estate trading platforms, multi-modal housing trading data, including structural information, location, and interior image data, are being accumulated. The accurate appraisal of real estate makes sense for government officials, urban policymakers, real estate sellers, and personal purchasers. In this study, we propose an interpretable multi-modal stacking-based ensemble learning (IMSEL) method that deals with various modalities for real estate appraisals. We crawl the structural and image data of real estate in Chengdu city, China from the nation's largest real estate transaction platform with the location information, including public services, within 2 km of the real estate using Baidu map. We then compare the predictive results from IMSEL with those from previous state-of-art methods in the literature in terms of the root mean square error, mean absolute percentage error, mean absolute error, and coefficient of determination (R-2). The comparison results show that IMSEL outperformed the other methods. We verified the improvement of introducing a data transformation strategy and deep visual features through a 10-fold cross-validation. We also discuss the managerial implications of our research findings.
C1 [Wang, Sutong; Wang, Yanzhang] Dalian Univ Technol, Inst Informat & Decis Technol, Dalian 116000, Liaoning, Peoples R China.
   [Zhu, Jiacheng; Wang, Dujuan] Sichuan Univ, Business Sch, Chengdu 610064, Sichuan, Peoples R China.
   [Yin, Yunqiang] Univ Elect Sci & Technol China, Sch Management & Econ, Chengdu 611731, Sichuan, Peoples R China.
   [Cheng, T. C. Edwin] Hong Kong Polytech Univ, Dept Logist & Maritime Studies, Hung Hom, Kowloon, Hong Kong, Peoples R China.
C3 Dalian University of Technology; Sichuan University; University of
   Electronic Science & Technology of China; Hong Kong Polytechnic
   University
RP Wang, DJ (corresponding author), Sichuan Univ, Business Sch, Chengdu 610064, Sichuan, Peoples R China.
EM wangsutong@mail.dlut.edu.cn; jczhuscu@163.com; yinyq@uestc.edu.cn;
   wangdujuan@dlut.edu.cn; edwin.cheng@polyu.edu.hk; yzwang@dlut.edu.cn
RI Wang, Dujuan/JFK-9849-2023; Wang, Sutong/HNO-8634-2023; Cheng,
   Edwin/D-5688-2015
OI Wang, Sutong/0000-0001-6603-6047; Cheng, Edwin/0000-0001-5127-6419;
   Wang, Dujuan/0000-0003-1617-7057
FU National Natural Science Foundation of China [72171161, 71971041,
   71871148]; National Key Research and Development Program of China
   [2019YFB1404702, 2018YFC0807500]; Sichuan University [SKSYL2021-08]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 72171161,71971041, and 71871148, in
   part by the National Key Research and Development Program of China under
   Grants 2019YFB1404702 and 2018YFC0807500,and in part by Sichuan
   University to Building a World-class University under Grant
   SKSYL2021-08.
CR Ahn JJ, 2012, EXPERT SYST APPL, V39, P8369, DOI 10.1016/j.eswa.2012.01.183
   Antipov EA, 2012, EXPERT SYST APPL, V39, P1772, DOI 10.1016/j.eswa.2011.08.077
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bency AJ, 2017, IEEE WINT CONF APPL, P320, DOI 10.1109/WACV.2017.42
   Bessinger Z, 2016, IEEE IMAGE PROC, P4388, DOI 10.1109/ICIP.2016.7533189
   Bin JC, 2020, NEUROCOMPUTING, V404, P70, DOI 10.1016/j.neucom.2020.05.013
   Bin JC, 2019, MULTIMED TOOLS APPL, V78, P31163, DOI 10.1007/s11042-019-07895-5
   Cui SZ, 2021, APPL SOFT COMPUT, V101, DOI 10.1016/j.asoc.2020.107038
   Del Giudice V, 2017, BUILDINGS-BASEL, V7, DOI 10.3390/buildings7020031
   Eriksen MD, 2019, J URBAN ECON, V111, P132, DOI 10.1016/j.jue.2019.04.007
   Fu YJ, 2016, ACM T KNOWL DISCOV D, V11, DOI 10.1145/2934692
   González S, 2020, INFORM FUSION, V64, P205, DOI 10.1016/j.inffus.2020.07.007
   Kang H, 2019, KSCE J CIV ENG, V23, P4984, DOI 10.1007/s12205-019-5885-y
   Kang YH, 2021, LAND USE POLICY, V111, DOI 10.1016/j.landusepol.2020.104919
   Kettani O, 2015, SOCIO-ECON PLAN SCI, V49, P1, DOI 10.1016/j.seps.2014.12.003
   Kontrimas V, 2011, APPL SOFT COMPUT, V11, P443, DOI 10.1016/j.asoc.2009.12.003
   Kostic Z, 2020, IEEE T MULTIMEDIA, V22, P1904, DOI 10.1109/TMM.2020.2966890
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liu XB, 2018, IEEE T KNOWL DATA EN, V30, P1496, DOI 10.1109/TKDE.2018.2791611
   Liu Y, 2018, DECIS SUPPORT SYST, V105, P1, DOI 10.1016/j.dss.2017.10.009
   Lundberg SM, 2017, ADV NEUR IN, V30
   Ma C, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107499
   Morano P, 2014, ADV MATER RES-SWITZ, V869-870, P14, DOI 10.4028/www.scientific.net/AMR.869-870.14
   Natividade-Jesus E, 2007, DECIS SUPPORT SYST, V43, P779, DOI 10.1016/j.dss.2006.03.014
   Pai PF, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10175832
   Papouskova M, 2019, DECIS SUPPORT SYST, V118, P33, DOI 10.1016/j.dss.2019.01.002
   Prokhorenkova L, 2018, ADV NEUR IN, V31
   Renigier-Bilozor M, 2019, LAND USE POLICY, V87, DOI 10.1016/j.landusepol.2019.104021
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Ullah F, 2018, SUSTAINABILITY-BASEL, V10, DOI 10.3390/su10093142
   Yilmazer S, 2020, LAND USE POLICY, V99, DOI 10.1016/j.landusepol.2020.104889
   You QZ, 2017, IEEE T MULTIMEDIA, V19, P2751, DOI 10.1109/TMM.2017.2710804
   Yuan X, 2013, INFORM SYST, V38, P231, DOI 10.1016/j.is.2012.08.004
   Zhang XG, 2019, DECIS SUPPORT SYST, V116, P48, DOI 10.1016/j.dss.2018.10.009
NR 34
TC 4
Z9 4
U1 8
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 315
EP 328
DI 10.1109/TMM.2021.3126153
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400022
DA 2024-07-18
ER

PT J
AU Wu, P
   Liu, XT
   Liu, J
AF Wu, Peng
   Liu, Xiaotao
   Liu, Jing
TI Weakly Supervised Audio-Visual Violence Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual signals; graph neural network; multimodality; violence
   detection; weak supervision
ID NETWORK; MOVIES; SCENES; AUDIO
AB Violence detection in videos is very promising in practical applications due to the emergence of massive videos in recent years. Most previous works define violence detection as a simple video classification task and use the single modality of small-scale datasets, e.g., visual signal. However, such solutions are undersupplied. To mitigate this problem, we study weakly supervised violence detection on the large-scale audio-visual violence data, and first introduce two complementary tasks, i.e., coarse-grained violent frame detection and fine-grained violent event detection, to advance the simple violence video classification to frame-level violent event localization, which aims to accurately locate the violent events on untrimmed videos. We then propose a novel network that takes as input audio-visual data and contains three parallel branches to capture different relationships among video snippets and further integrate features, where similarity branch and proximity branch capture long-range dependencies using similarity prior and proximity prior, respectively, and score branch dynamically captures the closeness of predicted score. In both coarse-grained and fine-grained tasks, our approach outperforms other state-of-the-art approaches on two public datasets. Moreover, experiment results also show the positive effect of audio-visual input and relationship modeling.
C1 [Wu, Peng; Liu, Xiaotao; Liu, Jing] Xidian Univ, Guangzhou Inst Technol, Guangzhou 510555, Peoples R China.
C3 Xidian University
RP Liu, J (corresponding author), Xidian Univ, Guangzhou Inst Technol, Guangzhou 510555, Peoples R China.
EM xdwupeng@gmail.com; xtliu@xidian.edu.cn; neouma@163.com
RI Wu, Peng/AAO-2670-2021
OI Liu, Xiaotao/0000-0003-4544-760X; Wu, Peng/0000-0003-2938-6798
CR Aytar Y, 2017, Arxiv, DOI arXiv:1706.00932
   Aytar Y, 2016, ADV NEUR IN, V29
   Nievas EB, 2011, LECT NOTES COMPUT SC, V6855, P332, DOI 10.1007/978-3-642-23678-5_39
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chatterjee Moitreya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P701, DOI 10.1007/978-3-030-58583-9_42
   Cheng M, 2021, INT C PATT RECOG, P4183, DOI 10.1109/ICPR48806.2021.9412502
   Cheng Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3884, DOI 10.1145/3394171.3413869
   Demarty CH, 2015, MULTIMED TOOLS APPL, V74, P7379, DOI 10.1007/s11042-014-1984-4
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25
   Feng JC, 2021, PROC CVPR IEEE, P14004, DOI 10.1109/CVPR46437.2021.01379
   Gan C, 2020, IEEE INT CONF ROBOT, P9701, DOI [10.1109/ICRA40945.2020.9197008, 10.1109/icra40945.2020.9197008]
   Gan Chuang, 2020, P IEEECVF C COMPUTER, P10478, DOI DOI 10.1109/CVPR42600.2020.01049
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Georgescu MI, 2021, PROC CVPR IEEE, P12737, DOI 10.1109/CVPR46437.2021.01255
   Giannakopoulos T., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3244, DOI 10.1109/ICPR.2010.793
   Giannakopoulos T, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P90, DOI 10.1109/MMSP.2007.4412825
   Giannakopoulos T, 2010, LECT NOTES ARTIF INT, V6040, P91, DOI 10.1007/978-3-642-12842-4_13
   Ginosar S, 2019, PROC CVPR IEEE, P3492, DOI 10.1109/CVPR.2019.00361
   Hanson A, 2019, LECT NOTES COMPUT SC, V11130, P280, DOI 10.1007/978-3-030-11012-3_24
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Hassner T., 2012, 2012 IEEE COMP SOC C, P1, DOI [DOI 10.1109/CVPRW.2012.6239348, 10.1109/CVPRW.2012.6239348]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KK, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3112784
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Islam A, 2020, IEEE WINT CONF APPL, P536, DOI 10.1109/WACV45572.2020.9093620
   Kazakos E, 2019, IEEE I CONF COMP VIS, P5491, DOI 10.1109/ICCV.2019.00559
   Lee J.-T., 2021, PROC INT C LEARN REP, P1
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Lin JA, 2009, LECT NOTES COMPUT SC, V5879, P930
   Lin SH, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909882
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Mohammadi S, 2016, LECT NOTES COMPUT SC, V9911, P3, DOI 10.1007/978-3-319-46478-7_1
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Oh TH, 2019, PROC CVPR IEEE, P7531, DOI 10.1109/CVPR.2019.00772
   Pang WF, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2260, DOI 10.1109/ICASSP39728.2021.9413686
   Paul S., 2018, P EUR C COMP VIS, P563
   Peixoto B, 2019, INT CONF ACOUST SPEE, P8276, DOI 10.1109/ICASSP.2019.8682833
   Penet C, 2012, INT CONF ACOUST SPEE, P2393, DOI 10.1109/ICASSP.2012.6288397
   Peng Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P322, DOI 10.1007/978-3-030-58577-8_20
   Perez M, 2019, INT CONF ACOUST SPEE, P2662, DOI [10.1109/icassp.2019.8683676, 10.1109/ICASSP.2019.8683676]
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Pistilli Francesca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P103, DOI 10.1007/978-3-030-58565-5_7
   Schölkopf B, 2000, ADV NEUR IN, V12, P582
   Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Singh A, 2018, IEEE COMPUT SOC CONF, P1710, DOI 10.1109/CVPRW.2018.00214
   Sudhakaran S., 2017, 2017 14 IEEE INT C A, P1, DOI DOI 10.1109/AVSS.2017.8078468
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   Tian YP, 2021, PROC CVPR IEEE, P2744, DOI 10.1109/CVPR46437.2021.00277
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   Tian Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4955, DOI 10.1109/ICCV48922.2021.00493
   Valsesia D, 2020, IEEE T IMAGE PROCESS, V29, P8226, DOI 10.1109/TIP.2020.3013166
   Wang J, 2019, IEEE I CONF COMP VIS, P8200, DOI 10.1109/ICCV.2019.00829
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wu K, 2022, IEEE T CYBERNETICS, V52, P12854, DOI 10.1109/TCYB.2021.3090769
   Wu P, 2021, IEEE T IMAGE PROCESS, V30, P3513, DOI 10.1109/TIP.2021.3062192
   Wu P, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107515
   Wu P, 2020, IEEE T NEUR NET LEAR, V31, P2609, DOI 10.1109/TNNLS.2019.2933554
   Wu Y, 2021, PROC CVPR IEEE, P1326, DOI 10.1109/CVPR46437.2021.00138
   Wu Y, 2019, IEEE I CONF COMP VIS, P6301, DOI 10.1109/ICCV.2019.00639
   Xuan HY, 2020, AAAI CONF ARTIF INTE, V34, P279
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yao Y, 2020, Arxiv, DOI arXiv:2004.03044
   Yapeng Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P436, DOI 10.1007/978-3-030-58580-8_26
   Yukun Su, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P74, DOI 10.1007/978-3-030-58548-8_5
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zajdel W, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P200, DOI 10.1109/AVSS.2007.4425310
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhai YH, 2022, IEEE T MULTIMEDIA, V24, P1857, DOI 10.1109/TMM.2021.3073235
   Zhang SW, 2020, IEEE T MULTIMEDIA, V22, P2610, DOI 10.1109/TMM.2019.2959425
   Zhao T, 2020, Arxiv, DOI arXiv:2008.07728
   Zhao T, 2021, INT J COMPUT VISION, V129, P2474, DOI 10.1007/s11263-021-01473-9
   Zhong JX, 2019, PROC CVPR IEEE, P1237, DOI 10.1109/CVPR.2019.00133
   Zhu LC, 2022, IEEE T PATTERN ANAL, V44, P273, DOI 10.1109/TPAMI.2020.3007511
   Zhu Yi, 2019, 30 BRIT MACH VIS C 2, P1
NR 82
TC 4
Z9 4
U1 7
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1674
EP 1685
DI 10.1109/TMM.2022.3147369
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100006
DA 2024-07-18
ER

PT J
AU Wu, ZH
   Wen, J
   Xu, Y
   Yang, J
   Zhang, D
AF Wu, Zhihao
   Wen, Jie
   Xu, Yong
   Yang, Jian
   Zhang, David
TI Multiple Instance Detection Networks With Adaptive Instance Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Weakly supervised object detection; multiple instance detection
   networks; instance refinement framework; proposal score
ID SUPERVISED OBJECT LOCALIZATION
AB Weakly supervised object detection (WSOD) aims to train object detectors by using only image-level annotations. Many recent works on WSOD adopt multiple instance detection networks (MIDN), which usually generate a certain number of proposals and regard proposal classification as a latent model learning within image classification. However, these methods tend to detect salient object, salient object parts and clustered objects due to lack of instance-level annotations during training. Thus a core issue is how to guarantee that the network learn as many objects with precise bounding boxes as possible. In this paper, we address this issue by exploiting the potential of proposal scores during training. We propose an adaptive instance refinement (AIR) framework with three novel designs, which can be integrated with MIDN into a single network. Specifically, adaptive instance mining attempts to discover all positive instances according to the score distribution of proposals and their spatial similarity. Adaptive score modulation dynamically adjusts proposal scores to make the network focus more on instances with different difficulties in different training iterations. Adaptive knowledge refinement distills important information from all previous stages by the weighted average of proposal scores. The experimental results on the PASCAL VOC 2007 and 2012 benchmarks and the MS COCO benchmark demonstrate that AIR significantly improves the performance of the original MIDN and achieves the state-of-the-art results.
C1 [Wu, Zhihao; Wen, Jie; Xu, Yong] Harbin Inst Technol, Biocomp Res Ctr, Shenzhen 518055, Peoples R China.
   [Wu, Zhihao; Wen, Jie; Xu, Yong] Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Xu, Yong] Pengcheng Lab, Shenzhen 518055, Peoples R China.
   [Yang, Jian] Nanjing Univ Sci & Technol, PCA Lab, Key Lab Intelligent Percept & Syst High Dimens Inf, Minist Educ, Nanjing 210094, Peoples R China.
   [Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Social, Nanjing 210094, Peoples R China.
   [Zhang, David] Chinese Univ Hong Kong Shenzhen, Sch Data Sci, Shenzhen 518172, Peoples R China.
   [Zhang, David] Shenzhen Res Inst Big Data, Shenzhen 518172, Peoples R China.
   [Zhang, David] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518172, Peoples R China.
C3 Harbin Institute of Technology; Nanjing University of Science &
   Technology; Nanjing University of Science & Technology; The Chinese
   University of Hong Kong, Shenzhen; Shenzhen Research Institute of Big
   Data; Shenzhen Institute of Artificial Intelligence & Robotics for
   Society
RP Xu, Y (corresponding author), Harbin Inst Technol, Biocomp Res Ctr, Shenzhen 518055, Peoples R China.; Xu, Y (corresponding author), Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
EM horatio_ng@163.com; jiewen_pr@126.com; yongxu@ymail.com;
   csjyang@njust.edu.cn; davidzhang@cuhk.edu.cn
RI li, xiaomin/KCX-9845-2024; Zhang, David/AEM-1699-2022; Zhang, David
   D/O-9396-2016; Wen, Jie/AAH-8083-2020; Wen, Jie/G-7235-2015
OI Zhang, David D/0000-0002-5027-5286; Wen, Jie/0000-0001-9554-2379
FU National Key Research and Development Program of China [2018AAA0100102];
   Establishment of Key Laboratory of Shenzhen Science and Technology
   Innovation Committee [ZDSYS20190902093015527]; Guangdong Basic and
   Applied Basic Research Foundation [2019A1515110582]
FX This work was supported by the National Key Research and Development
   Program of China under Project 2018AAA0100102, in part by the
   Establishment of Key Laboratory of Shenzhen Science and Technology
   Innovation Committee under Grant ZDSYS20190902093015527, in part by
   Guangdong Basic and Applied Basic Research Foundation under Grant
   2019A1515110582.
CR [Anonymous], 2021, PCL: Proposal Cluster Learning for Weakly Supervised Object Detection
   [Anonymous], 2016, P EUR C COMP VIS
   [Anonymous], 2014, Advances in Neural Information Processing Systems
   Arun A, 2019, PROC CVPR IEEE, P9424, DOI 10.1109/CVPR.2019.00966
   Bency AJ, 2016, LECT NOTES COMPUT SC, V9905, P714, DOI 10.1007/978-3-319-46448-0_43
   Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Bilen H, 2015, PROC CVPR IEEE, P1081, DOI 10.1109/CVPR.2015.7298711
   Che WB, 2020, IEEE T MULTIMEDIA, V22, P2307, DOI 10.1109/TMM.2019.2954750
   Chen L, 2021, IEEE T MULTIMEDIA, V23, P4297, DOI 10.1109/TMM.2020.3040539
   Chen XY, 2022, IEEE T MULTIMEDIA, V24, P1558, DOI 10.1109/TMM.2021.3067439
   Cheng G, 2020, IEEE T IMAGE PROCESS, V29, P5794, DOI 10.1109/TIP.2020.2987161
   Cinbis RG, 2017, IEEE T PATTERN ANAL, V39, P189, DOI 10.1109/TPAMI.2016.2535231
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deselaers T, 2012, INT J COMPUT VISION, V100, P275, DOI 10.1007/s11263-012-0538-3
   Diba A, 2017, PROC CVPR IEEE, P5131, DOI 10.1109/CVPR.2017.545
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao MF, 2018, LECT NOTES COMPUT SC, V11205, P155, DOI 10.1007/978-3-030-01246-5_10
   Gao Y, 2019, IEEE I CONF COMP VIS, P9833, DOI 10.1109/ICCV.2019.00993
   Girshick R., 2014, P 2014 IEEE C COMPUT, P580, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Guo YD, 2019, IEEE T MULTIMEDIA, V21, P2903, DOI 10.1109/TMM.2019.2912703
   Huang C, 2022, IEEE T IND INFORM, V18, P5171, DOI 10.1109/TII.2021.3122801
   Huang Z., 2020, P 34 INT C NEUR INF, V33, P16797
   Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525
   Jia QF, 2021, AAAI CONF ARTIF INTE, V35, P1682
   Kantorov V, 2016, LECT NOTES COMPUT SC, V9909, P350, DOI 10.1007/978-3-319-46454-1_22
   Ke Yang, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P8371, DOI 10.1109/ICCV.2019.00846
   Li D, 2020, IEEE T PATTERN ANAL, V42, P1424, DOI 10.1109/TPAMI.2019.2899839
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li Y, 2016, LECT NOTES COMPUT SC, V9906, P19, DOI 10.1007/978-3-319-46475-6_2
   Lin C., 2020, PROC AAAI C ARTIF IN, p11 482
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Papadopoulos DP, 2016, PROC CVPR IEEE, P854, DOI 10.1109/CVPR.2016.99
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren WQ, 2016, IEEE T PATTERN ANAL, V38, P405, DOI 10.1109/TPAMI.2015.2456908
   Shao S, 2019, IEEE I CONF COMP VIS, P8429, DOI 10.1109/ICCV.2019.00852
   Shen YH, 2019, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2019.00079
   Shi MJ, 2017, IEEE I CONF COMP VIS, P3401, DOI 10.1109/ICCV.2017.366
   Shi MJ, 2016, LECT NOTES COMPUT SC, V9909, P105, DOI 10.1007/978-3-319-46454-1_7
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh KK, 2019, PROC CVPR IEEE, P9406, DOI 10.1109/CVPR.2019.00964
   Singh KK, 2016, PROC CVPR IEEE, P3548, DOI 10.1109/CVPR.2016.386
   Su SC, 2016, PROC CVPR IEEE, pCP40, DOI 10.1109/CVPR.2016.382
   Tang P, 2018, LECT NOTES COMPUT SC, V11215, P370, DOI 10.1007/978-3-030-01252-6_22
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304
   Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326
   Tang P, 2017, PATTERN RECOGN, V71, P446, DOI 10.1016/j.patcog.2017.05.001
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Tao QY, 2019, IEEE T MULTIMEDIA, V21, P1135, DOI 10.1109/TMM.2018.2875597
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wan F, 2019, PROC CVPR IEEE, P2194, DOI 10.1109/CVPR.2019.00230
   Wan F, 2018, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2018.00141
   Wei YC, 2018, LECT NOTES COMPUT SC, V11215, P454, DOI 10.1007/978-3-030-01252-6_27
   Wen J, 2021, IEEE T CYBERNETICS, V51, P101, DOI 10.1109/TCYB.2020.2987164
   Wen J, 2020, IEEE T CYBERNETICS, V50, P1418, DOI 10.1109/TCYB.2018.2884715
   Wu S, 2022, IEEE T MULTIMEDIA, V24, P2058, DOI 10.1109/TMM.2021.3075323
   Yin YF, 2021, AAAI CONF ARTIF INTE, V35, P3190
   Yunhang Shen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P118, DOI 10.1007/978-3-030-58598-3_8
   Ze Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12992, DOI 10.1109/CVPR42600.2020.01301
   Zeni L. F., 2020, PROC IEEECVF C COMPU, P768
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P5866, DOI 10.1109/TPAMI.2021.3074313
   Zhang DW, 2020, IEEE T NEUR NET LEAR, V31, P5549, DOI 10.1109/TNNLS.2020.2969483
   Zhang XP, 2018, LECT NOTES COMPUT SC, V11207, P248, DOI 10.1007/978-3-030-01219-9_15
   Zhongzheng Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10595, DOI 10.1109/CVPR42600.2020.01061
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
NR 70
TC 17
Z9 17
U1 8
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 267
EP 279
DI 10.1109/TMM.2021.3125130
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400019
DA 2024-07-18
ER

PT J
AU Xie, QK
   Lu, ZB
   Zhou, WG
   Li, HQ
AF Xie, Qiaokang
   Lu, Zhenbo
   Zhou, Wengang
   Li, Houqiang
TI Improving Person Re-Identification With Multi-Cue Similarity Embedding
   and Propagation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Faces; Feature extraction; Training; Testing;
   Representation learning; Image resolution; Person re-identification;
   multi-cue retrieval; graph convolutional network
AB Most existing person re-identification (Re-ID) methods rely on the visual appearance of the human body. However, face cues are rarely explored in the Re-ID community despite the face that it is an important biometric identifier for human beings. In this work, we propose a Similarity Ensemble Framework (SEF) that uses multi-cue similarity embedding and propagation to effectively fuse body and face information for person re-identification. Specifically, for each query, we first perform standard pedestrian retrieval using body and face cues, respectively, to obtain some candidate results with high confidence. Next, the body and face similarities are combined and embedded into a shared space as node features, and two graphs with the same nodes and different edges with respect to body and face affinities are constructed. Then, the similarity features are propagated in both body and face graphs using graph convolution to capture the relationship among the candidates using different cues. Lastly, the refined features are used to compute the final similarities with the query. The proposed method not only combines the similarities of body and face, but also takes into account the relationship among all the other candidate samples under different cues. Extensive experiments demonstrate that the use of face cues effectively improves the performance of person Re-ID even if the performance obtained by the face alone is much lower than that of the body, suggesting that our approach is able to capture valuable information beyond body from weaker face cues in person Re-ID scenarios.
C1 [Xie, Qiaokang; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Lu, Zhenbo] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230030, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM xieqiaok@mail.ustc.edu.cn; luzhenbo_2018@163.com; zhwg@ustc.edu.cn;
   lihq@ustc.edu.cn
FU National Natural Science Foundation of China [62021001, 61836011,
   61822208]; GPU cluster built by theMCC Lab of Information Science and
   Technology Institution, USTC
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62021001, 61836011, and 61822208, and
   in part by the GPU cluster built by theMCC Lab of Information Science
   and Technology Institution, USTC. TheAssociate Editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Song Wang.
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Chen JR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2535, DOI 10.1145/3343031.3356067
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P4285, DOI 10.1109/TMM.2021.3114539
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   Dhingra N, 2021, IEEE INT CONF COMP V, P865, DOI 10.1109/ICCVW54120.2021.00102
   Dietlmeier J, 2021, INT C PATT RECOG, P6912, DOI 10.1109/ICPR48806.2021.9412340
   Dong CQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2531, DOI 10.1145/3343031.3356057
   Fang X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2526, DOI 10.1145/3343031.3356056
   Fangbin Wan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P3620, DOI 10.1109/CVPRW50498.2020.00423
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Fey Matthias, 2019, ICLR WORKSH REPR LEA
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Gong X, 2022, IEEE T MULTIMEDIA, V24, P217, DOI 10.1109/TMM.2021.3050082
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Hamilton WL, 2017, ADV NEUR IN, V30
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2020, Arxiv, DOI arXiv:2006.02631
   Hirzer M, 2012, LECT NOTES COMPUT SC, V7577, P780, DOI 10.1007/978-3-642-33783-3_56
   Hong PX, 2021, PROC CVPR IEEE, P10508, DOI 10.1109/CVPR46437.2021.01037
   Hou RB, 2019, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR.2019.00735
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Huang YK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P365, DOI 10.1145/3343031.3350994
   Huang ZX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2521, DOI 10.1145/3343031.3356050
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Jin X, 2020, AAAI CONF ARTIF INTE, V34, P11173
   Karanam S, 2015, IEEE I CONF COMP VIS, P4516, DOI 10.1109/ICCV.2015.513
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Kuan Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P346, DOI 10.1007/978-3-030-58580-8_21
   Lei Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13366, DOI 10.1109/CVPR42600.2020.01338
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li JN, 2019, AAAI CONF ARTIF INTE, P8618
   Li W, 2020, INT J COMPUT VISION, V128, P1635, DOI 10.1007/s11263-019-01274-1
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YY, 2021, KNOWL-BASED SYST, V228, DOI 10.1016/j.knosys.2021.107281
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu WY, 2016, PR MACH LEARN RES, V48
   Liu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2516, DOI 10.1145/3343031.3356081
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang PY, 2021, IEEE T IMAGE PROCESS, V30, P2908, DOI 10.1109/TIP.2021.3055952
   Wang Y, 2018, PROC CVPR IEEE, P8042, DOI [10.1109/CVPR.2018.00839, 10.1109/CVPR.2018.00736]
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Xia B, 2019, IEEE I CONF COMP VIS, P3759, DOI 10.1109/ICCV.2019.00386
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yang H, 2022, IEEE T IMAGE PROCESS, V31, P164, DOI 10.1109/TIP.2021.3129117
   Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2018, LECT NOTES COMPUT SC, V11211, P176, DOI 10.1007/978-3-030-01234-2_11
   Zhang N, 2015, PROC CVPR IEEE, P4804, DOI 10.1109/CVPR.2015.7299113
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng RC, 2021, NEUROCOMPUTING, V433, P19, DOI 10.1016/j.neucom.2020.12.100
   Zheng ZD, 2021, Arxiv, DOI arXiv:2006.04569
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou KY, 2022, IEEE T PATTERN ANAL, V44, P5056, DOI 10.1109/TPAMI.2021.3069237
   Zhou SP, 2019, IEEE I CONF COMP VIS, P8039, DOI 10.1109/ICCV.2019.00813
NR 81
TC 4
Z9 4
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6384
EP 6396
DI 10.1109/TMM.2022.3207949
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500052
DA 2024-07-18
ER

PT J
AU Xu, XX
   Zou, Q
   Lin, X
AF Xu, Xixia
   Zou, Qi
   Lin, Xue
TI Structure-Enriched Topology Learning For Cross-Domain Multi-Person Pose
   Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pose estimation; Semantics; Training; Topology; Heating systems;
   Adaptation models; Annotations; Adaptive human-topolopy learning; domain
   adaptation; multi-person pose estimation
ID NETWORK
AB Human pose estimation has been widely studied with much focus on supervised learning. However, in real applications, a pretrained pose estimation model usually needs be adapted to a novel domain without labels or with sparse labels. Existing domain adaptation methods cannot well deal with it since poses have flexible topological structures and need fine-grained local features. Aiming at the characteristics of human pose, we propose a novel domain adaptation method for multi-person pose estimation (MPPE) to alleviate the human-level shift. Firstly, the training samples of human poses are clustered into groups according to the posture similarity. Within the clustered space, we conduct three adaptation modules: Cross-Attentive Feature Alignment (CAFA), Intra-domain Structure Adaptation (ISA) and Adaptive Human-Topology Adaptation (AHTA). The CAFA adopts a bidirectional spatial attention mechanism to explore fine-grained local feature correlation between two humans, and thus to adaptively aggregate consistent features for adaptation. ISA only works in semi-supervised domain adaptation (SSDA) to exploit semantic relationship of corresponding keypoints for reducing the intra-domain bias. Importantly, we creatively propose an AHTA to enrich human topological knowledge for reducing the inter-domain discrepancy. Specifically, the pose structure and the cross-instance topological relations are modeled via graph networks. This flexible topology learning benefits the occluded or extreme pose inference. Extensive experiments are conducted on two popular benchmarks and additional two challenging datasets. Results demonstrate the competency of our method, which works in unsupervised or semi-supervised modes, compared with the existing supervised approaches.
C1 [Xu, Xixia; Zou, Qi; Lin, Xue] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Zou, Q (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM 18120432@bjtu.edu.cn; qzou@bjtu.edu.cn; 18112028@bjtu.edu.cn
OI Xu, xixia/0000-0001-6305-475X
FU Fundamental Research Funds for the Central Universities [2019JBM019];
   Beijing Natural Science Foundation [M22022]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant 2019JBM019 and in part by Beijing
   Natural Science Foundation under Grant M22022.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Bertasius G, 2019, ADV NEUR IN, V32
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Brasó G, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11833, DOI 10.1109/ICCV48922.2021.01164
   Bulat A, 2020, IEEE INT CONF AUTOMA, P8, DOI 10.1109/FG47880.2020.00014
   Cao JK, 2019, IEEE I CONF COMP VIS, P9497, DOI 10.1109/ICCV.2019.00959
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chen Y, 2017, IEEE I CONF COMP VIS, P1221, DOI 10.1109/ICCV.2017.137
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Choi O, 2019, IEEE T MULTIMEDIA, V21, P1487, DOI 10.1109/TMM.2018.2880608
   Chou CJ, 2018, ASIAPAC SIGN INFO PR, P17, DOI 10.23919/APSIPA.2018.8659538
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Dai Y, 2021, AAAI CONF ARTIF INTE, V35, P1193
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Grandvalet Y, 2004, Advances in neural information processing systems, V17
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hoffman J, 2016, Arxiv, DOI arXiv:1612.02649
   Jian Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P492, DOI 10.1007/978-3-030-58621-8_29
   Jin S., 2020, ECCV, P718
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Ke LP, 2018, LECT NOTES COMPUT SC, V11206, P731, DOI 10.1007/978-3-030-01216-8_44
   Kingma D. P., 2014, arXiv
   Kocabas M, 2018, LECT NOTES COMPUT SC, V11215, P437, DOI 10.1007/978-3-030-01252-6_26
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li K, 2021, PROC CVPR IEEE, P1944, DOI 10.1109/CVPR46437.2021.00198
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Li YJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11293, DOI 10.1109/ICCV48922.2021.01112
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lingteng Qiu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P488, DOI 10.1007/978-3-030-58529-7_29
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Luo Z., 2017, P 31 INT C NEUR INF, P165
   Luvizon DC, 2019, COMPUT GRAPH-UK, V85, P15, DOI 10.1016/j.cag.2019.09.002
   Mao WA, 2021, PROC CVPR IEEE, P9030, DOI 10.1109/CVPR46437.2021.00892
   Martinezgonzalez A., 2018, P EUR C COMP VIS, P346
   Newell A, 2017, ADV NEUR IN, V30
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nie XC, 2019, IEEE I CONF COMP VIS, P6950, DOI 10.1109/ICCV.2019.00705
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Pan BX, 2020, AAAI CONF ARTIF INTE, V34, P11815
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Qiu ZW, 2020, AAAI CONF ARTIF INTE, V34, P11924
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saito K, 2019, IEEE I CONF COMP VIS, P8049, DOI 10.1109/ICCV.2019.00814
   Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Su K, 2019, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2019.00582
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun K, 2017, IEEE I CONF COMP VIS, P5600, DOI 10.1109/ICCV.2017.597
   Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12
   Tang ZQ, 2018, LECT NOTES COMPUT SC, V11207, P348, DOI 10.1007/978-3-030-01219-9_21
   Tsai YH, 2019, IEEE I CONF COMP VIS, P1456, DOI 10.1109/ICCV.2019.00154
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei P, 2019, IEEE T MULTIMEDIA, V21, P2195, DOI 10.1109/TMM.2019.2897902
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu ZX, 2018, LECT NOTES COMPUT SC, V11209, P535, DOI 10.1007/978-3-030-01228-1_32
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xiao YB, 2022, AAAI CONF ARTIF INTE, P2822
   Xu XX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2326, DOI 10.1145/3394171.3414040
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang BY, 2019, IEEE T CIRC SYST VID, V29, P461, DOI 10.1109/TCSVT.2017.2789224
   Yang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11782, DOI 10.1109/ICCV48922.2021.01159
   Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Yanrui Bin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P606, DOI 10.1007/978-3-030-58529-7_36
   Zhang JB, 2021, AAAI CONF ARTIF INTE, V35, P3342
   Zhang SH, 2019, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2019.00098
   Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400
   Zhang XH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P926, DOI 10.1145/3343031.3351052
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 82
TC 0
Z9 0
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6272
EP 6284
DI 10.1109/TMM.2022.3207578
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500045
DA 2024-07-18
ER

PT J
AU Xue, C
   Zhong, XH
   Cai, MJ
   Chen, H
   Wang, WW
AF Xue, Cheng
   Zhong, Xionghu
   Cai, Minjie
   Chen, Hao
   Wang, Wenwu
TI Audio-Visual Event Localization by Learning Spatial and Semantic
   Co-Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Location awareness; Task analysis; Semantics; Feature
   extraction; Correlation; Automobiles; Audio-visual; event localization;
   cross-modal; co-attention; deep learning
AB This work aims to temporally localize events that are both audible and visible in video. Previous methods mainly focused on temporal modeling of events with simple fusion of audio and visual features. In natural scenes, a video records not only the events of interest but also ambient acoustic noise and visual background, resulting in redundant information in the raw audio and visual features. Thus, direct fusion of the two features often causes false localization of the events. In this paper, we propose a co-attention model to exploit the spatial and semantic correlations between the audio and visual features, which helps guide the extraction of discriminative features for better event localization. Our assumption is that in an audio-visual event, shared semantic information between audio and visual features exists and can be extracted by attention learning. Specifically, the proposed co-attention model is composed of a co-spatial attention module and a co-semantic attention module that are used to model the spatial and semantic correlations, respectively. The proposed co-attention model can be applied to various event localization tasks, such as cross-modality localization and multimodal event localization. Experiments on the public audio-visual event (AVE) dataset demonstrate that the proposed method achieves state-of-the-art performance by learning spatial and semantic co-attention.
C1 [Xue, Cheng; Zhong, Xionghu; Cai, Minjie; Chen, Hao] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Wang, Wenwu] Univ Surrey, Ctr Vis Speech & Signal Proc, Dept Elect & Elect Engn, Guildford GU2 7XH, England.
C3 Hunan University; University of Surrey
RP Cai, MJ (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
EM chengxue@hnu.edu.cn; xzhong@hnu.edu.cn; caiminjie@hnu.edu.cn;
   haochen@hnu.edu.cn; w.wang@surrey.ac.uk
RI Cai, Minjie/JLR-4487-2023
OI Zhong, Xionghu/0000-0002-7533-2347; Wang, Wenwu/0000-0002-8393-5703
FU National Natural Science Foundation of China [61971186, 61906064]; Hunan
   Provincial Natural Science Foundation of China [2020JJ5082]; Open
   Project Program of State Key Laboratory of Virtual Reality Technology,
   and Systems, Beihang University [VRLAB2020B09]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61971186 and 61906064, in part by the
   Hunan Provincial Natural Science Foundation of China under Grant
   2020JJ5082, and in part by the Open Project Program of State Key
   Laboratory of Virtual Reality Technology, and Systems, Beihang
   University under Grant VRLAB2020B09.
CR Andrew G., 2013, ICML, P1247
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Arandjelovic Relja, 2018, P EUR C COMP VIS ECC, P435
   Aytar Y, 2017, Arxiv, DOI arXiv:1706.00932
   Aytar Y, 2016, ADV NEUR IN, V29
   Bandanau D, 2016, INT CONF ACOUST SPEE, P4945, DOI 10.1109/ICASSP.2016.7472618
   Canton-Ferrer C., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P81, DOI 10.1109/CVPR.2009.5204264
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen Zhuo, 2017, Proc IEEE Int Conf Acoust Speech Signal Process, V2017, P246, DOI 10.1109/ICASSP.2017.7952155
   Chorowski J, 2015, ADV NEUR IN, V28
   Cristani M, 2007, IEEE T MULTIMEDIA, V9, P257, DOI 10.1109/TMM.2006.886263
   Fang SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P248, DOI 10.1145/3240508.3240571
   GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Heittola T, 2013, EURASIP J AUDIO SPEE, DOI 10.1186/1687-4722-2013-1
   Hershey J, 2000, ADV NEUR IN, V12, P813
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hong SX, 2020, INT CONF ACOUST SPEE, P296, DOI [10.1109/icassp40776.2020.9053427, 10.1109/ICASSP40776.2020.9053427]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kiliç V, 2015, IEEE T MULTIMEDIA, V17, P186, DOI 10.1109/TMM.2014.2377515
   Kong QQ, 2019, IEEE-ACM T AUDIO SPE, V27, P1791, DOI 10.1109/TASLP.2019.2930913
   Kong QQ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P316, DOI 10.1109/ICASSP.2018.8461392
   Korbar B, 2018, ADV NEUR IN, V31
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Lin Y., 2019, P 2019 3 INT C ENERG
   Liu Y, 2020, IEEE T MULTIMEDIA, V22, P934, DOI 10.1109/TMM.2019.2937185
   Lu JS, 2016, ADV NEUR IN, V29
   Nagrani A, 2018, LECT NOTES COMPUT SC, V11217, P73, DOI 10.1007/978-3-030-01261-8_5
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Parascandolo G, 2016, INT CONF ACOUST SPEE, P6440, DOI 10.1109/ICASSP.2016.7472917
   Patterson K, 2007, NAT REV NEUROSCI, V8, P976, DOI 10.1038/nrn2277
   Quiroga RQ, 2009, CURR BIOL, V19, P1308, DOI 10.1016/j.cub.2009.06.060
   Rivet B, 2014, IEEE SIGNAL PROC MAG, V31, P125, DOI 10.1109/MSP.2013.2296173
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sutskever I, 2014, ADV NEUR IN, V27
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu CF, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P519, DOI 10.1145/3240508.3240513
   Wu Y, 2019, IEEE I CONF COMP VIS, P6301, DOI 10.1109/ICCV.2019.00639
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
NR 46
TC 9
Z9 9
U1 4
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 418
EP 429
DI 10.1109/TMM.2021.3127029
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yao, X
   Wang, M
   Zhou, WG
   Li, HQ
AF Yao, Xin
   Wang, Min
   Zhou, Wengang
   Li, Houqiang
TI Hash Bit Selection With Reinforcement Learning for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Reinforcement learning; Binary codes; Databases; Hash functions;
   Redundancy; Quantization (signal); Optimization; Hashing; bit selection;
   reinforcement learning; image retrieval
AB In recent years, binary hashing methods have been widely used in large-scale multimedia retrieval because of the low computational complexity and memory cost. Generally, better retrieval accuracy can be achieved with a longer hash code, which, however, may suffer redundancy. In this paper, we propose a novel hash bit selection method, called Hash Bit Selection with Reinforcement Learning (HBS-RL), which aims to adaptively select the most informative bits from the database binary codes. In our approach, the hash bit selection problem is firstly modeled as a Markov Decision Process (MDP), which is solved with reinforcement learning. HBS-RL learns a policy for bit selection, which effectively identifies the most informative bits by directly maximizing mean Average Precision (mAP) during training. Specially, given a generated bit pool, our HBS-RL can sequentially select bits with different code lengths with a very lightweight fully-connected policy network. The proposed method is evaluated on the MNIST, CIFAR-10, ImageNet and NUS-WIDE datasets, and the results show that it significantly improves the retrieval performance of the existing unsupervised and deep supervised hashing methods. It also outperforms the state-of-the-art bit selection methods. For convenience of repeating our results, we release our source code at: https://github.com/ xyez/ HBS- RL.
C1 [Yao, Xin; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Wang, Min] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230030, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.; Wang, M (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230030, Peoples R China.
EM xinyao@mail.ustc.edu.cn; wangmin@iai.ustc.edu.cn; zhwg@ustc.edu.cn;
   lihq@ustc.edu.cn
OI Wang, Min/0000-0003-3048-6980; Yao, Xin/0000-0002-2166-9315
FU National Natural Science Foundation of China [62102128, 61836011,
   62021001]; GPU cluster built by the MCC Laboratory of Information
   Science and Technology Institution, USTC
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62102128, 61836011, and 62021001 and in
   part by the GPU cluster built by the MCC Laboratory of Information
   Science and Technology Institution, USTC. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr Ramazan S Aygun.
CR [Anonymous], 2013, P NEURIPS DEEP LEARN
   [Anonymous], 2009, NIPS
   Burda Yuri, 2019, ICLR
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chen YD, 2019, IEEE I CONF COMP VIS, P9795, DOI 10.1109/ICCV.2019.00989
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doan KD, 2022, PROC CVPR IEEE, P9437, DOI 10.1109/CVPR52688.2022.00923
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Haarnoja Tuomas, 2018, P MACHINE LEARNING R, V80
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hoe J. T., 2021, P NIPS, P24286
   Hu YJ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P368, DOI 10.1145/3219819.3219846
   Ie E, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2592
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Kingma D.P., 2014, ARXIV14126980
   Krizhevsky A., 2009, Tech. Rep.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li SY, 2021, PROC CVPR IEEE, P13544, DOI 10.1109/CVPR46437.2021.01334
   Li WJ, 2016, IJCAI, P1711
   Lillicrap, 2015, ARXIV150902971, P1
   Lin QH, 2022, AAAI CONF ARTIF INTE, P7488
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu XL, 2013, PROC CVPR IEEE, P1570, DOI 10.1109/CVPR.2013.206
   Mathe S, 2016, PROC CVPR IEEE, P2894, DOI 10.1109/CVPR.2016.316
   Mnih V, 2016, PR MACH LEARN RES, V48
   Mu YD, 2012, INT J MULTIMED INF R, V1, P59, DOI 10.1007/s13735-012-0003-7
   Paszke A, 2019, ADV NEUR IN, V32
   Peng YX, 2020, IEEE T MULTIMEDIA, V22, P2061, DOI 10.1109/TMM.2019.2951462
   Schull J, 2015, ASSETS'15: PROCEEDINGS OF THE 17TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS & ACCESSIBILITY, P1, DOI 10.1145/2700648.2809870
   Schulman J, 2017, Arxiv, DOI arXiv:1707.06347
   Schulman J, 2015, PR MACH LEARN RES, V37, P1889
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Song JK, 2020, INT J COMPUT VISION, V128, P2243, DOI 10.1007/s11263-020-01305-2
   Su SP, 2018, ADV NEUR IN, V31
   Do TT, 2020, IEEE T MULTIMEDIA, V22, P992, DOI 10.1109/TMM.2019.2935680
   Wang JP, 2022, AAAI CONF ARTIF INTE, P2468
   Wang JP, 2021, AAAI CONF ARTIF INTE, V35, P2755
   Weiss Y., 2008, PROC ADV NEURAL INFO, V21, P1753
   Weng ZY, 2021, IEEE T MULTIMEDIA, V23, P1868, DOI 10.1109/TMM.2020.3004962
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yan C, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1535, DOI 10.1145/3343031.3350927
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
NR 51
TC 4
Z9 4
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6678
EP 6687
DI 10.1109/TMM.2022.3213476
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500075
DA 2024-07-18
ER

PT J
AU Zhang, L
   Zhang, MX
   Song, R
   Zhao, ZY
   Li, XL
AF Zhang, Lin
   Zhang, Mingxin
   Song, Ran
   Zhao, Ziying
   Li, Xiaolei
TI Unsupervised Embedding Learning With Mutual-Information Graph
   Convolutional Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Unsupervised learning; image retrieval; embedding learning; graph neural
   network; mutual information
AB Recently, methods for unsupervised embedding learning have exhibited promising results for extracting desirable representations from unlabeled samples. In general, most methods learn the feature embeddings by handling each sample individually while the structural and semantic relationships between samples are not fully exploited. As a result, the learned embeddings are not sufficiently discriminative. To make use of such inter-sample information for deep embedding learning, this paper proposes an unsupervised method based on the graph convolutional network (GCN). On one hand, our method encodes structural information between the samples corresponding to the nodes in a local neighbourhood of the GCN graph. On the other hand, it leverages the mutual information between the original samples and the augmented ones to ensure that they are globally consistent with each other. Extensive experiments show that our method is not just robust to augmentation perturbations, but also learns discriminative embeddings. Consequently, it achieves the state-of-the-art performance on several challenging datasets.
C1 [Zhang, Lin; Zhang, Mingxin; Song, Ran; Li, Xiaolei] Shandong Univ, Sch Control Sci & Engn, Jinan 250012, Shandong, Peoples R China.
   [Zhao, Ziying] Sun Yat Sen Univ, Sch Engn & Comp Sci, Guangzhou 510275, Peoples R China.
C3 Shandong University; Sun Yat Sen University
RP Song, R (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250012, Shandong, Peoples R China.
EM zl935546110@gmail.com; 202020646@mail.sdu.edu.cn; ransong@sdu.edu.cn;
   zhaozy170@126.com; qylxl@163.com
OI Song, Ran/0000-0002-1344-4415
FU National Natural Science Foundation of China [62076148, 71804172,
   61991411]; Young Taishan Scholars Program of Shandong Province
   [tsqn201909029]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62076148, 71804172, and 61991411 and in
   part by the Young Taishan Scholars Program of Shandong Province under
   Grant tsqn201909029.
CR Bai C, 2021, IEEE T MULTIMEDIA, V23, P2199, DOI 10.1109/TMM.2021.3065578
   Belghazi MI, 2018, PR MACH LEARN RES, V80
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI [10.1109/ICCV.2017.626, 10.1109/ICCV.2017.627]
   Chen CH, 2019, PROC CVPR IEEE, P5707, DOI 10.1109/CVPR.2019.00586
   Coates A., 2011, P 14 INT C ART INT S, P215
   diaeresis>tze Hinrich Schu<spacing, 2008, INTRO INFORM RETRIEV, V39
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Doosti B, 2020, PROC CVPR IEEE, P6607, DOI 10.1109/CVPR42600.2020.00664
   Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141
   Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425
   Fujiwara K., 2020, P IEEE CVF C COMP VI, P11734
   Hamilton WL, 2017, ADV NEUR IN, V30
   Harwood B, 2017, IEEE I CONF COMP VIS, P2840, DOI 10.1109/ICCV.2017.307
   Hassani K., 2020, INT C MACHINE LEARNI, P4116, DOI DOI 10.48550/ARXIV.2006.05582
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hjelm R.D., 2019, P 7 INT C LEARNING R
   Huang Jiabo, 2019, P 36 INT C MACHINE L, P2849
   Iscen A, 2018, PROC CVPR IEEE, P7642, DOI 10.1109/CVPR.2018.00797
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Jing MM, 2021, IEEE T CYBERNETICS, V51, P3390, DOI 10.1109/TCYB.2020.2974106
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A., 2009, Tech. Rep.
   Li D, 2016, LECT NOTES COMPUT SC, V9908, P678, DOI 10.1007/978-3-319-46493-0_41
   Li JJ, 2022, IEEE T KNOWL DATA EN, V34, P5770, DOI 10.1109/TKDE.2021.3060473
   Li JJ, 2017, IEEE T CYBERNETICS, V47, P3516, DOI 10.1109/TCYB.2016.2565898
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li TJ, 2021, IEEE T IMAGE PROCESS, V30, P7677, DOI 10.1109/TIP.2021.3104183
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Lu C, 2015, AAAI CONF ARTIF INTE, P3811
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Nowozin S, 2016, ADV NEUR IN, V29
   Paszke Adam, 2017, NIPS 2017 WORKSH AUT
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Shu XJ, 2022, IEEE T CIRC SYST VID, V32, P4390, DOI 10.1109/TCSVT.2021.3128214
   Sohn K, 2016, ADV NEUR IN, V29
   Song HO, 2017, PROC CVPR IEEE, P2206, DOI 10.1109/CVPR.2017.237
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Song R, 2022, INT J COMPUT VISION, V130, P1210, DOI 10.1007/s11263-022-01592-x
   Sun C, 2019, Arxiv, DOI arXiv:1906.05743
   Sun JH, 2020, PROC CVPR IEEE, P657, DOI 10.1109/CVPR42600.2020.00074
   Sun MJ, 2022, IEEE T MULTIMEDIA, V24, P2567, DOI 10.1109/TMM.2021.3086727
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tao Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9260, DOI 10.1109/CVPR42600.2020.00928
   Tschannen M., 2019, ARXIV190713625
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Velickovic Petar, 2019, ICLR
   Velickovic Petar, 2018, INT C LEARN REPR
   Wah C, 2011, CALTECH UCSD BIRDS 2
   Wang X, 2020, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR42600.2020.00642
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Yan C., 2021, P IEEECVF INT C COMP, P10943
   Yang JR, 2020, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR42600.2020.00335
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Ye M, 2020, PROC CVPR IEEE, P5456, DOI 10.1109/CVPR42600.2020.00550
   Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Zhang WD, 2022, IEEE T IMAGE PROCESS, V31, P868, DOI 10.1109/TIP.2021.3131025
   Zhu XP, 2021, INT J COMPUT VISION, V129, P1580, DOI 10.1007/s11263-021-01440-4
NR 64
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5916
EP 5926
DI 10.1109/TMM.2022.3200852
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500020
DA 2024-07-18
ER

PT J
AU Zhang, PN
   Huang, FY
   Wu, DP
   Yang, BR
   Yang, ZG
   Tan, L
AF Zhang, Puning
   Huang, Fengyi
   Wu, Dapeng
   Yang, Boran
   Yang, Zhigang
   Tan, Lei
TI Device-Edge-Cloud Collaborative Acceleration Method Towards Occluded
   Face Recognition in High-Traffic Areas
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Feature extraction; Computational modeling;
   Collaboration; Image edge detection; Delays; Servers; Occluded face
   recognition; device-edge-cloud collaboration; recognition acceleration;
   model partitioning
AB Wearing masks can effectively inhibit the spread and damage of COVID-19. A device-edge-cloud collaborative recognition architecture is designed in this paper, and our proposed device-edge-cloud collaborative recognition acceleration method can make full use of the geographically widespread computing resources of devices, edge servers, and cloud clusters. First, we establish a hierarchical collaborative occluded face recognition model, including a lightweight occluded face detection module and a feature-enhanced elastic margin face recognition module, to achieve the accurate localization and precise recognition of occluded faces. Second, considering the responsiveness of occluded face detection services, a context-aware acceleration method is devised for collaborative occluded face recognition to minimize the service delay. Experimental results show that compared with state-of-the-art recognition models, the proposed acceleration method leveraging device-edge-cloud collaborations can effectively reduce the recognition delay by 16% while retaining the equivalent recognition accuracy.
C1 [Zhang, Puning; Huang, Fengyi; Wu, Dapeng; Tan, Lei] Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
   [Zhang, Puning; Huang, Fengyi; Wu, Dapeng; Tan, Lei] Chongqing Educ Commiss China, Chongqing Key Lab Ubiquitous Sensing & Networking, Adv Network & Intelligent Connect Technol Key Lab, Chongqing 400065, Peoples R China.
   [Zhang, Puning] Chongqing Innovat Ctr Ind Big Data Applicat Techno, Chongqing 400707, Peoples R China.
   [Yang, Boran] Chongqing Univ Technol, Sch Artificial Intelligence, Chongqing 401135, Peoples R China.
   [Yang, Zhigang] Chongqing Univ Posts & Telecommun, Sch Cyber Secur & Informat Law, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; Chongqing University
   of Technology; Chongqing University of Posts & Telecommunications
RP Wu, DP (corresponding author), Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
EM zhangpn@cqupt.edu.cn; 1293700309@qq.com; wudp@cqupt.edu.cn;
   bryangphd@163.com; yangzhigang@cqupt.edu.cn; s200131185@stu.cqupt.edu.cn
RI Wu, Dapeng/IWE-0674-2023; yang, boran/KHV-1770-2024
OI Wu, Dapeng/0000-0003-2105-9418; 
FU National Natural Science Foundation of China [61901071, 62271096,
   U20A20157]; Science and Natural Science Foundation of Chongqing, China
   [cstc2020jcyj-zdxmX0024, CSTB2022NSCQ-MSX0600]; University Innovation
   Research Group of Chongqing [CXQT20017]; Program for Innovation Team
   Building at Institutions of Higher Education in Chongqing
   [CXTDX201601020]; Science and Technology Research Program of Chongqing
   Municipal Education Commission [KJQN202000626]; Youth Innovation Group
   Support Program of ICE Discipline of CQUPT [SCIE-QN-2022-04]; Chongqing
   Municipal Technology Innovation and Application Development Special Key
   Project [cstc2020jscx-dxwtBX0053]; China Postdoctoral Science Foundation
   [2022MD723723]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61901071, 62271096, and U20A20157, in
   part by the Science and Natural Science Foundation of Chongqing, China
   under Grant cstc2020jcyj-zdxmX0024 and CSTB2022NSCQ-MSX0600, in part by
   the University Innovation Research Group of Chongqing under Grant
   CXQT20017, in part by the Program for Innovation Team Building at
   Institutions of Higher Education in Chongqing under Grant
   CXTDX201601020, in part by the Science and Technology Re-search Program
   of Chongqing Municipal Education Commission under Grant KJQN202000626,
   in part by Youth Innovation Group Support Program of ICE Discipline of
   CQUPT under Grant SCIE-QN-2022-04, in part by the Chongqing Municipal
   Technology Innovation and Application Development Special Key Project
   under Grant cstc2020jscx-dxwtBX0053, and in part by China Postdoctoral
   Science Foundation under Grant 2022MD723723. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shanshan Zhang.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2018, P EUR C COMP VIS, P3
   [Anonymous], 2021, IEEE INTERNET THINGS, V8, P15929
   [Anonymous], P IEEE CVF INT C COM
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], PROC
   [Anonymous], P IEEE 16 INT C AUT
   [Anonymous], P IEEE CVF C COMP VI
   [Anonymous], 2022, P IEEE INT C IM PROC, P726
   [Anonymous], 2020, P 28 ACM INT C MULT, P2281
   [Anonymous], 2019, P IEEE CVF INT C COM, P773
   [Anonymous], 2016, P IEEE 23 INT C PATT, P2464
   [Anonymous], IEEE T PATTERN ANAL, V42
   [Anonymous], 2019, P IEEE CHIN AUT C, P396, DOI [10.1109/CAC48633.2019.8997456, DOI 10.1109/CAC48633.2019.8997456]
   [Anonymous], 2021, P IEEE 25 INT C PATT, P2272
   [Anonymous], 2022, IEEE NETWORK, V33, P96
   [Anonymous], SMART HLTH, V19
   [Anonymous], ACM SIGPLAN NOTICES, V52, P615
   [Anonymous], P IEEE CHIN AUT C
   [Anonymous], P IEEE CVF C COMP
   [Anonymous], 2021, FAC MASK DET
   [Anonymous], 2021, NEUROCOMPUTING, V506, P146
   [Anonymous], 2020, P IEEE CVF C COMP VI, P1580
   [Anonymous], 2018, P 32 AAAI C ART INT
   [Anonymous], 2022, IEEE T IMAGE PROCESS, V31, P788
   [Anonymous], 2021, P IEEE C COMP COMM
   [Anonymous], 2016, P 14 EUR C COMP VIS, V9905, P21
   [Anonymous], 2021, P IEEE 10 INT C AUT, P01
   emotion recognition under cloud-edge-client collaborations, IEEE J SEL AREA COMM
   Guo J., 2016, IEEE SIGNAL PROCESS, V23, P1499
   Han K.Y., 2019, P IEEE C COMP COMM, P1423, DOI [10.1109/INFOCOM.2019.8737614, DOI 10.1109/INFOCOM.2019.8737614]
   Li CY, 2022, Arxiv, DOI [arXiv:2209.02976, 10.48550/arXiv.2209.02976]
   Liu W., 2017, MACH INTELL, V39, P1137
   Orlin JB, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P765
   Song L., 2021, P IEEE 18 INT C EL E, P785
   Teerapittayanon S., 2013, P ACM S THEOR COMP, P765
   Wang CY, 2022, Arxiv, DOI arXiv:2207.02696
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu DP, 2021, IEEE T MULTIMEDIA, V23, P2208, DOI 10.1109/TMM.2021.3066050
   Zeng L., 2019, P IEEE CVF C COMP VI, P4690
NR 40
TC 1
Z9 1
U1 11
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1513
EP 1520
DI 10.1109/TMM.2023.3240884
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000010
DA 2024-07-18
ER

PT J
AU Zhang, YC
   Dai, WR
   Li, Y
   Li, CL
   Hou, JH
   Zou, JN
   Xiong, HK
AF Zhang, Yuchen
   Dai, Wenrui
   Li, Yong
   Li, Chenglin
   Hou, Junhui
   Zou, Junni
   Xiong, Hongkai
TI Light Field Compression With Graph Learning and Dictionary-Guided Sparse
   Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Light field compression; graph learning; dictionary learning; graph
   adjacency matrix
AB Light field (LF) data are widely used in the immersive representations of the 3D world. To record the light rays along with different directions, an LF requires much larger storage space and transmission bandwidth than a conventional 2D image with similar spatial dimension. In this paper, we propose a novel framework for light field image compression that leverages graph learning and dictionary learning to remove structural redundancies between different views. Specifically, to significantly reduce the bit-rates, only a few key views are sampled and encoded, whereas the remaining non-key views are reconstructed via the graph adjacency matrix learned from the angular patch. Furthermore, dictionary-guided sparse coding is developed to compress the graph adjacency matrices and reduce the coding overheads. To our best knowledge, this paper is the first to achieve compact representation of cross-view structural information via adaptive learning on graphs. Experimental results demonstrate that the proposed framework achieves better performance than the standardized HEVC-based codec.
C1 [Zhang, Yuchen; Li, Chenglin; Xiong, Hongkai] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Dai, Wenrui; Zou, Junni] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Li, Yong] Shanghai Key Lab Aerosp Intelligent Control Techno, Shanghai 201109, Peoples R China.
   [Li, Yong] Shanghai Aerosp Control Technol Inst, Shanghai 201109, Peoples R China.
   [Hou, Junhui] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; City
   University of Hong Kong
RP Dai, WR (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
EM slyzzhangyc@sjtu.edu.cn; daiwenrui@sjtu.edu.cn; yli579@cityu.edu.hk;
   lcl1985@sjtu.edu.cn; jh.hou@cityu.edu.hk; zoujunni@sjtu.edu.cn;
   xionghongkai@sjtu.edu.cn
RI LI, CHENGLIN/JUF-8254-2023
OI Xiong, Hongkai/0000-0003-4552-0029; Hou, Junhui/0000-0003-3431-2021
FU National Natural Science Foundation of China [21211518, 11218121];
   Program of Shanghai Science and Technology Innovation Project [61932022,
   61931023, 61720106001, 61971285, 61871267, 61972256, 62125109,
   T2122024];  [20511100100]
FX The work of Junhui Hou was supported by the General Research Fund under
   Grants 21211518 and 11218121. This work was supported in part by the
   National Natural Science Foundation of China under Grants 61932022,
   61931023, 61720106001, 61971285, 61871267, 61972256, 62125109, and
   T2122024, and in part by the Program of Shanghai Science and Technology
   Innovation Project under Grant 20511100100.
CR Ahmad W, 2017, IEEE IMAGE PROC, P4557, DOI 10.1109/ICIP.2017.8297145
   Ahmad W, 2020, IEEE T IMAGE PROCESS, V29, P4269, DOI 10.1109/TIP.2020.2969087
   Bjontegaard G., 2001, Document VCEG-M33
   Chang CL, 2006, IEEE T IMAGE PROCESS, V15, P793, DOI 10.1109/TIP.2005.863954
   Chao YH, 2017, IEEE IMAGE PROC, P3240, DOI 10.1109/ICIP.2017.8296881
   Chen J, 2018, IEEE T IMAGE PROCESS, V27, P314, DOI 10.1109/TIP.2017.2750413
   Conti C, 2018, IEEE T MULTIMEDIA, V20, P2905, DOI 10.1109/TMM.2018.2825882
   gitlab, JPEG PLEN REF SOFTW
   Helin P, 2017, IEEE J-STSP, V11, P1146, DOI 10.1109/JSTSP.2017.2737967
   hhi, VERSATILE VIDEO CODI
   Hou JH, 2019, IEEE T CIRC SYST VID, V29, P517, DOI 10.1109/TCSVT.2018.2802943
   iwr, HCI LIGHT FIELD ARCH
   Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762
   Jia CM, 2017, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2017.8297148
   Jiang XR, 2017, IEEE J-STSP, V11, P1132, DOI 10.1109/JSTSP.2017.2747078
   Jin X, 2018, IEEE T IMAGE PROCESS, V27, P3954, DOI 10.1109/TIP.2018.2832449
   .jpeg, JPEG PLEN LIGHT FIED
   Kalofolias V, 2016, JMLR WORKSH CONF PRO, V51, P920
   Lafruit G., 2019, SMPTE Motion Imag. J., V128, P33
   Li L, 2020, IEEE T CIRC SYST VID, V30, P2694, DOI 10.1109/TCSVT.2019.2924313
   Li L, 2017, IEEE J-STSP, V11, P1107, DOI 10.1109/JSTSP.2017.2725198
   Li Y, 2016, IEEE T IMAGE PROCESS, V25, P80, DOI 10.1109/TIP.2015.2498406
   Liu D., 2016, ARXIV PREPRINT ARXIV, P1, DOI DOI 10.1109/ICMEW.2016.7574674
   Liu DY, 2020, IEEE T MULTIMEDIA, V22, P846, DOI 10.1109/TMM.2019.2934426
   Magnor M, 2000, IEEE T CIRC SYST VID, V10, P338, DOI 10.1109/76.836278
   Monteiro R, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574670
   Ng R, 2005, RES REPORT, DOI DOI 10.1145/3097571
   Pereira F, 2019, 84 JPEG M BRUSS BELG
   Perra C, 2021, PROC SPIE, V11353, DOI 10.1117/12.2555841
   Rerabek M., 2016, Call for Proposals and Evaluation Procedure
   Rizkallah M, 2021, IEEE T IMAGE PROCESS, V30, P5518, DOI 10.1109/TIP.2021.3085203
   Rizkallah M, 2020, IEEE T IMAGE PROCESS, V29, P3282, DOI 10.1109/TIP.2019.2959215
   Sakamoto T, 2012, 2012 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Schelkens P., 2015, JPEG PLENO SCOPE USE, V1, P1
   Schwarz H, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P1, DOI 10.1109/PCS.2012.6213271
   Shuai Yang, 2018, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1713, DOI 10.1109/ICASSP.2018.8462554
   Su X, 2017, IEEE IMAGE PROC, P4023, DOI 10.1109/ICIP.2017.8297038
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Taubman D.S., 2002, JPEG 2000: Image Compression Fundamentals, Standards and Practice, DOI 10.1007/978-1-4615-0799-4
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Viola I, 2018, PROC SPIE, V10752, DOI 10.1117/12.2322827
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Yun Li, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P539, DOI 10.1109/ICASSP.2014.6853654
   Zhao SY, 2017, IEEE IMAGE PROC, P4562, DOI 10.1109/ICIP.2017.8297146
   Zhong R, 2019, IEEE T CIRC SYST VID, V29, P1116, DOI 10.1109/TCSVT.2018.2826052
NR 45
TC 2
Z9 2
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3059
EP 3072
DI 10.1109/TMM.2022.3154928
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200009
DA 2024-07-18
ER

PT J
AU Zhao, XW
   Liu, XL
   Ma, YQ
   Bai, SH
   Shen, YF
   Hao, ZY
   Liu, AS
AF Zhao, Xiaowei
   Liu, Xianglong
   Ma, Yuqing
   Bai, Shihao
   Shen, Yifan
   Hao, Zeyu
   Liu, Aishan
TI Temporal Speciation Network for Few-Shot Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Training; Object detection; Task analysis; Detectors;
   Measurement; Feature extraction; Few-shot object detection; natural
   evolution; selective recombination module; mutational RPN; evolving
   training
ID ATTENTION
AB Recently, few-shot object detection (FSOD) has become an increasing research focus, which can largely alleviate the heavy dependency on expensive annotations in the traditional object detection task. However, existing FSOD approaches fail to generate sufficient high-quality positive region proposals which are the key to detection performance, due to the lack of informative knowledge from base classes and non-specific alteration for novel classes. To address the problem, this paper presents a simple yet effective few-shot object detection framework referred to as Temporal Speciation Network (TeSNet) with an evolving training, which improves the diversity and rationality of positive proposal generation. Our TeSNet, imitating the natural evolution which relies on inheritation and mutation, correspondingly consists of two key components: a Selective Recombination Module (SRM) for effectively inheriting from base classes and a Mutational Region Proposal Network (MRPN) for flexibly mutating according to the unique traits of novel samples. Specifically, SRM selects and reorganizes relevant base categories, and further instantiates diverse individuals to ensure the diversity of positive proposals. MRPN adapts the parameters trained on base classes aiming for accurately locating positive proposals. Extensive experiments are conducted on several commonly-used datasets, in which our TeSNet achieves state-of-the-art results and outperforms baselines by large margin.
C1 [Zhao, Xiaowei; Liu, Xianglong; Ma, Yuqing; Bai, Shihao; Shen, Yifan; Hao, Zeyu; Liu, Aishan] Beihang Univ, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
C3 Beihang University
RP Ma, YQ (corresponding author), Beihang Univ, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
EM xiaoweizhao@buaa.edu.cn; xlliu@buaa.edu.cn; mayuqing@buaa.edu.cn;
   16061167@buaa.edu.cn; shenyf@buaa.edu.cn; 19373300@buaa.edu.cn;
   liuaishan@buaa.edu.cn
OI Shen, Yifan/0009-0006-9205-3266; , Yuqing/0000-0003-1936-9396
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2017, PROC 31 INT C NEURAL
   Aoxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12573, DOI 10.1109/CVPR42600.2020.01259
   Bin Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P438, DOI 10.1007/978-3-030-58548-8_26
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chen H, 2018, AAAI CONF ARTIF INTE, P2836
   Chen L, 2021, IEEE T MULTIMEDIA, V23, P4297, DOI 10.1109/TMM.2020.3040539
   Das D, 2020, IEEE T IMAGE PROCESS, V29, P3336, DOI 10.1109/TIP.2019.2959254
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Elsken Thomas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12362, DOI 10.1109/CVPR42600.2020.01238
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan ZB, 2021, PROC CVPR IEEE, P4525, DOI 10.1109/CVPR46437.2021.00450
   Flennerhag S., 2020, P 8 INT C LEARN REPR
   Gidaris S, 2019, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2019.00011
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   Guo D, 2020, IEEE T IMAGE PROCESS, V29, P6655, DOI 10.1109/TIP.2020.2992888
   Guo WY, 2021, IEEE T IMAGE PROCESS, V30, P6730, DOI 10.1109/TIP.2021.3097180
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu YF, 2021, IEEE T MULTIMEDIA, V23, P4285, DOI 10.1109/TMM.2020.3039329
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Jamal MA, 2019, PROC CVPR IEEE, P11711, DOI 10.1109/CVPR.2019.01199
   Jiaxi Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P456, DOI 10.1007/978-3-030-58517-4_27
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kang BY, 2019, IEEE I CONF COMP VIS, P8419, DOI 10.1109/ICCV.2019.00851
   Karlinsky L, 2019, PROC CVPR IEEE, P5192, DOI 10.1109/CVPR.2019.00534
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li AX, 2021, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR46437.2021.00311
   Li BH, 2021, PROC CVPR IEEE, P7359, DOI 10.1109/CVPR46437.2021.00728
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Ling Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13387, DOI 10.1109/CVPR42600.2020.01340
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Ma YQ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P804
   Ma YQ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P811
   Phaphuangwittayakul A, 2022, IEEE T MULTIMEDIA, V24, P2205, DOI 10.1109/TMM.2021.3077729
   Ravi S., 2017, C TRACK P, P1
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Snell J, 2017, ADV NEUR IN, V30
   Song XH, 2020, IEEE T IMAGE PROCESS, V29, P525, DOI 10.1109/TIP.2019.2933728
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang Q, 2020, IEEE T IMAGE PROCESS, V29, P7549, DOI 10.1109/TIP.2020.3004249
   Wang X., 2020, P 37 INT C MACH LEAR, P9918
   Wang YX, 2019, IEEE I CONF COMP VIS, P9924, DOI 10.1109/ICCV.2019.01002
   Wertheimer D, 2021, PROC CVPR IEEE, P8008, DOI 10.1109/CVPR46437.2021.00792
   Wu S, 2022, IEEE T MULTIMEDIA, V24, P2058, DOI 10.1109/TMM.2021.3075323
   Wu Y., 2019, Detectron2
   Xu H., 2021, P IEEECVF INT C COMP, P8812
   Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669
   Yan XP, 2019, IEEE I CONF COMP VIS, P9576, DOI 10.1109/ICCV.2019.00967
   Yang Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P192, DOI 10.1007/978-3-030-58520-4_12
   Yang Y., 2020, P ADV NEUR INF PROC, P3521
   Yao HX, 2020, AAAI CONF ARTIF INTE, V34, P6656
   Zhang JC, 2020, IEEE T IMAGE PROCESS, V29, P6209, DOI 10.1109/TIP.2020.2988435
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
   Zhao Z, 2020, IEEE T IMAGE PROCESS, V29, P3859, DOI 10.1109/TIP.2020.2963950
   Zhu CC, 2021, PROC CVPR IEEE, P8778, DOI 10.1109/CVPR46437.2021.00867
   Zhu LC, 2021, IEEE T IMAGE PROCESS, V30, P4253, DOI 10.1109/TIP.2021.3070733
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
NR 63
TC 2
Z9 2
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8267
EP 8278
DI 10.1109/TMM.2023.3234368
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000057
DA 2024-07-18
ER

PT J
AU Adaloglou, N
   Chatzis, T
   Papastratis, I
   Stergioulas, A
   Papadopoulos, GT
   Zacharopoulou, V
   Xydopoulos, GJ
   Atzakas, K
   Papazachariou, D
   Daras, P
AF Adaloglou, Nikolas
   Chatzis, Theocharis
   Papastratis, Ilias
   Stergioulas, Andreas
   Papadopoulos, Georgios Th.
   Zacharopoulou, Vassia
   Xydopoulos, George J.
   Atzakas, Klimnis
   Papazachariou, Dimitris
   Daras, Petros
TI A Comprehensive Study on Deep Learning-Based Methods for Sign Language
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Hidden Markov models; Assistive technology; Task
   analysis; Gesture recognition; Training; Three-dimensional displays;
   Sign Language Recognition; Greek sign language; Deep neural networks;
   stimulated CTC; conditional entropy CTC
ID RECONSTRUCTION
AB In this paper, a comparative experimental assessment of computer vision-based methods for sign language recognition is conducted. By implementing the most recent deep neural network methods in this field, a thorough evaluation on multiple publicly available datasets is performed. The aim of the present study is to provide insights on sign language recognition, focusing on mapping non-segmented video streams to glosses. For this task, two new sequence training criteria, known from the fields of speech and scene text recognition, are introduced. Furthermore, a plethora of pretraining schemes is thoroughly discussed. Finally, a new RGB+D dataset for the Greek sign language is created. To the best of our knowledge, this is the first sign language dataset where three annotation levels are provided (individual gloss, sentence and spoken language) for the same set of video captures.
C1 [Adaloglou, Nikolas; Chatzis, Theocharis; Papastratis, Ilias; Stergioulas, Andreas; Papadopoulos, Georgios Th.; Daras, Petros] Informat Technol Inst, Ctr Res & Technol Hellas, Visual Comp Lab, Therme 60631, Greece.
   [Zacharopoulou, Vassia; Xydopoulos, George J.; Atzakas, Klimnis; Papazachariou, Dimitris] Univ Patras, Patras, Greece.
C3 Centre for Research & Technology Hellas; University of Patras
RP Adaloglou, N (corresponding author), Informat Technol Inst, Ctr Res & Technol Hellas, Visual Comp Lab, Therme 60631, Greece.
EM adaloglou@iti.gr; hatzis@iti.gr; papastrat@iti.gr; andrster@iti.gr;
   papad@iti.gr; vasiazacharopoulou@gmail.com; gjxydo@upatras.gr;
   k.antzakas@upatras.gr; papaz@upatras.gr; daras@iti.gr
RI Xydopoulos, George J./JGE-3122-2023; Daras, Petros/F-5284-2012
OI Xydopoulos, George J./0000-0003-4561-1238; Daras,
   Petros/0000-0003-3814-6710; Adaloglou, Nikolaos/0000-0003-4938-6322;
   Papastratis, Ilias/0000-0003-4664-2626; Stergioulas,
   Andreas/0000-0002-1145-5671; Papadopoulos, Georgios/0000-0003-1686-421X
FU Greek General Secretariat of Research and Technology [T1E.K-02469]
FX This work was supported by the Greek General Secretariat of Research and
   Technology under Contract T1E.K-02469 EPIKOINONO.
CR Alexiadis DS, 2017, IEEE T CIRC SYST VID, V27, P798, DOI 10.1109/TCSVT.2016.2576922
   Alexiadis DS, 2014, IEEE T MULTIMEDIA, V16, P1391, DOI 10.1109/TMM.2014.2317311
   [Anonymous], 2011, Visual Analysis of Humans: Looking at People, DOI DOI 10.1007/978-0-85729-997-027
   [Anonymous], 2008, IEEE INT C AUTOMATIC, DOI [DOI 10.1109/AFGR.2008.4813472, 10.1109/AFGR.2008.4813472]
   Bahdanau D., 2014, 3 INT C LEARN REPR
   Baker Anne, 2016, The linguistics of sign languages: An Introduction.
   Bragg D, 2019, ASSETS'19: THE 21ST INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P16, DOI 10.1145/3308561.3353774
   Camgoz Necati Cihan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10020, DOI 10.1109/CVPR42600.2020.01004
   Camgoz NC, 2017, IEEE I CONF COMP VIS, P3075, DOI 10.1109/ICCV.2017.332
   Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812
   Camgoz NC, 2016, INT C PATT RECOG, P49, DOI 10.1109/ICPR.2016.7899606
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cooper H, 2012, J MACH LEARN RES, V13, P2205
   Cristianini N., 2000, An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods, DOI [10.1017/CBO9780511801389, DOI 10.1017/CBO9780511801389]
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   Cuturi M, 2017, PR MACH LEARN RES, V70
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Emmorey Karen, 2001, SPATIAL SCHEMAS ABST, P148
   Evangelidis GD, 2015, LECT NOTES COMPUT SC, V8925, P595, DOI 10.1007/978-3-319-16178-5_42
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Forster J, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P1911
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heymann J, 2019, INT CONF ACOUST SPEE, P5701, DOI 10.1109/ICASSP.2019.8682700
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang J., 2018, 32 AAAI C ARTIFICIAL, V32
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Joze H. R. V., 2019, P BMVC
   Junfu Pu, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9917, P252, DOI 10.1007/978-3-319-48896-7_25
   Kadous M W., 1996, P WORKSHOP INTEGRATI, P165
   Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077
   Koller O, 2018, INT J COMPUT VISION, V126, P1311, DOI 10.1007/s11263-018-1121-3
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2016, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR.2016.412
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Koller Oscar., 2016, P BMVC, P1, DOI 10.5244/C.30.136
   Konstantinidis D, 2018, IEEE CONF IMAGING SY, P24
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li DX, 2020, IEEE WINT CONF APPL, P1448, DOI [10.1109/WACV45572.2020.9093512, 10.1109/wacv45572.2020.9093512]
   Liu H, 2018, ADV NEUR IN, V31
   Mitchell RE, 2006, SIGN LANG STUD, V6, P306, DOI 10.1353/sls.2006.0019
   Molchanov Pavlo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301342
   Moon TK, 1996, IEEE SIGNAL PROC MAG, V13, P47, DOI 10.1109/79.543975
   Neverova N, 2016, IEEE T PATTERN ANAL, V38, P1692, DOI 10.1109/TPAMI.2015.2461544
   Padden C., 1986, Proceedings of the 4th international symposium on sign language research and teaching, P44
   Papadopoulos GT, 2018, IEEE T CIRC SYST VID, V28, P1807, DOI 10.1109/TCSVT.2016.2643161
   Papastratis I, 2020, IEEE ACCESS, V8, P91170, DOI 10.1109/ACCESS.2020.2993650
   Pu JF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P885
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Ronchetti F., 2016, Congr. Argentine/ Ciencias la Comput
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Sandler W, 2006, SIGN LANGUAGE AND LINGUISTIC UNIVERSALS, P1, DOI 10.2277/ 0521483956
   Tan S, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P617, DOI 10.1109/ASRU.2015.7404853
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang S.B., 2006, P IEEE COMP SOC C CO
   Wu CY, 2018, IEEE-ACM T AUDIO SPE, V26, P256, DOI 10.1109/TASLP.2017.2774919
   Wu CY, 2016, INTERSPEECH, P400, DOI 10.21437/Interspeech.2016-580
   Wu D, 2016, IEEE T PATTERN ANAL, V38, P1583, DOI 10.1109/TPAMI.2016.2537340
   Yang Zhaoyang, 2019, arXiv:1908.01341
   Zhang JH, 2016, IEEE INT CON MULTI, DOI 10.1109/ICME.2016.7552950
   Zhou H, 2019, IEEE INT CON MULTI, P1282, DOI 10.1109/ICME.2019.00223
NR 65
TC 35
Z9 38
U1 3
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1750
EP 1762
DI 10.1109/TMM.2021.3070438
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cai, DS
   Qian, SS
   Fang, Q
   Xu, CS
AF Cai, Desheng
   Qian, Shengsheng
   Fang, Quan
   Xu, Changsheng
TI Heterogeneous Hierarchical Feature Aggregation Network for Personalized
   Micro-Video Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graph neural networks; Task analysis; Semantics; Aggregates; Data
   structures; Collaboration; Visualization; Heterogeneous graph;
   micro-video recommendation; multi-modal
ID INFORMATION
AB Micro-video recommendation has attracted extensive research attention with the increasing popularity of micro-video sharing platforms. Traditional approaches consider micro-video recommendation as a matching task and ignore the rich relationships among users and micro-videos from various modalities (e.g., visual, acoustic, and textual). Recently, GNN-based approaches show promising performance for the micro-video recommendation task. However, they mainly focus on the homogeneous graph which includes only one type of nodes or relations, and cannot be applied to the heterogeneous graph which consists of users, micro-videos, and related multi-modal information. In this paper, a novel Heterogeneous Hierarchical Feature Aggregation Network (HHFAN) is proposed for personalized micro-video recommendation. Our goal is to explore the highly complicated relationship information among users, micro-videos and related multi-modal information from a modality-aware Heterogeneous Information Graph (M-HIG), and thus generate high-quality user and micro-video embeddings for recommendation. The proposed model consists of two key components: (1) In data structure level, we build a heterogeneous graph and utilize a random walk based sampling strategy to sample neighbors for users and micro-videos. (2) In representation learning level, we design a hierarchical feature aggregation network including the intra- and inter-type feature aggregation networks to better capture the complex structure and rich semantic information in the heterogeneous graph. We evaluate our method on two real-world datasets and the results demonstrate that the proposed model outperforms the baseline methods.
C1 [Cai, Desheng] Hefei Univ Technol, Hefei 230009, Peoples R China.
   [Qian, Shengsheng; Fang, Quan; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Qian, Shengsheng; Fang, Quan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Hefei University of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM caidsml@gmail.com; shengsheng.qian@nlpria.ac.cn; qfang@nlpr.ia.ac.cn;
   csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023
OI xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2017YFB1002804];
   National Natural Science Foundation of China [62036012, 6207072426,
   61720106006, 61572503, 61802405, 61872424, 61702509, 61832002, 61936005,
   U1705262]; Key Research Program of Frontier Sciences, CAS
   [QYZDJ-SSWJSC039]; K.C. Wong Education Foundation; CCF-Tencent Open
   Fund; Open Research Projects of Zhejiang Laboratory [2021KE0AB05]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFB1002804, the National
   Natural Science Foundation of China under Grants 62036012, 6207072426,
   61720106006, 61572503, 61802405, 61872424, 61702509, 61832002, 61936005
   and U1705262, in part by the Key Research Program of Frontier Sciences,
   CAS, under Grant QYZDJ-SSWJSC039, and the K.C. Wong Education
   Foundation, in part by CCF-Tencent Open Fund, and in part by the Open
   Research Projects of Zhejiang Laboratory under Grant 2021KE0AB05.
CR [Anonymous], 2017, PAC RIM C MULT
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen XS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1146, DOI 10.1145/3240508.3240617
   Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190
   Ferracani A, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P351, DOI 10.1145/2911996.2912066
   Guo HF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1725
   Hamilton WL, 2017, ADV NEUR IN, V30
   Han TT, 2017, IEEE T MULTIMEDIA, V19, P712, DOI 10.1109/TMM.2016.2631881
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   He XN, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P549, DOI 10.1145/2911451.2911489
   Hu BB, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1531, DOI 10.1145/3219819.3219965
   Huang QH, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2598779
   Huang YX, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P35, DOI 10.1145/2882903.2903743
   Khoshgoftaar T.M., 2009, ADV ARTIFICIAL INTEL, DOI [10.1155/2009/421425, DOI 10.1155/2009/421425]
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Koenigstein N., 2013, 7th ACM Conf. on Rec. Systems, P129, DOI DOI 10.1145/2507157.2507168
   Liu S, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3020, DOI 10.1145/3308558.3313513
   Ma JW, 2018, MULTIMED TOOLS APPL, V77, P2991, DOI 10.1007/s11042-017-4827-2
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Le Q, 2014, PR MACH LEARN RES, V32, P1188
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Rendle S, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168771
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shepstone SE, 2014, IEEE T MULTIMEDIA, V16, P1999, DOI 10.1109/TMM.2014.2337845
   Shi C, 2019, WORLD WIDE WEB, V22, P153, DOI 10.1007/s11280-018-0553-6
   Shi C, 2019, IEEE T KNOWL DATA EN, V31, P357, DOI 10.1109/TKDE.2018.2833443
   Shi C, 2016, KNOWL INF SYST, V49, P835, DOI 10.1007/s10115-016-0925-0
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic P., 2018, 6 INT C LEARN REPR C
   Wang X, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2022, DOI 10.1145/3308558.3313562
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Xu J, 2018, ACM/SIGIR PROCEEDINGS 2018, P1365
   Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890
   Zhang CX, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P793, DOI 10.1145/3292500.3330961
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zheng J., 2017, INT J DATA SCI ANAL, V3, P35, DOI [DOI 10.1007/S41060-016-0031-0, 10.1007/s41060-016-0031-0]
   Zhou P, 2016, IEEE T MULTIMEDIA, V18, P1217, DOI 10.1109/TMM.2016.2537216
NR 42
TC 25
Z9 26
U1 3
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 805
EP 818
DI 10.1109/TMM.2021.3059508
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100023
DA 2024-07-18
ER

PT J
AU Deng, CF
   Wang, MM
   Liu, L
   Liu, Y
   Jiang, YL
AF Deng, Chunfang
   Wang, Mengmeng
   Liu, Liang
   Liu, Yong
   Jiang, Yunliang
TI Extended Feature Pyramid Network for Small Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Object detection; Detectors; Semantics;
   Superresolution; Signal resolution; Pipelines; deep learning; feature
   pyramid; feature super-resolution; knowledge distillation; Small object
   detection
AB Small object detection remains an unsolved challenge because it is hard to extract the information of small objects with only a few pixels. While scale-level corresponding detection in feature pyramid network alleviates this problem, we find feature coupling of various scales still impairs the performance of small objects. In this paper, we propose an extended feature pyramid network (EFPN) with an extra high-resolution pyramid level specialized for small object detection. Specifically, we design a novel module, named feature texture transfer (FTT), which is used to super-resolve features and extract credible regional details simultaneously. Moreover, we introduce a cross resolution distillation mechanism to transfer the ability of perceiving details across the scales of the network, where a foreground-background-balanced loss function is designed to alleviate area imbalance of foreground and background. In our experiments, the proposed EFPN is efficient on both computation and memory, and yields state-of-the-art results on small traffic-sign dataset Tsinghua-Tencent 100 K and small category of general object detection dataset MS COCO.
C1 [Deng, Chunfang; Wang, Mengmeng; Liu, Liang; Liu, Yong] Zhejiang Univ, Adv Percept Robot & Intelligent Learning Lab, Coll Control Sci & Engn, Hangzhou 310027, Peoples R China.
   [Jiang, Yunliang] Huzhou Univ, Huzhou 313000, Peoples R China.
C3 Zhejiang University; Huzhou University
RP Wang, MM; Liu, Y (corresponding author), Zhejiang Univ, Adv Percept Robot & Intelligent Learning Lab, Coll Control Sci & Engn, Hangzhou 310027, Peoples R China.
EM dengcf@zju.edu.cn; mengmengwang@zju.edu.cn; leonliuz@zju.edu.cn;
   yongliu@iipc.zju.edu.cn; jyl@zjhu.edu.cn
RI Wang, Mengmeng/AFG-9401-2022
OI Liu, Liang/0000-0001-7910-810X
FU National Natural Science Foundation of China [61836015]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61836015.
CR Bai YC, 2018, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2018.00010
   Bai YC, 2018, LECT NOTES COMPUT SC, V11217, P210, DOI 10.1007/978-3-030-01261-8_13
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Girshick Ross, 2018, Detectron
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Haris Muhammad, 2018, ARXIV180311316
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li L, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17051062
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI [10.1007/978-3-030-01240-3_21, 10.1007/978-3-030-01219-9_23]
   Liang ZW, 2018, LECT NOTES COMPUT SC, V11166, P554, DOI 10.1007/978-3-030-00764-5_51
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Noh J, 2019, IEEE I CONF COMP VIS, P9724, DOI 10.1109/ICCV.2019.00982
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yu F., 2015, ARXIV
   Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259
   Zheng HT, 2018, LECT NOTES COMPUT SC, V11210, P87, DOI 10.1007/978-3-030-01231-1_6
   Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062
   Zhou X, 2019, PSYCHOL HEALTH, V34, P811, DOI 10.1080/08870446.2019.1574348
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu Z, 2016, PROC CVPR IEEE, P2110, DOI 10.1109/CVPR.2016.232
NR 44
TC 165
Z9 180
U1 51
U2 374
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1968
EP 1979
DI 10.1109/TMM.2021.3074273
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200015
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Li, L
   Li, Z
   Liu, S
   Li, HQ
AF Li, Li
   Li, Zhu
   Liu, Shan
   Li, Houqiang
TI Motion Estimation and Coding Structure for Inter-Prediction of LiDAR
   Point Cloud Geometry
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hierarchical coding structure; inter-prediction; LiDAR point cloud
   geometry compression; motion compensation; motion estimation
ID COMPRESSION
AB In this paper, we investigate two fundamental problems of inter-prediction for Light Detection and Ranging (LiDAR) point cloud geometries: motion estimation (ME) and coding structure under the inter-exploration model of geometry-based point cloud compression (G-PCC). Under the inter-exploration model of G-PCC, the key to a good ME algorithm is to design an accurate criterion for estimating the bit cost of an octree node. In the previous work, a logarithmic relationship between the prediction distortion and the bit cost was used as the criterion. We first note that the multiscale binary prediction residue, instead of the prediction distortion, is the key factor in determining the bit cost. Then, a linear relationship between the number of 1s and 0s in the multiscale binary prediction residue and the bit cost is built and used as the ME criterion. In terms of the coding structure, only the IPPP coding structure is investigated in all previous geometry inter-prediction algorithms. The use of the hierarchical coding structure is first investigated in this paper. We further propose determining the use of the IPPP or hierarchical coding structure at the group of pictures (GoP)-level based on rate distortion optimization to improve the performance. The proposed algorithms are implemented in the inter-exploration model of G-PCC. The experimental results show that compared with the inter-exploration model of G-PCC, the proposed algorithms can provide an average of 2.1% bitrate savings.
C1 [Li, Li; Li, Houqiang] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei, Peoples R China.
   [Li, Zhu] Univ Missouri, Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
   [Liu, Shan] Tencent Amer LLC, Palo Alto, CA 94301 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Missouri System; University of Missouri Kansas
   City
RP Li, L (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei, Peoples R China.
EM lil1@ustc.edu.cn; lizhu@umkc.edu; shanl@tencent.com; lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013; huang, shan/JVN-1240-2024; Li,
   Zhu/AAD-8182-2021
OI Li, Zhu/0000-0002-8246-177X; , Shan/0000-0002-1442-1207
FU National Science Foundation of China [62021001]; USTC Research Funds of
   the Double First-Class Initiative [YD3490002001]
FX This work was supported in part by the National Science Foundation of
   China under Grant 62021001 and in part by USTC Research Funds of the
   Double First-Class Initiative under Grant YD3490002001.
CR [Anonymous], WIKIPEDIA
   Bjontegaard G., 2001, Document VCEG-M33
   Botsch M., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P53
   Bruder G, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P161, DOI 10.1109/3DUI.2014.6798870
   Budagavi M., 2017, JTC1SC29WG11M41808 I
   d'Eon E., 2017, JTC1SC29WG11M40059 I
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3886, DOI 10.1109/TIP.2017.2707807
   He LY, 2017, ASIA-PAC CONF COMMUN, P345
   Huang LL, 2020, PROC CVPR IEEE, P1310, DOI 10.1109/CVPR42600.2020.00139
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Kathariya B, 2018, IEEE INT CON MULTI
   Lasserre S., 2017, JTC1SC29WG11M41822 I
   Lasserre S., 2018, JTC1SC29WG11M44754 I
   Lasserre S., 2018, M42238 ISOIEC JTC1SC
   Lasserre S., 2018, M43600 ISOIEC JTC1SC
   Mammou K., 2017, document ISO/IEC JTC1/SC29/WG11 m41649
   Mammou K., 2020, JTC1SC29WG11M53618 I
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Park SB, 2009, IEEE T MULTIMEDIA, V11, P177, DOI 10.1109/TMM.2008.2008868
   Peng JL, 2005, ACM T GRAPHIC, V24, P609, DOI 10.1145/1073204.1073237
   pointcloudtechnology, LIGHT DET RANG US CA
   Preda M., 2017, JTC1SC29WG11W17251 I
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Schnabel R., 2006, P S POINT BAS GRAPH, V6, P111, DOI DOI 10.2312/SPBG/SPBG06/111-120
   Schwarz H, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1929, DOI 10.1109/ICME.2006.262934
   Schwarz S., 2017, JTC1SC29WG11M41779 I
   Schwarz S., 2018, ISO/IEC JTC1/SC29/WG11
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Sjöberg R, 2012, IEEE T CIRC SYST VID, V22, P1858, DOI 10.1109/TCSVT.2012.2223052
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506
   Tulvan C., 2016, JTC1SC29WG11 ISO IEC
   Zhang X, 2020, IEEE DATA COMPR CONF, P73, DOI 10.1109/DCC47342.2020.00015
   Zhu WJ, 2017, IEEE INT WORKSH MULT
NR 34
TC 4
Z9 5
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4504
EP 4513
DI 10.1109/TMM.2021.3119872
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 7B6NT
UT WOS:000899248400005
DA 2024-07-18
ER

PT J
AU Ma, YJ
   Shuai, HH
   Cheng, WH
AF Ma, Yu-Jen
   Shuai, Hong-Han
   Cheng, Wen-Huang
TI Spatiotemporal Dilated Convolution With Uncertain Matching for
   Video-Based Crowd Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Convolution; Training; Spatiotemporal phenomena;
   Annotations; Three-dimensional displays; Videos; Crowd counting; density
   map regression; dilated convolution; patch-wise regression loss;
   spatiotemporal modeling
ID PEOPLE; MOTION
AB In this paper, we propose a novel SpatioTemporal convolutional Dense Network (STDNet) to address the video-based crowd counting problem, which contains the decomposition of 3D convolution and the 3D spatiotemporal dilated dense convolution to alleviate the rapid growth of the model size caused by the Conv3D layer. Moreover, since the dilated convolution extracts the multiscale features, we combine the dilated convolution with the channel attention block to enhance the feature representations. Due to the error that occurs from the difficulty of labeling crowds, especially for videos, imprecise or standard-inconsistent labels may lead to poor convergence for the model. To address this issue, we further propose a new patch-wise regression loss (PRL) to improve the original pixel-wise loss. Experimental results on three video-based benchmarks, i.e., the UCSD, Mall and WorldExpo'10 datasets, show that STDNet outperforms both image- and video-based state-of-the-art methods. The source codes are released at https://github.com/STDNet/STDNet.
C1 [Ma, Yu-Jen; Shuai, Hong-Han] Natl Chiao Tung Univ, Dept Elect & Comp Engn, Hsinchu 30010, Taiwan.
   [Cheng, Wen-Huang] Natl Chiao Tung Univ, Inst Elect, Hsinchu 30010, Taiwan.
   [Cheng, Wen-Huang] Natl Chung Hsing Univ, Artificial Intelligence & Data Sci Program, Taichung 400, Taiwan.
C3 National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung
   University; National Chung Hsing University
RP Shuai, HH (corresponding author), Natl Chiao Tung Univ, Dept Elect & Comp Engn, Hsinchu 30010, Taiwan.
EM ninechi.eed08g@nctu.edu.tw; hhshuai@nctu.edu.tw; whcheng@nctu.edu.tw
OI Shuai, Hong-Han/0000-0003-2216-077X
FU Ministry of Science and Technology of Taiwan
   [MOST-109-2221-E-009-114MY3, MOST-109-2634-F-009-018,
   MOST-109-2221-E-001-015, MOST-1082218-E-002-055,
   MOST-109-2223-E-009-002-MY3, MOST-109-2218-E-009025,
   MOST-109-2218-E-002-015]
FX This work was supported by the Ministry of Science and Technology of
   Taiwan under Grants MOST-109-2221-E-009-114MY3, MOST-109-2634-F-009-018,
   MOST-109-2221-E-001-015, MOST-1082218-E-002-055,
   MOST-109-2223-E-009-002-MY3, MOST-109-2218-E-009025, and
   MOST-109-2218-E-002-015.
CR [Anonymous], 2006, COMPUTER VISION PATT, DOI 10.1109/CVPR.2006.92
   Basalamah Anas, 2016, US Patent, Patent No. [9,401,086, 9401086]
   Brostow G.J., 2006, CVPR, P594, DOI DOI 10.1109/CVPR.2006.320
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569
   Chan AB, 2009, IEEE I CONF COMP VIS, P545, DOI 10.1109/ICCV.2009.5459191
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625
   Cho SY, 1999, IEEE T SYST MAN CY B, V29, P535, DOI 10.1109/3477.775269
   Dai F, 2019, ARXIV190609707
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Fang YY, 2019, IEEE INT CON MULTI, P814, DOI 10.1109/ICME.2019.00145
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsieh D.-Y, 2020, P AAAI WORKSH ART IN, P1948
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Ihaddadene N, 2008, INT C PATT RECOG, P217
   Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumagai S, 2018, MACH VISION APPL, V29, P1119, DOI 10.1007/s00138-018-0955-6
   Lai WC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P202, DOI 10.1145/3394171.3413602
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Lin YX, 2019, IEEE IMAGE PROC, P1870, DOI [10.1109/icip.2019.8803192, 10.1109/ICIP.2019.8803192]
   Lin YX, 2019, IEEE INT CON MULTI, P218, DOI 10.1109/ICME.2019.00046
   Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545
   Liu LB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P849
   Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Liu XL, 2018, PROC CVPR IEEE, P7661, DOI 10.1109/CVPR.2018.00799
   Liu YT, 2019, PROC CVPR IEEE, P6462, DOI 10.1109/CVPR.2019.00663
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Miao Yunqi, 2020, AAAI CONF ARTIF INTE, P11765
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ranjan V, 2018, LECT NOTES COMPUT SC, V11211, P278, DOI 10.1007/978-3-030-01234-2_17
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren WH, 2018, PROC CVPR IEEE, P5353, DOI 10.1109/CVPR.2018.00561
   Sam DB, 2021, IEEE T PATTERN ANAL, V43, P2739, DOI 10.1109/TPAMI.2020.2974830
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Sanchez-Riera J, 2016, PATTERN RECOGN LETT, V73, P1, DOI 10.1016/j.patrec.2015.12.006
   Saxena S, 2008, LECT NOTES COMPUT SC, V5259, P970, DOI 10.1007/978-3-540-88458-3_88
   Shen IC, 2015, IEEE T MULTIMEDIA, V17, P526, DOI 10.1109/TMM.2015.2405350
   Shi MJ, 2019, PROC CVPR IEEE, P7271, DOI 10.1109/CVPR.2019.00745
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sindagi VA, 2020, IEEE T IMAGE PROCESS, V29, P323, DOI 10.1109/TIP.2019.2928634
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Sio C. H., 2019, P ACM MULT AS, P1
   Sio CH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1948, DOI 10.1145/3394171.3413611
   Subburaman VB, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P470, DOI 10.1109/AVSS.2012.87
   Tian YK, 2020, IEEE T IMAGE PROCESS, V29, P2714, DOI 10.1109/TIP.2019.2952083
   Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8
   Walach E, 2016, LECT NOTES COMPUT SC, V9906, P660, DOI 10.1007/978-3-319-46475-6_41
   Wu QY, 2020, IEEE IMAGE PROC, P1966, DOI [10.1109/ICIP40778.2020.9190701, 10.1109/icip40778.2020.9190701]
   Xia ZX, 2021, IEEE IND ELECTRON M, V15, P6, DOI 10.1109/MIE.2020.2970790
   Xiong F, 2017, IEEE I CONF COMP VIS, P5161, DOI 10.1109/ICCV.2017.551
   Xu ML, 2018, IEEE T CIRC SYST VID, V28, P2814, DOI 10.1109/TCSVT.2017.2731866
   Yan ZY, 2019, IEEE I CONF COMP VIS, P952, DOI 10.1109/ICCV.2019.00104
   Yang YF, 2020, PROC CVPR IEEE, P4373, DOI 10.1109/CVPR42600.2020.00443
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang C, 2016, IEEE T MULTIMEDIA, V18, P1048, DOI 10.1109/TMM.2016.2542585
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhang ZX, 2015, NEUROCOMPUTING, V166, P151, DOI 10.1016/j.neucom.2015.03.083
   Zou Z., 2019, BMVC, P250
   Zou ZK, 2020, FRONT ARTIF INTEL AP, V325, P2864, DOI 10.3233/FAIA200429
NR 69
TC 25
Z9 26
U1 2
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 261
EP 273
DI 10.1109/TMM.2021.3050059
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Mai, SJ
   Hu, HF
   Xing, SL
AF Mai, Sijie
   Hu, Haifeng
   Xing, Songlong
TI A Unimodal Representation Learning and Recurrent Decomposition Fusion
   Structure for Utterance-Level Multimodal Embedding Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal utterance embedding; unimodal representation learning; intra-
   and inter-modality dynamics; recurrent decomposition fusion network
ID SENTIMENT ANALYSIS; SPEECH
AB Learning a unified embedding for utterance-level video attracts significant attention recently due to the rapid development of social media and its broad applications. An utterance normally contains not only spoken language but also the nonverbal behaviors such as facial expressions and vocal patterns. Instead of directly learning utterance embedding based on low-level features, we firstly explore high-level representation for each modality separately via an unimodal representation learning gyroscope structure. In this way, the learnt unimodal representations are more representative and contain more abstract semantic information. In the gyroscope structure, we introduce multi-scale kernel learning, 'channel expansion' and 'channel fusion' operations to explore high-level features both spatially and channelwise. Another insight of' our method lies in that we fuse representations of all modalities to obtain a unified embedding by interpreting fusion procedure as the flow of intermodality information between various modalities, which is more specialized in terms of the information to he fused and the fusion process. Specifically, considering that each modality carries modality-specific and cross-modality interactions, we innovate to decompose unimodal representations into intra- and inter-modality dynamics using gating mechanism, and further fuse the intermodality dynamics by passing them from previous modalities to the following one using a recurrent neural fusion architecture. Extensive experiments demonstrate that our method achieves state-of-the-art performance on multiple benchmark datasets.
C1 [Mai, Sijie; Hu, Haifeng; Xing, Songlong] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
EM maisj@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn;
   xingslong@mail2.sysu.edu.cn
OI Xing, Songlong/0000-0002-2734-1695; Hu, Haifeng/0000-0002-4884-323X;
   Mai, Sijie/0000-0001-9763-375X
FU National Natural Science Foundation of China [62076262, 61673402,
   61273270, 60802069]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62076262, 61673402, 61273270, and 60802069. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. David Crandall.
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Barezi EJ, 2019, 4TH WORKSHOP ON REPRESENTATION LEARNING FOR NLP (REPL4NLP-2019), P260
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Castro S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4619
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI 10.1145/3136755.3136801
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Clevert D.-A., 2016, PREPRINT
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Devlin J., 2018, BERT PRE TRAINING DE
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Eyben Florian, 2010, P 18 ACM INT C MULT, P1459
   GOUDREAU MW, 1994, IEEE T NEURAL NETWOR, V5, P511, DOI 10.1109/72.286928
   Gu Y, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P537, DOI 10.1145/3240508.3240714
   Gu Y, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2225
   Guo WZ, 2019, IEEE ACCESS, V7, P63373, DOI 10.1109/ACCESS.2019.2916887
   Hasan MK, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2046
   Hazarika Devamanyu, 2018, Proc Conf, V2018, P2122, DOI 10.18653/v1/n18-1193
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou M, 2019, ADV NEUR IN, V32
   Hu H., 2020, ARXIV201113572
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kampman O, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P606
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liang PP, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P150
   Liang PP, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P2599
   Liang PP, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1569
   Liang PP, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P472, DOI 10.1145/3242969.3243019
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Lipton Z. C., 2015, ARXIV
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Mai S., 2020, IEEE T AFFECT COMPUT, P1
   Mai SJ, 2021, IEEE-ACM T AUDIO SPE, V29, P1424, DOI 10.1109/TASLP.2021.3068598
   Mai SJ, 2020, IEEE T MULTIMEDIA, V22, P122, DOI 10.1109/TMM.2019.2925966
   Mai SJ, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P481
   Mai SJ, 2019, COMPUT VIS IMAGE UND, V187, DOI 10.1016/j.cviu.2019.07.001
   Majumder N, 2018, KNOWL-BASED SYST, V161, P124, DOI 10.1016/j.knosys.2018.07.041
   Mehta S, 2019, PROC CVPR IEEE, P9182, DOI 10.1109/CVPR.2019.00941
   Morency L.-P., 2011, P 13 INT C MULT INT, P169, DOI DOI 10.1145/2070481.2070509
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   OLSON DR, 1977, HARVARD EDUC REV, V47, P257, DOI 10.17763/haer.47.3.8840364413869005
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Pham H., 2018, P ASS COMP LING GRAN
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Poria S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P527
   Poria S, 2017, IEEE DATA MINING, P1033, DOI 10.1109/ICDM.2017.134
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.178, 10.1109/ICDM.2016.0055]
   Rajagopalan SS, 2016, LECT NOTES COMPUT SC, V9911, P338, DOI 10.1007/978-3-319-46478-7_21
   Rosas VP, 2013, IEEE INTELL SYST, V28, P38, DOI 10.1109/MIS.2013.9
   Rozgic V, 2012, ASIAPAC SIGN INFO PR
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Tsai YH, 2019, IEEE IJCNN, DOI [10.1109/ijcnn.2019.8852023, 10.1109/ipcon.2019.8908433]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wöllmer M, 2013, IEEE INTELL SYST, V28, P46, DOI 10.1109/MIS.2013.34
   Wu CH, 2011, IEEE T AFFECT COMPUT, V2, P10, DOI 10.1109/T-AFFC.2010.16
   Yuan J, 2008, ACOUSTICAL SOC AM J, V123
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zadeh A, 2019, PROC CVPR IEEE, P8799, DOI 10.1109/CVPR.2019.00901
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh Amir, 2018, Proc AAAI Conf Artif Intell, V2018, P5642
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zhang B, 2020, IEEE T PATTERN ANAL, V42, P154, DOI 10.1109/TPAMI.2018.2876404
   Zhang Y., 2019, ARXIV190601004
NR 74
TC 11
Z9 12
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2488
EP 2501
DI 10.1109/TMM.2021.3082398
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600020
DA 2024-07-18
ER

PT J
AU Nguyen, D
   Nguyen, DT
   Zeng, R
   Nguyen, TT
   Tran, SN
   Nguyen, T
   Sridharan, S
   Fookes, C
AF Nguyen, Dung
   Nguyen, Duc Thanh
   Zeng, Rui
   Nguyen, Thanh Thi
   Tran, Son N.
   Nguyen, Thin
   Sridharan, Sridha
   Fookes, Clinton
TI Deep Auto-Encoders With Sequential Learning for Multimodal Dimensional
   Emotion Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; Feature extraction; Long short term memory;
   Visualization; Streaming media; Convolution; Two dimensional displays;
   Auto-encoder; dimensional emotion recognition; long short term memory;
   multimodal emotion recognition
ID EXPRESSION RECOGNITION
AB Multimodal dimensional emotion recognition has drawn a great attention from the affective computing community and numerous schemes have been extensively investigated, making a significant progress in this area. However, several questions still remain unanswered for most of existing approaches including: (i) how to simultaneously learn compact yet representative features from multimodal data, (ii) how to effectively capture complementary features from multimodal streams, and (iii) how to perform all the tasks in an end-to-end manner. To address these challenges, in this paper, we propose a novel deep neural network architecture consisting of a two-stream auto-encoder and a long short term memory for effectively integrating visual and audio signal streams for emotion recognition. To validate the robustness of our proposed architecture, we carry out extensive experiments on the multimodal emotion in the wild dataset: RECOLA. Experimental results show that the proposed method achieves state-of-the-art recognition performance.
C1 [Nguyen, Dung; Nguyen, Duc Thanh; Nguyen, Thanh Thi] Deakin Univ, Sch Informat Technol, Geelong, Vic 3216, Australia.
   [Nguyen, Dung; Nguyen, Duc Thanh] Monash Univ, Fac Informat Technol, Clayton, Vic 3216, Australia.
   [Zeng, Rui] Univ Sydney, Brain & Mind Ctr, Sydney, NSW 2006, Australia.
   [Tran, Son N.] Univ Tasmania, Informat & Commun Technol, Hobart, Tas 7005, Australia.
   [Nguyen, Thin] Deakin Univ, Appl Artificial Intelligence Inst, Geelong, Vic 3216, Australia.
   [Sridharan, Sridha] Queensland Univ Technol, Speech Audio Image & Video Technol Res Lab, Brisbane, Qld 4000, Australia.
   [Fookes, Clinton] Queensland Univ Technol, Brisbane, Qld, Australia.
C3 Deakin University; Monash University; University of Sydney; University
   of Tasmania; Deakin University; Queensland University of Technology
   (QUT); Queensland University of Technology (QUT)
RP Nguyen, D; Nguyen, DT (corresponding author), Deakin Univ, Sch Informat Technol, Geelong, Vic 3216, Australia.
EM dzung.nguyen@deakin.edu.au; duc.nguyen@deakin.edu.au;
   rui.zeng@sydney.edu.au; thanh.nguyen@deakin.edu.au; sn.tran@utas.edu.au;
   thin.nguyen@deakin.edu.au; s.sridharan@qut.edu.au
RI Nguyen, Thin/IXD-7832-2023; Fookes, Clinton/I-9786-2012
OI Nguyen, Thin/0000-0003-3467-8963; Nguyen, Thanh Thi/0000-0001-9709-1663;
   Sridharan, Sridha/0000-0003-4316-9001; Fookes,
   Clinton/0000-0002-8515-6324; Tran, Son/0000-0002-5912-293X
CR [Anonymous], 1997, NEURAL COMPUT
   Barros P, 2016, ADAPT BEHAV, V24, P373, DOI 10.1177/1059712316664017
   Barros P, 2015, IEEE-RAS INT C HUMAN, P582, DOI 10.1109/HUMANOIDS.2015.7363421
   Chen S., 2015, AVEC 2015-Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, co-Located with MM 2015, P49, DOI DOI 10.1145/2808196.2811638
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Graves A, 2012, STUD COMPUT INTELL, V385, P5
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Kalchbrenner N., 2015, arXiv preprint arXiv:1507.01526
   Kollias D., 2019, PROC BRIT MACH VIS C, P1
   KOLLIAS D, 2019, ARXIV191001417
   Kollias D, 2019, INT J COMPUT VISION, V127, P907, DOI 10.1007/s11263-019-01158-4
   Kollias D, 2017, IEEE COMPUT SOC CONF, P1972, DOI 10.1109/CVPRW.2017.247
   Kollias Dimitrios, 2018, ARXIV181107770
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Liang PP, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P472, DOI 10.1145/3242969.3243019
   LIN LI, 1989, BIOMETRICS, V45, P255, DOI 10.2307/2532051
   Mao XJ, 2016, ADV NEUR IN, V29
   Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Ong DC, 2021, IEEE T AFFECT COMPUT, V12, P579, DOI [10.1109/TAFFC.2019.2955949, 10.1109/taffc.2019.2955949]
   Pei EC, 2015, INT CONF AFFECT, P208, DOI 10.1109/ACII.2015.7344573
   Ringeval F, 2013, IEEE INT CONF AUTOMA
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, 14091556 ARXIV
   Tzirakis P, 2017, IEEE J-STSP, V11, P1301, DOI 10.1109/JSTSP.2017.2764438
   Wöllmer M, 2013, IMAGE VISION COMPUT, V31, P153, DOI 10.1016/j.imavis.2012.03.001
   Yoon S, 2018, IEEE W SP LANG TECH, P112, DOI 10.1109/SLT.2018.8639583
   Zafeiriou S, 2017, IEEE COMPUT SOC CONF, P1980, DOI 10.1109/CVPRW.2017.248
   Zhang SQ, 2018, IEEE T CIRC SYST VID, V28, P3030, DOI 10.1109/TCSVT.2017.2719043
   Zhang W, 2020, IEEE T MULTIMEDIA, V22, P515, DOI 10.1109/TMM.2019.2928998
   Zhang ZX, 2019, IEEE T MULTIMEDIA, V21, P1289, DOI 10.1109/TMM.2018.2871949
   Zhang ZX, 2016, INTERSPEECH, P3593, DOI 10.21437/Interspeech.2016-998
   Zheng WL, 2019, IEEE T CYBERNETICS, V49, P1110, DOI 10.1109/TCYB.2018.2797176
NR 34
TC 20
Z9 20
U1 2
U2 49
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1313
EP 1324
DI 10.1109/TMM.2021.3063612
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200006
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Shirian, A
   Tripathi, S
   Guha, T
AF Shirian, Amir
   Tripathi, Subarna
   Guha, Tanaya
TI Dynamic Emotion Modeling With Learnable Graphs and Graph Inception
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; Videos; Dynamics; Convolution; Databases; Task
   analysis; Speech recognition; Emotion recognition; graph learning; graph
   neural network; inception network
ID RECOGNITION; FUSION
AB Human emotion is expressed, perceived and captured using a variety of dynamic data modalities, such as speech (verbal), videos (facial expressions) and motion sensors (body gestures). We propose a generalized approach to emotion recognition that can adapt across modalities by modeling dynamic data as structured graphs. The motivation behind the graph approach is to build compact models without compromising on performance. To alleviate the problem of optimal graph construction, we cast this as a joint graph learning and classification task. To this end, we present the Learnable Graph Inception Network (L-GrIN) that jointly learns to recognize emotion and to identify the underlying graph structure in the dynamic data. Our architecture comprises multiple novel components: a new graph convolution operation, a graph inception layer, learnable adjacency, and a learnable pooling function that yields a graph-level embedding. We evaluate the proposed architecture on five benchmark emotion recognition databases spanning three different modalities (video, audio, motion capture), where each database captures one of the following emotional cues: facial expressions, speech and body gestures. We achieve state-of-the-art performance on all five databases outperforming several competitive baselines and relevant existing methods. Our graph architecture shows superior performance with significantly fewer parameters (compared to convolutional or recurrent neural networks) promising its applicability to resource-constrained devices. Our code is available at https://github.com/AmirSh15/graph_emotion_recognition.
C1 [Shirian, Amir; Guha, Tanaya] Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England.
   [Tripathi, Subarna] Intel Labs, San Diego, CA 92131 USA.
C3 University of Warwick; Intel Corporation
RP Guha, T (corresponding author), Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England.
EM Amir.Shirian@warwick.ac.uk; subarna.tripathi@intel.com;
   tanaya.guha@warwick.ac.uk
RI Shirian, Amir/KFB-6445-2024
OI Shirian, Amir/0000-0001-8948-5216; Tripathi, Subarna/0000-0002-2757-4923
CR Bhattacharya U., 2019, ARXIV191012906
   Bruna J., 2014, P INT C LEARN REPR
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Cheng YR, 2019, PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2019), DOI 10.1145/3331453.3361665
   Cordel MO, 2019, PROC CVPR IEEE, P4021, DOI 10.1109/CVPR.2019.00415
   Crenn A., 2017, P 19 ACM INT C MULT, P15
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Defferrard M, 2016, ADV NEUR IN, V29
   Eyben F., 2009, 3 INT C AFF COMP INT, P1, DOI DOI 10.1109/ACII.2009.5349350
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Ghaleb E, 2019, INT CONF AFFECT, DOI [10.1109/acii.2019.8925444, 10.1109/ACII.2019.8925444]
   Ghosh S, 2016, INTERSPEECH, P3603, DOI 10.21437/Interspeech.2016-692
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Gu JW, 2017, PROC CVPR IEEE, P1531, DOI 10.1109/CVPR.2017.167
   Gu Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P157, DOI 10.1145/3343031.3351039
   Hamilton WL, 2017, ADV NEUR IN, V30
   Han WJ, 2018, INTERSPEECH, P932, DOI 10.21437/Interspeech.2018-1858
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang CW, 2017, IEEE INT CON MULTI, P583, DOI 10.1109/ICME.2017.8019296
   Huang ZW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P801, DOI 10.1145/2647868.2654984
   Jaiswal S, 2016, IEEE WINT CONF APPL
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kipf TN, 2017, INT C LEARN REPR
   Latif S, 2019, INTERSPEECH, P3920, DOI 10.21437/Interspeech.2019-3252
   Lee J, 2019, IEEE I CONF COMP VIS, P10142, DOI 10.1109/ICCV.2019.01024
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Liu YZ, 2016, IEEE-ACM T AUDIO SPE, V24, P1946, DOI 10.1109/TASLP.2016.2593800
   Livingstone SR, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0196391
   Ma YX, 2019, INFORM FUSION, V46, P184, DOI 10.1016/j.inffus.2018.06.003
   Mangalam K, 2018, INTERSPEECH, P946, DOI 10.21437/Interspeech.2018-1872
   Mao F., 2018, PROC COMPUT VIS ECCV, P262
   Fernandez PDM, 2019, IEEE COMPUT SOC CONF, P837, DOI 10.1109/CVPRW.2019.00112
   Martin O., 2006, 22 INT C DATA ENG WO, P8, DOI [DOI 10.1109/ICDEW.2006.145, 10.1109/ICDEW.2006.145]
   Mirsamadi S, 2017, INT CONF ACOUST SPEE, P2227, DOI 10.1109/ICASSP.2017.7952552
   Niepert M, 2016, PR MACH LEARN RES, V48
   Pan BW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P566, DOI 10.1145/3343031.3351049
   Pan XZ, 2019, IEEE ACCESS, V7, P48807, DOI 10.1109/ACCESS.2019.2907271
   Parthasarathy S, 2020, IEEE-ACM T AUDIO SPE, V28, P2697, DOI 10.1109/TASLP.2020.3023632
   Peng ZC, 2018, IEEE INT CON MULTI
   Randhavane T., ARXIV190611884
   Schuller B, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P336
   Seng KP, 2018, IEEE T AFFECT COMPUT, V9, P3, DOI 10.1109/TAFFC.2016.2588488
   Seo Y, 2018, LECT NOTES COMPUT SC, V11301, P362, DOI 10.1007/978-3-030-04167-0_33
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan M., 2016, P INT C LEARN REPR W
   Tarantino L, 2019, INTERSPEECH, P2578, DOI 10.21437/Interspeech.2019-2822
   Vemulapalli R, 2019, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2019.00583
   Verma G, 2019, INT CONF ACOUST SPEE, P3975, DOI 10.1109/ICASSP.2019.8683133
   Volkova E, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0113647
   Wang YJ, 2008, IEEE T MULTIMEDIA, V10, P936, DOI 10.1109/TMM.2008.927665
   Wang YJ, 2012, IEEE T MULTIMEDIA, V14, P597, DOI 10.1109/TMM.2012.2189550
   Wang ZD, 2019, PROC CVPR IEEE, P1117, DOI 10.1109/CVPR.2019.00121
   Xu B., 2019, P INT C LEARN REPR
   Xu K, 2019, IEEE VTS VEH TECHNOL, DOI [10.1109/vtcfall.2019.8891597, 10.1109/biocas.2019.8918711]
   Yan JW, 2018, NEUROCOMPUTING, V309, P27, DOI 10.1016/j.neucom.2018.03.068
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Ying R, 2018, ADV NEUR IN, V31
   Zhang HM, 2018, IEEE T MULTIMEDIA, V20, P2824, DOI 10.1109/TMM.2018.2808760
   Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438
   Zhang W, 2020, IEEE T MULTIMEDIA, V22, P515, DOI 10.1109/TMM.2019.2928998
NR 62
TC 18
Z9 18
U1 6
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 780
EP 790
DI 10.1109/TMM.2021.3059169
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100021
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Slavic, G
   Baydoun, M
   Campo, D
   Marcenaro, L
   Regazzoni, C
AF Slavic, Giulia
   Baydoun, Mohamad
   Campo, Damian
   Marcenaro, Lucio
   Regazzoni, Carlo
TI Multilevel Anomaly Detection Through Variational Autoencoders and
   Bayesian Models for Self-Aware Embodied Agents
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Anomaly detection; Predictive models; Image
   reconstruction; Self-aware; Probabilistic logic; Data models; Anomaly
   detection; kalman filtering; particle filtering; variational autoencoder
ID NEURAL-NETWORKS; EVENT DETECTION; HISTOGRAMS; FLOW
AB Anomaly detection constitutes a fundamental step in developing self-aware autonomous agents capable of continuously learning from new situations, as it enables to distinguish novel experiences from already encountered ones. This paper combines Dynamic Bayesian Networks (DBNs) and Neural Networks (NNs) and proposes a method for detecting anomalies in video data at different abstraction levels. We use a Variational Autoencoder (VAE) to reduce the dimensionality of video frames, and Optical Flows between subsequent images, generating a latent space that captures both visual and dynamical information and that is comparable to low-dimensional sensory data (e.g., positioning, steering angle). An Adapted Markov Jump Particle Filter is employed to predict the following frames and detect anomalies in video data. Our method's evaluation is executed using different video data from a semi-autonomous vehicle performing different tasks in a closed environment. Tests on benchmark anomaly detection datasets have additionally been conducted.
C1 [Slavic, Giulia] Univ Genoa, DITEN, Fac Engn, I-16145 Genoa, Italy.
   [Baydoun, Mohamad; Campo, Damian; Marcenaro, Lucio; Regazzoni, Carlo] Univ Genoa, DITEN, I-16145 Genoa, Italy.
C3 University of Genoa; University of Genoa
RP Slavic, G (corresponding author), Univ Genoa, DITEN, Fac Engn, I-16145 Genoa, Italy.
EM slavic.giulia@gmail.com; mohamad.baydoun.it@gmail.com;
   damian.campo.work@gmail.com; lucio.marcenaro@unige.it;
   carlo.regazzoni@unige.it
RI Regazzoni, Carlo S/B-6092-2012
OI Regazzoni, Carlo S/0000-0001-6617-1417; Marcenaro,
   Lucio/0000-0003-1515-120X
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Alshazly H, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11121493
   Amini A, 2018, IEEE INT C INT ROBOT, P568, DOI 10.1109/IROS.2018.8594386
   Antipov G, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1263, DOI 10.1145/2733373.2806332
   Arnab A, 2018, IEEE SIGNAL PROC MAG, V35, P37, DOI 10.1109/MSP.2017.2762355
   Bastani V, 2016, IEEE T IMAGE PROCESS, V25, P2089, DOI 10.1109/TIP.2016.2540813
   Baur C, 2019, LECT NOTES COMPUT SC, V11383, P161, DOI 10.1007/978-3-030-11723-8_16
   Baydoun M, 2018, 2018 21ST INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P2606, DOI 10.23919/ICIF.2018.8455592
   Bengio Y, 2019, P INT C LEARN REPR
   Campo D, 2020, IEEE IMAGE PROC, P753, DOI [10.1109/icip40778.2020.9190980, 10.1109/ICIP40778.2020.9190980]
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Damasio A, 1999, FEELING WHAT HAPPENS
   de Gevigney VD, 2020, IEEE SYS MAN CYBERN, P3037, DOI [10.1109/SMC42975.2020.9282857, 10.1109/smc42975.2020.9282857]
   Doucet A., 2000, Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, P176
   Ehmck Philip Becker, 2019, P MACHINE LEARNING R, P553
   Fraccaro Marco, 2017, Advances in Neural Information Processing Systems, P3601
   Friston K, 2014, P IEEE, V102, P427, DOI 10.1109/JPROC.2014.2306251
   Fritzke B., 1995, Advances in Neural Information Processing Systems 7, P625
   Garg S, 2019, IEEE T MULTIMEDIA, V21, P566, DOI 10.1109/TMM.2019.2893549
   Ghahramani Z, 1998, LECT NOTES ARTIF INT, V1387, P168, DOI 10.1007/BFb0053999
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Haykin S, 2014, P IEEE, V102, P608, DOI 10.1109/JPROC.2014.2311211
   Huang DX, 2018, IEEE ACCESS, V6, P19161, DOI 10.1109/ACCESS.2018.2816564
   Iakovidis DK, 2018, IEEE T MED IMAGING, V37, P2196, DOI 10.1109/TMI.2018.2837002
   Iqbal H., 2020, P IEEE WORKSH SIGN P, P1
   Johnson MJ, 2016, ADV NEUR IN, V29
   Kanapram D, 2019, 2019 IEEE 5TH WORLD FORUM ON INTERNET OF THINGS (WF-IOT), P909, DOI [10.1109/WF-IoT.2019.8767204, 10.1109/wf-iot.2019.8767204]
   Ke N. R., 2019, ARXIV191001075
   Kim J, 2009, PROC CVPR IEEE, P2913
   Kingma D. P., 2014, arXiv
   Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056
   Kiran BR, 2018, J IMAGING, V4, DOI 10.3390/jimaging4020036
   Knill DC, 2004, TRENDS NEUROSCI, V27, P712, DOI 10.1016/j.tins.2004.10.007
   Koller D., 2009, Probabilistic graphical models: principles and techniques
   Krayani A, 2020, IEEE GLOB COMM CONF, DOI 10.1109/GLOBECOM42002.2020.9322583
   Li D, 2019, LECT NOTES COMPUT SC, V11730, P703, DOI 10.1007/978-3-030-30490-4_56
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Lu YW, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909850
   Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325
   Marin-Plaza P., 2016, JOINT C COMP VIS IM, V3, P703
   Medel J. R., 2016, Anomaly detection in video using predictive convolutional long short-term memory networks
   Monamo P, 2016, INFO SECUR S AFR, P129, DOI 10.1109/ISSA.2016.7802939
   Morin A, 2006, CONSCIOUS COGN, V15, P358, DOI 10.1016/j.concog.2005.09.006
   Nguyen K, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patog.2020.107358
   Ntalampiras S, 2011, IEEE T MULTIMEDIA, V13, P713, DOI 10.1109/TMM.2011.2122247
   Nugroho KA, 2018, 2018 2ND INTERNATIONAL CONFERENCE ON INFORMATICS AND COMPUTATIONAL SCIENCES (ICICOS), P141
   Oprea S., 2020, IEEE T PATTERN ANAL
   Papini O., 2020, A Guided Tour of Artificial Intelligence Research: Volume II: AI Algorithms, P209, DOI DOI 10.1007/978-3-030-06167-8_8
   Ravanbakhsh M, 2021, IEEE T INTELL TRANSP, V22, P3372, DOI 10.1109/TITS.2020.2984735
   Ravanbakhsh M, 2019, IEEE WINT CONF APPL, P1896, DOI 10.1109/WACV.2019.00206
   Regazzoni CS, 2020, P IEEE, V108, P987, DOI 10.1109/JPROC.2020.2986602
   Sarafijanovic-Djukic N, 2019, LECT NOTES ARTIF INT, V11828, P493, DOI 10.1007/978-3-030-33778-0_37
   Scholkopf B., 2019, CAUSALITY MACHINE LE
   Shi XJ, 2015, ADV NEUR IN, V28
   Slavic G, 2020, IEEE CONF EVOL ADAPT, DOI 10.1109/eais48028.2020.9122766
   Song H, 2020, IEEE T MULTIMEDIA, V22, P2138, DOI 10.1109/TMM.2019.2950530
   Sucar LE, 2015, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6699-3
   Tong A., 2020, IEEE INT WORKSHOP MA, P1
   Tran Hanh, 2017, P BRIT MACH VIS C
   Vondrick C, 2017, PROC CVPR IEEE, P2992, DOI 10.1109/CVPR.2017.319
   Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463
   Wei H, 2019, LECT NOTES COMPUT SC, V11754, P330, DOI 10.1007/978-3-030-34995-0_30
   Wen HG, 2018, CEREB CORTEX, V28, P4136, DOI 10.1093/cercor/bhx268
   Wu B, 2020, P I MECH ENG O-J RIS, V234, P422, DOI 10.1177/1748006X19825706
   Yan SY, 2020, IEEE T COGN DEV SYST, V12, P30, DOI 10.1109/TCDS.2018.2883368
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Zhao YR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1933, DOI 10.1145/3123266.3123451
   Zhou SF, 2016, SIGNAL PROCESS-IMAGE, V47, P358, DOI 10.1016/j.image.2016.06.007
NR 70
TC 13
Z9 13
U1 4
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1399
EP 1414
DI 10.1109/TMM.2021.3065232
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200013
DA 2024-07-18
ER

PT J
AU Zhu, ZY
   Toyoura, M
   Go, K
   Kashiwagi, K
   Fujishiro, I
   Wong, TT
   Mao, XY
AF Zhu, Zhenyang
   Toyoura, Masahiro
   Go, Kentaro
   Kashiwagi, Kenji
   Fujishiro, Issei
   Wong, Tien-Tsin
   Mao, Xiaoyang
TI Personalized Image Recoloring for Color Vision Deficiency Compensation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Sensitivity; Optimization; Computational modeling;
   Clustering algorithms; Adaptation models; Visualization; color vision
   deficiency; recoloring personalization; contrast enhancement;
   naturalness preservation
ID CONTRAST ENHANCEMENT; NATURALNESS; SIMULATION
AB Several image recoloring methods have been proposed to compensate for the loss of contrast caused by color vision deficiency (CVD). However, these methods only work for dichromacy (a case in which one of the three types of cone cells loses its function completely), while the majority of CVD is anomalous trichromacy (another case in which one of the three types of cone cells partially loses its function). In this paper, a novel degree-adaptable recoloring algorithm is presented, which recolors images by minimizing an objective function constrained by contrast enhancement and naturalness preservation. To assess the effectiveness of the proposed method, a quantitative evaluation using common metrics and subjective studies involving 14 volunteers with varying degrees of CVD are conducted. The results of the evaluation experiment show that the proposed personalized recoloring method outperforms the state-of-the-art methods, achieving desirable contrast enhancement adapted to different degrees of CVD while preserving naturalness as much as possible.
C1 [Zhu, Zhenyang] Univ Yamanashi, Grad Sch Engn, Kofu, Yamanashi 4008511, Japan.
   [Toyoura, Masahiro; Go, Kentaro; Mao, Xiaoyang] Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.
   [Kashiwagi, Kenji] Univ Yamanashi, Dept Ophthalmol, Chuo 4093898, Japan.
   [Fujishiro, Issei] Keio Univ, Dept Informat & Comp Sci, Yokohama, Kanagawa 2238522, Japan.
   [Wong, Tien-Tsin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 University of Yamanashi; University of Yamanashi; University of
   Yamanashi; Keio University; Chinese University of Hong Kong
RP Mao, XY (corresponding author), Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.
EM zhuyamanashi2016@gmail.com; mtoyoura@yamanashi.ac.jp;
   go@yamanashi.ac.jp; kenjik@yamanashi.ac.jp; fuji@ics.keio.ac.jp;
   ttwong@cse.cuhk.edu.hk; mao@yamanashi.ac.jp
OI Kashiwagi, Kenji/0000-0001-8506-8503; Fujishiro,
   Issei/0000-0002-8898-730X; Toyoura, Masahiro/0000-0002-5897-7573; Zhu,
   Zhenyang/0000-0003-1023-3193; Go, Kentaro/0000-0003-3451-7924
FU JSPS [17H00738, 20J15406]; Grants-in-Aid for Scientific Research
   [20K20408, 20J15406, 17H00738] Funding Source: KAKEN
FX This work was supported by JSPS Grants-in-Aid for Scientific Research,
   Japan under Grant 17H00738, 20J15406. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   F. Battisti.
CR [Anonymous], 2012, ACM T GRAPHIC
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Brettel H, 1997, J OPT SOC AM A, V14, P2647, DOI 10.1364/JOSAA.14.002647
   Chua SH, 2015, ACM T COMPUT-HUM INT, V21, DOI 10.1145/2687923
   Farnsworth D, 1943, J OPT SOC AM, V33, P568, DOI 10.1364/JOSA.33.000568
   Farnsworth D., 1947, FARNSWORTH DICHOTOMO
   Gómez-Robledo L, 2018, OPT EXPRESS, V26, P28693, DOI 10.1364/OE.26.028693
   Hassan MF, 2019, MULTIDIM SYST SIGN P, V30, P1975, DOI 10.1007/s11045-019-00638-7
   Hassan MF, 2017, SIGNAL PROCESS-IMAGE, V57, P126, DOI 10.1016/j.image.2017.05.011
   Hu XH, 2019, IEEE T COMPUT IMAG, V5, P649, DOI 10.1109/TCI.2019.2908291
   Huang CR, 2011, IEEE T MULTIMEDIA, V13, P950, DOI 10.1109/TMM.2011.2135844
   Huang CR, 2010, LECT NOTES COMPUT SC, V6298, P637, DOI 10.1007/978-3-642-15696-0_59
   Huang HB, 2007, IEEE SIGNAL PROC LET, V14, P711, DOI 10.1109/LSP.2007.898333
   Huang JB, 2009, INT CONF ACOUST SPEE, P1161, DOI 10.1109/ICASSP.2009.4959795
   Hung P., 2013, KONICA MINOLTA TECHN, V10, P30
   Ishihara S., 1987, Test for colour-blindness
   JUDD DB, 1966, P NATL ACAD SCI USA, V55, P1313, DOI 10.1073/pnas.55.6.1313
   Kuhn GR, 2008, IEEE T VIS COMPUT GR, V14, P1747, DOI 10.1109/TVCG.2008.112
   Lau C, 2011, IEEE I CONF COMP VIS, P1172, DOI 10.1109/ICCV.2011.6126366
   Li Y, 2010, COMPUT GRAPH FORUM, V29, P2049, DOI 10.1111/j.1467-8659.2010.01791.x
   Lin HY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19102250
   Machado GM, 2010, COMPUT GRAPH FORUM, V29, P933, DOI 10.1111/j.1467-8659.2009.01701.x
   Machado GM, 2009, IEEE T VIS COMPUT GR, V15, P1291, DOI 10.1109/TVCG.2009.113
   Martínez-Domingo MA, 2019, OPT EXPRESS, V27, P17954, DOI 10.1364/OE.27.017954
   Nam J, 2005, IEEE T MULTIMEDIA, V7, P435, DOI 10.1109/TMM.2005.846801
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Rasche K, 2005, IEEE COMPUT GRAPH, V25, P22, DOI 10.1109/MCG.2005.54
   Sajadi B, 2013, IEEE T VIS COMPUT GR, V19, P118, DOI 10.1109/TVCG.2012.93
   Sharpe L.T., 1999, COLOR VISION GENES P, P3
   Shen WY, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925878
   Viénot F, 1999, COLOR RES APPL, V24, P243, DOI 10.1002/(SICI)1520-6378(199908)24:4<243::AID-COL5>3.0.CO;2-3
   Wakita Ken., 2005, Assets '05: Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility, P158, DOI [10.1145/1090785.1090815, DOI 10.1145/1090785.1090815]
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xinghong H., 2019, ACM T GRAPHIC, V38
   Xu L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508404
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhu ZY, 2019, SIGNAL PROCESS-IMAGE, V76, P68, DOI 10.1016/j.image.2019.04.004
   Zhu Z, 2019, VISUAL COMPUT, V35, P1053, DOI 10.1007/s00371-019-01689-4
NR 38
TC 8
Z9 8
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1721
EP 1734
DI 10.1109/TMM.2021.3070108
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200036
OA hybrid
DA 2024-07-18
ER

PT J
AU Jiang, B
   Wang, XX
   Zheng, AH
   Tang, J
   Luo, B
AF Jiang, Bo
   Wang, Xixi
   Zheng, Aihua
   Tang, Jin
   Luo, Bin
TI PH-GCN: Person Retrieval With Part-Based Hierarchical Graph
   Convolutional Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Task analysis; Deep learning; Convolution; Computer
   architecture; Biological system modeling; Benchmark testing; Person
   re-identification; graph convolutional network; part-based
   representation; hierarchical graph
ID REIDENTIFICATION
AB Compact feature representation of person image is important for person re-identification (Re-ID) task. Recently, part-based representation models have been widely studied for extracting the more compact and robust feature representation for person image to improve person Re-ID results. However, existing part-based representation models mostly extract the features of different parts independently which ignore the spatial relationship information among different parts. To address this issue, in this paper we propose a novel deep learning framework, named Part-based Hierarchical Graph Convolutional Network (PH-GCN) for person Re-ID problem. Given a person image, PH-GCN first constructs a hierarchical graph to represent the spatial relationships among different parts. Then, both local and global feature learning is achieved by the feature information passing in PH-GCN, which takes the information of other parts into account for part feature representation. Finally, a perceptron layer is adopted for the final person part label prediction and re-identification. The proposed framework provides a general solution that integrates local, global and structural feature learning simultaneously in a unified end-to-end network representation and learning. Extensive experiments on several widely used benchmark datasets demonstrate the effectiveness and benefits of the proposed PH-GCN approach for person Re-ID task.
C1 [Jiang, Bo; Wang, Xixi; Tang, Jin; Luo, Bin] Anhui Univ, Sch Comp Sci Technol, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.
   [Zheng, Aihua] Anhui Univ, Sch Artificial Intelligence, Anhui Prov Key Lab Multimodal Cognit Computat, Informat Mat & Intelligent Sensing Lab Anhui Prov, Hefei 230601, Peoples R China.
C3 Anhui University; Anhui University
RP Zheng, AH (corresponding author), Anhui Univ, Sch Artificial Intelligence, Anhui Prov Key Lab Multimodal Cognit Computat, Informat Mat & Intelligent Sensing Lab Anhui Prov, Hefei 230601, Peoples R China.
EM jiangbo@ahu.edu.cn; sissiw0409@foxmail.com; ahzheng214@foxmail.com;
   ahu_tj@163.com; luobin@ahu.edu.cn
RI Zheng, Aihua/ABA-5196-2020; lu, bin/HPE-4790-2023; LUO, BIN/Y-1233-2018
OI LUO, BIN/0000-0001-5948-5055; Wang, Sissi/0000-0001-8510-0964
FU Key Project of Research and Development of Anhui Province
   [201904b11020037]; NSFC Key Projects of International (Regional)
   Cooperation and Exchanges [61860206004]; National Natural Science
   Foundation of China [62076004, 61976002]
FX This work was supported in part by the Key Project of Research and
   Development of Anhui Province under Grant 201904b11020037, in part by
   the NSFC Key Projects of International (Regional) Cooperation and
   Exchanges under Grant 61860206004, and in part by the National Natural
   Science Foundation of China under Grants 62076004 and 61976002.
CR [Anonymous], 2014, PROC 2 INT C LEARN R
   Atwood J, 2016, NIPS, P2001
   Bai S, 2017, PROC CVPR IEEE, P3356, DOI 10.1109/CVPR.2017.358
   Bao LQ, 2019, IEEE COMPUT SOC CONF, P1496, DOI 10.1109/CVPRW.2019.00191
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128
   Chen DP, 2018, PROC CVPR IEEE, P8649, DOI 10.1109/CVPR.2018.00902
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Defferrard M, 2016, ADV NEUR IN, V29
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Garcia V., 2018, PROC INT C LEARN REP
   Guo M, 2018, LECT NOTES COMPUT SC, V11205, P673, DOI 10.1007/978-3-030-01246-5_40
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Hu F., 2017, PROC INT C LEARN REP
   Jiang B, 2019, PROC CVPR IEEE, P11305, DOI 10.1109/CVPR.2019.01157
   Kipf T.N., 2017, P INT C LEARN REPR S
   Knyazev B., 2019, PROC BRI MACHINE VIS
   Li H, 2019, IEEE INT CON MULTI, P694, DOI 10.1109/ICME.2019.00125
   Li SZ, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107016
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Niepert M, 2016, PR MACH LEARN RES, V48
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Shen YT, 2018, PROC CVPR IEEE, P2265, DOI 10.1109/CVPR.2018.00241
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   Ustinova E, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Velickovi  c P., 2018, P INT C LEARN REPR, P1
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang YC, 2018, PROC CVPR IEEE, P1470, DOI 10.1109/CVPR.2018.00159
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yan YC, 2019, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR.2019.00226
   Yang JR, 2020, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR42600.2020.00335
   Yuan K, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1121
   Zhang Y, 2019, IEEE ACCESS, V7, P53585, DOI 10.1109/ACCESS.2019.2912844
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
NR 69
TC 10
Z9 10
U1 2
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 9
PY 2021
VL 24
BP 3218
EP 3228
DI 10.1109/TMM.2021.3095789
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NT
UT WOS:000824707800001
DA 2024-07-18
ER

PT J
AU Abrol, V
   Sharma, P
   Patra, A
AF Abrol, Vinayak
   Sharma, Pulkit
   Patra, Arijit
TI Improving Generative Modelling in VAEs Using Multimodal Prior
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object oriented modeling; Training; Mutual information; Decoding;
   Kernel; Data models; Uncertainty; Generative modelling; autoencoders;
   matching network; representation learning
ID IMAGE
AB In this paper we propose a conditional generative modelling (CGM) approach for unsupervised disentangled representation learning using variational autoencoder (VAE). CGM employs a multimodal/categorical conditional prior distribution in the latent space to learn global uncertainty in data by modelling the variations at local level. Thus, the proposed framework enforces the model to independently estimate the inherent patterns within each category, which improves the interpretability of the latent representations learned by the VAE model. The evidence lower bound objective for training the generative model is maximized using a mutual information criterion between the global latent categorical variable and the encoded inputs. Further, the approach has a built-in mechanism for bounding the information flow between the encoder and the decoder which addresses the problems of posterior collapse in conventional VAE models. Experiments on a variety of datasets demonstrate that our objective can learn disentangled representations and the proposed approach achieves competitive results on various task such as generative modelling, image classification and image denoising.
C1 [Abrol, Vinayak] Univ Oxford, Math Inst, Oxford OX26GG, England.
   [Sharma, Pulkit; Patra, Arijit] Univ Oxford, Dept Engn Sci, Oxford OX13PJ, England.
C3 University of Oxford; University of Oxford
RP Abrol, V (corresponding author), Univ Oxford, Math Inst, Oxford OX26GG, England.
EM abrol@maths.ox.ac.uk; pulkit.sharma@eng.ox.ac.uk;
   arijit.patra@exeter.ox.ac.uk
CR Alemi A. A., 2017, ICLR, DOI DOI 10.48550/ARXIV.1612.00410
   Bartunov S, 2017, P INT C LEARN REPR W, P1
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Binkowski Mikolaj, 2018, INT C LEARNING REPRE
   Braithwaite D, 2018, P DEEP LEARN DAY ACM, P1
   Chen X., 2017, PROC INT C LEARN REP
   Chen X, 2018, PR MACH LEARN RES, V80
   Chen X, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dilokthanakul N, 2016, DEEP UNSUPERVISED CL
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Engel J., 2018, PROC INT C LEARN REP
   Fortet R., 1953, Ann. Sci. Ecole Norm. Sup., V70, P267
   Garnelo M., 2018, INT C MACH LEARN, V80, P1690, DOI DOI 10.48550/ARXIV.1807.01613
   Ge PF, 2020, IEEE T NEUR NET LEAR, V31, P1417, DOI 10.1109/TNNLS.2019.2919948
   González-Díaz I, 2014, IEEE T MULTIMEDIA, V16, P169, DOI 10.1109/TMM.2013.2286083
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Green J. W., 1974, GEN TOPOLOGY APPL, V4, P297
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Higgins I., 2017, 5 INT C LEARN REPR T
   Huang HB, 2018, ADV NEUR IN, V31
   Jha AH, 2018, LECT NOTES COMPUT SC, V11207, P829, DOI 10.1007/978-3-030-01219-9_49
   Jin WK, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P465, DOI 10.1145/3331184.3331240
   Kingma D. P., 2014, arXiv
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li YJ, 2015, PR MACH LEARN RES, V37, P1718
   LUCAS T., 2018, Joint European Conference on Machine Learning and Knowledge Discovery in Databases, P443
   Ma X, 2019, P INT C LEARN REPR M, P1
   Mathieu E., 2019, P INT C MACH LEARN I, P4402
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Paszke A, 2019, ADV NEUR IN, V32
   Phuong M., 2018, MUTUAL AUTOENCODER C
   Radford A., 2016, 4 INT C LEARN REPR I
   Razavi A, 2019, P INT C LEARN REPR M, P1
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Ritter S., 2018, INT C MACHINE LEARNI, P4354
   Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140
   Sigtia S, 2016, IEEE-ACM T AUDIO SPE, V24, P927, DOI 10.1109/TASLP.2016.2533858
   Sohn K., 2015, Neural Information Processing Systems, P3483
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   van den Oord A, 2016, ADV NEUR IN, V29
   van den Oord A, 2016, PR MACH LEARN RES, V48
   Vishwanathan SVN, 2006, J MACH LEARN RES, V7, P1107
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xiao H., 2017, arXiv
   Xu WJ, 2019, IEEE T MULTIMEDIA, V21, P2387, DOI 10.1109/TMM.2019.2898777
   Zhao SJ, 2019, AAAI CONF ARTIF INTE, P5885
NR 55
TC 2
Z9 3
U1 2
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2153
EP 2161
DI 10.1109/TMM.2020.3008053
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100024
OA Green Published
DA 2024-07-18
ER

PT J
AU Beyan, C
   Shahid, M
   Murino, V
AF Beyan, Cigdem
   Shahid, Muhammad
   Murino, Vittorio
TI RealVAD: A Real-World Dataset and A Method for Voice Activity Detection
   by Body Motion Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Lips; Voice activity detection; Task
   analysis; Benchmark testing; Synchronization; Voice activity detection;
   active speaker; body motion analysis; nonverbal behavior; visual cues;
   real-world dataset; unsupervised domain adaptation
ID SPEAKER DIARIZATION; SPEECH; RECOGNITION; MOVEMENTS; AUDIO
AB We present an automatic voice activity detection (VAD) method that is solely based on visual cues. Unlike traditional approaches processing audio, we show that upper body motion analysis is desirable for the VAD task. The proposed method consists of components for body motion representation, feature extraction from a Convolutional Neural Network (CNN) architecture and unsupervised domain adaptation. The body motion representations as images are used by the feature extraction component, which is generic and person-invariant, thus, can be applied to a subject who has never been seen. The endmost component handles the domain-shift problem, which appears due to the fact that the way people move/ gesticulate while speaking might vary from subject to subject, which results in disparate body motion features and consequently poorer VAD performance. The experimental analyses applied on a publicly available real-world VAD dataset show that the proposed method performs better than the state-of-the-art video-only and multimodal VAD approaches. Moreover, the proposed method has a better generalization ability as VAD results are more consistent across different subjects. As another major contribution, we present a new multimodal dataset (called RealVAD), created from a real-world (no role-plays) panel discussion. This dataset contains many actual situations/ challenges that are missing in the previous VAD datasets. We benchmarked the RealVAD dataset by applying the proposed method as well as cross-dataset analyses. Particularly, the results of cross-dataset experiments highlight the remarkable positive contribution of the unsupervised domain adaptation applied.
C1 [Beyan, Cigdem; Shahid, Muhammad] Ist Italiano Tecnol, Pattern Anal & Comp Vis Res Line, I-16152 Genoa, Italy.
   [Shahid, Muhammad] Univ Genoa, Dipartimento Ingn Navale Elettr Elettron & Teleco, I-16145 Genoa, Italy.
   [Murino, Vittorio] Univ Verona, Dept Comp Sci, I-37129 Verona, Italy.
   [Murino, Vittorio] Ireland Res Ctr, Huawei Technol Ltd, Dublin 4, Dublin, Ireland.
   [Murino, Vittorio] Ist Italian Tecnol, Pattern Anal & Comp Vis Res Line, I-16152 Genoa, Italy.
C3 Istituto Italiano di Tecnologia - IIT; University of Genoa; University
   of Verona; Huawei Technologies; Istituto Italiano di Tecnologia - IIT
RP Beyan, C (corresponding author), Ist Italiano Tecnol, Pattern Anal & Comp Vis Res Line, I-16152 Genoa, Italy.
EM cigdem.beyan@iit.it; shahid.muhammad@iit.it; vittorio.murino@iit.it
RI Murino, Vittorio/A-5570-2011; Beyan, Cigdem/AAA-4235-2019
OI Beyan, Cigdem/0000-0002-9583-0087; Shahid, Muhammad/0000-0002-4573-0379;
   Murino, Vittorio/0000-0002-8645-2328
CR Abushakra A, 2013, IEEE J BIOMED HEALTH, V17, P493, DOI 10.1109/JBHI.2013.2244901
   Alameda-Pineda X, 2013, J MULTIMODAL USER IN, V7, P79, DOI 10.1007/s12193-012-0111-y
   [Anonymous], 2014, PLOS ONE
   [Anonymous], 1991, Gestures and Speech: Psychological Investigations
   Beyan C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P311, DOI 10.1145/3240508.3240685
   Beyan C, 2019, IEEE T MULTIMEDIA, V21, P2107, DOI 10.1109/TMM.2019.2895505
   Beyan C, 2015, PATTERN RECOGN, V48, P1653, DOI 10.1016/j.patcog.2014.10.032
   Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   Cabrera-Quiros L, 2021, IEEE T AFFECT COMPUT, V12, P113, DOI 10.1109/TAFFC.2018.2848914
   Campbell N, 2006, P WORKSH PROGR MAY, V10
   Chakravarty P, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P312, DOI 10.1145/2993148.2993172
   Chakravarty P, 2016, LECT NOTES COMPUT SC, V9909, P285, DOI 10.1007/978-3-319-46454-1_18
   Chakravarty P, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P87, DOI 10.1145/2818346.2820780
   Chung JS, 2018, COMPUT VIS IMAGE UND, V173, P76, DOI 10.1016/j.cviu.2018.02.001
   CRISTANI M., 2011, International Joint Conference on Ambient Intelligence, P72
   D'Arca E, 2016, SIGNAL PROCESS, V129, P137, DOI 10.1016/j.sigpro.2016.04.014
   Deleforge A, 2015, IEEE-ACM T AUDIO SPE, V23, P718, DOI 10.1109/TASLP.2015.2405475
   Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368
   Friedland G, 2009, INT CONF ACOUST SPEE, P4069, DOI 10.1109/ICASSP.2009.4960522
   Garg A., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P384, DOI 10.1109/AFGR.2000.840663
   Gebre Binyam Gebrekidan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1537, DOI 10.1109/ICASSP.2014.6853855
   Gebre BG, 2014, INTERSPEECH, P582
   Gebre BG, 2013, INT CONF ACOUST SPEE, P3751, DOI 10.1109/ICASSP.2013.6638359
   Gebru I. D., 2015, P IEEE INT C COMP VI, P15
   Gebru ID, 2018, IEEE T PATTERN ANAL, V40, P1086, DOI 10.1109/TPAMI.2017.2648793
   Gedik E, 2017, PERS UBIQUIT COMPUT, V21, P723, DOI 10.1007/s00779-017-1006-4
   Haider F, 2016, IEEE GLOB CONF SIG, P1207, DOI 10.1109/GlobalSIP.2016.7906033
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He WP, 2018, IEEE INT CONF ROBOT, P74
   Hoover K., 2017, ARXIVABS170600079
   Hu Y., 2015, ACM T MULTIMEDIA COM, V11
   Hung H, 2010, P INT C AC SPEECH SI
   Joosten B, 2015, J MULTIMODAL USER IN, V9, P183, DOI 10.1007/s12193-015-0187-2
   Kopp S, 2004, COMPUT ANIMAT VIRT W, V15, P39, DOI 10.1002/cav.6
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   McHugh ML, 2012, BIOCHEM MEDICA, V22, P276, DOI 10.11613/bm.2012.031
   MCNEILL D, 1985, PSYCHOL REV, V92, P350, DOI 10.1037/0033-295X.92.3.350
   Minotto VP, 2015, IEEE T MULTIMEDIA, V17, P1694, DOI 10.1109/TMM.2015.2463722
   Qian XY, 2019, IEEE T MULTIMEDIA, V21, P2576, DOI 10.1109/TMM.2019.2902489
   Rehg J. M., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P110, DOI 10.1109/CVPR.1999.784617
   Roth J., ARXIV190101342, V2019
   Shahid M, 2019, LECT NOTES COMPUT SC, V11751, P48, DOI 10.1007/978-3-030-30642-7_5
   Simonyan K., 2014, CORR
   Stefanov K., 2017, P GLU INT WORKSH GRO, P47
   Stefanov K, 2020, IEEE T COGN DEV SYST, V12, P250, DOI 10.1109/TCDS.2019.2927941
   Tao F, 2019, SPEECH COMMUN, V113, P25, DOI 10.1016/j.specom.2019.07.003
   Tao F, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2302
   Thermos S, 2016, IEEE W SP LANG TECH, P579, DOI 10.1109/SLT.2016.7846321
   Vajaria H, 2008, IEEE T CIRC SYST VID, V18, P1608, DOI 10.1109/TCSVT.2008.2005602
   Vinciarelli A., 2009, 2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops, P1
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wittenburg Peter, 2006, 5 INT C LANG RES EV, V2006
NR 52
TC 8
Z9 8
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2071
EP 2085
DI 10.1109/TMM.2020.3007350
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100018
DA 2024-07-18
ER

PT J
AU Chang, ZY
   Chan, SHG
AF Chang, Zhangyu
   Chan, S. -H. Gary
TI An Approximation Algorithm to Maximize User Capacity for an Auto-Scaling
   VoD System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Servers; Videos; Data centers; Cloud computing; Dispatching; Resource
   management; Optimization; Video-on-demand; auto-scaling cloud; video
   allocation; request dispatching; joint optimization
ID RESOURCE-ALLOCATION; MEDIA CLOUD; VIDEO; PREDICTION; OPTIMIZATION;
   REPLICATION; RETRIEVAL; EFFICIENT; SERVICES; STORAGE
AB In a video-on-demand (VoD) service, blockbuster videos have stable and predictable popularity, but the traffic can vary significantly within short timescale. To efficiently serve the user pool in a geographic region, we consider a regional auto-scaling cloud-based data center consisting of multiple servers. For efficient storage, we partition the videos into fixed-size blocks. To respond to dynamic user traffic in a timely and cost-effective manner, we may activate or deactivate each server according to the traffic while keeping at least one replica for each block in the active servers. We maximize the user capacity of the active servers (and hence minimizing the number of active servers at any time) by jointly optimizing block allocation in the servers, server selection at each traffic level, and request dispatching to a server. We believe that this is the first work to study such problem for an auto-scaling cloud-based VoD data center. We first formulate the problem and show its NP-hardness. We then propose AVARDO (Auto-scaling Video Allocation and Request Distribution Optimization), a simple but efficient approximation algorithm with proven optimality. AVARDO operates the servers like a stack, with a server being pushed into or popped from the existing active server set according to some optimized traffic thresholds. We prove that AVARDO approaches the theoretical optimum as the block size reduces. Trace-driven experimental results based on large-scale real-world video data further validate that AVARDO is closely optimal. It achieves significantly higher user capacity as compared with other state-of-the-art and traditional schemes, and reduces the optimality gap by multiple times.
C1 [Chang, Zhangyu; Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Chang, ZY (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
EM zchang@cse.ust.hk; gchan@cse.ust.hk
OI Chan, Gary Shueng Han/0000-0003-4207-764X; Chang,
   Zhangyu/0000-0002-1069-4048
FU Hong Kong General Research Fund [16200120]
FX YManuscript received December 17, 2019; revised June 15, 2020 and August
   24, 2020; accepted October 7, 2020. Date of publication October 15,
   2020; date of current version October 19, 2021. This work was supported,
   in part, by Hong Kong General Research Fund under grant 16200120. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Mea Wang. (Corresponding author:
   Zhangyu Chang.)
CR Alasaad A, 2015, IEEE T PARALL DISTR, V26, P1021, DOI 10.1109/TPDS.2014.2316827
   Anisfeld O, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON THE SCIENCE OF ELECTRICAL ENGINEERING IN ISRAEL (ICSEE)
   [Anonymous], 2015, INT J COMPUT APPL
   Apple gate D., 2010, P ACM CONEXT NOV
   Ayoub O, 2019, IEEE T GREEN COMMUN, V3, P159, DOI 10.1109/TGCN.2018.2878918
   Bagci KT, 2018, IEEE T MULTIMEDIA, V20, P3084, DOI 10.1109/TMM.2018.2823907
   Borst S, 2010, IEEE INFOCOM SER, DOI 10.1109/infcom.2010.5461964
   Bourtsoulatze E, 2018, IEEE T MULTIMEDIA, V20, P1561, DOI 10.1109/TMM.2017.2767778
   Cai CX, 2014, IEEE ICC, P4208, DOI 10.1109/ICC.2014.6883981
   Chan S. H. G., 2016, U.S. Patent, Patent No. [9,325,786, 9325786]
   Chan S. H. G., 2011, U.S. Patent, Patent No. [7,925,781, 7925781]
   Chan SHG, 2013, IEEE T MULTIMEDIA, V15, P2125, DOI 10.1109/TMM.2013.2280989
   Chen F, 2018, PEER PEER NETW APPL, V11, P1060, DOI 10.1007/s12083-017-0575-3
   Chen L, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P191, DOI 10.1145/3230543.3230551
   Cheng ZP, 2014, LECT NOTES COMPUT SC, V8889, P581, DOI 10.1007/978-3-319-13075-0_46
   Crainic TG, 2011, COMPUT OPER RES, V38, P1474, DOI 10.1016/j.cor.2011.01.001
   Da Deng, 2013, 2013 IEEE International Conference on Services Computing (SCC), P486, DOI 10.1109/SCC.2013.91
   De Cicco L, 2019, AD HOC NETW, V89, P170, DOI 10.1016/j.adhoc.2019.02.008
   De Cicco L, 2013, IEEE INT CONF COMM, P723, DOI 10.1109/ICCW.2013.6649328
   Du J, 2016, IEEE T MULTIMEDIA, V18, P820, DOI 10.1109/TMM.2016.2537781
   Feng YA, 2019, IEEE T MULTIMEDIA, V21, P1762, DOI 10.1109/TMM.2018.2885237
   Gao GY, 2015, IEEE ICC, P6880, DOI 10.1109/ICC.2015.7249422
   Hajiakhondi-Meybodi Z, 2019, IEEE T MOBILE COMPUT, V18, P1476, DOI 10.1109/TMC.2018.2864164
   Hu H, 2016, IEEE T CIRC SYST VID, V26, P1320, DOI 10.1109/TCSVT.2015.2455712
   Iqbal W, 2018, J NETW COMPUT APPL, V124, P94, DOI 10.1016/j.jnca.2018.09.023
   Jeon M, 2012, SYMP NETW CLOUD, P40, DOI 10.1109/NCCA.2012.10
   Li Y, 2018, IEEE T MULTIMEDIA, V20, P2427, DOI 10.1109/TMM.2018.2796246
   Liao WH, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON SMART CITY/SOCIALCOM/SUSTAINCOM (SMARTCITY), P1059, DOI 10.1109/SmartCity.2015.209
   Liu N, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2568194
   Mohan A, 2021, IEEE T CLOUD COMPUT, V9, P40, DOI 10.1109/TCC.2018.2836907
   Mousavi S., 2018, RECENT ADV TECHNOLOG, P289
   Niño-Mora J, 2019, COMPUT OPER RES, V103, P221, DOI 10.1016/j.cor.2018.11.012
   Niu D, 2012, IEEE INFOCOM SER, P711, DOI 10.1109/INFCOM.2012.6195816
   Niu D, 2012, IEEE INFOCOM SER, P460, DOI 10.1109/INFCOM.2012.6195785
   Papagianni C, 2013, IEEE T DEPEND SECURE, V10, P287, DOI 10.1109/TDSC.2013.12
   Silvestre G, 2012, INT C PAR DISTRIB SY, P189, DOI 10.1109/ICPADS.2012.35
   Tang J, 2018, IEEE T MULTIMEDIA, V20, P1008, DOI 10.1109/TMM.2017.2760627
   Tseng FH, 2018, IEEE SYST J, V12, P1688, DOI 10.1109/JSYST.2017.2722476
   Valliyammai C, 2019, ADV INTELL SYST, V755, P309, DOI 10.1007/978-981-13-1951-8_28
   Wang Zhi., 2012, Proceedings of the 20th ACM international conference on Multimedia, MM '12, P29
   Yang J, 2019, IEEE T MULTIMEDIA, V21, P494, DOI 10.1109/TMM.2018.2862349
   Yang JQ, 2014, INFORM SYST FRONT, V16, P7, DOI 10.1007/s10796-013-9459-0
   Yang P, 2019, IEEE T MULTIMEDIA, V21, P915, DOI 10.1109/TMM.2018.2870521
   Zhang YX, 2018, 2018 4TH INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING AND COMMUNICATIONS (BIGCOM 2018), P189, DOI 10.1109/BIGCOM.2018.00037
   Zhao H, 2019, MULTIMED TOOLS APPL, V78, P21827, DOI 10.1007/s11042-019-7457-z
   Zhao H, 2017, IEEE T MULTIMEDIA, V19, P149, DOI 10.1109/TMM.2016.2612123
   Zhou YP, 2015, IEEE ACM T NETWORK, V23, P1163, DOI 10.1109/TNET.2014.2321422
NR 47
TC 4
Z9 4
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3714
EP 3725
DI 10.1109/TMM.2020.3031068
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100022
DA 2024-07-18
ER

PT J
AU Cheung, CH
   Sheng, L
   Ngan, KN
AF Cheung, Chi Ho
   Sheng, Lu
   Ngan, King Ngi
TI Motion Compensated Virtual View Synthesis Using Novel Particle Cell
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Three-dimensional displays; Optical imaging; Predictive models;
   Motion estimation; Rendering (computer graphics); Motion compensation;
   Free-viewpoint communication; virtual view synthesis; depth-image-based
   rendering; 3D-warping
ID DEPTH MAP; VIDEO
AB Due to the wide interest in advanced multimedia experience, free-viewpoint communication is being greatly developed in recent years. In the free-viewpoint communication, viewers can perceive a view from any angle and any position of a scene. Even though the preferred views are not captured, we can generate the views through virtual view synthesis that synthesizes an arbitrary view from captured reference view(s). For daily use, only one or few cameras in baseline distance are given to capture the scene that makes the virtual view synthesis challenging. The task is more difficult when the camera is continuously moving. In this paper, we propose a particle cell to model a reference view sequence to a set of moving particles for virtual view synthesis. Using our novel hybrid motion estimation scheme, the projected coordinates of particles in each frame are obtained even they are occluded. The particles are warped to a virtual view and synthesized as a virtual view sequence. Our method is applicable for both dynamic camera setting and static camera setting. The experimental results show our method outperforms the state-of-the-art algorithms in dynamic camera datasets and presents improvement in static camera datasets in general.
C1 [Cheung, Chi Ho] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
   [Sheng, Lu] Beihang Univ, Coll Software, Beijing, Peoples R China.
   [Ngan, King Ngi] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu, Peoples R China.
C3 Chinese University of Hong Kong; Beihang University; University of
   Electronic Science & Technology of China
RP Cheung, CH (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
EM chcheung@link.cuhk.edu.hk; lsheng@ee.cuhk.edu.hk; knngan@cuhk.edu.hk
RI Ngan, N/E-8240-2014
OI Ngan, N/0000-0003-1946-3235; Sheng, Lu/0000-0002-8525-9163
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2011, 3DTV C TRUE VIS CAPT
   [Anonymous], 2008, ETRI 3DV DAT
   Atzpadin N, 2004, IEEE T CIRC SYST VID, V14, P321, DOI 10.1109/TCSVT.2004.823391
   Bao LC, 2014, PROC CVPR IEEE, P3534, DOI 10.1109/CVPR.2014.452
   Cheung C. H., 2015, P IEEE INT C MULT EX, P1
   Cheung CH, 2018, IEEE T MULTIMEDIA, V20, P1376, DOI 10.1109/TMM.2017.2772442
   Choi S, 2013, IEEE T IMAGE PROCESS, V22, P2429, DOI 10.1109/TIP.2013.2251646
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Daribo I, 2011, IEEE T BROADCAST, V57, P533, DOI 10.1109/TBC.2011.2125110
   Doan HN, 2015, LECT NOTES COMPUT SC, V9315, P598, DOI 10.1007/978-3-319-24078-7_61
   Electron. Telecommun. Res. InstituteGwangju Inst. Sci. Technol. Daejon Korea, 2008, 3DV SEQ ETRI GIST
   Fraunhofer Heinrich Hertz Institute, 2013, 3DV Sequences of HHI
   Grimson WEL, 1998, PROC CVPR IEEE, P22, DOI 10.1109/CVPR.1998.698583
   Guillemot C, 2014, IEEE SIGNAL PROC MAG, V31, P127, DOI 10.1109/MSP.2013.2273004
   Ho Yo-Sung, 2008, JTC1SC29WG11 ISOIEC
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hsu HA, 2014, IEEE T CIRC SYST VID, V24, P74, DOI 10.1109/TCSVT.2013.2276699
   Ince S, 2008, IEEE T IMAGE PROCESS, V17, P1443, DOI 10.1109/TIP.2008.925381
   KaewTraKulPong P, 2002, VIDEO-BASED SURVEILLANCE SYSTEMS: COMPUTER VISION AND DISTRIBUTED PROCESSING, P135
   Kim K, 2004, IEEE IMAGE PROC, P3061
   Köppel M, 2012, IEEE INT WORKSH MULT, P25, DOI 10.1109/MMSP.2012.6343410
   Kuster C., 2011, Proc. Annual Workshop on Vision, P17
   Lee PJ, 2011, IEEE T MULTIMEDIA, V13, P246, DOI 10.1109/TMM.2010.2100372
   Lu W., 2011, 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON), 2011, P1
   Luo GB, 2016, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2016.197
   Marek D., 2009, JTC1SC29WG11MPEG2009
   McMillan Jr L., 1997, THESIS U N CAROLINA
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Plath N, 2013, IEEE T IMAGE PROCESS, V22, P3420, DOI 10.1109/TIP.2013.2268940
   Rahaman D. M. M., 2016, IEEE INT CONF MULTI, P1
   Rahaman DMM, 2018, IEEE T IMAGE PROCESS, V27, P1190, DOI 10.1109/TIP.2017.2772858
   Rusanovskyy D., 2011, JTC1SC29WG11MPEG2010, V20028
   Sand Peter, 2008, International Journal of Computer Vision, V80, P72, DOI 10.1007/s11263-008-0136-6
   Smolic A, 2011, PATTERN RECOGN, V44, P1958, DOI 10.1016/j.patcog.2010.09.005
   Su PC, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010235
   Tanimoto laboratory Department of Information Electronics School of Engineering Nagoya University, 2013, MPEG FTV TEST SEQ
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Vazquez C., 2006, P SPIE C 3D TV VID D, p63 920D
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu L, 2019, IEEE T NEUR NET LEAR, V30, P3347, DOI 10.1109/TNNLS.2019.2891244
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu L, 2019, IEEE T CYBERNETICS, V49, P1791, DOI 10.1109/TCYB.2018.2813971
   Xu L, 2012, IEEE T PATTERN ANAL, V34, P1744, DOI 10.1109/TPAMI.2011.236
   Yao C, 2014, IEEE T BROADCAST, V60, P394, DOI 10.1109/TBC.2014.2321671
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
   Zhu LW, 2018, IEEE T IMAGE PROCESS, V27, P5365, DOI 10.1109/TIP.2018.2858022
NR 47
TC 2
Z9 2
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1908
EP 1923
DI 10.1109/TMM.2020.3004966
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100007
DA 2024-07-18
ER

PT J
AU Deng, JJ
   Pan, YW
   Yao, T
   Zhou, WG
   Li, HQ
   Mei, T
AF Deng, Jiajun
   Pan, Yingwei
   Yao, Ting
   Zhou, Wengang
   Li, Houqiang
   Mei, Tao
TI Single Shot Video Object Detector
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video object detection; single shot detection; feature aggregation
AB Single shot detectors that are potentially faster and simpler than two-stage detectors tend to be more applicable to object detection in videos. Nevertheless, the extension of such object detectors from image to video is not trivial especially when appearance deterioration exists in videos, e.g., motion blur or occlusion. A valid question is how to explore temporal coherence across frames for boosting detection. In this paper, we propose to address the problem by enhancing per-frame features through aggregation of neighboring frames. Specifically, we present Single Shot Video Object Detector (SSVD) - a new architecture that novelly integrates feature aggregation into a one-stage detector for object detection in videos. Technically, SSVD takes Feature Pyramid Network (FPN) as backbone network to produce multiscale features. Unlike the existing feature aggregation methods, SSVD, on one hand, estimates the motion and aggregates the nearby features along the motion path, and on the other, hallucinates features by directly sampling features from the adjacent frames in a two-stream structure. Extensive experiments are conducted on ImageNet VID dataset, and competitive results are reported when comparing to state-of-the-art approaches. More remarkably, for 448 x 448 input, SSVD achieves 79.2% mAP on ImageNet VID, by processing one frame in 85 ms on an Nvidia TitanXPascal GPU. The code is available at https://github.com/ ddjiajun/ SSVD.
C1 [Deng, Jiajun; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Hefei 230026, Peoples R China.
   [Pan, Yingwei; Yao, Ting] JD AI Res, Vis & Multimedia Lab, Beijing 100105, Peoples R China.
   [Mei, Tao] JD AI Res, Beijing 100105, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG (corresponding author), Univ Sci & Technol China, Hefei 230026, Peoples R China.; Yao, T (corresponding author), JD AI Res, Vis & Multimedia Lab, Beijing 100105, Peoples R China.
EM dengjj@mail.ustc.edu.cn; panyw.ustc@gmail.com; tingyao.ustc@gmail.com;
   zhwg@ustc.edu.cn; lihq@ustc.edu.cn; tmei@live.com
RI Deng, Jiajun/KIK-3592-2024; Li, Houqiang Li/B-6259-2013; Mei,
   Tao/GQZ-0596-2022; Pan, Yingwei/T-7649-2019
OI Mei, Tao/0000-0002-5990-7307; Pan, Yingwei/0000-0002-4344-8898; Deng,
   Jiajun/0000-0001-9624-7451; Yao, Ting/0000-0001-7587-101X
FU NSFC [61822208, 61632019]; Yuth Innovation Promotion Association CAS
   [2018497]
FX The work of Dr. Houqiang Li was supported by NSFC under Contract No.
   61836011 The work of Dr. Wengang Zhou was supported by NSFC under
   Contract Nos. 61822208 and 61632019. This work was also supported by
   Yuth Innovation Promotion Association CAS (No. 2018497).
CR [Anonymous], 2016, P INT JOINT C ARTIFI
   [Anonymous], COMPUT VIS PATTERN R
   [Anonymous], 2016, SEQ NMS VIDEO OBJECT
   Bertasius G, 2018, LECT NOTES COMPUT SC, V11216, P342, DOI 10.1007/978-3-030-01258-8_21
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Cai Q, 2019, PROC CVPR IEEE, P11449, DOI 10.1109/CVPR.2019.01172
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Chen Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P647, DOI 10.1145/3343031.3350937
   Dai JF, 2016, ADV NEUR IN, V29
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng JJ, 2019, IEEE I CONF COMP VIS, P7022, DOI 10.1109/ICCV.2019.00712
   Deng Y., ARXIV PREPRINT ARXIV
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Feichtenhofer C, 2017, IEEE I CONF COMP VIS, P3057, DOI 10.1109/ICCV.2017.330
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Guo CX, 2019, IEEE I CONF COMP VIS, P3908, DOI 10.1109/ICCV.2019.00401
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang ZK, 2019, AAAI CONF ARTIF INTE, P8529
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kang K, 2018, IEEE T CIRC SYST VID, V28, P2896, DOI 10.1109/TCSVT.2017.2736553
   Kang K, 2017, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2017.101
   Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95
   Khodabandeh M, 2019, IEEE I CONF COMP VIS, P480, DOI 10.1109/ICCV.2019.00057
   Kong T., 2019, ARXIV190403797
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee B, 2016, LECT NOTES COMPUT SC, V9914, P68, DOI 10.1007/978-3-319-48881-3_6
   Li D, 2018, LECT NOTES COMPUT SC, V11210, P306, DOI 10.1007/978-3-030-01231-1_19
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu MS, 2018, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2018.00596
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Luo H, 2019, AAAI CONF ARTIF INTE, P8803
   Mei T., 2020, P IEEE C COMP VIS
   Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Qiu ZF, 2019, PROC CVPR IEEE, P12048, DOI 10.1109/CVPR.2019.01233
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang Tang P. P., OBJECT DETECTION VID
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang SG, 2018, IEEE T MULTIMEDIA, V20, P3148, DOI 10.1109/TMM.2018.2829602
   Wang SY, 2018, LECT NOTES COMPUT SC, V11217, P557, DOI 10.1007/978-3-030-01261-8_33
   Wu HP, 2019, IEEE I CONF COMP VIS, P9216, DOI 10.1109/ICCV.2019.00931
   Xiao FY, 2018, LECT NOTES COMPUT SC, V11212, P494, DOI 10.1007/978-3-030-01237-3_30
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang J., 2016, ILSVRC2016 OBJECT DE
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13864, DOI 10.1109/CVPR42600.2020.01388
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhu XZ, 2018, PROC CVPR IEEE, P7210, DOI 10.1109/CVPR.2018.00753
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
   Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441
NR 68
TC 34
Z9 35
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 846
EP 858
DI 10.1109/TMM.2020.2990070
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Fu, ZQ
   Shao, F
   Jiang, QP
   Meng, XC
   Ho, YS
AF Fu, Zhenqi
   Shao, Feng
   Jiang, Qiuping
   Meng, Xiangchao
   Ho, Yo-Sung
TI Subjective and Objective Quality Assessment for Stereoscopic Image
   Retargeting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stereo image processing; Databases; Three-dimensional displays;
   Measurement; Quality assessment; Visualization; Strain; Stereoscopic
   image retargeting; quality assessment; monocular retargeting
   transformation; viewpoint transformation; grid deformation; information
   loss
ID STATISTICS; SALIENCY
AB Binocular stereoscopic image retargeting (SIR) aims to adjust 3D images into target aspect ratios. In recent years, various SIR methods have been proposed, but there are few researches on visual quality assessment. As a consequence, we construct a benchmark stereoscopic image retargeting quality assessment database (NBU-SIRQA), which contains 720 stereoscopic retargeted images generated by eight representative SIR operators. Subjective test is conducted to obtain the mean opinion score (MOS) for each stereoscopic retargeted image. Additionally, we propose an objective SIRQA metric based on grid deformation and information loss (GDIL). The main idea of GDIL is to decompose the SIR operator into two transformations: monocular image retargeting transformation and viewpoint transformation. In each transformation, grid deformation and information loss are extracted simultaneously to represent image quality and 3D perception quality. Experimental results validated on our established NBU-SIRQA database show the superiority of our metric in measuring the quality of stereoscopic retargeted images over the existing approaches.
C1 [Fu, Zhenqi; Shao, Feng; Jiang, Qiuping; Meng, Xiangchao] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol GIST, Sch Informat & Commun, Gwangju 500712, South Korea.
C3 Ningbo University; Gwangju Institute of Science & Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM fuzhenqi@stu.xmu.edu.cn; haofeng@nbu.edu.cn; jiangqiuping@nbu.edu.cn;
   mengxiangchao@nbu.edu.cn; hoyo@gist.ac.kr
RI Jiang, Qiuping/AAL-8273-2020; Fu, Zhenqi/ABD-7482-2021
OI Fu, Zhenqi/0000-0003-2950-7190; HO, YO-SUNG/0000-0002-7220-1034;
   Qiuping, Jiang/0000-0002-6025-9343
FU Natural Science Foundation of China [61622109, 61901236]; Zhejiang
   Natural Science Foundation of China [R18F010008]; K. C. Wong Magna Fund
   in Ningbo University
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61622109 and 61901236, in part by the Zhejiang
   Natural Science Foundation ofChina underGrant R18F010008, and in part by
   the by K. C. Wong Magna Fund in Ningbo University.
CR Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Chen YX, 2015, NEUROCOMPUTING, V151, P645, DOI 10.1016/j.neucom.2014.05.089
   Chen ZB, 2017, IEEE T IMAGE PROCESS, V26, P5138, DOI 10.1109/TIP.2017.2736422
   Dekel T, 2013, IEEE T PATTERN ANAL, V35, P2513, DOI 10.1109/TPAMI.2013.46
   Du SP, 2013, IEEE T VIS COMPUT GR, V19, P1288, DOI 10.1109/TVCG.2013.14
   Fang YM, 2014, IEEE J EM SEL TOP C, V4, P95, DOI 10.1109/JETCAS.2014.2298919
   Fu ZQ, 2018, IEEE ACCESS, V6, P12008, DOI 10.1109/ACCESS.2018.2808322
   Guo K, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC), P1, DOI 10.1109/ICIVC.2018.8492867
   Guo YC, 2018, SIGNAL PROCESS-IMAGE, V67, P171, DOI 10.1016/j.image.2018.06.010
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hsu CC, 2014, IEEE J-STSP, V8, P377, DOI 10.1109/JSTSP.2014.2311884
   Jiang QP, 2018, IEEE T CYBERNETICS, V48, P1276, DOI 10.1109/TCYB.2017.2690452
   Jiang QP, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.4.043002
   Jung C, 2015, IEEE IMAGE PROC, P4047, DOI 10.1109/ICIP.2015.7351566
   Jung YJ, 2013, IEEE T CIRC SYST VID, V23, P2077, DOI 10.1109/TCSVT.2013.2270394
   Junle Wang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P669, DOI 10.1109/ICASSP.2014.6853680
   Karimi M, 2018, MULTIMED TOOLS APPL, V77, P13799, DOI 10.1007/s11042-017-4994-1
   Kim HG, 2019, IEEE T CIRC SYST VID, V29, P956, DOI 10.1109/TCSVT.2018.2817250
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Lee KY, 2012, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2012.6247657
   Lee K, 2014, IEEE T IMAGE PROCESS, V23, P450, DOI 10.1109/TIP.2013.2290592
   Lei JJ, 2017, IEEE T MULTIMEDIA, V19, P1442, DOI 10.1109/TMM.2017.2660440
   Li B, 2015, IEEE T IMAGE PROCESS, V24, P2811, DOI 10.1109/TIP.2015.2431441
   Liang Y, 2017, IEEE T VIS COMPUT GR, V23, P1099, DOI 10.1109/TVCG.2016.2517641
   Lin SS, 2014, IEEE T CIRC SYST VID, V24, P759, DOI 10.1109/TCSVT.2013.2291282
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu Y., 2016, P IEEE INT C MULT EX, P1
   Ma L, 2012, IEEE J-STSP, V6, P626, DOI 10.1109/JSTSP.2012.2211996
   Oliveira SAF, 2018, COMPUT VIS IMAGE UND, V168, P172, DOI 10.1016/j.cviu.2017.11.011
   Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   SHAO F, IN PRESS, DOI DOI 10.1109/TSMC.2019.2917496
   Shao F, 2017, IEEE T IMAGE PROCESS, V26, P4790, DOI 10.1109/TIP.2017.2721546
   Shao F, 2016, J DISP TECHNOL, V12, P22, DOI 10.1109/JDT.2015.2446973
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Simakov D., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587842
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Wang WG, 2017, IEEE T VIS COMPUT GR, V23, P2014, DOI 10.1109/TVCG.2016.2600594
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wolf L, 2007, IEEE I CONF COMP VIS, P1418
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yan B, 2013, IEEE T CIRC SYST VID, V23, P313, DOI 10.1109/TCSVT.2012.2203740
   Yoo JW, 2013, IEEE SIGNAL PROC LET, V20, P519, DOI 10.1109/LSP.2013.2252165
   Yue B, 2013, EURASIP J WIREL COMM, DOI 10.1186/1687-1499-2013-116
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   Zhang YB, 2016, IEEE T IMAGE PROCESS, V25, P4286, DOI 10.1109/TIP.2016.2585884
   Zhang YC, 2017, IEEE T IMAGE PROCESS, V26, P5980, DOI 10.1109/TIP.2017.2746260
NR 51
TC 9
Z9 10
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2100
EP 2113
DI 10.1109/TMM.2020.3008054
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100020
DA 2024-07-18
ER

PT J
AU Gu, LC
   Liu, J
   Liu, XX
   Sun, JD
AF Gu, Lingchen
   Liu, Ju
   Liu, Xiaoxi
   Sun, Jiande
TI Deep Loss Driven Multi-Scale Hashing Based on Pyramid Connected Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Binary codes; Kernel; Convolution; Deep learning;
   Convolutional codes; Streaming media; Deep learning; deep supervised
   hashing; image retrieval; nearest neighbor search
ID GENERAL FRAMEWORK; QUANTIZATION; IMAGE; REPRESENTATION
AB Thanks to the great success of the deep learning, deep hashing for large-scale multimedia retrieval has made significant progress recently. However, most existing deep hashing algorithms suffer from slow convergence due to the gradient vanishing problem, caused by deep network structures and saturated activation functions. Moreover, a single convolution layer is often followed by down-sampling such as max pooling, resulting in local information loss that might affect the overall system robustness and performance. In this work, we propose a novel deep supervised hashing, Deep Loss Driven Multi-Scale Hashing (DLDMSH), which learns the high-quality approximate binary codes through an end-to-end network and improves the representative capacity of hash codes for large-scale image retrieval. Specifically, we design a Loss Driven Multi-Scale (LDMS) feature which is aggregated from convolutional feature maps. Moreover, a Pyramid Connected Convolutional Neural Network (PCNet) architecture is devised to generate LDMS feature, which inputs pairs of images during the training and outputs an image to approximate discrete values. In particular, 1 x 1 convolution kernels are applied to make a linear combination of features for realizing feature reduction, and the reduced features are fused in the fusion layer. This effectively improves the performance of deep features. A novel loss function preserving semantic information is integrated into an end-to-end learning scheme, which enhances the representative capacity of binary codes. Extensive experiments over four benchmark datasets show that DLDMSH significantly outperforms several other state-of-the-art hashing methods.
C1 [Gu, Lingchen; Liu, Ju; Liu, Xiaoxi] Shandong Univ, Sch Informat Sci & Engn, Qingdao 266237, Peoples R China.
   [Sun, Jiande] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Peoples R China.
C3 Shandong University; Shandong Normal University
RP Liu, J (corresponding author), Shandong Univ, Sch Informat Sci & Engn, Qingdao 266237, Peoples R China.
EM lingchengu@mail.sdu.edu.cn; juliu@sdu.edu.cn; xiaoxiliu@mail.sdu.edu.cn;
   jiandesun@hotmail.com
RI Gu, Lingchen/GZG-8349-2022; Liu, Xiaoxi/GSM-9212-2022
FU Shandong Province Key Innovation Project [2017CXGC1504]; Natural Science
   Foundation for Distinguished Young Scholars of Shandong Province
   [JQ201718]; National Natural Science Foundation of China [U1736122]
FX This work was supported in part by the Shandong Province Key Innovation
   Project under Grant 2017CXGC1504, in part by the Natural Science
   Foundation for Distinguished Young Scholars of Shandong Province under
   Grant JQ201718, and in part by the National Natural Science Foundation
   of China under Grant U1736122.
CR Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   [Anonymous], 2012, NIPS
   [Anonymous], 2009, NIPS
   [Anonymous], 2009, NEURIPS
   Cakir F, 2019, IEEE T PATTERN ANAL, V41, P2424, DOI 10.1109/TPAMI.2019.2914897
   Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Cao Y, 2018, PROC CVPR IEEE, P1287, DOI 10.1109/CVPR.2018.00140
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Ge TZ, 2013, PROC CVPR IEEE, P2946, DOI 10.1109/CVPR.2013.379
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He K, 2018, PROC CVPR IEEE, P4023, DOI 10.1109/CVPR.2018.00423
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang QY, 2018, IEEE T IMAGE PROCESS, V27, P5996, DOI 10.1109/TIP.2018.2864894
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Jin L, 2019, IEEE T NEUR NET LEAR, V30, P1429, DOI 10.1109/TNNLS.2018.2869601
   Jin L, 2018, IEEE T IMAGE PROCESS, V27, P1405, DOI 10.1109/TIP.2017.2776745
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li H, 2016, PROCEEDINGS OF 2016 12TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P415, DOI [10.1109/CIS.2016.0101, 10.1109/CIS.2016.100]
   Li Q, 2017, ADV NEUR IN, V30
   Li WJ, 2016, IJCAI, P1711
   Li Y, 2017, IEEE SIGNAL PROC LET, V24, P609, DOI 10.1109/LSP.2017.2665522
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Liong VE, 2020, IEEE T PATTERN ANAL, V42, P580, DOI 10.1109/TPAMI.2018.2882816
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   LIU W, 2011, P INT C MACH LEARN, P1, DOI DOI 10.1109/VPPC.2011.6043110
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu XL, 2017, IEEE T IMAGE PROCESS, V26, P5324, DOI 10.1109/TIP.2017.2729896
   Liu XL, 2017, IEEE T IMAGE PROCESS, V26, P5367, DOI 10.1109/TIP.2017.2695895
   Lv Q., 2007, P 33 INT C VER LARG, P950
   Netzer Yuval, 2011, ADV NEUR INF PROC SY
   Norouzi M.E., 2011, ICML
   Qi GJ, 2016, PROC CVPR IEEE, P2267, DOI 10.1109/CVPR.2016.249
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Seddati O, 2017, IEEE INT CONF COMP V, P1246, DOI 10.1109/ICCVW.2017.150
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Song JK, 2018, AAAI CONF ARTIF INTE, P394
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song JK, 2018, PATTERN RECOGN, V75, P175, DOI 10.1016/j.patcog.2017.03.021
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Tolias G., 2015, ARXIV151105879
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang M, 2018, IEEE T IMAGE PROCESS, V27, P907, DOI 10.1109/TIP.2017.2751150
   Wang XF, 2017, LECT NOTES COMPUT SC, V10111, P70, DOI 10.1007/978-3-319-54181-5_5
   Wang XH, 2017, IEEE SIGNAL PROC LET, V24, P510, DOI 10.1109/LSP.2016.2611485
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812
   Yang YY, 2019, PROGNOST SYST HEALT, P114, DOI 10.1109/PHM-Paris.2019.00027
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   ZHANG D, 2010, P ACM SIGIR C RES DE, P18
   Zhang J, 2019, IEEE T CIRC SYST VID, V29, P212, DOI 10.1109/TCSVT.2017.2771332
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zheng F, 2018, IEEE T PATTERN ANAL, V40, P1059, DOI 10.1109/TPAMI.2016.2645565
NR 74
TC 4
Z9 4
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 939
EP 954
DI 10.1109/TMM.2020.2991513
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300009
DA 2024-07-18
ER

PT J
AU Huang, JL
   Liao, J
   Kwong, S
AF Huang, Jialu
   Liao, Jing
   Kwong, Sam
TI Semantic Example Guided Image-to-Image Translation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Visualization; Semantics; Artificial neural networks;
   Generators; Artificial neural networks; image generation; image
   representation
ID STYLE TRANSFER
AB Many image-to-image (I2I) translation problems are in nature of high diversity that a single input may have various counterparts. The multi-modal network that can build a many-to-many mapping between two visual domains has been proposed in prior works. However, most of them are guided by sampled noises. Some others encode the reference image into a latent vector, which would eliminate the semantic information of the reference image. In this work, we aim to provide a solution to control the output based on references semantically. Given a reference image and an input in another domain, we first perform semantic matching between the two visual content and generate an auxiliary image, which explicitly encourages the semantic characteristic to be preserved. A deep network then is used for I2I translation and the final outputs are expected to be semantically similar to both the input and the reference. However, few paired data can satisfy that dual-similarity in a supervised fashion, and so we build up a self-supervised framework in the training stage. We improve the quality and diversity of the outputs by employing non-local blocks and a multi-task architecture. We assess the proposed method through extensive qualitative and quantitative evaluations and also present comparisons with several state-of-the-art models.
C1 [Huang, Jialu; Liao, Jing; Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 518057, Peoples R China.
C3 City University of Hong Kong; Shenzhen Research Institute, City
   University of Hong Kong; City University of Hong Kong
RP Kwong, S (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM jialhuang8-c@my.cityu.edu.hk; jingliao@cityu.edu.hk; cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261; HUANG, Jialu/0000-0002-0516-3736; LIAO,
   Jing/0000-0001-7014-5377
FU National Natural Science Foundation of China [61672443]; Hong Kong
   GRF-RGC General Research Fund [9042322 (CityU 11200116), 9042489 (CityU
   11206317), 9042816 (CityU 11209819)]; Hong Kong ECS [21209119]; Hong
   Kong UGC; CityU of Hong Kong [7200607]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61672443, in part by Hong Kong GRF-RGC
   General Research Fund under Grants 9042322 (CityU 11200116), 9042489
   (CityU 11206317), and 9042816 (CityU 11209819), in part by Hong Kong ECS
   under Grant 21209119, Hong Kong UGC, and in part by Start-up under Grant
   7200607, CityU of Hong Kong.
CR Almahairi A, 2018, PR MACH LEARN RES, V80
   [Anonymous], 2017, Advances in neural information processing systems
   [Anonymous], 2018, NIPS
   Anonymous Gwern Branwen Aaron Gokaslan, 2019, DANBOORU2019 LARGE S
   Bansal A., 2017, ARXIV170805349
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Cheng MM, 2020, IEEE T IMAGE PROCESS, V29, P909, DOI 10.1109/TIP.2019.2936746
   Elad M, 2017, IEEE T IMAGE PROCESS, V26, P2338, DOI 10.1109/TIP.2017.2678168
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Ghosh A, 2018, PROC CVPR IEEE, P8513, DOI 10.1109/CVPR.2018.00888
   Gonzalez-Garcia A, 2018, ADV NEUR IN, V31
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han XH, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P266, DOI [10.1109/BigMM.2019.00049, 10.1109/BigMM.2019.00-13]
   Hensel M, 2017, ADV NEUR IN, V30
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson DH., 2006, SCHOLARPEDIA, V1, P2088, DOI [DOI 10.4249/SCHOLARPEDIA.2088, 10.4249/scholarpedia.2088]
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li XY, 2019, IEEE INT CON MULTI, P652, DOI 10.1109/ICME.2019.00118
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P5881, DOI 10.1109/TIP.2019.2922854
   Liao J., 2017, ACM Trans. Graph.
   Lin SB, 2019, IEEE INT CON MULTI, P1330, DOI 10.1109/ICME.2019.00231
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Liu M.-Y., 2017, NIPS
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mathieu M., 2015, PROC INT C LEARN REP
   Miyato T, 2018, INT C LEARN REPR
   Odena A., 2016, DISTILL, V1, pe3, DOI 10.23915/distill.00003.-URL
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Qi XJ, 2018, PROC CVPR IEEE, P8808, DOI 10.1109/CVPR.2018.00918
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simo-Serra E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132703
   Simonyan K., 2014, ARXIV14091556
   Soh JW, 2019, IEEE INT SYM MULTIM, P160, DOI 10.1109/ISM46123.2019.00037
   Van Gool, 2018, ARXIV180511145
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wu XH, 2019, IEEE INT CON MULTI, P1162, DOI 10.1109/ICME.2019.00203
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   You S., 2019, ARXIV PREPRINT ARXIV
   Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yuanlue Zhu, 2019, 2019 IEEE International Conference on Multimedia and Expo (ICME). Proceedings, P1198, DOI 10.1109/ICME.2019.00209
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang W, 2013, IEEE T MULTIMEDIA, V15, P1594, DOI 10.1109/TMM.2013.2265675
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
NR 54
TC 24
Z9 24
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1654
EP 1665
DI 10.1109/TMM.2020.3001536
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lin, SS
   Bai, MC
   Liu, F
   Shen, LL
   Zhou, YC
AF Lin, Shisong
   Bai, Mengchao
   Liu, Feng
   Shen, Linlin
   Zhou, Yicong
TI Orthogonalization-Guided Feature Fusion Network for Multimodal 2D+3D
   Facial Expression Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal facial expression recognition; feature fusion
ID FACE
AB As 2D and 3D data present different views of the same face, the features extracted from them can he both complementary and redundant. In this paper, we present a novel and efficient orthogonalization-guided feature fusion network, namely OGF(2)Net, to fuse the features extracted from 2D and 3D faces for facial expression recognition. While 2D texture maps are fed into a 2D feature extraction pipeline (FE2DNet), the attribute maps generated from 3D data are concatenated as input of the 3D feature extraction pipeline (FE3DNet). The two networks are separately trained at the first stage and frozen in the second stage for late feature fusion, which can well address the unavailability of a large number of 3D+2D face pairs. To reduce the redundancies among features extracted from 2D and 3D streams, we design an orthogonal loss-guided feature fusion network to orthogonalize the features before fusing them. Experimental results show that the proposed method significantly outperforms the state-of-the-art algorithms on both the BU-3DFE and Bosphorus databases. While accuracies as high as 89.05% (P1 protocol) and 89.07% (P2 protocol) are achieved on the BU-3DFE database, an accuracy of 89.28% is achieved on the Bosphorus database. The complexity analysis also suggests that our approach achieves a higher processing speed while simultaneously requiring lower memory costs.
C1 [Lin, Shisong; Bai, Mengchao; Liu, Feng; Shen, Linlin] Shenzhen Univ, Sch Comp Sci & Software Engn, Comp Vis Inst, Shenzhen 518060, Peoples R China.
   [Lin, Shisong; Bai, Mengchao; Liu, Feng; Shen, Linlin] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518172, Peoples R China.
   [Shen, Linlin; Zhou, Yicong] Univ Macau, Fac Sci & Technol, Macau 999078, Peoples R China.
C3 Shenzhen University; Shenzhen Institute of Artificial Intelligence &
   Robotics for Society; University of Macau
RP Shen, LL (corresponding author), Shenzhen Univ, Sch Comp Sci & Software Engn, Comp Vis Inst, Shenzhen 518060, Peoples R China.; Shen, LL (corresponding author), Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518172, Peoples R China.
EM linshisong2018@email.szu.edu.cn; baimengchao2017@email.szu.edu.cn;
   feng.liu@szu.edu.cn; llshen@szu.edu.cn; yicongzhou@um.edu.mo
RI Liu, Feng/JKJ-4410-2023; Shen, Linlin/AEX-9392-2022; Zhou,
   Yicong/A-8017-2009
OI Liu, Feng/0000-0003-1493-5352; Shen, Linlin/0000-0003-1420-0815; Zhou,
   Yicong/0000-0002-4487-6384
FU National Natural Science Foundation of China [61672357, 91959108,
   U1713214]; Science and Technology Project of Guangdong Province
   [2018A050501014]; Shenzhen Fundamental Research fund
   [JCYJ20190808163401646, JCYJ20180305125822769]; Science and Technology
   Development Fund, Macau SAR [189/2017/A3]; Research Committee at the
   University of Macau [MYRG2016-00123-FST, MYRG2018-00136-FST]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672357, 91959108, andU1713214, in
   part by the Science and Technology Project of Guangdong Province under
   Grant 2018A050501014, in part by the Shenzhen Fundamental Research fund
   under Grants JCYJ20190808163401646 and JCYJ20180305125822769, in part by
   the Science and Technology Development Fund, Macau SAR (File no.
   189/2017/A3), and in part by the Research Committee at the University of
   Macau underGrantsMYRG2016-00123-FST andMYRG2018-00136-FST.
CR [Anonymous], 2015, ARXIV150905371
   [Anonymous], 2008, MATLAB Central File Exchange
   [Anonymous], 2015, ARXIV151103015
   Berretti S., 2010, P INT C PATTERN RECO, P4125
   Cai J, 2018, IEEE INT CONF AUTOMA, P302, DOI 10.1109/FG.2018.00051
   Chen ZX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P229, DOI 10.1145/3240508.3240568
   Ding YY, 2017, IEEE ACCESS, V5, P19409, DOI 10.1109/ACCESS.2017.2737821
   Gilani SZ, 2018, PROC CVPR IEEE, P1896, DOI 10.1109/CVPR.2018.00203
   Gong B., 2009, Proceedings of the 17th ACM International Conference on Multimedia, P569
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Huang YZ, 2010, IEEE T MULTIMEDIA, V12, P536, DOI 10.1109/TMM.2010.2052792
   Ioffe S., 2015, P INT C LEARN REPR S
   Jan A, 2018, IEEE INT CONF AUTOMA, P466, DOI 10.1109/FG.2018.00075
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kahou SE, 2016, J MULTIMODAL USER IN, V10, P99, DOI 10.1007/s12193-015-0195-2
   Kingma D. P., 2014, arXiv
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Li HB, 2015, COMPUT VIS IMAGE UND, V140, P83, DOI 10.1016/j.cviu.2015.07.005
   Li S., 2020, IEEE T AFFECT COMPUT, DOI DOI 10.1109/TAFFC.2020.2981446
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Maalej A, 2011, PATTERN RECOGN, V44, P1581, DOI 10.1016/j.patcog.2011.02.012
   Meng ZB, 2017, IEEE INT CONF AUTOMA, P558, DOI 10.1109/FG.2017.140
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mpiperis I, 2008, IEEE T INF FOREN SEC, V3, P498, DOI 10.1109/TIFS.2008.924598
   Nguyen Hieu V., 2011, P AS C COMP VIS, P709, DOI DOI 10.1007/978-3-642-19309-5_55
   Niu Xuesong, 2019, P IEEE C COMP VIS PA
   Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424
   Qingkai Zhen, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015, January 5-7, 2015, Proceedings: LNCS 8935, P522, DOI 10.1007/978-3-319-14445-0_45
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Tawari A, 2013, IEEE T MULTIMEDIA, V15, P1543, DOI 10.1109/TMM.2013.2266635
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang SF, 2013, IEEE T AFFECT COMPUT, V4, P34, DOI 10.1109/T-AFFC.2012.32
   Wang SF, 2010, IEEE T MULTIMEDIA, V12, P682, DOI 10.1109/TMM.2010.2060716
   Wei XF, 2018, IEEE INT CONF AUTOMA, P31, DOI 10.1109/FG.2018.00015
   Yang HY, 2018, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2018.00231
   Yang HY, 2017, INT CONF AFFECT, P556, DOI 10.1109/ACII.2017.8273654
   Yang X., 2015, P 11 IEEE INT C AUT, V1, P1, DOI DOI 10.1109/FG.2015.7163090
   Yi Dong, 2014, ARXIV14117923
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Yin LJ, 2008, IEEE INT CONF AUTOMA, P116
   Zhang CS, 2017, J SYST ENG ELECTRON, V28, P784, DOI 10.21629/JSEE.2017.04.18
   Zhang KH, 2017, IEEE T IMAGE PROCESS, V26, P4193, DOI 10.1109/TIP.2017.2689999
   Zhu KK, 2019, IEEE INT CONF AUTOMA, P590, DOI 10.1109/fg.2019.8756524
NR 46
TC 12
Z9 12
U1 4
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1581
EP 1591
DI 10.1109/TMM.2020.3001497
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SN5MI
UT WOS:000658333200003
DA 2024-07-18
ER

PT J
AU Liu, H
   Guo, YL
   Ma, YN
   Lei, YJ
   Wen, GJ
AF Liu, Hao
   Guo, Yulan
   Ma, Yanni
   Lei, Yinjie
   Wen, Gongjian
TI Semantic Context Encoding for Accurate 3D Point Cloud Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Three-dimensional displays; Image segmentation; Encoding;
   Convolution; Two dimensional displays; Feature extraction; Point clouds;
   3D data; semantic segmentation; semantic context
ID NETWORKS
AB Semantic context plays a significant role in image segmentation. However, few prior works have explored semantic contexts for 3D point cloud segmentation. In this paper, we propose a simple yet effective Point Context Encoding (PointCE) module to capture semantic contexts of a point cloud and adaptively highlight intermediate feature maps. We also introduce a Semantic Context Encoding loss (SCE-loss) to supervise the network to learn rich semantic context features. To avoid hyperparameter tuning and achieve better convergence performance, we further propose a geometric mean loss to integrate both SCE-loss and segmentation loss. Our PointCE module is general and lightweight, and can be integrated into any point cloud segmentation architecture to improve its segmentation performance with only marginal extra overheads. Experimental results on the ScanNet, S3DIS and Semantic3D datasets show that consistent and significant improvement can be achieved for several different networks by integrating our PointCE module.
C1 [Liu, Hao; Guo, Yulan; Ma, Yanni] Sun Yat Sen Univ, Sch Elect & Commun Engn, Guangzhou 510275, Peoples R China.
   [Guo, Yulan; Wen, Gongjian] Natl Univ Def Technol, Coll Elect Sci & Technol, Changsha 410073, Peoples R China.
   [Lei, Yinjie] Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Peoples R China.
C3 Sun Yat Sen University; National University of Defense Technology -
   China; Sichuan University
RP Guo, YL (corresponding author), Sun Yat Sen Univ, Sch Elect & Commun Engn, Guangzhou 510275, Peoples R China.
EM liuh327@mail2.sysu.edu.cn; yulan.guo@nudt.edu.cn;
   mayn3@mail2.sysu.edu.cn; yinjie@scu.edu.cn; wengongjian@sina.com
RI guo, yu/GQZ-1392-2022; Liu, Hao/M-1010-2019; Guo, Yulan/E-7102-2014
OI Liu, Hao/0000-0001-5955-1577; Guo, Yulan/0000-0001-7051-841X
FU National Natural Science Foundation of China [61972435, 61602499];
   Natural Science Foundation of Guangdong Province [2019A1515011271];
   Science and Technology Innovation Committee of Shenzhen Municipality
   [JCYJ20190807152209394]; Key Research and Development Program of Sichuan
   Province [2019YFG0409]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61972435 and 61602499, in part by the
   Natural Science Foundation of Guangdong Province under Grant
   2019A1515011271, in part by the Science and Technology Innovation
   Committee of Shenzhen Municipality under Grant JCYJ20190807152209394,
   and in part by the Key Research and Development Program of Sichuan
   Province under Grant 2019YFG0409.
CR [Anonymous], 2016, ADV NEURAL INFORM PR
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Boulch A., 2017, P 3DOR 17 WORKSHOP 3
   Boulch A, 2020, COMPUT GRAPH-UK, V88, P24, DOI 10.1016/j.cag.2020.02.005
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai A, 2018, LECT NOTES COMPUT SC, V11214, P458, DOI 10.1007/978-3-030-01249-6_28
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Hackel T., 2017, ISPRS ANN PHOTOGRAMM, VIV-1/W1, P91
   Hackel T, 2016, ISPRS ANN PHOTO REM, V3, P177, DOI 10.5194/isprsannals-III-3-177-2016
   Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li YZ, 2018, ADV NEUR IN, V31
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Ma Y., 2020, P IEEE WINT C APPL C, P2931
   Ma YX, 2018, INT C PATT RECOG, P1560, DOI 10.1109/ICPR.2018.8546281
   Montoya-Zegarra JA, 2014, LECT NOTES COMPUT SC, V8753, P212, DOI 10.1007/978-3-319-11752-2_17
   Narita G, 2019, IEEE INT C INT ROBOT, P4205, DOI [10.1109/IROS40897.2019.8967890, 10.1109/iros40897.2019.8967890]
   Pan Hao, 2018, ARXIV180804952
   Qi C. R., 2017, Advances in neural information processing systems, P5099
   Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Rethage D., 2018, ECCV, P596
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Rosa S., 2020, 2020 P IEEECVF C COM, P11108, DOI [DOI 10.1109/CVPR42600.2020.01112, 10.1109/CVPR42600.2020.01112]
   Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Truong G., 2019, DICTA, P1
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xu Y, 2018, ADV SOC SCI EDUC HUM, V284, P87
   Ye XQ, 2018, LECT NOTES COMPUT SC, V11211, P415, DOI 10.1007/978-3-030-01234-2_25
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 49
TC 34
Z9 35
U1 7
U2 89
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2045
EP 2055
DI 10.1109/TMM.2020.3007331
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100016
DA 2024-07-18
ER

PT J
AU Liu, X
   Sun, YB
   Liu, ZW
   Lin, DH
AF Liu, Xin
   Sun, Yongbin
   Liu, Ziwei
   Lin, Dahua
TI Learning Diverse Fashion Collocation by Neural Graph Filtering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Image color analysis; Footwear; Filtering; Image edge
   detection; Fashion recommendation; neural graph filtering; deep learning
AB Fashion recommendation systems are highly desired by customers to find visually-collocated fashion items, such as clothes, shoes, bags, etc. While existing methods demonstrate promising results, they remain lacking in flexibility and diversity, e.g. assuming a fixed number of items or favoring safe but boring recommendations. In this paper, we propose a novel fashion collocation framework, Neural Graph Filtering, that models a flexible set of fashion items via a graph neural network. Specifically, we consider the visual embeddings of each garment as a node in the graph, and describe the inter-garment relationship as the edge between nodes. By applying symmetric operations on the edge vectors, this framework allows varying numbers of inputs/outputs and is invariant to their ordering. We further include a style classifier augmented with focal loss to enable the collocation of significantly diverse styles, which are inherently imbalanced in the training set. To facilitate a comprehensive study on diverse fashion collocation, we reorganize Amazon Fashion dataset with carefully designed evaluation protocols. We evaluate the proposed approach on three popular benchmarks, the Polyvore dataset, the Polyvore-D dataset, and our reorganized Amazon Fashion dataset. Extensive experimental results show that our approach significantly outperforms the state-of-the-art methods with over 10% improvements on the standard AUC metric. More importantly, 82.5% of the users prefer our diverse-style recommendations over other alternatives in a real-world perception study.
C1 [Liu, Xin] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
   [Sun, Yongbin] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Liu, Ziwei; Lin, Dahua] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
C3 Duke University; Massachusetts Institute of Technology (MIT); Chinese
   University of Hong Kong
RP Liu, ZW (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.
EM xin.liu4@duke.edu; yb_sun@mit.edu; zwliu.hust@gmail.com;
   dhlin@ie.cuhk.edu.hk
RI Sun, Yongbin/AAU-9321-2021; Lin, Dahua/W-6576-2019; Liu,
   Ziwei/AAG-6939-2021
OI Lin, Dahua/0000-0002-8865-7896; 
CR [Anonymous], 2017, P ICLR
   Atwood J, 2016, NIPS, P2001
   Boscaini D, 2016, ADV NEUR IN, V29
   Bruna J., 2013, INT C LEARNING REPRE
   Cucurull G, 2019, PROC CVPR IEEE, P12609, DOI 10.1109/CVPR.2019.01290
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Defferrard M., 2016, P 30 INT C NEURAL IN, V29, P3844
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   Hsiao WL, 2018, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR.2018.00748
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   Kang WC, 2017, IEEE DATA MINING, P207, DOI 10.1109/ICDM.2017.30
   Kipf TN, 2016, ARXIV
   Levie R, 2019, IEEE T SIGNAL PROCES, V67, P97, DOI 10.1109/TSP.2018.2879624
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Litany O, 2017, IEEE I CONF COMP VIS, P5660, DOI 10.1109/ICCV.2017.603
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Liu ZW, 2016, LECT NOTES COMPUT SC, V9906, P229, DOI 10.1007/978-3-319-46475-6_15
   Lu Z, 2019, PROC CVPR IEEE, P10554, DOI 10.1109/CVPR.2019.01081
   Malacara D, 2011, COLOR VIS COLORIMETR
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Schütt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Veit A, 2017, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2017.193
   Verma N., 2017, ABS170605206 CORR
   Vicol P, 2018, PROC CVPR IEEE, P8581, DOI 10.1109/CVPR.2018.00895
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Yan SJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P172, DOI 10.1145/3123266.3123276
   Yang X, 2019, AAAI CONF ARTIF INTE, P403
NR 37
TC 14
Z9 15
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2894
EP 2901
DI 10.1109/TMM.2020.3018021
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600028
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Weng, ZY
   Zhu, YS
AF Weng, Zhenyu
   Zhu, Yuesheng
TI Online Hashing With Bit Selection for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hash functions; Binary codes; Correlation; Image retrieval; Measurement;
   Streaming media; Online hashing; approximate nearest neighbor search;
   bit selection; target codes; image retrieval
ID NEAREST-NEIGHBOR; DEEP; QUANTIZATION; NETWORK; RECONSTRUCTION; CODES
AB Online hashing methods have been intensively investigated in semantic image retrieval due to their efficiency in learning the hash functions with one pass through the streaming data. Among the online hashing methods, those based on the target codes are usually superior to others. However, the target codes in these methods are generated heuristically in advance and cannot be learned online to capture the characteristics of the data. In this paper, we propose a new online hashing method in which the target codes are constructed according to the data characteristics and are used to learn the hash functions online. By designing a metric to select the effective bits online for constructing the target codes, the learned hash functions are resistant to the bit-flipping error. At the same time, the correlation between the hash functions is also considered in the designed metric. Hence, the hash functions have low redundancy. Extensive experiments show that our method can achieve comparable or better performance than other online hashing methods on both the static database and the dynamic database.
C1 [Weng, Zhenyu; Zhu, Yuesheng] Peking Univ, Shenzhen Grad Sch, Commun & Informat Secur Lab, Shenzhen, Peoples R China.
C3 Peking University
RP Zhu, YS (corresponding author), Peking Univ, Shenzhen Grad Sch, Commun & Informat Secur Lab, Shenzhen, Peoples R China.
EM wzytumbler@pku.edu.cn; zhuys@pku.edu.cn
RI weng, zhenyu/KIG-5518-2024
OI weng, zhenyu/0000-0001-7857-8687
FU NSFCShenzhen Robot Jointed Founding [U1613215]; Shenzhen Municipal
   Development and Reform Commission (Disciplinary Development Program for
   Data Science and Intelligent Computing); Key-Area Research and
   Development Program of Guangdong Province [2019B010137001]
FX Manuscript received August 13, 2019; revised April 18, 2020 and June 3,
   2020; accepted June 6, 2020. Date of publication June 25, 2020; date of
   current version June 25, 2021. This work was supported in part by
   NSFCShenzhen Robot Jointed Founding under Grant U1613215, in part by
   Shenzhen Municipal Development and Reform Commission (Disciplinary
   Development Program for Data Science and Intelligent Computing), and in
   part by the Key-Area Research and Development Program of Guangdong
   Province under Grant 2019B010137001. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr
   Zhu Liu.
CR Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   Bai C, 2018, NEUROCOMPUTING, V303, P60, DOI 10.1016/j.neucom.2018.04.034
   Bai JL, 2019, IEEE T MULTIMEDIA, V21, P3178, DOI 10.1109/TMM.2019.2920601
   Cakir F, 2019, IEEE T PATTERN ANAL, V41, P2424, DOI 10.1109/TPAMI.2019.2914897
   Cakir F, 2017, IEEE I CONF COMP VIS, P437, DOI 10.1109/ICCV.2017.55
   Cakir F, 2017, COMPUT VIS IMAGE UND, V156, P162, DOI 10.1016/j.cviu.2016.10.009
   Cakir F, 2015, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2015.125
   Cao Y, 2018, PROC CVPR IEEE, P1287, DOI 10.1109/CVPR.2018.00140
   Carreira-Perpiñán MA, 2015, PROC CVPR IEEE, P557, DOI 10.1109/CVPR.2015.7298654
   Crammer K, 2006, J MACH LEARN RES, V7, P551
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ercoli S, 2017, IEEE T MULTIMEDIA, V19, P2521, DOI 10.1109/TMM.2017.2697824
   Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27
   Ge TZ, 2014, LECT NOTES COMPUT SC, V8695, P250, DOI 10.1007/978-3-319-10584-0_17
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Guo YC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1767
   Ha I, 2018, BUILD ENVIRON, V140, P23, DOI 10.1016/j.buildenv.2018.05.026
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Heo JP, 2015, IEEE T PATTERN ANAL, V37, P2304, DOI 10.1109/TPAMI.2015.2408363
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Hu MQ, 2018, IEEE T IMAGE PROCESS, V27, P545, DOI 10.1109/TIP.2017.2749147
   Huang CQ, 2018, INFORM SCIENCES, V430, P331, DOI 10.1016/j.ins.2017.11.043
   Huang LK, 2018, IEEE T NEUR NET LEAR, V29, P2309, DOI 10.1109/TNNLS.2017.2689242
   Jiang QY, 2018, IEEE T IMAGE PROCESS, V27, P5996, DOI 10.1109/TIP.2018.2864894
   King-Shy Goh, 2001, Proceedings of the 2001 ACM CIKM. Tenth International Conference on Information and Knowledge Management, P395
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai ZH, 2018, IEEE T IMAGE PROCESS, V27, P6147, DOI 10.1109/TIP.2018.2867956
   Leng C, 2015, PROC CVPR IEEE, P2503, DOI 10.1109/CVPR.2015.7298865
   Li XL, 2017, AAAI CONF ARTIF INTE, P2203
   Li YS, 2018, IEEE T GEOSCI REMOTE, V56, P6521, DOI 10.1109/TGRS.2018.2839705
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Lin MB, 2019, AAAI CONF ARTIF INTE, P8722
   Lin MB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1635, DOI 10.1145/3240508.3240519
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu DH, 2015, PATTERN RECOGN LETT, V68, P22, DOI 10.1016/j.patrec.2015.08.010
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu L, 2017, PROC CVPR IEEE, P5140, DOI 10.1109/CVPR.2017.546
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu XL, 2017, IEEE T IMAGE PROCESS, V26, P5324, DOI 10.1109/TIP.2017.2729896
   Liu XL, 2017, IEEE T IMAGE PROCESS, V26, P5367, DOI 10.1109/TIP.2017.2695895
   Lu J, 2016, MACH LEARN, V103, P141, DOI 10.1007/s10994-016-5555-y
   Luo YD, 2018, PATTERN RECOGN, V75, P128, DOI 10.1016/j.patcog.2017.02.034
   Ma C, 2019, IEEE T CYBERNETICS, V49, P781, DOI 10.1109/TCYB.2017.2785621
   Ma C, 2017, IEEE T IMAGE PROCESS, V26, P1939, DOI 10.1109/TIP.2017.2675342
   Ng WWY, 2019, IEEE T CYBERNETICS, V49, P3844, DOI 10.1109/TCYB.2018.2846760
   Ng WWY, 2017, IEEE T CYBERNETICS, V47, P3814, DOI 10.1109/TCYB.2016.2582530
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Saritha RR, 2019, CLUSTER COMPUT, V22, pS4187, DOI 10.1007/s10586-018-1731-0
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen FM, 2015, IEEE T IMAGE PROCESS, V24, P1839, DOI 10.1109/TIP.2015.2405340
   Shi XS, 2016, LECT NOTES COMPUT SC, V9911, P419, DOI 10.1007/978-3-319-46478-7_26
   Silpa-Anan C., 2008, [39] C. Silpa-Anan and R. Hartley, "Optimized KD-trees for fast image descriptor matching", IEEE Conf. on Computer Vision and Pattern Recognition, 2008, pp. 1-8., P1, DOI [DOI 10.1109/CVPR.2008.4587638, 10.1109/CVPR.2008.4587638]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JK, 2018, PATTERN RECOGN, V75, P175, DOI 10.1016/j.patcog.2017.03.021
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Tang JH, 2018, PATTERN RECOGN, V75, P25, DOI 10.1016/j.patcog.2017.03.028
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wu CX, 2013, IEEE T KNOWL DATA EN, V25, P1380, DOI 10.1109/TKDE.2012.76
   Xie PT, 2015, LECT NOTES ARTIF INT, V9284, P610, DOI 10.1007/978-3-319-23528-8_38
   Xu YH, 2017, IEEE INT CON MULTI, P133, DOI 10.1109/ICME.2017.8019425
   Yang E, 2018, IEEE T NEUR NET LEAR, V29, P5292, DOI 10.1109/TNNLS.2018.2793863
   Yang EK, 2017, AAAI CONF ARTIF INTE, P1618
   Yang Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1286, DOI 10.1145/2964284.2964319
   Yang Yang, 2015, IEEE Transactions on Big Data, V1, P162, DOI 10.1109/TBDATA.2016.2516024
   Yuan X, 2018, PATTERN RECOGN, V79, P147, DOI 10.1016/j.patcog.2018.02.003
   Zhai DM, 2018, IEEE T MULTIMEDIA, V20, P675, DOI 10.1109/TMM.2017.2749160
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhang LH, 2016, IEEE T IMAGE PROCESS, V25, P840, DOI 10.1109/TIP.2015.2509244
   Zhe XF, 2019, PATTERN RECOGN, V93, P113, DOI 10.1016/j.patcog.2019.04.005
   Zhu FQ, 2017, IEEE T IMAGE PROCESS, V26, P4806, DOI 10.1109/TIP.2017.2695101
NR 78
TC 11
Z9 11
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1868
EP 1881
DI 10.1109/TMM.2020.3004962
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100004
DA 2024-07-18
ER

PT J
AU Zhang, B
   Xiao, D
   Xiang, Y
AF Zhang, Bo
   Xiao, Di
   Xiang, Yong
TI Robust Coding of Encrypted Images via 2D Compressed Sensing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Two dimensional displays; Encryption; Computational
   complexity; Robustness; 2D compressed sensing;
   encryption-then-compression; image compression; image encryption
ID 2-DIMENSIONAL RANDOM PERMUTATION
AB In many practical scenarios, image encryption should be implemented before image compression. This leads to the requirement of compressing encrypted images. Compressed sensing (CS), a breakthrough in signal processing, has been demonstrated to be an effective method for compressing encrypted images with robustness. However, for the exiting CS-based image encryption-then-compression (ETC) systems, image encryption is usually performed by using linear operations. When linear operations are used, we cannot achieve low computational complexity and high security in the meantime. To solve this problem, a novel 2D CS (2DCS) based ETC (2DCS-ETC) scheme is proposed in this paper. First, two nonlinear operations, including global random permutation (GRP) and negative-positive transformation (NPT), are utilized to encrypt the original image for high security purpose. Second, the encrypted image is compressed by using 2DCS for low computational complexity purpose. Furthermore, a gray mapping operation is embedded prior to CS encoding. Since gray mapping strategy can reduce the dynamic range of the CS samples, this strategy is also helpful for the rate distortion (R-D) performance improvement. Third, a 2D projected gradient with embedding decryption (2DPG-ED) algorithm is proposed, which can be utilized for the original image reconstruction even if the encrypted image is not sparse anymore. Compared with the previous CS-based ETC methods, the proposed approach can simultaneously achieve high security and low computational complexity with better robustness.
C1 [Zhang, Bo] Army Engn Univ, Commun NCO Acad, Chongqing 400035, Peoples R China.
   [Xiao, Di] Chongqing Univ, Coll Comp Sci, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ, Chongqing 400044, Peoples R China.
   [Xiang, Yong] Deakin Univ, Sch Informat Technol, Geelong, Vic 3125, Australia.
C3 Army Engineering University of PLA; Chongqing University; Deakin
   University
RP Xiao, D (corresponding author), Chongqing Univ, Coll Comp Sci, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ, Chongqing 400044, Peoples R China.
EM zhangboswjtu@163.com; xiaodi_cqu@hotmail.com; yxiang@deakin.edu.au
RI Zhang, Bo/GLN-6323-2022
OI Zhang, Bo/0000-0001-6035-0887; Xiang, Yong/0000-0003-3545-7863
FU Chongqing Research Program of Basic Research and Frontier Technology
   [cstc2017jcyjBX0008]; National Natural Science Foundation of China
   [61572089]; Scientific and Technological Research Program of Chongqing
   Municipal Education Commission [KJZD-K201801901]; Chongqing Postgraduate
   Education Reform Project [yjg183018]; Chongqing University Postgraduate
   Education Reform Project [cquyjg18219]; Australian Research Council
   [LP190100594]; Australian Research Council [LP190100594] Funding Source:
   Australian Research Council
FX This work was supported in part by the Chongqing Research Program of
   Basic Research and Frontier Technology under Grant cstc2017jcyjBX0008,
   in part by the National Natural Science Foundation ofChina underGrant
   61572089, in part by the Scientific and Technological Research Program
   of Chongqing Municipal Education Commission under Grant KJZD-K201801901,
   in part by the Chongqing Postgraduate Education Reform Project under
   Grant yjg183018, in part by the Chongqing University Postgraduate
   Education Reform Project under Grant cquyjg18219 and in part by the
   Australian Research Council under Grant LP190100594.
CR [Anonymous], 2009, PROC TENCON 2009 IEE
   Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P118, DOI 10.1109/MSP.2007.4286571
   Blanchard JD, 2015, IEEE T SIGNAL PROCES, V63, P528, DOI 10.1109/TSP.2014.2379665
   Bo Zhang, 2019, 2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS). Proceedings, P1914, DOI 10.1109/HPCC/SmartCity/DSS.2019.00264
   Cao YQ, 2018, IEICE T FUND ELECTR, VE101A, P526, DOI 10.1587/transfun.E101.A.526
   Chen G, 2014, SIGNAL PROCESS, V104, P15, DOI 10.1016/j.sigpro.2014.03.039
   Chen Z, 2018, IEEE T MULTIMEDIA, V20, P1610, DOI 10.1109/TMM.2017.2774004
   Chuman T, 2019, IEEE T INF FOREN SEC, V14, P1515, DOI 10.1109/TIFS.2018.2881677
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2012, IEEE T IMAGE PROCESS, V21, P494, DOI 10.1109/TIP.2011.2165289
   Eftekhari A, 2011, SIGNAL PROCESS, V91, P1589, DOI 10.1016/j.sigpro.2011.01.002
   Fang Y, 2012, SCI CHINA INFORM SCI, V55, P889, DOI 10.1007/s11432-012-4551-5
   Ghaffari A, 2009, INT CONF ACOUST SPEE, P3157, DOI 10.1109/ICASSP.2009.4960294
   Johnson M, 2004, IEEE T SIGNAL PROCES, V52, P2992, DOI 10.1109/TSP.2004.833860
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Kumar AA, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P764
   Lazzeretti R., 2008, P 16 EUR SIGN PROC C, P1
   Liu W, 2010, IEEE T IMAGE PROCESS, V19, P1097, DOI 10.1109/TIP.2009.2038773
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Pudi V, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351505
   Rivenson Y, 2009, IEEE SIGNAL PROC LET, V16, P449, DOI 10.1109/LSP.2009.2017817
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sendur L, 2002, IEEE T SIGNAL PROCES, V50, P2744, DOI 10.1109/TSP.2002.804091
   Van de Ville D, 2004, IEEE T CIRC SYST VID, V14, P892, DOI 10.1109/TCSVT.2004.828325
   Yang JY, 2008, IEEE T IMAGE PROCESS, V17, P1555, DOI 10.1109/TIP.2008.926159
   Zhang B, 2018, INT CONF SIGN PROCES, P312, DOI 10.1109/ICSP.2018.8652451
   Zhang LY, 2014, COMMUN NONLINEAR SCI, V19, P3653, DOI 10.1016/j.cnsns.2014.03.016
   Zhang XP, 2014, IEEE T MULTIMEDIA, V16, P1327, DOI 10.1109/TMM.2014.2315974
   Zhang XP, 2014, MULTIMED TOOLS APPL, V72, P489, DOI 10.1007/s11042-013-1392-1
   Zhang XP, 2012, IEEE T IMAGE PROCESS, V21, P3108, DOI 10.1109/TIP.2012.2187671
   Zhang XP, 2011, IEEE T INF FOREN SEC, V6, P53, DOI 10.1109/TIFS.2010.2099114
   Zhang YS, 2015, SIGNAL PROCESS-IMAGE, V39, P202, DOI 10.1016/j.image.2015.09.001
   Zhang YS, 2014, COMMUN NONLINEAR SCI, V19, P74, DOI 10.1016/j.cnsns.2013.06.031
   Zhao C, 2016, IEEE DATA COMPR CONF, P171, DOI 10.1109/DCC.2016.103
   Zhou JT, 2014, IEEE T INF FOREN SEC, V9, P1857, DOI 10.1109/TIFS.2014.2352455
   Zhou JT, 2014, IEEE T INF FOREN SEC, V9, P39, DOI 10.1109/TIFS.2013.2291625
NR 36
TC 49
Z9 53
U1 12
U2 82
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2656
EP 2671
DI 10.1109/TMM.2020.3014489
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600009
DA 2024-07-18
ER

PT J
AU Zhang, XX
   Wang, RG
   Chen, D
   Zhao, Y
   Gao, W
AF Zhang, Xinxin
   Wang, Ronggang
   Chen, Da
   Zhao, Yang
   Gao, Wen
TI Handling Outliers by Robust M-Estimation in Blind Image Deblurring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Deconvolution; Image restoration; Robustness; Estimation; Shape;
   Cameras; Blind image deblurring; kernel estimation; M-estimator; light
   streak; outlier
ID DECONVOLUTION; REGISTRATION
AB The major task of traditional motion deblurring methods is to estimate the blur kernel and restore the latent image. In low-light conditions, the pointolite is likely to produce saturated light streaks in captured blurred images. The light streaks are usually double-edged swords-outliers to the deconvolution, but a cue to kernel estimation. In this paper, we propose a novel blind motion deblurring method for blurred images including light streaks. The main idea is to model the non-linear blur caused by outliers as the Huber's M-estimation in blind deconvolution and take the shape of the light streak as a cue to estimate the blur kernel. Specifically, the optimal light streak patch is selected automatically according to the characteristics of light streaks and the blur kernel. This simple yet effective selection strategy solves the problems of false detection of candidate light streaks and optimal light streak in existing methods. Then, the optimal light streak patch is parameterized as a prior and is combined with other regularizers to estimate the blur kernel. Compared with the state-of-the-art kernel estimation methods, the proposed algorithm reduces the influence of outliers on deconvolution and utilizes more information. Thus, the restored image is more accurate. Experimental results on both synthetic and real images demonstrate the high accuracy of our algorithm.
C1 [Zhang, Xinxin] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong 999077, Peoples R China.
   [Zhang, Xinxin] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Wang, Ronggang] Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
   [Chen, Da] Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Jinan 250013, Peoples R China.
   [Zhao, Yang] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
   [Gao, Wen] Peking Univ, Dept Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 Chinese University of Hong Kong; Shandong University; Peking University;
   Qilu University of Technology; Hefei University of Technology; Peking
   University
RP Wang, RG (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
EM xxzhangsdu@sdu.edu.cn; rgwang@pkusz.edu.cn; dachen.cn@hotmail.com;
   zhaoyang@pkusz.edu.cn; wgao@pku.edu.cn
RI Zhang, Xinxin/HZJ-1742-2023
OI Zhang, Xinxin/0000-0001-6069-5391; Wang, Ronggang/0000-0003-0873-0465
FU National Natural Science Foundation of China [61672063, 62072013];
   Shenzhen Research Projects [JCYJ20180503182128089, 201806080921419290]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672063 and 62072013 and in part by
   Shenzhen Research Projects JCYJ20180503182128089 and 201806080921419290.
   The associate editor coordinating the review of this manuscript and
   approving it for publicationwas Dr. Xiaoqing Zhu.
CR Arya KV, 2007, PATTERN RECOGN LETT, V28, P1957, DOI 10.1016/j.patrec.2007.05.006
   Awange J.L., 1999, SURV REV, V35, P146, DOI DOI 10.1179/SRE.1999.35.273.146
   Cho S, 2011, IEEE I CONF COMP VIS, P495, DOI 10.1109/ICCV.2011.6126280
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Dong JX, 2018, LECT NOTES COMPUT SC, V11215, P777, DOI 10.1007/978-3-030-01252-6_46
   Dong JX, 2017, IEEE I CONF COMP VIS, P2497, DOI 10.1109/ICCV.2017.271
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   FISH DA, 1995, J OPT SOC AM A, V12, P58, DOI 10.1364/JOSAA.12.000058
   Fouad MM, 2012, IEEE T IMAGE PROCESS, V21, P1046, DOI 10.1109/TIP.2011.2167344
   Fox John, 2002, An R and S-PLUS Companion to Applied Regression: A Web Appendix to the Book
   Fuchs H., 1982, MANUSCR GEODAET, V7, P151
   GEMAN D, 1995, IEEE T IMAGE PROCESS, V4, P932, DOI 10.1109/83.392335
   Gong D, 2018, NEUROCOMPUTING, V310, P190, DOI 10.1016/j.neucom.2018.05.025
   Han Y, 2019, SIGNAL PROCESS, V155, P14, DOI 10.1016/j.sigpro.2018.09.032
   HOLLAND PW, 1977, COMMUN STAT A-THEOR, V6, P813, DOI 10.1080/03610927708827533
   Hu Z, 2018, IEEE T PATTERN ANAL, V40, P2329, DOI 10.1109/TPAMI.2017.2768365
   Hu Z, 2014, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR.2014.432
   Hu Z, 2012, LECT NOTES COMPUT SC, V7576, P59, DOI 10.1007/978-3-642-33715-4_5
   Hua BS, 2011, IEEE IMAGE PROC, P1553, DOI 10.1109/ICIP.2011.6115743
   Krishnan D., 2009, ACM T GRAPHIC, P1033, DOI DOI 10.1145/1531326.1531402
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Li LRH, 2019, INT J COMPUT VISION, V127, P1025, DOI 10.1007/s11263-018-01146-0
   Liu HF, 2015, IEEE T IMAGE PROCESS, V24, P4637, DOI 10.1109/TIP.2015.2461445
   Liu XP, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.2.023016
   Liu YM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508391
   Mustaniemi J, 2019, IEEE WINT CONF APPL, P1914, DOI 10.1109/WACV.2019.00208
   Nimisha T. M., 2018, P EUR C COMP VIS ECC, P353
   Pan JS, 2019, IEEE T PATTERN ANAL, V41, P1412, DOI 10.1109/TPAMI.2018.2832125
   Pan JS, 2017, IEEE I CONF COMP VIS, P1077, DOI 10.1109/ICCV.2017.122
   Pan JS, 2016, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2016.306
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Pan JS, 2013, SIGNAL PROCESS-IMAGE, V28, P1156, DOI 10.1016/j.image.2013.05.001
   Peter DJ, 2010, J COMPUT SCI TECH-CH, V25, P623, DOI 10.1007/s11390-010-9351-z
   Rabie T, 2005, IEEE T IMAGE PROCESS, V14, P1755, DOI 10.1109/TIP.2005.857276
   Ruckstuhl A., 2014, LECT NOTES, P40
   Sekko E, 1999, SIGNAL PROCESS, V72, P23, DOI 10.1016/S0165-1684(98)00161-3
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Sim H, 2019, IEEE COMPUT SOC CONF, P2140, DOI 10.1109/CVPRW.2019.00267
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Whyte O, 2014, INT J COMPUT VISION, V110, P185, DOI 10.1007/s11263-014-0727-3
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Xu Peiliang, 1989, Bulletin Geodesique, V63, P237, DOI 10.1007/BF02520474
   Xu XY, 2018, IEEE T IMAGE PROCESS, V27, P194, DOI 10.1109/TIP.2017.2753658
   Xue F, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2014.2380174
   Xue F, 2013, IEEE T IMAGE PROCESS, V22, P1954, DOI 10.1109/TIP.2013.2240004
   Yu X, 2014, IEEE T MULTIMEDIA, V16, P1510, DOI 10.1109/TMM.2014.2321734
   Zhang XX, 2015, IEEE IMAGE PROC, P138, DOI 10.1109/ICIP.2015.7350775
NR 50
TC 5
Z9 5
U1 3
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3215
EP 3226
DI 10.1109/TMM.2020.3021989
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000021
DA 2024-07-18
ER

PT J
AU Zuo, YF
   Fang, YM
   An, P
   Shang, XW
   Yang, JN
AF Zuo, Yifan
   Fang, Yuming
   An, Ping
   Shang, Xiwu
   Yang, Junnan
TI Frequency-Dependent Depth Map Enhancement via Iterative Depth-Guided
   Affine Transformation and Intensity-Guided Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color; Image edge detection; Image resolution; Optimization; Robustness;
   Encoding; Dictionaries; Intensity-guided depth map enhancement;
   depth-guided intensity image filtering; deep convolutional neual
   network; residual learning; dense connection
ID SUPERRESOLUTION; RECOVERY
AB Recently, deep convolutional neural network sho-ws significant improvement for intensity-guided depth map enhancement. The most networks focus on either increasing depth or easing features propagation via residual learning and dense connection. However, it has not been explicitly considered yet to mitigate the artifacts caused by the differences of the distributions between the depth map and the corresponding color image, e.g., edge misalignment. In this paper, a novel depth-guided affine transformation is used to filter out the unrelated intensity features, which is further used to refine the depth features. Since the quality of initial depth features is low, the depth-guided intensity features filtering and the intensity-guided depth features refinement are iteratively performed, which progressively promotes effects of such tasks. To make full use of the iterations, all the refined depth features are dense connected followed by a 1 x 1 convolution layer. In addition, to improve the performance in the case of large upsampling factors (e.g., 16x), the depth features are enhanced from coarse to fine. In each frequency-dependent refinement of the depth features, the above iterative subnetwork as well as the residual learning are introduced. The proposed method is tested for the noise-free and noisy cases which compares against 16 state-of-the-art methods. Our experimental results show the improved performances based on the qualitative and quantitative evaluations.
C1 [Zuo, Yifan; Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Management, Nanchang 330013, Jiangxi, Peoples R China.
   [An, Ping] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Shang, Xiwu] Shanghai Univ Engn Sci, Sch Elect & Elect Engn, Shanghai 201620, Peoples R China.
   [Yang, Junnan] Univ Technol Sydney, Fac Engn & Informat Technol, Ultimo, NSW 2007, Australia.
C3 Jiangxi University of Finance & Economics; Shanghai University; Shanghai
   University of Engineering Science; University of Technology Sydney
RP Fang, YM (corresponding author), Jiangxi Univ Finance & Econ, Sch Informat Management, Nanchang 330013, Jiangxi, Peoples R China.
EM kenny0410@126.com; leo.fangyuming@foxmail.com; anping@shu.edu.cn;
   dxsxw@126.com; junnan.yang@student.uts.edu.au
RI Zuo, Yifan/JVZ-3041-2024
OI Zuo, Yifan/0000-0003-4980-7211
FU "2030 Megaproject" New Generation Artificial Intelligence
   [2018AAA0100601]; National Natural Science Foundation of China
   [61901197, 61822109]; Natural Science Foundation of Jiangxi Province
   [20192BAB217005, 20181BBH80002]; Foundation of Jiangxi Provincial
   Department of Education [GJJ190288]; Jiangxi province postdoctoral
   research projects [9KY52]; Double Thousand Plan of Jiangxi Province
FX This work was supported in part by "2030 Megaproject" New Generation
   Artificial Intelligence under Grant 2018AAA0100601, in part by
   theNational Natural Science Foundation of China under Grants 61901197
   and 61822109, in part by the Double Thousand Plan of Jiangxi Province,
   Natural Science Foundation of Jiangxi Province, under Grants
   20192BAB217005 and 20181BBH80002, in part by the Foundation of Jiangxi
   Provincial Department of Education under Grant GJJ190288, and in part by
   Jiangxi province postdoctoral research projects under Grant 9KY52.
CR [Anonymous], Middlebury Datasets [Online] (n.d.)
   [Anonymous], TensorFlow
   Bose NK, 2006, IEEE T IMAGE PROCESS, V15, P2239, DOI 10.1109/TIP.2006.877406
   Chen BL, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1473, DOI 10.1109/ICASSP.2018.8462043
   Choi O, 2014, IEEE T IMAGE PROCESS, V23, P3321, DOI 10.1109/TIP.2014.2329766
   Deng Xin, 2019, IEEE T CIRCUITS SYST
   Diebel J., 2005, P 18 INT C NEUR INF, V18, P291
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2017, IEEE T MULTIMEDIA, V19, P293, DOI 10.1109/TMM.2016.2613824
   Ferstl D, 2015, IEEE I CONF COMP VIS, P513, DOI 10.1109/ICCV.2015.66
   Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127
   Garcia F, 2012, IEEE J-STSP, V6, P425, DOI 10.1109/JSTSP.2012.2207090
   He K., 2010, Eccv, P1
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hua KL, 2016, IEEE MULTIMEDIA, V23, P72, DOI 10.1109/MMUL.2015.52
   Huang LQ, 2019, IEEE SIGNAL PROC LET, V26, P1723, DOI 10.1109/LSP.2019.2944646
   Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Kiechle M, 2013, IEEE I CONF COMP VIS, P1545, DOI 10.1109/ICCV.2013.195
   Kingma D. P., 2014, arXiv
   Kolb A, 2010, COMPUT GRAPH FORUM, V29, P141, DOI 10.1111/j.1467-8659.2009.01583.x
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Kwon H, 2015, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2015.7298611
   Li Y, 2016, LECT NOTES COMPUT SC, V9907, P717, DOI 10.1007/978-3-319-46487-9_44
   Liu MY, 2013, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2013.29
   Liu W, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2612826
   Liu XM, 2019, IEEE T IMAGE PROCESS, V28, P1636, DOI 10.1109/TIP.2018.2875506
   Lo KH, 2018, IEEE T CYBERNETICS, V48, P371, DOI 10.1109/TCYB.2016.2637661
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600
   Min DB, 2012, IEEE T IMAGE PROCESS, V21, P1176, DOI 10.1109/TIP.2011.2163164
   Park J, 2014, IEEE T IMAGE PROCESS, V23, P5559, DOI 10.1109/TIP.2014.2361034
   Riegler G., 2016, P BRIT MACH VIS C
   Riegler G, 2016, LECT NOTES COMPUT SC, V9907, P268, DOI 10.1007/978-3-319-46487-9_17
   Song WF, 2020, IEEE T MULTIMEDIA, V22, P1220, DOI 10.1109/TMM.2019.2941776
   Song XB, 2019, IEEE T CIRC SYST VID, V29, P2323, DOI 10.1109/TCSVT.2018.2866399
   Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844
   Wang ZF, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON FLUID POWER AND MECHATRONICS - FPM 2015, P370, DOI 10.1109/FPM.2015.7337142
   Wen Y, 2019, IEEE T IMAGE PROCESS, V28, P994, DOI 10.1109/TIP.2018.2874285
   Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211
   Xie J, 2015, IEEE T MULTIMEDIA, V17, P1525, DOI 10.1109/TMM.2015.2457678
   Yang JY, 2019, IEEE T BROADCAST, V65, P123, DOI 10.1109/TBC.2018.2818405
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Yanjie Li, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P152, DOI 10.1109/ICME.2012.30
   Yin B., 2019, IEEE T MULTIMEDIA
   Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767
   Zhang YB, 2020, IEEE T CIRC SYST VID, V30, P320, DOI 10.1109/TCSVT.2018.2890574
   Zuo Y, 2017, IEEE INT CON MULTI, P211, DOI 10.1109/ICME.2017.8019366
   Zuo Y., 2016, P IEEE INT C MULT EX, P1
   Zuo YF, 2020, IEEE T CIRC SYST VID, V30, P297, DOI 10.1109/TCSVT.2018.2890271
   Zuo YF, 2019, INFORM SCIENCES, V495, P52, DOI 10.1016/j.ins.2019.05.003
   Zuo YF, 2018, IEEE T IMAGE PROCESS, V27, P4145, DOI 10.1109/TIP.2018.2828335
   Zuo YF, 2018, IEEE T CIRC SYST VID, V28, P439, DOI 10.1109/TCSVT.2016.2609438
NR 53
TC 17
Z9 19
U1 5
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 772
EP 783
DI 10.1109/TMM.2020.2987706
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QI9PL
UT WOS:000619321200003
DA 2024-07-18
ER

PT J
AU Hao, SJ
   Han, X
   Guo, YR
   Xu, X
   Wang, M
AF Hao, Shijie
   Han, Xu
   Guo, Yanrong
   Xu, Xin
   Wang, Meng
TI Low-Light Image Enhancement With Semi-Decoupled Decomposition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Low-light images; image enhancement; Retinex model
ID CONTRAST ENHANCEMENT; ILLUMINATION; FUSION
AB Low-light image enhancement is important for high-quality image display and other visual applications. However, it is a challenging task as the enhancement is expected to improve the visibility of an image while keeping its visual naturalness. Retinex-based methods have well been recognized as a representative technique for this task, but they still have the following limitations. First, due to less-effective image decomposition or strong imaging noise, various artifacts can still be brought into enhanced results. Second, although the priori information can be explored to partially solve the first issue, it requires to carefully model the priori by a regularization term and usually makes the optimization process complicated. In this paper, we address these issues by proposing a novel Retinex-based low-light image enhancement method, in which the Retinex image decomposition is achieved in an efficient semi-decoupled way. Specifically, the illumination layer I is gradually estimated only with the input image S based on the proposed Gaussian Total Variation model, while the reflectance layer R is jointly estimated by S and the intermediate I. In addition, the imaging noise can be simultaneously suppressed during the estimation of R. Experimental results on several public datasets demonstrate that our method produces images with both higher visibility and better visual quality, which outperforms the state-of-the-art low-light enhancementmethods in terms of several objective and subjective evaluation metrics.
C1 [Hao, Shijie; Han, Xu; Guo, Yanrong; Wang, Meng] Hefei Univ Technol, Minist Educ, Key Lab Knowledge Engn Big Data, Hefei 230009, Peoples R China.
   [Hao, Shijie; Han, Xu; Guo, Yanrong; Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
   [Xu, Xin] Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430081, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology; Wuhan
   University of Science & Technology
RP Hao, SJ (corresponding author), Hefei Univ Technol, Minist Educ, Key Lab Knowledge Engn Big Data, Hefei 230009, Peoples R China.
EM hfut.hsj@gmail.com; xuhan@mail.hfut.edu.cn; yrguo@hfut.edu.cn;
   xuxin@wust.edu.cn; eric.mengwang@gmail.com
RI Xu, Xin/JRW-5800-2023; Wang, Meng/ITR-8699-2023
OI Hao, Shijie/0000-0003-3181-1220; han, xu/0000-0003-4749-8427; Xu,
   Xin/0000-0003-0748-3669
FU National Nature Science Foundation of China [2018YFB0804203]; 
   [61772171];  [61702156];  [61632007]
FX Thisworkwas supported in part by theNationalKey Research andDevelopment
   Program underGrant 2018YFB0804203, and in part by the National Nature
   Science Foundation of China under Grants 61772171, 61702156, and
   61632007. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Jingdong Wang.
CR [Anonymous], 2011, Proceedings of the Design, Automation Test in Europe
   Barrett R, 1994, TEMPLATES SOLUTION L, DOI DOI 10.1137/1.9781611971538
   Cai BL, 2017, IEEE IMAGE PROC, P250, DOI 10.1109/ICIP.2017.8296281
   Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guo XD, 2018, POLYM ADVAN TECHNOL, V29, P2665, DOI 10.1002/pat.4380
   Guo XJ, 2020, IEEE T PATTERN ANAL, V42, P694, DOI 10.1109/TPAMI.2018.2883553
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2019, MULTIMED TOOLS APPL, V78, P3817, DOI 10.1007/s11042-018-6257-1
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jiang Y., 2019, ARXIV190606972
   Kinoshita Y, 2019, IEEE T IMAGE PROCESS, V28, P4101, DOI 10.1109/TIP.2019.2906501
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li Y, 2014, LECT NOTES COMPUT SC, V8690, P174, DOI 10.1007/978-3-319-10605-2_12
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren X., 2018, P IEEE INT S CIRC SY, P1, DOI [10.1109/IS CAS.2018.8351427., DOI 10.1109/ISCAS.2018.8351427]
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Vardulakis A, 2018, INT C CONTROL DECISI, P19, DOI 10.1109/CoDIT.2018.8394798
   Wang QH, 2016, IEEE IMAGE PROC, P4077, DOI 10.1109/ICIP.2016.7533126
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Xu CM, 2018, LECT NOTES COMPUT SC, V11165, P3, DOI 10.1007/978-3-030-00767-6_1
   Xu L, 2012, EPL-EUROPHYS LETT, V100, DOI 10.1209/0295-5075/100/34001
   Yang H, 2019, IEEE T VIS COMPUT GR, V25, P2953, DOI 10.1109/TVCG.2018.2865555
   Yue HJ, 2017, IEEE T IMAGE PROCESS, V26, P3981, DOI 10.1109/TIP.2017.2703078
   Zhang HW, 2014, IEEE T IMAGE PROCESS, V23, P2996, DOI 10.1109/TIP.2014.2325784
   Zhang H, 2017, J CHEM-NY, V2017, DOI 10.1155/2017/4513410
   Zhang Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P582, DOI 10.1145/3240508.3240595
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
NR 44
TC 174
Z9 186
U1 7
U2 68
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3025
EP 3038
DI 10.1109/TMM.2020.2969790
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700001
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zhang, HZ
   Dong, LS
   Gao, GY
   Hu, H
   Wen, YG
   Guan, K
AF Zhang, Huaizheng
   Dong, Linsen
   Gao, Guanyu
   Hu, Han
   Wen, Yonggang
   Guan, Kyle
TI DeepQoE: A Multimodal Learning Framework for Video Quality of Experience
   (QoE) Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of experience; Feature extraction; Task analysis; Streaming
   media; Machine learning; Predictive models; Measurement; Video quality
   of experience; deep learning; feature; representation; adaptive video
   streaming
ID MODELS
AB Recently, many models have been developed to predict video Quality of Experience (QoE), yet the applicability of these models still faces significant challenges. Firstly, many models rely on features that are unique to a specific dataset and thus lack the capability to generalize. Due to the intricate interactions among these features, a unified representation that is independent of datasets with different modalities is needed. Secondly, existing models often lack the configurability to perform both classification and regression tasks. Thirdly, the sample size of the available datasets to develop these models is often very small, and the impact of limited data on the performance of QoE models has not been adequately addressed. To address these issues, in this work we develop a novel and end-to-end framework termed as DeepQoE. The proposed framework first uses a combination of deep learning techniques, such as word embedding and 3D convolutional neural network (C3D), to extract generalized features. Next, these features are combined and fed into a neural network for representation learning. A learned representation will then serve as input for classification or regression tasks. We evaluate the performance of DeepQoE with three datasets. The results show that for small datasets (e.g., WHU-MVQoE2016 and Live-Netflix Video Database), the performance of state-of-the-art machine learning algorithms is greatly improved by using the QoE representation from DeepQoE (e.g., 35.71% to 44.82%); while for the large dataset (e.g., VideoSet), our DeepQoE framework achieves significant performance improvement in comparison to the best baseline method (90.94% vs. 82.84%). In addition to the much improved performance, DeepQoE has the flexibility to fit different datasets, to learn QoE representation, and to perform both classification and regression problems. We also develop a DeepQoE based adaptive bitrate streaming (ABR) system to verify that our framework can be easily applied to multimedia communication service. The software package of the DeepQoE framework has been released to facilitate the current research on QoE.
C1 [Zhang, Huaizheng; Dong, Linsen; Wen, Yonggang] Nanyang Technol Univ, Sch Comp Sci & Engn, Nanyang Ave, Singapore 639798, Singapore.
   [Gao, Guanyu] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Hu, Han] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
   [Guan, Kyle] Nokia Bell Labs, Holmdel, NJ 07733 USA.
C3 Nanyang Technological University; Nanjing University of Science &
   Technology; Beijing Institute of Technology; Nokia Corporation; Nokia
   Bell Labs
RP Guan, K (corresponding author), Nokia Bell Labs, Holmdel, NJ 07733 USA.
EM huaizhen001@ntu.edu.sg; linsen001@ntu.edu.sg; gygao@njust.edu.cn;
   hhu@bit.edu.cn; ygwen@ntu.edu.sg; kyle.guan@nokia.com
RI Zhang, Huaizheng/JUU-2766-2023; Wen, Yonggang/P-9406-2017; Gao,
   Guanyu/ACR-3456-2022
OI Zhang, Huaizheng/0000-0002-0153-6400; Wen, Yonggang/0000-0002-2751-5114;
   Hu, Han/0000-0001-7532-0496; Dong, Linsen/0000-0002-0315-1125
FU Microsoft Research Asia [FY18-Research-Theme-051]; DSAIR@NTU; BSEWWT
   project fund from Singapore National Research Foundation
   [BSEWWT2017_2_06]; National Natural Science Foundation of China (NSFC)
   [61971457]
FX This work was supported in part and jointly by a gift fund from
   Microsoft Research Asia (Ref. FY18-Research-Theme-051), a project fund
   from DSAIR@NTU, and a BSEWWT project fund from Singapore National
   Research Foundation, administrated through theBSEWWTprogram office (Ref.
   BSEWWT2017_2_06), and in part by National Natural Science Foundation of
   China (NSFC) under Grant 61971457.
CR Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
   [Anonymous], 1999, Subjective Video Quality Assessment Methods for Multimedia Applications
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Bampis C. G., 2017, arXiv:1703.00633
   Bampis CG, 2018, IEEE T IMAGE PROCESS, V27, P3316, DOI 10.1109/TIP.2018.2815842
   Bampis CG, 2017, IEEE T IMAGE PROCESS, V26, P5217, DOI 10.1109/TIP.2017.2729891
   Bohez S, 2017, IEEE INT C INT ROBOT, P2365, DOI 10.1109/IROS.2017.8206048
   Bovik AC, 2013, P IEEE, V101, P2008, DOI 10.1109/JPROC.2013.2257632
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang HS, 2018, IEEE T MULTIMEDIA, V20, P3337, DOI 10.1109/TMM.2018.2831639
   Chen YJ, 2015, IEEE COMMUN SURV TUT, V17, P1126, DOI 10.1109/COMST.2014.2363139
   Cisco S. J., 2017, CISC VIS NETW IND GL
   Cubelos J, 2020, IEEE T MULTIMEDIA, V22, P69, DOI 10.1109/TMM.2019.2924575
   Deng X., 2014, 2014 Montreal, P1, DOI DOI 10.1109/VTCFALL.2014.6965834
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Gao GY, 2018, IEEE T MULTIMEDIA, V20, P3399, DOI 10.1109/TMM.2018.2838330
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huynh-Thu Q, 2008, IEEE T BROADCAST, V54, P641, DOI 10.1109/TBC.2008.2001246
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Khan A, 2012, IEEE T MULTIMEDIA, V14, P431, DOI 10.1109/TMM.2011.2176324
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P965, DOI 10.1109/TMM.2017.2757761
   Li J, 2013, INT WORK QUAL MULTIM, P46, DOI 10.1109/QoMEX.2013.6603207
   Li Z, 2016, SCI REP-UK, V6, DOI 10.1038/srep30338
   Lillicrap Timothy P, 2015, ARXIV150902971
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Maia OB, 2015, COMPUT COMMUN, V57, P1, DOI 10.1016/j.comcom.2014.11.005
   Malekmohamadi H, 2012, IEEE INT CONF MULTI, P581, DOI 10.1109/ICMEW.2012.107
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mikolov T., 2013, P 26 INT C NEURAL IN, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Mnih V, 2013, ARXIV
   Mnih V, 2016, PR MACH LEARN RES, V48
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mok R.K., 2011, PROC ACM SIGCOMM WOR, P31, DOI DOI 10.1145/2018602.2018611
   Mok RKP, 2017, IEEE T MULTIMEDIA, V19, P530, DOI 10.1109/TMM.2016.2619901
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Paszke A, 2019, ADV NEUR IN, V32
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Radford Alec, 2018, IMPROVING LANGUAGE U, DOI DOI 10.18653/V1/N18-1202
   Ren S., 2017, IEEETransactionsonPatternAnalysisandMachineIntelligence, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sidaty NO, 2014, 2014 5TH EUROPEAN WORKSHOP ON VISUAL INFORMATION PROCESSING (EUVIP 2014)
   Simonyan K, 2014, ADV NEUR IN, V27
   Song JR, 2016, IEEE T MULTIMEDIA, V18, P444, DOI 10.1109/TMM.2016.2520090
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tasaka S, 2017, IEEE T MULTIMEDIA, V19, P1195, DOI 10.1109/TMM.2017.2652064
   Wang HQ, 2017, J VIS COMMUN IMAGE R, V46, P292, DOI 10.1016/j.jvcir.2017.04.009
   Wang ZH, 2014, METHOD ENZYMOL, V548, P1, DOI 10.1016/B978-0-12-397918-6.00001-X
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Yu L, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION, CYBERNETICS AND COMPUTATIONAL SOCIAL SYSTEMS (ICCSS), P1, DOI 10.1109/ICCSS.2017.8091372
   Zhang HX, 2018, PROCEEDINGS OF ICRCA 2018: 2018 THE 3RD INTERNATIONAL CONFERENCE ON ROBOTICS, CONTROL AND AUTOMATION / ICRMV 2018: 2018 THE 3RD INTERNATIONAL CONFERENCE ON ROBOTICS AND MACHINE VISION, P1, DOI 10.1145/3265639.3265642
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhang Y., 2016, WHU MVQOE2016 QUALIT
   Zhao TS, 2017, IEEE COMMUN SURV TUT, V19, P285, DOI 10.1109/COMST.2016.2619982
NR 63
TC 38
Z9 38
U1 4
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3210
EP 3223
DI 10.1109/TMM.2020.2973828
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700015
DA 2024-07-18
ER

PT J
AU Fu, X
   Zhao, Y
   Wei, YC
   Zhao, YF
   Wei, SK
AF Fu, Xin
   Zhao, Yao
   Wei, Yunchao
   Zhao, Yufeng
   Wei, Shikui
TI Rich Features Embedding for Cross-Modal Retrieval: A Simple Baseline
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Image representation; Visualization; Task analysis;
   Correlation; Training; Data models; Rich features embedding; image-text
   matching; deep representation learning; cross-modal retrieval
AB During the past few years, significant progress has been made on cross-modal retrieval, benefiting from the development of deep neural networks. Meanwhile, the overall frameworks are becoming more and more complex, making the training as well as the analysis more difficult. In this paper, we provide a Rich Features Embedding (RFE) approach to tackle the cross-modal retrieval tasks in a simple yet effective way. RFE proposes to construct rich representations for both images and texts, which is further leveraged to learn the rich features embedding in the common space according to a simple hard triplet loss. Without any bells and whistles in constructing complex components, the proposed RFE is concise and easy to implement. More importantly, our RFE obtains the state-of-the-art results on several popular benchmarks such as MS COCO and Flickr 30 K. In particular, the image-to-text and text-to-image retrieval achieve 76.1% and 61.1% (R@1) on MS COCO, which outperform others more than 3.4% and 2.3%, respectively. We hope our RFE will serve as a solid baseline and help ease future research in cross-modal retrieval.
C1 [Fu, Xin; Zhao, Yao; Wei, Shikui] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Fu, Xin; Zhao, Yao; Wei, Shikui] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Wei, Yunchao] Univ Illinois, Beckman Inst, Champaign, IL 61801 USA.
   [Zhao, Yufeng] China Acad Chinese Med Sci, Inst Basic Res Clin Med, Beijing 100700, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; University of
   Illinois System; University of Illinois Urbana-Champaign; Institute of
   Basic Research In Clinical Medicine, CACMS; China Academy of Chinese
   Medical Sciences
RP Zhao, Y (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM xinfu@bjtu.edu.cn; yzhao@bjtu.edu.cn; yunchao@illinois.edu;
   snowmanzhao@163.com; shkwei@bjtu.edu.cn
OI Fu, Xin/0000-0002-6212-6499; Zhao, Yao/0000-0002-8581-9554
FU National Key Research and Development of China [2016YFB0800404];
   National Science Foundation of China [U1936212, 61532005, 61972022];
   Program of China Scholarships Council [201807095006]; Fundamental
   Research Funds for the Central Universities [2018JBZ001, 2018YJS028]
FX This work was supported in part by the National Key Research and
   Development of China under Grant 2016YFB0800404, in part by the National
   Science Foundation of China under Grants U1936212, 61532005, and
   61972022, in part by Program of China Scholarships Council under Grant
   201807095006, and in part by the Fundamental Research Funds for the
   Central Universities under Grants 2018JBZ001 and 2018YJS028.
CR Andrew G., 2013, P INT C MACH LEARN, P1247
   [Anonymous], 2017, IEEE INT CON MULTI
   [Anonymous], 2017, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2016.2610969, DOI 10.1109/TPAMI.2016.2610969]
   [Anonymous], 2016, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-46475-6_17
   Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201
   Faghri F., 2018, P BRIT MACH VIS C
   Fukui A., 2016, P EMP METH NAT LANG, P457
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu D, 2019, INT CONF ACOUST SPEE, P3941, DOI 10.1109/ICASSP.2019.8683898
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Huang YF, 2017, IEEE INT CONF COMP V, P2313, DOI 10.1109/ICCVW.2017.273
   Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Joe Yue-Hei Ng, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P53, DOI 10.1109/CVPRW.2015.7301272
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Lee K.-H., 2018, P AUSTR C ROB AUT LI
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu RY, 2018, IEEE MULTIMEDIA, V25, P71, DOI 10.1109/MMUL.2018.112142537
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Mao J., 2015, P INT C LEARN REPR
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Paszke A, 2019, ADV NEUR IN, V32
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P5585, DOI 10.1109/TIP.2018.2852503
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Vendrov I., 2016, P INT C LEARN REPR
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang SH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1398, DOI 10.1145/3240508.3240535
   Wehrmann J, 2018, PROC CVPR IEEE, P7718, DOI 10.1109/CVPR.2018.00805
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Wei YC, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2775109
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yang Y, 2019, IEEE T KNOWL DATA EN, V31, P757, DOI 10.1109/TKDE.2018.2842190
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Zhang L, 2014, IEEE T KNOWL DATA EN, V26, P2745, DOI 10.1109/TKDE.2014.2313866
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhang X, 2018, LECT NOTES COMPUT SC, V11219, P614, DOI 10.1007/978-3-030-01267-0_36
   Zhang Y., 2018, P EUR C COMP VIS, P686
   Zheng Z., 2017, J HANSHAN NORM U
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11
   Zhuang YT, 2018, IEEE T CIRC SYST VID, V28, P76, DOI 10.1109/TCSVT.2016.2606648
NR 59
TC 9
Z9 10
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2354
EP 2365
DI 10.1109/TMM.2019.2957948
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200011
DA 2024-07-18
ER

PT J
AU Amini, S
   Ghaemmaghami, S
AF Amini, Sajjad
   Ghaemmaghami, Shahrokh
TI Towards Improving Robustness of Deep Neural Networks to Adversarial
   Perturbations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Robustness; Perturbation methods; Training; Deep learning; Computer
   architecture; Neural networks; Signal to noise ratio; Convolutional
   neural network; regularizer; robust; gradient descent; proximal
   operator; interpretable
ID FRAMEWORK
AB Deep neural networks have presented superlative performance in many machine learning based perception and recognition tasks, where they have even outperformed human precision in some applications. However, it has been found that human perception system is much more robust to adversarial perturbation, as compared to these artificial networks. It has been shown that a deep architecture with a lower Lipschitz constant can generalize better and tolerate higher level of adversarial perturbation. Smooth regularization has been proposed to control the Lipschitz constant of a deep architecture and in this work, we show how a deep convolutional neural network (CNN), based on non-smooth regularization of convolution and fully connected layers, can present enhanced generalization and robustness to adversarial perturbation, simultaneously. We propose two non-smooth regularizers that present specific features for adversarial samples with different levels of signal-to-noise ratios. The regularizers build direct interconnections for the weight matrices in each layer, through which they control the Lipschitz constant of architecture and improve the consistency of input-output mapping of the network. This leads to more reliable and interpretable network mapping and reduces abrupt changes in the networks output. We develop an efficient algorithm to solve the non-smooth learning problems, which presents a gradual complexity addition property. Our simulation results over three benchmark datasets signify the superiority of the proposed formulations over previously reported methods for improving the robustness of deep architecture, towards human robustness to adversarial samples.
C1 [Amini, Sajjad; Ghaemmaghami, Shahrokh] Sharif Univ Technol, Dept Elect Engn & Elect, Res Inst, Tehran 113658639, Iran.
C3 Sharif University of Technology
RP Ghaemmaghami, S (corresponding author), Sharif Univ Technol, Dept Elect Engn & Elect, Res Inst, Tehran 113658639, Iran.
EM sa1368_elec@yahoo.com; ghaemmag@sharif.ir
RI Amini, Sajjad/AAX-4276-2020
OI Amini, Sajjad/0000-0002-0322-9324
CR Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385
   Amini S, 2019, IEEE T SIGNAL PROCES, V67, P1860, DOI 10.1109/TSP.2019.2899294
   [Anonymous], 2019, ARXIV190906978
   [Anonymous], CLIN ORTHOPAEDICS RE
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P IEEE WINT C APPL C
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], P 32 INT C NEUR INF
   [Anonymous], CLIN ORTHOPAEDICS RE
   [Anonymous], ICLR
   [Anonymous], IEEE ACCESS
   [Anonymous], ARXIV190904839
   [Anonymous], 2017, ARXIV171002338
   [Anonymous], 2019, ARXIV PREPRINT ARXIV
   [Anonymous], 2019, ARXIV190909034
   [Anonymous], P IEEE COMP SOC C CO
   Bhagoji A.N., 2018, WORKSH SEC MACH LEAR
   Boyd S., 2004, CONVEX OPTIMIZATION
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Cisse M, 2017, PR MACH LEARN RES, V70
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Elad M, 2007, IEEE T SIGNAL PROCES, V55, P5695, DOI 10.1109/TSP.2007.900760
   Gao J, 2018, INT J ADV MANUF TECH, V94, P1545, DOI 10.1007/s00170-017-0140-5
   Goodfellow I., 2015, International Conference on learning and recognition, P1
   Goodfellow Ian, DEEP LEARNING, V1
   Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang L, 2018, AAAI CONF ARTIF INTE, P3271
   Huang L, 2017, IEEE I CONF COMP VIS, P2822, DOI 10.1109/ICCV.2017.305
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Kovacevic J, 2008, FOUND TRENDS SIGNAL, V2, P1, DOI 10.1561/2000000006
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Luo YB, 2016, ADV SOC SCI EDUC HUM, V44, P1
   Lyu CC, 2015, IEEE DATA MINING, P301, DOI 10.1109/ICDM.2015.84
   Miyato T., 2016, Patenting software
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Netzer Y., 2011, ADV NEURAL INF PROCE, P1
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Ros AS, 2018, AAAI CONF ARTIF INTE, P1660
   Sadeghi M, 2017, IEEE SIGNAL PROC LET, V24, P32, DOI 10.1109/LSP.2016.2632199
   Shaham U, 2018, NEUROCOMPUTING, V307, P195, DOI 10.1016/j.neucom.2018.04.027
   Smith L, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P560
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   Tisdall EKM, 2014, STUD CHILDHOOD YOUTH, P1
   Tsuzuku Y, 2018, Advances in Neural Information Processing Systems (NeurIPS)
   Zhang C, 2020, PEER PEER NETW APPL, V13, P16, DOI 10.1007/s12083-018-0715-4
   Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485
NR 51
TC 15
Z9 15
U1 2
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1889
EP 1903
DI 10.1109/TMM.2020.2969784
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500019
DA 2024-07-18
ER

PT J
AU Sanchez-Matilla, R
   Li, CY
   Shamsabadi, AS
   Mazzon, R
   Cavallaro, A
AF Sanchez-Matilla, Ricardo
   Li, Chau Yi
   Shamsabadi, Ali Shahin
   Mazzon, Riccardo
   Cavallaro, Andrea
TI Exploiting Vulnerabilities of Deep Neural Networks for Privacy
   Protection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; adversarial images; privacy protection
AB Adversarial perturbations can be added to images to protect their content from unwanted inferences. These perturbations may, however, be ineffective against classifiers that were not seen during the generation of the perturbation, or against defenses based on re-quantization, median filtering or JPEG compression. To address these limitations, we present an adversarial attack that is specifically designed to protect visual content against unseen classifiers and known defenses. We craft perturbations using an iterative process that is based on the Fast Gradient Signed Method and that randomly selects a classifier and a defense, at each iteration. This randomization prevents an undesirable overfitting to a specific classifier or defense. We validate the proposed attack in both targeted and untargeted settings on the private classes of the Places365-Standard dataset. Using ResNet18, ResNet50, AlexNet and DenseNet161 as classifiers, the performance of the proposed attack exceeds that of eleven state-of-the-art attacks.
C1 [Sanchez-Matilla, Ricardo; Li, Chau Yi; Shamsabadi, Ali Shahin; Mazzon, Riccardo; Cavallaro, Andrea] Queen Mary Univ London QMUL, Ctr Intelligent Sensing CIS, London E1 4NS, England.
C3 University of London; Queen Mary University London
RP Sanchez-Matilla, R (corresponding author), Queen Mary Univ London QMUL, Ctr Intelligent Sensing CIS, London E1 4NS, England.
EM ricardo.sanchezmatilla@qmul.ac.uk; chauyi.li@qmul.ac.uk;
   a.shahinshamsabadi@qmul.ac.uk; r.mazzon@qmul.ac.uk;
   a.cavallaro@qmul.ac.uk
RI Shahin Shamsabadi, Ali/HGC-3874-2022
OI Li, Chau Yi/0000-0003-0845-0770; Sanchez-Matilla,
   Ricardo/0000-0003-2330-0973
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2018, P NETW DISTR SYST SE
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2019, CVPR
   [Anonymous], 2017, P INT C LEARN REPR A
   [Anonymous], P ADV NEUR INF PROC
   [Anonymous], 2017, AUTOMATIC DIFFERENTI
   [Anonymous], P INT C LEARN REPR A
   [Anonymous], 2015, P INT C LEARN REPR M
   [Anonymous], P INT C LEARN REPR A
   Athalye A, 2018, PR MACH LEARN RES, V80
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Das Nilaksh, 2017, Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression
   Dziugaite Gintare Karolina, 2016, CoRR
   Fawzi A, 2018, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2018.00396
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kurakin A., 2017, P INT C LEARN REPR W
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li CY, 2019, INT CONF ACOUST SPEE, P2502, DOI [10.1109/ICASSP.2019.8682225, 10.1109/icassp.2019.8682225]
   Liu ZH, 2019, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2019.00095
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Ruderman DL, 1998, J OPT SOC AM A, V15, P2036, DOI 10.1364/JOSAA.15.002036
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 27
TC 12
Z9 12
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1862
EP 1873
DI 10.1109/TMM.2020.2987694
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, XY
   Feng, FL
   He, XN
   Huang, HY
   Guan, XY
   Feng, C
   Ming, ZY
   Chua, TS
AF Gao, Xiaoyan
   Feng, Fuli
   He, Xiangnan
   Huang, Heyan
   Guan, Xinyu
   Feng, Chong
   Ming, Zhaoyan
   Chua, Tat-Seng
TI Hierarchical Attention Network for Visually-Aware Food Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Recommender systems; Collaboration; Encoding; Task
   analysis; Feature extraction; History; Food Recommender Systems;
   Hierarchical Attention; Collaborative Filtering; Ingredients; Recipe
   Image
ID EAT
AB Food recommender systems play an important role in assisting users to identify the desired food to eat. Deciding what food to eat is a complex and multi-faceted process, which is influenced by many factors such as the ingredients, appearance of the recipe, the user's personal preference on food, and various contexts like what had been eaten in the past meals. This work formulates the food recommendation problem as predicting user preference on recipes based on three key factors that determine a user's choice on food, namely, 1) the user's (and other users') history; 2) the ingredients of a recipe; and 3) the descriptive image of a recipe. To address this challenging problem, this work develops a dedicated neural network-based solution Hierarchical Attention based Food Recommendation (HAFR) which is capable of: 1) capturing the collaborative filtering effect like what similar users tend to eat; 2) inferring a user's preference at the ingredient level; and 3) learning user preference from the recipe's visual images. To evaluate our proposed method, this work constructs a large-scale dataset consisting of millions of ratings from AllRecipes.com. Extensive experiments show that our method outperforms several competing recommender solutions like Factorization Machine and Visual Bayesian Personalized Ranking with an average improvement of 12%, offering promising results in predicting user preference on food.
C1 [Gao, Xiaoyan; Huang, Heyan; Feng, Chong] Beijing Inst Technol, Beijing Engn Res Ctr High Volume Language Informa, Sch Comp, Beijing 100081, Peoples R China.
   [Feng, Fuli; Ming, Zhaoyan; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [He, Xiangnan] Univ Sci & Technol China, Hefei 230031, Peoples R China.
   [Guan, Xinyu] Xi An Jiao Tong Univ, Syst Engn Inst, Xian 710049, Peoples R China.
C3 Beijing Institute of Technology; National University of Singapore;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Xi'an Jiaotong University
RP Huang, HY (corresponding author), Beijing Inst Technol, Beijing Engn Res Ctr High Volume Language Informa, Sch Comp, Beijing 100081, Peoples R China.
EM xygao@bit.edu.cn; fulifeng93@gmail.com; xiangnanhe@gmail.com;
   hhy63@bit.edu.cn; xinyu_guan@foxmail.com; fengchong@bit.edu.cn;
   dcsming@nus.edu.sg; dcscts@nus.edu.sg
OI Feng, Chong/0000-0002-1691-1584
FU National Key Research and Development Program of China [2016QY03D0602];
   National Natural Science Foundation of China (Key Program) [61751201];
   China Scholarship Council (CSC); NExT++ project - National Research
   Foundation, Prime Ministers Office, Singapore under its IRC@Singapore
   Funding Initiative
FX The work was supported in part by the National Key Research and
   Development Program of China under Grant No.2016QY03D0602, National
   Natural Science Foundation of China (Key Program) under Grant 61751201,
   China Scholarship Council (CSC) and NExT++ project supported by the
   National Research Foundation, Prime Ministers Office, Singapore under
   its IRC@Singapore Funding Initiative. This work is done during Xiaoyan
   Gao's internship in National University of Singapore (NUS), supervised
   by Fuli Feng andXiangnanHe. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Shaoen Wu.
CR Aguilar E, 2018, IEEE T MULTIMEDIA, V20, P3266, DOI 10.1109/TMM.2018.2831627
   Amine E, 2003, WHO TECHNICAL REPORT, V916
   [Anonymous], 2018, P 2018 ACM MULT C, DOI DOI 10.1145/3240508.3240627
   [Anonymous], 2015, P 9 ACM C REC SYST N
   [Anonymous], P INT AAAI C WEB SOC
   [Anonymous], 2009, P ACM MULTIMEDIA 200
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Cai XY, 2018, AAAI CONF ARTIF INTE, P5747
   Chen JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1771, DOI 10.1145/3123266.3123428
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chokr M, 2017, AAAI CONF ARTIF INTE, P4664
   Ciocca G, 2017, LECT NOTES COMPUT SC, V10590, P426, DOI 10.1007/978-3-319-70742-6_41
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Elsweiler D., 2015, CEUR Workshop Proceedings, V1533, P33
   Elsweiler D, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P575, DOI 10.1145/3077136.3080826
   Farinella GM, 2016, COMPUT BIOL MED, V77, P23, DOI 10.1016/j.compbiomed.2016.07.006
   Freyne J, 2010, IUI 2010, P321
   Freyne J, 2010, LECT NOTES COMPUT SC, V6075, P381, DOI 10.1007/978-3-642-13470-8_36
   Ge M., 2015, P 5 INT C DIGITAL HL, P105, DOI DOI 10.1145/2750511.2750528
   Gelli F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1828, DOI 10.1145/3123266.3127909
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   He XN, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2227
   He XN, 2018, ACM/SIGIR PROCEEDINGS 2018, P355, DOI 10.1145/3209978.3209981
   He XN, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P355, DOI 10.1145/3077136.3080777
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   He XN, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P549, DOI 10.1145/2911451.2911489
   Herranz L, 2017, IEEE T MULTIMEDIA, V19, P430, DOI 10.1109/TMM.2016.2614861
   Huang J, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P573, DOI 10.1145/3289600.3290972
   Kagaya H, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1085, DOI 10.1145/2647868.2654970
   Kim KJ, 2016, IEEE ACCESS, V4, P8199, DOI 10.1109/ACCESS.2016.2600699
   Lee H, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P957, DOI 10.1145/2911451.2914734
   Liu H, 2018, IEEE ANTENNAS PROP, P349, DOI 10.1109/APUSNCURSINRSM.2018.8608269
   Liu P, 2016, 2016 17TH INTERNATIONAL CONFERENCE ON ELECTRONIC PACKAGING TECHNOLOGY (ICEPT), P1480, DOI 10.1109/ICEPT.2016.7583403
   Martinel N, 2015, P 2015 IEEE INT C CO, P92
   Martinel N, 2018, IEEE WINT CONF APPL, P567, DOI 10.1109/WACV.2018.00068
   Min WQ, 2018, IEEE T MULTIMEDIA, V20, P950, DOI 10.1109/TMM.2017.2759499
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Pochet M, 2012, IEEE PHOTON CONF, P18, DOI 10.1109/IPCon.2012.6358467
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Rendle S, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168771
   Rokicki M., 2016, P 2016 C USER MODELI, P207
   Teng CY, 2012, PROCEEDINGS OF THE 3RD ANNUAL ACM WEB SCIENCE CONFERENCE, 2012, P298
   Trattner C, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P489, DOI 10.1145/3038912.3052573
   Wang SH, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P391, DOI 10.1145/3038912.3052638
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wang X, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P950, DOI 10.1145/3292500.3330989
   Wang X, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1543, DOI 10.1145/3178876.3186066
   Wang X, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P185, DOI 10.1145/3077136.3080771
   Wu Y, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P153, DOI 10.1145/2835776.2835837
   Xiao J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3119
   Yang LQ, 2017, ACM T INFORM SYST, V36, DOI 10.1145/3072614
   Yang Longqi., 2015, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, P183, DOI [DOI 10.1145/2806416, 10.1145/2806416]
   Yu WH, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P649, DOI 10.1145/3178876.3186146
   Zhao XY, 2018, 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS), P95, DOI 10.1145/3240323.3240374
NR 58
TC 50
Z9 52
U1 2
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1647
EP 1659
DI 10.1109/TMM.2019.2945180
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100022
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kao, CC
   Wang, YX
   Waltman, J
   Sen, P
AF Kao, Chieh-Chi
   Wang, Yuxiang
   Waltman, Jonathan
   Sen, Pradeep
TI Patch-Based Image Hallucination for Super Resolution With Detail
   Reconstruction From Similar Sample Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image hallucination; image super-resolution; large image databases
ID SUPERRESOLUTION; INTERPOLATION; COMPLETION
AB Image hallucination and super-resolution have been studied for decades, and many approaches have been proposed to upsample low-resolution images using information from the images themselves, multiple example images, or large image databases. However, most of this work has focused exclusively on small magnification levels because the algorithms simply sharpen the blurry edges in the upsampled images - no actual new detail is typically reconstructed in the final result. In this paper, we present a patch-based algorithm for image hallucination which, for the first time, properly synthesizes novel high frequency detail. To do this, we pose the synthesis problem as a patch-based optimization which inserts coherent, high-frequency detail from contextually-similar images of the same physical scene/subject provided from either a personal image collection or a large online database. The resulting image is visually plausible and contains coherent high frequency information. We demonstrate the robustness of our algorithm by testing it on a large number of images and show that its performance is considerably superior to all state-of-the-art approaches, a result that is verified to be statistically significant through a randomized user study.
C1 [Kao, Chieh-Chi; Wang, Yuxiang; Waltman, Jonathan] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
   [Kao, Chieh-Chi] Amazon Alexa, Cambridge, MA 02142 USA.
   [Wang, Yuxiang] Pinterest Inc, San Francisco, CA 94103 USA.
   [Waltman, Jonathan] Toyon Res, Goleta, CA 93117 USA.
   [Sen, Pradeep] Univ Calif Santa Barbara, Dept Elect & Comp Engn, UCSB MIRAGE Lab, Santa Barbara, CA 93106 USA.
C3 University of California System; University of California Santa Barbara;
   University of California System; University of California Santa Barbara
RP Kao, CC (corresponding author), Amazon Alexa, Cambridge, MA 02142 USA.
EM chiehchi.kao@gmail.com
RI Williams, Jonathan/HJA-2081-2022
OI Sen, Pradeep/0000-0002-8042-924X
FU National Science Foundation [IIS-1342931, IIS-1321168, IIS-1619376]
FX This work was supported in part by National Science Foundation under
   Grants IIS-1342931, IIS-1321168, and IIS-1619376. This work was done
   while C.-C. Kao, Y. Wang, and J. Waltman studied at the Department of
   Electrical and Computer Engineering, UCSB. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Qi Tian.
CR [Anonymous], [No title captured]
   [Anonymous], 2015, P IEEE C COMP VIS PA
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], P IEEE WORKSH STAT C
   Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Darabi S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185578
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239546, 10.1145/1276377.1276496]
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   HaCohen Y., 2010, ICCP, P1
   HaCohen Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964965
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   HOU HS, 1978, IEEE T ACOUST SPEECH, V26, P508
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Kalantari NK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508402
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Kumar N, 2016, IEEE T MULTIMEDIA, V18, P1504, DOI 10.1109/TMM.2016.2571625
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Liu JY, 2017, IEEE T MULTIMEDIA, V19, P302, DOI 10.1109/TMM.2016.2614427
   Qin XM, 2015, IEEE T MULTIMEDIA, V17, P295, DOI 10.1109/TMM.2015.2395078
   Salvador J, 2015, IEEE I CONF COMP VIS, P325, DOI 10.1109/ICCV.2015.45
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409106
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Sun JA, 2010, PROC CVPR IEEE, P231, DOI 10.1109/CVPR.2010.5540206
   Sun L, 2012, INT C COMP PHOT, P1, DOI DOI 10.1109/ICCPHOT.2012.6215221
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Xiong ZW, 2009, PROC CVPR IEEE, P2074, DOI 10.1109/CVPRW.2009.5206630
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Yue HJ, 2013, IEEE T IMAGE PROCESS, V22, P4865, DOI 10.1109/TIP.2013.2279315
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 54
TC 4
Z9 4
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1139
EP 1152
DI 10.1109/TMM.2019.2938911
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200003
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Qiu, S
   Zhao, Y
   Jiao, JB
   Wei, YC
   Wei, SK
AF Qiu, Shuang
   Zhao, Yao
   Jiao, Jianbo
   Wei, Yunchao
   Wei, Shikui
TI Referring Image Segmentation by Generative Adversarial Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Semantics; Feature extraction; Natural languages;
   Generators; Generative adversarial networks; Visualization; Image
   referring segmentation; Adversarial training
ID PRIMARY OBJECTS; TEXT
AB Referring expression is a kind of language expression being used for referring to particular objects. In this paper, we focus on the problem of image segmentation from natural language referring expressions. Existing works tackle this problem by augmenting the convolutional semantic segmentation networks with an LSTM sentence encoder, which is optimized by a pixel-wise classification loss. We argue that the distribution similarity between the inference and ground truth plays an important role in referring image segmentation. Therefore we introduce a complementary loss considering the consistency between the two distributions. To this end, we propose to train the referring image segmentation model in a generative adversarial fashion, which well addresses the distribution similarity problem. In particular, the proposed adversarial semantic guidance network (ASGN) includes the following advantages: a) more detailed visual information is incorporated by the detail enhancement; b) semantic information counteracts the word embedding impact; c) the proposed adversarial learning approach relieves the distribution inconsistencies. Experimental results on four standard datasets show significant improvements over all the compared baseline models, demonstrating the effectiveness of our method.
C1 [Qiu, Shuang; Zhao, Yao; Wei, Shikui] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Qiu, Shuang; Zhao, Yao; Wei, Shikui] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Jiao, Jianbo] Univ Oxford, Dept Engn Sci, Oxford OX1 2JD, England.
   [Wei, Yunchao] Univ Illinois, Beckman Inst, Champaign, IL 61820 USA.
C3 Beijing Jiaotong University; Beijing Jiaotong University; University of
   Oxford; University of Illinois System; University of Illinois
   Urbana-Champaign
RP Zhao, Y (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM 14120332@bjtu.edu.cn; yzhao@bjtu.edu.cn; jiaojianbo.i@gmail.com;
   wychao1987@gmail.com; shkwei@bjtu.edu.cn
OI Zhao, Yao/0000-0002-8581-9554; Jiao, Jianbo/0000-0003-0833-5115
FU National Key Research and Development of China [2016YFB0800404];
   National Natural Science Foundation of China [61532005, 61972022];
   Program of China Scholarships Council [201807095006]; Fundamental
   Research Funds for the Central Universities [2018JBZ001]
FX This work was supported in part by National Key Research and Development
   of China (2016YFB0800404), in part by National Natural Science
   Foundation of China (61532005 and 61972022), in part by Program of China
   Scholarships Council (201807095006), and in part by Fundamental Research
   Funds for the Central Universities (2018JBZ001).
CR [Anonymous], 2016, P ADV NEUR INF PROC
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2018, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2018.2811621
   [Anonymous], 2017, P INT C LEARN REPR
   Chen JB, 2018, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR.2018.00909
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hu R, 2016, ARXIV160808305
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiao JB, 2019, PROC CVPR IEEE, P2864, DOI 10.1109/CVPR.2019.00298
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laput G.P., 2013, P SIGCHI C HUM FACT, P2185, DOI [DOI 10.1145/2470654.2481301, 10.1145/2470654.2481301]
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li J, 2019, IEEE MULTIMEDIA, V26, P9, DOI 10.1109/MMUL.2018.2883136
   Liew JH, 2017, IEEE I CONF COMP VIS, P2746, DOI 10.1109/ICCV.2017.297
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2017, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2017.143
   Liu MY, 2017, ADV NEUR IN, V30
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lou ZY, 2014, IEEE T MULTIMEDIA, V16, P2110, DOI 10.1109/TMM.2014.2363936
   Mao J., 2012, P INT C LEARN REPR, P1000
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Margffoy-Tuay E, 2018, LECT NOTES COMPUT SC, V11215, P656, DOI 10.1007/978-3-030-01252-6_39
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Reed S, 2016, PR MACH LEARN RES, V48
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schwing AG, 2015, COMPUT SCI, V3, P469
   Shi HC, 2018, LECT NOTES COMPUT SC, V11210, P38, DOI 10.1007/978-3-030-01231-1_3
   Tighe J, 2014, PROC CVPR IEEE, P3748, DOI 10.1109/CVPR.2014.479
   Wang C, 2014, IEEE T MULTIMEDIA, V16, P903, DOI 10.1109/TMM.2014.2306393
   Wang T, 2016, IEEE T MULTIMEDIA, V18, P2358, DOI 10.1109/TMM.2016.2600441
   Wang WG, 2016, IEEE T MULTIMEDIA, V18, P1011, DOI 10.1109/TMM.2016.2545409
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Xiao HX, 2018, AAAI CONF ARTIF INTE, P7420
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Yu Fisher, 2016, PROC INT C LEARN REP
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang LS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P474, DOI 10.1145/3240508.3240540
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
NR 58
TC 38
Z9 39
U1 3
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1333
EP 1344
DI 10.1109/TMM.2019.2942480
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200018
DA 2024-07-18
ER

PT J
AU Zhang, LM
   Yin, JW
   Li, P
   Shang, YH
   Zimmermann, R
   Shao, L
AF Zhang, Luming
   Yin, Jianwei
   Li, Ping
   Shang, Yongheng
   Zimmermann, Roger
   Shao, Ling
TI Flickr Image Community Analytics by Deep Noise-Refined Matrix
   Factorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Machine learning; deep model; noise-refined; matrix factorization;
   community
ID MODEL
AB Accurately categorizing Flickr images into multiple pre-defined communities (e.g., "architecture" and "peaceful") is an indispensable technique in multimedia analysis, graphic design, fashion recommendation, etc. In practice, these communities are constructed and updated manually, which is subjective and intolerably time consuming. To alleviate these shortcomings, a noise-refined deep matrix factorization (MF) framework is proposed to intelligently discover communities from million-scale Flickr users, wherein the semantic tag correlations and community correlations are simultaneously encoded. More specifically, it is believable that Flickr communities are high-level clues on the basis of human visual semantic perception. Thereby, a MF algorithm is employed to approximate the community label matrix by the product of pairwise factor matrices, which represent the latent representations of user-provided tags and the corresponding basis matrix respectively. Subsequently, an end-to-end deep model is formulated to hierarchically derive the latent deep representation from raw image pixels to semantic tags. To robustly handle contaminated image semantic tags and community labels, an $l_1$ norm constraint is encoded to enhance the MF. Meanwhile, to optimally exploit the rich context information of Flickr images, the intrinsic structure between image semantic tags and between communities are collaboratively captured. Finally, the upgraded MF and the deep model are seamlessly combined into a unified framework, which is solved by an iterative algorithm. Experiments on 2 M Flickr images have demonstrated the superiority of our approach. Besides, the discovered Flickr communities can improve photo retargeting and visual aesthetics assessment significantly.
C1 [Zhang, Luming; Yin, Jianwei] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
   [Li, Ping] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Shang, Yongheng] Zhejiang Univ, Sch Aeronaut & Astronaut, Hangzhou 310027, Peoples R China.
   [Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
C3 Zhejiang University; Hangzhou Dianzi University; Zhejiang University;
   National University of Singapore
RP Shang, YH (corresponding author), Zhejiang Univ, Sch Aeronaut & Astronaut, Hangzhou 310027, Peoples R China.
EM zglumg@gmail.com; zjuyjw@zju.edu.cn; patriclouis.lee@gmail.com;
   yh_shang@zju.edu.cn; roger@comp.nus.edu.sg; ling.shao@ieee.org
RI Shao, Ling/D-3535-2011; Lei, Ming/JAD-1050-2023; Zimmermann,
   Roger/D-7944-2015; zhang, lu/GRO-2969-2022
OI Zimmermann, Roger/0000-0002-7410-2590; Li, Ping/0000-0002-8515-7773;
   Shao, Ling/0000-0002-8264-6117
FU National Key Research and Development Program of China [2017YFB1400603];
   National Natural Science Foundation of China [61825205, 61772459];
   National Science and Technology Major Project of China
   [50-D36B02-9002-16/19]; Fundamental Research Funds for the Central
   Universities [124002*172210195]
FX Thisworkwas supported in part by theNationalKey Research andDevelopment
   Program of China underGrant 2017YFB1400603, in part by National pound
   Natural Science Foundation of China under Grant 61825205, Grant
   61772459, in part by National Science and Technology Major Project of
   China under Grant 50-D36B02-9002-16/19, and in part by the Fundamental
   Research Funds for the Central Universities (124002*172210195).
CR Ahn YY, 2010, NATURE, V466, P761, DOI 10.1038/nature09182
   Alam MR, 2017, IEEE T MULTIMEDIA, V19, P317, DOI 10.1109/TMM.2016.2615524
   [Anonymous], 2016, CoRR
   [Anonymous], 2010, ACM MULTIMEDIA
   [Anonymous], 2004, PHYS REV
   Aytekin C, 2018, IEEE T MULTIMEDIA, V20, P82, DOI 10.1109/TMM.2017.2713982
   Bateni M., 2017, P 31 C NEUR INF PROC, P6864
   Biernacki C, 2000, IEEE T PATTERN ANAL, V22, P719, DOI 10.1109/34.865189
   Cai D, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1010
   Chen YW., 2005, Combining SVMs with Various Feature Selection Strategies
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Frank M, 2012, J MACH LEARN RES, V13, P459
   Fuglede B, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P31
   Gregory S, 2008, LECT NOTES ARTIF INT, V5211, P408, DOI 10.1007/978-3-540-87479-9_45
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Hadizadeh H, 2018, IEEE T MULTIMEDIA, V20, P392, DOI 10.1109/TMM.2017.2740023
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Hong RC, 2016, IEEE T MULTIMEDIA, V18, P1555, DOI 10.1109/TMM.2016.2567071
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P1124, DOI 10.1109/TIP.2016.2514499
   JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Karayev S., 2014, P BRIT MACH VIS C, P1, DOI [DOI 10.5244/C.28.122, 10.5244/c.28.122, 10.5244%2Fc.28.122, 10.5244/C.28.122]
   Ke QF, 2005, PROC CVPR IEEE, P739
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuhn H., 1956, NAVAL RES LOGIST Q, V3, P253, DOI [10.1002/nav.3800030404, https://doi.org/10.1002/nav.3800030404, DOI 10.1002/NAV.3800030404]
   Kumano S, 2017, IEEE T MULTIMEDIA, V19, P107, DOI 10.1109/TMM.2016.2608002
   Lancichinetti A, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.046110
   Li ZN, 2015, AER ADV ENG RES, V17, P1, DOI 10.1109/PLASMA.2015.7180009
   Liu TL, 2016, IEEE T NEUR NET LEAR, V27, P1851, DOI 10.1109/TNNLS.2015.2458986
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Mahoney Michael, 2010, P 19 INT C WORLD WID, P631, DOI DOI 10.1145/1772690.1772755
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2015, INT J COMPUT VISION, V113, P246, DOI 10.1007/s11263-014-0789-2
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Mazumdar A, 2017, ADV NEUR IN, V30
   Mukherjee SS, 2017, ADV NEUR IN, V30
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Papadimitriou S, 2008, LECT NOTES ARTIF INT, V5212, P170, DOI 10.1007/978-3-540-87481-2_12
   Renoust B, 2016, IEEE T MULTIMEDIA, V18, P2184, DOI 10.1109/TMM.2016.2614224
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Somandepalli K, 2018, IEEE T MULTIMEDIA, V20, P539, DOI 10.1109/TMM.2017.2745712
   Sunderrajan S, 2016, IEEE T MULTIMEDIA, V18, P51, DOI 10.1109/TMM.2015.2496139
   Trigeorgis G, 2017, IEEE T PATTERN ANAL, V39, P417, DOI 10.1109/TPAMI.2016.2554555
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Yang J, 2012, IEEE DATA MINING, P1170, DOI 10.1109/ICDM.2012.139
   Yang TB, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P927
   Yoshida T, 2010, IEEE IMAGE PROC, P349, DOI 10.1109/ICIP.2010.5651018
   Zhang LM, 2018, IEEE T MULTIMEDIA, V20, P1462, DOI 10.1109/TMM.2017.2769799
   Zhang LM, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P491, DOI 10.1145/2733373.2806255
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
   Zhang YZ, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P997
NR 58
TC 3
Z9 3
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1273
EP 1284
DI 10.1109/TMM.2019.2938664
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200013
DA 2024-07-18
ER

PT J
AU Wu, LX
   Xu, M
   Wang, JQ
   Perry, S
AF Wu, Lingxiang
   Xu, Min
   Wang, Jinqiao
   Perry, Stuart
TI Recall What You See Continually Using GridLSTM in Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Decoding; Task analysis; Neural networks; Training;
   Computational modeling; Logic gates; Image captioning; GridLSTM;
   recurrent neural network
ID CLASSIFICATION; ATTENTION
AB The goal of image captioning is to automatically describe an image with a sentence, and the task has attracted research attention from both the computer vision and natural-language processing research communities. The existing encoder-decoder model and its variants, which are the most popular models for image captioning, use the image features in three ways: first, they inject the encoded image features into the decoder only once at the initial step, which does not enable the rich image content to be explored sufficiently while gradually generating a text caption; second, they concatenate the encoded image features with text as extra inputs at every step, which introduces unnecessary noise; and, third, they using an attention mechanism, which increases the computational complexity due to the introduction of extra neural nets to identify the attention regions. Different from the existing methods, in this paper, we propose a novel network, Recall Network, for generating captions that are consistent with the images. The recall network selectively involves the visual features by using a GridLSTM and, thus, is able to recall image contents while generating each word. By importing the visual information as the latent memory along the depth dimension LSTM, the decoder is able to admit the visual features dynamically through the inherent LSTM structure without adding any extra neural nets or parameters. The Recall Network efficiently prevents the decoder from deviating from the original image content. To verify the efficiency of our model, we conducted exhaustive experiments on full and dense image captioning. The experimental results clearly demonstrate that our recall network outperforms the conventional encoder-decoder model by a large margin and that it performs comparably to the state-of-the-art methods.
C1 [Wu, Lingxiang; Xu, Min] Univ Technol Sydney, Global Big Data Technol Ctr, Sch Elect & Data Engn, Ultimo, NSW 2007, Australia.
   [Wang, Jinqiao] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Perry, Stuart] Univ Technol Sydney, Sch Elect & Data Engn, Perceptual Imaging Lab, Ultimo, NSW 2007, Australia.
C3 University of Technology Sydney; Chinese Academy of Sciences; Institute
   of Automation, CAS; University of Technology Sydney
RP Xu, M (corresponding author), Univ Technol Sydney, Global Big Data Technol Ctr, Sch Elect & Data Engn, Ultimo, NSW 2007, Australia.
EM Lingxiang.Wu@student.uts.edu.au; Min.Xu@uts.edu.au;
   jqwang@nlpr.ia.ac.cn; Stuart.Perry@uts.edu.au
RI Perry, Stuart W/H-9545-2016
OI Perry, Stuart W/0000-0002-2794-3178; Wu, Lingxiang/0000-0001-9346-3597;
   Xu, Min/0000-0001-9581-8849; wang, jin qiao/0000-0002-9118-2780
CR Altintakan UL, 2015, IEEE T MULTIMEDIA, V17, P323, DOI 10.1109/TMM.2014.2388312
   [Anonymous], 2016, P 4 INT C LEARN REPR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2017, CVPR
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Choi Y, 2011, P 15 C COMPUTATIONAL, P220
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong L, 2018, IEEE T MULTIMEDIA, V20, P2012, DOI 10.1109/TMM.2017.2788205
   ElHihi S, 1996, ADV NEUR IN, V8, P493
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Harris D., 2010, DIGITAL DESIGN COMPU
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks LA, 2016, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2016.8
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Huang C, 2016, IEEE T MULTIMEDIA, V18, P2372, DOI 10.1109/TMM.2016.2602060
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   King DB, 2015, ACS SYM SER, V1214, P1
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kulkarni G, 2011, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR.2011.5995466
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Mao JH, 2015, IEEE I CONF COMP VIS, P2533, DOI 10.1109/ICCV.2015.291
   Mikolov T., 2013, INT C LEARNING REPRE, P1
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren Z, 2017, PROC CVPR IEEE, P1151, DOI 10.1109/CVPR.2017.128
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang JH, 2015, IEEE T MULTIMEDIA, V17, P1899, DOI 10.1109/TMM.2015.2476660
   Ushiku Y, 2015, IEEE I CONF COMP VIS, P2668, DOI 10.1109/ICCV.2015.306
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115432
   Wang Y, 2016, IEEE T MULTIMEDIA, V18, P1869, DOI 10.1109/TMM.2016.2581580
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang Z, 2016, INT CONF ACOUST SPEE, P3236, DOI 10.1109/ICASSP.2016.7472275
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhao N, 2016, PROCEEDINGS OF 2016 8TH INTERNATIONAL CONFERENCE ON MODELLING, IDENTIFICATION & CONTROL (ICMIC 2016), P989, DOI 10.1109/ICMIC.2016.7804258
NR 51
TC 29
Z9 31
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 808
EP 818
DI 10.1109/TMM.2019.2931815
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700019
DA 2024-07-18
ER

PT J
AU Jiang, Z
   Zou, Q
   Lin, YW
   Chen, L
   Wang, S
AF Jiang, Zheng
   Zou, Qin
   Lin, Yuewei
   Chen, Long
   Wang, Song
TI Improved Deep Hashing With Soft Pairwise Similarity for Multi-Label
   Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Semantics; Hash functions; Binary codes; Computer
   science; Neural networks; Kernel; Image retrieval; convolutional neural
   network; semantic label; pairwise similarity; deep hashing
ID REPRESENTATION; CODES
AB Hash coding has been widely used in the approximate nearest neighbor search for large-scale image retrieval. Recently, many deep hashing methods have been proposed and shown largely improved performance over traditional feature-learning methods. Most of these methods examine the pairwise similarity on the semantic-level labels, where the pairwise similarity is generally defined in a hard-assignment way. That is, the pairwise similarity is "1" if they share no less than one class label and "0" if they do not share any. However, such similarity definition cannot reflect the similarity ranking for pairwise images that hold multiple labels. In this paper, an improved deep hashing method is proposed to enhance the ability of multi-label image retrieval. We introduce a pairwise quantified similarity calculated on the normalized semantic labels. Based on this, we divide the pairwise similarity into two situations-"hard similarity" and "soft similarity," where cross-entropy loss and mean square error loss are adapted respectively for more robust feature learning and hash coding. Experiments on four popular datasets demonstrate that the proposed method outperforms the competing methods and achieves the state-of-the-art performance in multi-label image retrieval.
C1 [Jiang, Zheng; Zou, Qin] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Lin, Yuewei] Brookhaven Natl Lab, Computat Sci Initiat, Upton, NY 11973 USA.
   [Chen, Long] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 518001, Peoples R China.
   [Wang, Song] Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29201 USA.
   [Wang, Song] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
C3 Wuhan University; United States Department of Energy (DOE); Brookhaven
   National Laboratory; Sun Yat Sen University; University of South
   Carolina System; University of South Carolina Columbia; Tianjin
   University
RP Zou, Q (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM zhengzhang@whu.edu.cn; qzou@whu.edu.cn; ywlin@bnl.gov;
   chen146@mail.sysu.edu.cn; songwang@cec.sc.edu
RI Zou, Qin/AFM-0040-2022; Lin, Yuewei/D-9454-2017; Zou, Qin/GVU-2237-2022
OI Wang, Song/0000-0003-4152-5295; Zou, Qin/0000-0001-7955-0782; lin,
   yuewei/0000-0002-1429-4543
FU National Natural Science Foundation of China [61872277, 41571437,
   61672376, U1803264]; Hubei Provincial Natural Science Foundation
   [2018CFB482]; BNL LDRD [18-009]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872277, 41571437, 61672376, and
   U1803264, in part by the Hubei Provincial Natural Science Foundation
   under Grant 2018CFB482. The work of Y. Lin was supported by BNL LDRD
   18-009. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Marco Bertini.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2014, Hashing for similarity search: A survey
   [Anonymous], 2017, ARXIV170510999
   [Anonymous], 1999, MODERN INFORM RETRIE
   [Anonymous], HABIR HASHING BASELI
   [Anonymous], 2017, ARXIV170200758
   [Anonymous], 2009, NIPS
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2009, P ACM INT C IM VID R
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4
   Ercoli S, 2017, IEEE T MULTIMEDIA, V19, P2521, DOI 10.1109/TMM.2017.2697824
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gui J, 2018, IEEE T NEUR NET LEAR, V29, P608, DOI 10.1109/TNNLS.2016.2636870
   He K, 2018, PROC CVPR IEEE, P4023, DOI 10.1109/CVPR.2018.00423
   Huang LK, 2018, IEEE T NEUR NET LEAR, V29, P2309, DOI 10.1109/TNNLS.2017.2689242
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Lei Z, 2015, LECT NOTES ENG COMP, P27
   Li CS, 2015, IEEE T NEUR NET LEAR, V26, P1551, DOI 10.1109/TNNLS.2014.2339100
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Liang JQ, 2017, IEEE T MULTIMEDIA, V19, P1077, DOI 10.1109/TMM.2016.2644862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu L, 2016, IEEE T NEUR NET LEAR, V27, P2526, DOI 10.1109/TNNLS.2015.2495345
   Liu Qingshan, 2018, IEEE Trans Neural Netw Learn Syst, V29, P2441, DOI 10.1109/TNNLS.2017.2696053
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu XQ, 2017, IEEE T IMAGE PROCESS, V26, P355, DOI 10.1109/TIP.2016.2627801
   Nan DL, 2016, C IND ELECT APPL, P1175, DOI 10.1109/ICIEA.2016.7603762
   Ning QQ, 2017, IEEE T MULTIMEDIA, V19, P586, DOI 10.1109/TMM.2016.2625260
   Norouzi M.E., 2011, ICML
   Sablayrolles A, 2017, INT CONF ACOUST SPEE, P1732, DOI 10.1109/ICASSP.2017.7952453
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang XF, 2017, LECT NOTES COMPUT SC, V10111, P70, DOI 10.1007/978-3-319-54181-5_5
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Wu DY, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P155, DOI 10.1145/3078971.3078989
   Wu SZ, 2019, INT J COMPUT VISION, V127, P560, DOI 10.1007/s11263-019-01157-5
   Xia H, 2014, IEEE T PATTERN ANAL, V36, P536, DOI 10.1109/TPAMI.2013.149
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhou X, 2020, IEEE T CYBERNETICS, V50, P1460, DOI 10.1109/TCYB.2018.2883970
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
   Zou Q, 2019, IEEE T IMAGE PROCESS, V28, P1498, DOI 10.1109/TIP.2018.2878966
   Zou Q, 2018, IEEE T CYBERNETICS, V48, P1136, DOI 10.1109/TCYB.2017.2682280
   Zou Q, 2015, IEEE GEOSCI REMOTE S, V12, P2321, DOI 10.1109/LGRS.2015.2475299
NR 60
TC 95
Z9 100
U1 3
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 540
EP 553
DI 10.1109/TMM.2019.2929957
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, W
   He, XY
   Lu, WZ
AF Zhang, Wei
   He, Xuanyu
   Lu, Weizhi
TI Exploring Discriminative Representations for Image Emotion Recognition
   With CNNs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image emotion classification; discriminative representation; emotional
   inference; deep learning; convolutional neural networks
AB Image emotion recognition aims to automatically categorize the emotion conveyed by an image. The potential of deep representation has been demonstrated in recent research on image emotion recognition. To better understand how CNNs work in emotion recognition, we investigate the deep features by visualizing them in this work. This study shows that the deep models mainly rely on the image content but miss the image style information such as color, texture, and shapes that are low-level visual features but are vital for evoking emotions. To form a more discriminative representation for emotion recognition, we propose a novel CNN model that learns and integrates the content information from the high layers of the deep network with the style information from the lower layers. The uncertainty of image emotion labels is also investigated in this paper. Rather than using the emotion labels for training directly, as in previous work, a new loss function is designed by including the emotion labeling quality to optimize the proposed inference model. Extensive experiments on benchmark datasets are conducted to demonstrate the superiority of the proposed representation.
C1 [Zhang, Wei; He, Xuanyu; Lu, Weizhi] Shandong Univ, Sch Control Sci & Engn, Jinan 250000, Peoples R China.
C3 Shandong University
RP He, XY (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250000, Peoples R China.
EM david.zhang@sdu.edu.cn; hexif-fer@outlook.com; wzlu@sdu.edu.cn
OI He, Xuanyu/0000-0002-5079-246X
FU National Key Research and Development Plan of China [2017YFB1300205];
   Major Research Program of Shandong Province [2018CXGC1503]
FX This work was supported in part by the National Key Research and
   Development Plan of China under Grant 2017YFB1300205 and in part by the
   Major Research Program of Shandong Province under Grant 2018CXGC1503.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Wenwu Zhu.
CR Alameda-Pineda X, 2016, PROC CVPR IEEE, P5240, DOI 10.1109/CVPR.2016.566
   Borth D., 2013, P 21 ACM INT C MULT, P459
   Bradley MM, 1999, CTR RES PSYCHOPHYSIO
   Elad M, 2017, IEEE T IMAGE PROCESS, V26, P2338, DOI 10.1109/TIP.2017.2678168
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XY, 2018, NEUROCOMPUTING, V291, P187, DOI 10.1016/j.neucom.2018.02.073
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Lu YW, 2018, IEEE T IMAGE PROCESS, V27, P5248, DOI 10.1109/TIP.2018.2855433
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Rao Tianrong, 2016, NEURAL PROCESSING LE, P1
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sartori A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P311, DOI 10.1145/2733373.2806250
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang WN, 2004, IEEE SYS MAN CYBERN, P6407
   Wei-Ning W, 2006, IEEE SYS MAN CYBERN, P3534, DOI 10.1109/ICSMC.2006.384667
   Yang JF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3266
   Yang JF, 2018, AAAI CONF ARTIF INTE, P491
   Yang JF, 2018, PROC CVPR IEEE, P7584, DOI 10.1109/CVPR.2018.00791
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yanulevskaya V, 2008, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2008.4711701
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   Zhang W, 2018, IEEE T CIRC SYST VID, V28, P2768, DOI 10.1109/TCSVT.2017.2718188
   Zhang W, 2018, NEUROCOMPUTING, V275, P781, DOI 10.1016/j.neucom.2017.09.012
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P2042, DOI 10.1109/TIP.2017.2672440
   Zhang ZW, 2018, IEEE DATA MINING, P787, DOI 10.1109/ICDM.2018.00094
   Zhao SC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5534
   Zhao SC, 2019, AAAI CONF ARTIF INTE, P2620
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhu XG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3595
NR 36
TC 48
Z9 51
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 515
EP 523
DI 10.1109/TMM.2019.2928998
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300018
DA 2024-07-18
ER

PT J
AU Mo, DM
   Lai, ZH
   Wong, WK
AF Mo, Dongmei
   Lai, Zhihui
   Wong, Waikeung
TI Locally Joint Sparse Marginal Embedding for Feature Extraction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; classification; discriminant analysis; joint
   sparsity; robustness
ID LINEAR DISCRIMINANT-ANALYSIS; FACE RECOGNITION; DIMENSIONALITY
   REDUCTION; EIGENFACES; REGRESSION; ALGORITHM; CRITERION; FRAMEWORK; PCA
AB Classical linear discriminant analysis (LDA) has the limitation that it requires the within-class scatter matrix to be nonsingular so that it can perform eigen-decomposition to obtain optimal solutions. To break through this limitation, many methods based on LDA have been proposed. However, these methods are either sensitive to outliers or lack joint sparsity for effective feature extraction. To release these problems, this paper proposes a locally joint sparse marginal embedding (LJSME) method. LJSME reconstructs the scatter matrices and utilizes the locality graph to weigh each pair of data, such that it is robust to outliers and able to preserve the neighborhood relationship of the data. Moreover, LJSME can easily avoid the small sample-size problem by a maximum margin criterion and obtain joint sparsity for effective feature extraction by using joint sparse regularization. The comprehensive analysis between the proposed LJSME and the related methods is presented, which indicates the advantages of the proposed method. A series of experiments was conducted to evaluate the performance of LJSME when compared with the state-of-the-art methods. The MATLAB code of LJSME can be downloaded from https://github.com/TungmeeMo/LJSME.git.
C1 [Mo, Dongmei; Lai, Zhihui; Wong, Waikeung] Hong Kong Polytech Univ, Inst Text & Clothing, Hung Hom, Hong Kong, Peoples R China.
   [Lai, Zhihui] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Wong, Waikeung] Hong Kong Polytech Univ, Shenzhen Res Inst, Shenzhen 518057, Peoples R China.
C3 Hong Kong Polytechnic University; Shenzhen University; Hong Kong
   Polytechnic University
RP Wong, WK (corresponding author), Hong Kong Polytech Univ, Inst Text & Clothing, Hung Hom, Hong Kong, Peoples R China.
EM dongmei_mo@qq.com; lai_zhi_hui@163.com; calvin.wong@polyu.edu.hk
RI Lai, Zhihui/R-1000-2019
OI Lai, Zhihui/0000-0002-4388-3080; Wong, Wai Keung/0000-0002-5214-7114
FU Hong Kong Polytechnic University [RHR1]
FX This work was supported by the Hong Kong Polytechnic University under
   Project RHR1. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zixiang Xiong.
CR [Anonymous], 2014, HYPERSPECTRAL REMOTE
   [Anonymous], 2004, ADV NEURAL INF PROCE
   [Anonymous], 2011, IJCAI INT JOINT C AR
   [Anonymous], 2004, ADV NEURAL INFORM PR
   [Anonymous], NEW YORK ACAD
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9
   Clemmensen L, 2011, TECHNOMETRICS, V53, P406, DOI 10.1198/TECH.2011.08118
   Cui Y, 2012, PATTERN RECOGN, V45, P1471, DOI 10.1016/j.patcog.2011.10.006
   d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269
   Di W, 2010, IEEE T SYST MAN CY A, V40, P1354, DOI 10.1109/TSMCA.2010.2052603
   Ding C., 2006, P 23 INT C MACH LEAR, P281, DOI DOI 10.1145/1143844.1143880
   Feng QX, 2016, IEEE T MULTIMEDIA, V18, P1956, DOI 10.1109/TMM.2016.2602062
   Gu Q., 2011, Proceedings of the Twenty-Second international joint conference on Artificial Intelligence, V2, P1294
   He R, 2012, PROC CVPR IEEE, P2504, DOI 10.1109/CVPR.2012.6247966
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Hou CP, 2014, IEEE T CYBERNETICS, V44, P793, DOI 10.1109/TCYB.2013.2272642
   Hu RX, 2010, NEUROCOMPUTING, V73, P1541, DOI 10.1016/j.neucom.2009.11.036
   Huang J, 2018, IEEE T CYBERNETICS, V48, P876, DOI 10.1109/TCYB.2017.2663838
   Jiang JJ, 2018, IEEE T GEOSCI REMOTE, V56, P4581, DOI 10.1109/TGRS.2018.2828029
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Koren Y, 2004, IEEE T VIS COMPUT GR, V10, P459, DOI 10.1109/TVCG.2004.17
   Lai ZH, 2019, IEEE T CIRC SYST VID, V29, P756, DOI 10.1109/TCSVT.2018.2812802
   Lai ZH, 2017, IEEE T CYBERNETICS, V47, P3733, DOI 10.1109/TCYB.2016.2578642
   Lai ZH, 2014, IEEE T NEUR NET LEAR, V25, P1942, DOI 10.1109/TNNLS.2013.2297381
   Li HF, 2006, IEEE T NEURAL NETWOR, V17, P157, DOI 10.1109/TNN.2005.860852
   Li M, 2005, PATTERN RECOGN LETT, V26, P527, DOI 10.1016/j.patrec.2004.09.007
   Li X, 2010, NEUROCOMPUTING, V73, P2571, DOI 10.1016/j.neucom.2010.05.016
   Lu YJ, 2009, IEEE T MULTIMEDIA, V11, P1289, DOI 10.1109/TMM.2009.2030632
   Lu YW, 2017, IEEE T MULTIMEDIA, V19, P2391, DOI 10.1109/TMM.2017.2703130
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Martinez A., 1998, AR FACE DATABASE
   Martìnez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Nie F., 2010, ADV NEURAL INFORM PR, P1813
   Panda R, 2017, IEEE T MULTIMEDIA, V19, P2010, DOI 10.1109/TMM.2017.2708981
   Pang YW, 2005, LECT NOTES COMPUT SC, V3644, P117
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shi XS, 2014, PATTERN RECOGN, V47, P2447, DOI 10.1016/j.patcog.2014.01.007
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Swets DL, 1996, IEEE T PATTERN ANAL, V18, P831, DOI 10.1109/34.531802
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Wang HX, 2014, IEEE T CYBERNETICS, V44, P828, DOI 10.1109/TCYB.2013.2273355
   Wang SP, 2017, IEEE T MULTIMEDIA, V19, P1454, DOI 10.1109/TMM.2017.2663324
   Wang XG, 2004, PROC CVPR IEEE, P564
   Wen JJ, 2016, PATTERN RECOGN, V60, P515, DOI 10.1016/j.patcog.2016.06.006
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Xie WC, 2017, IEEE T MULTIMEDIA, V19, P279, DOI 10.1109/TMM.2016.2614429
   Yan H, 2015, PATTERN RECOGN, V48, P1827, DOI 10.1016/j.patcog.2014.10.021
   Yan SC, 2007, IEEE T IMAGE PROCESS, V16, P212, DOI 10.1109/TIP.2006.884929
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang M, 2013, IEEE T IMAGE PROCESS, V22, P1753, DOI 10.1109/TIP.2012.2235849
   Yang WK, 2009, PATTERN RECOGN, V42, P2327, DOI 10.1016/j.patcog.2009.03.017
   Yang Y., 2011, P 22 INT JOINT C ART, P1589
   Ye JP, 2005, J MACH LEARN RES, V6, P483
   Yu H, 2001, PATTERN RECOGN, V34, P2067, DOI 10.1016/S0031-3203(00)00162-X
   Zhang J, 2015, NEUROCOMPUTING, V166, P455, DOI 10.1016/j.neucom.2015.03.033
   Zhang Z, 2016, IEEE T IMAGE PROCESS, V25, P2429, DOI 10.1109/TIP.2016.2547180
   Zhihua Qiao, 2009, IAENG International Journal of Applied Mathematics, V39, P48
   Zhong FJ, 2014, IEEE T NEUR NET LEAR, V25, P2065, DOI 10.1109/TNNLS.2014.2303798
   Zhong FJ, 2013, IEEE T IMAGE PROCESS, V22, P3018, DOI 10.1109/TIP.2013.2253476
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 64
TC 8
Z9 10
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3038
EP 3052
DI 10.1109/TMM.2019.2916093
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200006
DA 2024-07-18
ER

PT J
AU Ding, GD
   Zhang, SS
   Khan, S
   Tang, ZM
   Zhang, J
   Porikli, F
AF Ding, Guodong
   Zhang, Shanshan
   Khan, Salman
   Tang, Zhenmin
   Zhang, Jian
   Porikli, Fatih
TI Feature Affinity-Based Pseudo Labeling for Semi-Supervised Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Labeling; Training; Gallium nitride; Generative adversarial networks;
   Encoding; Semisupervised learning; Task analysis; Pseudo-labeling;
   semi-supervised learning; person re-identification; deep networks;
   generative modeling
ID TRACKING
AB Vision-based person re-identification aims to match a persons identity across multiple images, which is a fundamental task in multimedia content analysis and retrieval. Deep neural networks have recently manifested great potential in this task. However, a major bottleneck of existing supervised deep networks is their reliance on a large amount of annotated training data. Manual labeling for person identities in large-scale surveillance camera systems is quite challenging and incurs significant costs. Some recent studies adopt generative model outputs as training data augmentation. To more effectively use these synthetic data for an improved feature learning and re-identification performance, this paper proposes a novel feature affinity-based pseudo labeling method with two possible label encodings. To the best of our knowledge, this is the first study that employs pseudo-labeling by measuring the affinity of unlabeled samples with the underlying clusters of labeled data samples using the intermediate feature representations from deep networks. We propose training the network with the joint supervision of cross-entropy loss together with a center regularization term, which not only ensures discriminative feature representation learning but also simultaneously predicts pseudo-labels for unlabeled data. We show that both label encodings can be learned in a unified manner and help improve the overall performance. Our extensive experiments on three person re-identification datasets: Market-1501, DukeMTMC-reID, and CUHK03, demonstrate significant performance boost over the state-of-the-art person re-identification approaches.
C1 [Ding, Guodong; Zhang, Shanshan; Tang, Zhenmin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
   [Khan, Salman; Porikli, Fatih] Australian Natl Univ, Coll Engn & Comp Sci, Canberra, ACT 0200, Australia.
   [Zhang, Jian] Univ Technol Sydney, Sch Elect & Data Engn, Ultimo, NSW 2007, Australia.
C3 Nanjing University of Science & Technology; Australian National
   University; University of Technology Sydney
RP Ding, GD (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
EM guodong.ding@njust.edu.cn; shanshan.zhang@njust.edu.cn;
   salman.khan@anu.edu.au; tzm.cs@njust.edu.cn; Jian.Zhang@uts.edu.au;
   fatih.porikli@anu.edu.au
RI Tang, Zhenmin/AAY-6058-2020; Zhang, shanshan/HLP-6320-2023; Zhang,
   Shuo/IUO-8909-2023; Khan, Salman Hameed/M-4834-2016
OI Tang, Zhenmin/0000-0001-6708-2205; Zhang, Jian/0000-0002-7240-3541;
   Ding, Guodong/0000-0001-6080-5220; Khan, Salman
   Hameed/0000-0002-9502-1749
FU Funds for International Cooperation and Exchange of the National Natural
   Science Foundation of China [61861136011]; National Natural Science
   Foundation of China [61702262]; Natural Science Foundation of Jiangsu
   Province, China [BK20181299]; CCF-Tencent Open Fund [RAGR20180113];
   Fundamental Research Funds for the Central Universities [30918011322]
FX This work was supported in part by Funds for International Cooperation
   and Exchange of the National Natural Science Foundation of China (Grant
   No. 61861136011); National Natural Science Foundation of China (Grant
   No. 61702262); Natural Science Foundation of Jiangsu Province, China
   (Grant No. BK20181299); CCF-Tencent Open Fund (RAGR20180113); and in
   part by "the Fundamental Research Funds for the Central Universities"
   No. 30918011322.
CR [Anonymous], 2017, ICCV
   [Anonymous], 2018, ARXIV180308580
   [Anonymous], 2017, ARXIV170307220
   [Anonymous], 2016, ARXIV
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   [Anonymous], ACM T MULTIM COMPUT
   Arjovsky M., 2017, ARXIV170107875
   Bai S, 2017, PROC CVPR IEEE, P3356, DOI 10.1109/CVPR.2017.358
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Chen F, 2014, IEEE T MULTIMEDIA, V16, P455, DOI 10.1109/TMM.2013.2291967
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Ding GD, 2020, PATTERN RECOGN LETT, V137, P91, DOI 10.1016/j.patrec.2019.02.015
   Doquire G, 2013, NEUROCOMPUTING, V121, P5, DOI 10.1016/j.neucom.2012.10.028
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gulrajani Ishaan, 2017, P ADV NEUR INF PROC, P5767
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Y., 2018, ARXIV180106742
   Huang Y, 2017, NEUROCOMPUTING, V241, P191, DOI 10.1016/j.neucom.2017.02.055
   Kingma D. P., 2014, Advances in neural information processing systems, P3581
   Lee Dong-Hyun, 2013, INT C MACH LEARN WOR, P1
   Lee KH, 2015, IEEE T MULTIMEDIA, V17, P1429, DOI 10.1109/TMM.2015.2455418
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin J, 2017, PROC CVPR IEEE, P3396, DOI 10.1109/CVPR.2017.362
   Liu Z, 2018, IEEE T MULTIMEDIA, V20, P1321, DOI 10.1109/TMM.2017.2767781
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Odena Augustus, 2016, SEMISUPERVISED LEARN
   Radford A., 2015, ARXIV
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Schumann A, 2017, IEEE COMPUT SOC CONF, P1435, DOI 10.1109/CVPRW.2017.186
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Varior Rahul Rama, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang M, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P73, DOI 10.1145/2600428.2609599
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhou SP, 2017, PROC CVPR IEEE, P5028, DOI 10.1109/CVPR.2017.534
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
   Zhu SH, 2016, NEUROCOMPUTING, V208, P136, DOI 10.1016/j.neucom.2016.02.072
NR 59
TC 47
Z9 51
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2891
EP 2902
DI 10.1109/TMM.2019.2916456
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, Y
   Niu, BN
   Guan, H
   Zhang, SW
AF Huang, Ying
   Niu, Baoning
   Guan, Hu
   Zhang, Shuwu
TI Enhancing Image Watermarking With Adaptive Embedding Parameter and PSNR
   Guarantee
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive watermarking; differential quantization; image watermarking;
   spread spectrum
ID SINGULAR-VALUE DECOMPOSITION; ROBUST; SCHEME; DOMAIN; TRANSFORM;
   FRAMEWORK; DECODER
AB Watermarking plays an important role in identifying the copyright of an image and related issues. The state-of-the-art watermark embedding schemes, spread spectrum and quantization, suffer from host signal interference (HSI) and scaling attacks, respectively. Both of them use a fixed embedding parameter, which is difficult to take both robustness and imperceptibility into account for all images. This paper solves the problems by proposing two novel blind watermarking schemes: a spread spectrum scheme with adaptive embedding strength (SSAES) and a differential quantization scheme with adaptive quantization threshold (DQAQT). Their adaptiveness comes from the proposed adaptive embedding strategy (AEP), which maximizes the embedding strength or quantization threshold by guaranteeing the peak signal-to-noise ratio (PSNR) of the host image after embedding the watermark, and strikes the balance between robustness and imperceptibility. SSAES is HSI free by factoring in the priori knowledge about HSI. In DQAQT, an effective quantization mode is proposed to resist scaling attacks by utilizing the difference between two selected DCT coefficients with high stability. Both SSAES and DQAQT can be easily applied to other watermarking frameworks. We introduce a notion called error threshold to theoretically analyze the performance of our proposed methods in details. The experimental results consistently demonstrate that SSAES and DQAQT outperform the state-of-the-art methods in terms of imperceptibility, robustness, computational cost, and adaptability.
C1 [Huang, Ying; Niu, Baoning] Taiyuan Univ Technol, Sch Informat & Comp, Taiyuan 030024, Shanxi, Peoples R China.
   [Guan, Hu; Zhang, Shuwu] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Zhang, Shuwu] Beijing Film Acad, Adv Innovat Ctr Future Visual Entertainment, Beijing 100088, Peoples R China.
C3 Taiyuan University of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Beijing Film Academy
RP Niu, BN (corresponding author), Taiyuan Univ Technol, Sch Informat & Comp, Taiyuan 030024, Shanxi, Peoples R China.
EM huangying0041@link.tyut.edu.cn; niubaoning@tyut.edu.cn;
   hu.guan@ia.ac.cn; shuwu.zhang@ia.ac.cn
RI Niu, Baoning/ACC-8776-2022
OI Niu, Baoning/0000-0002-7924-3384; guan, hu/0000-0001-7102-2299; Huang,
   Ying/0000-0002-7554-063X
FU National Key R&D Program of China [2017YFB1401000]; Key Laboratory of
   Digital Rights Services
FX This work was supported by the National Key R&D Program of China under
   Grant 2017YFB1401000 and in part by the Key Laboratory of Digital Rights
   Services.
CR Ali M, 2015, INFORM SCIENCES, V301, P44, DOI 10.1016/j.ins.2014.12.042
   Asikuzzaman Md, 2014, IEEE Transactions on Information Forensics and Security, V9, P1502, DOI 10.1109/TIFS.2014.2338274
   Asikuzzaman M, 2018, IEEE T CIRC SYST VID, V28, P2131, DOI 10.1109/TCSVT.2017.2712162
   Asikuzzaman M, 2016, IEEE T MULTIMEDIA, V18, P1733, DOI 10.1109/TMM.2016.2589208
   Bhinder P, 2018, MULTIMED TOOLS APPL, V77, P10303, DOI 10.1007/s11042-018-5635-z
   Chen B., 2000, P INT S INF THEOR, V47, P1423
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   Deng C, 2010, SIGNAL PROCESS, V90, P3256, DOI 10.1016/j.sigpro.2010.05.032
   Fazlali HR, 2017, MULTIMED TOOLS APPL, V76, P3105, DOI 10.1007/s11042-015-3200-6
   Gao XB, 2010, COGN COMPUT, V2, P68, DOI 10.1007/s12559-010-9033-8
   Guan H, 2014, 2014 INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE, ELECTRONICS AND ELECTRICAL ENGINEERING (ISEEE), VOLS 1-3, P1803
   Hwang MJ, 2018, IEEE T MULTIMEDIA, V20, P45, DOI 10.1109/TMM.2017.2721642
   Li CL, 2015, J ELECTR COMPUT ENG, V2015, DOI 10.1155/2015/370615
   Li XL, 2013, IEEE T IMAGE PROCESS, V22, P2181, DOI 10.1109/TIP.2013.2246179
   Liu JH, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20120945
   Liu JH, 2018, INFORMATION, V9, DOI 10.3390/info9080194
   Makbol NM, 2017, INFORM SCIENCES, V417, P381, DOI 10.1016/j.ins.2017.07.026
   Mehta R, 2017, INT J MACH LEARN CYB, V8, P379, DOI 10.1007/s13042-015-0331-z
   Mohrekesh M, 2018, MULTIMED TOOLS APPL, V77, P30865, DOI 10.1007/s11042-018-6129-8
   Nasir I, 2012, IET IMAGE PROCESS, V6, P354, DOI 10.1049/iet-ipr.2010.0421
   Parah SA, 2016, DIGIT SIGNAL PROCESS, V53, P11, DOI 10.1016/j.dsp.2016.02.005
   Sadreazami H, 2016, IEEE T MULTIMEDIA, V18, P196, DOI 10.1109/TMM.2015.2508147
   Su QT, 2018, SOFT COMPUT, V22, P91, DOI 10.1007/s00500-017-2489-7
   Thien HT, 2018, INFORM SCIENCES, V426, P1, DOI 10.1016/j.ins.2017.10.016
   Thien HT, 2016, EXPERT SYST APPL, V62, P177, DOI 10.1016/j.eswa.2016.06.015
   Valizadeh A., 2011, IEEE Transactions on Information Forensics and Security, V6, P267, DOI 10.1109/TIFS.2010.2103061
   Valizadeh A, 2012, IEEE T INF FOREN SEC, V7, P1127, DOI 10.1109/TIFS.2012.2199312
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P311, DOI 10.1109/TMM.2013.2291658
   Ye X, 2015, INT C IM SIGN PROC, P323
   Zeng GR, 2008, 2008 INTERNATIONAL SYMPOSIUM ON INTELLIGENT INFORMATION TECHNOLOGY APPLICATION WORKSHOP: IITA 2008 WORKSHOPS, PROCEEDINGS, P573, DOI 10.1109/IITA.Workshops.2008.126
   Zhu XS, 2014, IEEE T MULTIMEDIA, V16, P1888, DOI 10.1109/TMM.2014.2340695
   Zong TR, 2016, IEEE ACCESS, V4, P1689, DOI 10.1109/ACCESS.2016.2556723
NR 33
TC 53
Z9 59
U1 2
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2447
EP 2460
DI 10.1109/TMM.2019.2907475
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400002
DA 2024-07-18
ER

PT J
AU Liu, LB
   Li, GB
   Xie, Y
   Yu, YZ
   Wang, Q
   Lin, L
AF Liu, Lingbo
   Li, Guanbin
   Xie, Yuan
   Yu, Yizhou
   Wang, Qing
   Lin, Liang
TI Facial Landmark Machines: A Backbone-Branches Architecture With
   Progressive Representation Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial landmark localization; cascaded backbone-branches; fully
   convolutional neural networks; unconstrained settings
ID FACE ALIGNMENT; NETWORK
AB Facial landmark localization plays a critical role in face recognition and analysis. In this paper, we propose a novel cascaded backbone-branches fully convolutional neural network (BB-FCN) for rapidly and accurately localizing facial landmarks in unconstrained and cluttered settings. Our proposed BB-FCN generates facial landmark response maps directly from raw images without any preprocessing. BB-FCN follows a coarseto-fine cascaded pipeline, which consists of a backbone network to roughly detect the locations of all facial landmarks and one branch network for each type of detected landmark to further refine its location. Furthermore, to facilitate the facial landmark localization under unconstrained settings, we propose a large-scale benchmark named SYSU16K, which contains 16 000 faces with large variations in pose, expression, illumination, and resolution. Extensive experimental evaluations demonstrate that our proposed BB-FCN can significantly outperform the state of the art under both constrained (i.e., within detected facial regions only) and unconstrained settings. We further confirm that high-quality facial landmarks localized with our proposed network can also improve the precision and recall of face detection.
C1 [Liu, Lingbo; Li, Guanbin; Xie, Yuan; Wang, Qing; Lin, Liang] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
   [Yu, Yizhou] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Sun Yat Sen University; University of Hong Kong
RP Li, GB (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
EM liulingb@mail2.sysu.edu.cn; liguanbin@mail.sysu.edu.cn;
   xiey39@mail2.sysu.edu.cn; yizhouy@acm.org; wangq79@mail.sysu.edu.cn;
   linliang@ieee.org
RI Lin, Kuan-Yu/JXM-6653-2024; zhang, cl/JDW-6549-2023; L, J/JEF-9564-2023;
   Lin, Liang/IQR-8601-2023; LU, LU/JEZ-4760-2023; l, j/JVZ-8480-2024; l,
   j/HNC-5728-2023; Li, Jiaxi/HTS-3430-2023; Lin, L/HKO-8213-2023;
   /F-3345-2010
OI Lin, Liang/0000-0003-2248-3755; Li, Jiaxi/0000-0002-8197-8590;
   /0000-0002-0470-5548; Liu, Lingbo/0000-0001-8179-6685
FU National Key Research and Development Program of China [2018YFC0830103];
   NSFC-Shenzhen Robotics Projects [U1613211]; Hong Kong Research Grants
   Council under General Research Funds [HKU17206218]; National Natural
   Science Foundation of China [61836012]; Ministry of Public Security
   Science and Technology Police Foundation Project [2016GABJC48];
   SenseTime Research Fund
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFC0830103, in part by the
   NSFC-Shenzhen Robotics Projects (U1613211), in part by the Hong Kong
   Research Grants Council under General Research Funds (HKU17206218), in
   part by the National Natural Science Foundation of China under Grant
   61836012, in part by the Ministry of Public Security Science and
   Technology Police Foundation Project 2016GABJC48, and in part by the
   SenseTime Research Fund. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Fatih
   Porikli.
CR [Anonymous], 2010, BMVC
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Chen D, 2014, LECT NOTES COMPUT SC, V8694, P109, DOI 10.1007/978-3-319-10599-4_8
   Chen TS, 2016, IEEE T NEUR NET LEAR, V27, P1135, DOI 10.1109/TNNLS.2015.2506664
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Dantone M, 2012, PROC CVPR IEEE, P2578, DOI 10.1109/CVPR.2012.6247976
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dong XY, 2018, PROC CVPR IEEE, P360, DOI 10.1109/CVPR.2018.00045
   Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan X, 2018, IEEE T MULTIMEDIA, V20, P567, DOI 10.1109/TMM.2017.2751143
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Hao ZK, 2017, PROC CVPR IEEE, P1913, DOI 10.1109/CVPR.2017.207
   Honari S, 2018, PROC CVPR IEEE, P1546, DOI 10.1109/CVPR.2018.00167
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   Jourabloo A, 2017, IEEE I CONF COMP VIS, P3219, DOI 10.1109/ICCV.2017.347
   Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454
   KAZEMI V, 2014, PROC CVPR IEEE, P1867, DOI [DOI 10.1109/CVPR.2014.241, 10.1109/CVPR.2014.241]
   Koestinger M., 2011, IEEE INT C COMP VIS, P2144, DOI DOI 10.1109/ICCVW.2011.6130513
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Li GB, 2018, IEEE T NEUR NET LEAR, V29, P6038, DOI 10.1109/TNNLS.2018.2817540
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li HX, 2014, PROC CVPR IEEE, P1843, DOI 10.1109/CVPR.2014.238
   LI HX, 2015, PROC CVPR IEEE, P5325, DOI DOI 10.1109/CVPR.2015.7299170
   Li Y, 2017, COMM COM INF SC, V772, P62, DOI 10.1007/978-981-10-7302-1_6
   Li YZ, 2016, LECT NOTES COMPUT SC, V9907, P420, DOI 10.1007/978-3-319-46487-9_26
   Liang L, 2008, LECT NOTES COMPUT SC, V5303, P72, DOI 10.1007/978-3-540-88688-4_6
   Liu C, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON SMART CITY AND SYSTEMS ENGINEERING (ICSCSE), P849, DOI 10.1109/ICSCSE.2018.00183
   Liu F, 2016, LECT NOTES COMPUT SC, V9909, P545, DOI 10.1007/978-3-319-46454-1_33
   Liu HL, 2016, IEEE T MULTIMEDIA, V18, P1233, DOI 10.1109/TMM.2016.2556859
   Liu LB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1553, DOI 10.1145/3240508.3240681
   Liu LQ, 2016, IEEE T MULTIMEDIA, V18, P64, DOI 10.1109/TMM.2015.2500730
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu C, 2015, AAAI CONF ARTIF INTE, P3811
   Luo P, 2013, IEEE I CONF COMP VIS, P2864, DOI 10.1109/ICCV.2013.356
   Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393
   Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47
   Merget D, 2018, PROC CVPR IEEE, P781, DOI 10.1109/CVPR.2018.00088
   Peng X, 2016, LECT NOTES COMPUT SC, V9905, P38, DOI 10.1007/978-3-319-46448-0_3
   QIN HW, 2016, PROC CVPR IEEE, P3456, DOI DOI 10.1109/CVPR.2016.376
   Ranjana J. Shourie, 2015, 2015 40th International Conference on Infrared, Millimeter and Terahertz Waves (IRMMW-THz), P1, DOI 10.1109/IRMMW-THz.2015.7327917
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4
   Sauer P, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.30
   Shen XH, 2013, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2013.444
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tang ZQ, 2018, LECT NOTES COMPUT SC, V11207, P348, DOI 10.1007/978-3-030-01219-9_21
   Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453
   TUZEL O, 2016, PROC EUR CONF COMPUT, V9909, P825, DOI DOI 10.1007/978-3-319-46454-1_50
   Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Valstar M, 2010, PROC CVPR IEEE, P2729, DOI 10.1109/CVPR.2010.5539996
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508
   Xiao ST, 2017, IEEE I CONF COMP VIS, P1642, DOI 10.1109/ICCV.2017.181
   Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yan JJ, 2014, IMAGE VISION COMPUT, V32, P790, DOI 10.1016/j.imavis.2013.12.004
   Yan ZC, 2015, IEEE I CONF COMP VIS, P2740, DOI 10.1109/ICCV.2015.314
   Yang B, 2015, IEEE I CONF COMP VIS, P82, DOI 10.1109/ICCV.2015.18
   Yang H, 2015, INT J ENVIRON SCI TE, V12, P1173, DOI 10.1007/s13762-014-0514-2
   Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419
   Yu X, 2013, IEEE I CONF COMP VIS, P1944, DOI 10.1109/ICCV.2013.244
   Zhang J, 2016, PROC CVPR IEEE, P3428, DOI 10.1109/CVPR.2016.373
   Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1
   ZHANG L, 2017, IEEE SIGNAL PROCESS, V23, P11499, DOI DOI 10.1002/CHEM.201702745
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhang YT, 2018, PROC CVPR IEEE, P2694, DOI 10.1109/CVPR.2018.00285
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
   Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 85
TC 18
Z9 20
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2248
EP 2262
DI 10.1109/TMM.2019.2902096
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ma, C
   Guo, YL
   Yang, JG
   An, W
AF Ma, Chao
   Guo, Yulan
   Yang, Jungang
   An, Wei
TI Learning Multi-View Representation With LSTM for 3-D Shape Recognition
   and Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D shape; multi-view; object recognition; object retrieval; CNN; LSTM
ID 3D OBJECT RECOGNITION; NEURAL-NETWORKS; SURFACE-FEATURE
AB Shape representation for 3-D models is an important topic in computer vision, multimedia analysis, and computer graphics. Recent multiview-based methods demonstrate promising performance for 3-D shape recognition and retrieval. However, most multiview-based methods ignore the correlations of multiple views or suffer from high computional cost. In this paper, we propose a novel multiview-based network architecture for 3-D shape recognition and retrieval. Our network combines convolutional neural networks (CNNs) with long short-term memory (LSTM) to exploit the correlative information from multiple views. Well-pretrained CNNs with residual connections are first used to extract a low-level feature of each view image rendered from a 3-D shape. Then, a LSTM and a sequence voting layer are employed to aggregate these features into a shape descriptor. The highway network and a three-step training strategy are also adopted to boost the optimization of the deep network. Experimental results on two public datasets demonstrate that the proposed method achieves promising performance for 3-D shape recognition and the state-of-the-art performance for the 3-D shape retrieval.
C1 [Ma, Chao; Guo, Yulan; Yang, Jungang; An, Wei] Natl Univ Def Technol, Coll Elect Sci, Changsha 410073, Hunan, Peoples R China.
   [Guo, Yulan] Sun Yat Sen Univ, Sch Elect & Commun Engn, Guangzhou 510275, Guangdong, Peoples R China.
C3 National University of Defense Technology - China; Sun Yat Sen
   University
RP Guo, YL (corresponding author), Natl Univ Def Technol, Coll Elect Sci, Changsha 410073, Hunan, Peoples R China.
EM machao0408@nudt.edu.cn; yulan.guo@nudt.edu.cn; yangjungang@nudt.edu.cn;
   anwei@nudt.edu.cn
RI guo, yu/GQZ-1392-2022; Ma, Chao/AAD-8826-2019; Guo, Yulan/E-7102-2014
OI Ma, Chao/0000-0003-0751-7865; Guo, Yulan/0000-0001-7051-841X; Yang,
   Jungang/0000-0002-3127-8705
FU National Natural Science Foundation of China [61602499, 61471371,
   61401474]; Hunan Provincial Natural Science Foundation [2016JJ3025];
   National Postdoctoral Program for Innovative Talents [BX201600172];
   China Postdoctoral Science Foundation; Fundamental Research Funds for
   the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61602499, Grant 61471371, and Grant
   61401474, in part by the Hunan Provincial Natural Science Foundation
   under Grant 2016JJ3025, in part by the National Postdoctoral Program for
   Innovative Talents (BX201600172), and in part by the China Postdoctoral
   Science Foundation and Fundamental Research Funds for the Central
   Universities.
CR [Anonymous], 2017, EUROGRAPHICS WORKSHO
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2009, IEEE INT C ROB AUT
   [Anonymous], 2016, P EUR WORKSH 3D OBJ
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2016, NEURIPS 3D DEEP LEAR
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], PROC CHIN CONF IMAGE
   [Anonymous], PROC 13TH AAAI CONF
   [Anonymous], 2016, P IEEE INT S CIRC SY
   [Anonymous], INT J COMPUT VIS
   [Anonymous], 2016, ARXIV160705695
   [Anonymous], ORIENTATION BOOSTED
   [Anonymous], 2015, ARXIV150204623
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Funkhouser T.A., 2015, P COMP VIS PATT REC
   Gong BQ, 2013, IEEE T MULTIMEDIA, V15, P369, DOI 10.1109/TMM.2012.2231059
   Guo YL, 2015, INFORM SCIENCES, V293, P196, DOI 10.1016/j.ins.2014.09.015
   Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Ioannidou A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3042064
   Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414
   Johnson AE, 1998, IMAGE VISION COMPUT, V16, P635, DOI 10.1016/S0262-8856(98)00074-2
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Paquet E, 2000, SIGNAL PROCESS-IMAGE, V16, P103, DOI 10.1016/S0923-5965(00)00020-5
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Rusu RB, 2010, IEEE INT C INT ROBOT, P2155, DOI 10.1109/IROS.2010.5651280
   Savva M., 2016, P EUR WORKSH 3D OBJ, P89, DOI DOI 10.2312/3DOR.20161092
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sfikas K, 2018, COMPUT GRAPH-UK, V71, P208, DOI 10.1016/j.cag.2017.12.001
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Simonyan K., 2014, 14091556 ARXIV
   Srivastava RK, 2015, ARXIV150500387
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Sutskever I, 2014, ADV NEUR IN, V27
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Xu K., 2015, COMPUTER SCI, P2048
   Xuanhan Wang, 2017, IEEE Signal Processing Letters, V24, P510, DOI 10.1109/LSP.2016.2611485
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
   Zhi SF, 2018, COMPUT GRAPH-UK, V71, P199, DOI 10.1016/j.cag.2017.10.007
NR 54
TC 121
Z9 138
U1 2
U2 46
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1169
EP 1182
DI 10.1109/TMM.2018.2875512
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600008
DA 2024-07-18
ER

PT J
AU Merler, M
   Mac, KNC
   Joshi, D
   Nguyen, QB
   Hammer, S
   Kent, J
   Xiong, JJ
   Do, MN
   Smith, JR
   Feris, RS
AF Merler, Michele
   Mac, Khoi-Nguyen C.
   Joshi, Dhiraj
   Quoc-Bao Nguyen
   Hammer, Stephen
   Kent, John
   Xiong, Jinjun
   Do, Minh N.
   Smith, John R.
   Feris, Rogerio Schmidt
TI Automatic Curation of Sports Highlights Using Multimodal Excitement
   Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Highlights generation; sport analytics; multimodal video analysis
AB The production of sports highlight packages summarizing a game's most exciting moments is an essential task for broadcast media. Yet, it requires labor-intensive video editing. We propose a novel approach for auto-curating sports highlights, and demonstrate it to create a first of a kind, real-world system for the editorial aid of golf and tennis highlight reels. Our method fuses information from the players' reactions (action recognition such as high-fives and fist pumps), players' expressions (aggressive, tense, smiling, and neutral), spectators (crowd cheering), commentator (tone of the voice and word analysis), and game analytics to determine the most interesting moments of a game. We accurately identify the start and end frames of key shot highlights with additional metadata, such as the player's name and the whole number, or analysts input allowing personalized content summarization and retrieval. In addition, we introduce new techniques for learning our classifiers with reduced manual training data annotation by exploiting the correlation of different modalities. Our work has been demonstrated at a major golf tournament (2017 Masters) and two major international tennis tournaments (2017 Wimbledon and U.S. Open), successfully extracting highlights through the course of the sporting events. For the 2017 Masters, 54% of the clips selected by our system overlapped with the official highlights reels. Furthermore, user studies showed that 90% of the non-overlapping ones were of the same quality of the official clips for the 2017 Masters, while the automatic selection of clips for highlights of 2017 Wimbledon and 2017 US Open agreed with human preferences 80% and 84.2% of the time, respectively.
C1 [Merler, Michele; Joshi, Dhiraj; Quoc-Bao Nguyen; Xiong, Jinjun; Smith, John R.; Feris, Rogerio Schmidt] IBM Res, Yorktown Hts, NY 10598 USA.
   [Mac, Khoi-Nguyen C.; Do, Minh N.] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
   [Hammer, Stephen; Kent, John] IBM iX, New York, NY 10003 USA.
C3 International Business Machines (IBM); University of Illinois System;
   University of Illinois Urbana-Champaign
RP Merler, M (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.
EM mimerler@us.ibm.com; knmac@illinois.edu; djoshi@us.ibm.com;
   quocbao@us.ibm.com; hammers@us.ibm.edu; johnkent@us.ibm.edu;
   jinjun@us.ibm.com; minhdo@illinois.edu; jsmith@us.ibm.com;
   rsferis@us.ibm.com
RI Mac, Khoi-Nguyen C./JXM-0189-2024; Do, Minh N./AAX-8498-2020; Smith,
   John/Y-2316-2019; ARSLAN, Okan/AAA-3232-2020; Smith, John/HJB-2300-2022;
   Smith, John/GYJ-1302-2022
OI Mac, Khoi-Nguyen C./0000-0003-4623-4016; Do, Minh
   N./0000-0001-5132-4986; Smith, John/0000-0001-6885-1117; Kent,
   John/0000-0002-2235-9431; Xiong, Jinjun/0000-0002-2620-4859; Merler,
   Michele/0000-0002-4358-8671
CR Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13
   [Anonymous], 2002, P 10 ACM INT C MULT
   [Anonymous], P 2006 IEEE COMP SOC
   Aytar Y, 2016, ADV NEUR IN, V29
   Baijal A, 2015, I SYMP CONSUM ELECTR, P20, DOI 10.1109/ICCE.2015.7066303
   Bettadapura Vinay., 2016, P ACM INT C MULT, P908
   Decroos T, 2017, AAAI CONF ARTIF INTE, P1302
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Hasan T, 2013, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2013-173
   Irie G., 2010, P 18 ACM INT C MULTI, P839
   Javed A, 2016, IEEE SIGNAL PROC LET, V23, P954, DOI 10.1109/LSP.2016.2573042
   Jayaraman D, 2015, IEEE I CONF COMP VIS, P1413, DOI 10.1109/ICCV.2015.166
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Joshi D, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1249, DOI 10.1145/3123266.3127924
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Ma SG, 2017, PATTERN RECOGN, V68, P334, DOI 10.1016/j.patcog.2017.01.027
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Merler M, 2017, IEEE COMPUT SOC CONF, P57, DOI 10.1109/CVPRW.2017.14
   Mobahi H., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P737
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith JR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1799, DOI 10.1145/3123266.3127906
   Smith R, 2007, PROC INT CONF DOC, P629, DOI 10.1109/icdar.2007.4376991
   Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51
   Tang Anthony., 2012, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. CHI '12, P1569
   Veit A, 2017, PROC CVPR IEEE, P6575, DOI 10.1109/CVPR.2017.696
   Wang J, 2016, PROC CVPR IEEE, P2295, DOI 10.1109/CVPR.2016.252
   Xiong ZY, 2003, IEEE IMAGE PROC, P5
   Xiong ZY, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P632
   Xu HT, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2198
   Yang H, 2015, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2015.526
   Yao Ting, 2016, PROC CVPR IEEE, P982, DOI DOI 10.1109/CVPR.2016.112
   Zhang D., 2002, ACM Multimedia, P315
   Zhang J, 2018, IEEE T IMAGE PROCESS, V27, P2420, DOI 10.1109/TIP.2018.2804218
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao Z, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1613, DOI 10.1109/ICME.2006.262855
NR 41
TC 36
Z9 37
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1147
EP 1160
DI 10.1109/TMM.2018.2876046
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600006
DA 2024-07-18
ER

PT J
AU Tang, MH
   Wen, JT
   Zhang, Y
   Gu, JW
   Junker, P
   Guo, BC
   Jhao, G
   Zhu, ZY
   Han, YX
AF Tang, Minhao
   Wen, Jiangtao
   Zhang, Yu
   Gu, Jiawen
   Junker, Philip
   Guo, Bichuan
   Jhao, Guansyun
   Zhu, Ziyu
   Han, Yuxing
TI A Universal Optical Flow Based Real-Time Low-Latency Omnidirectional
   Stereo Video System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Omnidirectional video; video coding; video stitch
ID EFFICIENCY; H.264/AVC; DECISION; OBJECTS
AB Omnidirectional stereoscopic video (ODSV) is a key element of creating an immersive experience for virtual reality that has attracted extensive interest while presenting many technical challenges. Two such key challenges are real-time, low-latency high-quality seamless video stitching from multiple cameras, and faithful reconstruction of 3-D information. Even though various attempts have been made to achieve different combinations of real-time, low-latency, automation, and high output resolution in stereoscopic panoramic video communication, achieving these characteristics simultaneously remains a challenge to be tackled. In this paper, we present a universally applicable and practical end-to-end system based on a novel real-time optical flow algorithm to produce high-quality real-time ODSV with reconstructed 3-D depth information at low latency. Through a configurable process, various camera systems can be calibrated and seamlessly stitched together using the proposed system. The stitched 3-D panoramic video is encoded with a standard compliant video encoder that is optimized for panoramic video. Thanks to various optimizations introduced in this paper, the proposed system is capable of producing real-time ODSV of ultra High definition resolution with a glass to glass latency of 2.2 s using a desktop computer with a single Nvidia graphic card. Experiments show that the proposed system achieves an encoding performance superior to existing open-source HEVC implementations and an optical flow estimation performance better than the Facebook algorithm while running two orders of magnitudes faster.
C1 [Tang, Minhao; Wen, Jiangtao; Zhang, Yu; Gu, Jiawen; Guo, Bichuan; Jhao, Guansyun] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Junker, Philip] Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland.
   [Zhu, Ziyu] Beihang Univ, Beijing 100083, Peoples R China.
   [Han, Yuxing] South China Agr Univ, Guangzhou 510640, Guangdong, Peoples R China.
C3 Tsinghua University; Swiss Federal Institutes of Technology Domain; ETH
   Zurich; Beihang University; South China Agricultural University
RP Han, YX (corresponding author), South China Agr Univ, Guangzhou 510640, Guangdong, Peoples R China.
EM tmh14@mails.tsinghua.edu.cn; jtwen@mail.tsinghua.edu.cn;
   yuzhangiot@gmail.com; gujw15@mails.tsinghua.edu.cn;
   philipjunker@gmail.com; gbc16@mails.tsinghua.edu.cn;
   zgx16@mails.tsinghua.edu.cn; zhuziyu.edward@gmail.com;
   yuxinghan@scau.edu.cn
RI zhang, yu/AAK-7937-2021; Tang, Minhao/AAF-7177-2019
OI Tang, Minhao/0000-0002-5421-1116; zhu, ziyu/0000-0003-1556-0791
FU Shenzhen Boyan Information Technology Ltd.
FX This work was supported by the Shenzhen Boyan Information Technology
   Ltd. The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Cha Zhang. (Corresponding author:
   Yuxing Han.)
CR Adelson E. H., 1984, RCA engineer, V29, P33, DOI 10.1.1.59.9419.
   Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   Anderson R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980257
   [Anonymous], EUROPEAN SCI NOTES
   [Anonymous], 2017, CVPR
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Bao LC, 2014, PROC CVPR IEEE, P3534, DOI 10.1109/CVPR.2014.452
   Bjontegaard G., 2008, VCEGAI11
   Bouguet J.-Y., 2004, CAMERA CALIBRATION T
   Chen YC, 2015, IEEE T CIRC SYST VID, V25, P1423, DOI 10.1109/TCSVT.2014.2380231
   Chen ZY, 2013, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2013.316
   Chi YM, 2007, INT CONF ACOUST SPEE, P1017
   Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Hachicha W, 2015, IEEE T MULTIMEDIA, V17, P765, DOI 10.1109/TMM.2015.2417099
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Heikkila J, 1997, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.1997.609468
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156
   Hu Q, 2016, IEEE T MULTIMEDIA, V18, P379, DOI 10.1109/TMM.2015.2512799
   Hu YL, 2016, PROC CVPR IEEE, P5704, DOI 10.1109/CVPR.2016.615
   Knorr S., 2017, PROC IRISH MACH VIS, P4
   Krishnamurthy R., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P570, DOI 10.1109/ICIP.1995.531430
   Lang M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185530
   Lee Z, 2015, IEEE T MULTIMEDIA, V17, P792, DOI 10.1109/TMM.2015.2425141
   Lei JJ, 2017, IEEE T MULTIMEDIA, V19, P1442, DOI 10.1109/TMM.2017.2660440
   Levin A, 2004, LECT NOTES COMPUT SC, V2034, P377
   Li JS, 2016, IEEE IMAGE PROC, P370, DOI 10.1109/ICIP.2016.7532381
   Lin S, 1997, INT CONF ACOUST SPEE, P2869, DOI 10.1109/ICASSP.1997.595388
   Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Ng KT, 2005, IEEE T CIRC SYST VID, V15, P82, DOI 10.1109/TCSVT.2004.839989
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Richardson I. E., 2003, CISC VIS NETW IND GL
   Richardt C, 2013, PROC CVPR IEEE, P1256, DOI 10.1109/CVPR.2013.166
   Santoro M, 2012, IEEE INT WORKSH MULT, P186, DOI 10.1109/MMSP.2012.6343438
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen T, 2013, IEEE DATA COMPR CONF, P241, DOI 10.1109/DCC.2013.32
   Snyder J. P., 1997, FLATTENING EARTH 200
   Sobel I, 1968, STAND ART INT PROJ
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Tang M., 2018, IEEE T CIRCUITS SYST
   TANG MH, 2015, P IEEE INT S CIRC SY, P2748
   Tang MH, 2017, IEEE INT CON MULTI, P799, DOI 10.1109/ICME.2017.8019460
   Tao M, 2012, COMPUT GRAPH FORUM, V31, P345, DOI 10.1111/j.1467-8659.2012.03013.x
   TURIN GL, 1960, IRE T INFORM THEOR, V6, P311, DOI 10.1109/TIT.1960.1057571
   Uyttendaele M, 2001, PROC CVPR IEEE, P509
   Wang RG, 2017, IEEE T MULTIMEDIA, V19, P1392, DOI 10.1109/TMM.2017.2654120
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Yu Matt., 2015, P 3 INT WORKSHOP IMM, P1
   Zhang H, 2014, IEEE T CIRC SYST VID, V24, P660, DOI 10.1109/TCSVT.2013.2290578
NR 55
TC 15
Z9 15
U1 3
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 957
EP 972
DI 10.1109/TMM.2018.2867266
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700012
DA 2024-07-18
ER

PT J
AU Mada, BE
   Bagaa, M
   Taleb, T
AF Mada, Badr Eddine
   Bagaa, Miloud
   Taleb, Tarik
TI Trust-Based Video Management Framework for Social Multimedia Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social multimedia network; video streaming; trust model; and trust
   management
AB Social multimedia networks (SMNs) have attracted much attention from both academia and industry due to their impact on our daily lives. The requirements of SMN users are increasing along with time, which make the satisfaction of those requirements a very challenging process. One important challenge facing SMNs consists of their internal users that can upload and manipulate insecure, untrusted, and unauthorized contents. For this purpose, controlling and verifying content delivered to end users is becoming a highly challenging process. So far, many researchers have investigated the possibilities of implementing a trustworthy SMN. In this vein, the aim of this paper is to propose a framework that allows collaboration between humans and machines to ensure secure delivery of trusted video content over SMNs while ensuring an optimal deployment cost in the form of CPU, RAM, and storage. The key concepts beneath the proposed framework consist in assigning to each user a level of trust based on his/her history, creating an intelligent agent that decides which content can be automatically published on the network and which content should be reviewed or rejected, and checking the videos' integrity and delivery during the streaming process. Accordingly, we ensure that the trust level of the SMNs increases. Simultaneously, efficient capital expenditure and operational expenditures can be achieved.
C1 [Mada, Badr Eddine; Bagaa, Miloud; Taleb, Tarik] Aalto Univ, Dept Commun & Networking, Sch Elect Engn, Espoo 02150, Finland.
   [Taleb, Tarik] Univ Oulu, Ctr Wireless Commun, FI-90014 Oulu, Finland.
   [Taleb, Tarik] Sejong Univ, Comp & Informat Secur Dept, Seoul 143747 0500, South Korea.
C3 Aalto University; University of Oulu; Sejong University
RP Bagaa, M (corresponding author), Aalto Univ, Dept Commun & Networking, Sch Elect Engn, Espoo 02150, Finland.
EM badr.mada@aalto.fi; miloud.bagaa@aalto.fi; tarik.taleb@aalto.fi
RI Taleb, Tarik/ABD-6339-2021; Bagaa, Miloud/M-9806-2016
OI Mada, Badr eddine/0000-0001-5262-3793; Bagaa,
   Miloud/0000-0001-5280-3276; Taleb, Tarik/0000-0003-1119-1239
FU ANASTACIA Project from European Unions Horizon 2020 Research and
   Innovation Programme [731558]; Swiss State Secretariat for Education,
   Research and Innovation; Academy of Finland 6Genesis Flagship [318927];
   H2020 Societal Challenges Programme [731558] Funding Source: H2020
   Societal Challenges Programme
FX This work was supported in part by the ANASTACIA Project, which has
   received funding from the European Unions Horizon 2020 Research and
   Innovation Programme under Grant Agreement 731558 and from the Swiss
   State Secretariat for Education, Research and Innovation, and in part by
   the Academy of Finland 6Genesis Flagship under Grant 318927. The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Zhou Su. (Corresponding author: Miloud Bagaa.)
CR Agriomallos I, 2018, IEEE ROBOT AUTOM LET, V3, P942, DOI 10.1109/LRA.2018.2793346
   [Anonymous], 2014, Markov decision processes: discrete stochastic dynamic programming
   [Anonymous], 2017, P IEEE GLOB COMM C
   Cao L., 2012, 2012 Third International Conference on Networking and Distributed Computing, Hangzhou, P161, DOI [10.1109/ICNDC.2012.46, DOI 10.1109/ICNDC.2012.46]
   Chang W, 2015, PROC INT CONF PARAL, P570, DOI 10.1109/ICPP.2015.66
   Das K, 2017, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT SUSTAINABLE SYSTEMS (ICISS 2017), P28, DOI 10.1109/ISS1.2017.8389423
   DuBois T., 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P418, DOI 10.1109/PASSAT/SocialCom.2011.56
   DuBois T, 2009, 2009 IEEE/WIC/ACM INTERNATIONAL JOINT CONFERENCES ON WEB INTELLIGENCE (WI) AND INTELLIGENT AGENT TECHNOLOGIES (IAT), VOL 1, P655
   Ganz  A., 2007, P 37 ANN FRONT ED C
   Gao L., 2016, SIGGRAPH ASIA 2016 M
   Hall S., 2011, NAECON 2011 - IEEE National Aerospace and Electronics Conference, P139, DOI 10.1109/NAECON.2011.6183091
   Hall S, 2012, PROC NAECON IEEE NAT, P51, DOI 10.1109/NAECON.2012.6531028
   Hussain S., 2011, P ICUMT BUD HUNG, P1
   Jabeen F, 2018, IEEE ACCESS, V6, P17246, DOI 10.1109/ACCESS.2018.2810337
   Jabloncik  F., 2018, P ELEKTRO MAY, P1
   Jia C, 2012, IEEE T SYST MAN CY A, V42, P164, DOI 10.1109/TSMCA.2011.2162497
   Liang XH, 2014, IEEE T PARALL DISTR, V25, P310, DOI 10.1109/TPDS.2013.37
   Lu BW, 2018, CHIN CONT DECIS CONF, P2471, DOI 10.1109/CCDC.2018.8407540
   Noh G, 2015, J COMMUN NETW-S KOR, V17, P145, DOI 10.1109/JCN.2015.000028
   Oh SW., 2017, 2017 INT C PLATF TEC, P1, DOI DOI 10.1109/CAIPT.2017.8320715
   Phukseng, 2017, 2017 12 INT C INT SY, P1, DOI [10.1109/ISKE.2017.8258748, DOI 10.1109/ISKE.2017.8258748]
   Puterman ML., 2014, MARKOV DECISION PROC, DOI DOI 10.1002/9780470316887
   Rahman Mohammad Khaliqur, 2016, 2016 14th Annual Conference on Privacy, Security and Trust (PST), P62, DOI 10.1109/PST.2016.7906938
   Sang JT, 2015, 2015 1ST IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P5, DOI 10.1109/BigMM.2015.60
   Statista, Social media usage worldwide
   Taleb T, 2003, SIGNAL PROCESS-IMAGE, V18, P515, DOI 10.1016/S0923-5965(03)00039-0
   Taleb  T., 2011, Patent, Patent No. [PCT/US2011/049 159, 2011049159]
   Taleb T, 2016, IEEE T WIREL COMMUN, V15, P2859, DOI 10.1109/TWC.2015.2512274
   Taleb T, 2013, IEEE ICC, P5934, DOI 10.1109/ICC.2013.6655547
   Tian YH, 2010, COMPUTER, V43, P27, DOI 10.1109/MC.2010.188
   Wang EK, 2018, IEEE T NETW SERV MAN, V15, P319, DOI 10.1109/TNSM.2017.2776350
   Wang RF, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION PROBLEM-SOLVING (ICCP), P520, DOI 10.1109/ICCPS.2015.7454219
   Wang XC, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS, NETWORKING AND APPLICATIONS (WCNA2017), P205, DOI 10.1145/3180496.3180633
   Wang YJ, 2017, INT C INTEL HUM MACH, P133, DOI 10.1109/IHMSC.2017.145
   Yang LJ, 2012, INT CONF GENET EVOL, P9, DOI 10.1109/ICGEC.2012.41
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
   Zolfaghar Kiyana, 2010, Proceedings of the 2010 IEEE 6th International Conference on Intelligent Computer Communication and Processing (ICCP 2010), P73, DOI 10.1109/ICCP.2010.5606460
NR 37
TC 9
Z9 9
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 603
EP 616
DI 10.1109/TMM.2019.2893548
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800007
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Song, H
   Wu, XX
   Zhu, B
   Wu, YW
   Chen, M
   Jia, YD
AF Song, Hao
   Wu, Xinxiao
   Zhu, Bing
   Wu, Yuwei
   Chen, Mei
   Jia, Yunde
TI Temporal Action Localization in Untrimmed Videos Using Action Pattern
   Trees
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporal action localization; action pattern tree; informative action
   maps; overlap loss function
ID ACTION RECOGNITION
AB In this paper, we present a novel framework of automatically localizing action instances based on action pattern trees (AP-Trees) in a long untrimmed video. For localizing action instances in videos with varied temporal lengths, we first split videos into sequential segments and then use the AP-Trees to produce precise temporal boundaries of action instances. The AP-Trees can exploit the temporal information between segments of videos based on the label vectors of segments, by learning the occurrence frequency and order of segments. In AP-Trees, nodes stand for action class labels of segments and edges represent the temporal relationships between two consecutive segments. Thus, we can discover the occurrence frequencies of segments by searching paths of AP-Trees. In order to obtain accurate labels of video segments, we introduce deep neural networks to annotate the segments by simultaneously leveraging the spatio-temporal information and the high-level semantic feature of segments. In the networks, informative action maps are generated by a global average pooling layer to retain the spatio-temporal information of segments. An overlap loss function is employed to further improve the precision of label vectors of segments by considering the temporal overlap between segments and the ground truth. The experiments on THUMOS2014, MSR ActionII, and MPII Cooking datasets demonstrate the effectiveness of the method.
C1 [Song, Hao; Wu, Xinxiao; Zhu, Bing; Wu, Yuwei; Jia, Yunde] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Chen, Mei] SUNY Albany, Dept Elect & Comp Engn, Albany, NY 12222 USA.
C3 Beijing Institute of Technology; State University of New York (SUNY)
   System; State University of New York (SUNY) Albany
RP Wu, XX (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM songhao@bit.edu.cn; wuxinxiao@bit.edu.cn; wuyuwei@bit.edu.cn;
   meichen@albany.edu; jiayunde@bit.edu.cn
OI Wu, Xinxiao/0000-0002-2056-6947
FU Natural Science Foundation of China (NSFC) [61673062, 61472038]
FX This work was supported by the Natural Science Foundation of China
   (NSFC) under Grants 61673062 and 61472038. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xavier Giro-i-Nieto. (Corresponding author: Xinxiao
   Wu.)
CR Aditya P., 2016, MARKET BASKET ANAL U
   [Anonymous], 2014, DISCRETE DYN NAT SOC, DOI DOI 10.1155/2014/934369
   [Anonymous], ARXIV150105964
   [Anonymous], 2014, THUMOS14 ACTION RECO
   Baraldi L, 2017, IEEE T MULTIMEDIA, V19, P955, DOI 10.1109/TMM.2016.2644872
   Borzeshi EZ, 2011, LECT NOTES COMPUT SC, V6979, P19, DOI 10.1007/978-3-642-24088-1_3
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Chang HY, 2016, PROCEEDINGS 2016 INTERNATIONAL CONFERENCE ON NETWORKING AND NETWORK APPLICATIONS NANA 2016, P375, DOI 10.1109/NaNA.2016.77
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Dharmaraajan K, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER APPLICATIONS (ICACA), P170, DOI 10.1109/ICACA.2016.7887945
   Donahue J, 2014, PR MACH LEARN RES, V32
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65
   Gao Jiyang, 2017, BMVC
   Gemert JC., 2015, Apt: Action localization proposals from dense trajectories
   Han JW, 2000, SIGMOD RECORD, V29, P1
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karaman Svebor., 2014, ECCV2014 THUMOS Challenge, P1
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Ma SG, 2015, PROC CVPR IEEE, P5024, DOI 10.1109/CVPR.2015.7299137
   Ma SG, 2013, IEEE I CONF COMP VIS, P2744, DOI 10.1109/ICCV.2013.341
   Mikolajczyk K, 2011, COMPUT VIS IMAGE UND, V115, P426, DOI 10.1016/j.cviu.2010.11.002
   Richard A, 2016, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2016.341
   Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Sun C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P371, DOI 10.1145/2733373.2806226
   Truyen T.T., Proceedings from Computer Vision and Pattern Recognition Conference, 2006, P1686, DOI [10.1109/CVPR.2006.49, DOI 10.1109/CVPR.2006.49]
   Wang  B., 2016, P IEEE C COMP VIS PA, P1
   Wang H, 2014, IEEE T MULTIMEDIA, V16, P1282, DOI 10.1109/TMM.2014.2312251
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Xu KH, 2015, IEEE IMAGE PROC, P3977, DOI 10.1109/ICIP.2015.7351552
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yi Y, 2016, PATTERN RECOGN, V53, P148, DOI 10.1016/j.patcog.2015.11.022
   Yu G, 2015, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2015.7298735
   Yu G, 2011, IEEE T MULTIMEDIA, V13, P507, DOI 10.1109/TMM.2011.2128301
   Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPRW.2009.5206671, 10.1109/CVPR.2009.5206671]
   Yuan ZH, 2017, PROC CVPR IEEE, P3215, DOI 10.1109/CVPR.2017.342
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
   Zhu Y, 2017, IEEE WINT CONF APPL, P197, DOI 10.1109/WACV.2017.29
NR 55
TC 25
Z9 25
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 717
EP 730
DI 10.1109/TMM.2018.2866370
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800016
DA 2024-07-18
ER

PT J
AU Xu, L
   Bao, T
   Zhu, LH
   Zhang, Y
AF Xu, Lei
   Bao, Ting
   Zhu, Liehuang
   Zhang, Yan
TI Trust-Based Privacy-Preserving Photo Sharing in Online Social Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social trust; anonymization; privacy preserving; photo sharing; online
   social networks
ID MEDIA; SECURITY
AB With the development of social media technologies, sharing photos in online social networks has now become a popular way for users to maintain social connections with others. However, the rich information contained in a photo makes it easier for a malicious viewer to infer sensitive information about those who appear in the photo. How to deal with the privacy disclosure problem incurred by photo sharing has attracted much attention in recent years. When sharing a photo that involves multiple users, the publisher of the photo should take into all related users' privacy into account. In this paper, we propose a trust-based privacy preserving mechanism for sharing such coowned photos. The basic idea is to anonymize the original photo so that users who may suffer a high privacy loss from the sharing of the photo cannot be identified from the anonymized photo. The privacy loss to a user depends on how much he or she trusts the receiver of the photo. And the user's trust in the publisher is affected by privacy loss. The anonymiation result of a photo is controlled by a threshold specified by the publisher. We propose a greedy method for the publisher to tune the threshold, in the purpose of balancing between the privacy preserved by anonymization and the information shared with others. Simulation results demonstrate that the trust-based photo sharing mechanism is helpful to reduce the privacy loss, and the proposed threshold tuning method can bring a good payoff to the user.
C1 [Xu, Lei; Bao, Ting; Zhu, Liehuang] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
   [Zhang, Yan] Univ Oslo, Dept Informat, N-0315 Oslo, Norway.
C3 Beijing Institute of Technology; University of Oslo
RP Zhu, LH (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
EM xul04ster@gmail.com; tingbao@bit.edu.cn; liehuangz@bit.edu.cn;
   yanzhang@ieee.org
RI Zhang, Yan/AFK-8566-2022; ZHU, LIEHUANG/A-6174-2018; R, Dr
   Jegadeesan/ABC-6189-2022
OI R, Dr Jegadeesan/0000-0001-8885-7241; Zhang, Yan/0000-0002-8561-5092
FU Natural Science Foundation of China [61871037, 61571300]; Beijing
   Institute of Technology Research Fund Program for Young Scholars
FX This work was supported in part by the Natural Science Foundation of
   China Grants 61871037 and 61571300 and in part by the Beijing Institute
   of Technology Research Fund Program for Young Scholars. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Zhou Su. (Corresponding author: Liehuang Zhu.)
CR [Anonymous], IEEE INTERNET THINGS
   Bethencourt J, 2007, P IEEE S SECUR PRIV, P321, DOI 10.1109/sp.2007.11
   Datta A., 2010, DECENTRALIZED ONLINE
   Duggan M, 2013, DEMOGRAPHICS SOCIAL, V14
   Fiesler C, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P567, DOI 10.1145/2998181.2998223
   Gay R., 2017, FPS, P18
   Hu HX, 2011, 27TH ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE (ACSAC 2011), P103
   Ilia P, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P781, DOI 10.1145/2810103.2813603
   Kaplan AM, 2010, BUS HORIZONS, V53, P59, DOI 10.1016/j.bushor.2009.09.003
   Kumar NS, 2016, PROCEDIA COMPUT SCI, V78, P114, DOI 10.1016/j.procs.2016.02.019
   Lee C, 2016, 2016 IEEE FIRST INTERNATIONAL CONFERENCE ON DATA SCIENCE IN CYBERSPACE (DSC 2016), P440, DOI 10.1109/DSC.2016.113
   Leskovec J., 2014, SNAP Datasets: Stanford large network dataset collection
   Ma CS, 2019, IEEE T MULTIMEDIA, V21, P173, DOI 10.1109/TMM.2018.2851446
   Mangold WG, 2009, BUS HORIZONS, V52, P357, DOI 10.1016/j.bushor.2009.03.002
   Muchnik L., 2013, COMPLEX NETWORKS PAC
   Obar JA, 2015, TELECOMMUN POLICY, V39, P745, DOI 10.1016/j.telpol.2015.07.014
   Rathore NC, 2017, SOC NETW ANAL MIN, V7, DOI 10.1007/s13278-017-0425-6
   Richters O, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0018384
   Sherchan W, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2501654.2501661
   Squicciarini Anna, 2009, P 18 INT C WORLD WID, P521
   Su Z, 2018, IEEE J SEL AREA COMM, V36, P2175, DOI 10.1109/JSAC.2018.2869948
   Such JM, 2016, IEEE T KNOWL DATA EN, V28, P1851, DOI 10.1109/TKDE.2016.2539165
   Vishwamitra N, 2017, PROCEEDINGS OF THE 22ND ACM SYMPOSIUM ON ACCESS CONTROL MODELS AND TECHNOLOGIES (SACMAT'17), P155, DOI 10.1145/3078861.3078875
   Xu KH, 2017, IEEE T DEPEND SECURE, V14, P199, DOI 10.1109/TDSC.2015.2443795
   Xu L., IEEE T CYBERNETICS
   Xu L, 2019, IEEE T INF FOREN SEC, V14, P48, DOI 10.1109/TIFS.2018.2840488
   Xu L, 2017, IEEE T INF FOREN SEC, V12, P271, DOI 10.1109/TIFS.2016.2611487
   Xu L, 2014, IEEE ACCESS, V2, P1149, DOI 10.1109/ACCESS.2014.2362522
   Xu L, 2015, IEEE J-STSP, V9, P1256, DOI 10.1109/JSTSP.2015.2425798
   Xu QC, 2019, IEEE INTERNET THINGS, V6, P4536, DOI 10.1109/JIOT.2018.2876417
   Yu J, 2018, IEEE T INF FOREN SEC, V13, P1317, DOI 10.1109/TIFS.2017.2787986
   Yuan L, 2015, IEEE CONF COMPUT, P185, DOI 10.1109/INFCOMW.2015.7179382
NR 32
TC 14
Z9 14
U1 3
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 591
EP 602
DI 10.1109/TMM.2018.2887019
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800006
DA 2024-07-18
ER

PT J
AU Xu, XZ
   Deng, J
   Coutinho, E
   Wu, C
   Zhao, L
   Schuller, BW
AF Xu, Xinzhou
   Deng, Jun
   Coutinho, Eduardo
   Wu, Chen
   Zhao, Li
   Schuller, Bjoern W.
TI Connecting Subspace Learning and Extreme Learning Machine in Speech
   Emotion Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Speech emotion recognition; extreme learning machine; subspace learning;
   graph embedding; spectral regression
ID DISCRIMINANT-ANALYSIS; SPECTRAL REGRESSION; GENERAL FRAMEWORK;
   APPROXIMATION; ALGORITHMS; SELECTION
AB Speech emotion recognition (SER) is a powerful tool for endowing computers with the capacity to process information about the affective states of users in human-machine interactions. Recent research has shown the effectiveness of graph embedding-based subspace learning and extreme learning machine applied to SER, but there are still various drawbacks in these two techniques that limit their application. Regarding subspace learning, the change from linearity to nonlinearity is usually achieved through kernelization, whereas extreme learning machines only take label information into consideration at the output layer. In order to overcome these drawbacks, this paper leverages extreme learning machines for dimensionality reduction and proposes a novel framework to combine spectral regression-based subspace learning and extreme learning machines. The proposed framework contains three stages-data mapping, graph decomposition, and regression. At the data mapping stage, various mapping strategies provide different views of the samples. At the graph decomposition stage, specifically designed embedding graphs provide a possibility to better represent the structure of data through generating virtual coordinates. Finally, at the regression stage, dimension-reduced mappings are achieved by connecting the virtual coordinates and data mapping. Using this framework, we propose several novel dimensionality reduction algorithms, apply them to SER tasks, and compare their performance to relevant state-of-the-art methods. Our results on several paralinguistic corpora show that our proposed techniques lead to significant improvements.
C1 [Xu, Xinzhou; Wu, Chen] Nanjing Univ Posts & Telecommun, Coll Internet Things, Nanjing 210003, Jiangsu, Peoples R China.
   [Xu, Xinzhou; Zhao, Li] Southeast Univ, Minist Educ, Key Lab Underwater Acoust Signal Proc, Nanjing 210096, Jiangsu, Peoples R China.
   [Xu, Xinzhou] Tech Univ Munich, MMK, Machine Intelligence & Signal Proc Grp, D-80290 Munich, Germany.
   [Deng, Jun] AudEERING GmbH, D-82205 Gilching, Germany.
   [Coutinho, Eduardo] Imperial Coll London, Dept Comp, London SW7 2AZ, England.
   [Coutinho, Eduardo] Univ Liverpool, Dept Mus, Liverpool L69 3BX, Merseyside, England.
   [Schuller, Bjoern W.] Imperial Coll London, Grp Language Audio & Mus, London SW7 2AZ, England.
   [Schuller, Bjoern W.] Univ Augsburg, Chair Embedded Intelligence Hlth Care & Wellbeing, D-86159 Augsburg, Germany.
C3 Nanjing University of Posts & Telecommunications; Southeast University -
   China; Technical University of Munich; Imperial College London;
   University of Liverpool; Imperial College London; University of Augsburg
RP Xu, XZ (corresponding author), Nanjing Univ Posts & Telecommun, Coll Internet Things, Nanjing 210003, Jiangsu, Peoples R China.; Xu, XZ (corresponding author), Southeast Univ, Minist Educ, Key Lab Underwater Acoust Signal Proc, Nanjing 210096, Jiangsu, Peoples R China.
EM xinzhou.xu@tum.de; jdeng@audeering.com; e.coutinho@imperial.ac.uk;
   wuchen@njupt.edu.cn; zhaoli@seu.edu.cn; schuller@ieee.org
RI Coutinho, Eduardo/K-1391-2019; Schuller, Björn Wolfgang/D-3241-2011
OI Coutinho, Eduardo/0000-0001-5234-1497; Schuller, Björn
   Wolfgang/0000-0002-6478-8699
FU China Scholarship Council; European Union [338164]; European Union's
   Horizon 2020 Research and Innovation Programme [645378, 645094]; Natural
   Science Foundation of China [61673108, 61231002, 11701290]; Natural
   Science Foundation for Jiangsu Higher Education Institutions
   [16KJB510031, 17KJB110012]; NUPTSF [NY217149, NY217150]; H2020 -
   Industrial Leadership [645094] Funding Source: H2020 - Industrial
   Leadership
FX This work was supported in part by the China Scholarship Council; in
   part by the European Union's Seventh Framework Programme under Grant
   Agreement 338164 (ERC Starting Grant iHEARu); in part by the European
   Union's Horizon 2020 Research and Innovation Programme Under Grant
   Agreement 645378 (ARIA-VALUSPA) and Grant Agreement 645094 (SEWA); in
   part by the Natural Science Foundation of China under Grant 61673108,
   Grant 61231002, and Grant 11701290; in part by the Natural Science
   Foundation for Jiangsu Higher Education Institutions under Grant
   16KJB510031 and Grant 17KJB110012; and in part by the NUPTSF under Grant
   NY217149 and Grant NY217150. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Martha Larson. (Corresponding author: Xinzhou Xu.)
CR An S., 2007, P IEEE C COMP VIS PA, DOI 10.1109/CVPR.2007.383105.
   [Anonymous], 2006, 22 INT C DAT ENG WOR
   [Anonymous], Environmental Psychology & Nonverbal Behavior
   Bänziger T, 2012, EMOTION, V12, P1161, DOI 10.1037/a0025827
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Cai D., 2007, Proceedings of the 15th international conference on Multimedia, P403, DOI [DOI 10.1145/1291233.1291329, 10.1145/1291233.1291329]
   Cai D., 2007, PROC IEEE 11 INT C C
   Cai  D., 2007, 2856 U ILL URB CHAMP
   Cai D, 2007, IEEE DATA MINING, P427, DOI 10.1109/ICDM.2007.88
   Cai D, 2007, IEEE DATA MINING, P73, DOI 10.1109/ICDM.2007.89
   Cai D, 2008, IEEE T KNOWL DATA EN, V20, P1, DOI 10.1109/TKDE.2007.190669
   Cai D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3359
   Cai HP, 2011, IEEE T PATTERN ANAL, V33, P338, DOI 10.1109/TPAMI.2010.89
   Chen HT, 2005, PROC CVPR IEEE, P846
   Cui Y, 2012, PATTERN RECOGN, V45, P1471, DOI 10.1016/j.patcog.2011.10.006
   Drineas P, 2006, SIAM J COMPUT, V36, P158, DOI 10.1137/S0097539704442696
   Eyben F., 2013, P 21 ACM INT C MULT, P835, DOI DOI 10.1145/2502081.2502224
   Grimm M, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P865, DOI 10.1109/ICME.2008.4607572
   Han K, 2014, INTERSPEECH, P223
   Harandi M. T., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2705, DOI 10.1109/CVPR.2011.5995564
   He XF, 2004, ADV NEUR IN, V16, P153
   Huang G, 2015, NEURAL NETWORKS, V61, P32, DOI 10.1016/j.neunet.2014.10.001
   Huang G, 2014, IEEE T CYBERNETICS, V44, P2405, DOI 10.1109/TCYB.2014.2307349
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Huang GB, 2015, IEEE COMPUT INTELL M, V10, P18, DOI 10.1109/MCI.2015.2405316
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Huang GB, 2010, NEUROCOMPUTING, V74, P155, DOI 10.1016/j.neucom.2010.02.019
   Iosifidis A, 2016, IEEE T CYBERNETICS, V46, P311, DOI 10.1109/TCYB.2015.2401973
   Liu B, 2015, NEUROCOMPUTING, V149, P171, DOI 10.1016/j.neucom.2013.09.073
   Liu  P., 2014, INT J MACH LEARN CYB, V7, P1
   Luengo I, 2010, IEEE T MULTIMEDIA, V12, P490, DOI 10.1109/TMM.2010.2051872
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Mariooryad S, 2014, SPEECH COMMUN, V57, P1, DOI 10.1016/j.specom.2013.07.011
   Ngo TT, 2010, SIAM J MATRIX ANAL A, V31, P2950, DOI 10.1137/090776603
   Salzberg SL, 1997, DATA MIN KNOWL DISC, V1, P317, DOI 10.1023/A:1009752403260
   Schuller B, 2007, INT CONF ACOUST SPEE, P733
   Schuller B, 2013, INTERSPEECH, P148
   Schuller B, 2009, 2009 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION & UNDERSTANDING (ASRU 2009), P552, DOI 10.1109/ASRU.2009.5372886
   Schuller BW, 2015, LECT NOTES ARTIF INT, V9302, P3, DOI 10.1007/978-3-319-24033-6_1
   Song FX, 2005, PATTERN RECOGN, V38, P311, DOI 10.1016/j.patcog.2004.06.007
   Song P, 2016, INT CONF ACOUST SPEE, P5180, DOI 10.1109/ICASSP.2016.7472665
   Tarvainen J, 2014, IEEE T MULTIMEDIA, V16, P2085, DOI 10.1109/TMM.2014.2357688
   Tawari A, 2010, IEEE T MULTIMEDIA, V12, P502, DOI 10.1109/TMM.2010.2058095
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Weninger F, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00292
   Xu XZ, 2017, IEEE-ACM T AUDIO SPE, V25, P1436, DOI 10.1109/TASLP.2017.2694704
   Xu XZ, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P233, DOI 10.1145/2993148.2993184
   Xu XZ, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1532
   Yan SC, 2005, PROC CVPR IEEE, P830
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang J, 2005, IEEE T PATTERN ANAL, V27, P230, DOI 10.1109/TPAMI.2005.33
   Ye JP, 2005, J MACH LEARN RES, V6, P483
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 54
TC 34
Z9 38
U1 4
U2 58
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 795
EP 808
DI 10.1109/TMM.2018.2865834
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800022
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chang, HS
   Hsu, CF
   Hossfeld, T
   Chen, KT
AF Chang, Haw-Shiuan
   Hsu, Chih-Fan
   Hossfeld, Tobias
   Chen, Kuan-Ta
TI Active Learning for Crowdsourced QoE Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active learning; multidimensional QoS-QoE model; IQX and MIQX model;
   maximin sampling; crowdsourcing; video quality assessment
ID QUALITY
AB Quality of experience (QoE) models predict the subjective quality of multimedia based on the relevant quality of service (QoS) factors. Due to the large space of QoS factors and the high casts of conducting subjective tests, efficient sampling strategies are required to determine which QoS configurations are to be queried, that is, evaluated by subjects. In this study, we extend the IQX model proposed by [M. Fiedler, T. Hossfeld, and P. Tran-Gia, "A generic quantitative relationship between quality of experience and quality of service," IEEE Netw., vol. 24, no. 2, pp. 36-41, Mar./Apr.2010.] toward a multidimensional QoS-QoE model (MIQX). To explore the complicated interaction between QoS factors more efficiently, we develop active learning algorithms for the multidimensional QoE model. Then, we conduct comprehensive experiments to compare the effectiveness of applying different sampling methods to crowdsourced video quality assessment tasks. In offline experiments that assume annotators give the same scores after changing the querying order, we demonstrate that active learning performs best and that a space-filling algorithm performs significantly better than random sampling. However, when we analyze the performance of the active sampling approaches more deeply using a novel field experiment, we observe that the active learning algorithms, which have been shown to be effective in the offline setting, can fail due to the habituation effect and individual differences of annotators. The active learning methods can also succeed when these issues are mitigated. These findings suggest that simply simulating the sample acquisition order, which is widely adopted in previous active learning literature [2]-[5], is not sufficient for multimedia quality assessment tasks.
C1 [Chang, Haw-Shiuan] Univ Massachusetts, Amherst, MA 01003 USA.
   [Hsu, Chih-Fan; Chen, Kuan-Ta] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
   [Hossfeld, Tobias] Univ Wurzburg, Chair Commun Networks, D-97070 Wrzburg, Germany.
C3 University of Massachusetts System; University of Massachusetts Amherst;
   Academia Sinica - Taiwan; University of Wurzburg
RP Chen, KT (corresponding author), Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
EM hschang@cs.umass.edu; chihfan@iis.sinica.edu.tw;
   tobias.hossfeld@uni-wuerzburg.de; swc@iis.sinica.edu.tw
RI Chang, Haw-Shiuan/HDN-6000-2022
OI Chang, Haw-Shiuan/0000-0003-4607-936X; Hsu,
   Chih-Fan/0000-0002-4180-8255; Hossfeld, Tobias/0000-0003-0173-595X
FU DFG Crowdsourcing [HO 4770/2-2]
FX This work was supported by the DFG Crowdsourcing under Grant HO
   4770/2-2. (Corresponding author: Kuan-Ta Chen.)
CR Alreshoodi M., 2013, International Journal of Distributed and Parallel Systems, V4, P53
   [Anonymous], 2016, P 5 ISCA DEGA WORKSH
   [Anonymous], 2010, U WISCONSIN MADISON, V52, P11, DOI DOI 10.1016/J.MATLET.2010.11.072
   [Anonymous], 2003, COM9C60E ITUT
   [Anonymous], 2012, J. Eng. Sci. Technol. Rev.
   [Anonymous], 2013, Inferring ground truth from multi-annotator ordinal data: a probabilistic approach
   Aroussi S, 2014, 2014 INTERNATIONAL CONFERENCE ON COMPUTING, MANAGEMENT AND TELECOMMUNICATIONS (COMMANTEL), P200, DOI 10.1109/ComManTel.2014.6825604
   Azzalini A., 1997, OXFORD STAT SCI SERI
   Battisti F., 2014, Euro Med Telco Conference (EMTC), 2014, P1
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chen KT, 2010, IEEE NETWORK, V24, P28, DOI 10.1109/MNET.2010.5430141
   Chikkerur S, 2011, IEEE T BROADCAST, V57, P165, DOI 10.1109/TBC.2011.2104671
   Coleman TF, 1996, SIAM J OPTIMIZ, V6, P418, DOI 10.1137/0806023
   Elkotob M, 2010, C LOCAL COMPUT NETW, P324, DOI 10.1109/LCN.2010.5735733
   Fedorov V. V., 1972, Theory of Optimal Experiments
   Fiedler M, 2010, IEEE NETWORK, V24, P36, DOI 10.1109/MNET.2010.5430142
   Gardlo Bruno., 2015, Proceedings of the Fourth International Workshop on Crowdsourcing for Multimedia. CrowdMM'15, P15
   Hosfeld T., 2011, Proceedings of the 2011 23rd International Teletraffic Congress (ITC 2011), P103
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Hossfeld T., 2016, Quality and User Experience, V1, P1, DOI [10.1007/S41233-016-0002-1, DOI 10.1007/S41233-016-0002-1, 10.1007/s41233-016-0002-1]
   Hossfeld T, 2015, PROCEEDINGS OF THE 2015 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM), P1249, DOI 10.1109/INM.2015.7140476
   Hossfeld T, 2014, T-LAB SER TELECOMMUN, P315, DOI 10.1007/978-3-319-02681-7_21
   Hossfeld T, 2011, INT WORK QUAL MULTIM, P131, DOI 10.1109/QoMEX.2011.6065690
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   Janowski Lucjan, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P251, DOI 10.1109/QoMEX.2014.6982327
   JOHNSON ME, 1990, J STAT PLAN INFER, V26, P131, DOI 10.1016/0378-3758(90)90122-B
   Korhonen J, 2012, INT WORK QUAL MULTIM, P57, DOI 10.1109/QoMEX.2012.6263839
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590
   Menkovski V, 2012, SIGNAL PROCESS-IMAGE, V27, P788, DOI 10.1016/j.image.2012.01.004
   Menkovski Vlado, 2011, P INT C MOB MULT COM, P1
   Mushtaq MS., 2012, Networks and Optical Communications (NOC), 2012 17th European Conference on, P1, DOI DOI 10.1109/NOC.2012.6249939
   Nowak R., 2005, Proc. Adv. in Neural Processing Systems NIPS, V18, P179
   Osting B, 2016, APPL COMPUT HARMON A, V41, P540, DOI 10.1016/j.acha.2016.03.007
   Pinson M, 2003, PROC SPIE, V5150, P573, DOI 10.1117/12.509908
   Pronzato L, 2012, STAT COMPUT, V22, P681, DOI 10.1007/s11222-011-9242-3
   Schatz Raimund, 2013, Data Traffic Monitoring and Analysis. From Measurement, Classification, and Anomaly Detection to Quality of Experience, P219, DOI 10.1007/978-3-642-36784-7_10
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Seufert Michael., 2016, 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX), P1
   Tsolkas D, 2017, J NETW COMPUT APPL, V77, P1, DOI 10.1016/j.jnca.2016.10.016
   Xu XS, 2010, INT CONF COMP SCI, P506, DOI 10.1109/ICCSIT.2010.5563749
   Ye P, 2014, PROC CVPR IEEE, P4249, DOI 10.1109/CVPR.2014.541
   Yu K., 2006, P 23 INT C MACH LEAR, P1081
NR 42
TC 12
Z9 13
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3337
EP 3352
DI 10.1109/TMM.2018.2831639
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600013
DA 2024-07-18
ER

PT J
AU Sun, LF
   Pang, HT
   Gao, L
AF Sun, Lifeng
   Pang, Haitian
   Gao, Lin
TI Joint Sponsor Scheduling in Cellular and Edge Caching Networks for
   Mobile Video Delivery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data sponsoring; edge caching; mobile video streaming
AB The explosive growth of mobile video traffic introduces new challenges for the network infrastructure. Edge caching, as one of the key technologies in 5G wireless networks, has shown great potential to improve the quality of mobile video services by reducing the transmission overhead over backhaul links. With edge caching, content providers (CPs) need to decide not only the traditional data sponsoring strategy on cellular networks (where CPs cover part or all of the mobile users' cellular data cost), but also a novel cache sponsoring strategy on the edge caching networks (where CPs place part of contents on edge networks in advance). In this paper, we study the joint optimization of both sponsors on cellular and edge caching networks for a single CP, aiming at maximizing the CP's revenue. Specifically, we formulate the joint optimization problem as a two-stage sequential decision problem. In stage I, the CP determines the edge caching policy (for a relatively long time period). In stage II, the CP decides the real-time data sponsoring strategy for each content request within the period. We analyze this two-stage decision problem systematically. First, we propose an online sponsoring strategy in stage II based on Lyapunov optimization framework. Then, we propose an edge caching strategy in stage I via predicting the number of aggregate user requests. Simulations on real data traces show that such a joint optimization policy can increase the CP's revenue by 124%-454%, comparing with the traditional data sponsoring policy (i.e., without edge caching). Moreover, the proposed online strategy can achieve 90% of the maximum revenue in the offline benchmark.
C1 [Sun, Lifeng; Pang, Haitian] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Gao, Lin] Harbin Inst Technol, Sch Elect & Informat Engn, Shenzhen 150001, Peoples R China.
C3 Tsinghua University; Harbin Institute of Technology
RP Pang, HT (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM sunlf@tsinghua.edu.cn; pht14@mails.tsinghua.edu.cn; gaol@hit.edu.cn
RI Gao, Lin/JNF-0375-2023; GAO, Lin/N-8286-2015
FU National Natural Science Foundation of China (NSFC) [61472204,
   61521002]; Beijing Key Laboratory of Networked Multimedia
   [Z161100005016051]; Alibaba Cooperation Funding; NSFC [61771162]
FX The work of L. Sun and H. Pang was supported in part by the National
   Natural Science Foundation of China (NSFC) under Grant 61472204 and
   Grant 61521002, in part by Beijing Key Laboratory of Networked
   Multimedia under Grant Z161100005016051, and in part by the Alibaba
   Cooperation Funding. The work of L. Gao was supported by the NSFC under
   Grant 61771162. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Maria G. Martini.
   (Lifeng Sun and Haitian Pang contributed equally to this work.)
   (Corresponding author: Haitian Pang.)
CR Andrews M, 2013, IEEE CONF COMPUT, P345
   [Anonymous], IEEE ACM T NETW
   [Anonymous], 2016, 2016 IEEE INT C COMM
   [Anonymous], 2016, PROC IEEE SENSOR ARR
   [Anonymous], INT J COMPUT NETW TE
   Bastug E, 2014, IEEE COMMUN MAG, V52, P82, DOI 10.1109/MCOM.2014.6871674
   Chen L, 2015, AER ADV ENG RES, V31, P43
   Cisco, 2017, Cisco7 Feb.
   ElDelgawy R, 2015, IEEE ICC, P5890, DOI 10.1109/ICC.2015.7249261
   Gao L., 2018, IEEE T MOBILE COMPUT, P1
   Gao L, 2017, IEEE T MOBILE COMPUT, V16, P538, DOI 10.1109/TMC.2016.2557794
   Gao L, 2011, IEEE J SEL AREA COMM, V29, P843, DOI 10.1109/JSAC.2011.110415
   Gao L, 2011, IEEE T MOBILE COMPUT, V10, P1144, DOI 10.1109/TMC.2010.220
   Golrezaei N, 2012, IEEE INFOCOM SER, P1107, DOI 10.1109/INFCOM.2012.6195469
   Hsu YP, 2017, IEEE INT SYMP INFO, P561, DOI 10.1109/ISIT.2017.8006590
   Hsu YP, 2017, IEEE T WIREL COMMUN, V16, P4880, DOI 10.1109/TWC.2017.2703619
   Hu H, 2016, IEEE T CIRC SYST VID, V26, P1320, DOI 10.1109/TCSVT.2015.2455712
   Jiang CK, 2018, IEEE T MOBILE COMPUT, V17, P898, DOI 10.1109/TMC.2017.2743718
   Joe-Wong Carlee, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P1499, DOI 10.1109/INFOCOM.2015.7218528
   Khalili MM, 2015, IEEE CONF COMPUT, P498, DOI 10.1109/INFCOMW.2015.7179434
   Krishnappa DK, 2011, LECT NOTES COMPUT SC, V6579, P72, DOI 10.1007/978-3-642-19260-9_8
   Li SH, 2016, IEEE T MULTIMEDIA, V18, P2503, DOI 10.1109/TMM.2016.2596042
   Lin Gao, 2016, 2016 Annual Conference on Information Science and Systems (CISS), P366, DOI 10.1109/CISS.2016.7460530
   Liu D., 2016, DESIGN AUTOMATION C, P1, DOI DOI 10.1109/GLOCOM.2016.7842078
   Luo Y, 2016, IEEE J SEL AREA COMM, V34, P3326, DOI 10.1109/JSAC.2016.2600118
   Ma G, 2017, IEEE J SEL AREA COMM, V35, P1076, DOI 10.1109/JSAC.2017.2680958
   Neely M. J., 2010, STOCHASTIC NETWORK O
   Pang H., 2017, P 23 APCC PERTH WA A, P1, DOI 10.1109/GLOCOM.2017.8254120
   Ristanovic N., 2011, 2011 IEEE 8th International Conference on Mobile Ad-Hoc and Sensor Systems, P202, DOI 10.1109/MASS.2011.27
   Simsek M, 2016, IEEE J SEL AREA COMM, V34, P460, DOI 10.1109/JSAC.2016.2525398
   Sung J, 2016, IEEE T MULTIMEDIA, V18, P1163, DOI 10.1109/TMM.2016.2543658
   Tang M.-F., 2016, Sensor Array and Multichannel Signal Processing Workshop (SAM), 2016 IEEE, P1
   Tang MY, 2017, AEBMR ADV ECON, V53, P1
   Tang M, 2017, IEEE COMMUN MAG, V55, P21, DOI 10.1109/MCOM.2017.1600830
   Wang XF, 2014, IEEE COMMUN MAG, V52, P131, DOI 10.1109/MCOM.2014.6736753
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wu Y, 2011, IEEE INFOCOM SER, P596, DOI 10.1109/INFCOM.2011.5935234
   XUE JI., 2014, 11th International. Conference on Autonomic Computing, ICAC '14, Philadelphia, PA. USA, June 18-20, P1
   Zhang LX, 2015, ACSR ADV COMPUT, V39, P85
NR 39
TC 22
Z9 23
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3414
EP 3427
DI 10.1109/TMM.2018.2834861
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600019
DA 2024-07-18
ER

PT J
AU Zong, Y
   Huang, XH
   Zheng, WM
   Cui, Z
   Zhao, GY
AF Zong, Yuan
   Huang, Xiaohua
   Zheng, Wenming
   Cui, Zhen
   Zhao, Guoying
TI Learning From Hierarchical Spatiotemporal Descriptors for
   Micro-Expression Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Micro-expression recognition; spatiotemporal descriptor; hierarchical
   spatial division; group sparse learning; kernelized group sparse
   learning
AB Micro-expression recognition aims to infer genuine emotions that people try to conceal from facial video clips. It is a very challenging task because micro-expressions have a very low intensity and short duration, which makes micro-expressions difficult to observe. Recently, researchers have designed various spatiotemporal descriptors to describe micro-expressions. It is notable that for better capturing the low-intensity facial muscle movement, a fixed spatial division grid, 8 x 8 for example, is commonly used to partition the facial images into a few facial blocks before extracting descriptors. However, it is hard to choose an ideal division grid for different micro-expression samples because the division grids affect the discriminative ability of spatiotemporal descriptors to distinguish micro-expressions. To address this problem, in this paper, we design a hierarchical spatial division scheme for spatiotemporal descriptor extraction. By using the proposed scheme, it would not be a problem to deter mine which division grid is most suitable regarding different micro-expression samples. Furthermore, we propose a kernelized group sparse learning (KGSL) model to process hierarchical scheme based spatiotemporal descriptors such that they are more effective for micro-expression recognition tasks. To evaluate the performance of the proposed micro-expression recognition method consisting of the hierarchical scheme based spatiotemporal descriptors and KGSL, extensive experiments are conducted on two public micro-expression databases: CASMEII and SMIC. Compared with many recent state-of-the-art approaches, our method achieves more promising recognition results.
C1 [Zong, Yuan; Zheng, Wenming] Southeast Univ, Sch Biol Sci & Med Engn, Key Lab Child Dev & Learning Sci, Minist Educ, Nanjing 210096, Jiangsu, Peoples R China.
   [Zong, Yuan] Univ Oulu, Ctr Machine Vis & Signal Anal, Faulty Informat Technol & Elect Engn, FI-90014 Oulu, Finland.
   [Huang, Xiaohua; Zhao, Guoying] Univ Oulu, Ctr Machine Vis & Signal Anal, Fac Informat Technol & Elect Engn, FI-90014 Oulu, Finland.
   [Cui, Zhen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
C3 Southeast University - China; University of Oulu; University of Oulu;
   Nanjing University of Science & Technology
RP Zheng, WM (corresponding author), Southeast Univ, Sch Biol Sci & Med Engn, Key Lab Child Dev & Learning Sci, Minist Educ, Nanjing 210096, Jiangsu, Peoples R China.
EM xhzongyuan@seu.edu.cn; xiaohua.huang@oulu.fi; wenming_zheng@seu.edu.cn;
   zhen.cui@njust.edu.cn; guoying.zhao@oulu.fi
RI Zhao, Guoying/ABE-7716-2020; Huang, Xiaohua/A-4878-2011
OI Zhao, Guoying/0000-0003-3694-206X; Huang, Xiaohua/0000-0001-8897-3517
FU National Basic Research Program of China [2015CB351704]; National
   Natural Science Foundation of China [61572009, 61772276, 61602244];
   Jiangsu Provincial Key Research and Development Program [BE2016616];
   China Scholarship Council; Scientific Research Foundation of Graduate
   School of Southeast University [YBJJ1774]; Academy of Finland; Tekes
   Fidipro Program; Infotech Oulu; Jorma Ollila Grant of Nokia Foundation;
   Central Fund of Finnish Cultural Foundation
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2015CB351704, in part by the National Natural
   Science Foundation of China under Grant 61572009, Grant 61772276, and
   Grant 61602244, in part by the Jiangsu Provincial Key Research and
   Development Program under Grant BE2016616, in part by China Scholarship
   Council, in part by the Scientific Research Foundation of Graduate
   School of Southeast University under Grant YBJJ1774, in part by Academy
   of Finland, in part by Tekes Fidipro Program, in part by Infotech Oulu,
   and in part by Jorma Ollila Grant of Nokia Foundation and Central Fund
   of Finnish Cultural Foundation.
CR Ngo ACL, 2016, INT CONF ACOUST SPEE, P1243, DOI 10.1109/ICASSP.2016.7471875
   [Anonymous], 2014, ACCV, DOI DOI 10.1007/978-3-319-16865-4_34
   [Anonymous], 2014, ACCV WORKSH SING, DOI DOI 10.1007/978-3-319-16631-5_47
   [Anonymous], 2014, Proceedings of the Asian Conference on Computer Vision
   [Anonymous], 2009, Proceedings of the 26th Annual International Conference on Machine Learning
   [Anonymous], 2003, METT. micro expression training tool
   [Anonymous], 2014, ECCV WORKSH ZUR, DOI DOI 10.1007/978-3-319-16178-5_23
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2017, IEEE T AFFECTIVE COM
   [Anonymous], 2009, Protecting Airline Passengers in the Age of Terrorism, DOI DOI 10.5040/9798216002246.CH-005
   [Anonymous], 2009, The philosophy of deception, DOI DOI 10.1093/ACPROF:OSO/9780195327939.003.0008
   [Anonymous], 2009, ANN M INT COMM ASS S
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Davison A.K., 2014, European conference on computer vision, P111
   Duda R. O., 2012, PATTERN CLASSIFICATI, DOI DOI 10.1007/978-3-319-57027-3_4
   EKMAN P, 1969, PSYCHIATR, V32, P88, DOI 10.1080/00332747.1969.11023575
   Ekman Paul, 1997, What the Face Reveals: Basic and Applied Studies of Spontaneous Expression using the Facial Action Coding System (FACS)
   Hong XP, 2016, NEUROCOMPUTING, V184, P99, DOI 10.1016/j.neucom.2015.07.134
   Huang XH, 2016, NEUROCOMPUTING, V175, P564, DOI 10.1016/j.neucom.2015.10.096
   Huang XH, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1, DOI 10.1109/ICCVW.2015.10
   Huang XH, 2012, PATTERN RECOGN LETT, V33, P2181, DOI 10.1016/j.patrec.2012.07.015
   Kim DH, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P382, DOI 10.1145/2964284.2967247
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li X., 2013, PROC 10 IEEE INT C W, P1
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liong ST, 2014, I S INTELL SIG PROC, P180, DOI 10.1109/ISPACS.2014.7024448
   Liu YJ, 2016, IEEE T AFFECT COMPUT, V7, P299, DOI 10.1109/TAFFC.2015.2485205
   O'Sullivan M, 2009, LAW HUMAN BEHAV, V33, P530, DOI 10.1007/s10979-008-9166-4
   Oh YH, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P1237, DOI 10.1109/ICDSP.2015.7252078
   Park SY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P911, DOI 10.1145/2733373.2806362
   Pfister T, 2011, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2011.6126401
   Tawari A, 2013, IEEE T MULTIMEDIA, V15, P1543, DOI 10.1109/TMM.2013.2266635
   Wang SF, 2010, IEEE T MULTIMEDIA, V12, P682, DOI 10.1109/TMM.2010.2060716
   Wang SJ, 2015, IEEE T IMAGE PROCESS, V24, P6034, DOI 10.1109/TIP.2015.2496314
   Wang SJ, 2014, INT C PATT RECOG, P4678, DOI 10.1109/ICPR.2014.800
   Wang YD, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0124674
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Xu F, 2017, IEEE T AFFECT COMPUT, V8, P254, DOI 10.1109/TAFFC.2016.2518162
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao GY, 2009, PATTERN RECOGN LETT, V30, P1117, DOI 10.1016/j.patrec.2009.03.018
   Zheng WM, 2014, IEEE T AFFECT COMPUT, V5, P71, DOI 10.1109/TAFFC.2014.2304712
NR 42
TC 87
Z9 93
U1 3
U2 56
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3160
EP 3172
DI 10.1109/TMM.2018.2820321
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800023
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Tian, JD
   Han, Z
   Ren, WH
   Chen, X
   Tang, YD
AF Tian, Jiandong
   Han, Zhi
   Ren, Weihong
   Chen, Xiai
   Tang, Yandong
TI Snowflake Removal for Videos via Global and Local Low-Rank Decomposition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Snowflake removal; desnowing; low-rank decomposition
ID RAIN STREAKS REMOVAL; IMAGE; SPARSE
AB Falling snow not only blocks human vision, but also significantly degrades the effectiveness of computer vision systems in outdoor environment. In this paper, we aim to remove snowflakes in videos by using the global and local low-rank property of snowflake-removed scenes. The stationary background and the mixture of moving foreground as well as falling snowflake are extracted via the global low-rank matrix decomposition. Some snowflake features, such as its color and size, are used to separate out the snowflakes from other moving objects. Then, the mean absolute difference based patch matching is applied to align every same moving object over frames to grab its low-rank structure. As such, the falling snowflake in front of moving objects can be removed via the local low-rank decomposition. Finally, the snowflake removed videos are generated by pasting moving foreground to stationary backgrounds. Experiments show that our method can remove snowflakes effectively and outperforms the comparison methods.
C1 [Tian, Jiandong; Han, Zhi; Ren, Weihong; Chen, Xiai; Tang, Yandong] Chinese Acad Sci, State Key Lab Robot, Shenyang Inst Automat, Shenyang 110016, Liaoning, Peoples R China.
   [Ren, Weihong; Chen, Xiai] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Shenyang Institute of Automation, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Tian, JD (corresponding author), Chinese Acad Sci, State Key Lab Robot, Shenyang Inst Automat, Shenyang 110016, Liaoning, Peoples R China.
EM tianjd@sia.cn; hanzhi@sia.cn; renweihong@sia.cn; chenxiai@sia.cn;
   ytang@sia.cn
OI chen, xi ai/0000-0003-4756-3962; Han, Zhi/0000-0002-8039-6679; Tang,
   Yandong/0000-0003-3805-7654; ren, wei hong/0000-0003-3839-0078
FU Natural Science Foundation of China [91648118, 61473280, 61333019,
   61773367]; Youth Innovation Promotion Association CAS
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 91648118, 61473280, 61333019, and 61773367 and in
   part by the Youth Innovation Promotion Association CAS. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Sen-Ching Samson Cheung.
CR [Anonymous], P INT C COMP SCI AUT
   Barnum PC, 2010, INT J COMPUT VISION, V86, P256, DOI 10.1007/s11263-008-0200-2
   Bossu J, 2011, INT J COMPUT VISION, V93, P348, DOI 10.1007/s11263-011-0421-7
   Brewer N, 2008, LECT NOTES COMPUT SC, V5342, P451, DOI 10.1007/978-3-540-89689-0_49
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793
   Chen DY, 2014, IEEE T CIRC SYST VID, V24, P1430, DOI 10.1109/TCSVT.2014.2308627
   Chen Zhen, 2013, Journal of Multimedia, V8, P168, DOI 10.4304/jmm.8.2.168-174
   De Charette R., 2012, PROC IEEE INT C COMP, P1
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Fedorov R, 2016, IEEE T MULTIMEDIA, V18, P1187, DOI 10.1109/TMM.2016.2535356
   Garg K, 2005, IEEE I CONF COMP VIS, P1067
   GARG K, 2004, PROC CVPR IEEE, P528, DOI DOI 10.1109/CVPR.2004.1315077
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   Ji H, 2010, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2010.5539849
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kim JH, 2015, IEEE T IMAGE PROCESS, V24, P2658, DOI 10.1109/TIP.2015.2428933
   Lin Z., 2009, Technical Report (No. UILU-ENG-09-2215
   Liu B, 2007, ICIAS 2007: INTERNATIONAL CONFERENCE ON INTELLIGENT & ADVANCED SYSTEMS, VOLS 1-3, PROCEEDINGS, P647
   ORCHARD MT, 1994, IEEE T IMAGE PROCESS, V3, P693, DOI 10.1109/83.334974
   Sakaino H, 2002, INT C PATT RECOG, P60, DOI 10.1109/ICPR.2002.1047400
   Sakaino H, 2012, IEEE T IMAGE PROCESS, V21, P441, DOI 10.1109/TIP.2011.2165220
   Sakaino H, 2009, IEEE IMAGE PROC, P1609, DOI 10.1109/ICIP.2009.5413658
   Santhaseelan V, 2015, INT J COMPUT VISION, V112, P71, DOI 10.1007/s11263-014-0759-8
   Szeliski R, 2011, TEXTS COMPUT SCI, P1, DOI 10.1007/978-1-84882-935-0
   Tamburo R, 2014, LECT NOTES COMPUT SC, V8692, P750, DOI 10.1007/978-3-319-10593-2_49
   Wang HY, 2017, IEEE T MULTIMEDIA, V19, P969, DOI 10.1109/TMM.2016.2638624
   Xu J., 2012, CANADIAN CTR SCI ED, V5, P49, DOI DOI 10.5539/CIS.V5N3P49
   Zhang XP, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P461, DOI 10.1109/ICME.2006.262572
   Zheng XC, 2013, PROCEEDINGS OF THE 5TH (2013) INTERNATIONAL CONFERENCE ON FINANCIAL RISK AND CORPORATE FINANCE MANAGEMENT, VOLS I AND II, P258
   Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132
NR 31
TC 16
Z9 18
U1 4
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2659
EP 2669
DI 10.1109/TMM.2018.2808763
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000010
DA 2024-07-18
ER

PT J
AU Coleman, P
   Franck, A
   Francombe, J
   Liu, QJ
   de Campos, T
   Hughes, RJ
   Menzies, D
   Gálvez, MFS
   Tang, Y
   Woodcock, J
   Jackson, PJB
   Melchior, F
   Pike, C
   Fazi, FM
   Cox, TJ
   Hilton, A
AF Coleman, Philip
   Franck, Andreas
   Francombe, Jon
   Liu, Qingju
   de Campos, Teofilo
   Hughes, Richard J.
   Menzies, Dylan
   Galvez, Marcos F. Simon
   Tang, Yan
   Woodcock, James
   Jackson, Philip J. B.
   Melchior, Frank
   Pike, Chris
   Fazi, Filippo Maria
   Cox, Trevor J.
   Hilton, Adrian
TI An Audio-Visual System for Object-Based Audio: From Recording to
   Listening
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio systems; audio-visual systems
ID SOURCE SEPARATION; REPRODUCTION
AB Object-based audio is an emerging representation for audio content, where content is represented in a reproduction-format-agnostic way and, thus, produced once for consumption on many different kinds of devices. This affords new opportunities for immersive, personalized, and interactive listening experiences. This paper introduces an end-to-end object-based spatial audio pipeline, from sound recording to listening. A high-level system architecture is proposed, which includes novel audio-visual interfaces to support object-based capture and listener-tracked rendering, and incorporates a proposed component for objectification, that is, recording content directly into an object-based form. Text-based and extensible metadata enable communication between the system components. An open architecture for object rendering is also proposed. The system's capabilities are evaluated in two parts. First, listener-tracked reproduction of metadata automatically estimated from two moving talkers is evaluated using an objective binaural localization model. Second, object-based scene capture with audio extracted using blind source separation (to remix between two talkers) and beamforming (to remix a recording of a jazz group) is evaluated with perceptually motivated objective and subjective experiments. These experiments demonstrate that the novel components of the system add capabilities beyond the state of the art. Finally, we discuss challenges and future perspectives for object-based audio workflows.
C1 [Coleman, Philip; Liu, Qingju; de Campos, Teofilo; Jackson, Philip J. B.; Hilton, Adrian] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
   [Coleman, Philip; Francombe, Jon] Univ Surrey, Inst Sound Recording, Guildford GU2 7XH, Surrey, England.
   [Franck, Andreas; Menzies, Dylan; Galvez, Marcos F. Simon; Fazi, Filippo Maria] Univ Southampton, Inst Sound & Vibrat Res, Southampton SO17 1BJ, Hants, England.
   [Francombe, Jon; Melchior, Frank; Pike, Chris] British Broadcasting Corp Res & Dev, Salford M50 2LH, Lancs, England.
   [de Campos, Teofilo] Univ Brasilia, BR-70910900 Gama, DF, Brazil.
   [Hughes, Richard J.; Tang, Yan; Woodcock, James; Cox, Trevor J.] Univ Salford, Acoust Res Ctr, Salford M5 4WT, Lancs, England.
C3 University of Surrey; University of Surrey; University of Southampton;
   Universidade de Brasilia; University of Salford
RP Coleman, P (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.; Coleman, P (corresponding author), Univ Surrey, Inst Sound Recording, Guildford GU2 7XH, Surrey, England.
EM p.d.coleman@surrey.ac.uk; A.Franck@soton.ac.uk; jon.francombe@bbc.co.uk;
   q.liu@surrey.ac.uk; t.decampos@surrey.ac.uk; r.j.hughes@salford.ac.uk;
   d.menzies@soton.ac.uk; m.f.simon-galvez@soton.ac.uk;
   y.tang@salford.ac.uk; j.s.woodcock@salford.ac.uk;
   p.jackson@surrey.ac.uk; chris.pike@bbc.co.uk; Filippo.Fazi@soton.ac.uk;
   t.j.cox@salford.ac.uk; a.hilton@surrey.ac.uk
RI de Campos, Teofilo/ABF-8003-2020; Fazi, Filippo/AAB-1044-2019; Jackson,
   Philip J B/E-8422-2013; Hilton, Adrian/N-3736-2014
OI de Campos, Teofilo/0000-0001-6172-0229; Jackson, Philip J
   B/0000-0001-7933-5935; LIU, Qingju/0000-0003-0778-2992; Coleman,
   Philip/0000-0002-3266-7358; Tang, Yan/0000-0003-1149-4272; Fazi, Filippo
   Maria/0000-0003-4129-1433; Hilton, Adrian/0000-0003-4223-238X;
   Francombe, Jon/0000-0003-3227-5001
FU EPSRC [EP/L000539/1]; EPSRC [EP/P022529/1, EP/M028321/1, EP/L000539/1]
   Funding Source: UKRI
FX This work was supported by the EPSRC Programme Grant S3A: Future Spatial
   Audio for an Immersive Listener Experience at Home (EP/L000539/1). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Wolfgang Hurst.
CR [Anonymous], 2014, BMVC
   [Anonymous], 2016, ETSITS103448
   [Anonymous], 1991, AES101991
   [Anonymous], 2015, ETSITS1031902
   Armstrong Mike, 2014, IET C P JAN 2014, P12
   Barnard M, 2014, IEEE T MULTIMEDIA, V16, P864, DOI 10.1109/TMM.2014.2301977
   Bernschutz B., 2013, P 40 IT ANN C AC 39
   Coleman  P., 2015, P 138 CONV AUD ENG S
   Coleman P, 2017, J AUDIO ENG SOC, V65, P66, DOI 10.17743/jaes.2016.0059
   Comminiello D, 2015, IEEE T MULTIMEDIA, V17, P1262, DOI 10.1109/TMM.2015.2442151
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   Corteel E., 2016, P 140 CONV AUD ENG S
   Dietz M, 2011, SPEECH COMMUN, V53, P592, DOI 10.1016/j.specom.2010.05.006
   Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038
   Evans M., 2016, P IBC C AMST NETH SE, P34
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   Fazi F.M, 2016, P 22 INT C AC BUEN A, P1
   Francombe J., 2015, P 138 CONV AUD ENG S
   Francombe J., 2015, P 139 CONV AUD ENG S
   Francombe J, 2017, J AUDIO ENG SOC, V65, P212, DOI 10.17743/jaes.2016.0071
   Fug S., 2014, P 137 CONV AUD ENG S
   Gálvez MFS, 2016, J AUDIO ENG SOC, V64, P740, DOI 10.17743/jaes.2016.0027
   Geier M., 2012, P LIN AUD C STANF CA, P183
   Geier M, 2010, ORGAN SOUND, V15, P219, DOI 10.1017/S1355771810000324
   Heineman GeorgeT., 2001, COMPONENT BASED SOFT
   Herre J, 2015, IEEE J-STSP, V9, P770, DOI 10.1109/JSTSP.2015.2411578
   Huang YT, 2011, IEEE SIGNAL PROC MAG, V28, P20, DOI 10.1109/MSP.2010.938754
   IETF, 2014, 7159 IETF RFC
   ITU-R, 2015, REC BS 1770 4 ALG ME
   ITU-R, 2014, REC ITU R BS 2051 0
   ITU-R, 2015, REC BS 2076 0 AUD DE
   ITU-R, 2015, REC ITU R BS 1116 3
   Katsaggelos AK, 2015, P IEEE, V103, P1635, DOI 10.1109/JPROC.2015.2459017
   Kinoshita K, 2016, EURASIP J ADV SIG PR, DOI 10.1186/s13634-016-0306-6
   Kirkeby O, 1998, J ACOUST SOC AM, V104, P1973, DOI 10.1121/1.423763
   KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830
   Komori T., 2015, P 138 CONV AUD ENG S
   Kumatani K, 2012, IEEE SIGNAL PROC MAG, V29, P127, DOI 10.1109/MSP.2012.2205285
   Liu QJ, 2016, INT CONF ACOUST SPEE, P1506, DOI 10.1109/ICASSP.2016.7471928
   Liu QJ, 2015, EUR SIGNAL PR CONF, P1088, DOI 10.1109/EUSIPCO.2015.7362551
   Mahler RPS, 2003, IEEE T AERO ELEC SYS, V39, P1152, DOI 10.1109/TAES.2003.1261119
   Mandel MI, 2010, IEEE T AUDIO SPEECH, V18, P382, DOI 10.1109/TASL.2009.2029711
   Mann M., 2013, Proc. ACM International workshop on Immersive media experiences, P13
   Mao Ye, 2013, Time-Of-Flight and Depth Imaging. Sensors, Algorithms and Applications. Dagstuhl 2012 Seminar on Time-of-Flight Imaging and GCPR 2013 Workshop on Imaging New Modalities: LNCS 8200, P149, DOI 10.1007/978-3-642-44964-2_8
   MDA, 2015, ETSITS103223
   Menzies D., 2015, P 138 CONV AUD ENG S
   Mouchtaris A, 2000, IEEE T MULTIMEDIA, V2, P77, DOI 10.1109/6046.845012
   Nixon T., 2015, P 3 INT C SPAT AUD G, P1
   Nugraha AA, 2016, IEEE-ACM T AUDIO SPE, V24, P1652, DOI 10.1109/TASLP.2016.2580946
   Oldfield R., 2013, IEEE ICME WORKSH, P1
   Oldfield R, 2015, MULTIMED TOOLS APPL, V74, P2717, DOI 10.1007/s11042-013-1472-2
   Ozerov A, 2010, IEEE T AUDIO SPEECH, V18, P550, DOI 10.1109/TASL.2009.2031510
   Peters N, 2013, COMPUT MUSIC J, V37, P11, DOI 10.1162/COMJ_a_00167
   Pike C., 2013, P 134 CONV AUD ENG S
   Pike C., 2016, P INT C HEADPH TECHN
   Pike C., 2016, P AES INT C AUD VIRT
   Pulkki V, 1997, J AUDIO ENG SOC, V45, P456
   Rix AW, 2001, INT CONF ACOUST SPEE, P749, DOI 10.1109/ICASSP.2001.941023
   Rumsey F, 2012, Spatial audio
   Rumsey Francis., 2006, Sound and Recording: An Introduction
   Saragih JM, 2009, IEEE I CONF COMP VIS, P1034, DOI 10.1109/ICCV.2009.5459377
   SHIRLEY B., 2013, MEDIA PRODUCTION DEL, P130
   Shirley B, 2015, J AUDIO ENG SOC, V63, P245, DOI 10.17743/jaes.2015.0017
   Sondergaardand P L, 2013, TECHNOLOGY BINAURAL, DOI [DOI 10.1007/978-3-642-37762-4_2, 10.1007/978-3-642-37762-4_2]
   Song MS, 2011, IEEE T MULTIMEDIA, V13, P844, DOI 10.1109/TMM.2011.2162581
   Spors S, 2013, P IEEE, V101, P1920, DOI 10.1109/JPROC.2013.2264784
   Taal CH, 2011, IEEE T AUDIO SPEECH, V19, P2125, DOI 10.1109/TASL.2011.2114881
   Takeuchi T, 2002, J ACOUST SOC AM, V112, P2786, DOI 10.1121/1.1513363
   Takeuchi T., 2016, P 140 CONV AUD ENG S
   Tang Y, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2568
   Thomas G., 2013, MEDIA PRODUCTION DEL, P5
   Van Veen B. D., 1988, IEEE ASSP Magazine, V5, P4, DOI 10.1109/53.665
   Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005
   Wang DL, 2005, SPEECH SEPARATION BY HUMANS AND MACHINES, P181, DOI 10.1007/0-387-22794-6_12
   Weitnauer M., 2017, TECH REP
   Westner A. G., 1999, THESIS
   Woodcock J., 2016, P 140 CONV AUD ENG S
   Zotter F, 2012, J AUDIO ENG SOC, V60, P807
NR 78
TC 25
Z9 27
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 1919
EP 1931
DI 10.1109/TMM.2018.2794780
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600001
OA Green Accepted, hybrid
DA 2024-07-18
ER

PT J
AU Bak, C
   Kocak, A
   Erdem, E
   Erdem, A
AF Bak, Cagdas
   Kocak, Aysun
   Erdem, Erkut
   Erdem, Aykut
TI Spatio-Temporal Saliency Networks for Dynamic Saliency Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic saliency; deep learning
ID VISUAL SALIENCY; MODEL; QUALITY; GAZE
AB Computational saliency models for still images have gained significant popularity in recent years. Saliency prediction from videos, on the other hand, has received relatively little interest from the community. Motivated by this, in this paper, we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks. The key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information. We evaluate our models on the dynamic images and eye movements and University of Central Florida-Sports datasets and present highly competitive results against the existing state-of-the-art models. We also carry out some experiments on a number of still images from the MIT300 dataset by exploiting the optical flow maps predicted from these images. Our results show that considering inherent motion information in this way can be helpful for static saliency estimation.
C1 [Bak, Cagdas; Kocak, Aysun; Erdem, Erkut; Erdem, Aykut] Hacettepe Univ, Dept Comp Engn, TR-06800 Ankara, Turkey.
C3 Hacettepe University
RP Erdem, E (corresponding author), Hacettepe Univ, Dept Comp Engn, TR-06800 Ankara, Turkey.
EM cgds77@gmail.com; aysunko-cak@cs.hacettepe.edu.tr;
   erkut@cs.hacettepe.edu.tr; aykut@cs.hacettepe.edu.tr
RI Erdem, Aykut/A-2290-2012; Erdem, Erkut/A-2291-2012
OI Erdem, Erkut/0000-0002-6744-8614; Erdem, Aykut/0000-0002-6280-8422
FU TUBITAK [113E497]; Hacettepe [BAP FDS-2016-10202]
FX This work was supported in part by TUBITAK Career Development Award
   113E497 and in part by the Hacettepe BAP FDS-2016-10202.
CR [Anonymous], 2016, CVPR
   [Anonymous], MIT Saliency Benchmark
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bruce NDB, 2016, PROC CVPR IEEE, P516, DOI 10.1109/CVPR.2016.62
   Bylinskii Z, 2016, LECT NOTES COMPUT SC, V9909, P809, DOI 10.1007/978-3-319-46454-1_49
   Chen DY, 2013, IEEE T MULTIMEDIA, V15, P1616, DOI 10.1109/TMM.2013.2267725
   Cheng G., 2008, TRANSPORTATION RES B, P1
   Cichy RM, 2016, SCI REP-UK, V6, DOI 10.1038/srep27755
   Cui Xinyi, 2009, P 17 ACM INT C MULT, P617, DOI DOI 10.1145/1631272.1631370
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P3910, DOI 10.1109/TIP.2014.2336549
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang J, 2011, IEEE T MULTIMEDIA, V13, P653, DOI 10.1109/TMM.2011.2127463
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Jetley S, 2016, PROC CVPR IEEE, P5753, DOI 10.1109/CVPR.2016.620
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Judd T, 2011, J VISION, V11, DOI 10.1167/11.4.14
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Khatoonabadi SH, 2015, PROC CVPR IEEE, P5501, DOI 10.1109/CVPR.2015.7299189
   Kim H, 2015, IEEE T MULTIMEDIA, V17, P2198, DOI 10.1109/TMM.2015.2493367
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620
   Kruthiventi SSS, 2016, PROC CVPR IEEE, P5781, DOI 10.1109/CVPR.2016.623
   Kummerer M., 2015, INT C LEARN REPR ICL
   Leborán V, 2017, IEEE T PATTERN ANAL, V39, P893, DOI 10.1109/TPAMI.2016.2567391
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mathe S, 2015, IEEE T PATTERN ANAL, V37, P1408, DOI 10.1109/TPAMI.2014.2366154
   Mathe S, 2012, LECT NOTES COMPUT SC, V7573, P842, DOI 10.1007/978-3-642-33709-3_60
   Mauthner T, 2015, PROC CVPR IEEE, P2494, DOI 10.1109/CVPR.2015.7298864
   Mital PK, 2011, COGN COMPUT, V3, P5, DOI 10.1007/s12559-010-9074-z
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Ren Z., 2012, P INT C MULT RETR
   Rudoy D, 2013, PROC CVPR IEEE, P1147, DOI 10.1109/CVPR.2013.152
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Shen CY, 2015, IEEE T MULTIMEDIA, V17, P2084, DOI 10.1109/TMM.2015.2483370
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sultani W, 2014, PROC CVPR IEEE, P764, DOI 10.1109/CVPR.2014.103
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
   Xuanhan Wang, 2017, IEEE Signal Processing Letters, V24, P510, DOI 10.1109/LSP.2016.2611485
   Yu JG, 2016, IEEE T MULTIMEDIA, V18, P273, DOI 10.1109/TMM.2015.2505908
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhao JP, 2015, PROC CVPR IEEE, P3174, DOI 10.1109/CVPR.2015.7298937
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhou F, 2014, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2014.429
NR 66
TC 109
Z9 117
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1688
EP 1698
DI 10.1109/TMM.2017.2777665
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sallam, AI
   Faragallah, OS
   El-Rabaie, EM
AF Sallam, Ahmed I.
   Faragallah, Osama S.
   El-Rabaie, El-Sayed M.
TI HEVC Selective Encryption Using RC6 Block Cipher Technique
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video compression; HEVC; context adaptive binary arithmetic coding
   (CABAC); video encryption; selective encryption; RC6; advanced
   encryption standard (AES)
ID VIDEO
AB The high efficiency video coding (HEVC) partial encryption (PE) technique depends on encrypting the highly sensitive data on the video hit stream. The HEVC PE technique should keep the video format compliance, should be of the same hit rate, and ensure real-time constraints. The paper suggests an effective RC6 HEVC PE technique which encrypts sensible video data bits with low complexity overhead, fast encoding time for realtime applications, and fixed HEVC bitrate. These features result from using the low computational complexity RC6 block cipher for encrypting the selective video bins. The proposed RC6 HEVC PE encrypts the discrete cosine transform (DCT) coefficients sign bit, the DCT remaining absolute values suffixes that are binarized by Exp-Golomb (EGk) order zero, the motion vector difference (MVD) sign bits, and MVD absolute values suffixes that are binarized by EGk order one. Also, this paper introduces experimental results that compare between the proposed RC6 HEVC PE and HEVC PE algorithms that use the Advanced Encryption Standard in different operation modes. This paper presents more details about the security analysis of RC6 HEVC PE including encryption quality test, key space test, statistical analysis such as histogram and correlation coefficient analysis, and sensitivity analysis such as the key sensitivity analysis. The achieved test results ensured and confirmed the security, reliability, and robustness of RC6 HEVC PE technique.
C1 [Sallam, Ahmed I.; Faragallah, Osama S.] Menoufia Univ, Fac Elect Engn, Dept Comp Sci & Engn, Menoufia 32952, Egypt.
   [Faragallah, Osama S.] Taif Univ, Coll Comp & Informat Technol, Dept Informat Technol, At Taif 21974, Saudi Arabia.
   [El-Rabaie, El-Sayed M.] Menoufia Univ, Fac Elect Engn, Dept Elect & Commun Engn, Menoufia 32952, Egypt.
C3 Egyptian Knowledge Bank (EKB); Menofia University; Taif University;
   Egyptian Knowledge Bank (EKB); Menofia University
RP Faragallah, OS (corresponding author), Menoufia Univ, Fac Elect Engn, Dept Comp Sci & Engn, Menoufia 32952, Egypt.
EM a_sallam82@hotmail.com; osam_sal@yahoo.com; srabie1@yahoo.com
RI Faragallah, Osama S./AHB-8031-2022
OI Faragallah, Osama S./0000-0003-1982-335X; EL-Rabaie,
   El-Sayed/0000-0001-6854-5881
CR Ahmad J., 2012, International Journal of Video and Image Processing and Network Security, V12, P18
   Ahmed HEDH, 2007, INFORM-J COMPUT INFO, V31, P121
   [Anonymous], 2001, FIPS PUB
   [Anonymous], 2001, ITU T VCEG M AUST TE
   Avneetkaur Lakhwinderkaur, 2012, P INT J COMPUT APPL, V59, P32
   Bossen F, 2012, IEEE T CIRC SYST VID, V22, P1685, DOI 10.1109/TCSVT.2012.2221255
   Bross B., 2012, JOINT COLL TEAM VID
   Contini S., 1998, SECURITY RC6TM BLOCK
   DAEMAN J, 1999, AES PROPOSAL RIJNDAE
   de Souza DF, 2017, IEEE T MULTIMEDIA, V19, P459, DOI 10.1109/TMM.2016.2625261
   Fraunhofer Heinrich Hertz Institute, 2015, HIGH EFF VID COD HEV
   Gao Y, 2016, IEEE T MULTIMEDIA, V18, P2321, DOI 10.1109/TMM.2016.2598481
   GLADMAN B, 2003, SPECIFICATION RIJNDA
   Hofbauer Heinz, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1986, DOI 10.1109/ICASSP.2014.6853946
   Rivest R, 1998, RC6 BLOCK CIPHER SIM
   Shahid Z, 2014, IEEE T MULTIMEDIA, V16, P24, DOI 10.1109/TMM.2013.2281029
   Shen WW, 2016, IEEE T MULTIMEDIA, V18, P1022, DOI 10.1109/TMM.2016.2532606
   Sze V., 2014, ALGORITHMS ARCHITECT
   Tew Y, 2015, ASIAPAC SIGN INFO PR, P963, DOI 10.1109/APSIPA.2015.7415415
   tutorialspoint, ADV ENCRYPTION STAND
   Tutorialspoint, BLOCK CIPH MOD OP
   Ultra Video Group, VID TEST SEQ
   Van Wallendael G, 2013, IEEE T CONSUM ELECTR, V59, P634, DOI 10.1109/TCE.2013.6626250
   Wang MH, 2014, IEEE T MULTIMEDIA, V16, P933, DOI 10.1109/TMM.2014.2305579
   Xiph.org, TEST MED
NR 25
TC 45
Z9 48
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1636
EP 1644
DI 10.1109/TMM.2017.2777470
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100004
DA 2024-07-18
ER

PT J
AU Lie, WN
   Hsieh, CY
   Lin, GS
AF Lie, Wen-Nung
   Hsieh, Chia-Yung
   Lin, Guo-Shiang
TI Key-Frame-Based Background Sprite Generation for Hole Filling in Depth
   Image-Based Rendering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth image-based rendering; hole filling; disocclusion; sprite
   generation; global motion estimation
ID VIEW SYNTHESIS; VIDEO; DISOCCLUSION; ALGORITHM
AB In this paper, we propose a new depth image-based rending scheme for 3DTV applications, where a background sprite model is utilized for dis-occlusion/hole filling purpose. Dissimilar to traditional spatial (e.g., interpolation or inpainting) and temporal methods, our algorithm is capable of recovering the holes with true background information by incrementally integrating the spatial and temporal information of the video in a unified background sprite model. The technique of background sprite model construction in this paper is featured of resolving camera motions existing in most of the consumer videos, accurate registration of multiframe information, and efficient memory use and computation in realistic 3DTV application. To register/stitch each input frame to the background sprite model accurately, foreground removal considering color and depth information, and an adaptive key-frame-based scheme are developed for transform computation. Experimental results show that our proposed scheme has a large temporal reference distance and can retrieve true background information accurately, thus leading to better quality after novel view synthesis compared to existing spatial or spatio-temporal algorithms, especially for videos with significant camera motions or complex backgrounds.
C1 [Lie, Wen-Nung; Hsieh, Chia-Yung] Natl Chung Cheng Univ, Dept Elect Engn, Chiayi 621, Taiwan.
   [Lin, Guo-Shiang] Da Yeh Univ, Dept Comp Sci & Informat Engn, Changhua 515, Taiwan.
C3 National Chung Cheng University; Da Yeh University
RP Lie, WN (corresponding author), Natl Chung Cheng Univ, Dept Elect Engn, Chiayi 621, Taiwan.
EM ieewnl@ccu.edu.tw; u9613039@yuntech.edu.tw; khlin@mail.dyu.edu.tw
RI Lie, Wen-Nung/AFP-1266-2022
OI Lie, Wen-Nung/0000-0002-8166-2844
CR [Anonymous], 2013, JTC1SC29WG11 ISOIEC
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2006, Digital Video Image Quality and Perceptual Coding
   Chen K. Y., 2010, P IEEE 3DTV C TRUE V
   Cheng CM, 2011, IEEE T BROADCAST, V57, P523, DOI 10.1109/TBC.2011.2139090
   Choi S, 2013, IEEE T IMAGE PROCESS, V22, P2429, DOI 10.1109/TIP.2013.2251646
   Daribo Ismael, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P145, DOI 10.1109/MMSP.2010.5662009
   Daribo I, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/258920
   Hewage CTER, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P485, DOI 10.1109/ICME.2008.4607477
   Hsieh Chun-An, 2013, THESIS
   Huang Y., 2008, P INT C IEEE PATT RE
   Jin J, 2016, IEEE T MULTIMEDIA, V18, P953, DOI 10.1109/TMM.2016.2539825
   Jung J. I., 2011, P AS PAC SIGN INF PR
   Jung J. I., 2012, P INT C IEEE COMP CO
   Köppel M, 2012, IEEE INT WORKSH MULT, P25, DOI 10.1109/MMSP.2012.6343410
   Kuo IS, 2006, INT J PATTERN RECOGN, V20, P1139, DOI 10.1142/S0218001406005162
   Lee PJ, 2011, IEEE T MULTIMEDIA, V13, P246, DOI 10.1109/TMM.2010.2100372
   Lin GS, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-237
   Liu H., 2012, P 3DTV C TRUE VIS CA
   Mao Y, 2016, IEEE T MULTIMEDIA, V18, P1453, DOI 10.1109/TMM.2016.2573142
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Purica AI, 2015, INT CONF ACOUST SPEE, P1191, DOI 10.1109/ICASSP.2015.7178158
   Reel S., 2013, P ANN SUMM C SIGN IN
   Schmeing M., 2010, P 3DTV C TRUE VIS CA
   Schmeing M, 2015, IEEE T MULTIMEDIA, V17, P2160, DOI 10.1109/TMM.2015.2476372
   Schreer O, 2005, 3D VIDEOCOMMUNICATION: ALGORITHMS, CONCEPTS AND REAL-TIME SYSTEMS IN HUMAN CENTRED COMMUNICATION, P1, DOI 10.1002/0470022736
   Sonka M., 2008, IMAGE PROCESSING ANA
   Su YB, 2005, IEEE T CIRC SYST VID, V15, P232, DOI 10.1109/TCSVT.2004.841656
   Sun WX, 2012, IEEE IMAGE PROC, P2721, DOI 10.1109/ICIP.2012.6467461
   Tauber Z, 2007, IEEE T SYST MAN CY C, V37, P527, DOI 10.1109/TSMCC.2006.886967
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Yao C, 2014, IEEE T BROADCAST, V60, P394, DOI 10.1109/TBC.2014.2321671
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
NR 33
TC 13
Z9 16
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1075
EP 1087
DI 10.1109/TMM.2017.2763319
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400005
DA 2024-07-18
ER

PT J
AU Shao, F
   Tian, WJ
   Lin, WS
   Jiang, GY
   Dai, QH
AF Shao, Feng
   Tian, Weijun
   Lin, Weisi
   Jiang, Gangyi
   Dai, Qionghai
TI Learning Sparse Representation for No-Reference Quality Assessment of
   Multiply Distorted Stereoscopic Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind/no reference; multiply distorted stereoscopic image (MDSI); sparse
   representation; binocular combination
ID STRUCTURAL SIMILARITY; NATURAL IMAGES; COMPRESSION; DOMAIN; INDEX;
   INFORMATION; DICTIONARY; FRAMEWORK; SCORES; ENERGY
AB Binocular combination under different distortion types poses a great challenge to three-dimensional image quality assessment (3D-IQA). However, the research works on 3D-IQA with multiple distortion types are very limited. In this paper, we first construct a new multiply distorted stereoscopic image database (NBU-MDSID), which is composed of 270 multiply distorted stereoscopic images and 90 singly distorted stereoscopic images that are corrupted simultaneously and independently by blurring, JPEG compression, and noise injection. We then propose a new multimodal blind metric for quality assessment of multiply distorted stereoscopic images. Inspired by multimodal sparse representation framework, modality-specific dictionaries and the corresponding projection matrices are learned from the singly distorted training database at the training stage, and the testing stage only needs to estimate the quality score based on the reconstruction errors. Experimental results demonstrate the effectiveness of our blind metric.
C1 [Shao, Feng; Tian, Weijun; Jiang, Gangyi] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Ctr Multimedia & Network Technol, Sch Comp Engn, Singapore 639798, Singapore.
   [Dai, Qionghai] Tsinghua Univ, Broadband Networks & Digital Media Lab, Beijing 100084, Peoples R China.
C3 Ningbo University; Nanyang Technological University; Tsinghua University
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
EM shaofeng@nbu.edu.cn; 827843779@qq.com; wslin@ntu.edu.sg;
   jianggangyi@nbu.edu.cn; qhdai@tsinghua.edu.cn
RI Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011; Dai,
   Qionghai/ABD-5298-2021; jiang, gang/KII-8233-2024
OI Lin, Weisi/0000-0001-9866-1947; Dai, Qionghai/0000-0001-7043-3061; 
FU Natural Science Foundation of China [61622109, 61271021, U1301257]; K.
   C. Wong Magna Fund in Ningbo University
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61622109, Grant 61271021, and Grant U1301257, and in
   part by the K. C. Wong Magna Fund in Ningbo University. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Christian Timmerer.
CR [Anonymous], IEEE SIGNAL PROC LET
   [Anonymous], 2014, INT S BROADB MULT SY
   Bahrampour S, 2016, IEEE T IMAGE PROCESS, V25, P24, DOI 10.1109/TIP.2015.2496275
   Bensalma R, 2013, MULTIDIM SYST SIGN P, V24, P281, DOI 10.1007/s11045-012-0178-3
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Gu K, 2013, IEEE WORKSHOP SIG, P241, DOI 10.1109/SiPS.2013.6674512
   Gu K, 2012, J ELECTR COMPUT ENG, V2012, DOI 10.1155/2012/436031
   Hewage CTER, 2011, IEEE T CONSUM ELECTR, V57, P1185, DOI 10.1109/TCE.2011.6018873
   ITU-R BT. 500-13, 2002, BT50013 ITUR
   ITU-T P. 911, 1999, P911 ITUT, P911
   ITU-T-RECOMMENDATION, 1999, SERIES P TELEPHONE T, P910
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Lee K, 2015, IEEE J-STSP, V9, P533, DOI 10.1109/JSTSP.2015.2393296
   Li CF, 2015, IEEE IMAGE PROC, P4883, DOI 10.1109/ICIP.2015.7351735
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Lin YH, 2014, IEEE T IMAGE PROCESS, V23, P1527, DOI 10.1109/TIP.2014.2302686
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu TJ, 2017, IEEE T NEUR NET LEAR, V28, P107, DOI 10.1109/TNNLS.2015.2500268
   Lu YA, 2015, IEEE SIGNAL PROC LET, V22, P1811, DOI 10.1109/LSP.2015.2436908
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Mantiuk R, 2005, PROC SPIE, V5666, P204, DOI 10.1117/12.586757
   Marziliano P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P57, DOI 10.1109/ICIP.2002.1038902
   Meegan DV, 2001, J EXP PSYCHOL-APPL, V7, P143, DOI 10.1037//1076-898X.7.2.143
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2013, SIGNAL PROCESS-IMAGE, V28, P870, DOI 10.1016/j.image.2012.08.004
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Qi F, 2015, IEEE T MULTIMEDIA, V17, P2338, DOI 10.1109/TMM.2015.2493781
   Rehman A, 2012, IEEE T IMAGE PROCESS, V21, P3378, DOI 10.1109/TIP.2012.2197011
   Ryu S, 2014, IEEE T CIRC SYST VID, V24, P591, DOI 10.1109/TCSVT.2013.2279971
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Said CP, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002991
   Sazzad ZMP, 2012, ADV MULTIMED, V2012, DOI 10.1155/2012/256130
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Seuntiens P., 2006, ACM T APPL PERCEPT, V3, P95
   Shao F, 2016, IEEE T CYBERNETICS, V46, P730, DOI 10.1109/TCYB.2015.2414479
   Shao F, 2015, IEEE T IMAGE PROCESS, V24, P2971, DOI 10.1109/TIP.2015.2436332
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Su CC, 2015, IEEE T IMAGE PROCESS, V24, P1685, DOI 10.1109/TIP.2015.2409558
   Tang HX, 2011, PROC CVPR IEEE, P305, DOI 10.1109/CVPR.2011.5995446
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang JH, 2015, IEEE T IMAGE PROCESS, V24, P3400, DOI 10.1109/TIP.2015.2446942
   Wang X, 2015, NEUROCOMPUTING, V151, P683, DOI 10.1016/j.neucom.2014.05.090
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE IMAGE PROC, P477
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Woods AJ., 2010, Keynote Presentation at the Three-Dimensional Systems and Applications Conference, Tokyo, Japan, P19
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Ye P, 2014, PROC CVPR IEEE, P4241, DOI 10.1109/CVPR.2014.540
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhang LJ, 2012, IEEE T IMAGE PROCESS, V21, P2379, DOI 10.1109/TIP.2012.2183879
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang Y, 2015, IEEE T IMAGE PROCESS, V24, P3810, DOI 10.1109/TIP.2015.2456414
   Zhao Y, 2013, IEEE IMAGE PROC, P132, DOI 10.1109/ICIP.2013.6738028
   Zhou WJ, 2014, SIGNAL PROCESS-IMAGE, V29, P167, DOI 10.1016/j.image.2013.10.005
   Zoran D, 2009, IEEE I CONF COMP VIS, P2209, DOI 10.1109/ICCV.2009.5459476
NR 68
TC 22
Z9 23
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1821
EP 1836
DI 10.1109/TMM.2017.2685240
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400011
DA 2024-07-18
ER

PT J
AU Jing, PG
   Su, YT
   Nie, LQ
   Gu, HM
AF Jing, Peiguang
   Su, Yuting
   Nie, Liqiang
   Gu, Huimin
TI Predicting Image Memorability Through Adaptive Transfer Learning From
   External Sources
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image attribute; memorability prediction; regression; transfer learning;
   visual feature
ID SCENE
AB Remembering images is an innate human capability. Camera images are captured by different people under varying environmental conditions, which leads to highly diverse image memorability scores. However, the factors that make an image more or less memorable are unclear, and it remains unknown how we can more accurately predict image memorability by using such factors. In this paper, we propose a novel framework called multiview transfer learning from external sources (MTLES) to predict image memorability. In this framework, we simultaneously leverage different types of visual feature sets and multiple types of predefined image attributes derived from external sources. In particular, to enhance representation ability of visual features, we construct connections between visual feature sets and higher level image attributes by transferring attribute knowledge from external sources. MTLES integrates weak learning through external sources, transfer learning, and multiview consistency loss with different types of feature sets into a joint framework. To better solve this joint optimization problem, we further develop an alternating iterative algorithm to deal with it. Experiments performed on the publicly available LaMem dataset demonstrate the effectiveness of the proposed scheme.
C1 [Jing, Peiguang; Su, Yuting; Gu, Huimin] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
C3 Tianjin University; Shandong University
RP Jing, PG (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM pgjing@tju.edu.cn; ytsu@tju.edu.cn; nieliqiang@gmail.com;
   sherryghm@tju.edu.cn
FU National Natural Science Foundation of China [61572356, 61303208];
   Tianjin Research Program of Application Foundation and Advanced
   Technology [15JCQNJC41600]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61572356 and Grant 61303208, and in part
   by the Tianjin Research Program of Application Foundation and Advanced
   Technology under Grant 15JCQNJC41600. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Marco Bertini.
CR [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2007.383198
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2000, Em: Proceedings of the 2000 ACM workshops on Multimedia, DOI DOI 10.1145/357744.357758
   [Anonymous], 2004, P 21 C MACH LEARN IC
   [Anonymous], 2011, ADV NEURAL INF PROCE, DOI DOI 10.21236/ADA554133
   [Anonymous], 2011, P 25 AAAI C ART INT
   Arnold A., 2007, P 7 IEEE INT C DAT M, P77, DOI [10.1109/ICDMW.2007.109, DOI 10.1109/ICDMW.2007.109]
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Bhattacharya S., 2013, Proc. 21st ACM Internat. Conf. Multimedia (ACM, P361
   Borkin MA, 2013, IEEE T VIS COMPUT GR, V19, P2306, DOI 10.1109/TVCG.2013.234
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Brady TF, 2008, P NATL ACAD SCI USA, V105, P14325, DOI 10.1073/pnas.0803390105
   Bylinskii Z, 2015, VISION RES, V116, P165, DOI 10.1016/j.visres.2015.03.005
   Cao LL, 2010, PROC CVPR IEEE, P1998, DOI 10.1109/CVPR.2010.5539875
   Celikkale B, 2013, IEEE COMPUT SOC CONF, P976, DOI 10.1109/CVPRW.2013.142
   DIENER E, 1978, J PERS SOC PSYCHOL, V36, P333, DOI 10.1037/0022-3514.36.3.333
   Ding ZM, 2015, IEEE T IMAGE PROCESS, V24, P4322, DOI 10.1109/TIP.2015.2462023
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Dubey R, 2015, IEEE I CONF COMP VIS, P1089, DOI 10.1109/ICCV.2015.130
   Fu YW, 2016, IEEE T PATTERN ANAL, V38, P563, DOI 10.1109/TPAMI.2015.2456887
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hays J, 2008, PROC CVPR IEEE, P3436
   Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y
   Hou CP, 2013, IEEE T IMAGE PROCESS, V22, P340, DOI 10.1109/TIP.2012.2214044
   Hu WM, 2016, IEEE T MULTIMEDIA, V18, P76, DOI 10.1109/TMM.2015.2496372
   Huang G.B., 2008, PROC WORKSHOP FACES
   Isola P, 2014, IEEE T PATTERN ANAL, V36, P1469, DOI 10.1109/TPAMI.2013.200
   Isola P, 2011, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2011.5995721
   Khosla A, 2015, IEEE I CONF COMP VIS, P2390, DOI 10.1109/ICCV.2015.275
   Khosla A, 2013, IEEE I CONF COMP VIS, P3200, DOI 10.1109/ICCV.2013.397
   Kim J., P 21 ACM INT C MULT, P761
   Konkle T, 2010, J EXP PSYCHOL GEN, V139, P558, DOI 10.1037/a0019165
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Ma ZG, 2013, PROC CVPR IEEE, P2627, DOI 10.1109/CVPR.2013.339
   Mancas M, 2013, IEEE IMAGE PROC, P196, DOI 10.1109/ICIP.2013.6738041
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nie LQ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P591, DOI 10.1145/2733373.2806217
   Nie LQ, 2016, IEEE T CYBERNETICS, V46, P2991, DOI 10.1109/TCYB.2015.2493558
   Nie LQ, 2013, IEEE T MULTIMEDIA, V15, P426, DOI 10.1109/TMM.2012.2229971
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z
   Peng HW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1147, DOI 10.1145/2733373.2806303
   Plutchik R., 1980, EMOTION PSYCHOEVOLUT
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Raina R., 2007, P 24 INT C MACH LEAR, P759
   Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Sun C, 2015, IEEE T MULTIMEDIA, V17, P1747, DOI 10.1109/TMM.2015.2463218
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang SF, 2015, IEEE T MULTIMEDIA, V17, P2185, DOI 10.1109/TMM.2015.2484966
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yu FLX, 2013, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2013.105
   Zhang L., IEEE T CYBE IN PRESS
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817
   Zheng J., 2012, BMVC, V1, P7
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhu F, 2014, INT J COMPUT VISION, V109, P42, DOI 10.1007/s11263-014-0703-y
NR 62
TC 41
Z9 42
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 1050
EP 1062
DI 10.1109/TMM.2016.2644866
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000013
DA 2024-07-18
ER

PT J
AU Li, LD
   Xia, WH
   Lin, WS
   Fang, YM
   Wang, SQ
AF Li, Leida
   Xia, Wenhan
   Lin, Weisi
   Fang, Yuming
   Wang, Shiqi
TI No-Reference and Robust Image Sharpness Evaluation Based on Multiscale
   Spatial and Spectral Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image sharpness evaluation; scale space; gradient; singular value
   decomposition; entropy; support vector regression (SVR)
ID QUALITY ASSESSMENT; BLUR
AB The human visual system exhibits multiscale characteristic when perceiving visual scenes. The hierarchical structures of an image are contained in its scale space representation, in which the image can be portrayed by a series of increasingly smoothed images. Inspired by this, this paper presents a no-reference and robust image sharpness evaluation (RISE) method by learning multiscale features extracted in both the spatial and spectral domains. For an image, the scale space is first built. Then sharpness-aware features are extracted in gradient domain and singular value decomposition domain, respectively. In order to take into account the impact of viewing distance on image quality, the input image is also down-sampled by several times, and the DCT-domain entropies are calculated as quality features. Finally, all features are utilized to learn a support vector regression model for sharpness prediction. Extensive experiments are conducted on four synthetically and two real blurred image databases. The experimental results demonstrate that the proposed RISE metric is superior to the relevant state-of-the-art methods for evaluating both synthetic and real blurring. Furthermore, the proposedmetric is robust, which means that it has very good generalization ability.
C1 [Li, Leida; Xia, Wenhan] China Univ Min & Technol, Sch Informat & Elect Engn, Xuzhou 221116, Peoples R China.
   [Lin, Weisi; Wang, Shiqi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Jiangxi, Peoples R China.
C3 China University of Mining & Technology; Nanyang Technological
   University; Jiangxi University of Finance & Economics
RP Fang, YM (corresponding author), Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Jiangxi, Peoples R China.
EM reader1104@hotmail.com; 969872673@qq.com; wslin@ntu.edu.sg;
   fa0001ng@e.ntu.edu.sg; sqwang1986@gmail.com
RI Li, Li/AEM-3636-2022; Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011;
   li, li/HII-4157-2022
OI Lin, Weisi/0000-0001-9866-1947; 
FU National Natural Science Foundation of China [61379143]; Fundamental
   Research Funds for the Central Universities [2015XKMS032]; Qing Lan
   Project
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61379143, in part by the Fundamental
   Research Funds for the Central Universities under Grant 2015XKMS032, and
   in part by the Qing Lan Project. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Lingfen Sun. (Corresponding author: Yuming Fang.)
CR [Anonymous], 2013, International Scholarly Research Notices., DOI DOI 10.1155/2013/905685
   [Anonymous], 2003, Final report from the video quality experts group on the validation of objective models of video quality assessment
   Bahrami K, 2014, IEEE SIGNAL PROC LET, V21, P751, DOI 10.1109/LSP.2014.2314487
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Gu K, 2016, IEEE T BROADCAST, V62, P446, DOI 10.1109/TBC.2015.2511624
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gu K, 2015, IEEE T BROADCAST, V61, P520, DOI 10.1109/TBC.2015.2459851
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Hameed A, 2016, IEEE T MULTIMEDIA, V18, P764, DOI 10.1109/TMM.2016.2525862
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   Jang S, 2016, IEEE T MULTIMEDIA, V18, P1808, DOI 10.1109/TMM.2016.2581582
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   Li LD, 2016, CHINA COMMUN, V13, P121, DOI 10.1109/CC.2016.7582304
   Li LD, 2016, IEEE T IMAGE PROCESS, V25, P3775, DOI 10.1109/TIP.2016.2577891
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Lia LD, 2016, SIGNAL PROCESS-IMAGE, V48, P81, DOI 10.1016/j.image.2016.09.005
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Narwaria M, 2012, IEEE T SYST MAN CY B, V42, P347, DOI 10.1109/TSMCB.2011.2163391
   Nercessian SC, 2013, IEEE T IMAGE PROCESS, V22, P3549, DOI 10.1109/TIP.2013.2262287
   Pan ZQ, 2016, IEEE T BROADCAST, V62, P675, DOI 10.1109/TBC.2016.2580920
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Qi F, 2015, IEEE T MULTIMEDIA, V17, P2338, DOI 10.1109/TMM.2015.2493781
   Romeny B. M. ter Haar, 2003, COMP IMAG VIS, P1
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sang QB, 2014, J VIS COMMUN IMAGE R, V25, P1625, DOI 10.1016/j.jvcir.2014.08.002
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Vu CT, 2012, IEEE T IMAGE PROCESS, V21, P934, DOI 10.1109/TIP.2011.2169974
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang SQ, 2017, IEEE T MULTIMEDIA, V19, P660, DOI 10.1109/TMM.2016.2625276
   Wang SQ, 2016, IEEE T MULTIMEDIA, V18, P219, DOI 10.1109/TMM.2015.2510326
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1700, DOI 10.1109/TMM.2013.2266093
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Zhou WJ, 2016, IEEE T MULTIMEDIA, V18, P1077, DOI 10.1109/TMM.2016.2542580
   Zhou ZL, 2017, IEEE T INF FOREN SEC, V12, P48, DOI 10.1109/TIFS.2016.2601065
NR 51
TC 110
Z9 112
U1 1
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 1030
EP 1040
DI 10.1109/TMM.2016.2640762
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000011
DA 2024-07-18
ER

PT J
AU Lin, YT
   Wang, CM
   Chen, WS
   Lin, FP
   Lin, WI
AF Lin, Yun-Te
   Wang, Chung-Ming
   Chen, Wei-Sung
   Lin, Fang-Pang
   Lin, Woei
TI A Novel Data Hiding Algorithm for High Dynamic Range Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive; data hiding; high dynamic range (HDR) images; OpenEXR; optimal
   base; visual difference predictor
ID STEGANOGRAPHY; WATERMARKING
AB In this paper, we propose a novel data hiding algorithm for high dynamic range (HDR) images encoded by the OpenEXR file format. The proposed algorithm exploits each of three 10-bit mantissa fields as an embedding unit in order to conceal k bits of a secret message using an optimal base which produces the least pixel variation. An aggressive bit encoding and decomposition scheme is recommended, which offers a high probability to convey (k + 1) bits without increasing the pixel variation caused bymessage concealment. In addition, we present a bit inversion embedding strategy to further increase the capacities when the probability of appearance of secret bit "1" is greater than 0.5. Furthermore, we introduce an adaptive data hiding approach for concealing more secret messages in pixels with low luminance, exploiting the features of the human visual system to achieve luminance-aware adaptive data hiding. The stego HDR images produced by our algorithm coincide with the HDR image file format, causing no suspicion from malicious eavesdroppers. The generated stego HDR images and their tone-mapped low dynamic range (LDR) images reveal no perceptual differences when subjected to quantitative testing by visual difference predictor. Our algorithm can resist steganalytic attacks from the HDR and LDR RS and SPAM steganalyzers. We present the first data hiding algorithm for OpenEXR HDR images offering a high embedding rate and producing high visual quality of the stego images. Our algorithm outperforms the current state-of-the-art works.
C1 [Lin, Yun-Te; Wang, Chung-Ming; Chen, Wei-Sung; Lin, Woei] Natl Chung Hsing Univ, Dept Comp Sci & Engn, Taichung 40227, Taiwan.
   [Lin, Yun-Te] Natl Ctr High Performance Comp, NARLabs, Taichung 40763, Taiwan.
   [Lin, Fang-Pang] Natl Ctr High Performance Comp, NARLabs, Hsinchu 30076, Taiwan.
C3 National Chung Hsing University
RP Lin, YT (corresponding author), Natl Chung Hsing Univ, Dept Comp Sci & Engn, Taichung 40227, Taiwan.; Lin, YT (corresponding author), Natl Ctr High Performance Comp, NARLabs, Taichung 40763, Taiwan.
EM lsi@cs.nchu.edu.tw; cmwang@cs.nchu.edu.tw; phd9804@cs.nchu.edu.tw;
   fplin@nchc.narl.org.tw; wlin@cs.nchu.edu.tw
RI Chen, Wei-Sung/R-2122-2017
OI Chen, Wei-Sung/0000-0003-1314-4487
FU Taiwan Information Security Center, Academia Sinica, Taipei, Taiwan,
   R.O.C.;  [MOST 104-2221-E-005-026-MY3];  [MOST 104-2218-E-001-002]
FX This work was supported in part by the Minister of Science and
   Technology, Taiwan, under Grant MOST 104-2221-E-005-026-MY3 and Grant
   MOST 104-2218-E-001-002. The work of W.-S. Chen and C.-M. Wang was
   supported in part by Taiwan Information Security Center, Academia
   Sinica, Taipei, Taiwan, R.O.C. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Alessandro Piva.
CR Akyuz A. O., 2006, J ELECTRON IMAGING, V13, P1
   [Anonymous], 2016, LUMINANCE HDR OPEN S
   [Anonymous], INTRO THEORY STAT 19
   Artz D, 2001, IEEE INTERNET COMPUT, V5, P75, DOI 10.1109/4236.935180
   Autrusseau F, 2013, IEEE IMAGE PROC, P4527, DOI 10.1109/ICIP.2013.6738932
   Banterle F, 2011, ADVANCED HIGH DYNAMIC RANGE IMAGING: THEORY AND PRACTICE, P1
   Bianchi T, 2013, IEEE SIGNAL PROC MAG, V30, P87, DOI 10.1109/MSP.2012.2228342
   Chan CK, 2004, PATTERN RECOGN, V37, P469, DOI 10.1016/j.patcog.2003.08.007
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang CC, 2016, MULTIMED TOOLS APPL, V75, P145, DOI 10.1007/s11042-014-2279-5
   Cheng YM, 2009, IEEE MULTIMEDIA, V16, P70, DOI 10.1109/MMUL.2009.43
   Chin-Chen Chang, 2013, Journal of Electronic Science and Technology, V11, P20, DOI 10.3969/j.issn.1674-862X.2013.01.005
   Eilertsen G, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818092
   Fernando R., 2004, GPU Gems: Programming Techniques, Tips and Tricks for Real-Time Graphics
   Fridrich J., 2001, IEEE Multimedia, V8, P22, DOI 10.1109/93.959097
   Guerrini F, 2011, IEEE T INF FOREN SEC, V6, P283, DOI 10.1109/TIFS.2011.2109383
   Hanhart P, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0091-4
   Hoefflinger B., 2007, HIGH DYNAMIC RANGE H, P181
   Industrial Light & Magic San Fransisco CA USA, 2015, OPENEXR
   Larson G. W., 1998, Journal of Graphics Tools, V3, P15, DOI 10.1080/10867651.1998.10487485
   Li MT, 2011, INT J INNOV COMPUT I, V7, P2021
   Maiorana E, 2016, SECUR COMMUN NETW, V9, P705, DOI 10.1002/sec.1345
   Maiorana E, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0100-7
   Maiorana E, 2013, INT SYMP IMAGE SIG, P284
   Mantiuk R, 2004, IEEE SYS MAN CYBERN, P2763
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Narwaria M, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.1.010501
   Pevny T, 2009, MM&SEC'09: PROCEEDINGS OF THE 2009 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P75
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Reinhard E., 2002, Journal of Graphics Tools, V7, P45, DOI 10.1080/10867651.2002.10487554
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Reinhard E., 2010, HIGH DYNAMIC RANGE I, P103
   RODGERS JL, 1988, AM STAT, V42, P59, DOI 10.2307/2685263
   Solachidis V., 2013, INT C DIG SIGN PROC, P1
   Subhedar MS, 2014, COMPUT SCI REV, V13-14, P95, DOI 10.1016/j.cosrev.2014.09.001
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Ward G. J., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P459, DOI 10.1145/192161.192286
   Wu J.L., 2012, WATERMARKING, V2, P229
   Xue XW, 2011, IEICE T FUND ELECTR, VE94A, P2334, DOI 10.1587/transfun.E94.A.2334
   Yu CM, 2011, DISPLAYS, V32, P225, DOI 10.1016/j.displa.2011.02.004
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang Lin., 2014, VSI: A Visual Saliency-Induced Index for Perceptual Image Quality Assessment". In
   Zhi-Hui Wang, 2012, 2012 4th International Conference on Digital Home (ICDH 2012), P33, DOI 10.1109/ICDH.2012.49
NR 45
TC 48
Z9 51
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 196
EP 211
DI 10.1109/TMM.2016.2605499
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200016
DA 2024-07-18
ER

PT J
AU Gao, GY
   Liu, CH
   Chen, M
   Guo, S
   Leung, KK
AF Gao, Guangyu
   Liu, Chi Harold
   Chen, Min
   Guo, Song
   Leung, Kin K.
TI Cloud-Based Actor Identification With Batch-Orthogonal Local-Sensitive
   Hashing and Sparse Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Actor identification; cloud computing; locality-sensitive hashing; shot
   boundary detection; sparse representation
ID ENERGY-EFFICIENCY; INTERNET; LOCALIZATION; RECOGNITION; SYSTEM
AB Recognizing and retrieving multimedia content with movie/TV series actors, especially querying actor-specific videos in large scale video datasets, has attracted much attention in both the video processing and computer vision research field. However, many existing methods have low efficiency both in training and testing processes and also a less than satisfactory performance. Considering these challenges, in this paper, we propose an efficient cloud-based actor identification approach with batch-orthogonal local-sensitive hashing (BOLSH) and multi-task joint sparse representation classification. Our approach is featured by the following: 1) videos from movie/TV series are segmented into shots with the cloud-based shot boundary detection; 2) while faces in each shot are detected and tracked, the cloud-based BOLSH is then implemented on these faces for feature description; 3) the sparse representation is then adopted for actor identification in each shot; and 4) finally, a simple application, actor-specific shots retrieval is realized to verify our approach. We conduct extensive experiments and empirical evaluations on a large scale dataset, to demonstrate the satisfying performance of our approach considering both accuracy and efficiency.
C1 [Gao, Guangyu; Liu, Chi Harold] Beijing Inst Technol, Sch Software, Beijing 10081, Peoples R China.
   [Liu, Chi Harold] Sejong Univ, Dept Comp Informat & Secur, Seoul 143747, South Korea.
   [Chen, Min] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
   [Guo, Song] Univ Aizu, Sch Comp Sci & Engn, Fukushima 9658580, Japan.
   [Leung, Kin K.] Imperial Coll, EEE Dept, London SW7 2BT, England.
   [Leung, Kin K.] Imperial Coll, Dept Comp, London SW7 2BT, England.
C3 Beijing Institute of Technology; Sejong University; Huazhong University
   of Science & Technology; University of Aizu; Imperial College London;
   Imperial College London
RP Liu, CH (corresponding author), Beijing Inst Technol, Sch Software, Beijing 10081, Peoples R China.
EM guangyugao@bit.edu.cn; chiliu@bit.edu.cn; minchen2012@hust.edu.cn;
   sguo@u-aizu.ac.jp; kin.leung@imperial.ac.uk
RI Liu, Chi Harold/Z-4017-2019; Guo, Song/AAZ-4542-2020; Chen,
   Min/N-9350-2015
OI Liu, Chi Harold/0000-0002-0252-329X; Guo, Song/0000-0001-9831-2202;
   Chen, Min/0000-0002-0960-4447
FU National Natural Science Foundation of China [61572220, 61401023,
   61300179]; Fundamental Research Funds for the Central Universities
   [HUST: 2016YXMS070]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61572220, Grant 61401023, and Grant 61300179, and also
   by the Fundamental Research Funds for the Central Universities under
   HUST: 2016YXMS070. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Honggang Wang.
   (Corresponding author: Chi Harold Liu.)
CR [Anonymous], IEEE SYST J IN PRESS
   [Anonymous], IEEE T CLOU IN PRESS
   Bäuml M, 2013, PROC CVPR IEEE, P3602, DOI 10.1109/CVPR.2013.462
   Bojanowski P, 2013, IEEE I CONF COMP VIS, P2280, DOI 10.1109/ICCV.2013.283
   Chen SSB, 2001, SIAM REV, V43, P129, DOI [10.1137/S003614450037906X, 10.1137/S1064827596304010]
   Everingham M., 2006, BMVC, V2, P6
   Gao GY, 2014, MULTIMED TOOLS APPL, V71, P1749, DOI 10.1007/s11042-012-1301-z
   Gao H, 2015, IEEE COMMUN SURV TUT, V17, P918, DOI 10.1109/COMST.2014.2387836
   Gao Y, 2011, PROC INT CONF DOC, P885, DOI 10.1109/ICDAR.2011.181
   Ge XH, 2014, IEEE T VEH TECHNOL, V63, P2127, DOI 10.1109/TVT.2014.2310773
   Gkelias A, 2008, 2008 3RD INTERNATIONAL SYMPOSIUM ON WIRELESS PERVASIVE COMPUTING, VOLS 1-2, P46, DOI 10.1109/ISWPC.2008.4556163
   Hossain MS, 2016, MOBILE NETW APPL, V21, P753, DOI 10.1007/s11036-016-0685-9
   Hossain MS, 2016, COMPUT NETW, V101, P192, DOI 10.1016/j.comnet.2016.01.009
   Hossain MS, 2015, MOBILE NETW APPL, V20, P391, DOI 10.1007/s11036-015-0586-3
   Huang J., 2016, IEEE T CIRC IN PRESS
   Ji JQ, 2014, IEEE T PATTERN ANAL, V36, P1963, DOI 10.1109/TPAMI.2014.2315806
   Jitao Sang, 2012, Journal of Multimedia, V7, P9, DOI 10.4304/jmm.7.1.9-20
   Lai CF, 2013, IEEE T MULTIMEDIA, V15, P747, DOI 10.1109/TMM.2013.2240270
   Lin K., IEEE T CIRC IN PRESS
   Liu Chi Harold, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P241, DOI 10.1109/INFCOMW.2011.5928816
   Liu C. H., 2010, Proceedings of the Conference on Wireless Communications and Networking, P1
   Liu C. H., 2009, P SPIE DEF SEC SENS
   Liu CZ, 2011, PROC SPIE, V8201, DOI [10.1117/12.904710, 10.1109/SAHCN.2011.5984882]
   Liu CH, 2015, IEEE T VEH TECHNOL, V64, P4684, DOI 10.1109/TVT.2014.2367029
   Liu CH, 2014, IEEE T EMERG TOP COM, V2, P473, DOI 10.1109/TETC.2014.2364915
   Liu CH, 2014, AD HOC NETW, V18, P85, DOI 10.1016/j.adhoc.2013.02.008
   Liu CH, 2014, IEEE T WIREL COMMUN, V13, P604, DOI 10.1109/TWC.2013.010214.121856
   Liu LA, 2011, IEEE T WIREL COMMUN, V10, P484, DOI 10.1109/TWC.2010.01.080956
   Liu LA, 2010, IEEE T VEH TECHNOL, V59, P3562, DOI 10.1109/TVT.2009.2031454
   Ma H., IEEE INTERN IN PRESS
   Ma HD, 2011, J COMPUT SCI TECH-CH, V26, P919, DOI 10.1007/s11390-011-1189-5
   Ma HD, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2089094.2089107
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P586, DOI 10.1109/TMM.2012.2188784
   Sheng ZG, 2015, IEEE T VEH TECHNOL, V64, P1156, DOI 10.1109/TVT.2014.2322653
   Sivic J, 2009, PROC CVPR IEEE, P1145, DOI 10.1109/CVPRW.2009.5206513
   Song Z, 2014, IEEE INT CONF SENS, P248, DOI 10.1109/SAHCN.2014.6990360
   Suzuki T., 2013, Patterns of distribution and abundance of the stalked barnacle in the central and southwest coast of continal Portugal, P1
   Tapaswi M, 2012, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2012.6247986
   Wang H, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0104023
   Wang S, 2011, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2011.6126385
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xiong C, 2014, IEEE T MULTIMEDIA, V16, P1473, DOI 10.1109/TMM.2014.2316475
   Yu L., P IEEE INT IN PRESS
   Yuan XT, 2010, PROC CVPR IEEE, P3493, DOI 10.1109/CVPR.2010.5539967
   Yurur O, 2013, ELECTRON LETT, V49, DOI 10.1049/el.2013.0592
   Zaharia M., 2012, 9 USENIX S NETWORKED
   Zhang B, 2016, COMPUT NETW, V101, P29, DOI 10.1016/j.comnet.2015.12.022
   Zhang B, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2630074
   Zhang X, 2012, IEEE T MULTIMEDIA, V14, P995, DOI 10.1109/TMM.2012.2186121
   Zhang Y, 2015, MOBILE NETW APPL, V20, P348, DOI 10.1007/s11036-014-0537-4
   Zhu LZ, 2014, 2014 INTERNATIONAL CONFERENCE ON IDENTIFICATION, INFORMATION AND KNOWLEDGE IN THE INTERNET OF THINGS (IIKI 2014), P68, DOI 10.1109/IIKI.2014.21
NR 51
TC 4
Z9 4
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1749
EP 1761
DI 10.1109/TMM.2016.2579305
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800006
DA 2024-07-18
ER

PT J
AU Chatzilari, E
   Nikolopoulos, S
   Kompatsiaris, Y
   Kittler, J
AF Chatzilari, Elisavet
   Nikolopoulos, Spiros
   Kompatsiaris, Yiannis
   Kittler, Josef
TI SALIC: Social Active Learning for Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active learning; image classification; large scale; multi-modal fusion;
   social context; user tagged images
AB In this paper, we present SALIC, an active learning method for selecting the most appropriate user tagged images to expand the training set of a binary classifier. The process of active learning can be fully automated in this social context by replacing the human oraclewith the images' tags. However, their noisy nature adds further complexity to the sample selection process since, apart from the images' informativeness (i.e., how much they are expected to inform the classifier if we knew their label), our confidence about their actual label should also be maximized (i.e., how certain the oracle is on the images' true contents). The main contribution of this work is in proposing a probabilistic approach for jointly maximizing the two aforementioned quantities. In the examined noisy context, the oracle's confidence is necessary to provide a contextual-based indication of the images' true contents, while the samples' informativeness is required to reduce the computational complexity and minimize the mistakes of the unreliable oracle. To prove this, first, we show that SALIC allows us to select training data as effectively as typical active learning, without the cost of manual annotation. Finally, we argue that the speed-up achieved when learning actively in this social context (where labels can be obtained without the cost of human annotation) is necessary to cope with the continuously growing requirements of large-scale applications. In this respect, we demonstrate that SALIC requires ten times less training data in order to reach the same performance as a straightforward informativeness-agnostic learning approach.
C1 [Chatzilari, Elisavet; Nikolopoulos, Spiros; Kompatsiaris, Yiannis] Ctr Res & Technol Hellas, Inst Informat Technol, Thessaloniki 57001, Greece.
   [Kittler, Josef] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 Centre for Research & Technology Hellas; University of Surrey
RP Chatzilari, E (corresponding author), Ctr Res & Technol Hellas, Inst Informat Technol, Thessaloniki 57001, Greece.
EM ehatzi@iti.gr; nikolopo@iti.gr; ikom@iti.gr; j.kittler@surrey.ac.uk
RI Kompatsiaris, Ioannis/P-8594-2015
OI Kompatsiaris, Ioannis/0000-0001-6447-9020; Nikolopoulos,
   Spiros/0000-0002-1367-5133; Chatzilari, Elisavet/0000-0001-5775-1356
FU European Community [FP7-ICT-600676, FP7-601138]
FX This work was supported by the European Community's Seventh Framework
   Programme (FP7) under Grant FP7-ICT-600676 "i-Treasures: Intangible
   Treasures-Capturing the Intangible Cultural Heritage and Learning the
   Rare Know-How of Living Human Treasures" and Grant FP7-601138 "PERICLES
   Digital Preservation." The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Tao Mei.
CR [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], P ACM INT C IM VID R
   [Anonymous], 2009, Technical report
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   Bart T., 2012, WORK NOT CLEF 2012 L
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chatzilari E., 2014, 9 INT C COMP VIS THE
   COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1023/A:1022673506211
   Cui P, 2016, IEEE MULTIMEDIA, V23, P92, DOI 10.1109/MMUL.2016.8
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ebert S, 2012, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2012.6248108
   Fang M, 2014, AAAI CONF ARTIF INTE, P1809
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Freytag A, 2014, LECT NOTES COMPUT SC, V8692, P562, DOI 10.1007/978-3-319-10593-2_37
   Fu JL, 2015, IEEE I CONF COMP VIS, P1985, DOI 10.1109/ICCV.2015.230
   Golge E, 2014, LECT NOTES COMPUT SC, V8695, P439, DOI 10.1007/978-3-319-10584-0_29
   Hoi SCH, 2005, PROC CVPR IEEE, P302
   Huiskes M., 2010, Proceedings of the international conference on Multimedia information retrieval
   Izadinia H, 2015, MMCOMMONS'15: PROCEEDINGS OF THE 2015 WORKSHOP ON COMMUNITY-ORGANIZED MULTIMODAL MINING: OPPORTUNITIES FOR NOVEL SOLUTIONS, P13, DOI 10.1145/2814815.2814821
   Joachims Thorsten, 1998, EUROPEAN C MACHINE L, P137, DOI 10.1007/
   Kordumova S, 2015, MULTIMED TOOLS APPL, V74, P1291, DOI 10.1007/s11042-014-2056-5
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li W, 2014, LECT NOTES COMPUT SC, V8693, P437, DOI 10.1007/978-3-319-10602-1_29
   Li X., 2013, MM 2013 P 2013 ACM M, P485, DOI DOI 10.1145/2502081.2502129
   Li X, 2013, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2013.116
   Li XR, 2013, IEEE T MULTIMEDIA, V15, P933, DOI 10.1109/TMM.2013.2238523
   Liu SW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P109, DOI 10.1145/2733373.2806247
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Ng V, 2003, PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P113
   Nowak S., 2010, P INT C MULT INF RET, P557, DOI [10.1145/1743384.1743478, DOI 10.1145/1743384.1743478]
   Papadopoulou O, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P531, DOI 10.1145/2671188.2749338
   Platt JC, 2000, ADV NEUR IN, P61
   Rodrigues F, 2014, PR MACH LEARN RES, V32, P433
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Vijayanarasimhan S, 2014, INT J COMPUT VISION, V108, P97, DOI 10.1007/s11263-014-0721-9
   Williams C.K.I., PASCAL VISUAL OBJECT
   Yan Y., 2011, P 28 INT C MACH LEAR, V11, P1161, DOI DOI 10.1161/CIRCULATIONAHA.105.170812
   Zhang L., 2011, P 1 ACM INT C MULT R, V46-1-46-8
NR 40
TC 3
Z9 3
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1488
EP 1503
DI 10.1109/TMM.2016.2565440
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hong, RC
   Zhang, LM
   Zhang, C
   Zimmermann, R
AF Hong, Richang
   Zhang, Luming
   Zhang, Chao
   Zimmermann, Roger
TI Flickr Circles: Aesthetic Tendency Discovery by Multi-View Regularized
   Topic Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetic; Flickr circle; multi-view; tendency
AB Aesthetic tendency discovery is a useful and interesting application in social media. In this paper we propose to categorize large-scale Flickr users into multiple circles, each containing users with similar aesthetic interests (e.g., landscapes). We notice that: 1) an aesthetic model should be flexible as different visual features may be used to describe different image sets; 2) the numbers of photos from different Flickr users vary significantly and some users may have very few photos; and 3) visual features from each Flickr photo should be seamlessly integrated at both low-level and high-level. To meet these challenges, we propose to fuze color, textural, and semantic channel features using a multi-view learning framework, where the feature weights are adjusted automatically. Then, a regularized topic model is developed to quantify each user's aesthetic interest as a distribution in the latent space. Afterward, a graph is constructed to describe the discrepancy of aesthetic interests among users. Apparently, densely connected users are with similar aesthetic interests. Thus, an efficient dense subgraph mining algorithm is adopted to group Flickr users into multiple circles. Experiments have shown that our approach performs competitively on a million-scale image set crawled from Flickr. Besides, our method can enhance the transferal-based photo cropping [40] as reported by the user study.
C1 [Hong, Richang; Zhang, Luming] Hefei Univ Technol, Dept CSIE, Hefei 230009, Peoples R China.
   [Zhang, Chao] Univ Illinois, Dept Comp Sci, Champaign, IL 61801 USA.
   [Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 Hefei University of Technology; University of Illinois System;
   University of Illinois Urbana-Champaign; National University of
   Singapore
RP Hong, RC (corresponding author), Hefei Univ Technol, Dept CSIE, Hefei 230009, Peoples R China.
EM hongrc.hfut@gmail.com; zglumg@gmail.com; chaozhangzju@gmail.com;
   rogerz@comp.nus.edu.sg
RI Lei, Ming/JAD-1050-2023; Zhang, Chao/AAR-7251-2020; Zimmermann,
   Roger/D-7944-2015; zhang, lu/GRO-2969-2022
OI Zhang, Chao/0000-0003-3009-598X; Zimmermann, Roger/0000-0002-7410-2590; 
FU National Basic Research Program [2013CB336500]; National High-Tech
   Development Program [2014AA015104]; Fundamental Research Funds for the
   Central Universities; Program for New Century Excellent Talents
   [NCET-13-0764]; National Nature Science Foundation of China [61472116,
   61572169]; Anhui Fund for Distinguished Young Scholars [1508085J04];
   National University of Singapore (Suzhou) Research Institute, Suzhou
   Industrial Park, Jiang Su, China
FX This work was supported in part by the National Basic Research Program
   under Grant 2013CB336500, in part by the National High-Tech Development
   Program under Grant 2014AA015104, in part by the Fundamental Research
   Funds for the Central Universities, in part by the Program for New
   Century Excellent Talents under Grant NCET-13-0764, in part by the
   Project from National Nature Science Foundation of China under Grant
   61472116 and Grant 61572169, in part by the Anhui Fund for Distinguished
   Young Scholars under Grant 1508085J04, and in part by the National
   University of Singapore (Suzhou) Research Institute, Suzhou Industrial
   Park, Jiang Su, China. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Chengcui Zhang.
CR Ahn YY, 2010, NATURE, V466, P761, DOI 10.1038/nature09182
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2006, P 15 INT C WORLD WID
   [Anonymous], 2010, ACM MULTIMEDIA
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chen X., 2011, SIAM INT C DAT MIN M
   Chen Y. W., 2005, Feature Extraction
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Costa G, 2012, 2012 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM), P194, DOI 10.1109/ASONAM.2012.42
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Datta R., 2006, 9 EUR C COMP VIS GRA
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Fengzhang Luo, 2015, 2015 IEEE Power & Energy Society General Meeting, P1, DOI 10.1109/PESGM.2015.7285736
   Frank M, 2012, J MACH LEARN RES, V13, P459
   He X., 2003, ADV NEURAL INFORM PR, P153
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P1124, DOI 10.1109/TIP.2016.2514499
   JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588
   Li X, 2014, IEEE T KNOWL DATA EN, V26, P2588, DOI 10.1109/TKDE.2013.126
   Li YX, 2012, IEEE T MULTIMEDIA, V14, P1618, DOI 10.1109/TMM.2012.2199292
   Liu H., 2010, Proceedings of the 27th International Conference on Machine Learning (ICML-10), P671
   Luo W., 2011, INT C COMP VIS BARC
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1662, DOI 10.1109/TMM.2012.2199293
   Ni BB, 2013, IEEE T MULTIMEDIA, V15, P1138, DOI 10.1109/TMM.2013.2241042
   Palla G, 2005, NATURE, V435, P814, DOI 10.1038/nature03607
   Powers DMW, 2020, J MACH LEARN TECHNOL, P37, DOI DOI 10.9735/2229-3981
   Press W. H., 2007, NUM REC ART SCI COMP
   Wang M., 2007, ACM Multi- media, P862
   Wong L.-K., 2009, 16 IEEE INT C IM PRO
   Yang YR, 2012, ELECTRON J PROBAB, V17, P1, DOI 10.1214/EJP.v17-2239
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yao E., 2014, SPEC INT GROUP INF R
   Yin ZJ, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2337542.2337548
   Yoshida T., 2010, IEEE INT C DAT MIN W
   Zhang H., 2012, IEEE 11 INT C MACH L
   Zhang LM, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P237, DOI 10.1145/2647868.2654903
   Zhang LM, 2015, IEEE T MULTIMEDIA, V17, P1538, DOI 10.1109/TMM.2015.2451954
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
NR 40
TC 47
Z9 47
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1555
EP 1567
DI 10.1109/TMM.2016.2567071
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000009
DA 2024-07-18
ER

PT J
AU Lei, CY
   Liu, D
   Li, WP
AF Lei, Chenyi
   Liu, Dong
   Li, Weiping
TI Social Diffusion Analysis With Common-Interest Model for Image
   Annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Common-interest; image annotation; learning to rank; social diffusion
   analysis; social media
AB Automatic image annotation has been extensively studied, mostly from a content-based approach, whose effectiveness is restricted by the "semantic gap" between low-level image features and semantic annotations, and by the irrelevance of annotations to image content. We propose a social diffusion analysis approach to image annotation, which exploits abundant social diffusion records about how images are disseminated within online social networks. Specifically, we propose a common-interest model to analyze social diffusion records, with the assumption that the diffusion pattern of an image in social networks is highly related to the relevance between image annotations and user preferences. In our proposed model, user preferences are represented as common interests of pairwise users instead of individual user interests. We find the notion of common interests not only facilitates the analysis of social diffusion patterns, but also leads to more accurate profiling of user preferences compared to individual interests. Based on the common-interest model, we design an image annotation framework via social diffusion analysis, which consists of the mining of common interests from social diffusion records, the feature extraction from diffusion graphs and common interests, and the automatic annotation by the learning-to-rank method. Experimental results on real-world data sets show that our proposed common-interest based approach outperforms individual-interest based methods, and also achieves superior performance than state-of-the-art content-based image annotation methods.
C1 [Lei, Chenyi; Liu, Dong; Li, Weiping] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Lei, CY; Liu, D; Li, WP (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM leichy@mail.ustc.edu.cn; dongeliu@ustc.edu.cn; wpli@ustc.edu.cn
RI Liu, Dong/K-7488-2012
OI Liu, Dong/0000-0001-9100-2906
FU National Program on Key Basic Research Projects (973 Program)
   [2015CB351800]; Natural Science Foundation of China (NSFC) [61303149,
   61331017, 61390512]; Fundamental Research Funds for Central Universities
   [WK2100060011, WK3490000001]
FX This work was supported by the National Program on Key Basic Research
   Projects (973 Program) under Grant 2015CB351800, by the Natural Science
   Foundation of China (NSFC) under Grant 61303149, Grant 61331017, and
   Grant 61390512, and by the Fundamental Research Funds for the Central
   Universities under Grant WK2100060011 and Grant WK3490000001. This paper
   was presented at the 2014 IEEE International Workshop on Frontier of
   Crowdsourcing in Multimedia Computing, Chengdu, China, July 2014. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Martha Larson.
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], P ACM INT C IM VID R
   [Anonymous], P INT C MULT INF RET
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2009, Proceedings of the 18th international conference on World wide web, DOI [10.1145/1526709.1526758, DOI 10.1145/1526709.1526758]
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], SCI CHINA INFORM SCI
   Bregman L. M., 1967, USSR COMP MATH MATH, V7, P200, DOI [10.1016/0041-5553(67)90040-7, DOI 10.1016/0041-5553(67)90040-7]
   Burges C., 2005, ICML, P89
   Cai Y, 2014, IEEE T KNOWL DATA EN, V26, P766, DOI 10.1109/TKDE.2013.7
   Cao Z., 2007, P 24 INT C MACHINE L, P129, DOI DOI 10.1145/1273496.1273513
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Crandall DavidJ., 2008, KDD, P160, DOI DOI 10.1145/1401890.1401914
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Dong J, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2501643.2501646
   Fan Jianping., 2004, ACM International Conference on Multimedia, P540, DOI [DOI 10.1145/1027527, DOI 10.1145/1027527.1027660]
   Fang Q, 2014, IEEE T MULTIMEDIA, V16, P796, DOI 10.1109/TMM.2014.2298216
   Feng ZY, 2013, IEEE I CONF COMP VIS, P1609, DOI 10.1109/ICCV.2013.203
   Gallagher A., 2008, P 16 ACM INT C MULTI, P681
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Hu WS, 2012, 2012 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE AND INTELLIGENT AGENT TECHNOLOGY (WI-IAT 2012), VOL 1, P724, DOI 10.1109/WI-IAT.2012.79
   Jayarathna S, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1849, DOI 10.1145/2505515.2507878
   Jun Xu, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P391
   Lewis K, 2012, P NATL ACAD SCI USA, V109, P68, DOI 10.1073/pnas.1109739109
   Li X., 2015, CORR
   Li XH, 2008, IEEE T SIGNAL PROCES, V56, P675, DOI 10.1109/TSP.2007.907820
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Li Xirong., 2011, Proceedings of the 19th ACM International Conference on Multimedia, MM '11, P233
   Lin N, 2012, J AM SOC INF SCI TEC, V63, P139, DOI 10.1002/asi.21628
   Liu Dong., 2010, Proceedings of the International Conference on Multimedia, P491
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Makadia A, 2010, INT J COMPUT VISION, V90, P88, DOI 10.1007/s11263-010-0338-6
   McAuley J, 2012, LECT NOTES COMPUT SC, V7575, P828, DOI 10.1007/978-3-642-33765-9_59
   Nguyen C, 2013, P 23 INT JOINT C ART, P1558
   Piwowarski B., 2007, P 16 ACM C CONFERENC, P175, DOI [10.1145/1321440.1321467, DOI 10.1145/1321440.1321467, 10]
   Sang JT, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2502436
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Shepitsen A, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P259
   Sun YY, 2010, AAAI CONF ARTIF INTE, P593
   Tang J, 2011, MACH LEARN, V82, P211, DOI 10.1007/s10994-010-5212-9
   Tang J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P807
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Tingting Wang, 2013, Advances in Knowledge Discovery and Data Mining. 17th Pacific-Asia Conference (PAKDD 2013). Proceedings, P85, DOI 10.1007/978-3-642-37456-2_8
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang XH, 2015, IEEE T AFFECT COMPUT, V6, P286, DOI 10.1109/TAFFC.2015.2400917
   Wang XJ, 2008, IEEE T PATTERN ANAL, V30, P1919, DOI 10.1109/TPAMI.2008.127
   Wang ZL, 2013, IEEE T IMAGE PROCESS, V22, P537, DOI 10.1109/TIP.2012.2218826
   Wen Z., 2010, P 16 ACM SIGKDD INT, P373
   White RW, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P363, DOI 10.1145/1571941.1572005
   Xu T, 2012, IEEE DATA MINING, P1158, DOI 10.1109/ICDM.2012.23
   Yang Y, 2011, PROC CVPR IEEE, P881, DOI 10.1109/CVPR.2011.5995499
   Yao Ting., 2013, Proceedings of the ACM International Conference on Multimedia, P977
   Zhang J, 2015, ACM T KNOWL DISCOV D, V9, DOI 10.1145/2700398
   Zhang Y, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1839490.1839495
   Zhou DH, 2015, SCI CHINA INFORM SCI, V58, DOI 10.1007/s11432-015-5288-8
NR 57
TC 17
Z9 17
U1 1
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 687
EP 701
DI 10.1109/TMM.2015.2477277
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300011
DA 2024-07-18
ER

PT J
AU Xu, L
   Lin, WS
   Ma, L
   Zhang, YB
   Fang, Y
   Ngan, KN
   Li, SN
   Yan, YH
AF Xu, Long
   Lin, Weisi
   Ma, Lin
   Zhang, Yongbing
   Fang, Yuming
   Ngan, King Ngi
   Li, Songnan
   Yan, Yihua
TI Free-Energy Principle Inspired Video Quality Metric and Its Use in Video
   Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Free-energy principle; perceptual video coding optimization; video
   coding; visual quality assessment (VQA)
ID DISTRIBUTIONS; OPTIMIZATION; SIMILARITY; BRAIN; ERROR
AB In this paper, we extend the free-energy principle to video quality assessment (VQA) by incorporating with the recent psychophysical study on human visual speed perception (HVSP). A novel video quality metric, namely the free-energy principle inspired video quality metric (FePVQ), is therefore developed and applied to perceptual video coding optimization. The free-energy principle suggests that the human visual system (HVS) can actively predict "orderly" information and avoid "disorderly" information for image perception. Basically, "orderly" is associated with the skeletons and edges of objects, and "disorderly" mostly concerns textures in images. Based on this principle, an image is separated into orderly and disorderly regions, and processed differently in image quality assessment. For videos, visual attention, or fixation, is associated with the objects with significant motion according to HVSP, resulting in a motion strength factor in the FePVQ so that the free-energy principle is extended into spatio-temporal domain for VQA. In addition, we investigate the application of the FePVQ in perceptual rate distortion optimization (RDO). For this purpose, the FePVQ is realized with low computational cost by using the relative total variation model and the block-wise motion vectors of video coding to simulate the free-energy principle and the HVSP, respectively. The experimental results indicate that the proposed FePVQ is highly consistent with the HVS perception. The linear correlation coefficient and Spearman's rank-order correlation coefficient are up to 0.8324 and 0.8281 on the LIVE video database. Better perceptual quality of encoded video sequences is achieved by FePVQ-motivated RDO in video coding.
C1 [Xu, Long; Yan, Yihua] Chinese Acad Sci, Key Lab Solar Act, Natl Astron Observ, Beijing 100012, Peoples R China.
   [Xu, Long] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Jiangsu Engn Ctr Network Monitoring, Nanjing 210044, Jiangsu, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Dept Comp Engn, Singapore 639798, Singapore.
   [Ma, Lin] Huawei Noahs Ark Lab, Hong Kong 999077, Hong Kong, Peoples R China.
   [Zhang, Yongbing] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518005, Peoples R China.
   [Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
   [Ngan, King Ngi; Li, Songnan] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong 999077, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; National Astronomical Observatory, CAS;
   Nanjing University of Information Science & Technology; Nanyang
   Technological University; Huawei Technologies; Tsinghua Shenzhen
   International Graduate School; Tsinghua University; Jiangxi University
   of Finance & Economics; Chinese University of Hong Kong
RP Ma, L (corresponding author), Huawei Noahs Ark Lab, Hong Kong 999077, Hong Kong, Peoples R China.
EM lxu@nao.cas.cn; WSlin@ntu.edu.sg; forest.linma@gmail.com;
   zhang.yongbing@sz.tsinghua.edu.cn; fa0001ng@e.ntu.edu.sg;
   kn-ngan@ee.cuhk.edu.hk; snli@ee.cuhk.edu.hk; yyh@nao.cas.cn
RI Ngan, N/E-8240-2014; Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011;
   Yan, Yihua/AGY-9819-2022; Xu, Long/AAH-9908-2019
OI Ngan, N/0000-0003-1946-3235; Lin, Weisi/0000-0001-9866-1947; Yan,
   Yihua/0000-0002-7106-6029; Xu, Long/0000-0002-9286-2876
FU National Natural Science Foundation of China (NSFC) [61232016, U1405254,
   61572461, 61202242, 61571212, 11433006]; CAS "100-Talents"; NSFC
   Guangdong Joint Fund [U1201255, U1301257]; Research Grants Council of
   the Hong Kong SAR [CUHK 415712]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 61232016, Grant U1405254, Grant
   61572461, Grant 61202242, Grant 61571212, and Grant 11433006, in part by
   CAS "100-Talents" (Dr. X. Long), in part by the NSFC Guangdong Joint
   Fund under Grant U1201255 and Grant U1301257, and in part by the
   Research Grants Council of the Hong Kong SAR under Project CUHK 415712.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Klara Nahrstedt.
CR [Anonymous], 2000, FIN REP VID QUAL EXP
   [Anonymous], FFMPEG TOOL
   Bhat A, 2012, IEEE T CIRC SYST VID, V22, P165, DOI 10.1109/TCSVT.2011.2158465
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Gu B, 2015, IEEE T NEUR NET LEAR, V26, P1403, DOI 10.1109/TNNLS.2014.2342533
   Gu B, 2015, NEURAL NETWORKS, V67, P140, DOI 10.1016/j.neunet.2015.03.013
   Knill DC, 2004, TRENDS NEUROSCI, V27, P712, DOI 10.1016/j.tins.2004.10.007
   Lambrecht CJV, 1996, P SOC PHOTO-OPT INS, V2668, P450, DOI 10.1117/12.235440
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lu ZK, 2005, IEEE T IMAGE PROCESS, V14, P1928, DOI 10.1109/TIP.2005.854478
   Lubin Jeffrey, 1993, P163
   Masry M, 2006, IEEE T CIRC SYST VID, V16, P260, DOI 10.1109/TCSVT.2005.861946
   Moorthy AK, 2010, IEEE T CIRC SYST VID, V20, P1653, DOI 10.1109/TCSVT.2010.2087470
   Narwaria M, 2012, IEEE T MULTIMEDIA, V14, P525, DOI 10.1109/TMM.2012.2190589
   Newman MEJ, 2005, CONTEMP PHYS, V46, P323, DOI 10.1080/00107510500052444
   Ou YF, 2011, IEEE T CIRC SYST VID, V21, P286, DOI 10.1109/TCSVT.2010.2087833
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Seshadrinathan K, 2009, Live video quality database
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   Stocker AA, 2006, NAT NEUROSCI, V9, P578, DOI 10.1038/nn1669
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Vu P. V., 2014, J ELECTRON IMAGING, V23, P13
   Wang SQ, 2012, IEEE T CIRC SYST VID, V22, P516, DOI 10.1109/TCSVT.2011.2168269
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2003, IEEE T IMAGE PROCESS, V12, P243, DOI 10.1109/TIP.2003.809015
   Wang Z, 2007, J OPT SOC AM A, V24, pB61, DOI 10.1364/JOSAA.24.000B61
   Watson AB, 2001, J ELECTRON IMAGING, V10, P20, DOI 10.1117/1.1329896
   Winkler S, 1999, PROC SPIE, V3644, P175, DOI 10.1117/12.348438
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1705, DOI 10.1109/TMM.2013.2268053
   Xu L., 2012, ACM T GRAPHIC, V32
   Xu L, 2007, LECT NOTES COMPUT SC, V4810, P638
   Xu L, 2013, IEEE T CIRC SYST VID, V23, P975, DOI 10.1109/TCSVT.2013.2243657
   Xu L, 2011, IEEE T IMAGE PROCESS, V20, P723, DOI 10.1109/TIP.2010.2063708
   You J., 2011, Proceedings of the 19th ACM International Conference on Multimedia (ACM MM), P1293
   You JY, 2014, IEEE T IMAGE PROCESS, V23, P200, DOI 10.1109/TIP.2013.2287611
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang F., 2011, IVP VIDEO QUALITY DA
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang XD, 2013, IEEE SIGNAL PROC LET, V20, P319, DOI 10.1109/LSP.2013.2244081
NR 48
TC 34
Z9 35
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 590
EP 602
DI 10.1109/TMM.2016.2525004
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300004
DA 2024-07-18
ER

PT J
AU Gao, GY
   Zhang, WW
   Wen, YG
   Wang, Z
   Zhu, WW
AF Gao, Guanyu
   Zhang, Weiwen
   Wen, Yonggang
   Wang, Zhi
   Zhu, Wenwu
TI Towards Cost-Efficient Video Transcoding in Media Cloud: Insights
   Learned From User Viewing Patterns
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Media cloud; partial transcoding scheme; user viewing pattern; viewer
   behavior
ID SYSTEMS
AB Video transcoding in an adaptive bitrate streaming (ABR) system is demanded to support video streaming over heterogenous devices and varying networks. However, it could incur a tremendous cost. Meanwhile, most viewers terminate viewing sessions within 20% of their durations; only a small fraction of each video is consumed. Built upon this user viewing pattern, we propose a Partial Transcoding Scheme for content management in media clouds. Particularly, each content is encoded into different bitrates and split into segments. Some of the segments are stored in cache, resulting in storage cost; others are transcoded online in the case of cache miss, resulting in computing cost. We aim to minimize the long-term overall cost by determining whether a segment should be cached or transcoded online. We formulate it as a constrained stochastic optimization problem. Leveraging Lyapunov optimization framework and Lagrangian relaxation, we design an online algorithm which can achieve the optimal solution within provable upper bounds. Experiments demonstrate that our proposed method can reduce 30% of operational cost, compared with the scheme of caching all the segments.
C1 [Gao, Guanyu; Zhang, Weiwen; Wen, Yonggang] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Wang, Zhi] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
   [Zhu, Wenwu] Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
C3 Nanyang Technological University; Tsinghua University; Tsinghua Shenzhen
   International Graduate School; Tsinghua University
RP Gao, GY (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM ggao001@ntu.edu.sg; wzhang9@ntu.edu.sg; ygwen@ntu.edu.sg;
   wangzhi@sz.tsinghua.edu.cn; wwzhu@tsinghua.edu.cn
RI Gao, Guanyu/ACR-3456-2022; Wen, Yonggang/P-9406-2017; Wen,
   Yonggang/B-8848-2011
OI Wen, Yonggang/0000-0002-2751-5114; 
FU Singapore MOE [MOE Tier-1]; Singapore EMA [EIRP02]; Singapore National
   Research Foundation under its IDM Futures Funding Initiative; NSFC
   [61402247]
FX This work was supported in part by the Singapore MOE under Grant MOE
   Tier-1, and in part by the Singapore EMA under Grant EIRP02. The work of
   G. Gao and Y. Wen was supported by the Singapore National Research
   Foundation under its IDM Futures Funding Initiative and administered by
   the Interactive & Digital Media Programme Office, Media Development
   Authority. The work of Z. Wang was supported by the NSFC under Grant
   61402247. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Tommaso Melodia.
CR Ahlehagh H, 2013, 2013 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P1357
   Ahuja R. K., 1993, Network flows: theory, algorithms, and applications
   [Anonymous], 2014, 1H2014 SANDV
   [Anonymous], 2014, 2014 IEEE INT C MULT, DOI DOI 10.1109/ICME.2014.6890255
   [Anonymous], 2013, CISC VIS NETW IND FO
   Balachandran A., 2013, Proceedings of the 2013 conference on Internet measurement conference, P43
   Cha M, 2009, IEEE ACM T NETWORK, V17, P1357, DOI 10.1109/TNET.2008.2011358
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Finamore A., 2011, ACM IMC, P345, DOI DOI 10.1145/2068816.2068849
   Gao G., P 2015 IEEE IN PRESS
   Hu H, 2014, IEEE ACCESS, V2, P652, DOI 10.1109/ACCESS.2014.2332453
   Hu H, 2014, IEEE MULTIMEDIA, V21, P10, DOI 10.1109/MMUL.2014.2
   Huang ZX, 2011, IEEE INFOCOM SER, P201, DOI 10.1109/INFCOM.2011.5935009
   Johnsen FT, 2006, IEEE INT SYM MULTIM, P724
   Krishnan R. K., 2012, P INT MEAS C, P211
   Liu Y, 2012, IEEE INFOCOM SER, P1332, DOI 10.1109/INFCOM.2012.6195496
   Martello Silvano, 1990, Knapsack Problems: Algorithms and Computer Implementations
   Miranda LCO, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P1085
   Neely, 2010, STOCHASTIC NETWORK O
   Shen B, 2004, IEEE T MULTIMEDIA, V6, P375, DOI 10.1109/TMM.2003.822791
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Vetro A, 2002, IEEE IMAGE PROC, P29
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Whitelaw B, 2011, TELEGRAPH
   Yang J, 2011, IEEE T MULTIMEDIA, V13, P1141, DOI 10.1109/TMM.2011.2160158
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zink M, 2009, COMPUT NETW, V53, P501, DOI 10.1016/j.comnet.2008.09.022
NR 28
TC 71
Z9 74
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1286
EP 1296
DI 10.1109/TMM.2015.2438713
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000014
DA 2024-07-18
ER

PT J
AU Zhong, ZY
   Zhu, JK
   Hoi, SCH
AF Zhong, Zhiyuan
   Zhu, Jianke
   Hoi, Steven C. H.
TI Fast Object Retrieval Using Direct Spatial Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Images reranking; log tf-idf; object retrieval; spatial matching
ID IMAGE; SIMILARITY; GEOMETRY
AB The conventional bag-of-visual-words (BoW) model is popular for the large-scale object retrieval system but suffers from the critical drawback of ignoring spatial information. RANSAC-based methods attempt to remedy this drawback, but often require traversing all the feature matches for each hypothesis, leading to the heavy computational cost which limits the number of gallery images to be verified for each online query. We propose an efficient direct spatial matching (DSM) approach to directly estimate the scale variation using region sizes, in which all feature matches voted for estimating geometric transformation. DSM is much faster than RANSAC-based methods and exhaustive enumeration approaches. A logarithmic term frequency-inverse document frequency (log tf-idf) weighting scheme is introduced to boost the performance of the base system. We have conducted extensive experimental evaluations on four benchmark datasets for object retrieval. The proposed DSM method, together with a carefully-tailored reranking scheme, achieves the state-of-the-art results on the Oxford buildings and Paris datasets, which demonstrates the efficacy and scalability of our novel DSM technique for large scale object retrieval systems.
C1 [Zhong, Zhiyuan; Zhu, Jianke] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Zhejiang, Peoples R China.
   [Hoi, Steven C. H.] Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore.
C3 Zhejiang University; Singapore Management University
RP Zhu, JK (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Zhejiang, Peoples R China.
EM zyzhong@zju.edu.cn; jkzhu@zju.edu.cn; chhoi@smu.edu.sg
RI HOI, Steven C. H./A-3736-2011
OI Hoi, Steven/0000-0002-4584-3453
FU National Natural Science Foundation of China [61103105, 91120302]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61103105 and Grant 91120302. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Enrico Magli.
CR Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Avrithis Y, 2014, INT J COMPUT VISION, V107, P1, DOI 10.1007/s11263-013-0659-3
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Leibe Bastian, 2004, WORKSH STAT LEARN CO
   Liu Z., 2012, P ACM INT C MULTIMED, P199
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikulík A, 2010, LECT NOTES COMPUT SC, V6313, P1
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Perd'och M, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPRW.2009.5206529
   Philbin J., 2008, P CVPR, P1
   Qin DF, 2013, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2013.211
   Ribeiro-Neto Berthier., 1999, MODERN INFORM RETRIE
   Shen XH, 2012, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR.2012.6248031
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Stewenius H., 2012, Proc. ECCV, P674, DOI DOI 10.1007/978-3-642-33709-348
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   Tolias G, 2011, IEEE I CONF COMP VIS, P1653, DOI 10.1109/ICCV.2011.6126427
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Xie LX, 2014, COMPUT VIS IMAGE UND, V124, P31, DOI 10.1016/j.cviu.2013.12.011
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zhou W., 2011, Proceedings of ACM Multimedia, P1349, DOI DOI 10.1145/2072298.2072012
NR 27
TC 21
Z9 21
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1391
EP 1397
DI 10.1109/TMM.2015.2446201
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000023
OA Green Published
DA 2024-07-18
ER

PT J
AU Xiao, B
   Georgiou, P
   Baucom, B
   Narayanan, SS
AF Xiao, Bo
   Georgiou, Panayiotis
   Baucom, Brian
   Narayanan, Shrikanth S.
TI Head Motion Modeling for Human Behavior Analysis in Dyadic Interaction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Behavioral characteristics; entrainment; Gaussian mixture model; head
   motion; kinesics; linear predictive analysis
ID NONVERBAL SYNCHRONY; KALMAN FILTER; MOVEMENT; SPEECH; TRACKING;
   GESTURES; RAPPORT; CONTEXT; NOD
AB This paper presents a computational study of head motion in human interaction, notably of its role in conveying interlocutors' behavioral characteristics. Head motion is physically complex and carries rich information; current modeling approaches based on visual signals, however, are still limited in their ability to adequately capture these important properties. Guided by the methodology of kinesics, we propose a data-driven approach to identify typical head motion patterns. The approach follows the steps of first segmenting motion events, then parametrically representing the motion by linear predictive features, and finally generalizing the motion types using Gaussian mixture models. The proposed approach is experimentally validated using video recordings of communication sessions from real couples involved in a couples therapy study. In particular we use the head motion model to classify binarized expert judgments of the interactants' specific behavioral characteristics where entrainment in head motion is hypothesized to play a role: Acceptance, Blame, Positive, and Negative behavior. We achieve accuracies in the range of 60% to 70% for the various experimental settings and conditions. In addition, we describe a measure of motion similarity between the interaction partners based on the proposed model. We show that the relative change of head motion similarity during the interaction significantly correlates with the expert judgments of the interactants' behavioral characteristics. These findings demonstrate the effectiveness of the proposed head motion model, and underscore the promise of analyzing human behavioral characteristics through signal processing methods.
C1 [Xiao, Bo; Georgiou, Panayiotis; Narayanan, Shrikanth S.] Univ So Calif, Dept Elect Engn, Signal & Image Proc Inst, Los Angeles, CA 90089 USA.
   [Baucom, Brian] Univ Utah, Dept Psychol, Salt Lake City, UT 84112 USA.
C3 University of Southern California; Utah System of Higher Education;
   University of Utah
RP Xiao, B (corresponding author), Univ So Calif, Dept Elect Engn, Signal & Image Proc Inst, Los Angeles, CA 90089 USA.
EM boxiao@usc.edu; georgiou@sipi.usc.edu; brian.baucom@psych.utah.edu;
   shri@sipi.usc.edu
RI Georgiou, Panayiotis Panos/E-8387-2018; Narayanan, Shrikanth
   S/D-5676-2012
OI Georgiou, Panayiotis Panos/0000-0002-0790-7161; 
FU NIH; NSF; DoD; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [0911009] Funding Source: National
   Science Foundation; Direct For Education and Human Resources; Division
   Of Research On Learning [1008372] Funding Source: National Science
   Foundation; Division of Computing and Communication Foundations; Direct
   For Computer & Info Scie & Enginr [1029373] Funding Source: National
   Science Foundation
FX This work was supported by the NIH, by the NSF, and by the DoD. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Klara Nahrstedt.
CR Ababsa FE, 2004, IEEE INT CONF ROBOT, P1021, DOI 10.1109/ROBOT.2004.1307284
   Akakin HÇ, 2011, IMAGE VISION COMPUT, V29, P470, DOI 10.1016/j.imavis.2011.03.001
   [Anonymous], 2011, Wireless Communications and Signal Processing (WCSP), 2011 International Conference on
   [Anonymous], 2006, PATTERN RECOGN, DOI DOI 10.2514/1.59844
   [Anonymous], 2011, inverted exclamationChicana Power! Contested Histories of Feminism in the Chicano Movement
   [Anonymous], 2002, Couples interaction rating system-2nd edition (CIRS-2)
   BERNIERI FJ, 1988, J NONVERBAL BEHAV, V12, P120, DOI 10.1007/BF00986930
   Birdwhistell R., 1970, KINESICS CONTEXT ESS, V2
   Black MP, 2013, SPEECH COMMUN, V55, P1, DOI 10.1016/j.specom.2011.12.003
   Bone D., 2012, InterSpeech, P1043
   Bousmalis K, 2013, IMAGE VISION COMPUT, V31, P203, DOI 10.1016/j.imavis.2012.07.003
   Bradski G, 2000, DR DOBBS J, V25, P120
   Busso C, 2007, IEEE T AUDIO SPEECH, V15, P1075, DOI 10.1109/TASL.2006.885910
   Butner J, 2007, PERS RELATIONSHIP, V14, P431, DOI 10.1111/j.1475-6811.2007.00164.x
   Can DG, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P2251
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chartrand TL, 2009, ADV EXP SOC PSYCHOL, V41, P219, DOI 10.1016/S0065-2601(08)00405-X
   Chaspari T, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P1318
   Christensen A, 2004, J CONSULT CLIN PSYCH, V72, P176, DOI 10.1037/0022-006X.72.2.176
   Delaherche E, 2012, IEEE T AFFECT COMPUT, V3, P349, DOI 10.1109/T-AFFC.2012.12
   Ekman P., 1969, Semiotica, V1, P49, DOI [10.1515/semi.1969.1.1.49, DOI 10.1515/SEMI.1969.1.1.49]
   Fanelli G., 2012, INT S COMM CONTR SIG, P1
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Fox EB, 2011, ANN APPL STAT, V5, P1020, DOI 10.1214/10-AOAS395
   Georgiou PC, 2011, LECT NOTES COMPUT SC, V6974, P87, DOI 10.1007/978-3-642-24600-5_12
   Gibson J, 2013, IEEE INT CONF MULTI
   HADAR U, 1984, HUM MOVEMENT SCI, V3, P237, DOI 10.1016/0167-9457(84)90018-6
   HADAR U, 1983, LANG SPEECH, V26, P117, DOI 10.1177/002383098302600202
   HADAR U, 1984, LANG SPEECH, V27, P333, DOI 10.1177/002383098402700404
   HADAR U, 1983, HUM MOVEMENT SCI, V2, P35, DOI 10.1016/0167-9457(83)90004-0
   HADAR U, 1985, J NONVERBAL BEHAV, V9, P214, DOI 10.1007/BF00986881
   Harrigan J.A., 2005, The New Handbook of Methods in Nonverbal Behavior Research
   Jones J., 1998, Couples interaction study: Social support interaction rating system
   KABAL P, 1986, IEEE T ACOUST SPEECH, V34, P1419, DOI 10.1109/TASSP.1986.1164983
   Kang YG, 2006, LECT NOTES ARTIF INT, V4253, P707
   Kapoor Ashish., 2001, Proceedings of the 2001 workshop on Perceptive user interfaces, P1
   Katsamanis A, 2011, LECT NOTES COMPUT SC, V6974, P145, DOI 10.1007/978-3-642-24600-5_18
   Kendon A, 1996, SEMIOTICA, V112, P231, DOI 10.1515/semi.1996.112.3-4.231
   Kiruluta A, 1997, IEEE T SYST MAN CY B, V27, P326, DOI 10.1109/3477.558841
   LAFRANCE M, 1979, US SOC PSYCHOL, V42, P66, DOI 10.2307/3033875
   Nguyen L, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P289
   Lee CC, 2014, COMPUT SPEECH LANG, V28, P518, DOI 10.1016/j.csl.2012.06.006
   Li PH, 2004, IMAGE VISION COMPUT, V22, P157, DOI 10.1016/j.imavis.2003.07.004
   Martin S, 2012, INT C PATT RECOG, P605
   McClave EZ, 2000, J PRAGMATICS, V32, P855, DOI 10.1016/S0378-2166(99)00079-X
   Miao YQ, 2011, LECT NOTES COMPUT SC, V6753, P385, DOI 10.1007/978-3-642-21593-3_39
   Morency LP, 2007, ARTIF INTELL, V171, P568, DOI 10.1016/j.artint.2007.04.003
   Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106
   Narayanan S, 2013, P IEEE, V101, P1203, DOI 10.1109/JPROC.2012.2236291
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Ramseyer F., 2006, Constructivism in the Human Sciences, V11, P150, DOI DOI 10.1007/978-3-642-12397-9-15
   Ramseyer F, 2011, J CONSULT CLIN PSYCH, V79, P284, DOI 10.1037/a0023419
   Reed RG, 2013, INT J PSYCHOPHYSIOL, V88, P309, DOI 10.1016/j.ijpsycho.2012.08.009
   Sargin ME, 2008, IEEE T PATTERN ANAL, V30, P1330, DOI 10.1109/TPAMI.2007.70797
   Tan WZ, 2003, EXPERT SYST APPL, V25, P461, DOI 10.1016/S0957-4174(03)00088-5
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Varni G, 2010, IEEE T MULTIMEDIA, V12, P576, DOI 10.1109/TMM.2010.2052592
   Verhofstadt LL, 2008, EMOTION, V8, P792, DOI 10.1037/a0013976
   Vinciarelli A, 2012, IEEE T AFFECT COMPUT, V3, P69, DOI 10.1109/T-AFFC.2011.27
   Wheatley T, 2012, SOC PERSONAL PSYCHOL, V6, P589, DOI 10.1111/j.1751-9004.2012.00450.x
   Xiao B., 2013, P IEEE INT C MULT EX, P1
   Xiao B, 2013, INTERSPEECH, P2860
   Xiao B, 2013, INT CONF ACOUST SPEE, P3766, DOI 10.1109/ICASSP.2013.6638362
   Xiao B, 2012, ASIAPAC SIGN INFO PR
   Xu-Ming Sun, 2011, Proceedings of the 2011 6th IEEE International Conference on Nano/Micro Engineered and Molecular Systems (NEMS 2011), P367, DOI 10.1109/NEMS.2011.6017369
   Zhao G., 2007, P 15 INT C MULTIMEDI, P807
NR 67
TC 31
Z9 35
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 1107
EP 1119
DI 10.1109/TMM.2015.2432671
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300016
PM 26557047
OA Green Accepted, Bronze
DA 2024-07-18
ER

PT J
AU Argyriou, A
   Kosmanos, D
   Tassiulas, L
AF Argyriou, Antonios
   Kosmanos, Dimitrios
   Tassiulas, Leandros
TI Joint Time-Domain Resource Partitioning, Rate Allocation, and Video
   Quality Adaptation in Heterogeneous Cellular Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 5G wireless networks; DASH; heterogeneous cellular networks; intra-cell
   interference; optimization; rate allocation; resource allocation; small
   cells; video distribution; video streaming
ID OPTIMIZATION; DELIVERY
AB Heterogenous cellular networks (HCN) introduce small cells within the transmission range of a macrocell. For the efficient operation of HCNs it is essential that the high-power macrocell shuts off its transmissions for an appropriate amount of time in order for the low-power small cells to transmit. This is a mechanism that allows time-domain resource partitioning (TDRP) and is critical to be optimized for maximizing the throughput of the complete HCN. In this paper, we investigate video communication in HCNs when TDRP is employed. After defining a detailed system model for video streaming in such an HCN, we consider the problem of maximizing the experienced video quality at all the users, by jointly optimizing the TDRP for the HCN, the rate allocated to each specific user, and the selected video quality transmitted to a user. The NP-hard problem is solved with a primal-dual approximation algorithm that decomposes the problem into simpler subproblems, making them amenable to fast well-known solution algorithms. Consequently, the calculated solution can be enforced in the time scale of real-life video streaming sessions. This last observation motivates the enhancement of the proposed framework to support video delivery with dynamic adaptive streaming over HTTP (DASH). Our extensive simulation results demonstrate clearly the need for our holistic approach for improving the video quality and playback performance of the video streaming users in HCNs.
C1 [Argyriou, Antonios; Kosmanos, Dimitrios; Tassiulas, Leandros] Univ Thessaly, Dept Elect & Comp Engn, Volos 38221, Greece.
C3 University of Thessaly
RP Argyriou, A (corresponding author), Univ Thessaly, Dept Elect & Comp Engn, Volos 38221, Greece.
RI Argyriou, Antonios/AAF-9586-2021
OI Argyriou, Antonios/0000-0002-2510-3124
CR Ahmedin A, 2013, CONEXT STUDENT WORKSHOP '13, P43, DOI 10.1145/2537148.2537158
   [Anonymous], ER MOB REP PULS NETW
   [Anonymous], MSRTR200135
   [Anonymous], SMALL CELL FOR UK
   [Anonymous], CUCS01813
   [Anonymous], P IEEE INT IN PRESS
   Argyriou A., 2007, Advances in neural information processing systems, P41
   Argyriou A, 2008, IEEE T MULTIMEDIA, V10, P691, DOI 10.1109/TMM.2008.922776
   Bertsekas D. P., 2003, CONVEX ANAL OPTIMIZA
   Chakareski J, 2005, IEEE T CIRC SYST VID, V15, P1257, DOI 10.1109/TCSVT.2005.854227
   Chen J., 2013, Proceedings of the 19th Annual International Conference on Mobile Computing Networking, P389
   Deb S, 2014, IEEE ACM T NETWORK, V22, P137, DOI 10.1109/TNET.2013.2246820
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Freris NM, 2013, IEEE ACM T NETWORK, V21, P469, DOI 10.1109/TNET.2012.2203608
   Golrezaei N, 2012, IEEE INFOCOM SER, P1107, DOI 10.1109/INFCOM.2012.6195469
   Joseph V, 2014, IEEE INFOCOM SER, P82, DOI 10.1109/INFOCOM.2014.6847927
   Kosmanos D, 2015, SIGNAL PROCESS-IMAGE, V31, P151, DOI 10.1016/j.image.2014.12.005
   Mansouri Ali, 2013, 2013 10th International Multi-Conference on Systems, Signals and Devices (SSD 2013), P1
   Palomar DP, 2006, IEEE J SEL AREA COMM, V24, P1439, DOI 10.1109/JSAC.2006.879350
   Poularakis K, 2014, IEEE INFOCOM SER, P1087, DOI 10.1109/INFOCOM.2014.6848038
   Proakis J., 2008, Digital Communication, Vthird
   Shakkottai S, 2007, FOUND TRENDS NETW, V2, P271, DOI 10.1561/1300000007
   Singh S., 2012, IEEE International Conference on Communications (ICC 2012), P7071, DOI 10.1109/ICC.2012.6364806
   Singh S, 2014, IEEE T WIREL COMMUN, V13, P888, DOI 10.1109/TWC.2013.120713.130548
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Tse D., 2005, Fundementals of Wireless Communications
   Woo S., 2013, Proc. ACM Int. Conf. on Mobile Systems, Applications, P319
   Wu W., 2009, MM 09, P481, DOI DOI 10.1145/1631272.1631338
NR 28
TC 29
Z9 30
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 736
EP 745
DI 10.1109/TMM.2015.2408254
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Dán, G
   Khan, MA
   Fodor, V
AF Dan, Gyorgy
   Khan, Muhammad Altamash
   Fodor, Viktoria
TI Characterization of SURF and BRISK Interest Point Distribution for
   Distributed Feature Extraction in Visual Sensor Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE BRISK; distributed feature extraction; interest point distribution;
   SURF; visual sensor network (VSN)
AB We study the statistical characteristics of SURF and BRISK interest points and descriptors, with the aim of supporting the design of distributed processing across sensor nodes in a resource-constrained visual sensor network (VSN). Our results show high variability in the density, the spatial distribution, and the octave layer distribution of the interest points. The high variability implies that balancing the processing load among the sensor nodes is a very challenging task, and obtaining a priori information is essential, e.g., through prediction. Our results show that if a priori information is available about the images, then Top-interest point selection, limited, octave-based processing at the camera node, together with area-based interest point detection and extraction at the processing nodes, can balance the processing load and limit the transmission cost in the network. Complete interest point detection at the camera node with optimized descriptor extraction delegation to the processing nodes in turn can further decrease the transmission load and allow a better balance of the processing load among the network nodes.
C1 [Dan, Gyorgy; Khan, Muhammad Altamash; Fodor, Viktoria] KTH Royal Inst Technol, Sch Elect Engn, ACCESS Linnaeus Ctr, S-10044 Stockholm, Sweden.
C3 Royal Institute of Technology
RP Dán, G (corresponding author), KTH Royal Inst Technol, Sch Elect Engn, ACCESS Linnaeus Ctr, S-10044 Stockholm, Sweden.
EM gyuri@ee.kth.se; khan6@ee.kth.se; vfodor@ee.kth.se
RI Dan, György/H-8604-2012
OI Dan, György/0000-0002-4876-0223
FU European Commission under FET-Open [296676]
FX Manuscript received October 17, 2014; accepted February 18, 2015. Date
   of publication February 24, 2015; date of current version April 15,
   2015. This work was supported by the Future and Emerging Technologies
   Programme within the Seventh Framework Programme for Research of the
   European Commission under FET-Open Grant 296676. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Pal Halvorsen.
CR [Anonymous], 2013, P 18 IEEE INT C DIG
   Baroffio L, 2014, IEEE IMAGE PROC, P3408, DOI 10.1109/ICIP.2014.7025690
   Baroffio L, 2013, IEEE IMAGE PROC, P1895, DOI 10.1109/ICIP.2013.6738390
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bharadwaj V, 2002, IMAGE VISION COMPUT, V20, P917, DOI 10.1016/S0262-8856(02)00090-2
   Bostanci E, 2014, IEEE T IMAGE PROCESS, V23, P153, DOI 10.1109/TIP.2013.2286907
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chandrasekhar V., 2010, ACM multimedia workshop on Mobile cloud media computing, P41
   Chao JS, 2013, IEEE IMAGE PROC, P1675, DOI 10.1109/ICIP.2013.6738345
   Dan G., 2010, P 9 INT C PEER TO PE, P12
   Duan LY, 2012, 2012 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Duy-Nguyen Ta, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2937, DOI 10.1109/CVPRW.2009.5206831
   Ehsan S., 2010, P INT C MACH VIS APR, P411
   Eriksson Emil, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P674, DOI 10.1109/ICASSP.2014.6853681
   Eriksson E, 2014, IEEE INT CONF DISTR, P152, DOI 10.1109/DCOSS.2014.30
   Fang ZM, 2011, INT SYM PERFORM ANAL, P154, DOI 10.1109/ISPASS.2011.5762732
   Griffin G., 2007, CALTECH 256 OBJECT C
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Lyra ML, 2003, EUROPHYS LETT, V62, P131, DOI 10.1209/epl/i2003-00371-6
   Marcus A, 2012, IEEE POTENTIALS, V31, P38, DOI 10.1109/MPOT.2011.2178279
   Redondi A, 2012, IEEE IMAGE PROC, P1105, DOI 10.1109/ICIP.2012.6467057
   Redondi A, 2015, AD HOC NETW, V28, P38, DOI 10.1016/j.adhoc.2015.01.008
   Redondi A, 2013, IEEE IMAGE PROC, P2910, DOI 10.1109/ICIP.2013.6738599
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
   Takacs G, 2013, SIGNAL PROCESS-IMAGE, V28, P334, DOI 10.1016/j.image.2012.11.004
NR 27
TC 16
Z9 16
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 591
EP 602
DI 10.1109/TMM.2015.2406574
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300002
DA 2024-07-18
ER

PT J
AU Zhang, LM
   Gao, Y
   Hong, RC
   Hu, YX
   Ji, RR
   Dai, QH
AF Zhang, Luming
   Gao, Yue
   Hong, Richang
   Hu, Yuxing
   Ji, Rongrong
   Dai, Qionghai
TI Probabilistic Skimlets Fusion for Summarizing Multiple Consumer Landmark
   Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active learning; multi-video; probabilistic model; summarization; video
   skimlet
ID QUALITY ASSESSMENT
AB It is difficult to develop a computational model that can accurately predict the quality of the video summary. This paper proposes a novel algorithm to summarize one-shot landmark videos. The algorithm can optimally combine multiple unedited consumer video skims into an aesthetically pleasing summary. In particular, to effectively select the representative key frames from multiple videos, an active learning algorithm is derived by taking advantage of the locality of the frames within each video. Toward a smooth video summary, we define skimlet, a video clip with adjustable length, starting frame, and positioned by each skim. Thereby, a probabilistic framework is developed to transfer the visual cues from a collection of aesthetically pleasing photos into the video summary. The length and the starting frame of each skimlet are calculated to maximally smoothen the video summary. At the same time, the unstable frames are removed from each skimlet. Experiments on multiple videos taken from different sceneries demonstrated the aesthetics, the smoothness, and the stability of the generated summary.
C1 [Zhang, Luming] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
   [Gao, Yue; Dai, Qionghai] Tsinghua Univ, Dept Automat, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Hong, Richang] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
   [Hu, Yuxing] Tsinghua Univ, Sch Aerosp, Beijing 100084, Peoples R China.
   [Ji, Rongrong] Xiamen Univ, Dept Cognit Sci, Xiamen 361006, Peoples R China.
C3 National University of Singapore; Tsinghua University; Hefei University
   of Technology; Tsinghua University; Xiamen University
RP Gao, Y (corresponding author), Tsinghua Univ, Dept Automat, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM kevin.gaoy@gmail.com
RI Dai, Qionghai/ABD-5298-2021; Gao, Yue/B-3376-2012; zhang,
   lu/GRO-2969-2022; Lei, Ming/JAD-1050-2023
OI Dai, Qionghai/0000-0001-7043-3061; 
FU National Natural Science Foundation of China [61035002, 61120106003,
   61422210, 61373076]; Fundamental Research Funds for the Central
   Universities [2013121026]
FX This work was supported by the Project of National Natural Science
   Foundation of China under Grant 61035002, Grant 61120106003, Grant
   61422210, and Grant 61373076, and by the Fundamental Research Funds for
   the Central Universities under Grant 2013121026. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Xiao-Ping Zhang. (Corresponding author: Yue Gao.)
CR [Anonymous], 2006, ADV NEURAL INF PROCE
   [Anonymous], 1996, MATRIX COMPUTATION
   [Anonymous], 2012, Popul. Res, DOI DOI 10.1145/2155555.2155565
   [Anonymous], 1995, STORAGE RETRIEVAL IM, DOI DOI 10.1117/12.205308
   Bouguettaya A, 1996, IEEE T KNOWL DATA EN, V8, P333, DOI 10.1109/69.494170
   Cai D, 2012, IEEE T KNOWL DATA EN, V24, P707, DOI 10.1109/TKDE.2011.104
   Cao Z., 2007, P 24 INT C MACHINE L, P129, DOI DOI 10.1145/1273496.1273513
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Coleman TF, 1996, SIAM J OPTIMIZ, V6, P1040, DOI 10.1137/S1052623494240456
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Gao Y, 2014, IEEE T CIRC SYST VID, V24, P1122, DOI 10.1109/TCSVT.2014.2302366
   Grill Tom., 1990, Photographic composition
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   He XF, 2004, ADV NEUR IN, V16, P153
   Herranz L, 2010, IEEE T CIRC SYST VID, V20, P1265, DOI 10.1109/TCSVT.2010.2057020
   Luo J., 2012, IEEE T CIRCUITS SYST, V19, P289
   Mei T, 2007, IEEE T CIRC SYST VID, V17, P699, DOI 10.1109/TCSVT.2007.896640
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Moorthy AK, 2010, LECT NOTES COMPUT SC, V6315, P1, DOI 10.1007/978-3-642-15555-0_1
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Ni BB, 2013, IEEE T MULTIMEDIA, V15, P1138, DOI 10.1109/TMM.2013.2241042
   Roy Nicholas, 2001, Toward optimal active learning through monte carlo estimation of error reduction, P441
   Saini M.K., 2012, Proceedings of the 20th International Conference on Multimedia, P139, DOI DOI 10.1145/2393347.2393373
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   She JY, 2011, 2011 FIRST ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P490, DOI 10.1109/ACPR.2011.6166623
   Song YC, 2012, IEEE T MULTIMEDIA, V14, P456, DOI 10.1109/TMM.2011.2172937
   Stricker M., 2006, P STOR RETR IM VID D, P381
   Tompkin J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185564
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Wang F, 2009, IEEE INT CON MULTI, P1326, DOI 10.1109/ICME.2009.5202747
   Yeh C.-H., 2010, Proceedings of the international conference on Multimedia - MM'10, page, P211
   Yeh HH, 2013, IEEE T MULTIMEDIA, V15, P1944, DOI 10.1109/TMM.2013.2280250
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
   Zhang SL, 2013, IEEE T IMAGE PROCESS, V22, P2889, DOI 10.1109/TIP.2013.2251650
   Zhu X., 2010, MULTIMEDIA SYST, V9, P31
NR 37
TC 18
Z9 18
U1 3
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2015
VL 17
IS 1
BP 40
EP 49
DI 10.1109/TMM.2014.2370257
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AX6QW
UT WOS:000347047400005
DA 2024-07-18
ER

PT J
AU Seo, JW
   Kim, SD
AF Seo, Ja-Won
   Kim, Seong Dae
TI Recursive On-Line (2D)<SUP>2</SUP>PCA and Its Application to Long-Term
   Background Subtraction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Background model; long-term background subtraction; object detection;
   recursive on-line (2D)(2)PCA (two-directional two-dimensional principal
   component analysis)
ID FACE REPRESENTATION; 2-DIMENSIONAL PCA; MODEL
AB In this paper, we propose a novel background subtraction method which enables reliable detection of foreground objects in a long surveillance video stream. Recently, although much progress has been made in the field of background subtraction, there are still challenging scenarios (e. g., high frequency motion of dynamic texture, non-stationary motion of camera, abrupt changes of illumination, etc.) in the long surveillance videos in which even state-of-the-art methods are often prone to fail. To cope with these challenging scenarios effectively, in the proposed method, a background model is initialized in a low-dimensional subspace and then updated periodically based on a novel recursive on-line (2D)(2)PCA algorithm developed in this paper. Moreover, a threshold map is also updated in a scene-adaptive manner for labeling each pixel in a scene either foreground or background independently. Based on this on-line framework, the background of a surveillance video stream is reconstructed over time, thereby facilitating the detection of foreground objects reliably. In extensive experiments, we demonstrate that the proposed background subtraction method can cope with the aforementioned challenging scenarios more favorably than the state-of-the-art methods.
C1 [Seo, Ja-Won; Kim, Seong Dae] Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Seo, JW (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
EM jawon.seo@kaist.ac.kr; sdkim@kaist.ac.kr
RI Kim, Seong-Dae/C-1892-2011
CR [Anonymous], IEEE INT C AC SPEECH
   [Anonymous], J ACM
   [Anonymous], 2000, P EUR C COMP VIS, DOI DOI 10.1007/3-540-45053-X_48
   [Anonymous], INT C IM PROC
   [Anonymous], 2012, P IEEE C COMP VIS PA
   [Anonymous], 2012, AS C COMP VIS
   Candes Emmanuel, 2010, 2010 IEEE Sensor Array and Multichannel Signal Processing Workshop (SAM 2010), P201, DOI 10.1109/SAM.2010.5606734
   Chan AB, 2011, MACH VISION APPL, V22, P751, DOI 10.1007/s00138-010-0262-3
   Cong Z., 2011, PROC EURASIP J IMAGE, P1
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Elgammal A, 2002, P IEEE, V90, P1151, DOI 10.1109/JPROC.2002.801448
   Guo JM, 2013, IEEE T CIRC SYST VID, V23, P1809, DOI 10.1109/TCSVT.2013.2269011
   Guyon C, 2012, IEEE IMAGE PROC, P1225, DOI 10.1109/ICIP.2012.6467087
   Harville M, 2002, LECT NOTES COMPUT SC, V2352, P543
   Huang JZ, 2009, IEEE I CONF COMP VIS, P64, DOI 10.1109/ICCV.2009.5459202
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Kim W, 2012, IEEE SIGNAL PROC LET, V19, P127, DOI 10.1109/LSP.2011.2182648
   Lee DS, 2005, IEEE T PATTERN ANAL, V27, P827, DOI 10.1109/TPAMI.2005.102
   Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169
   Monnet A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1305
   Sheikh Y, 2005, IEEE T PATTERN ANAL, V27, P1778, DOI 10.1109/TPAMI.2005.213
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Yoo S, 2013, PATTERN RECOGN LETT, V34, P2086, DOI 10.1016/j.patrec.2013.07.008
   Zhang DQ, 2005, NEUROCOMPUTING, V69, P224, DOI 10.1016/j.neucom.2005.06.004
   Zhang SP, 2008, IEEE IMAGE PROC, P1556, DOI 10.1109/ICIP.2008.4712065
   Zhong J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P44, DOI 10.1109/ICCV.2003.1238312
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 31
TC 14
Z9 17
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2333
EP 2344
DI 10.1109/TMM.2014.2353772
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300021
DA 2024-07-18
ER

PT J
AU Biel, JI
   Gatica-Perez, D
AF Biel, Joan-Isaac
   Gatica-Perez, Daniel
TI Mining Crowdsourced First Impressions in Online Social Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attractiveness; automatic prediction; crowdsourcing; mood; nonverbal
   behavior; personality; topic models; verbal content; vlogs
ID PHYSICAL ATTRACTIVENESS; PERSONALITY IMPRESSIONS; BEHAVIOR
AB While multimedia and social computing research have used crowdsourcing techniques to annotate objects, actions, and scenes in social video sites like YouTube, little work has addressed the crowdsourcing of personal and social traits in online social video or social media content in general. In this paper, we address the problems of (1) crowdsourcing the annotation of first impressions of video bloggers (vloggers) personal and social traits in conversational YouTube videos, and (2) mining the impressions with the goal of modeling the interplay of different vlogger facets. First, we design a human annotation task to crowdsource impressions of vloggers that extends a tradition of studies of personality impressions with the addition of attractiveness and mood impressions. Second, we propose a probabilistic framework using Topic Models to discover prototypical impressions that are data driven, and that combine multiple facets of vloggers. Finally, we address the task of automatically predicting topic impressions using nonverbal and verbal content extracted from videos and comments. Our study of 442 YouTube vlogs and 2,210 annotations collected in Mechanical Turk supports recent literature showing the feasibility to crowdsource interpersonal human impression with comparable quality to what is reported in social psychology research, and provides insights on the interplay among human first impressions. We also show that topic models are useful to discover meaningful prototypical impressions that can be validated by humans, and that different topics can be predicted using different sources of information from vloggers' nonverbal and verbal content, as well as comments from the audience.
C1 [Biel, Joan-Isaac; Gatica-Perez, Daniel] Idiap Res Inst, CH-1920 Martigny, Switzerland.
RP Biel, JI (corresponding author), Idiap Res Inst, CH-1920 Martigny, Switzerland.
EM jibiel@idiap.ch; gatica@idiap.ch
FU Swiss National Science Foundation (SNSF) under the IM2 project
FX This work was supported by the Swiss National Science Foundation (SNSF)
   under the IM2 project. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Sheng-Wei
   (Kuan-Ta) Chen.
CR AMBADY N, 1995, J PERS SOC PSYCHOL, V69, P518, DOI 10.1037/0022-3514.69.3.518
   AMBADY N, 1992, PSYCHOL BULL, V111, P256, DOI 10.1037/0033-2909.111.2.256
   [Anonymous], 2007, P AAAI INT C WEBL SO
   [Anonymous], P 4 INT WORKSH CORP
   [Anonymous], 2009, Proceedings of the 18th international conference on World wide web, DOI [10.1145/1526709.1526758, DOI 10.1145/1526709.1526758]
   [Anonymous], HONEST SIGNALS THEY
   Balog K., 2006, P INT C WORLD WID WE
   Behrend TS, 2011, BEHAV RES METHODS, V43, P800, DOI 10.3758/s13428-011-0081-0
   Berinsky A. J., 2012, POLIT ANAL, V2
   Biel J.-I., 2012, P INT C MULT INT ICM, P55
   Biel JI, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P119, DOI 10.1145/2522848.2522877
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032
   Biel Joan-Isaac, 2012, P AAAI INT C WEBL SO
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   BORKENAU P, 1992, J PERS SOC PSYCHOL, V62, P645, DOI 10.1037/0022-3514.62.4.645
   Brew A, 2010, FRONT ARTIF INTEL AP, V215, P145, DOI 10.3233/978-1-60750-606-5-145
   Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980
   Chang J., 2009, Advances in neural information processing systems, P22, DOI DOI 10.5555/2984093.2984126
   Diakopoulos NA, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1195
   DION KK, 1990, J CROSS CULT PSYCHOL, V21, P158, DOI 10.1177/0022022190212002
   Evans D. C., 2008, P AAAI INT C WEBL SO
   Filippova K, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P835
   Fiore A., 2008, P ACM SIG INT C HUM
   Gosling S. D., 2007, P AAAI INT C WEBL SO
   Gosling SD, 2002, J PERS SOC PSYCHOL, V82, P379, DOI 10.1037//0022-3514.82.3.379
   Gosling SD, 2003, J RES PERS, V37, P504, DOI 10.1016/S0092-6566(03)00046-1
   Hitsch G., 2005, SOC EC DYNAM, V207
   Iwata Tomoharu, 2009, Advances in Neural Information Processing Systems, P835
   Kniffin KM, 2004, EVOL HUM BEHAV, V25, P88, DOI 10.1016/S1090-5138(04)00006-6
   Kramer ADI, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P287
   Krestel R., 2009, Proceedings of the 3rd ACM Conference on Recommender Systems, P61, DOI [DOI 10.1145/1639714.1639726, 10.1145/1639714.1639726]
   Lepri B, 2009, LECT NOTES COMPUT SC, V5535, P114, DOI 10.1007/978-3-642-02247-0_13
   Li J, 2010, INT J HUM-COMPUT ST, V68, P589, DOI 10.1016/j.ijhcs.2010.04.001
   Mairesse F, 2007, J ARTIF INTELL RES, V30, P457, DOI 10.1613/jair.2349
   MCCRAE RR, 1992, J PERS, V60, P175, DOI 10.1111/j.1467-6494.1992.tb00970.x
   Mehl MR, 2006, J PERS SOC PSYCHOL, V90, P862, DOI 10.1037/0022-3514.90.5.862
   Mihalcea R., 2006, P AAAI SPRING S COMP, P19
   Mishne G., 2005, P ACM SIGIR 2005 STY
   Mohammadi G., 2010, Proceedings of the 2nd international workshop on Social signal processing, SSPW'10, P17, DOI [10.1145/1878116.1878123, DOI 10.1145/1878116.1878123]
   Nguyen T, 2010, LECT NOTES ARTIF INT, V6119, P283
   Pennebaker JW, 1999, J PERS SOC PSYCHOL, V77, P1296, DOI 10.1037/0022-3514.77.6.1296
   SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420
   Soler M. D., 2010, Albeitar, P4
   Stecher K., 2008, P AAAI INT C WEBL SO
   Steele F, 2009, P AAAI INT C WEBL SO
   Stillwell David J, 2004, American Psychologist, V59, P93
   Vazire S, 2004, J PERS SOC PSYCHOL, V87, P123, DOI 10.1037/0022-3514.87.1.123
   Walther JB, 2008, HUM COMMUN RES, V34, P28, DOI 10.1111/j.1468-2958.2007.00312.x
   Wang SS, 2010, COMPUT HUM BEHAV, V26, P226, DOI 10.1016/j.chb.2009.10.001
   Yarkoni T, 2010, J RES PERS, V44, P363, DOI 10.1016/j.jrp.2010.04.001
NR 50
TC 6
Z9 8
U1 0
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 2062
EP 2074
DI 10.1109/TMM.2014.2346471
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300022
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Mekuria, R
   Sanna, M
   Izquierdo, E
   Bulterman, DCA
   Cesar, P
AF Mekuria, Rufael
   Sanna, Michele
   Izquierdo, Ebroul
   Bulterman, Dick C. A.
   Cesar, Pablo
TI Enabling Geometry-Based 3-D Tele-Immersion With Fast Mesh Compression
   and Linear Rateless Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Block codes; geometry processing; multimedia communication; multimedia
   systems; source coding; 3-D mesh streaming; 3-D tele-immersion
AB 3-D tele-immersion (3DTI) enables participants in remote locations to share, in real time, an activity. It offers users interactive and immersive experiences, but it challenges current media-streaming solutions. Work in the past has mainly focused on the efficient delivery of image-based 3-D videos and on realistic rendering and reconstruction of geometry-based 3-D objects. The contribution of this paper is a real-time streaming component for 3DTI with dynamic reconstructed geometry. This component includes both a novel fast compression method and a rateless packet protection scheme specifically designed towards the requirements imposed by real time transmission of live-reconstructed mesh geometry. Tests on a large dataset show an encoding speed-up up to ten times at comparable compression ratio and quality, when compared with the high-end MPEG-4 SC3DMC mesh encoders. The implemented rateless code ensures complete packet loss protection of the triangle mesh object and a delivery delay within interactive bounds. Contrary to most linear fountain codes, the designed codec enables real-time progressive decoding allowing partial decoding each time a packet is received. This approach is compared with transmission over TCP in packet loss rates and latencies, typical in managed WAN and MAN networks, and heavily outperforms it in terms of end-to-end delay. The streaming component has been integrated into a larger 3DTI environment that includes state of the art 3-D reconstruction and rendering modules. This resulted in a prototype that can capture, compress transmit, and render triangle mesh geometry in real-time in realistic internet conditions as shown in experiments. Compared with alternative methods, lower interactive end-to-end delay and frame rates over three times higher are achieved.
C1 [Mekuria, Rufael; Cesar, Pablo] Ctr Wiskunde Informat, Distributed & Interact Syst Dept, NL-1098 XG Amsterdam, Netherlands.
   [Sanna, Michele; Izquierdo, Ebroul] Queen Mary Univ London, Dept Elect Engn & Comp Sci, London E1 4NS, England.
   [Bulterman, Dick C. A.] Vrije Univ Amsterdam, Dept Comp Sci, NL-1081 HV Amsterdam, Netherlands.
C3 University of London; Queen Mary University London; Vrije Universiteit
   Amsterdam
RP Mekuria, R (corresponding author), Ctr Wiskunde Informat, Distributed & Interact Syst Dept, NL-1098 XG Amsterdam, Netherlands.
EM rufael.mekuria@cwi.nl; michele.sanna@eecs.qmul.ac.uk;
   izquierdo@eecs.qmul.ac.uk; Dick.Bulterman@few.vu.nl; P.S.Cesar@cwi.nl
OI Cesar, Pablo/0000-0003-1752-6837
FU European Community [ICT-2011-7-287723]
FX This work was supported by the European Community's Seventh Framework
   Programme (FP7/2007-2013) under the REVERIE project, Grant
   ICT-2011-7-287723. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Klara Nahrstedt.
CR Alexiadis D., 2013, DATASETS MULTIPLE KI
   Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   Alregib G, 2005, IEEE T MULTIMEDIA, V7, P1149, DOI 10.1109/TMM.2005.858404
   [Anonymous], 2013, NEWT NETW EM WIND
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   Bajcsy R., 2013, SPRINGER VIRTUAL REA, V17, P29
   Cheng W, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/1925101.1925105
   Chou P.A., 2003, Proc. Annual Allerton Conference on Communication control and Computing, V41, P40
   Forte G., 2010, P 16 INT C VIRT SYST, P155
   Gailly M., 2013, ZLIB MASSIVELY SPIFF
   Horn C. R., 1991, Topics in Matrix Analysis
   Jang ES, 2010, ETRI J, V32, P163, DOI 10.4218/etrij.10.0209.0357
   Jovanova B, 2009, SIGNAL PROCESS-IMAGE, V24, P101, DOI 10.1016/j.image.2008.10.011
   Kum S., 2003, Proceedings of the eleventh ACM international conference on Multimedia, P185
   Kurillo G, 2009, STUD HEALTH TECHNOL, V142, P148, DOI 10.3233/978-1-58603-964-6-148
   Li H, 2006, ACM T MULTIM COMPUT, V2, P282, DOI 10.1145/1201730.1201733
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Mamou K., 2008, THESIS U RENE DESCAR
   Mamou K, 2009, COMPUT ANIMAT VIRT W, V20, P343, DOI 10.1002/cav.319
   Mekuria R., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys '13, New York, NY, USA, P24, DOI DOI 10.1145/2483977.2483980
   Miller E. L., GF COMPLETE COMPREHE
   Peng JL, 2005, J VIS COMMUN IMAGE R, V16, P688, DOI 10.1016/j.jvcir.2005.03.001
   Plank J., 2013, PROC 11 USENIX FAST
   Shokralli A., 2007, RFC 5053 RAPTOR FORW
   Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241
   Vasudevan R, 2011, IEEE T MULTIMEDIA, V13, P573, DOI 10.1109/TMM.2011.2123871
   Wang CH, 2010, IEEE INT SYMP INFO, P2028, DOI 10.1109/ISIT.2010.5513368
   Wu W., 2007, P ACM WORKSH SUPP CR
   Wu W., 2011, P 19 ACM INT C MULTI, P13
NR 29
TC 19
Z9 28
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1809
EP 1820
DI 10.1109/TMM.2014.2331919
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, QJ
   Aubrey, AJ
   Wang, WW
AF Liu, Qingju
   Aubrey, Andrew J.
   Wang, Wenwu
TI Interference Reduction in Reverberant Speech Separation With Visual
   Voice Activity Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaboosting; binaural; blind source separation; interference removal;
   visual voice activity detection
ID BLIND SOURCE SEPARATION; INDEPENDENT COMPONENT ANALYSIS; NOISE; MODEL;
   SIGNALS
AB The visual modality, deemed to be complementary to the audio modality, has recently been exploited to improve the performance of blind source separation (BSS) of speech mixtures, especially in adverse environments where the performance of audio-domain methods deteriorates steadily. In this paper, we present an enhancement method to audio-domain BSS with the integration of voice activity information, obtained via a visual voice activity detection (VAD) algorithm. Mimicking aspects of human hearing, binaural speech mixtures are considered in our two-stage system. Firstly, in the off-line training stage, a speaker-independent voice activity detector is formed using the visual stimuli via the adaboosting algorithm. In the on-line separation stage, interaural phase difference (IPD) and interaural level difference (ILD) cues are statistically analyzed to assign probabilistically each time-frequency (TF) point of the audio mixtures to the source signals. Next, the detected voice activity cues (found via the visual VAD) are integrated to reduce the interference residual. Detection of the interference residual takes place gradually, with two layers of boundaries in the correlation and energy ratio map. We have tested our algorithm on speech mixtures generated using room impulse responses at different reverberation times and noise levels. Simulation results show performance improvement of the proposed method for target speech extraction in noisy and reverberant environments, in terms of signal-to-interference ratio (SIR) and perceptual evaluation of speech quality (PESQ).
C1 [Liu, Qingju; Wang, Wenwu] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
   [Aubrey, Andrew J.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, S Glam, Wales.
C3 University of Surrey; Cardiff University
RP Liu, QJ (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
EM q.liu@surrey.ac.uk; a.j.aubrey@cs.cardiff.ac.uk; w.wang@surrey.ac.uk
RI wang, wenwu/HOF-4371-2023
FU Engineering and Physical Sciences Research Council (EPSRC) of the U.K.
   [EP/H012842/1]; EPSRC [EP/L000539/1, EP/H012842/1] Funding Source: UKRI
FX This work was supported in part by the Engineering and Physical Sciences
   Research Council (EPSRC) of the U.K. under Grant EP/H012842/1. The
   associate editor responsible for coordinating the review of this paper
   and approving it for publication was Dr. Chong-Wah Ngo.
CR Aichner R., 2006, P INT C AC SPEECH SI, V5
   Aubrey AJ, 2010, IET IMAGE PROCESS, V4, P463, DOI 10.1049/iet-ipr.2009.0042
   BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Benyassine A, 1997, IEEE COMMUN MAG, V35, P64, DOI 10.1109/35.620527
   BOLL SF, 1979, IEEE T ACOUST SPEECH, V27, P113, DOI 10.1109/TASSP.1979.1163209
   Bulkin DA, 2006, CURR OPIN NEUROBIOL, V16, P415, DOI 10.1016/j.conb.2006.06.008
   CARDOSO JF, 1993, IEE PROC-F, V140, P362, DOI 10.1049/ip-f-2.1993.0054
   CHERRY EC, 1953, J ACOUST SOC AM, V25, P975, DOI 10.1121/1.1907229
   Choi C, 2004, LECT NOTES COMPUT SC, V3195, P857
   Cichocki A, 2005, CLIN NEUROPHYSIOL, V116, P729, DOI 10.1016/j.clinph.2004.09.017
   Cohen I, 2003, IEEE T SPEECH AUDI P, V11, P466, DOI 10.1109/TSA.2003.811544
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771
   Hummersone C., 2011, THESIS U SURREY SURR
   Jang KS, 2007, INT J COMPUT SCI NET, V7, P148
   JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588
   Jourjine A, 2000, INT CONF ACOUST SPEE, P2985, DOI 10.1109/ICASSP.2000.861162
   JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X
   Liew AWC, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P255, DOI 10.1109/ICIP.2000.899293
   Liu P., 2004, P INT C AC SPEECH SI, V1
   Liu Q., 2011, P SENS SIGN PROC DEF
   Liu QJ, 2012, 2012 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P664, DOI 10.1109/SSP.2012.6319789
   Liu QJ, 2012, SIGNAL PROCESS, V92, P1916, DOI 10.1016/j.sigpro.2011.11.007
   Liu Q, 2011, PROCEEDINGS OF THE 3RD (2011) INTERNATIONAL CONFERENCE ON FINANCIAL RISK AND CORPORATE FINANCE MANAGEMENT, VOLS 1 AND 2, P457, DOI 10.1109/ICAwST.2011.6163194
   Low SY, 2004, IEEE T SPEECH AUDI P, V12, P539, DOI 10.1109/TSA.2004.832993
   Madhu N, 2008, INT CONF ACOUST SPEE, P45, DOI 10.1109/ICASSP.2008.4517542
   Magarey J, 1998, IEEE T SIGNAL PROCES, V46, P1069, DOI 10.1109/78.668557
   Mandel MI, 2010, IEEE T AUDIO SPEECH, V18, P382, DOI 10.1109/TASL.2009.2029711
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Messer K., 1999, 2 INT C AUD VID BAS, V964, P965
   Ong E.-J., 2008, P IEEE INT C AUT FAC
   Park KS, 2006, IEEE SIGNAL PROC LET, V13, P749, DOI 10.1109/LSP.2006.879985
   Parra LC, 2002, IEEE T SPEECH AUDI P, V10, P352, DOI 10.1109/TSA.2002.803443
   PETERSON GE, 1960, J ACOUST SOC AM, V32, P693, DOI 10.1121/1.1908183
   Platt J., 1998, SEQUENTIAL MINIMAL O
   Rivet B, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P223
   Saruwatari H, 2003, EURASIP J APPL SIG P, V2003, P1135, DOI 10.1155/S1110865703305104
   Sawada H, 2011, IEEE T AUDIO SPEECH, V19, P516, DOI 10.1109/TASL.2010.2051355
   Schwartz JL, 2004, COGNITION, V93, pB69, DOI 10.1016/j.cognition.2004.01.006
   Sheerman-Chase T., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1242, DOI 10.1109/ICCVW.2011.6130393
   Sodoyer D, 2009, J ACOUST SOC AM, V125, P1184, DOI 10.1121/1.3050257
   Sohn J, 1999, IEEE SIGNAL PROC LET, V6, P1, DOI 10.1109/97.736233
   Titze I., 1994, Principles of Voice Production
   Valin JM, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P221
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Vorobyov S, 2002, BIOL CYBERN, V86, P293, DOI 10.1007/s00422-001-0298-6
   Wang DL, 2005, SPEECH SEPARATION BY HUMANS AND MACHINES, P181, DOI 10.1007/0-387-22794-6_12
   Woodruff J, 2012, IEEE T AUDIO SPEECH, V20, P1503, DOI 10.1109/TASL.2012.2183869
NR 50
TC 8
Z9 10
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1610
EP 1623
DI 10.1109/TMM.2014.2322824
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200010
DA 2024-07-18
ER

PT J
AU Yoo, SB
   Choi, K
   Ra, JB
AF Yoo, Seok Bong
   Choi, Kyuha
   Ra, Jong Beom
TI Post-Processing for Blocking Artifact Reduction Based on Inter-Block
   Correlation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blocking artifacts; group-based filtering; inter-block correlation;
   three lowest frequency DCT coefficients
ID JPEG-COMPRESSED IMAGES; QUALITY ASSESSMENT; DEBLOCKING FILTER; DCT;
   RECONSTRUCTION; SUPPRESSION
AB Block-based coding introduces an undesirable discontinuity between neighboring blocks in reconstructed images. This image degradation, referred to as blocking artifacts, arises mainly due to the loss of inter-block correlation in the quantization process of discrete cosine transform coefficients. In many multimedia broadcasting applications, such as a television, decoded video sequences suffer from blocking artifacts. In this paper, we present a novel post-processing algorithm based on increment of inter-block correlation aimed at reducing blocking artifacts. We first smooth the three lowest frequency discrete cosine transform (DCT) coefficients between neighboring blocks, in order to reduce blocking artifacts in the flat region, which are most sensitive to the human visual system. We then group each edge block and its matched blocks together and apply group-based filtering to increase the correlation between grouped blocks. This suppresses blocking artifacts in the edge region while preserving details. In addition, the algorithm is extended to reduce flickering artifacts as well as blocking artifacts in video sequences. Experimental results show that the proposed method successfully alleviates blocking artifacts in both images and videos coded with low bit-rates.
C1 [Yoo, Seok Bong; Choi, Kyuha; Ra, Jong Beom] Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Yoo, SB (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
EM sbyoo@issserver.kaist.ac.kr; kyuha75.choi@samsung.com; jbra@kaist.ac.kr
RI Yoo, Seok Bong/Q-7410-2019; Beom, Jong/C-1958-2011
OI Yoo, Seok Bong/0000-0002-6528-701X; 
FU Samsung Electronics Co., Ltd.
FX This work was supported by Samsung Electronics Co., Ltd. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], 2008, C402 ITUT SG16
   [Anonymous], 2003, P VLSM
   [Anonymous], 2008, VCEGAI14 ITUT SG16 Q
   [Anonymous], 2008, VCEGAJ13 ITUSG16 Q6
   [Anonymous], 2009, VCEGAK22 ITUT SG16 Q
   Chen Q., 2011, VEHICULAR TECHNOLOGY, P1
   Chen T, 2001, IEEE T CIRC SYST VID, V11, P594, DOI 10.1109/76.920189
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Esche M, 2012, IEEE T CIRC SYST VID, V22, P659, DOI 10.1109/TCSVT.2011.2177142
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Francisco NC, 2012, SIGNAL PROCESS-IMAGE, V27, P985, DOI 10.1016/j.image.2012.05.005
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Hsung TC, 1998, IEEE T IMAGE PROCESS, V7, P1488, DOI 10.1109/83.718489
   Jung C, 2012, SIGNAL PROCESS-IMAGE, V27, P663, DOI 10.1016/j.image.2012.03.002
   Kim NC, 1998, IEEE T CIRC SYST VID, V8, P253, DOI 10.1109/76.678618
   Kim SD, 1999, IEEE T CIRC SYST VID, V9, P156, DOI 10.1109/76.744282
   Krutz A, 2012, IEEE T CIRC SYST VID, V22, P1802, DOI 10.1109/TCSVT.2012.2223012
   Kuszpet Y., 2007, P PICT COD S LISB PO
   Li Z, 2005, IEEE T CIRC SYST VID, V15, P1583, DOI 10.1109/TCSVT.2005.858613
   Liew AWC, 2005, IEEE T CIRC SYST VID, V15, P795, DOI 10.1109/TCSVT.2005.848303
   Liew AWC, 2004, IEEE T CIRC SYST VID, V14, P450, DOI 10.1109/TCSVT.2004.825555
   LIM KW, 1995, IEEE T IMAGE PROCESS, V4, P1146, DOI 10.1109/83.403420
   List P, 2003, IEEE T CIRC SYST VID, V13, P614, DOI 10.1109/TCSVT.2003.815175
   Liu SZ, 2002, IEEE T CIRC SYST VID, V12, P1139, DOI 10.1109/TCSVT.2002.806819
   Luo Y, 2003, IEEE T IMAGE PROCESS, V12, P838, DOI 10.1109/TIP.2003.814252
   MINAMI S, 1995, IEEE T CIRC SYST VID, V5, P74, DOI 10.1109/76.388056
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Nosratinia A, 2001, J VLSI SIG PROCESS S, V27, P69, DOI 10.1023/A:1008167430544
   ORourke TP, 1995, IEEE T CIRC SYST VID, V5, P490, DOI 10.1109/76.475891
   Paek H, 1998, IEEE T CIRC SYST VID, V8, P358, DOI 10.1109/76.678636
   Park HW, 1999, IEEE T CIRC SYST VID, V9, P161, DOI 10.1109/76.744283
   Robertson MA, 2005, IEEE T CIRC SYST VID, V15, P27, DOI 10.1109/TCSVT.2004.839995
   Robertson MA, 2001, IEEE T CIRC SYST VID, V11, P1121, DOI 10.1109/76.954498
   Samadani R, 2004, IEEE IMAGE PROC, P1799
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Sun DQ, 2007, IEEE T IMAGE PROCESS, V16, P2743, DOI 10.1109/TIP.2007.904969
   Triantafyllidis GA, 2002, IEEE T CIRC SYST VID, V12, P877, DOI 10.1109/TCSVT.2002.804880
   Tsai CY, 2013, IEEE J-STSP, V7, P934, DOI 10.1109/JSTSP.2013.2271974
   Vo DT, 2009, IEEE T IMAGE PROCESS, V18, P1166, DOI 10.1109/TIP.2009.2017341
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang C, 2013, SIGNAL PROCESS-IMAGE, V28, P522, DOI 10.1016/j.image.2013.01.006
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xinfeng Zhang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P836, DOI 10.1109/ICME.2012.159
   Xiong ZX, 1997, IEEE T CIRC SYST VID, V7, P433, DOI 10.1109/76.564123
   Yang JX, 2010, IEEE T CIRC SYST VID, V20, P458, DOI 10.1109/TCSVT.2009.2035850
   Yang YY, 1993, IEEE T CIRC SYST VID, V3, P421, DOI 10.1109/76.260198
   YANG YY, 1995, IEEE T IMAGE PROCESS, V4, P896, DOI 10.1109/83.392332
   Yim C, 2011, IEEE T IMAGE PROCESS, V20, P88, DOI 10.1109/TIP.2010.2061859
   Yoo S. B., 2011, P IEEE ICIP, P1541
   Yunfei Zheng, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3501, DOI 10.1109/ICIP.2011.6116469
   Zakhor A, 1992, IEEE T CIRC SYST VID, V2, P91, DOI 10.1109/76.134377
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P735, DOI 10.1109/TMM.2008.922849
   Zhai GT, 2008, IEEE T CIRC SYST VID, V18, P122, DOI 10.1109/TCSVT.2007.906942
NR 55
TC 41
Z9 41
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1536
EP 1548
DI 10.1109/TMM.2014.2327563
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200004
DA 2024-07-18
ER

PT J
AU Jiang, JJ
   Hu, RM
   Wang, ZY
   Han, Z
AF Jiang, Junjun
   Hu, Ruimin
   Wang, Zhongyuan
   Han, Zhen
TI Noise Robust Face Hallucination via Locality-Constrained Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face hallucination; locality-constrained representation; neighbor
   embedding; position-patch; sparse representation; super-resolution
ID SUPERRESOLUTION; EQUATIONS; SYSTEMS
AB Recently, position-patch based approaches have been proposed to replace the probabilistic graph-based or manifold learning-based models for face hallucination. In order to obtain the optimal weights of face hallucination, these approaches represent one image patch through other patches at the same position of training faces by employing least square estimation or sparse coding. However, they cannot provide unbiased approximations or satisfy rational priors, thus the obtained representation is not satisfactory. In this paper, we propose a simpler yet more effective scheme called Locality-constrained Representation (LcR). Compared with Least Square Representation (LSR) and Sparse Representation (SR), our scheme incorporates a locality constraint into the least square inversion problem to maintain locality and sparsity simultaneously. Our scheme is capable of capturing the non-linear manifold structure of image patch samples while exploiting the sparse property of the redundant data representation. Moreover, when the locality constraint is satisfied, face hallucination is robust to noise, a property that is desirable for video surveillance applications. A statistical analysis of the properties of LcR is given together with experimental results on some public face databases and surveillance images to show the superiority of our proposed scheme over state-of-the-art face hallucination approaches.
C1 [Jiang, Junjun; Hu, Ruimin; Wang, Zhongyuan; Han, Zhen] Wuhan Univ, Sch Comp, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Hu, RM (corresponding author), Wuhan Univ, Sch Comp, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Peoples R China.
EM junjun0595@163.com; hrm1964@163.com; wzy_hope@163.com;
   hanzhen_1980@163.com
RI Wang, Zhongyuan/ABD-2189-2020; Jiang, Junjun/L-7087-2019
OI Jiang, Junjun/0000-0002-5694-505X
FU major national science and technology special projects
   [2010ZX03004-003-03]; National Key Technologies RD Program
   [2013AA014602]; National Natural Science Foundation of China [61231015,
   61070080, 61003184, 61172173, 61303114, 61170023]; China Postdoctoral
   Science Foundation [2013M530350]
FX This work was supported by the major national science and technology
   special projects (2010ZX03004-003-03), the National Key Technologies R&D
   Program (2013AA014602), the National Natural Science Foundation of China
   (61231015, 61070080, 61003184, 61172173, 61303114, and 61170023), and
   the China Postdoctoral Science Foundation funded project (2013M530350).
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Shin'ichi Satoh.
CR [Anonymous], 2009, P ADV NEUR INF PROC
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], PATTERN RECOGNIT
   Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Bruckstein AM, 2009, SIAM REV, V51, P34, DOI 10.1137/060657704
   Candes E., 2005, l1-magic: Recovery of sparse signals via convex programming
   Chakrabarti A, 2007, IEEE T MULTIMEDIA, V9, P888, DOI 10.1109/TMM.2007.893346
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Gajjar PP, 2010, IEEE T IMAGE PROCESS, V19, P1201, DOI 10.1109/TIP.2010.2041408
   Gao XB, 2012, IEEE T IMAGE PROCESS, V21, P469, DOI 10.1109/TIP.2011.2161482
   Jia K, 2008, IEEE T IMAGE PROCESS, V17, P873, DOI 10.1109/TIP.2008.922421
   Jung CK, 2011, IEEE SIGNAL PROC LET, V18, P367, DOI 10.1109/LSP.2011.2140370
   Junjun Jiang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P212, DOI 10.1109/ICME.2012.152
   Li B, 2009, IEEE SIGNAL PROC LET, V16, P957, DOI 10.1109/LSP.2009.2027657
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Ma X, 2009, IEEE INT CON MULTI, P290, DOI 10.1109/ICME.2009.5202492
   Ma X, 2010, IEEE SIGNAL PROC LET, V17, P579, DOI 10.1109/LSP.2010.2047317
   Park JS, 2008, IEEE T IMAGE PROCESS, V17, P1806, DOI 10.1109/TIP.2008.2001394
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Thomaz CE, 2010, IMAGE VISION COMPUT, V28, P902, DOI 10.1016/j.imavis.2009.11.005
   Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Yang CY, 2013, PROC CVPR IEEE, P1099, DOI 10.1109/CVPR.2013.146
   Yang J, 2008, IEEE IMAGE PROC, P1264, DOI 10.1109/ICIP.2008.4711992
   Zhang W., 2009, PROC IEEE INT C COMP
   Zhang W, 2011, IEEE T IMAGE PROCESS, V20, P2769, DOI 10.1109/TIP.2011.2142001
   Zhou DY, 2004, ADV NEUR IN, V16, P169
   Zonoobi D, 2011, IEEE J-STSP, V5, P927, DOI 10.1109/JSTSP.2011.2160711
NR 34
TC 180
Z9 188
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1268
EP 1281
DI 10.1109/TMM.2014.2311320
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600009
DA 2024-07-18
ER

PT J
AU Wang, S
   Zheng, D
   Zhao, JY
   Tam, WJ
   Speranza, F
AF Wang, Sha
   Zheng, Dong
   Zhao, Jiying
   Tam, Wa James
   Speranza, Filippo
TI Adaptive Watermarking and Tree Structure Based Image Quality Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bitplane decomposition; complexity analysis; DWT based watermark
   embedding; HVS masking; SPIHT tree structure; watermarking based image
   quality estimation
ID NO REFERENCE
AB Image quality evaluation is very important. In applications involving signal transmission, the Reduced- or No-Reference quality metrics are generally more practical than the Full-Reference metrics. In this study, we propose a quality estimation method based on a novel semi-fragile and adaptive watermarking scheme. The proposed scheme uses the embedded watermark to estimate the degradation of cover image under different distortions. The watermarking process is implemented in DWT domain of the cover image. The correlated DWT coefficients across the DWT sub-bands are categorized into Set Partitioning in Hierarchical Trees (SPIHT). Those SPHIT trees are further decomposed into a set of bitplanes. The watermark is embedded into the selected bitplanes of the selected DWT coefficients of the selected tree without causing significant fidelity loss to the cover image. The accuracy of the quality estimation is made to approach that of Full-Reference metrics by referring to an "Ideal Mapping Curve" computed a priori. The experimental results show that the proposed scheme can estimate image quality in terms of PSNR, wPSNR, JND and SSIM with high accuracy under JPEG compression, JPEG2000 compression, Gaussian low-pass filtering and Gaussian noise distortion. The results also show that the proposed scheme has good computational efficiency for practical applications.
C1 [Wang, Sha; Zhao, Jiying] Univ Ottawa, Sch Elect Engn & Comp Sci, Ottawa, ON K1N 6N5, Canada.
   [Zheng, Dong; Tam, Wa James; Speranza, Filippo] Commun Res Ctr Canada, Ottawa, ON K2H 8S2, Canada.
C3 University of Ottawa; Communications Research Centre Canada
RP Wang, S (corresponding author), Univ Ottawa, Sch Elect Engn & Comp Sci, Ottawa, ON K1N 6N5, Canada.
EM shawang@eecs.uottawa.ca; dong.zheng@crc.ca; jyzhao@eecs.uottawa.ca;
   james.tam@crc.ca; filippo.speranza@crc.ca
RI Zheng, Dong/G-9278-2012
CR Altous S, 2011, ELMAR PROC, P97
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   Barni M, 2001, IEEE T IMAGE PROCESS, V10, P783, DOI 10.1109/83.918570
   Battisti F., 2012, P 2 JOINT INT C MULT, P1
   Bhattacharya A, 2011, INT SYMP IMAGE SIG, P431
   Carnec M, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P477, DOI 10.1109/ISSPA.2003.1224743
   Cox IJ, 2008, MKS MULTIMED INFORM, P1
   Farias MCQ, 2005, IEEE T CONSUM ELECTR, V51, P983, DOI 10.1109/TCE.2005.1510512
   Jamzad M., 2006, SCIENTIA IRANICA JOU, V13, P404
   Kusuma TM, 2005, 2005 SYSTEMS COMMUNICATIONS, PROCEEDINGS, P178, DOI 10.1109/ICW.2005.60
   Le Callet P, 2006, IEICE T COMMUN, VE89B, P289, DOI 10.1093/ietcom/e89-b.2.289
   Lewis AS, 1992, IEEE T IMAGE PROCESS, V1, P244, DOI 10.1109/83.136601
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Marziliano P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P57, DOI 10.1109/ICIP.2002.1038902
   Mishra A., 2011, "Int.J. Image Process., V5, P199
   Nezhadarya E, 2009, IEEE IMAGE PROC, P2233, DOI 10.1109/ICIP.2009.5413955
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Sheskin D. J., 2000, Handbook of Parametric and Nonparametric Statistical Procedures
   Shnayderman A, 2006, IEEE T IMAGE PROCESS, V15, P422, DOI 10.1109/TIP.2005.860605
   Turaga DS, 2004, SIGNAL PROCESS-IMAGE, V19, P173, DOI 10.1016/j.image.2003.09.001
   W Ci, IEEE MULTIM IN PRESS
   Wang S, 2007, IEEE T CIRC SYST VID, V17, P98, DOI 10.1109/TCSVT.2006.887086
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2006, IEEE T IMAGE PROCESS, V15, P1680, DOI 10.1109/TIP.2005.864165
   Watson AB, 2001, J ELECTRON IMAGING, V10, P20, DOI 10.1117/1.1329896
   WOLF S, 1991, IEEE PACIF, P477, DOI 10.1109/PACRIM.1991.160780
   Wu HR, 1997, IEEE SIGNAL PROC LET, V4, P317, DOI 10.1109/97.641398
   Zhai GT, 2006, IEEE INT SYMP CIRC S, P1715
   Zheng D, 2003, 2ND IEEE INTERNATIONAL WORKSHOP ON HAPTIC, AUDIO AND VISUAL ENVIRONMENTS AND THEIR APPLICATIONS - HAVE 2003, P65
NR 29
TC 41
Z9 44
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 311
EP 325
DI 10.1109/TMM.2013.2291658
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800003
DA 2024-07-18
ER

PT J
AU Shahid, Z
   Puech, W
AF Shahid, Zafar
   Puech, William
TI Visual Protection of HEVC Video by Selective Encryption of CABAC
   Binstrings
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE AES-CFB; CABAC; HEVC; non-dyadic encryption space; selective encryption;
   truncated rice code
ID ACCESS-CONTROL; DESIGN
AB This paper presents one of the first methods allowing the protection of the newly emerging video codec HEVC (High Efficiency Video Coding). Visual protection is achieved through selective encryption (SE) of HEVC-CABAC binstrings in a format compliant manner. The SE approach developed for HEVC is different from that of H. 264/AVC in several aspects. Truncated rice code is introduced for binarization of quantized transform coefficients (QTCs) instead of truncated unary code. The encryption space (ES) of binstrings of truncated rice codes is not always dyadic and cannot be represented by an integer number of bits. Hence they cannot be concatenated together to create plaintext for the CFB (Cipher Feedback) mode of AES, which is a self-synchronizing stream cipher for so-called AES-CFB. Another challenge for SE in HEVC concerns the introduction of context, which is adaptive to QTC. This work presents a thorough investigation of HEVC-CABAC from an encryption standpoint. An algorithm is devised for conversion of non-dyadic ES to dyadic, which can be concatenated to form plaintext for AES-CFB. For selectively encrypted binstrings, the context of truncated rice code for binarization of future syntax elements is guaranteed to remain unchanged. Hence the encrypted bitstream is format-compliant and has exactly the same bit-rate. The proposed technique requires very little processing power and is ideal for playback on hand held devices. The proposed scheme is acceptable for DRM of a wide range of applications, since it protects the contour and motion information, along with texture. Several benchmark video sequences of different resolutions and diverse contents were used for experimental evaluation of the proposed algorithm. A detailed security analysis of the proposed scheme verified the validity of the proposed encryption scheme for content protection in a wide range of applications.
C1 [Shahid, Zafar; Puech, William] Univ Montpellier 2, Lab LIRMM, UMR 5506, CNRS, F-34392 Montpellier 05, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de Montpellier
RP Shahid, Z (corresponding author), Univ Montpellier 2, Lab LIRMM, UMR 5506, CNRS, F-34392 Montpellier 05, France.
EM zafar.shahid@lirmm.fr; william.puech@lirmm.fr
OI Puech, William/0000-0001-9383-2401; Shahid, Zafar/0000-0002-3759-1591
CR Ahmed HAM, 2007, INZ MINER, P1
   [Anonymous], ICIP 2005
   [Anonymous], P 14 ANN ACM INT C M
   [Anonymous], WORKSH MULT SIGN PRO
   Asghar M. N., 2012, 2012 IEEE 11th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom), P443, DOI 10.1109/TrustCom.2012.268
   Au Yeung SK, 2011, INT CONF ACOUST SPEE, P2436
   Baroncini V, 2010, JCTVCA204 ITUTISOIEC
   Carrillo P., 2009, EURASIP J INF SECURI, V2009, P13
   Dubois L, 2011, EUR SIGNAL PR CONF, P2185
   Dufaux F, 2008, IEEE T CIRC SYST VID, V18, P1168, DOI 10.1109/TCSVT.2008.928225
   Grangetto M, 2006, IEEE T MULTIMEDIA, V8, P905, DOI 10.1109/TMM.2006.879919
   HEVC, 2012, JCTVCH1003 HEVC ITUT
   Jakimoski G, 2008, IEEE T MULTIMEDIA, V10, P330, DOI 10.1109/TMM.2008.917355
   Jiang JG, 2009, MINES 2009: FIRST INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION NETWORKING AND SECURITY, VOL 1, PROCEEDINGS, P478, DOI 10.1109/MINES.2009.26
   Kim C., 2012, JCTVC10124 ITUTISOIE
   Li CH, 2008, LECT NOTES COMPUT SC, V5353, P496
   Lian SG, 2005, LECT NOTES COMPUT SC, V3768, P281, DOI 10.1007/11582267_25
   Lian SG, 2007, IEEE T CIRC SYST VID, V17, P774, DOI 10.1109/TCSVT.2007.896635
   Nguyen T., 2011, JCTVCE253 ITUTISOIEC
   Park SW, 2009, IEICE T INF SYST, VE92D, P851, DOI 10.1587/transinf.E92.D.851
   Shahid Z., 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P2201
   Shahid Z, 2011, IEEE T CIRC SYST VID, V21, P565, DOI 10.1109/TCSVT.2011.2129090
   Sohn H, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P424, DOI 10.1109/AVSS.2009.48
   Taneja N, 2011, INT J WAVELETS MULTI, V9, P317, DOI 10.1142/S0219691311004092
   Ugur K., 2010, JCTVCA119 JVT
   Wang DY, 2009, MINES 2009: FIRST INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION NETWORKING AND SECURITY, VOL 2, PROCEEDINGS, P99, DOI 10.1109/MINES.2009.186
   Wang JD, 2007, ASICON 2007: 2007 7TH INTERNATIONAL CONFERENCE ON ASIC, VOLS 1 AND 2, PROCEEDINGS, P802, DOI 10.1109/ICASIC.2007.4415752
   Wen JT, 2006, IEEE SIGNAL PROC LET, V13, P69, DOI 10.1109/LSP.2005.861589
   Winken Martin, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3693, DOI 10.1109/ICIP.2011.6116521
   Won YG, 2006, LECT NOTES COMPUT SC, V4283, P407
   Wu CP, 2005, IEEE T MULTIMEDIA, V7, P828, DOI 10.1109/TMM.2005.854469
   Yeung SKA, 2011, IEEE T CIRC SYST VID, V21, P1341, DOI 10.1109/TCSVT.2011.2125630
NR 32
TC 91
Z9 94
U1 3
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 24
EP 36
DI 10.1109/TMM.2013.2281029
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100003
OA Green Published
DA 2024-07-18
ER

PT J
AU Vukobratovic, D
   Khirallah, C
   Stankovic, V
   Thompson, JS
AF Vukobratovic, Dejan
   Khirallah, Chadi
   Stankovic, Vladimir
   Thompson, John S.
TI Random Network Coding for Multimedia Delivery Services in
   LTE/LTE-Advanced
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE LTE/LTE-A; random network coding; wireless video delivery
ID LAYER
AB Random Network Coding (RNC) has recently been investigated as a promising solution for reliable multimedia delivery over wireless networks. RNC possess the potential for flexible and adaptive matching of packet-level error resilience to both video content importance and variable wireless channel conditions. As the demand for massive multimedia delivery over fourth generation wireless cellular standards such as Long-Term Evolution (LTE)/LTE-Advanced (LTE-A) increases, novel video-aware transmission techniques are needed. In this paper, we investigate RNC as one such promising technique, building upon our recent work on RNC integration within the LTE/LTE-A Radio Access Network at the Multiple Access Control (MAC) layer (MAC-RNC). The paper argues that the proposed MAC-RNC solution provides fundamentally new set of opportunities for dynamic collaborative transmission, content awareness, resource allocation and unequal error protection (UEP) necessary for efficient wireless multimedia delivery in LTE/LTE-A.
C1 [Vukobratovic, Dejan] Univ Novi Sad, Dept Power Elect & Commun Engn, Novi Sad 21000, Serbia.
   [Khirallah, Chadi; Thompson, John S.] Univ Edinburgh, Sch Engn & Elect, Edinburgh, Midlothian, Scotland.
   [Stankovic, Vladimir] Univ Strathclyde, Dept Elect & Elect Engn, Glasgow G1 1XW, Lanark, Scotland.
C3 University of Novi Sad; University of Edinburgh; University of
   Strathclyde
RP Vukobratovic, D (corresponding author), Univ Novi Sad, Dept Power Elect & Commun Engn, Novi Sad 21000, Serbia.
EM de-janv@uns.ac.rs; c.khi-rallah@ed.ac.uk;
   vladimir.stankovic@eee.strath.ac.uk; john.thompson@ed.ac.uk
RI Vukobratovic, Dejan/HHZ-7827-2022; Stankovic, Vladimir/L-6584-2016
OI Vukobratovic, Dejan/0000-0002-5305-8420; Stankovic,
   Vladimir/0000-0002-1075-2420
FU EPSRC Sensor Signal Processing Platform Grant [EP/J015180/1]; Marie
   Curie European Reintegration Grant FP7-PEOPLE-ERG MMCODESTREAM within
   the 7th EU Framework Programme; EPSRC [EP/J015180/1, EP/G060584/1]
   Funding Source: UKRI
FX This work was supported in part by the EPSRC Sensor Signal Processing
   Platform Grant EP/J015180/1. The work of D. Vukobratovic was supported
   by a Marie Curie European Reintegration Grant FP7-PEOPLE-ERG-2010
   MMCODESTREAM within the 7th EU Framework Programme. This paper is an
   extended version of the original paper which appeared in the Proceedings
   of IEEE ICME 2012 Conference and was among the top-rated 4% of ICME'12
   submissions. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Enrico Magli.
CR *3GP DASH, 2012, 26247 ETSI TS
   ANGELOPOULOS G, 2011, P IFIP NET WORKING 2
   [Anonymous], 2009, 36913 3GPP TR
   CHOU PA, 2003, P ALL C 2003 MONT IL
   Chou PA, 2007, IEEE SIGNAL PROC MAG, V24, P77, DOI 10.1109/MSP.2007.904818
   Fragouli C, 2007, 2007 41ST ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1 AND 2, P248, DOI 10.1109/CISS.2007.4298308
   Gruber M, 2011, IEEE COMMUN MAG, V49, P176, DOI 10.1109/MCOM.2011.6094023
   Holma H., 2011, LTE for UMTS: Evolution to LTE-advanced, V2nd
   JIN J, 2008, P IEEE INFOCOM2008 P
   Khandekar Aamod, 2010, 2010 European Wireless Conference (EW), P978, DOI 10.1109/EW.2010.5483516
   Khirallah C, 2012, IEEE T WIREL COMMUN, V11, P4275, DOI 10.1109/TWC.2012.102612.111380
   Larmo A, 2009, IEEE COMMUN MAG, V47, P52, DOI 10.1109/MCOM.2009.4907407
   Liu X, 2009, IEEE T MULTIMEDIA, V11, P730, DOI 10.1109/TMM.2009.2017636
   Liva G, 2010, IEEE COMMUN LETT, V14, P178, DOI 10.1109/LCOMM.2010.02.092080
   Luo HY, 2010, IEEE COMMUN MAG, V48, P102, DOI 10.1109/MCOM.2010.5402671
   MUNARETTO D, 2011, SPRINGER MOBILE NETW, V16
   Oyman O, 2010, IEEE COMMUN MAG, V48, P68, DOI 10.1109/MCOM.2010.5534589
   SCHOTSCH B, 2011, P ALL 2011 US SEP
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   SEFEROGLU H, 2011, P ALL 2011 US SEP
   SHOJANIA H, 2009, P ACM NOSSDAV 2009 W
   Stockhammer T, 2008, INTERNET COMMUN, P239
   TASSI A, 2013, P IEEE ICC 2013 BUD
   TEERAPITTAYANON S, 2012, P MACOM 2012 MAYN IR
   VINGELMANN P, 2011, P IEEE CCNC 2011 LAS
   Vukobratovic D, 2012, IEEE T COMMUN, V60, P1243, DOI 10.1109/TCOMM.2012.030712.100454
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zhang QQ, 1999, IEEE T COMMUN, V47, P1688, DOI 10.1109/26.803503
   2011, 26346 ETSI TS
   2011, 26234 ETSI TS
NR 30
TC 29
Z9 29
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 277
EP 282
DI 10.1109/TMM.2013.2282129
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100024
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Wu, JZ
   Hu, DW
AF Wu, Jianzhai
   Hu, Dewen
TI Learning Effective Event Models to Recognize a Large Number of Human
   Actions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; latent SVM; quadratic programming (QP); temporal
   pyramid model (TPM)
ID REPRESENTATION
AB Human action recognition in videos is an important problem in computer vision, but it is very challenging, especially when recognizing a large number of human actions. First, it is difficult to capture the crucial motion patterns that discriminate among these actions. Second, the method should be scalable for large datasets because more training examples are often collected for more action classes. In this paper, we employ latent models to capture the crucial motion patterns, and we propose an effective learning algorithm that can efficiently address large datasets. To capture the crucial motion patterns, we define an "event" for each category, and we add a latent variable that indicates the start of the event. The event has a length of several frames that can differ across the categories. To train effective latent models for a large number of action classes, we employ amulti-class formulation with latent variables, and we address this problem by solving a dual quadratic programming (QP) problem with linear inequality constraints. To make the algorithm scalable for large datasets, we propose an improved QP solver that converges quickly for large QP problems that have a very large number of linear inequality constraints in real-world applications. We examine the proposed approach on the HMDB51 and UCF50 datasets. Comparison results have been reported to demonstrate the effectiveness of the proposed technique. Our approach outperforms state-of-the-art results for both datasets.
C1 [Wu, Jianzhai; Hu, Dewen] Natl Univ Def Technol, Dept Automat Control, Coll Mechatron Engn & Automat, Changsha 410073, Hunan, Peoples R China.
C3 National University of Defense Technology - China
RP Wu, JZ (corresponding author), Natl Univ Def Technol, Dept Automat Control, Coll Mechatron Engn & Automat, Changsha 410073, Hunan, Peoples R China.
EM wjz_gfkd@163.com; dwhu@nudt.edu.cn
RI Hu, Dewen/AAN-8511-2020; Hu, Dewen/D-1978-2015
FU National Basic Research Program of China [2013CB329401, 2011CB707802];
   Natural Science Foundation of China [61375034]; National High Technology
   Research and Development Program of China [2012AA011601]
FX This work was supported in part by the National Basic Research Program
   of China under Grants 2013CB329401, 2011CB707802, the Natural Science
   Foundation of China under Grant (61375034, and the National High
   Technology Research and Development Program of China under Grant
   2012AA011601. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Cees Snoek.
CR [Anonymous], P ICCV
   [Anonymous], 1999, Advances in kernel methods: Support vector learning
   [Anonymous], 2011, P CVPR
   [Anonymous], 2008, P CVPR
   [Anonymous], 2009, P BMVC
   [Anonymous], 2006, IEEE COMP SOC C COMP
   [Anonymous], 2005, P CVPR
   [Anonymous], P ECCV
   [Anonymous], P CVPR
   [Anonymous], 2008, P CVPR
   [Anonymous], 2009, P CVPR
   [Anonymous], P CVPR
   Ballan L, 2012, IEEE T MULTIMEDIA, V14, P1234, DOI 10.1109/TMM.2012.2191268
   Blank M., 2005, P ICCV
   Cui P, 2012, IEEE T MULTIMEDIA, V14, P102, DOI 10.1109/TMM.2011.2176110
   Fan R.-E., 2008, J MACH LEARN RES
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Franc V., 2006, A Novel Algorithm for Learning Support Vector Machines with Structured Output Spaces
   Ikizler-Cinbis N, 2012, IEEE T MULTIMEDIA, V14, P1031, DOI 10.1109/TMM.2012.2187180
   Jhuang H., 2007, P ICCV
   Jiang Y.-G., 2012, P ECCV
   Kliper-Gross O., 2012, P ECCV
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Liu J., 2008, P CVPR
   Liu J., 2009, P CVPR
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Messing C. P. R., 2009, P ICCV
   Niebles J. C., 2010, P ECCV
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Sadanand S., 2012, P CVPR
   Schuldt C., 2004, P ICPR
   Scovanner P., 2007, P ACM MULT
   Taskar B., 2004, P NIPS
   Tsochantaridis I., 2004, P ICML
   Wang F, 2012, IEEE T MULTIMEDIA, V14, P76, DOI 10.1109/TMM.2011.2165531
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P975, DOI 10.1109/TMM.2012.2185041
   Wang Y., 2009, P CVPR
   Wang Y, 2011, IEEE T PATTERN ANAL, V33, P1310, DOI 10.1109/TPAMI.2010.214
   Yu Chun-Nam John, 2009, P ICML
   Yuille A. L., 2001, Advances in Neural Information Processing Systems, P1033
NR 40
TC 16
Z9 16
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 147
EP 158
DI 10.1109/TMM.2013.2283846
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100013
DA 2024-07-18
ER

PT J
AU Chan, SHG
   Xu, ZL
AF Chan, S. -H. Gary
   Xu, Zhuolin
TI LP-SR: Approaching Optimal Storage and Retrieval for Video-on-Demand
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distributed video-on-demand; linear programming; optimization; segment
   storage and retrieval
ID DISTRIBUTED SERVERS ARCHITECTURE; REPLICATION
AB In a distributed large-scale video-on-demand (VoD) streaming network, a content provider often deploys local servers close to their users. A movie is partitioned into kappa segments which the servers collaboratively replicate and retrieve (kappa >= 1). A critical but challenging problem is how to minimize overall system deployment cost consisting of server bandwidth, server storage, and network traffic among servers. In this paper, we address this problem through jointly optimizing movie storage and retrieval in the server network. We first formulate the optimization problem and show that it is NP-hard. To address the problem, we propose a novel, effective and implementable heuristic termed LP-SR. LP-SR decomposes the optimization problem into two computationally efficient linear programs (LPs) for segment storage and retrieval, respectively. The strength of LP-SR is that it is asymptotically optimal in terms of kappa, and kappa is not high to be closely optimal (around 5 to 10 in our study). For large movie pool, we propose a movie grouping algorithm to further reduce the computational complexity without compromising much on the performance. Through extensive simulation, LP-SR is shown to perform significantly the best as compared with other state-of-the-art and traditional schemes, reducing the deployment cost by a wide margin (by multiple times in many cases). It attains performance very close to the global optimum.
C1 [Chan, S. -H. Gary; Xu, Zhuolin] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Chan, SHG (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
EM gchan@cse.ust.hk
OI Chan, Gary Shueng Han/0000-0003-4207-764X
CR Alam S., 2005, 2005 2nd International Conference on Broadband Networks (Broadnets) (IEEE Cat. No. 05EX1116), P896
   Annapureddy S., 2007, Proc. Int'l WWW Conference, P903
   [Anonymous], P 6 INT C CONEXT 10
   [Anonymous], IEEE INT C COMM ICC
   [Anonymous], MULT SIGN PROC 2005
   [Anonymous], P GLOB 2012 COMM SOF
   [Anonymous], MSRTR2005147
   [Anonymous], IEEE ACM T NETW
   [Anonymous], 1997, THESIS ERASMUS U ROT
   [Anonymous], MODELING ANAL INTRO
   [Anonymous], P IEEE INT C INT MUL
   Borst S, 2010, IEEE INFOCOM SER, DOI 10.1109/infcom.2010.5461964
   Chan SHG, 2001, IEEE ACM T NETWORK, V9, P125, DOI 10.1109/90.917070
   Chan SHG, 2001, IEEE COMMUN LETT, V5, P384, DOI 10.1109/4234.951385
   Choe Y.R., 2007, MULTIMEDIA 07 P 15 I, P117
   Dai J, 2012, IEEE INFOCOM SER, P2444, DOI 10.1109/INFCOM.2012.6195634
   Grant M., 2020, CVX MATLAB SOFTWARE
   Haitao Li, 2011, Proceedings of the 2011 IEEE 4th International Conference on Cloud Computing (CLOUD 2011), P203, DOI 10.1109/CLOUD.2011.41
   He YF, 2009, IEEE T MULTIMEDIA, V11, P138, DOI 10.1109/TMM.2008.2008929
   Hefeeda M, 2010, IEEE T PARALL DISTR, V21, P998, DOI 10.1109/TPDS.2009.130
   Huang C, 2007, ACM SIGCOMM COMP COM, V37, P133, DOI 10.1145/1282427.1282396
   Kangasharju J, 2007, IEEE INFOCOM SER, P1973, DOI 10.1109/INFCOM.2007.229
   Lv JM, 2007, EIGHTH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED COMPUTING, APPLICATIONS AND TECHNOLOGIES, PROCEEDINGS, P501, DOI 10.1109/.27
   Mehyar M, 2007, IEEE ACM T NETWORK, V15, P512, DOI 10.1109/TNET.2007.893226
   Niu D, 2011, IEEE INFOCOM SER, P421, DOI 10.1109/INFCOM.2011.5935196
   Thomas M. U., 1976, Queueing Systems, V18, P512
   Vinay A., 2010, 2010 International Conference on Communication Control and Computing Technologies, P425, DOI 10.1109/ICCCCT.2010.5670589
   Wu WJ, 2011, IEEE INFOCOM SER, P1206, DOI 10.1109/INFCOM.2011.5934900
   Yiu WPK, 2007, IEEE J SEL AREA COMM, V25, P1717, DOI 10.1109/JSAC.2007.071210
   Zaman S, 2011, IEEE T PARALL DISTR, V22, P1455, DOI 10.1109/TPDS.2011.27
   Zhou YP, 2012, IEEE INFOCOM SER, P1530, DOI 10.1109/INFCOM.2012.6195520
NR 31
TC 3
Z9 3
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2125
EP 2136
DI 10.1109/TMM.2013.2280989
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900032
OA Green Published
DA 2024-07-18
ER

PT J
AU Song, JK
   Yang, Y
   Huang, Z
   Shen, HT
   Luo, JB
AF Song, Jingkuan
   Yang, Yi
   Huang, Zi
   Shen, Heng Tao
   Luo, Jiebo
TI Effective Multiple Feature Hashing for Large-Scale Near-Duplicate Video
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hashing; manifold learning; near-duplicate video retrieval;
   optimization; video indexing.
AB Near-duplicate video retrieval (NDVR) has recently attracted much research attention due to the exponential growth of online videos. It has many applications, such as copyright protection, automatic video tagging and online video monitoring. Many existing approaches use only a single feature to represent a video for NDVR. However, a single feature is often insufficient to characterize the video content. Moreover, while the accuracy is the main concern in previous literatures, the scalability of NDVR algorithms for large scale video datasets has been rarely addressed. In this paper, we present a novel approach-Multiple Feature Hashing (MFH) to tackle both the accuracy and the scalability issues of NDVR. MFH preserves the local structural information of each individual feature and also globally considers the local structures for all the features to learn a group of hash functions to map the video keyframes into the Hamming space and generate a series of binary codes to represent the video dataset. We evaluate our approach on a public video dataset and a large scale video dataset consisting of 132,647 videos collected from YouTube by ourselves. This dataset has been released (http://itee.uq.edu.au/shenht/UQ_VIDEO/). The experimental results show that the proposed method outperforms the state-of-the-art techniques in both accuracy and efficiency.
C1 [Song, Jingkuan; Huang, Zi; Shen, Heng Tao] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
   [Yang, Yi] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
C3 University of Queensland; Carnegie Mellon University; University of
   Rochester
RP Song, JK (corresponding author), Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
EM jk.song@itee.uq.edu.au; yiyang@cs.cmu.edu; huang@itee.uq.edu.au;
   shenht@itee.uq.edu.au; jluo@cs.rochester.edu
RI yang, yang/HGT-7999-2022; yang, yang/GWB-9426-2022; Yang,
   Yi/B-9273-2017; Lang, Ming/HIK-0758-2022; Luo, Jiebo/AAI-7549-2020;
   Shen, Heng Tao/ABD-5331-2021; yang, yang/GVT-5210-2022
OI Yang, Yi/0000-0002-0512-880X; Luo, Jiebo/0000-0002-4516-9729; HUANG,
   ZI/0000-0002-9738-4949
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], 2008, Proceedings of the 21st International Conference on Neural Information Processing Systems
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Cherubini Mauro., 2009, Proceedings of the 17th ACM International Conference on Multimedia, number April in MM '09, P35, DOI DOI 10.1145/1631272.1631280
   Cui Bin., 2010, ACM SIGMOD International Conference on Management of Data, P435
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Douze M, 2010, LECT NOTES COMPUT SC, V6311, P522, DOI 10.1007/978-3-642-15549-9_38
   Douze M, 2010, IEEE T MULTIMEDIA, V12, P257, DOI 10.1109/TMM.2010.2046265
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Huang Z., 2011, SIGMOD, P1021
   Huang Z., 2009, TOIS, V27
   Huang Z, 2010, IEEE T MULTIMEDIA, V12, P386, DOI 10.1109/TMM.2010.2050737
   Jain P., 2008, P CVPR, P1
   Kulis B, 2009, IEEE T PATTERN ANAL, V31, P2143, DOI 10.1109/TPAMI.2009.151
   Law-To Julien., 2007, P 6 ACM INT C IMAGE, P371
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Liu J., 2011, TOIS
   Liu J., 2013, ACM COMPUT SURVEYS, V45
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lv Q., 2007, P 33 INT C VER LARG, P950
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Nie FP, 2010, IEEE T IMAGE PROCESS, V19, P1921, DOI 10.1109/TIP.2010.2044958
   PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009
   Poullot S., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3203, DOI 10.1109/ICPR.2010.1139
   Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750
   Shang L., 2010, Proceedings of the International Conference on Multimedia, P531
   Shen H. T., 2007, P ACM MULT C, P164
   Shen H.T., 2007, VLDB, P1374
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Tan H.-K., 2008, P ACM INT C MULT, P861, DOI DOI 10.1145/1459359.1459506
   Tao Y., 2010, ACM TODS, V35
   Tollari S., 2007, CIVR 07, P65
   Torralba A., 2008, Proc. CVPR
   Turnbull D, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P387, DOI 10.1145/1571941.1572009
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wu A. G., 2007, P ACM MM, P218
   Wu XB, 2007, PROCEEDINGS OF THE 2007 INTERNATIONAL CONFERENCE ON AGRICULTURE ENGINEERING, P162
   Wu X, 2009, IEEE T MULTIMEDIA, V11, P196, DOI 10.1109/TMM.2008.2009673
   Wu Z., 2009, Proceedings of the 17th ACM international conference on Multimedia, P549
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Yi., 2009, Proceedings of the 17th ACM international conference on Multimedia, P175
   Yen-Yu Lin, 2005, 13th Annual ACM International Conference on Multimedia, P249, DOI 10.1145/1101149.1101193
   Zhang D., SIGIR, V29
   Zhang D, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P18
   Zhou XM, 2009, IEEE T MULTIMEDIA, V11, P879, DOI 10.1109/TMM.2009.2021794
NR 48
TC 198
Z9 216
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1997
EP 2008
DI 10.1109/TMM.2013.2271746
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900022
DA 2024-07-18
ER

PT J
AU Wan, ZG
   Liu, J
   Zhang, R
   Deng, RH
AF Wan, Zhiguo
   Liu, Jun'e
   Zhang, Rui
   Deng, Robert H.
TI A Collusion-Resistant Conditional Access System for
   Flexible-Pay-Per-Channel Pay-TV Broadcasting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cable TV; Access Control; Communication System Security
ID EFFICIENT KEY DISTRIBUTION
AB Pay-TV broadcasting system is an extensively deployed application that charges users based on their subscription. To ensure security for the Pay-TV broadcasting application, a conditional access system (CAS) is designed to control TV channel/program access to only the authorized subscribers. Several key management schemes with a four-level hierarchical key structure have been proposed. In this paper, we point out a severe security weakness of these schemes against collusion attacks. Then we propose a new CAS scheme with a three-level hierarchical key structure using ciphertext-policy attribute-set-based encryption (ASBE), an extension of ciphertext-policy attribute-based encryption (CP-ABE). Our scheme achieves scalable, flexible, fine-grained, and most importantly, collusion-resistant access control for Pay-TV broadcasting applications. The proposed scheme is designed to support all operations in Pay-TV applications. We then provide a detailed analysis on security and performance of our scheme. We also implement the scheme and it is showed to be both efficient and flexible for Pay-TV broadcasting applications.
C1 [Wan, Zhiguo; Liu, Jun'e] Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Minist Educ, Key Lab Informat Syst Secur, Beijing 100084, Peoples R China.
   [Wan, Zhiguo; Liu, Jun'e] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   [Zhang, Rui] Chinese Acad Sci, IIE, State Key Lab Informat Secur SKLOIS, Beijing 100864, Peoples R China.
   [Deng, Robert H.] Singapore Management Univ, Sch Informat Syst, Singapore, Singapore.
C3 Tsinghua University; Tsinghua University; Chinese Academy of Sciences;
   Singapore Management University
RP Wan, ZG (corresponding author), Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Minist Educ, Key Lab Informat Syst Secur, Beijing 100084, Peoples R China.
EM wanzhiguo@tsinghua.edu.cn; liujune252525@126.com; r-zhang@iie.ac.cn;
   robertdeng@smu.edu.sg
RI DENG, Robert H./E-8547-2012
OI Deng, Robert/0000-0003-3491-8146
FU Scientific Foundation for Returned Overseas Chinese Scholars, Ministry
   of Education; National Natural Science Foundation of China [61003223,
   61128005]; Office of Research, Singapore Management University
FX Manuscript received May 22, 2012; revised October 05, 2012; accepted
   December 03, 2012. Date of publication March 20, 2013; date of current
   version September 13, 2013. This work was supported in part by
   Scientific Foundation for Returned Overseas Chinese Scholars, Ministry
   of Education; the National Natural Science Foundation of China under
   Grant No. 61003223 and No. 61128005. This work is also supported in part
   by the Office of Research, Singapore Management University. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Ton Kalker.
CR [Anonymous], 1992, CONDITIONAL ACCESS B
   [Anonymous], P IEEE S SEC PRIV
   [Anonymous], P ESORICS
   Bobba R, 2010, ACM T INFORM SYST SE, V13, DOI 10.1145/1880022.1880025
   Boneh D, 2005, LECT NOTES COMPUT SC, V3494, P440, DOI 10.1007/11426639_26
   Fiat A., 1993, LECT NOTES COMPUTER, P480, DOI DOI 10.1007/3-540-48329-2
   Huang YL, 2004, IEEE T MULTIMEDIA, V6, P760, DOI 10.1109/TMM.2004.834861
   Kim JY, 2010, IEEE T MULTIMEDIA, V12, P337, DOI 10.1109/TMM.2010.2046362
   LEE W, 1996, P INT C CRYPT INF SE, P82
   Liu BF, 2004, IEEE T CONSUM ELECTR, V50, P632, DOI 10.1109/TCE.2004.1309442
   Naor D., 2001, Advances in Cryptology - CRTPTO 2001. 21st Annual International Cryptology Conference, Proceedings (Lecture Notes in Computer Science Vol.2139), P41
   Sun HM, 2008, IEEE T MULTIMEDIA, V10, P1109, DOI 10.1109/TMM.2008.2001381
   Traynor P., 2008, P NDSS
   Tu FK, 1999, IEEE T CONSUM ELECTR, V45, P151, DOI 10.1109/30.754430
   Wang SY, 2008, IEEE T MULTIMEDIA, V10, P480, DOI 10.1109/TMM.2008.917417
NR 15
TC 11
Z9 13
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1353
EP 1364
DI 10.1109/TMM.2013.2250493
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400012
OA Green Published
DA 2024-07-18
ER

PT J
AU Liu, CL
   Wang, D
   Zhu, J
   Zhang, B
AF Liu, Cailiang
   Wang, Dong
   Zhu, Jun
   Zhang, Bo
TI Learning a Contextual Multi-Thread Model for Movie/TV Scene Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic context; dynamic programming; multiple instance learning; scene
   alignment; scene segmentation; segmentation and grouping
ID VIDEO
AB Compared with general videos, movies and TV shows attract a significantly larger portion of people across time and contain very rich and interesting narrative patterns of shots and scenes. In this paper, we aim to recover the inherent structure of scenes and shots in such video narratives. The obtained structure could be useful for subsequent video analysis tasks such as tracking objects across cuts, action retrieval, as well as enriching user browsing and video editing interfaces. Recent research on this problem has mainly focused on combining multiple cues such as scripts, subtitles, sound, or human faces. However, considering that visual information is sufficient for human to identify scene boundaries and some cues are not always available, we are motivated to design a purely visual approach. Observing that dialog patterns occur frequently in a movie/TV show to form a scene, we propose a probabilistic framework to imitate the authoring process. The multi-thread shot model and contextual visual dynamics are embedded into a unified framework to capture the video hierarchy. We devise an efficient algorithm to jointly learn the parameters of the unified model. Experiments on two large datasets containing six movies and 24 episodes of Lost, a popular TV show with complex plot structures, are conducted. Comparative results show that, leveraging only visual cues, our method could successfully recover complicated shot threads and outperform several approaches. Moreover, our method is fast and advantageous for large-scale computation.
C1 [Liu, Cailiang; Zhu, Jun; Zhang, Bo] Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Tsinghua Natl Lab Informat Sci & Technol TNList, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Wang, Dong] Youku Tudou Inc, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Liu, CL (corresponding author), Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Tsinghua Natl Lab Informat Sci & Technol TNList, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM cl-liu07@mails.ts-inghua.edu.cn; dwang97@gmail.com;
   dcszj@tsinghua.edu.cn; dcszb@tsinghua.edu.cn
FU National Basic Research Program (973 Program) of China [2013CB329403,
   2012CB316301]; National Natural Science Foundation of China [91120011,
   61273023]; Tsinghua University Initiative Scientific Research Program
   [20121088071]
FX This work was supported in part by the National Basic Research Program
   (973 Program) of China under Grants 2013CB329403 and 2012CB316301, the
   National Natural Science Foundation of China under Grants 91120011 and
   61273023, and the Tsinghua University Initiative Scientific Research
   Program 20121088071. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Nicu Sebe.
CR Andrews Stuart, 2002, PROC 25 ANN C NEURAL, P561
   [Anonymous], 2007, P IEEE MILCOM 2007
   [Anonymous], 2001, PROC 18 INT C MACH L
   BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147
   Bordwell D., 2001, FILM ART INTRO
   Cao Y, 2003, LECT NOTES COMPUT SC, V2728, P446
   Chao Liang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3377, DOI 10.1109/CVPR.2011.5995681
   Charniak E, 1997, AI MAG, V18, P33
   Chasanis VT, 2009, IEEE T MULTIMEDIA, V11, P89, DOI 10.1109/TMM.2008.2008924
   Cour T, 2008, LECT NOTES COMPUT SC, V5305, P158, DOI 10.1007/978-3-540-88693-8_12
   DAY WHE, 1984, J CLASSIF, V1, P7, DOI 10.1007/BF01890115
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Goela N, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P532
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   Kender JR, 1998, PROC CVPR IEEE, P367, DOI 10.1109/CVPR.1998.698632
   LEE KF, 1990, SPEECH COMMUN, V9, P497, DOI 10.1016/0167-6393(90)90025-5
   Liang C, 2009, LECT NOTES COMPUT SC, V5879, P917, DOI 10.1007/978-3-642-10467-1_82
   Lienhart R, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P685, DOI 10.1109/MMCS.1999.779282
   Neal RM, 1998, NATO ADV SCI I D-BEH, V89, P355
   Nitanda N, 2005, IEEE INT SYMP CIRC S, P4030, DOI 10.1109/ISCAS.2005.1465515
   Park HJ, 2010, IEEE MTT S INT MICR, P49, DOI 10.1109/MWSYM.2010.5518084
   Parshin V, 2006, LECT NOTES COMPUT SC, V3736, P279
   Permuter H., 2003, P IEEE INT C AC SPEE, V3, P25
   Philbin J., 2008, P CVPR, P1
   Rasheed Z, 2005, IEEE T MULTIMEDIA, V7, P1097, DOI 10.1109/TMM.2005.858392
   Rho S, 2004, 24TH INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING SYSTEMS WORKSHOPS, PROCEEDINGS, P124
   Rui Y, 1999, MULTIMEDIA SYST, V7, P359, DOI 10.1007/s005300050138
   Shu-Ching Chen, 2002, Proceedings 2002 IEEE International Conference on Multimedia and Expo (Cat. No.02TH8604), P365, DOI 10.1109/ICME.2002.1035606
   Sidiropoulos P, 2011, IEEE T CIRC SYST VID, V21, P1163, DOI 10.1109/TCSVT.2011.2138830
   Sundaram H, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1145, DOI 10.1109/ICME.2000.871563
   Tavanapong W, 2004, IEEE T MULTIMEDIA, V6, P517, DOI 10.1109/tmm.2004.830810
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   wah Ngo C., 2004, P ACM INT C IM VID R, P227
   White S, 2005, SIAM PROC S, P274
   Wilson K., 2009, P C STOR RETR IM VID, V7255, P2
   Yeung M, 1998, COMPUT VIS IMAGE UND, V71, P94, DOI 10.1006/cviu.1997.0628
   YEUNG MM, 1995, P SOC PHOTO-OPT INS, V2417, P399, DOI 10.1117/12.206067
   Yuan JH, 2007, IEEE T CIRC SYST VID, V17, P168, DOI 10.1109/TCSVT.2006.888023
   Zhai Y, 2006, IEEE T MULTIMEDIA, V8, P686, DOI 10.1109/TMM.2006.876299
   Zhu YY, 2003, ICCIMA 2003: FIFTH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND MULTIMEDIA APPLICATIONS, PROCEEDINGS, P229
NR 40
TC 25
Z9 25
U1 1
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 884
EP 897
DI 10.1109/TMM.2013.2238522
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500016
OA hybrid
DA 2024-07-18
ER

PT J
AU Andreopoulos, Y
AF Andreopoulos, Yiannis
TI Error Tolerant Multimedia Stream Processing: There's Plenty of Room at
   the Top (of the System Stack)
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer system resilience; error-tolerant multimedia; new
   computation paradigms; stochastic computing; throughput-distortion
   computation
ID RDTC OPTIMIZED COMPRESSION; RATE-DISTORTION; INCREMENTAL REFINEMENT;
   COMPLEXITY; IMAGE; COMPUTATION; ENERGY; SIGNAL; REPRESENTATIONS;
   ARCHITECTURES
AB There is a growing realization that the expected fault rates and energy dissipation stemming from increases in CMOS integration will lead to the abandonment of traditional system reliability in favor of approaches that offer reliability to hardware-induced errors across the application, runtime support, architecture, device and integrated-circuit (IC) layers. Commercial stakeholders of multimedia stream processing (MSP) applications, such as information retrieval, stream mining systems, and high-throughput image and video processing systems already feel the strain of inadequate system-level scaling and robustness under the always-increasing user demand. While such applications can tolerate certain imprecision in their results, today's MSP systems do not support a systematic way to exploit this aspect for cross-layer system resilience. However, research is currently emerging that attempts to utilize the error-tolerant nature of MSP applications for this purpose. This is achieved by modifications to all layers of the system stack, from algorithms and software to the architecture and device layer, and even the IC digital logic synthesis itself. Unlike conventional processing that aims for worst-case performance and accuracy guarantees, error-tolerant MSP attempts to provide guarantees for the expected performance and accuracy. In this paper we review recent advances in this field from an MSP and a system (layer-by-layer) perspective, and attempt to foresee some of the components of future cross-layer error-tolerant system design that may influence the multimedia and the general computing landscape within the next ten years.
C1 UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
C3 University of London; University College London
RP Andreopoulos, Y (corresponding author), UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
EM i.andreopoulos@ucl.ac.uk
CR Agullo E, 2009, J PHYS CONF SER, V180, DOI 10.1088/1742-6596/180/1/012037
   Allen JD, 1996, SIGNAL PROCESS-IMAGE, V8, P3, DOI 10.1016/0923-5965(94)00047-6
   Anam MA, 2012, IEEE T MULTIMEDIA, V14, P797, DOI 10.1109/TMM.2012.2184742
   Anastasia D, 2010, DES AUT TEST EUROPE, P1719
   Anastasia D, 2012, IEEE T SIGNAL PROCES, V60, P2024, DOI 10.1109/TSP.2011.2176337
   Anastasia D, 2010, IEEE T IMAGE PROCESS, V19, P2099, DOI 10.1109/TIP.2010.2045702
   Anastasia D, 2010, IEEE SIGNAL PROC LET, V17, P375, DOI 10.1109/LSP.2010.2041583
   Andreopoulos Y, 2008, IEEE T IMAGE PROCESS, V17, P1685, DOI 10.1109/TIP.2008.2001051
   Andreopoulos Y, 2008, IEEE T SIGNAL PROCES, V56, P140, DOI 10.1109/TSP.2007.906727
   Andreopoulos Y, 2007, IEEE T CIRC SYST VID, V17, P751, DOI 10.1109/TCSVT.2007.896662
   Andreopoulos Y, 2007, IEEE T SIGNAL PROCES, V55, P1967, DOI 10.1109/TSP.2006.890867
   [Anonymous], 2012, P 9 C COMP FRONT CAG, DOI DOI 10.1145/2212908.2212912
   [Anonymous], 2006, Tech. rep.
   Asanovic K., 1991, Experimental determination of precision requirements for back-propagation training of artificial neural networks
   Asthana H, 2011, LECT NOTES COMPUT SC, V6931, P125, DOI 10.1007/978-3-642-23318-0_13
   Austin T, 2004, COMPUTER, V37, P57, DOI 10.1109/MC.2004.1274005
   Bartlett MS, 2002, IEEE T NEURAL NETWOR, V13, P1450, DOI 10.1109/TNN.2002.804287
   Bauermann I, 2008, IEEE T IMAGE PROCESS, V17, P724, DOI 10.1109/TIP.2008.920501
   Bauermann I, 2008, IEEE T IMAGE PROCESS, V17, P709, DOI 10.1109/TIP.2008.918962
   Bhattacharyya SS, 2011, J SIGNAL PROCESS SYS, V63, P251, DOI 10.1007/s11265-009-0399-3
   Breuer M, 2010, DES AUT CON, P871
   Buatois L, 2009, INT J PARALLEL EMERG, V24, P205, DOI 10.1080/17445760802337010
   Cao Z, 2010, IEEE T CIRCUITS-I, V57, P681, DOI 10.1109/TCSI.2009.2023941
   Carter N., 2010, P DES AUT TEST EUR C, P1023
   Chandrakasan A, 1996, 1996 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN - DIGEST OF TECHNICAL PAPERS, P347, DOI 10.1109/LPE.1996.547537
   Chen Y, 2006, P IEEE INT C AC SPEE, V2, pII
   Chung H, 2005, INT SYM DEFEC FAU TO, P514, DOI 10.1109/DFTVS.2005.19
   Dally B, 2009, P ACM IEEE INT C SUP
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   DeHon A, CCC CROSS LAYER RELI
   DeHon A, 2010, DES AUT TEST EUROPE, P1017
   Dongarra J, 2011, INT J HIGH PERFORM C, V25, P3, DOI 10.1177/1094342010391989
   Ellis DRW, 2008, INT CONF ACOUST SPEE, P57, DOI 10.1109/ICASSP.2008.4517545
   Esmaeilzadeh H, 2011, ISCA 2011: PROCEEDINGS OF THE 38TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, P365, DOI 10.1145/2024723.2000108
   Foo B, 2008, IEEE T SIGNAL PROCES, V56, P797, DOI 10.1109/TSP.2007.906685
   Foo B, 2010, IEEE T IMAGE PROCESS, V19, P3035, DOI 10.1109/TIP.2010.2051866
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Goto K, 2008, ACM T MATH SOFTWARE, V35, DOI 10.1145/1377603.1377607
   Goyal VK, 1997, INT CONF ACOUST SPEE, P2729, DOI 10.1109/ICASSP.1997.595353
   He ZH, 2005, IEEE T CIRC SYST VID, V15, P645, DOI 10.1109/TCSVT.2005.846433
   Horowitz M, 2003, IEEE T CIRC SYST VID, V13, P704, DOI 10.1109/TCSVT.2003.814967
   Jun D. M., 2010, Proceedings of the 2010 IEEE Workshop on Signal Processing Systems (SiPS 2010), P1, DOI 10.1109/SIPS.2010.5624823
   Kadyrov A, 2006, IEEE T PATTERN ANAL, V28, P1882, DOI 10.1109/TPAMI.2006.234
   Kaul H., 2012, 2012 IEEE International Solid-State Circuits Conference (ISSCC), P182, DOI 10.1109/ISSCC.2012.6176987
   Khailany BK, 2008, IEEE J SOLID-ST CIRC, V43, P202, DOI 10.1109/JSSC.2007.909331
   Kim T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618469
   Korkmaz P, 2008, IEEE T CIRCUITS-I, V55, P2249, DOI 10.1109/TCSI.2008.920139
   Lammers D, 2010, IEEE SPECTRUM, V47, P15, DOI 10.1109/MSPEC.2010.5605876
   Langou J., 2006, P 2006 ACM IEEE C SU, P50
   Lee DU, 2012, IEEE T IMAGE PROCESS, V21, P768, DOI 10.1109/TIP.2011.2163519
   Lee GG, 2009, IEEE T CIRC SYST VID, V19, P1576, DOI 10.1109/TCSVT.2009.2031376
   Leem L, 2010, DES AUT TEST EUROPE, P1560
   Lengwehasatit K, 2004, IEEE T CIRC SYST VID, V14, P1236, DOI 10.1109/TCSVT.2004.835151
   Li WL, 2009, J SIGNAL PROCESS SYS, V57, P213, DOI 10.1007/s11265-008-0320-5
   Lin CJ, 2000, IEEE T CIRC SYST VID, V10, P1496, DOI 10.1109/76.889059
   Liu M, 2010, DES AUT TEST EUROPE, P973
   Lopes AR, 2009, ANN IEEE SYM FIELD P, P209, DOI 10.1109/FCCM.2009.19
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Ludwig JT, 1996, IEEE J SOLID-ST CIRC, V31, P395, DOI 10.1109/4.494201
   Mainali P, 2011, IEEE T CIRC SYST VID, V21, P435, DOI 10.1109/TCSVT.2011.2125411
   Maruyama N., 2009, P S APPL ACC HIGH PE
   Mastronarde NH, 2008, IEEE T CIRC SYST VID, V18, P453, DOI 10.1109/TCSVT.2008.918440
   Mitra S, 2010, DES AUT TEST EUROPE, P1029
   Murray D. G., 2008, P USENIX 4 C HOT TOP, P5
   Narayanan S, 2010, DES AUT TEST EUROPE, P335
   Nassif SR, 2010, DES AUT TEST EUROPE, P1011
   Nepal K, 2007, J ELECTRON TEST, V23, P255, DOI 10.1007/s10836-006-0553-9
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Nicolaidis M, 2005, IEEE T DEVICE MAT RE, V5, P405, DOI 10.1109/TDMR.2005.855790
   Nicolaidis M, 1999, IEEE VLSI TEST SYMP, P86, DOI 10.1109/VTEST.1999.766651
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Patterson D, 2010, IEEE SPECTRUM, V47, P29
   PEARL J, 1976, IEEE T INFORM THEORY, V22, P580, DOI 10.1109/TIT.1976.1055603
   Petridis K, 2006, LECT NOTES ARTIF INT, V4253, P633
   Phillips E, 2009, P ACM IEEE INT C SUP
   Portero A, 2011, IEEE T CIRC SYST VID, V21, P1027, DOI 10.1109/TCSVT.2011.2129750
   Qian WK, 2011, IEEE T COMPUT, V60, P93, DOI 10.1109/TC.2010.202
   Regunathan SL, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P289
   Schmid C, 2000, INT J COMPUT VISION, V37, P151, DOI 10.1023/A:1008199403446
   Shanbhag NR, 2010, DES AUT CON, P859
   Sheaffer JW, 2007, GRAPHICS HARDWARE 2007: ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P55
   Sow DM, 2003, IEEE T INFORM THEORY, V49, P604, DOI 10.1109/TIT.2002.808135
   Srinivasan J., 2003, 17 ANN INT C SUPER C, P109, DOI [10.1145/782814.782831, DOI 10.1145/782814.782831]
   Stolberg HJ, 2005, J VLSI SIG PROC SYST, V41, P139, DOI 10.1007/s11265-005-6646-3
   Subramanyan P, 2010, DES AUT TEST EUROPE, P1572
   Todman TJ, 2005, IEE P-COMPUT DIG T, V152, P193, DOI 10.1049/ip-cdt:20045086
   Turaga DS, 2005, IEEE T CIRC SYST VID, V15, P982, DOI 10.1109/TCSVT.2005.852399
   Valentim J, 2002, IEEE T CIRC SYST VID, V12, P1034, DOI 10.1109/TCSVT.2002.805497
   van der Schaar M, 2005, IEEE T MULTIMEDIA, V7, P471, DOI 10.1109/TMM.2005.846790
   Varatkar GV, 2010, IEEE T VLSI SYST, V18, P1421, DOI 10.1109/TVLSI.2009.2024673
   Vereshchagin N, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P798, DOI 10.1109/ISIT.2006.261723
   Vereshchagin NK, 2010, IEEE T INFORM THEORY, V56, P3438, DOI 10.1109/TIT.2010.2048491
   Xanthopoulos T, 2000, IEEE J SOLID-ST CIRC, V35, P740, DOI 10.1109/4.841502
   Xu W., 2003, P 26 ANN INT ACM SIG, P267
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Yeh TY, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640448
   Yuan W., 2004, Proc. of ACM International Conference on Multimedia, P924
   Yuan WH, 2006, IEEE T MOBILE COMPUT, V5, P799, DOI 10.1109/TMC.2006.98
   Zhu Z, 2008, LECT NOTES COMPUT SC, V5039, P111
NR 100
TC 8
Z9 9
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 291
EP 303
DI 10.1109/TMM.2012.2232649
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500006
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Mu, M
   Ishmael, J
   Knowles, W
   Rouncefield, M
   Race, N
   Stuart, M
   Wright, G
AF Mu, Mu
   Ishmael, Johnathan
   Knowles, William
   Rouncefield, Mark
   Race, Nicholas
   Stuart, Mark
   Wright, George
TI P2P-Based IPTV Services: Design, Deployment, and QoE Measurement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image processing; IPTV; multimedia systems; peer-to-peer computing;
   quality of service
ID PEER-TO-PEER; INTERNET; QUALITY
AB This paper introduces the recent design and development of a converged IPTV service that has been deployed within a live test-bed (Living Lab) at Lancaster University for thousands of students. High quality audio-visual content is distributed over heterogeneous IP-based content networks, on both set-top box and web-based platforms. Peer-to-peer (P2P) technologies are exploited to provide energy efficient and low-cost delivery for commercial and user-generated content. The infrastructure and functional components are first presented exploring a number of key designs that facilitate the entire eco-system of content ingest, transcoding, P2P tracking, distribution, statistics, end systems, as well as integration of social networking. Due to the dynamic nature of P2P distribution, a quality measurement service with respect to user experience is also essential for the service evaluation and diagnosis. A multimodal QoE measurement framework which evaluates the IPTV services by collaborating measurements with a variety of different aspects is presented. Results of a use case are also described to verify the effectiveness of the measurement framework in exploiting relevant metrics from service components.
C1 [Mu, Mu; Ishmael, Johnathan; Knowles, William; Rouncefield, Mark; Race, Nicholas] Univ Lancaster, Sch Comp & Commun, Lancaster LA1 4WA, England.
   [Stuart, Mark] Pioneer Digital Design, Slough SL2 4QP, Berks, England.
   [Wright, George] BBC R&D, London W12 7SB, England.
C3 Lancaster University
RP Mu, M (corresponding author), Univ Lancaster, Sch Comp & Commun, Lancaster LA1 4WA, England.
EM m.mu@lancaster.ac.uk; j.ishmael@lancaster.ac.uk;
   w.knowles@lancaster.ac.uk; m.rouncefield@lancaster.ac.uk;
   n.race@lan-caster.ac.uk; mark@pddresearch.com; george.wright@bbc.co.uk
RI Mu, Mu/W-2822-2019
OI Mu, Mu/0000-0003-1931-7959; Race, Nicholas/0000-0002-6870-8078
FU European Commission; FIRM project (Framework for Innovation and Research
   in MediaCityUK); EPSRC [EP/H003738/1] Funding Source: UKRI
FX This work was supported by the European Commission within the FP7
   Project: P2P-Next and FIRM project (Framework for Innovation and
   Research in MediaCityUK). The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Oscar
   Bonastre.
CR Abeni L., 2010, Peer-to-Peer Computing, P1
   [Anonymous], 2009, Tech. Rep.
   [Anonymous], 2008, Tech. rep.
   [Anonymous], 2008, FINAL REPORT OF VQEG
   [Anonymous], 10 IEEE INT C PEER C
   [Anonymous], 2011, Tech. rep.
   Apple, HTTP Live Streaming Overview
   Application layer reliability solutions for IPTV services, 2006, ITU T FG WORKING DOC
   Chen KT, 2009, IEEE INFOCOM SER, P702, DOI 10.1109/INFCOM.2009.5061978
   D'Acunto L., 2010, PROC IEEE INT PARALL, P1
   Dynamic adaptive streaming over HTTP (DASH), 2012, ISO IEC PRF 23009 1
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Huang F, 2010, LECT NOTES COMPUT SC, V6490, P459, DOI 10.1007/978-3-642-17653-1_33
   Ishmael J, 2008, IEEE INTERNET COMPUT, V12, P22, DOI 10.1109/MIC.2008.76
   Jimenez R., 2009, PROC 6TH SWEDISH NAT
   Li B, 2007, IEEE COMMUN MAG, V45, P94, DOI 10.1109/MCOM.2007.374425
   Lin TL, 2010, IEEE T IMAGE PROCESS, V19, P722, DOI 10.1109/TIP.2009.2038834
   Liu JC, 2008, P IEEE, V96, P11, DOI 10.1109/JPROC.2007.909921
   Liu ZY, 2009, IEEE T MULTIMEDIA, V11, P1340, DOI 10.1109/TMM.2009.2030656
   Martin M., 2007, Management magazine, P31, DOI DOI 10.1145/1290050.1290058
   Meeyoung C., 2007, PROC 7TH INT CONF PE
   Microsoft Smooth Streaming Protocol Specification, 2011, SMOOTH STREAMING PRO
   Mol J., 2008, PROC 15TH SPIE ACM M
   Mu M., PROC ACM MULTI MEDIA
   Mu M., 2011, ELSEVIER J SIGNAL PR
   Mu M., 2009, PROC ACM 16TH ANNU M
   Ofcom Communications Market Report 2010, 2010, OFCOM COMMUNICATIONS
   Perceptual Evaluation of Video Quality, 2007, PERCEPTUAL EVALUATIO
   Pinson M., 2003, PROC SPIE VIDEO COMM
   Quality of experience requirements for IPTV services, 2008, ITU T FG IPTV RECOMM, P1080
   Rahrer T., 2006, TECH REP TR 126
   SEFERIDIS V, 1992, ELECTRON LETT, V28, P2013, DOI 10.1049/el:19921290
   Silverston T., 2007, PROC ACM INT WORKSHO
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Sripanidkulchai K, 2004, ACM SIGCOMM COMP COM, V34, P107, DOI 10.1145/1030194.1015480
   Steinmetz R., 2004, Informatik Spektrum, V27, P51, DOI 10.1007/s00287-003-0362-9
   Tran DA, 2004, IEEE J SEL AREA COMM, V22, P121, DOI 10.1109/JSAC.2003.818803
   Wang Z., 2002, PROC IEEE INT CONF I
   Wolf S, 1999, P SOC PHOTO-OPT INS, V3845, P266, DOI 10.1117/12.371210
   Wu CA, 2009, IEEE INFOCOM SER, P2731, DOI 10.1109/INFCOM.2009.5062221
   You FH, 2009, PROCEEDINGS OF THE 8TH IEEE/ACIS INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE, P824, DOI 10.1109/ICIS.2009.24
NR 41
TC 13
Z9 14
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1515
EP 1527
DI 10.1109/TMM.2012.2217119
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400002
DA 2024-07-18
ER

PT J
AU Tsai, CY
AF Tsai, Chi-Yi
TI A Fast Dynamic Range Compression With Local Contrast Preservation
   Algorithm and Its Application to Real-Time Video Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic range compression; linear color remapping; local contrast
   preservation; low dynamic range image enhancement; real-time video
   enhancement
ID IMAGE; RETINEX
AB This study addresses low dynamic range (LDR) image/video enhancement for digital video cameras. A new fast dynamic range compression format with a local-contrast-preservation (FDRCLCP) algorithm resolves this problem efficiently. The proposed FDRCLCP algorithm can combine with any continuously differentiable intensity transfer function to achieve LDR image enhancement. In combination with the FDRCLCP algorithm, a new intensity-transfer function is proposed, adaptively accomplishing dynamic range compression and edge-contrast enhancement depending on the local mean value of the input luminance image. The proposed method also extends to a linear color remapping approach, not only preserving the original image's color information, but also controlling color saturation of the resulting image. Moreover, a look-up-table (LUT) acceleration approach improves the processing speed of the proposed FDRCLCP algorithm in processing video signals, allowing real-time video enhancement processing. Experimental results show that the proposed method not only provides good visual representation in both quantitative and visual comparisons, but also achieves real-time performance for video processing.
C1 Tamkang Univ, Dept Elect Engn, New Taipei City 25137, Taiwan.
C3 Tamkang University
RP Tsai, CY (corresponding author), Tamkang Univ, Dept Elect Engn, New Taipei City 25137, Taiwan.
EM chiyi_tsai@mail.tku.edu.tw
RI Tsai, Chi-Yi/AFJ-8560-2022; Tsai, Chi-Yi/AAT-2837-2021
OI Tsai, Chi-Yi/0000-0001-9872-4338; Tsai, Chi-Yi/0000-0001-9872-4338
FU National Science Council of Taiwan, R.O.C. [NSC 100-2221-E-032-011]
FX This work was supported by the National Science Council of Taiwan,
   R.O.C. under grant NSC 100-2221-E-032-011. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Zhu Liu.
CR [Anonymous], P SPIE
   [Anonymous], 2005, IEEE INT C COMP VIS
   Bennett EP, 2005, ACM T GRAPHIC, V24, P845, DOI 10.1145/1073204.1073272
   Bertalmío M, 2007, IEEE T IMAGE PROCESS, V16, P1058, DOI 10.1109/TIP.2007.891777
   Bressan M, 2007, PROC SPIE, V6493, DOI 10.1117/12.724721
   Buades A, 2008, INT J COMPUT VISION, V76, P123, DOI 10.1007/s11263-007-0052-1
   Chen SH, 2010, EURASIP J IMAGE VIDE, DOI 10.1155/2010/175203
   Choudhury Anustup, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1893, DOI 10.1109/ICCVW.2009.5457513
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Garnett R, 2005, IEEE T IMAGE PROCESS, V14, P1747, DOI 10.1109/TIP.2005.857261
   Horiuchi T, 2010, EURASIP J IMAGE VIDE, DOI 10.1155/2010/438958
   Hu KJ, 2010, EURASIP J IMAGE VIDE, DOI 10.1155/2010/137134
   International Telecommunications Union, BT601 ITUR
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 2002, P SOC PHOTO-OPT INS, V4736, P25, DOI 10.1117/12.477589
   LAND EH, 1986, VISION RES, V26, P7, DOI 10.1016/0042-6989(86)90067-2
   Marsi S, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/80971
   Meylan L, 2006, IEEE T IMAGE PROCESS, V15, P2820, DOI 10.1109/TIP.2006.877312
   Monobe Y, 2005, IEEE T CONSUM ELECTR, V51, P1, DOI 10.1109/TCE.2005.1405691
   Palma-Amestoy R, 2009, IEEE T PATTERN ANAL, V31, P458, DOI 10.1109/TPAMI.2008.86
   PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Reinhard E, 2007, J SOC INF DISPLAY, V15, P997, DOI 10.1889/1.2825110
   Tao L, 2005, J ELECTRON IMAGING, V14, DOI 10.1117/1.2136903
   Tao L., 2006, International Journal of Computational Intelligence Research, V4, P327
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Unaldi N., 2009, P SOC PHOTO-OPT INS, V7341
   Wang C, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/165792
NR 28
TC 23
Z9 26
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1140
EP 1152
DI 10.1109/TMM.2012.2190390
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400003
DA 2024-07-18
ER

PT J
AU Zhou, XM
   Chen, L
   Zhou, XF
AF Zhou, Xiangmin
   Chen, Lei
   Zhou, Xiaofang
TI Structure Tensor Series-Based Large Scale Near-Duplicate Video Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive structure video tensor (ASVT) series; ASVT series distance
   measure; dimensionality reduction
ID SEARCH
AB With the huge amount of video data and its exponential growth in recent years, many new challenges, like storage, search and navigation, have arisen. Among these challenges, near-duplicate video retrieval aims to find clips that are identical or nearly identical in content to a query clip. This has attracted much attention due to its wide applications including copyright detection, commercial monitoring and news video tracking. In this paper, we propose a practical solution based on 3-D structure tensor model for this problem. We first propose a novel video representation, adaptive structure video tensor series, together with a robust similarity measure, to improve the retrieval effectiveness. Then, we design a dimensionality reduction technique for tensor series to improve the search efficiency. Finally, we prove the effectiveness and efficiency of the proposed method by extensive experiments on hundreds of hours of real video data.
C1 [Zhou, Xiangmin] CSIRO, ICT Ctr, Canberra, ACT, Australia.
   [Chen, Lei] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Zhou, Xiaofang] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld, Australia.
C3 Commonwealth Scientific & Industrial Research Organisation (CSIRO); Hong
   Kong University of Science & Technology; University of Queensland
RP Zhou, XM (corresponding author), CSIRO, ICT Ctr, Canberra, ACT, Australia.
EM xiangmin.zhou@csiro.au; leichen@cse.ust.hk; zxf@itee.uq.edu.au
RI Zhou, Xiaofang/C-6169-2013; Zhou, Xiangfeng/KDO-8724-2024; Zhou,
   Xiangmin/B-4341-2011; Chen, Lei/HMD-2646-2023
OI Zhou, Xiaofang/0000-0001-6343-1455; Chen, Lei/0000-0003-3718-9268; Chen,
   Lei/0000-0002-8257-5806
FU Hong Kong RGC GRF [611411]; National Grand Fundamental Research 973
   Program of China [2012CB316200]; Microsoft Research Asia Theme Grant
   [MRA11EG0]; HP IRP; HP Catalyst Program; ARC DP [DP110103423]
FX This work was supported in part by Hong Kong RGC GRF Project No. 611411,
   National Grand Fundamental Research 973 Program of China under Grant
   2012CB316200, Microsoft Research Asia Theme Grant MRA11EG0, HP IRP
   Project Grant 2011 and HP Catalyst Program 2011, and ARC DP grant
   DP110103423. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Daniel Gatica-Perez.
CR Adjeroh DA, 1999, COMPUT VIS IMAGE UND, V75, P25, DOI 10.1006/cviu.1999.0764
   [Anonymous], 1994, The VLDB Journal, DOI DOI 10.1007/BF01231606
   [Anonymous], 2009, P ACM INT C MULT, DOI DOI 10.1145/1631272.1631295
   [Anonymous], 2007, Proceedings of the 33rd International Conference on Very Large Data Bases. VLDB'07
   Cai Y., 2004, P SIGMOD, P921
   Chakrabarti K., 2000, VLDB C, P89
   Chen L, 2004, P 30 INT C VER LARG, V30, P792, DOI [DOI 10.1016/B978-012088469-8.50070-X, DOI 10.1016/B978-012088469-8/50070-X]
   Chen L., 2005, P 2005 ACM SIGMOD IN, P491
   Cheung SCS, 2005, IEEE T MULTIMEDIA, V7, P524, DOI 10.1109/TMM.2005.846906
   Chiu CY, 2006, INT C PATT RECOG, P228
   Faloutsos C., 1994, SIGMOD Record, V23, P419, DOI 10.1145/191843.191925
   Guttman A., 1984, SIGMOD Record, V14, P47, DOI 10.1145/971697.602266
   Hjaltason GR, 1999, ACM T DATABASE SYST, V24, P265, DOI 10.1145/320248.320255
   Hoad TC, 2006, ACM T INFORM SYST, V24, P1, DOI 10.1145/1125857.1125858
   Hsu W. H., 2004, P ICIP, P141
   Huang Z., 2009, TOIS, V27
   IZENMAN AJ, 1991, J AM STAT ASSOC, V86, P205, DOI 10.2307/2289732
   Jiang Y. G., 2006, P TRECVID
   Jin H, 2003, PROC INT CONF DATA, P87, DOI 10.1109/ICDE.2003.1260784
   Joly A., 2008, P NIST TRECVID WORKS
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Ke Yan., 2004, MULTIMEDIA 04 P 12 A, P869, DOI DOI 10.1145/1027527.1027729
   Keogh E., 2002, Proceedings of the Twenty-eighth International Conference on Very Large Data Bases, P406
   Keogh E., 2001, Knowledge and Information Systems, V3, P263, DOI 10.1007/PL00011669
   Keogh E.J., 2001, ACM Transactions on Database Systems, V27, P188
   Kim C, 2005, IEEE T CIRC SYST VID, V15, P127
   Korn F., 1997, SIGMOD Record, V26, P289, DOI 10.1145/253262.253332
   Law-To J., 2006, P ACM INT C MULTIMED, P835
   Lee JeongKyu, 2005, P 2005 ACM SIGMOD IN, P718, DOI DOI 10.1145/1066157.1066239
   Liu Z., 2007, P TRECVID
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ngo C. W., 2010, P TRECVID
   Ngo C-W., 2006, ACM MULTIMEDIA 06, P845
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Rafiei Davood., 1998, Proceedings of the FODO Conference, Kobe, P249
   Shang L., 2010, Proceedings of the International Conference on Multimedia, P531
   Shen H.T., 2005, P ACM SIGMOD INT C M, P730
   Smeaton A. F., 2006, P MIR
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   UKKONEN E, 1992, THEOR COMPUT SCI, V92, P191, DOI 10.1016/0304-3975(92)90143-4
   Wang HY, 2004, EURASIP J APPL SIG P, V2004, P798, DOI 10.1155/S111086570440122X
   Wang ZZ, 2004, PROC CVPR IEEE, P228
   Wu A. G., 2007, P ACM MM, P218
   Wu X, 2009, IEEE T MULTIMEDIA, V11, P196, DOI 10.1109/TMM.2008.2009673
   Wu Z., 2009, Proceedings of the 17th ACM international conference on Multimedia, P549
   Yan Y, 2008, PROC INT CONF DATA, P853, DOI 10.1109/ICDE.2008.4497494
   Zhou X., 2011, ACM MULTIMEDIA, P1057
   Zhou XM, 2010, IEEE T KNOWL DATA EN, V22, P1372, DOI 10.1109/TKDE.2009.171
   Zhou Xiangmin., 2010, Proceedings of the 18th ACM international conference on multimedia, P521
NR 49
TC 19
Z9 20
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1220
EP 1233
DI 10.1109/TMM.2012.2194481
PN 2
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400009
DA 2024-07-18
ER

PT J
AU Chen, X
   Hero, AO
   Savarese, S
AF Chen, Xu
   Hero, Alfred O., III
   Savarese, Silvio
TI Multimodal Video Indexing and Retrieval Using Directed Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-video pattern recognition; multimedia content retrieval;
   multimodal feature fusion; nonlinear information flow; overfitting
   prevention; shrinkage optimization
AB We propose a novel framework for multimodal video indexing and retrieval using shrinkage optimized directed information assessment (SODA) as similarity measure. The directed information (DI) is a variant of the classical mutual information which attempts to capture the direction of information flow that videos naturally possess. It is applied directly to the empirical probability distributions of both audio-visual features over successive frames. We utilize RASTA-PLP features for audio feature representation and SIFT features for visual feature representation. We compute the joint probability density functions of audio and visual features in order to fuse features from different modalities. With SODA, we further estimate the DI in a manner that is suitable for high dimensional features p and small sample size n (large p small n) between pairs of video-audio modalities. We demonstrate the superiority of the SODA approach in video indexing, retrieval, and activity recognition as compared to the state-of-the-art methods such as hidden Markov models (HMM), support vector machine (SVM), cross-media indexing space (CMIS), and other noncausal divergence measures such as mutual information (MI). We also demonstrate the success of SODA in audio and video localization and indexing/retrieval of data with missaligned modalities.
C1 [Chen, Xu; Hero, Alfred O., III; Savarese, Silvio] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48105 USA.
C3 University of Michigan System; University of Michigan
RP Chen, X (corresponding author), Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48105 USA.
EM xhen@umich.edu; hero@umich.edu; silvio@eecs.umich.edu
OI Hero, Alfred/0000-0002-2531-9670
FU US Army Research Office [W911NF-09-1-0310]
FX This work was supported in part by a grant from the US Army Research
   Office, grant W911NF-09-1-0310. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Changsheng Xu.
CR ALLEN JF, 1983, COMMUN ACM, V26, P832, DOI 10.1145/182.358434
   Amblard PO, 2011, J COMPUT NEUROSCI, V30, P7, DOI 10.1007/s10827-010-0231-x
   [Anonymous], 2007, 2007 IEEE C COMPUTER
   [Anonymous], 2009, P IEEE INT C COMP VI
   Benjamini Y, 2001, ANN STAT, V29, P1165
   BICKEL PJ, 2005, MATH STAT BASIC IDEA, V1
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fisher JW, 2004, IEEE T MULTIMEDIA, V6, P406, DOI 10.1109/TMM.2004.827503
   Germana J., 1996, INTEGRAT PHYSL BEHAV, V31
   Hausser J, 2009, J MACH LEARN RES, V10, P1469
   HERMANSKY H, 1990, J ACOUST SOC AM, V87, P1738, DOI 10.1121/1.399423
   Hermansky H, 1994, IEEE T SPEECH AUDI P, V2, P578, DOI 10.1109/89.326616
   Hornler B., 2009, P IEEE INT C MULT EX
   Hospedales T., 2009, P ICCV
   Huang J., 1999, P IEEE SIGN PROC SOC
   Ke Y., 2007, P IEEE INT C COMP VI
   Krumin M., 2010, COMPUTAT INTELL NEUR
   Ledoit O., 2004, J MULTIVARIATE ANAL
   Lin W., 2002, P ACM MULT C
   Liu J., 2008, P IEEE C COMP VIS PA
   Liu J. H. Z., 1998, P IEEE SIGN PROC SOC
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Massey J., 1990, P S INF THEOR ITS AP
   Morris RJ, 2000, INT J COMPUT VISION, V37, P209, DOI 10.1023/A:1008159822101
   Niebles J. C., 2010, P IEEE EUR C COMP VI
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Quinn C., 2011, J COMPUTAT NEUROSCI, V30
   RAO A, 2007, EURASIP J BIOINFORMA
   Rasiwasia N., 2010, P ACM 18 INT C MULT
   Romano J., 2008, P TEST
   Snoek C., 2002, MULTIMED TOOLS APPL, V25, P5
   Snoek CGM, 2005, IEEE T MULTIMEDIA, V7, P638, DOI 10.1109/TMM.2005.850966
   Sun Z., 2006, P IEEE C COMP VIS PA
   Wang X., 2006, P ECCV
   Yang Y, 2010, PATTERN RECOGN, V43, P2927, DOI 10.1016/j.patcog.2010.02.015
   Zhong H., 2004, P CVPR
   Zhou X., 2008, P ACM INT C MULT
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 39
TC 27
Z9 28
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 3
EP 16
DI 10.1109/TMM.2011.2167223
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Van Mieghem, P
AF Van Mieghem, Piet
TI Human Psychology of Common Appraisal: The Reddit Score
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collective behavior; network; random walk; voting
AB The Reddit score reflects a common appraisal by a community of Reddit subscribers of a submitted item, called a story. The general random walk with random maximum boundary is demonstrated to describe the distribution function of the Reddit score of an arbitrary story in the online social news aggregator Reddit.com. Exponential tails, predicted by the analysis, are observed, while a curious intermediate "power law-like" region seems to correspond to a remarkable empirical observation that the total number of downvotes depends in "power law" fashion on the total number of upvotes. Stronger even, those downvotes increase faster than the upvotes, which is a surprising fact that asks for a (socio-psychological?) explanation.
C1 Delft Univ Technol, Fac Elect Engn Math & Comp Sci, NL-2600 GA Delft, Netherlands.
C3 Delft University of Technology
RP Van Mieghem, P (corresponding author), Delft Univ Technol, Fac Elect Engn Math & Comp Sci, NL-2600 GA Delft, Netherlands.
EM P.F.A.VanMieghem@tudelft.nl
CR Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111
   Tang SY, 2011, IEEE T MULTIMEDIA, V13, P1163, DOI 10.1109/TMM.2011.2159706
   Thelwall M, 2011, J AM SOC INF SCI TEC, V62, P406, DOI 10.1002/asi.21462
   Van Mieghem P., 2006, Performance analysis of communications networks and systems
   VANMIEGHEM P, EUR PHYS J B
   Wu F, 2007, P NATL ACAD SCI USA, V104, P17599, DOI 10.1073/pnas.0704916104
NR 6
TC 7
Z9 9
U1 3
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1404
EP 1406
DI 10.1109/TMM.2011.2165054
PG 3
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400019
DA 2024-07-18
ER

PT J
AU Huang, CR
   Chiu, KC
   Chen, CS
AF Huang, Chun-Rong
   Chiu, Kuo-Chuan
   Chen, Chu-Song
TI Temporal Color Consistency-Based Video Reproduction for Dichromats
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Colorblind video; video processing
ID SEARCH; PATTERN; IMAGES
AB In this paper, a video re-coloring algorithm for dichromats is presented. Different from image re-coloring schemes, reproducing a video for dichromats requires maintaining temporal color consistency between frames, i.e., the same color in different frames should be re-colored to the identical new color. To achieve this goal, we extract video key colors from shots after motion estimation at first. Based on the importance of video key colors, a process order is defined to perform efficient color remapping and solve the contrast maintaining problem. Then, the remapped frame pixel values are interpolated by the re-mapped video key colors with spatial-temporal constraints. Experimental results show that our method can increase the visibility for dichromats and guarantee temporal color consistency.
C1 [Huang, Chun-Rong] Natl Chung Hsing Univ, Inst Networking & Multimedia, Taichung 40227, Taiwan.
   [Huang, Chun-Rong] Natl Chung Hsing Univ, Dept Comp Sci & Engn, Taichung 40227, Taiwan.
   [Chiu, Kuo-Chuan; Chen, Chu-Song] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   [Chen, Chu-Song] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
C3 National Chung Hsing University; National Chung Hsing University;
   Academia Sinica - Taiwan; Academia Sinica - Taiwan
RP Huang, CR (corresponding author), Natl Chung Hsing Univ, Inst Networking & Multimedia, Taichung 40227, Taiwan.
EM crhuang@cs.nchu.edu.tw; kcchiu@iis.sinica.edu.tw; song@iis.sinica.edu.tw
OI Huang, Chun-Rong/0000-0003-2372-5429
FU National Science Council of Taiwan, R.O.C. NSC [100-2631-H-001-013]
FX Manuscript received August 09, 2010; revised December 24, 2010 and March
   07, 2011; accepted March 09, 2011. Date of publication April 05, 2011;
   date of current version September 16, 2011. This work was supported by
   the National Science Council of Taiwan, R.O.C., under Grant NSC
   100-2631-H-001-013. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Marcel Worring.
CR [Anonymous], 1999, COLOR VISION GENES P
   Antoniou A., 2007, PRACTICAL OPTIMIZATI
   Brettel H, 1997, J OPT SOC AM A, V14, P2647, DOI 10.1364/JOSAA.14.002647
   Burnham R.W., 1963, Color: A guide to basic facts and concepts: Basic elements of color education
   DUAN QY, 1993, J OPTIMIZ THEORY APP, V76, P501, DOI 10.1007/BF00939380
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   HECKBERT P, 1982, P SIGGRAPH 82, P297
   Huang CR, 2008, IEEE T MULTIMEDIA, V10, P1097, DOI 10.1109/TMM.2008.2001374
   Huang CR, 2008, PATTERN RECOGN, V41, P3071, DOI 10.1016/j.patcog.2008.03.013
   Huang JB, 2009, INT CONF ACOUST SPEE, P1161, DOI 10.1109/ICASSP.2009.4959795
   Jefferson L, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1535
   Jefferson Luke., 2006, P 8 INT ACM SIGACCES, P40, DOI DOI 10.1145/1168987.1168996
   Kovalev VA, 2006, LECT NOTES ARTIF INT, V4065, P431
   Kuhn GR, 2008, IEEE T VIS COMPUT GR, V14, P1747, DOI 10.1109/TVCG.2008.112
   Lin YC, 1997, IEEE T COMMUN, V45, P527, DOI 10.1109/26.592551
   Liu B, 2009, IEEE INT CON MULTI, P906, DOI 10.1109/ICME.2009.5202642
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Machado GM, 2010, COMPUT GRAPH FORUM, V29, P933, DOI 10.1111/j.1467-8659.2009.01701.x
   Nam J, 2005, IEEE T MULTIMEDIA, V7, P435, DOI 10.1109/TMM.2005.846801
   Nie Y, 2002, IEEE T IMAGE PROCESS, V11, P1442, DOI 10.1109/TIP.2002.806251
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Rasche K, 2005, IEEE COMPUT GRAPH, V25, P22, DOI 10.1109/MCG.2005.54
   Rigden C, 1999, BRIT TELECOMMUN ENG, V17, P291
   Shepard D., 1968, P 1968 23 ACM NAT C, P517, DOI DOI 10.1145/800186.810616
   Viénot F, 1999, COLOR RES APPL, V24, P243, DOI 10.1002/(SICI)1520-6378(199908)24:4<243::AID-COL5>3.0.CO;2-3
   Wakita Ken., 2005, Assets '05: Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility, P158, DOI [10.1145/1090785.1090815, DOI 10.1145/1090785.1090815]
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P330, DOI 10.1109/TMM.2010.2046364
   Wang M, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE OF NATURAL PRODUCT AND TRADITIONAL MEDICINE, VOLS 1 AND 2, P291, DOI 10.1145/1631272.1631314
   YANG S, 2008, EURASIP J IMAGE VIDE
   Zhu C, 2002, IEEE T CIRC SYST VID, V12, P349, DOI 10.1109/TCSVT.2002.1003474
NR 30
TC 26
Z9 28
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 950
EP 960
DI 10.1109/TMM.2011.2135844
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hellge, C
   Gómez-Barquero, D
   Schierl, T
   Wiegand, T
AF Hellge, Cornelius
   Gomez-Barquero, David
   Schierl, Thomas
   Wiegand, Thomas
TI Layer-Aware Forward Error Correction for Mobile Broadcast of Layered
   Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE LA-FEC; layered media; mobile TV; MVC; SVC; UEP
ID CODES
AB The bitstream structure of layered media formats such as scalable video coding (SVC) or multiview video coding (MVC) opens up new opportunities for their distribution in Mobile TV services. Features like graceful degradation or the support of the 3-D experience in a backwards-compatible way are enabled. The reason is that parts of the media stream are more important than others with each part itself providing a useful media representation. Typically, the decoding of some parts of the bitstream is only possible, if the corresponding more important parts are correctly received. Hence, unequal error protection (UEP) can be applied protecting important parts of the bitstream more strongly than others. Mobile broadcast systems typically apply forward error correction (FEC) on upper layers to cope with transmission errors, which the physical layer FEC cannot correct. Today's FEC solutions are optimized to transmit single layer video. The exploitation of the dependencies in layered media codecs for UEP using FEC is the subject of this paper. The presented scheme, which is called layer-aware FEC (LA-FEC), incorporates the dependencies of the layered video codec into the FEC code construction. A combinatorial analysis is derived to show the potential theoretical gain in terms of FEC decoding probability and video quality. Furthermore, the implementation of LA-FEC as an extension of the Raptor FEC and the related signaling are described. The performance of layer-aware Raptor code with SVC is shown by experimental results in a DVB-H environment showing significant improvements achieved by LA-FEC.
C1 [Hellge, Cornelius; Gomez-Barquero, David; Schierl, Thomas] Fraunhofer HHI, Image Proc Dept, Multimedia Commun Grp, D-10587 Berlin, Germany.
   [Hellge, Cornelius; Schierl, Thomas; Wiegand, Thomas] Berlin Inst Technol, Image Commun Lab, D-10623 Berlin, Germany.
   [Gomez-Barquero, David] Univ Politecn Valencia, Valencia 46022, Spain.
C3 Technical University of Berlin; Universitat Politecnica de Valencia
RP Hellge, C (corresponding author), Fraunhofer HHI, Image Proc Dept, Multimedia Commun Grp, D-10587 Berlin, Germany.
EM cornelius.hellge@hhi.fraunhofer.de; dagobar@iteam.upv.es;
   thomas.schierl@hhi.fraunhofer.de; twiegand@ieee.org
RI Gomez-Barquero, David/J-7098-2014
OI Gomez-Barquero, David/0000-0003-2610-7765
CR *3GPP, 36346 3GPP TS
   [Anonymous], 102472 ETSI TS
   [Anonymous], 2009, 302755V111 ETSI EN
   [Anonymous], [No title captured]
   [Anonymous], 102584 ETSI TS
   [Anonymous], 2009, A153 ATSC
   BEGEN A, 2010, SDP ELEMENTS FEC FRA
   Berrou C, 2003, IEEE COMMUN MAG, V41, P110, DOI 10.1109/MCOM.2003.1222726
   Bogino M., 2007, P ISCAS 2007 MAY
   BOUABDALLAH A, 2006, J ZHEJIANG UNIV-SC A, V7, P27
   BOYARINOV IM, 1981, IEEE T INFORM THEORY, V27, P168, DOI 10.1109/TIT.1981.1056327
   *CFT, 2009, DVB TM H NGH CALL TE
   Digital Video Broadcasting (DVB), 2011, 102993 ETSI TR
   *ETSIEN, 2010, 302583 ETSIEN
   Faria G, 2006, P IEEE, V94, P194, DOI 10.1109/JPROC.2005.861011
   HANDLY M, 2006, 4566 IETF RFC
   HELLGE C, 2008, P IEEE INT C COMM IC
   HELLGE C, 2010, P ICME 2010 SING JUL
   HELLGE C, 2008, P IEEE INT C MULT EX
   Hellge C, 2008, IEEE IMAGE PROC, P2304, DOI 10.1109/ICIP.2008.4712252
   Hellge Cornelius, 2009, P IEEE INT S BROADB, P1
   KILGUS CC, 1972, IEEE T INFORM THEORY, V18, P687, DOI 10.1109/TIT.1972.1054869
   Luby M., 2010, RAPTORQ FOR IN PRESS
   Luby M., 2002, P ACM S FDN COMP SCI
   MacKay DJC, 1997, ELECTRON LETT, V33, P457, DOI 10.1049/el:19970362
   MASNICK B, 1967, IEEE T INFORM THEORY, V13, P600, DOI 10.1109/TIT.1967.1054054
   Merkle P, 2009, SIGNAL PROCESS-IMAGE, V24, P73, DOI 10.1016/j.image.2008.10.010
   MIRTA S, 2010, P ASMA SPSC SEP
   POSTEL J, 1981, 6 IETF STD
   Rahnavard N, 2006, IEEE COMMUN LETT, V10, P43, DOI 10.1109/LCOMM.2006.1576564
   Rahnavard N, 2007, IEEE T INFORM THEORY, V53, P1521, DOI 10.1109/TIT.2007.892814
   SCHIERL T, 2009, SIGNALING MEDIA DECO
   Schulzrinne H., 2003, 0064 IETF STD
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sejdinovic D, 2009, IEEE T COMMUN, V57, P2510, DOI 10.1109/TCOMM.2009.09.070616
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   *TM, 2010, TMNGH092
   Vukobratovic D, 2009, IEEE T MULTIMEDIA, V11, P1094, DOI 10.1109/TMM.2009.2026087
   Wang Y.-K., 2010, RTP PAYLOAD IN PRESS
   WATSON M, 2010, RAPTOR FEC SCHEMES F
   WATSON M, 2010, FORWARD ERROR CORREC
   WATSON M, 2010, RTP PAYLOAD FORMAT R
   Wenger S., 2010, RTP PAYLOAD IN PRESS
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   ZIMMERMANN H, 1980, IEEE T COMMUN, V28, P425, DOI 10.1109/TCOM.1980.1094702
NR 45
TC 62
Z9 68
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 551
EP 562
DI 10.1109/TMM.2011.2129499
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700014
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Wang, W
   Peng, DM
   Wang, HG
   Sharif, H
   Chen, HH
AF Wang, Wei
   Peng, Dongming
   Wang, Honggang
   Sharif, Hamid
   Chen, Hsiao-Hwa
TI A Multimedia Quality-Driven Network Resource Management Architecture for
   Wireless Sensor Networks With Stream Authentication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross layer resource allocation; stream authentication; wireless
   multimedia sensor network
ID IMAGE; JPEG2000; SCHEME; SECURE; OPTIMIZATION
AB Media integrity, transmission quality, and energy efficiency are critical for secure wireless image streaming in a wireless multimedia sensor network (WMSN). However, conventional data authentication and resource allocation schemes cannot be applied directly to WMSN due to the constraints on limited energy and computing resources. In this paper, we propose a quality-driven scheme to optimize stream authentication and unequal error protection (UEP) jointly. This scheme can provide digital image authentication, image transmission quality optimization, and high energy efficiency for WMSN. The contribution of this research is two-fold as summarized below. First, a new resource allocationaware greedy stream authentication approach is proposed to simplify the authentication process. Second, an authentication-aware wireless network resource allocation scheme is developed to reduce image distortion and energy consumption in transmission. The scheme is studied by unequally protected image packets with the consideration of coding and authentication dependency. Simulation results demonstrate that the proposed scheme achieves a performance gain of 3 similar to 5 dB in terms of authenticated image distortion.
C1 [Wang, Wei] S Dakota State Univ, Dept Elect Engn & Comp Sci, Brookings, SD 57007 USA.
   [Peng, Dongming; Sharif, Hamid] Univ Nebraska, Dept Elect & Comp Engn, Omaha, NE 68182 USA.
   [Wang, Honggang] Univ Massachusetts Dartmouth, Dept Elect & Comp Engn, N Dartmouth, MA 02747 USA.
   [Chen, Hsiao-Hwa] Natl Cheng Kung Univ, Dept Engn Sci, Tainan 701, Taiwan.
C3 South Dakota State University; University of Nebraska System; University
   of Massachusetts System; University Massachusetts Dartmouth; National
   Cheng Kung University
RP Wang, W (corresponding author), S Dakota State Univ, Dept Elect Engn & Comp Sci, Brookings, SD 57007 USA.
EM wei.wang@sdstate.edu; dpeng@unlnotes.unl.edu; hwang1@umassd.edu;
   hsharif@unlnotes.unl.edu; hshwchen@ieee.org
RI Sharif, Haidar/AAR-6783-2021; Wang, Honggang/D-6079-2013
OI Sharif, Haidar/0000-0001-7235-6004; Wang, Honggang/0000-0001-9475-2630;
   Sharif-Kashani, Hamid/0000-0001-6229-2043
FU US NSF [0707944]; Taiwan National Science Council
   [NSC97-2219-E-006-004]; Direct For Computer & Info Scie & Enginr;
   Division Of Computer and Network Systems [0707944] Funding Source:
   National Science Foundation
FX Manuscript received September 26, 2008; revised February 07, 2009;
   accepted May 27, 2009. Date of publication May 18, 2010; date of current
   version July 16, 2010. This work was supported in part by US NSF grant
   No. 0707944 for wireless sensor networks research and in part by Taiwan
   National Science Council research grant NSC97-2219-E-006-004. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Song Ci.
CR Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468
   Li Q, 2004, IEEE T MULTIMEDIA, V6, P278, DOI 10.1109/TMM.2003.822792
   Li Z, 2007, IEEE T MULTIMEDIA, V9, P837, DOI 10.1109/TMM.2007.893338
   Ozcelebi T, 2007, IEEE T MULTIMEDIA, V9, P826, DOI 10.1109/TMM.2007.895670
   QIAO D, 2007, P ACM QSHINE AUG
   Qiao D., 2003, PROC MOBILE COMPUTIN, P161, DOI DOI 10.1145/938985.939003
   Schurgers C, 2001, ISLPED'01: PROCEEDINGS OF THE 2001 INTERNATIONAL SYMPOSIUM ON LOWPOWER ELECTRONICS AND DESIGN, P96, DOI 10.1109/LPE.2001.945382
   SIMON H, 1994, COMMUNICATION SYSTEM, P510
   Skraparlis D, 2003, IEEE T CONSUM ELECTR, V49, P417, DOI 10.1109/TCE.2003.1209535
   Sun QB, 2005, IEEE T MULTIMEDIA, V7, P480, DOI 10.1109/TMM.2005.846776
   Sun QB, 2008, P IEEE, V96, P97, DOI 10.1109/JPROC.2007.909926
   Sun QB, 2006, IEEE T CIRC SYST VID, V16, P1232, DOI 10.1109/TCSVT.2006.882540
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   van der Schaar M, 2007, IEEE T MULTIMEDIA, V9, P185, DOI 10.1109/TMM.2006.886384
   van Dijk TA, 2006, DISCOURSE STUD, V8, P5, DOI 10.1177/1461445606059544
   Varma K, 2004, IEEE SIGNAL PROC MAG, V21, P70, DOI 10.1109/MSP.2004.1359144
   Wang W, 2008, IEEE T MULTIMEDIA, V10, P1169, DOI 10.1109/TMM.2008.2001354
   Wang W, 2008, IEEE WCNC, P2810
   Willig TN, 2000, CURR OPIN HEMATOL, V7, P85, DOI 10.1097/00062752-200003000-00003
   Wong CK, 1999, IEEE ACM T NETWORK, V7, P502, DOI 10.1109/90.793005
   Zhang ZS, 2007, IEEE T CIRC SYST VID, V17, P544, DOI 10.1109/TCSVT.2006.888822
   Zhang ZS, 2007, IEEE T MULTIMEDIA, V9, P320, DOI 10.1109/TMM.2006.886281
NR 23
TC 17
Z9 18
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2010
VL 12
IS 5
BP 439
EP 447
DI 10.1109/TMM.2010.2050736
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 656DN
UT WOS:000282306500008
DA 2024-07-18
ER

PT J
AU Tang, JH
   Li, HJ
   Qi, GJ
   Chua, TS
AF Tang, Jinhui
   Li, Haojie
   Qi, Guo-Jun
   Chua, Tat-Seng
TI Image Annotation by Graph-Based Inference With Integrated
   Multiple/Single Instance Representations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image annotation; multiple/single instance learning
AB In most of the learning-based image annotation approaches, images are represented using multiple-instance (local) or single-instance (global) features. Their performances, however, are mixed as for certain concepts, the single-instance representations of images are more suitable, while for others, the multiple-instance representations are better. Thus this paper explores a unified learning framework that combines the multiple-instance and single-instance representations for image annotation. More specifically, we propose an integrated graph-based semi-supervised learning framework to utilize these two types of representations simultaneously. We further explore three strategies to convert from multiple-instance representation into a single-instance one. Experiments conducted on the COREL image dataset demonstrate the effectiveness and efficiency of the proposed integrated framework and the conversion strategies.
C1 [Tang, Jinhui; Li, Haojie; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117590, Singapore.
   [Qi, Guo-Jun] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL 61820 USA.
   [Qi, Guo-Jun] Univ Illinois, Beckman Inst, Champaign, IL 61820 USA.
C3 National University of Singapore; University of Illinois System;
   University of Illinois Urbana-Champaign; University of Illinois System;
   University of Illinois Urbana-Champaign
RP Tang, JH (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117590, Singapore.
EM lihj@comp.nus.edu.sg
RI Tang, Jinhui/KBR-0891-2024; Qi, Guo-Jun/AAH-8294-2019
OI Qi, Guo-Jun/0000-0003-3508-1851
CR [Anonymous], 1999, The Nature Statist. Learn. Theory
   [Anonymous], 2006, PROC IEEE C COMPUTER
   [Anonymous], 2004, P ECCV WORKSH STAT L
   [Anonymous], 2005, SEMISUPERVISED LEARN
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248
   Chen YX, 2004, J MACH LEARN RES, V5, P913
   Cusano C., 2004, P INTERNET IMAGING, V5304
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Grauman K., 2005, P IEEE C COMP VIS PA
   Guo Y., 2007, P IEEE C COMP VIS PA
   JEON J, 2003, P ACM SIGIR C
   Maron O., 1998, P 15 INT C MACH LEAR
   RAHMANI R, 2006, P 23 INT C MACH LEAR
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   STRICKER M, 2000, SPIE, V2420
   TANG J, 2007, P ACM MULT
   TANG J, 2008, P ACM MULTIMEDIA
   Tang JH, 2008, IEEE T MULTIMEDIA, V10, P620, DOI 10.1109/TMM.2008.921853
   TONG H, 2005, P ACM MULT
   WANG C, 2006, P ACM MULT
   WANG D, 2006, P EUR C MACH LEARN
   Wang F, 2008, IEEE T KNOWL DATA EN, V20, P55, DOI 10.1109/TKDE.2007.190672
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   ZHOU ZH, 2007, P 24 INT C MACH LEAR
NR 27
TC 87
Z9 95
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2010
VL 12
IS 2
BP 131
EP 141
DI 10.1109/TMM.2009.2037373
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 573OA
UT WOS:000275922000004
DA 2024-07-18
ER

PT J
AU Liang, C
   Guo, Y
   Liu, Y
AF Liang, Chao
   Guo, Yang
   Liu, Yong
TI Investigating the Scheduling Sensitivity of P2P Video Streaming: An
   Experimental Study
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Peer-to-peer networks; scheduling; system design; video streaming
AB Peer-to-peer (P2P) technology has recently been employed to deliver large scale video multicast services on the Internet. Considerable efforts have been made by both academia and industry on P2P streaming design. While academia mostly focus on exploring design space to approach the theoretical performance bounds, our recent measurement study on several commercial P2P streaming systems indicates that they are able to deliver good user quality of experience with seemingly simple designs. One intriguing question remains: how elaborate should a good P2P video streaming design be? Towards answering this question, we developed and implemented several representative P2P streaming designs, ranging from theoretically proved optimal designs to straightforward "naive" designs. Through an extensive comparison study on PlanetLab, we unveil several key factors contributing to the successes of simple P2P streaming designs, including system resource index, server capacity and chunk scheduling rule, peer download buffering and peering degree. We also identify regions where naive designs are inadequate and more elaborate designs can improve things considerably. Our study not only brings us better understandings and more insights into the operation of existing systems, it also sheds lights on the design of future systems that can achieve a good balance between the performance and the complexity.
C1 [Liang, Chao; Liu, Yong] NYU, Polytech Inst, Brooklyn, NY 11201 USA.
   [Guo, Yang] Thomson Corp Res, Princeton, NJ 08540 USA.
C3 New York University; New York University Tandon School of Engineering
RP Liang, C (corresponding author), NYU, Polytech Inst, Brooklyn, NY 11201 USA.
EM cliang@photon.poly.edu; yongliu@poly.edu; Yang.Guo@thomson.net
RI Liang, Chao/B-9709-2012
CR [Anonymous], P ACM SIGCOMM AS WOR
   [Anonymous], P IFIP NETW
   CHA M, 2007, P INT MEAS C
   Chen M., 2008, P ACM SIGMETRICS
   CHIU DM, 2006, P IEEE NETCOD
   Chu Y., 2000, P ACM SIGMETRICS
   Hei XJ, 2007, IEEE J SEL AREA COMM, V25, P1640, DOI 10.1109/JSAC.2007.071204
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Kumar R., 2007, P IEEE INFOCOM
   LIANG C, 2007, P GLOBECOM
   LIANG C, 2008, P INT C DISTR COMP S
   Liu Y., 2007, P ACM MULT
   Magharei N., 2007, P IEEE INFOCOM
   Massoulie L., 2007, P IEEE INFOCOM
   Pai V., 2005, P IPTPS
   *PLANETLAB, PLANETLAB HOM
   SENGUPTA S, 2008, P 2008 IEEE INT S IN
   Vishnumurthy V., 2006, P IEEE INFOCOM
   Wang M, 2007, IEEE J SEL AREA COMM, V25, P1655, DOI 10.1109/JSAC.2007.071205
   WU C, 2007, P INT C DISTR COMP S
   Zhang M, 2007, IEEE J SEL AREA COMM, V25, P1678, DOI 10.1109/JSAC.2007.071207
   ZHANG X, 2005, P IEEE INFOCOM
   TRICKLE HOMEPAGE
NR 23
TC 22
Z9 23
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 348
EP 360
DI 10.1109/TMM.2009.2012909
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, BW
   Wang, JC
   Wang, JF
AF Chen, Bo-Wei
   Wang, Jia-Ching
   Wang, Jhing-Fa
TI A Novel Video Summarization Based on Mining the Story-Structure and
   Semantic Relations Among Concept Entities
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Concept expansion tree; graph entropy; graph mining; structural video
   contents; video browsing; video indexing; video summarization
ID RETRIEVAL; ATTENTION; FRAMEWORK; MODEL
AB Video summarization techniques have been proposed for years to offer people comprehensive understanding of the whole story in the video. Roughly speaking, existing approaches can be classified into the two types: one is static storyboard, and the other is dynamic skimming. However, despite that these traditional methods give brief summaries for users, they still do not provide with a concept-organized and systematic view. In this paper, we present a structural video content browsing system and a novel summarization method by utilizing the four kinds of entities: who, what, where, and when to establish the framework of the video contents. With the assistance of the above-mentioned indexed information, the structure of the story can be built up according to the characters, the things, the places, and the time. Therefore, users can not only browse the video efficiently but also focus on what they are interested in via the browsing interface. In order to construct the fundamental system, we employ maximum entropy criterion to integrate visual and text features extracted from video frames and speech transcripts, generating high-level concept entities. A novel concept expansion method is introduced to explore the associations among these entities. After constructing the relational graph, we exploit graph entropy model to detect meaningful shots and relations, which serve as the indices for users. The results demonstrate that our system can achieve better performance and information coverage.
C1 [Chen, Bo-Wei; Wang, Jia-Ching; Wang, Jhing-Fa] Natl Cheng Kung Univ, Dept Elect Engn, Tainan 701, Taiwan.
C3 National Cheng Kung University
RP Chen, BW (corresponding author), Natl Cheng Kung Univ, Dept Elect Engn, Tainan 701, Taiwan.
EM chenbw@icwang.ee.ncku.edu.tw; wjc@icwang.ee.ncku.edu.tw;
   wangjf@csie.ncku.edu.tw
RI Chen, Bowei/AAB-7002-2021
OI Chen, Bowei/0000-0002-4045-3253
CR Aner A, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, pA237
   [Anonymous], 1998, Regression and Time Series Model Selection
   Argillander J, 2005, INT CONF ACOUST SPEE, P153
   Ba Tu Truong, 2000, Proceedings ACM Multimedia 2000, P219, DOI 10.1145/354384.354481
   BAGESHREE S, 2007, P 2007 C DIG LIB VAN, P127
   Bagga A, 2002, INT C PATT RECOG, P818, DOI 10.1109/ICPR.2002.1048428
   Berger AL, 1996, COMPUT LINGUIST, V22, P39
   Cerneková Z, 2006, IEEE T CIRC SYST VID, V16, P82, DOI 10.1109/TCSVT.2005.856896
   Christel M.G., 2008, Proceedings of the International Conference on Content-based Image and Video Retrieval (CIVR '08), P447
   Christel MG, 2001, INT CONF ACOUST SPEE, P1409, DOI 10.1109/ICASSP.2001.941193
   DETYNIECKI M, 2007, P INT WORKSH TRECVID, P65
   GAO S, 2007, P 15 ACM INT C MULT, P872
   Gibbon DC, 1998, EIGHTH INTERNATIONAL WORKSHOP ON RESEARCH ISSUES IN DATA ENGINEERING - CONTINUOUS-MEDIA DATABASES AND APPLICATIONS, PROCEEDINGS, P26, DOI 10.1109/RIDE.1998.658275
   Girgensohn A, 2001, COMPUTER, V34, P61, DOI 10.1109/2.947093
   Graham J, 2002, 2002 INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, DIGEST OF TECHNICAL PAPERS, P214, DOI 10.1109/ICCE.2002.1013998
   HAUBOLD A, 2007, P 6 ACM INT C IM VID, P14
   Hoogs A, 2003, PROC CVPR IEEE, P327
   Hsu WHM, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1091, DOI 10.1109/ICME.2004.1394400
   HULTH A, 2006, P 21 INT C COMP LING, P537
   Kang SeungShik, 2003, P 6 INT WORKSHOP INF, P132
   KATASHI N, 2002, P 19 INT C COMP LING, P1
   Khan L, 2004, VLDB J, V13, P71, DOI 10.1007/s00778-003-0105-1
   Li Y, 2006, IEEE SIGNAL PROC MAG, V23, P79
   Li Z, 2005, IEEE T CIRC SYST VID, V15, P1245, DOI 10.1109/TCSVT.2005.854230
   Liu H, 2003, LECT NOTES ARTIF INT, V2680, P218
   Lu S., 2005, P IEEE AER C 5 12 MA, P1
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Miller G., 1990, INT J LEXICOGR, V3, P245, DOI DOI 10.1093/IJL/3.4.245
   Navigli R, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1683
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Odobez JM, 2003, LECT NOTES COMPUT SC, V2728, P310
   ODOBEZ JM, 2003, P 3 INT WORKSH CONT, P94
   Pan JY, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P491, DOI 10.1109/ICDM.2004.10033
   Patwardhan S., 2005, Interactive poster and demonstration sessions, P73
   PEKER KA, 2005, P 2005 INT WORKSH VE
   Pelleg D., 2000, P 17 INT C MACH LEAR, DOI DOI 10.1007/3-540-44491-2_3
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Rui Y, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P237, DOI 10.1109/MMCS.1998.693648
   Rui Y, 1999, MULTIMEDIA SYST, V7, P359, DOI 10.1007/s005300050138
   Schwarzkopf O.., 2000, Computational Geometry: Algorithms and Applications, V2nd
   Shetty J., 2005, P 3 INT WORKSHOP LIN, P74
   Singh L, 2007, IEEE INT CONF INF VI, P672
   Tan L, 2007, INFORM SCIENCES, V177, P3110, DOI 10.1016/j.ins.2007.01.029
   TANG L, 2006, P INT C INT US INT I, P318
   Taskiran CM, 2006, IEEE T MULTIMEDIA, V8, P775, DOI 10.1109/TMM.2006.876282
   Truong B. T., 2007, P INT WORKSH TRECVID, P30, DOI [10.1145/1290031.1290036, DOI 10.1145/1290031.1290036]
   Tsai TH, 2005, IEEE INT SYMP CIRC S, P4590
   Upcroft B., 2004, P 2004 AUSTR C ROB A
   VELIVELLI A, 2006, P 2006 IEEE INT C CO, P115
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   You JY, 2007, IEEE T CIRC SYST VID, V17, P273, DOI 10.1109/TCSVT.2007.890857
   Zhu XQ, 2003, MULTIMEDIA SYST, V9, P31, DOI 10.1007/s00530-003-0076-5
NR 54
TC 83
Z9 91
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 295
EP 312
DI 10.1109/TMM.2008.2009703
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800010
DA 2024-07-18
ER

PT J
AU Zhang, XP
   Wang, SZ
AF Zhang, Xinpeng
   Wang, Shuozhong
TI Fragile Watermarking With Error-Free Restoration Capability
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Error-free restoration; fragile watermarking; lossless data hiding
ID IMAGE AUTHENTICATION; SCHEME; LOCALIZATION; EXPANSION; FRAMEWORK
AB This paper proposes a novel fragile watermarking scheme capable of perfectly recovering the original image from its tampered version. In the scheme, a tailor-made watermark consisting of reference-bits and check-bits is embedded into the host image using a lossless data hiding method. On the receiver side, by comparing the extracted and calculated check-bits, one can identify the tampered image-blocks. Then, the reliable reference-bits extracted from other blocks are used to exactly reconstruct the original image. Although content replacement may destroy a portion of the embedded watermark data, as long as the tampered area is not too extensive, the original image information can be restored without any error.
C1 [Zhang, Xinpeng; Wang, Shuozhong] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200072, Peoples R China.
C3 Shanghai University
RP Zhang, XP (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200072, Peoples R China.
EM xzhang@shu.edu.cn
FU National Natural Science Foundation of China [60872116, 60832010,
   60773079]; High-Tech Research and Development Program of China
   [2007AA01Z477]; Shanghai Leading Academic Discipline Project [T0102]
FX Manuscript received February 05, 2008 revised August 22, 2008. Current
   version published December 10, 2008, This work was supported in part by
   the National Natural Science Foundation of China under Grants 60872116.
   60832010, and 60773079, in part by the High-Tech Research and
   Development Program of China under Grant 2007AA01Z477, and in part by
   the Shanghai Leading Academic Discipline Project under-Grants T0102. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Alex C. Kot.
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   Altun O, 2006, IEEE T INF FOREN SEC, V1, P479, DOI 10.1109/TIFS.2006.885018
   Celik MU, 2006, IEEE T IMAGE PROCESS, V15, P1042, DOI 10.1109/TIP.2005.863053
   Celik MU, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P157
   De Vleeschouwer C, 2003, IEEE T MULTIMEDIA, V5, P97, DOI 10.1109/TMM.2003.809729
   De Vleeschouwer C, 2002, P IEEE, V90, P64, DOI 10.1109/5.982406
   Fei CH, 2006, IEEE T INF FOREN SEC, V1, P43, DOI 10.1109/TIFS.2005.863505
   Fridrich J, 2002, P SOC PHOTO-OPT INS, V4675, P572, DOI 10.1117/12.465317
   Fridrich J, 2002, PROC SPIE, V4675, P691, DOI 10.1117/12.465330
   Fridrich J., 1999, P 1999 INT C IM PROC, P792, DOI [10.1109/ICIP.1999.817228, DOI 10.1109/ICIP.1999.817228]
   Goljan M., 2001, 4th Information Hiding Workshop, LNCS, V2137, P27, DOI DOI 10.1007/3-540-45496-9
   He HJ, 2006, LECT NOTES COMPUT SC, V4283, P422
   Holliman M, 2000, IEEE T IMAGE PROCESS, V9, P432, DOI 10.1109/83.826780
   Liu SH, 2007, APPL MATH COMPUT, V185, P869, DOI 10.1016/j.amc.2006.07.036
   Lu HT, 2003, ELECTRON LETT, V39, P898, DOI 10.1049/el:20030589
   Lu ZM, 2005, IEEE T IMAGE PROCESS, V14, P822, DOI 10.1109/TIP.2005.847324
   Maeno K, 2006, IEEE T MULTIMEDIA, V8, P32, DOI 10.1109/TMM.2005.861293
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   Suthaharan S, 2004, PATTERN RECOGN LETT, V25, P1893, DOI 10.1016/j.patrec.2004.08.017
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wong PW, 2001, IEEE T IMAGE PROCESS, V10, P1593, DOI 10.1109/83.951543
   Wu JH, 2004, IEEE IMAGE PROC, P1573
   Yang HJ, 2006, IEEE SIGNAL PROC LET, V13, P741, DOI 10.1109/LSP.2006.879829
   Zhang X, 2007, IEEE SIGNAL PROC LET, V14, P727, DOI 10.1109/LSP.2007.896436
   Zhu X, 2007, SIGNAL PROCESS-IMAGE, V22, P515, DOI 10.1016/j.image.2007.03.004
NR 26
TC 115
Z9 124
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1490
EP 1499
DI 10.1109/TMM.2008.2007334
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600006
DA 2024-07-18
ER

PT J
AU Albanese, M
   Chellappa, R
   Moscato, V
   Picariello, A
   Subrahmanian, VS
   Turaga, P
   Udrea, O
AF Albanese, Massimiliano
   Chellappa, Rama
   Moscato, Vincenzo
   Picariello, Antonio
   Subrahmanian, V. S.
   Turaga, Pavan
   Udrea, Octavian
TI A Constrained Probabilistic Petri Net Framework for Human Activity
   Detection in Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Algorithms; machine vision; Petri nets; surveillance
ID SURVEILLANCE
AB Recognition of human activities in restricted settings such as airports, parking lots and banks is of significant interest in security and automated surveillance systems. In such settings, data is usually in the form of surveillance videos with wide variation in quality and granularity. Interpretation and identification of human activities requires an activity model that a) is rich enough to handle complex multi-agent interactions, b) is robust to uncertainty in low-level processing and c) can handle ambiguities in the unfolding of activities. We present a computational framework for human activity representation based on Petri nets. We propose an extension-Probabilistic Petri Nets (PPN)-and show how this model is well suited to address each of the above requirements in a wide variety of settings. We then focus on answering two types of questions: (i) what are the minimal sub-videos in which a given activity is identified with a probability above a certain threshold and (ii) for a given video, which activity from a given set occurred with the highest probability? We provide the PPN-MPS algorithm for the first problem, as well as two different algorithms (naive PPN-MPA and PPN-MPA) to solve the second. Our experimental results on a dataset consisting of bank surveillance videos and an unconstrained TSA tarmac surveillance dataset show that our algorithms are both fast and provide high quality results.
C1 [Albanese, Massimiliano; Chellappa, Rama; Subrahmanian, V. S.; Turaga, Pavan; Udrea, Octavian] Univ Maryland, Inst Adv Comp Studies, College Pk, MD 20742 USA.
   [Picariello, Antonio] Univ Naples Federico II, Dipartimento Informat & Sistemist, Naples, Italy.
C3 University System of Maryland; University of Maryland College Park;
   University of Naples Federico II
RP Albanese, M (corresponding author), Univ Maryland, Inst Adv Comp Studies, College Pk, MD 20742 USA.
EM albanese@umiacs.utnd.edu; rama@umiacs.umd.edu; vmoscato@unina.it;
   pious@unina.it; vs@umiacs.umd.edu; pturaga@umiacs.umd.edu;
   udrea@umiacs.umd.edu
RI Chellappa, Rama/B-6573-2012; Moscato, Vincenzo/H-2526-2012; Turaga,
   Pavan/W-6186-2019; Chellappa, Rama/AAV-8690-2020; Albanese,
   Massimiliano/H-5093-2019; Subrahmanian, Venkatramanan/ABA-7399-2021;
   PICARIELLO, Antonio/L-6820-2015
OI Albanese, Massimiliano/0000-0002-2675-5810; Moscato,
   Vincenzo/0000-0002-0754-7696; PICARIELLO, Antonio/0000-0003-4804-1007
CR Albanese M, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1802
   Bobick A, 1996, THIRD IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV '96, PROCEEDINGS, P39, DOI 10.1109/ACV.1996.571995
   Brand M, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P94, DOI 10.1109/AFGR.1996.557249
   BREMOND F, 1997, P INT FLOR AI RES S
   BUXTON H, 1995, ARTIF INTELL, V78, P431, DOI 10.1016/0004-3702(95)00041-0
   CAI Q, 1996, P 13 INT C PATT REC
   Cardoso J, 1999, IEEE T SYST MAN CY B, V29, P573, DOI 10.1109/3477.790440
   CASTEL C, 1996, P ECCV WORKSH CONC D
   CHELLAPPA R, 2008, UNDERSTANDING EVENTS
   CHEN S, 1990, SOFTWARE MAINTENANCE, V2, P3
   DAVIES TF, 1994, ISRAEL J MED SCI, V30, P2
   GHANEM N, 2004, P 2 IEEE WORKSH EV M, P112
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Hongeng S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P84, DOI 10.1109/ICCV.2001.937608
   HUANG T, 1994, PROCEEDINGS OF THE TWELFTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P966
   Ivanov YA, 2000, IEEE T PATTERN ANAL, V22, P852, DOI 10.1109/34.868686
   Joo SW, 2006, IEEE IMAGE PROC, P2897, DOI 10.1109/ICIP.2006.313035
   KHAN S, 2000, P AS C COMP VIS JAN
   Lesire C, 2005, LECT NOTES COMPUT SC, V3536, P329
   MARSAN MA, 1991, MICROELECTRON RELIAB, V31, P699, DOI 10.1016/0026-2714(91)90010-5
   Moore D, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P770
   MURATA T, 1989, P IEEE, V77, P541, DOI 10.1109/5.24143
   Pearl J., 1988, PROBABILISTIC REASON
   Prati A, 2003, IEEE T PATTERN ANAL, V25, P918, DOI 10.1109/TPAMI.2003.1206520
   Shet VD, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P224
   Siebel NilsT., 2004, ECCV 2004 workshop Applications of Computer Vision (ACV), P103
   SIEBEL NT, 2002, P 7 EUR C COMP VIS E, V6, P373
   Vaswani N, 2003, PROC CVPR IEEE, P633
   VU VT, 2003, P 18 INT JOINT C ART
   Zhu Y, 2000, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2000.855879
NR 30
TC 46
Z9 54
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 982
EP 996
DI 10.1109/TMM.2008.2001369
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600004
DA 2024-07-18
ER

PT J
AU Chen, YH
   Chia, TL
   Lee, YK
   Huang, SY
   Wang, RZ
AF Chen, Yen-Hsu
   Chia, Tsorng-Lin
   Lee, Yeuan-Kuen
   Huang, Shih-Yu
   Wang, Ran-Zan
TI A vision-based augmented-reality system for multiuser collaborative
   environments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE augmented reality; camera calibration; multiuser collaborative
   environment; pseudo-random arrays
AB This work presented a novel vision-based augmented-reality system for applications in multiuser collaborative environments. The kernel technology of this vision-based system locates the cameras that are utilized to point and simulate the positions of multiple viewers. Camera calibration based on computer vision is employed during the camera's locating process. The applications in multiuser collaborative environments allow the viewers to view various positions and in numerous directions. However, traditional calibration approaches are not suitable for these cases sufficiently. A novel calibration pattern based on pseudo-random arrays is designed for multiuser collaborative applications. The pattern has a simple and regular structure, easily extracts features, achieves robust recognition using local information, and does not limit viewer positions and directions. Experimental results indicate that the proposed system provides a effective platform for applications in multiuser collaborative environments.
C1 [Chen, Yen-Hsu; Lee, Yeuan-Kuen; Huang, Shih-Yu] Ming Chuan Univ, Dept Comp Sci & Informat Engn, Tao Yuan 333, Taiwan.
   [Chia, Tsorng-Lin] Ming Chuan Univ, Dept Comp & Commun Engn, Tao Yuan 333, Taiwan.
   [Wang, Ran-Zan] Yuan Ze Univ, Dept Comp Engn & Sci, Chungli 320, Taiwan.
C3 Ming Chuan University; Ming Chuan University; Yuan Ze University
RP Chen, YH (corresponding author), Ming Chuan Univ, Dept Comp Sci & Informat Engn, Tao Yuan 333, Taiwan.
EM s0170234@mail.csie.edu.tw; tlchia@mcu.edu.tw; yklee@mcu.edu.tw;
   syhuang@mcu.edu.tw; rzwang@saturn.yzu.edu.tw
RI Chen, Yu-Cheng/ISS-5682-2023
OI Chen, Yu-Cheng/0000-0003-1696-4667; Chen, Yen-hsu/0000-0002-8667-4844
CR Abawi DF, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P260, DOI 10.1109/ISMAR.2004.8
   [Anonymous], 1999, proceedings of Conference on Computer Vision and Pattern Recognition, DOI DOI 10.1109/CVPR.1999.786974
   Baxes G. A., 1994, Digital Image Processing-Principles and Applications
   Berlekamp E. R., 1984, Algebraic Coding Theory
   Bianchi G, 2005, International Symposium on Mixed and Augmented Reality, Proceedings, P188
   Chia TL, 1996, IEEE T IMAGE PROCESS, V5, P1276, DOI 10.1109/83.506763
   Fiala M, 2005, PROC CVPR IEEE, P590
   FIALA M, 2005, 48306ERB1130 NRC
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Jiang G, 2005, IEEE I CONF COMP VIS, P333
   Kawano T, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P262, DOI 10.1109/ISMAR.2003.1240711
   Kim JS, 2005, IEEE T PATTERN ANAL, V27, P637, DOI 10.1109/TPAMI.2005.80
   Lin S., 1983, Error Control Coding: Fundamentals and Applications
   LLOYD S, 1991, HP91159
   MACWILLIAMS FJ, 1976, P IEEE, V64, P1715, DOI 10.1109/PROC.1976.10411
   MATSUNAGA C, 2000, IAPR WORKSH MACH VIS, P561
   Moehring M., 2004, ISMAR 2004 3 IEEE AC, P252, DOI DOI 10.1109/ISMAR.2004.63
   Sherman WilliamR., 2003, UNDERSTANDING VIRTUA
   SINLAPEECHEEWA C, 2002, P IEEE ICIT, V1, P405
   VUYLSTEKE P, 1990, IEEE T PATTERN ANAL, V12, P148, DOI 10.1109/34.44402
   Wagner D, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P335, DOI 10.1109/ISMAR.2003.1240747
   Zhang Zhengyou., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, V1, P0, DOI DOI 10.1109/ICCV.1999.791289
NR 22
TC 4
Z9 4
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2008
VL 10
IS 4
BP 585
EP 595
DI 10.1109/TMM.2008.921741
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EK
UT WOS:000258767200004
DA 2024-07-18
ER

PT J
AU Wang, SY
   Laih, CS
AF Wang, Shyh-Yih
   Laih, Chi-Sung
TI Efficient key distribution for access control in pay-TV systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE access control; hierarchy; key distribution; pay-TV
ID SCHEME; MANAGEMENT
AB The conditional access system (CAS) is an essential part of digital pay-TV systems for controlling access to the program services. Conventionally, due to the restrictions of bandwidth and computational capability, a CAS only supports period subscription services that are charged on a monthly basis. In this paper, based on the concept of hierarchical key assignment, we propose three key distribution schemes for the access control of pay-TV systems. With these schemes, a CAS can support more charging strategies for service providers, such as adopting a smaller charging unit and allowing a subscription of any subset of channels with little communication and computational overhead. In addition, the piracy management problem can also be dealt with easily.
C1 [Wang, Shyh-Yih] Natl Kaohsiung Normal Univ, Dept Optoelect & Commun Engn, Kaohsiung 802, Taiwan.
   [Laih, Chi-Sung] Natl Cheng Kung Univ, Dept Elect Engn, Tainan 701, Taiwan.
   [Laih, Chi-Sung] NCKU, Comp & Network Ctr, Tainan, Taiwan.
C3 National Kaohsiung Normal University; National Cheng Kung University;
   National Cheng Kung University
RP Wang, SY (corresponding author), Natl Kaohsiung Normal Univ, Dept Optoelect & Commun Engn, Kaohsiung 802, Taiwan.
EM swee.wang@msa.hinet.net; laihcs@cembox.ncku.edu.tw
FU TWISC@NCKU; National Science Council [NSC 96-2219-E-006-009]
FX This work wits supported in part by TWISC@NCKU, National Science Council
   under the Grants NSC 96-2219-E-006-009. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Mohan S. Kankanhalli.
CR AKL SG, 1983, ACM T COMPUT SYST, V1, P239, DOI 10.1145/357369.357372
   [Anonymous], 1995, EBU Technical Review, P64
   [Anonymous], 1996, 289 ETR
   [Anonymous], 2001, LECT NOTES COMPUTER
   Canetti R, 1999, IEEE INFOCOM SER, P708, DOI 10.1109/INFCOM.1999.751457
   Crinon RJ, 2006, P IEEE, V94, P102, DOI 10.1109/JPROC.2005.861020
   *DEVERSYS CORP, ULTR HIGH SPEED AES
   Emmanuel S, 2003, MULTIMEDIA SYST, V8, P444, DOI 10.1007/s00530-002-0066-z
   Fiat A., 1993, LECT NOTES COMPUTER, P480, DOI DOI 10.1007/3-540-48329-2
   *FIPS, 2001, 197 FIPS
   Halevy D, 2002, LECT NOTES COMPUT SC, V2442, P47
   Harney H., 1999, LOGICAL KEY HIERARCH
   Huang YL, 2004, IEEE T MULTIMEDIA, V6, P760, DOI 10.1109/TMM.2004.834861
   Kanjanarin W, 2001, NINTH IEEE INTERNATIONAL CONFERENCE ON NETWORKS, PROCEEDINGS, P140, DOI 10.1109/ICON.2001.962331
   LEE JW, 1996, P INT C CRYPT INF SE, P82
   Liu BF, 2004, IEEE T CONSUM ELECTR, V50, P632, DOI 10.1109/TCE.2004.1309442
   Loebbecke C, 2005, PACIFIC ASIA CONFERENCE ON INFORMATION SYSTEMS 2005, SECTIONS 1-8 AND POSTER SESSIONS 1-6, P1046
   MACQ BM, 1995, P IEEE, V83, P944, DOI 10.1109/5.387094
   Sherman AT, 2003, IEEE T SOFTWARE ENG, V29, P444, DOI 10.1109/TSE.2003.1199073
   Sun Y, 2004, IEEE INFOCOM SER, P1296
   Tu FK, 1999, IEEE T CONSUM ELECTR, V45, P151, DOI 10.1109/30.754430
   Tzeng WG, 2002, IEEE T KNOWL DATA EN, V14, P182, DOI 10.1109/69.979981
   *US DEP COMM, 2002, FIPS PUBL, V1802
   WALLNER DM, 1999, 2627 RFC IETF
   Wang DS, 2006, CHEM-ASIAN J, V1, P91, DOI 10.1002/asia.200600078
   Wu YY, 2006, P IEEE, V94, P8, DOI 10.1109/JPROC.2005.861000
   FRESH VISION NEW DVB
NR 27
TC 22
Z9 26
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 480
EP 492
DI 10.1109/TMM.2008.917417
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100016
DA 2024-07-18
ER

PT J
AU Yang, HJ
   Kot, AC
   Rahardja, S
AF Yang, Huijuan
   Kot, Alex C.
   Rahardja, Susanto
TI Orthogonal data embedding for binary images in morphological transform
   domain - A high-capacity approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE authentication; binary images; data hiding; morphological binary wavelet
   transform; orthogonal embedding
ID DIGITAL WATERMARKING; AUTHENTICATION; DECOMPOSITION; SCHEMES
AB This paper proposes a data-hiding technique for binary images in morphological transform domain for authentication purpose. To achieve blind watermark extraction, it is difficult to use the detail coefficients directly as a location map to determine the data-hiding locations. Hence, we view flipping an edge pixel in binary images as shifting the edge location one pixel horizontally and vertically. Based on this observation, we propose an interlaced morphological binary wavelet transform to track the shifted edges, which thus facilitates blind watermark extraction and incorporation of cryptographic signature. Unlike existing block-based approach, in which the block size is constrained by 3 x 3 pixels or larger, we process an image in 2 x 2 pixel blocks. This allows flexibility in tracking the edges and also achieves low computational complexity. The two processing cases that flipping the candidates of one does not affect the flippability conditions of another are employed for orthogonal embedding, which renders more suitable candidates can be identified such that a larger capacity can be achieved. A novel effective Backward-Forward Minimization method is proposed, which considers both backwardly those neighboring processed embeddable candidates and forwardly those unprocessed flippable candidates that may be affected by flipping the current pixel. In this way, the total visual distortion can be minimized. Experimental results demonstrate the validity of our arguments.
C1 [Yang, Huijuan; Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Rahardja, Susanto] ASTAR, Inst Infocomm Res I2R, Signal Proc Dept, Singapore, Singapore.
C3 Nanyang Technological University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Yang, HJ (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM ehjyang@ntu.edu.sg; eackot@ntu.edu.sg; rsusanto@i2r.a-star.edu.sg
RI Yang, Huijuan/HQZ-2610-2023
OI Yang, Huijuan/0000-0002-5433-778X
CR Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Cox I., 2001, Digital Watermarking
   Furht B., 2005, MULTIMEDIA SECURITY
   Heijmans HJAM, 2000, IEEE T IMAGE PROCESS, V9, P1897, DOI 10.1109/83.877211
   Hwang K.F., 2002, Proceedings of Pacific Rim Workshop on Digital Steganography, P71
   Kim HY, 2005, LECT NOTES COMPUT SC, V3304, P125
   Kundur D, 1999, P IEEE, V87, P1167, DOI 10.1109/5.771070
   Lin CY, 2001, IEEE T CIRC SYST VID, V11, P153, DOI 10.1109/76.905982
   Liu Y, 1999, P SOC PHOTO-OPT INS, V3657, P317, DOI 10.1117/12.344682
   Lu HP, 2004, IEEE SIGNAL PROC LET, V11, P228, DOI 10.1109/LSP.2003.821748
   Lu HP, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P300
   Mei Q, 2001, PROC SPIE, V4314, P369, DOI 10.1117/12.435420
   Swanson MD, 1996, IEEE T IMAGE PROCESS, V5, P1637, DOI 10.1109/83.544571
   Sweldens W, 1998, SIAM J MATH ANAL, V29, P511, DOI 10.1137/S0036141095289051
   Tseng YC, 2002, IEEE T COMPUT, V51, P873, DOI 10.1109/TC.2002.1017706
   Wong PW, 2001, IEEE T IMAGE PROCESS, V10, P1593, DOI 10.1109/83.951543
   WU J, 2004, P IEEE INT C MULT EX, V2, P931
   Wu M, 2004, IEEE T MULTIMEDIA, V6, P528, DOI 10.1109/tmm.2004.830814
   Yang HJ, 2007, IEEE T MULTIMEDIA, V9, P475, DOI 10.1109/TMM.2006.887990
   Zhu BB, 2004, IEEE SIGNAL PROC MAG, V21, P40, DOI 10.1109/MSP.2004.1276112
NR 20
TC 36
Z9 39
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 339
EP 351
DI 10.1109/TMM.2008.917404
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100004
DA 2024-07-18
ER

PT J
AU Liu, D
   Chen, T
AF Liu, David
   Chen, Tsuhan
TI DISCOV: A framework for discovering objects in video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia data mining; unsupervised learning; video object discovery;
   video segmentation
ID FEATURES; MOTION
AB This paper presents a probabilistic framework for discovering objects in video. The video can switch between different shots, the unknown objects can leave or enter the scene at multiple times, and the background can be cluttered. The framework consists of an appearance model and a motion model. The appearance model exploits the consistency of object parts in appearance across frames. We use maximally stable extremal regions as observations in the model and hence provide robustness to object variations in scale, lighting and viewpoint. The appearance model provides location and scale estimates of the unknown objects through a compact probabilistic representation. The compact representation contains knowledge of the scene at the object level, thus allowing us to augment it with motion information using a motion model. This framework can be applied to a wide range of different videos and object types, and provides a basis for higher level video content analysis tasks. We present applications of video object discovery to video content analysis problems such as video segmentation and threading, and demonstrate superior performance to methods that exploit global image statistics and frequent itemset data mining techniques.
C1 [Liu, David; Chen, Tsuhan] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University
RP Liu, D (corresponding author), Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
EM dliu@cmu.edu; tsuhan@cmu.edu
OI Chen, Tsuhan/0000-0003-3951-7931
CR Bar-Shalom Y., 1988, TRACKING DATA ASS
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Cressie N., 1993, STAT SPATIAL DATA
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Dollar P., 2005, VISUAL SURVEILLANCE, V14, P65, DOI DOI 10.1109/VSPETS.2005.1570899
   Doulamis A, 2003, IEEE T NEURAL NETWOR, V14, P616, DOI 10.1109/TNN.2003.810605
   Duda R., 1973, Pattern Classification and Scene Analysis
   Ebadollahi S, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P881, DOI 10.1109/ICME.2006.262691
   Elgammal A, 2002, P IEEE, V90, P1151, DOI 10.1109/JPROC.2002.801448
   Fergus R, 2005, IEEE I CONF COMP VIS, P1816
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Günsel B, 1998, J ELECTRON IMAGING, V7, P592, DOI 10.1117/1.482613
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   Huber Peter J, 2011, ROBUST STAT, P1248
   Irani M, 1998, IEEE T PATTERN ANAL, V20, P577, DOI 10.1109/34.683770
   Ke QF, 2004, PROC CVPR IEEE, P592
   Kubica J., 2005, P NIPS, P691
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Leordeanu M, 2005, PROC CVPR IEEE, P1142
   LEUNG T, 2004, P EUR C COMP VIS, P203
   LIU D, 2006, P IEEE CVPR WORKSH P, P16
   Liu D, 2007, IEEE I CONF COMP VIS, P191
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma YF, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P94
   Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384
   Mittal A, 2004, PROC CVPR IEEE, P302
   NEILL DB, 2005, ADV NEURAL INFORM PR, P969
   NIEBLES JC, 2006, P BRIT MACH VIS C, P814
   Pan ZL, 2007, IEEE T MULTIMEDIA, V9, P268, DOI 10.1109/TMM.2006.887992
   Quelhas P, 2005, IEEE I CONF COMP VIS, P883
   Ramanan D, 2006, IEEE T PATTERN ANAL, V28, P1319, DOI 10.1109/TPAMI.2006.155
   Sand P., 2006, Proc. CVPR'06, V2, P2195
   Schneiderman H, 2004, INT J COMPUT VISION, V56, P151, DOI 10.1023/B:VISI.0000011202.85607.00
   Sivic J, 2005, IEEE I CONF COMP VIS, P370
   Sivic J, 2004, PROC CVPR IEEE, P488
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   TAN P, 2006, P EUR C COMP VIS, P58
   UNNIKRISHNAN R, 2006, P BRIT MACH VIS C, P124
   Wang J., 2003, Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, P236
   Winn J, 2005, IEEE I CONF COMP VIS, P756
   Wixson L, 2000, IEEE T PATTERN ANAL, V22, P774, DOI 10.1109/34.868680
   Xie LX, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P297, DOI 10.1109/ICME.2006.262457
   Zhao W, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P752, DOI 10.1109/MMCS.1999.778579
NR 44
TC 22
Z9 25
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2008
VL 10
IS 2
BP 200
EP 208
DI 10.1109/TMM.2007.911781
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 254HC
UT WOS:000252576700004
DA 2024-07-18
ER

PT J
AU Yankov, D
   Keogh, E
   Wei, L
   Xi, XP
   Hodges, W
AF Yankov, Dragomir
   Keogh, Eamonn
   Wei, Li
   Xi, Xiaopeng
   Hodges, Wendy
TI Fast best-match shape searching in rotation-invariant metric spaces
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 2-D shapes; indexing; nearest neighbor queries; metrics; rotation
   invariance
ID RECOGNITION; POINTS
AB Object recognition and content-based image retrieval systems rely heavily on the accurate and efficient identification of 2-D shapes. Features such as color, texture, positioning etc., are insufficient to convey the information that could be obtained through shape analysis. A fundamental requirement in this analysis is that shape similarities are computed invariantly to basic geometric transformations, e.g., scaling, shifting, and most importantly, rotations. And while scale and shift invariance are easily achievable through a suitable shape representation, rotation invariance is much harder to deal with. In this work, we explore the metric properties of the rotation-invariant distance measures and propose an algorithm for fast similarity search in the shape space. The algorithm can be utilized in a number of important data mining tasks such as shape clustering and classification, or for discovering of motifs and discords in large image collections. The technique is demonstrated to introduce a dramatic speed-up over the current approaches, and is guaranteed to introduce no false dismissals.
C1 [Yankov, Dragomir; Keogh, Eamonn; Wei, Li; Xi, Xiaopeng] Univ Calif Riverside, Dept Comp Sci & Engn, Riverside, CA 92507 USA.
   [Hodges, Wendy] Univ Texas Permian Basin, Dept Sci & Math, Odessa, TX 79762 USA.
C3 University of California System; University of California Riverside;
   University of Texas System
RP Yankov, D (corresponding author), Univ Calif Riverside, Dept Comp Sci & Engn, Riverside, CA 92507 USA.
EM dyankov@cs.ucr.edu; eamonn@cs.ucr.edu; wli@cs.ucr.edu; xxi@cs.ucr.edu;
   hodges_w@utpb.edu
CR Adamek T, 2004, IEEE T CIRC SYST VID, V14, P742, DOI 10.1109/TCSVT.2004.826776
   Adamek Tomasz., 2003, P 5 ACM SIGMM INT WO, P138
   [Anonymous], HPL2002251
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BENGIO J, 2003, ADV NEURAL INFORM PR, V16
   Bhanu B, 2004, INT C PATT RECOG, P499, DOI 10.1109/ICPR.2004.1333820
   BORGEFORS G, 1988, IEEE T PATTERN ANAL, V10, P849, DOI 10.1109/34.9107
   BURKHARD WA, 1973, COMMUN ACM, V16, P230, DOI 10.1145/362003.362025
   Cardone A., 2003, J COMPUT INF SCI ENG, V3, P109, DOI DOI 10.1115/1.1577356
   CHANG CC, 1991, PATTERN RECOGN, V24, P1053, DOI 10.1016/0031-3203(91)90121-K
   FUKUNAGA K, 1975, IEEE T COMPUT, VC 24, P750, DOI 10.1109/T-C.1975.224297
   Hodges WL, 2004, MOL PHYLOGENET EVOL, V31, P961, DOI 10.1016/j.ympev.2003.11.005
   Iwabe N, 2005, MOL BIOL EVOL, V22, P810, DOI 10.1093/molbev/msi075
   Jalba AC, 2005, MACH VISION APPL, V16, P217, DOI 10.1007/s00138-005-0175-8
   KARTIKEYAN B, 1989, IEEE T PATTERN ANAL, V11, P977, DOI 10.1109/34.35501
   KEOGH E, 2002, KDD 02, P102, DOI DOI 10.1145/775047.775062
   Kim SW, 2001, PROC INT CONF DATA, P607, DOI 10.1109/ICDE.2001.914875
   Mollineda RA, 2002, INT J PATTERN RECOGN, V16, P291, DOI 10.1142/S0218001402001678
   O'Brien M., 2003, Essential Tensions in Archaeological Method and Theory, P115
   Olson CF, 1997, IEEE T IMAGE PROCESS, V6, P103, DOI 10.1109/83.552100
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   SHAPIRO M, 1977, COMMUN ACM, V20, P339, DOI 10.1145/359581.359599
   Tabbone S, 2006, COMPUT VIS IMAGE UND, V102, P42, DOI 10.1016/j.cviu.2005.06.005
   Ueno K, 2006, IEEE DATA MINING, P623
   VELTKAMP R, 2006, IFCS C DAT SCI CLASS
   Vlachos M., 2004, P 2004 ACM SIGMOD IN, P131, DOI DOI 10.1145/1007568.1007586
   Wang ZY, 2000, LECT NOTES COMPUT SC, V1929, P477
   Wei L, 2006, IEEE DATA MINING, P711
   Yankov D, 2006, IEEE DATA MINING, P1167
   YIANILOS PN, 1993, PROCEEDINGS OF THE FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P311
   Zhang SG, 2001, NETWORKS, V37, P102, DOI 10.1002/1097-0037(200103)37:2<102::AID-NET5>3.0.CO;2-S
   Zunic J, 2006, LECT NOTES COMPUT SC, V3852, P11
NR 32
TC 5
Z9 8
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2008
VL 10
IS 2
BP 230
EP 239
DI 10.1109/TMM.2007.911824
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 254HC
UT WOS:000252576700007
DA 2024-07-18
ER

PT J
AU Guo, JM
   Pei, SC
   Lee, H
AF Guo, Jin-Ming
   Pei, Soo-Chang
   Lee, Hua
TI Paired subimage matching watermarking method on ordered dither images
   and its high-quality progressive coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bit-interleaving; error diffusion; halftone; ordered dither;
   watermarking
ID HALF-TONE IMAGES; ERROR DIFFUSION
AB In this paper, we present two novel robust methods for embedding watermarks into dithered halftone images. The first method is named Paired Subimage Matching Ordered Dithering (PSMOD), of which the decoder is provided with a priori information of the original watermark, and the corresponding application is copyright protection. The other method, Blind Paired Subimage Matching Ordered Dithering (BPSMOD), does not require the knowledge of the original watermark, and the main application is secret communication. Both methods utilize the bit and sub-subimage interleaving preprocesses. The experiments show that both techniques are sufficiently robust to guard against the cropping, tampering, and print-and-scan degradation processes, in either B/W or color dithered images. Both techniques are also sufficiently flexible for various levels of embedded capacities. Furthermore, a novel progressive coding scheme is also presented in this paper for the efficient display of dithered images. After the preprocessing of bit-interleaving, this algorithm utilizes the characteristic of reordered image to determine the transmitting order and then progressively reconstructs the dithered image. Moreover, the dithered images are further compressed by lossy and lossless procedures. The experimental results demonstrate high-quality reconstructions while maintaining low transmitted bit rates.
C1 [Guo, Jin-Ming] Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei, Taiwan.
   [Pei, Soo-Chang] Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
   [Lee, Hua] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
C3 National Taiwan University of Science & Technology; National Taiwan
   University; University of California System; University of California
   Santa Barbara
RP Guo, JM (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei, Taiwan.
EM jmguo@seed.net.tw; pei@cc.ee.ntu.edu.tw; hualee@ece.ucsb.edu
RI Lee, Hua/AAC-9489-2021
CR ANASTASSIOU D, 1988, UNPUB IEEE S CIRC SY
   [Anonymous], 1975, P SID
   Baharav Z, 1999, P SOC PHOTO-OPT INS, V3657, P307, DOI 10.1117/12.344681
   Fu M.S., 2004, P IEEE INT C AC SPEE, V3, P381
   Fu MS, 2003, SIGNAL PROCESS, V83, P2171, DOI 10.1016/S0165-1684(03)00173-7
   Fu MS, 2002, IEEE T IMAGE PROCESS, V11, P477, DOI 10.1109/TIP.2002.999680
   Fu MS, 2000, PROC SPIE, V4067, P1671, DOI 10.1117/12.386631
   FU MS, 2000, P IEEE INT C AC SPEE
   Goldschneider JR, 1997, IEEE T IMAGE PROCESS, V6, P956, DOI 10.1109/83.597271
   GOLDSCHNEIDER JR, 1996, P IEEE INT C IM PROC, V1, P565
   Hel-Or HZ, 2001, J ELECTRON IMAGING, V10, P794, DOI 10.1117/1.1382612
   Jarvis JF, 1976, Comput Graph Image Process, V5, P13, DOI DOI 10.1016/S0146-664X(76)80003-2
   JUDICE CN, 1976, P SOC INFORM DISPLAY, V17, P91
   Kacker D, 2003, IEEE T SIGNAL PROCES, V51, P1054, DOI 10.1109/TSP.2003.809369
   KOLLIAS S, 1992, IEEE J SEL AREA COMM, V10, P944, DOI 10.1109/49.138999
   Lau DanielL., 2001, Modern Digital Halftoning
   Lu ZM, 2006, LECT NOTES COMPUT SC, V4283, P71
   MANNOS J, 1974, IEEE T INFORM THEORY, V20, P526
   Pei SC, 2003, IEEE SIGNAL PROC LET, V10, P349, DOI 10.1109/LSP.2003.817856
   Pei SC, 2003, IEEE T CIRC SYST VID, V13, P867, DOI 10.1109/TCSVT.2003.815943
   STUCKI P, 1981, RZ1060 IBM RES LAB
   Ulichney R., 1987, DIGITAL HALFTONING
   Wang SG, 2000, PROC SPIE, V3971, P218, DOI 10.1117/12.384976
NR 23
TC 16
Z9 17
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 16
EP 30
DI 10.1109/TMM.2007.911259
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200003
DA 2024-07-18
ER

PT J
AU Wang, M
   Li, BC
AF Wang, Mea
   Li, Baochun
TI Network coding in live peer-to-peer streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia streaming; network coding; peer-to-peer networks
AB In recent literature, network coding has emerged as a promising information theoretic approach to improve the performance of both peer-to-peer (P2P) and wireless networks. It has been widely accepted and acknowledged that network coding can theoretically improve network throughput of multicast sessions in directed acyclic graphs, achieving their cut-set capacity bounds. Recent studies have also supported the claim that network coding is beneficial for large-scale P2P content distribution, as it solves the problem of locating the last missing blocks to complete the download.
   We seek to perform a reality check of using network coding for P2P live multimedia streaming. We start with the following critical question: How helpful is network coding in P2P streaming? To address this question, we first implement the decoding process using Gauss-Jordan elimination, such that it can be performed while coded blocks are progressively received. We then implement a realistic testbed, called Lava, with actual network traffic to meticulously evaluate the benefits and tradeoffs involved in using network coding in P2P streaming. We present the architectural design challenges in implementing network coding for the purpose of streaming, along with a pull-based P2P live streaming protocol in our comparison studies. Our experimental results show that network coding makes it possible to perform streaming with a finer granularity, which reduces the redundancy of bandwidth usage, improves resilience to network dynamics, and is most instrumental when the bandwidth supply barely meets the streaming demand.
C1 Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.
C3 University of Toronto
RP Wang, M (corresponding author), Univ Toronto, Dept Elect & Comp Engn, 100 Coll St, Toronto, ON M5S 3G4, Canada.
EM mea@eecg.toronto.edu; bli@eecg.toronto.edu
RI baochun, Li/AAD-3188-2022; Tavares, António JV/A-7115-2008
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   [Anonymous], P ALL C COMM CONTR C
   [Anonymous], P 15 ACM S PAR ALG A
   [Anonymous], 2003, 51 ALL C COMM CONTR
   CHEN J, 2004, P IEEE ACM INT S MOD, P275
   GKANDSITDIS C, 2006, P IEEE INFOCOM 2005
   GKANDSITDIS C, 2006, P 5 INT WORKSH PEER
   HO T, 2003, P ALL C COMM CONTR C
   HOL T, 2003, P INT S INF THEORY
   KATTI S, 2006, P ACM SIGCOMM 2006
   KATTI S, 2005, P IEEE INFOCOM
   Koetter R, 2003, IEEE ACM T NETWORK, V11, P782, DOI 10.1109/TNET.2003.818197
   Li SYR, 2003, IEEE T INFORM THEORY, V49, P371, DOI 10.1109/TIT.2002.807285
   PETERSON L, 2002, P 1 WORKSH HOT TOP N
   STUTZBACH D, 2005, CISTR0503 U OR EUG
   WANG M, 2007, UNPUB P IEEE INFOCOM
   Wang M, 2006, INT WORKSH QUAL SERV, P274, DOI 10.1109/IWQOS.2006.250480
   WHITE B, 2002, IN PRESS P 5 S OP SY
   ZHANG X, 2005, P IEEE INFOCOM
NR 19
TC 58
Z9 68
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2007
VL 9
IS 8
BP 1554
EP 1567
DI 10.1109/TMM.2007.907460
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 233SF
UT WOS:000251109900003
DA 2024-07-18
ER

PT J
AU Jurca, D
   Frossard, P
AF Jurca, Dan
   Frossard, Pascal
TI Media flow rate allocation in multipath networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multipath networks; path selection; rate allocation; video distortion
ID VIDEO; SELECTION
AB We address the problem of joint path selection and source rate allocation in order to optimize the media specific quality of service in streaming of stored video sequences on multipath networks. An optimization problem is proposed in order to minimize the end-to-end distortion, which depends on video sequence dependent parameters, and network properties. An in-depth analysis of the media distortion characteristics allows us to define a low complexity algorithm for an optimal flow rate allocation in multipath network scenarios. In particular, we show that a greedy allocation of rate along paths with increasing error probability leads to an optimal solution. We argue that a network path shall not be chosen for transmission, unless all other available paths with lower error probability have been chosen. Moreover, the chosen paths should be used at their maximum available end-to-end bandwidth. Simulation results show that the optimal flow rate allocation carefully adapts the total streaming rate and the number of chosen paths, to the end-to-end transmission error probability. In many scenarios, the optimal rate allocation provides more than 20% improvement in received video quality, compared to heuristic-based algorithms. This motivates its use in multipath networks, where it optimizes media specific quality of service, and simultaneously saves network resources at the price of a very low computational complexity.
C1 Ecole Polytech Fed Lausanne, Signal Proc Inst, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Jurca, D (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Inst, CH-1015 Lausanne, Switzerland.
EM dan.jurca@epfl.ch; pascal.frossard@epfl.ch
RI Jigang, Wen/B-3394-2008; Frossard, Pascal/AAF-2268-2019
CR Apostolopoulos J, 2002, IEEE INFOCOM SER, P1736, DOI 10.1109/INFCOM.2002.1019427
   Apostolopoulos JG, 2004, IEEE COMMUN MAG, V42, P80, DOI 10.1109/MCOM.2004.1321395
   Begen AC, 2005, SIGNAL PROCESS-IMAGE, V20, P39, DOI 10.1016/j.image.2004.09.002
   CHANDRA R, 2005, P IEEE INFOCOM, V2, P882
   Chen JC, 2004, IEEE J SEL AREA COMM, V22, P1920, DOI 10.1109/JSAC.2004.836000
   Cui Y, 2003, IEEE INFOCOM SER, P1414
   DAI M, 2003, NOSSDAV 03, P60
   EPSHTEIN G, 2005, BETTER WAY IMPLEMENT
   Frossard P, 2001, IEEE T IMAGE PROCESS, V10, P1815, DOI 10.1109/83.974566
   Golubchik L, 2002, PERFORM EVALUATION, V49, P429, DOI 10.1016/S0166-5316(02)00125-6
   *ITU, 2005, H264 ITU
   JURCA D, 2006, P IEEEE ICME JUL
   JURCA D, 2005, P IEEE ICME JUL
   Jurca D, 2007, IEEE T MULTIMEDIA, V9, P629, DOI 10.1109/TMM.2006.888017
   Korkmaz T, 2003, IEEE T MULTIMEDIA, V5, P429, DOI 10.1109/TMM.2003.811627
   Leung KC, 2003, J COMMUN NETW-S KOR, V5, P230, DOI 10.1109/JCN.2003.6596817
   Li YH, 2004, FIRST INTERNATIONAL CONFERENCE ON BROADBAND NETWORKS, PROCEEDINGS, P486
   LIANG YJ, 2003, P IEEE ICASSP APR, V5, P687
   LIANG YJ, 2003, P IEEE ICASSP
   MA Z, 2004, P IEEE ICC
   Mao SW, 2003, IEEE J SEL AREA COMM, V21, P1721, DOI 10.1109/JSAC.2003.815965
   MICHALEWICZ Z, 2000, SOLVE IT MODERN HEAR
   Nguyen T, 2003, IEEE INFOCOM SER, P663
   NGUYEN T, 2002, P PACK VID WORKSH PI
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   Ribeiro V., 2003, P PASSIVE ACTIVE MEA
   SAVAGE S, P ACM SIGCOMM 1999, P289
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   *SWISSC MOB UNL, UMTS GPRS WLAN
   TAO S, 2004, P 12 ANN ACM INT C M, P136
   Vergetis E, 2005, ACM SIGCOMM COMP COM, V35, P15, DOI 10.1145/1096536.1096539
   VONRICKENBACH P, 2005, P IEEEE WMAN 05
   Vutukury S, 2001, IEEE INFOCOM SER, P557, DOI 10.1109/INFCOM.2001.916780
   Wang Z, 1996, IEEE J SEL AREA COMM, V14, P1228, DOI 10.1109/49.536364
   Wei W, 2004, FIRST INTERNATIONAL CONFERENCE ON BROADBAND NETWORKS, PROCEEDINGS, P496, DOI 10.1109/BROADNETS.2004.48
NR 35
TC 65
Z9 72
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1227
EP 1240
DI 10.1109/TMM.2007.902852
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000013
OA Green Published
DA 2024-07-18
ER

PT J
AU Roeder, M
   Cardinal, J
   Hamzaoui, R
AF Roeder, Martin
   Cardinal, Jean
   Hamzaoui, Raouf
TI Efficient rate-distortion optimized media streaming or tree-structured
   packet dependencies
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 31st IEEE International Conference on Acoustics, Speech and Signal
   Processing
CY MAY 14-19, 2006
CL Toulouse, FRANCE
SP IEEE Signal Proc Soc
DE dynamic programming; media streaming; packet scheduling; rate-distortion
   optimization; tree reducible graphs
AB When streaming packetized media data over a lossy packet network, it is desirable to use transmission strategies that minimize the expected distortion subject to a constraint on the expected transmission rate. Because the computation of such optimal strategies is usually an intractable problem, fast heuristic techniques are often used. We first show that when the graph that gives the decoding dependencies between the data packets is reducible to a tree, optimal transmission strategies can be efficiently computed with dynamic programming algorithms. The proposed algorithms are much faster than other exact algorithms developed for arbitrary dependency graphs. They are slower than previous heuristic techniques but can provide much better solutions. We also show how to apply our algorithms to find high-quality approximate solutions when the dependency graph is not tree reducible. To validate our approach, we run simulations for MPEG1 and H.264 video data. We first consider a simulated packet erasure channel. Then we implement a real video streaming system and provide experimental results for an Internet connection.
C1 Univ Konstanz, Dept Comp & Informat Sci, D-7750 Constance, Germany.
   Univ Libre Bruxelles, Dept Comp Sci, Brussels, Belgium.
   De Montfort Univ, Fac Comp Sci & Engn, Leicester LE1 9BH, Leics, England.
C3 University of Konstanz; Universite Libre de Bruxelles; De Montfort
   University
RP Roeder, M (corresponding author), Univ Konstanz, Dept Comp & Informat Sci, D-7750 Constance, Germany.
EM roeder@inf.uni-konstanz.de; jcardin@ulb.ac.be; rhamzaoui@dmu.ac.uk
CR Aho A. V., 1972, SIAM Journal on Computing, V1, P131, DOI 10.1137/0201008
   [Anonymous], MSRTR200135
   Chakareski J, 2004, IEEE DATA COMPR CONF, P202
   Chakareski J, 2002, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.2002.999943
   CHAKARESKI J, 2004, P IEEE ICIP 2004 SIN
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   De Vleeschouwer C, 2007, IEEE T MULTIMEDIA, V9, P348, DOI 10.1109/TMM.2006.886283
   GORALCITKOVA A, 1979, P INT S MATH FDN COM, V74, P301
   KALMAN M, 2003, P IEEE ICIP 03 BARC
   Miao ZR, 2000, CONF REC ASILOMAR C, P1357, DOI 10.1109/ACSSC.2000.911213
   MILLS DL, 1992, IMPLEMENTATION ANAL, V1305
   Podolsky MG, 2001, J VLSI SIG PROC SYST, V27, P81, DOI 10.1023/A:1008123631453
   Ramchandran K, 1993, IEEE T IMAGE PROCESS, V2, P160, DOI 10.1109/83.217221
   Röder M, 2006, PROC SPIE, V6071, DOI 10.1117/12.651229
   Röder M, 2006, IEEE T MULTIMEDIA, V8, P170, DOI 10.1109/TMM.2005.861281
   RODER M, 2004, KONSTANZER SCHRIFTEN, V195
   SEGHAL A, 2002, P IEEE ICME 02 LAUS
   SEHGAL A, 2004, P IEEE ICIP 04 SING, V3
   SZWARCFITER JL, 1985, NETWORKS, V15, P49, DOI 10.1002/net.3230150106
   WIEGAND T, 2003, 264ISOIEC1449610AVC
NR 20
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1259
EP 1272
DI 10.1109/TMM.2007.902872
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000015
DA 2024-07-18
ER

PT J
AU Rasiwasia, N
   Moreno, PJ
   Vasconcelos, N
AF Rasiwasia, Nikhil
   Moreno, Pedro J.
   Vasconcelos, Nuno
TI Bridging the gap: Query by semantic example
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based image retrieval; Gaussian mixtures; image similarity;
   multiple instance learning; query by example; semantic retrieval;
   semantic space
ID USERS RELEVANCE FEEDBACK; IMAGE; SPACE
AB A combination of query-by-visual-example (QBVE) and semantic retrieval (SR), denoted as query-by-semantic-example (QBSE), is proposed. Images are labeled with respect to a vocabulary of visual concepts, as is usual in SR. Each image is then represented by a vector, referred to as a semantic multinomial, of posterior concept probabilities. Retrieval is based on the query-by-example paradigm: the user provides a query image, for which 1) a semantic multinomial is computed and 2) matched to those in the database. QBSE is shown to have two main properties of interest, one mostly practical and the other philosophical. From a practical standpoint, because it inherits the generalization ability of SR inside the space of known visual concepts (referred to as the semantic space) but performs much better outside of it, QBSE produces retrieval systems that are more accurate than what was previously possible. Philosophically, because it allows a direct comparison of visual and semantic representations under a common query paradigm, QBSE enables the design of experiments that explicitly test the value of semantic representations for image retrieval. An implementation of QBSE under the minimum probability of error (MPE) retrieval framework, previously applied with success to both QBVE and SR, is proposed, and used to demonstrate the two properties. In particular, an extensive objective comparison of QBSE with QBVE is presented, showing that the former significantly outperforms the latter both inside and outside the semantic space. By carefully controlling the structure of the semantic space, it is also shown that this improvement can only be attributed to the semantic nature of the representation on which QBSE is based.
C1 Univ Calif San Diego, Dept Elect & Comp Engn, Stat Visual Comp Lab, La Jolla, CA 92093 USA.
   Google Inc, Google Res & Dev Ctr, New York, NY 10011 USA.
C3 University of California System; University of California San Diego;
   Google Incorporated
RP Rasiwasia, N (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, Stat Visual Comp Lab, La Jolla, CA 92093 USA.
EM nikux@ucsd.edu; pedro@google.com; nuno@ece.ucsd.edu
OI Vasconcelos, Nuno/0000-0002-9024-4302
CR AMIR A, 2005, P NIST TRECVID WORKS
   [Anonymous], 1998, FRAMEWORK MULTIPLE I
   [Anonymous], 1983, INTRO MODERN INFORM
   [Anonymous], P ACM SIGIR C RES DE
   Auer P., 1997, Proceedings of the Fourteenth International Conference on Machine Learning, P21
   Barnard K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P408, DOI 10.1109/ICCV.2001.937654
   CARNEIO G, 2005, P IEEE CVPR SAN DIEG
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Cover T. M., 1991, ELEMENTS INFORM THEO
   COX IJ, 1997, P IEEE CVPR
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Duda R. O., 2000, PATTERN CLASSIFICATI
   DUYGULU P, 2002, P ECCV COP DENM
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   FENG S, 2004, P IEEE CVPR WASH DC
   Forsyth DA, 1997, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.1997.609399
   Haering N, 1997, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P18, DOI 10.1109/IVL.1997.629716
   HAN J, 2002, P INT C IM PROC
   He XF, 2003, IEEE T CIRC SYST VID, V13, P39, DOI 10.1109/TCSVT.2002.808087
   JAIN A, 1996, PATTERN RECOGNIT J, V29
   Kiyoki Y., 1994, SIGMOD Record, V23, P34, DOI 10.1145/190627.190639
   KRAAIJ W, 2006, P TRECVID
   KUECK PCH, 2004, P ECCV PRAG CZECH RE
   Lavrenko V., 2003, MODEL LEARNING SEMAN
   Lee CS, 1999, PROC SPIE, V3846, P294, DOI 10.1117/12.360434
   LU LWY, 2003, IEEE T MULTIMEDIA, V5, P339
   LU MZJ, 2005, P IEEE NLP KE
   MANMATHA R, 1997, P SPIE, V3016
   Muller H., 2002, PROC INT C IMAGE VID, P38
   NIBLACK W, 1993, P SOC PHOTO-OPT INS, V1908, P173
   Pentland A, 1996, INT J COMPUT VISION, V18, P233, DOI 10.1007/BF00123143
   PICARD RW, 1995, P AS C COMP VIS SING
   Platt J., 1999, Advances in Large Margin Classifiers, V6174
   RUI Y, 2000, P IEEE C COMP VIS PA
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith J. R., 2003, Proceedings of the SPIE - The International Society for Optical Engineering, V5307, P271, DOI 10.1117/12.536579
   Smith JR, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P445
   SMITH JR, 1998, P IEEE WORKSH CONT B
   SMITH JR, 1996, VISUALSEEK FULLY AUT, P87
   SNOEK C, 2006, P INT C IM VID RETR, P11
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Szummer M., 1998, P WORKSH CONT BAS AC
   Tahaghoghi SMM, 2001, AUST COMP S, V23, P138, DOI 10.1109/ADC.2001.904476
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Vailaya A, 1998, PATTERN RECOGN, V31, P1921, DOI 10.1016/S0031-3203(98)00079-X
   Vasconcelos N, 2004, IEEE T SIGNAL PROCES, V52, P2322, DOI 10.1109/TSP.2004.831125
   VASCONCELOS N, 2001, P INT C PATT REC BAR
   VASCONCELOS N, 2001, P INT C IM PROC THES
   VASCONCELOS N, 2000, P ECCV DUBL IR
   WESTERVELD T, 2003, P MULT INF RETR WORK
NR 51
TC 151
Z9 172
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 923
EP 938
DI 10.1109/TMM.2007.900138
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800003
OA Green Published
DA 2024-07-18
ER

PT J
AU De Vleeschouwer, C
   Chakareski, J
   Frossard, P
AF De Vleeschouwer, Christophe
   Chakareski, Jacob
   Frossard, Pascal
TI The virtue of patience in low-complexity scheduling of packetized media
   with feedback
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio coding; channel coding; error control; greedy packet scheduling;
   Internet; Markov processes; multimedia communicationoptimal control;
   protocols; rate-distortion optimization; video coding; video streaming
AB We consider streaming pre-encoded and packetized media over best-effort networks in the presence of acknowledgment feedbacks. We first review a rate-distortion (RD) optimization framework that can be employed in such scenarios. As part of the framework, a scheduling algorithm selects the data to send over the network at any given time, so as to minimize the end-to-end distortion, given an estimate of channel resources and a history of previous transmissions and received acknowledgements. In practice, a greedy scheduling strategy is often considered to limit the solution search space, and reduce the computational complexity associated to the RD optimization framework. Our work observes that popular greedy schedulers are strongly penalized by early retransmissions. Therefore, we propose a scheduling algorithm that avoids premature retransmissions, while preserving the low computational complexity aspect of the greedy paradigm. Such a scheduling strategy maintains close to optimal RD performance when adapting to network bandwidth fluctuations. Our experimental results demonstrate that the proposed patient greedy scheduler provides a reduction of up to 50% in transmission rate relative to conventional greedy approaches, and that it brings up to 2 dB of quality improvement in scheduling classical MPEG-based packet video streams.
C1 Catholic Univ Louvain, Telecommun Lab, B-1348 Louvain, Belgium.
   Ecole Polytech Fed Lausanne, Signal Proc Inst LTS4, Lausanne, Switzerland.
C3 Universite Catholique Louvain; Swiss Federal Institutes of Technology
   Domain; Ecole Polytechnique Federale de Lausanne
RP De Vleeschouwer, C (corresponding author), Catholic Univ Louvain, Telecommun Lab, B-1348 Louvain, Belgium.
EM devlees@tele.ucl.ac.be; io@jakov.org; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019
CR Advanced Video Coding (AVC), 2003, H264 ITU T
   [Anonymous], MSRTR200135
   BEGEN A, 2003, P IEEE INT C IM PROC
   CHAKARESKI J, 2003, P IEEE INT C IM PROC
   Chakareski J., 2004, P IEEE INT C IM PROC
   CHAKARESKI J, 2005, P INT C MULT EXH AMS
   CHAKARESKI J, 2003, IEEE DAT COMPR C DCC
   CHAKARESKI J, 2002, P IEEE WORKSH MULT S
   CHAKARESKI J, 2004, P IEEE DAT COMPR C
   CHANG CL, 2004, P IEEE INT C IM PROC
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Girod B, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P1
   Hsu CY, 1999, IEEE J SEL AREA COMM, V17, P756, DOI 10.1109/49.768193
   LI D, 2004, P IEEE INT C IM PROC
   MIAO Z, 2000, AS C SIGN SYST COMP
   Ortega A, 1996, DCC '96 - DATA COMPRESSION CONFERENCE, PROCEEDINGS, P349, DOI 10.1109/DCC.1996.488340
   Ramchandran K, 1993, IEEE T IMAGE PROCESS, V2, P160, DOI 10.1109/83.217221
   Röder M, 2006, IEEE T MULTIMEDIA, V8, P170, DOI 10.1109/TMM.2005.861281
   Röder M, 2004, IEEE DATA COMPR CONF, P192
   SEHGAL A, 2005, P IEEE INT C IM PROC
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   TIAN D, 2004, P IEEE WCNC
   ZHOURONG M, 2002, AS C SIGN SYST COMP
NR 23
TC 12
Z9 12
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 348
EP 365
DI 10.1109/TMM.2006.886283
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900013
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Zhang, ZS
   Sun, QB
   Wong, WC
   Apostolopoulos, J
   Wee, S
AF Zhang, Zhishou
   Sun, Qibin
   Wong, Wai-Choong
   Apostolopoulos, John
   Wee, Susie
TI An optimized content-aware authentication scheme for streaming JPEG-2000
   images over lossy networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-aware; digital signature; JPEG-2000; signature amortization;
   stream authentication
AB This paper proposes an optimized content-aware authentication scheme for JPEG-2000 streams over lossy networks, where a received packet is consumed only when it is both decodable and authenticated. In a JPEG-2000 codestream, some packets are more important than others in terms of coding dependency and image quality. This naturally motivates allocating more redundant authentication information for the more important packets in order to maximize their probability of authentication and thereby minimize the distortion at the receiver. Towards this goal, with the awareness of its corresponding image content, we formulate an optimization framework to compute an authentication graph to maximize the expected media quality at the receiver, given specific authentication overhead and knowledge of network loss rate. System analysis and experimental results demonstrate that the proposed scheme achieves our design goal in that the rate-distortion (R-D) curve of the authenticated image is very close to the R-D curve when no authentication is required.
C1 I2R, Dept Elect & Comp Engn, Singapore 119613, Singapore.
   Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
   Hewlett Packard Labs, Palo Alto, CA 94304 USA.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); National University of Singapore;
   Hewlett-Packard
RP Zhang, ZS (corresponding author), I2R, Dept Elect & Comp Engn, Singapore 119613, Singapore.
EM zszhang@i2r.a-star.edu.sg; qibin@i2r.a-star.edu.sg;
   wong_lawrence@nus.edu.sg; john_apostolopoulos@hp.com; susie.wee@hp.com
RI Sun, Qibin/Q-5360-2017
OI Sun, Qibin/0000-0001-5715-0497; Wong, Lawrence/0000-0001-6581-234X
CR [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 1990, 1191 RFC
   DESHPANDE S, 2001, ACM MULTIMEDIA   OCT
   Fletcher R., 1987, PRACTICAL METHOD OPT, V2nd
   GENNARO R, ADV CRYPTOLOGY CRYPT, P180
   Golle P., 2001, ISOC NETWORK DISTRIB, P13
   *INT TECHN, 2004, 154449 ISO IEC ITU I
   *ITU REC, 2000, 154441 ISO IEC ITU I
   *JPEG, 2000, JTC1SC29WG11N3573 IS
   *JPEG, 2000, JTC1SC29WG1N3480 ISO
   MERKLE RC, 1990, LECT NOTES COMPUT SC, V435, P218, DOI 10.1007/0-387-34805-0_21
   Perrig A, 2000, P IEEE S SECUR PRIV, P56, DOI 10.1109/SECPRI.2000.848446
   QIU R, 2001, WUCS0137 TRANSM SCH
   Rabbani M, 2002, SIGNAL PROCESS-IMAGE, V17, P3, DOI 10.1016/S0923-5965(01)00024-8
   Schneier B., 1996, Applied Cryptography, VSecond, P429
   SCHULZRINNE H, 2006, 3550 RFC
   Sun QB, 2005, INT J IMAGE GRAPH, V5, P135, DOI 10.1142/S0219467805001690
   TAUBMAN D, 2002, P IEEE INT C IM PROC
   TAUBMAN D, 2001, JPEG2000 IMAGE COMPR, P375
   WEE S, 2004, P IEEE INT C IM PROC
   WEE SJ, 2003, P IEEE INT C IM PROC
   WONG CK, 1998, TR9815 U TEX AUST DE
   ZHANG Z, 2004, P IEEE INT C MULT EX
NR 23
TC 16
Z9 22
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 320
EP 331
DI 10.1109/TMM.2006.886281
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lin, HYS
   Liao, HYM
   Lin, JC
AF Lin, Hsueh-Yi Sean
   Liao, Hong-Yuan Mark
   Lin, Ja-Chen
TI Visual salience-guided mesh decomposition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE mesh decomposition; perceptual organization; visual salience
ID SEGMENTATION; ORGANIZATION; PARTS; FORM
AB In this paper, we propose a novel mesh-decomposition scheme called "visual salience-guided mesh decomposition." The concept of "part salience," which originated in cognitive Psychology, asserts that the salience of a part can be determined by (at least) three factors: the protrusion, the boundary strength, and the relative size of the part. We try to convert these conceptual rules into real computational processes, and use them to guide a three-dimensional (3-D) mesh decomposition process in such a way that the significant components can be precisely identified and efficiently extracted from a given 3-D mesh. The proposed decomposition scheme not only identifies the parts' boundaries defined by the minima rule, but also labels each part with a quantitative degree of visual salience during the mesh decomposition process. The experimental results show that the proposed scheme is indeed effective and powerful in decomposing a 3-D mesh into its significant components.
C1 Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
   Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
C3 National Yang Ming Chiao Tung University; Academia Sinica - Taiwan
RP Lin, HYS (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
EM hylin@cis.nctu.edu.tw; jclin@cs.nctu.edu.tw; liao@iis.sinica.edu.tw
RI Liao, Hong-Yuan Mark/AAQ-5514-2021
CR BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   Bischoff S, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P361, DOI 10.1109/ICME.2002.1035793
   Bischoff S, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P480, DOI 10.1109/TDPVT.2002.1024103
   Chuang JH, 2000, IEEE T PATTERN ANAL, V22, P1241
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   Dorai C, 1997, IEEE T PATTERN ANAL, V19, P1115, DOI 10.1109/34.625113
   FUNKHOUSER T, 2004, P SIGGRAPH LOS ANG C
   Garland M., 2001, I3D 01, P49, DOI [DOI 10.1145/364338.364345, 10.1145/364338.364345]
   GOLDBERG AV, 1988, J ACM, V35, P921, DOI 10.1145/48014.61051
   Guillemin V, 1974, Differential topology
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hoffman DD, 1997, COGNITION, V63, P29, DOI 10.1016/S0010-0277(96)00791-3
   HOFFMAN DD, 1984, COGNITION, V18, P65, DOI 10.1016/0010-0277(84)90022-2
   Hubeli A, 2001, IEEE VISUAL, P287, DOI 10.1109/VISUAL.2001.964523
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   LAZARUS F., 1999, P ACM S SOL MOD APPL, P130, DOI [DOI 10.1145/304012.304025, 10.1145/304012.304025]
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Li X., 2001, P 2001 S INT 3D GRAP, P35, DOI DOI 10.1145/364338.364343
   Liefman S.K. G., 2005, VISUAL COMPUT, V21, P865
   Lien J.-M., 2004, SCG '04: Proceedings of the twentieth annual symposium on Computational geometry, P457
   LIN HS, 2004, P IEEE INT WORKSH MU
   Liu R, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P298
   Mallat S.G. A., 1989, IEEE T PATTERN ANAL, V11, P7
   Mangan AP, 1999, IEEE T VIS COMPUT GR, V5, P308, DOI 10.1109/2945.817348
   Page DL, 2003, PROC CVPR IEEE, P27
   Parker J.R., 1996, Algorithms for Image Processing and Computer Vision
   REED TR, 1990, IEEE T PATTERN ANAL, V12, P1, DOI 10.1109/34.41379
   SANDER PV, 2003, P EUR ACM SIGGRAPH S, P146
   SARKAR S, 1993, IEEE T SYST MAN CYB, V23, P382, DOI 10.1109/21.229452
   Serra J., 1982, IMAGE ANAL MATH MORP
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P66, DOI 10.1109/38.90568
   Shlafman S, 2002, COMPUT GRAPH FORUM, V21, P219, DOI 10.1111/1467-8659.00581
   SIDDIQI K, 1995, IEEE T PATTERN ANAL, V17, P239, DOI 10.1109/34.368189
   Svensson S, 2002, IMAGE VISION COMPUT, V20, P529, DOI 10.1016/S0262-8856(02)00042-2
   Wu KN, 1997, IEEE T PATTERN ANAL, V19, P1223, DOI 10.1109/34.632982
   Xiao YJ, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P378
   ZAHN CT, 1971, IEEE T COMPUT, VC 20, P68, DOI 10.1109/T-C.1971.223083
   Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1170, DOI 10.1109/34.809110
   Zuckerberger E, 2002, COMPUT GRAPH-UK, V26, P733, DOI 10.1016/S0097-8493(02)00128-0
NR 40
TC 25
Z9 28
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 46
EP 57
DI 10.1109/TMM.2006.886344
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500006
DA 2024-07-18
ER

PT J
AU Mei, T
   Hua, XS
   Zhou, HQ
   Li, SP
AF Mei, Tao
   Hua, Xian-Sheng
   Zhou, He-Qin
   Li, Shipeng
TI Modeling and mining of users' capture intention for home videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE attention detection; capture intention; video content analysis
AB With the rapid adoption of consumer digital video recorders and an increase of home video data, content analysis has become an interesting and key research issue to provide personalized experiences and services for both camcorder users and viewers. In this paper, we present a novel view to tackle this issue, which aims at modeling and mining of the capture intention of camcorder users. Based on the study of intention mechanism in psychology, a set of domain-specific capture intention concepts is defined. A comprehensive and extensible scheme consisting of video structure decomposition, intention-oriented feature analysis, as well as singular-value-decomposition-based intention segmentation and learning-based intention classification is proposed to mine the users' capture intention. Experiments were carried on home video sequences of 90 h in total, taken by 16 persons over the past 20 years. Both the user study and objective evaluations indicate that our proposed intention-based approach is an effective complement to existing home video content analysis schemes.
C1 Univ Sci & Technol China, Hefei 230027, Peoples R China.
   Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia
RP Mei, T (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM tmei@microsoft.com; xshua@microsoft.com; hqzhou@ustc.edu.cn;
   spli@microsoft.com
RI Mei, Tao/GQZ-0596-2022; Li, Shipeng/AAA-3374-2020
OI Mei, Tao/0000-0002-5990-7307; Li, Shipeng/0000-0001-5368-4256
CR [Anonymous], 2003, P 11 ACM INT C MULTI, DOI DOI 10.1145/957013.957094
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Bordwell David., 2001, Film Art: An Introduction, V6th
   Bratman Michael., 1987, Intention, Plans, and
   Cai R, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL IV, PROCEEDINGS, P345
   CHEN Z, 2002, WORLD WIDE WEB, V5, P181
   Do E.Y., 2000, DESIGN STUD, V21, P483, DOI DOI 10.1016/S0142-694X(00)00020-X
   Enns J., 1990, The Development of Attention: Research and Theory
   Gatica-Perez D, 2003, IEEE T CIRC SYST VID, V13, P539, DOI 10.1109/TCSVT.2003.813428
   GERRIG RJ, 2001, PSYCHOL LIFE
   Girgensohn A, 2001, HUMAN-COMPUTER INTERACTION - INTERACT'01, P464
   GIRGENSOHN A, 2000, P UIST 00, P81
   HAUPTMANN A, 2004, P TREC VID RETR EV
   Hua XS, 2004, IEEE T CIRC SYST VID, V14, P572, DOI 10.1109/TCSVT.2004.826750
   KENDER JR, 2000, P AS C COMP VIS TAIP
   Kim JG, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1171, DOI 10.1109/ICME.2000.871569
   KONRAD J, 1998, JTC1SC29WG11M3096 IS
   Li SZ, 2002, LECT NOTES COMPUT SC, V2353, P67
   Lienhart R, 2000, PROC SPIE, V3972, P378
   LIN T, 2002, P INT C PATT REC QUE, P39
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   MAO JC, 1992, PATTERN RECOGN, V25, P173, DOI 10.1016/0031-3203(92)90099-5
   Mei T, 2005, PROC SPIE, V5960, P268, DOI 10.1117/12.631387
   MEI T, 2005, P ACM MULT SING NOV, P531
   Naphade MR, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P109, DOI 10.1109/ICME.2004.1394137
   NAPHADE MR, 2005, RC23612 IBM
   Oami R, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1475, DOI 10.1109/ICME.2004.1394514
   Pan Zailiang., 2004, MIR 04, P69
   Pass G., 1996, P 4 ACM INT C MULT, V96, P65, DOI DOI 10.1145/244130.244148
   Rui Y, 2000, PROC CVPR IEEE, P111, DOI 10.1109/CVPR.2000.855807
   Schapire RE, 1999, LECT NOTES ARTIF INT, V1720, P13
   Sikora T, 2001, IEEE T CIRC SYST VID, V11, P696, DOI 10.1109/76.927422
   Syeda-Mahmood T., 2001, PROC ACM MULITMEDIA, V9, P119, DOI DOI 10.1145/500141.500161
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Tong HG, 2004, LECT NOTES COMPUT SC, V3331, P198
   Vapnik V., 1999, NATURE STAT LEARNING
   Volkmer T., 2005, PROC 13 ANN ACM INT, P892, DOI DOI 10.1145/1101149.1101341
   Wu P, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P957
   Yan R, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P1
   Yan Wei-Qi., 2002, MULTIMEDIA 02, P107
   Zhang L, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P95, DOI 10.1109/ACV.2002.1182164
   2002, MERRIAMWEBSTER DICT
NR 42
TC 23
Z9 24
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 66
EP 77
DI 10.1109/TMM.2006.886357
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500008
DA 2024-07-18
ER

PT J
AU Nascimento, JC
   Marques, JS
AF Nascimento, Jacinto C.
   Marques, Jorge S.
TI Performance evaluation of object detection algorithms for video
   surveillance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE ground truth; metrics; multiple interpretations; performance evaluation;
   segmentation; surveillance systems
ID SEGMENTATION; TRACKING
AB In this paper, we propose novel methods to evaluate the performance of object detection algorithms in video sequences. This procedure allows us to highlight characteristics (e.g., region splitting or merging) which are specific of the method being used. The proposed framework compares the output of the algorithm with the ground truth and measures the differences according to objective metrics. In this way it is possible to perform a fair comparison among different methods, evaluating their strengths and weaknesses and allowing the user to perform a reliable choice of the best method for a specific application. We apply this methodology to segmentation algorithms recently proposed and describe their performance. These methods were evaluated in order to assess how well they can detect moving regions in an outdoor scene in fixed-camera situations.
C1 Univ Tecn Lisboa, ISR, Inst Super Tecn, P-1049001 Lisbon, Portugal.
C3 Universidade de Lisboa
RP Nascimento, JC (corresponding author), Univ Tecn Lisboa, ISR, Inst Super Tecn, P-1049001 Lisbon, Portugal.
EM jan@isr.ist.utl.pt; jsm@isr.ist.utl.pt
RI Marques, Jorge/C-1427-2010; Nascimento, Jacinto/B-6128-2009
OI Marques, Jorge/0000-0002-3800-7756; Nascimento,
   Jacinto/0000-0001-7468-5127
CR [Anonymous], CMURITR0012
   [Anonymous], P 3 IEEE INT WORKSH
   [Anonymous], P IEEE WORKSH PERF E
   [Anonymous], P AM NUCL SOC ANS 8
   [Anonymous], P IEEE WORKSH PERF E
   Black J., 2003, In Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance VS-PETS, P125
   Boult TE, 2001, P IEEE, V89, P1382, DOI 10.1109/5.959337
   Boult TE, 1999, SECOND IEEE WORKSHOP ON VISUAL SURVEILLANCE (VS'99), PROCEEDINGS, P48
   Chalidabhongse T., 2003, P JOINT IEEE INT WOR, P1937
   Correia P, 2000, IEEE IMAGE PROC, P308, DOI 10.1109/ICIP.2000.900956
   Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909
   Erdem ÇE, 2004, IEEE T IMAGE PROCESS, V13, P937, DOI 10.1109/TIP.2004.828427
   Gao X, 2000, PROC CVPR IEEE, P503, DOI 10.1109/CVPR.2000.855861
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Haritaoglu I, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P222, DOI 10.1109/AFGR.1998.670952
   KOLLER D, 1994, INT C PATT RECOG, P126, DOI 10.1109/ICPR.1994.576243
   MA YF, 2001, IEEE INT C MULT EXP
   Mariano VY, 2002, INT C PATT RECOG, P965, DOI 10.1109/ICPR.2002.1048198
   McKenna SJ, 1999, IMAGE VISION COMPUT, V17, P225, DOI 10.1016/S0262-8856(98)00104-8
   Monnet A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1305
   OBERTI F, 1999, IEEE INT C IM PROC I, V2, P949
   Ohta N, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P481, DOI 10.1109/ICCV.2001.937664
   Seki M, 2000, FIFTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P207, DOI 10.1109/WACV.2000.895424
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   SUN H, 2000, IEEE INT C PATT REC, V1, P843
   Trees H. L., 2001, Detection, Estimation, and Modulation Theory
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Zhong J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P44, DOI 10.1109/ICCV.2003.1238312
NR 29
TC 151
Z9 173
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 761
EP 774
DI 10.1109/TMM.2006.876287
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300011
DA 2024-07-18
ER

PT J
AU Gu, XH
   Nahrstedt, K
AF Gu, XH
   Nahrstedt, K
TI Distributed multimedia service composition with statistical QoS
   assurances
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE middleware; quality-of-service (QoS); service composition; service
   overlay network
ID QUALITY
AB Service composition allows multimedia services to be automatically composed from atomic service components based on dynamic service requirements. Previous work falls short for distributed multimedia service composition in terms of scalability, flexibility and quality-of-service (QoS) management. In this paper, we present a fully decentralized service composition framework, called SpiderNet, to address the challenges. SpiderNet provides statistical multiconstrained QoS assurances and load balancing for service composition. Moreover, SpiderNet supports directed acyclic graph composition topologies and exchangeable composition orders. We have implemented a prototype of SpiderNet and conducted experiments on both wide-area networks and a simulation testbed. Our experimental results show the feasibility and efficiency of the SpiderNet service composition framework.
C1 IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
   Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
C3 International Business Machines (IBM); University of Illinois System;
   University of Illinois Urbana-Champaign
RP IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
EM xiaohui@us.ibm.com; klara@cs.uiuc.edu
CR ANDERSEN D, 2001, P ACM SOSP 2001
   [Anonymous], 1986, Probability, random processes, and estimation theory for engineers
   [Anonymous], UMCSETR45602
   Black AP, 2002, MULTIMEDIA SYST, V8, P406, DOI 10.1007/s005300200062
   CAMPBELL A, 1996, P ACM INT C MULT ACM
   Chen SG, 1998, IEEE NETWORK, V12, P64, DOI 10.1109/65.752646
   Coulson G, 2002, MULTIMEDIA SYST, V8, P340, DOI 10.1007/s005300200056
   ECKLUND D, 2001, ACM MULTIMEDIA SYST, V8, P431
   Gu X., 2004, P IEEE INT S HIGH PE
   GU X, 2004, P IEEE INT C MULT EX
   GU X, 2004, DISTRIBUTED MULTIMED
   Gu XH, 2002, J VISUAL LANG COMPUT, V13, P61, DOI 10.1006/jvlc.2001.0227
   Knightly EW, 1999, IEEE NETWORK, V13, P20, DOI 10.1109/65.768485
   RAMAN B, 2003, P IEEE INFOCOM 2003
   RAMAN B, 2003, COMPUT COMMUN J  MAY
   RATNASAMY S, 2002, P INFOCOM 2002
   Rowstron A, 2003, IFIPACM INT C DISTRI
   Vanegas R, 1998, P IFIP INT C DISTR S
   XU D, 2002, P SPIE ACM MULT COMP
NR 19
TC 59
Z9 73
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 141
EP 151
DI 10.1109/TMM.2005.861284
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000013
DA 2024-07-18
ER

PT J
AU Ghinea, G
   Magoulas, GD
   Siamitros, C
AF Ghinea, G
   Magoulas, GD
   Siamitros, C
TI Intelligent synthesis mechanism for deriving streaming priorities of
   multimedia content
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE communication protocols; fuzzy prioritization methods; multimedia
   communications; quality of service; subjective multimedia quality
AB We address the problem of integrating user preferences with network Quality of Service parameters for the streaming of media content, and suggest protocol stack configurations that satisfy user and technical requirements to the best available degree. Our approach is able to handle inconsistencies between user and networking considerations, formulating the problem of construction of tailor-made protocols as a prioritization problem, solvable using fuzzy programming.
C1 Brunel Univ, Sch Informat Syst Comp & Math, Uxbridge UB8 3PH, Middx, England.
   Univ London Birkbeck Coll, Sch Comp Sci & Informat Syst, London WC1E 7HX, England.
C3 Brunel University; University of London; Birkbeck University London
RP Brunel Univ, Sch Informat Syst Comp & Math, Uxbridge UB8 3PH, Middx, England.
EM george.ghinea@brunel.ac.uk; g.inagoulas@dcs.bbk.ac.uk;
   christos.siamitros@brunel.ac.uk
RI Ghinea, Gheorghita/AAG-6770-2020; Magoulas, George/D-5597-2014
OI Ghinea, Gheorghita/0000-0003-2578-5580; Magoulas,
   George/0000-0003-1884-0772
CR [Anonymous], 1999, SYSTEMS MAN CYBERNET
   [Anonymous], 2014, P 2 IFIP IEEE INT WO, DOI DOI 10.1109/ICME.2001.1237740
   [Anonymous], 1992, Fuzzy Set Theory-and Its Applications
   APTEKER RT, 1995, IEEE MULTIMEDIA, V2, P32, DOI 10.1109/93.410510
   Bouch A., 2000, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, P297, DOI DOI 10.1145/332040.332447
   Chen S. J., 1992, LECT NOTES EC MATH S, P375
   Fish RS, 1998, EUROMICRO CONF PROC, P1029, DOI 10.1109/EURMIC.1998.708137
   Ghinea G., 2004, International Journal on Artificial Intelligence Tools (Architectures, Languages, Algorithms), V13, P199, DOI 10.1142/S021821300400148X
   GHINEA G, IN PRESS IEEE T MULT
   GHINEA G, P GLOB 99 RIO BRAZ, P2061
   Hess CK, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P903, DOI 10.1109/MMCS.1999.778608
   Hikichi K., 2001, P IEEE INT C MULT EX, P744
   KRASIC C, 2000, P NOSSDAV 03 MONT CA, P307
   Mahanti A, 2003, IEEE ACM T NETWORK, V11, P195, DOI 10.1109/TNET.2003.810311
   Mikhailov L, 2000, J OPER RES SOC, V51, P341, DOI 10.2307/254092
   Mikhailov L, 2003, FUZZY SET SYST, V134, P365, DOI 10.1016/S0165-0114(02)00383-4
   MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158
   SERMADEVI Y, 2004, P SPIE VIS COMM IM P
   Varadarajan S, 2002, IEEE ACM T NETWORK, V10, P139, DOI 10.1109/90.986585
   Varadarajan S., 2000, Proceedings 20th IEEE International Conference on Distributed Computing Systems, P475, DOI 10.1109/ICDCS.2000.840960
   Verscheure O, 1999, REAL-TIME IMAGING, V5, P305, DOI 10.1006/rtim.1999.0175
   YADAVALLI G, 2003, P IEEE ICIP 2003 BAR, P441
NR 22
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1047
EP 1053
DI 10.1109/TMM.2005.858391
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200006
OA Green Submitted, Green Published, Green Accepted
DA 2024-07-18
ER

PT J
AU Lee, PY
   Hui, SC
   Fong, ACM
AF Lee, PY
   Hui, SC
   Fong, ACM
TI An intelligent categorization engine for bilingual web content filtering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Internet; neural network applications; text processing; unsupervised
   learning
AB It is important to protect children and unsuspecting adults from the harmful effects of objectionable materials, such as pornography, violence, and hate messages, which are now prevalent on the World-Wide Web. This calls for effective tools for web content analysis and filtering of objectionable contents. Our study of existing web content filtering systems has identified a number of deficiencies in these systems. Using the analysis of pornographic web pages as a case study, we present an intelligent bilingual web page categorization engine that can determine if an English or Chinese language web page contains pornographic materials. We have implemented the categorization engine to perform offline web page analysis and near-instantaneous online filtering. Performance evaluation of our system has verified its effectiveness.
C1 Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM puiyen_lee@yahoo.com.sg; asschui@ntu.edu.sg; acmfong@iee.org
CR [Anonymous], 1995, SELF ORG MAPS
   CARPENTER GA, 1991, NEURAL NETWORKS, V4, P759, DOI 10.1016/0893-6080(91)90056-B
   Dalton J., 1991, IEEE Potentials, V10, P33, DOI 10.1109/45.84097
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Flexer A., 2001, Intelligent Data Analysis, V5, P373
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   KOEHLER GJ, 1990, DECISION SCI, V21, P63, DOI 10.1111/j.1540-5915.1990.tb00317.x
   Kwok KL, 1997, PROCEEDINGS OF THE 20TH ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P34, DOI 10.1145/278459.258531
   Salton G., 1989, Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer
   WU ZM, 1993, J AM SOC INFORM SCI, V44, P532, DOI 10.1002/(SICI)1097-4571(199310)44:9<532::AID-ASI3>3.0.CO;2-M
   YANG Y, 1993, P 16 ANN INT ACM SIG, P281
   Yang YM, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P42, DOI 10.1145/312624.312647
   Yiming Yang, 1999, Information Retrieval, V1, P69, DOI 10.1023/A:1009982220290
NR 13
TC 38
Z9 45
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1183
EP 1190
DI 10.1109/TMM.2005.858414
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200019
DA 2024-07-18
ER

PT J
AU Cheng, S
   Xiong, ZX
AF Cheng, S
   Xiong, ZX
TI Audio coding and image denoising based on the nonuniform modulated
   complex lapped transform
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio coding; image denoising; lapped orthogonal transform; modulated
   lapped transform
ID COMPRESSION; BLOCKING
AB Xiong and Malvar recently introduced a nonuniform modulated complex lapped transform (NMCLT) with good time-localization and controllable frequency resolution by using an oversampled nonuniform filter bank to generate its real and the imaginary components. In this paper, we first show that oversampling in the NMCLT is not necessary in theory but a by-product of fast implementation in practice. We also point out that the amount of oversampling, which can be flexibly controlled, depends on the application. We then describe in detail the implementation of the inverse transform, which was not addressed clearly by Xiong and Malvar. We present the first applications of the NMCLT to audio coding and image denoising. A scalable audio coder has been implemented by controlling the amount of oversampling and exploiting redundancy among the NMCLT coefficients via predictive coding. Experimental results show that the audio coder reduces pre-echoes and improves the sound quality of audio clips with transient sounds. A simple denoising algorithm based on the NMCLT has also been devised to provide images with better visual quality than those obtained with wavelet-based soft thresholding.
C1 Texas A&M Univ, Dept Elect Engn, College Stn, TX 77840 USA.
C3 Texas A&M University System; Texas A&M University College Station
RP Cheng, S (corresponding author), Texas A&M Univ, Dept Elect Engn, College Stn, TX 77840 USA.
EM phsamuel@ee.tamu.edu; zx@lena.tamu.edu
OI Cheng, Samuel/0000-0002-5439-1137
CR [Anonymous], 1993, Coding of Moving Pictures and Associated Audio for Digital Storage Media at up to About 1.5 Mbit/s-Part 2: Video, ISO/IEC 11172-2 (MPEG-1)
   Brandenburg K, 2000, SIGNAL PROCESS-IMAGE, V15, P423, DOI 10.1016/S0923-5965(99)00056-9
   BRANDENBURG K, 1992, P 11 INT AES C PORTL
   CASSEREAU P, 1985, THESIS MIT CAMBRIDGE
   Chang SG, 2000, IEEE T IMAGE PROCESS, V9, P1532, DOI 10.1109/83.862633
   de Queiroz RL, 1998, INT CONF ACOUST SPEE, P1341, DOI 10.1109/ICASSP.1998.681694
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   GOYAL V, 1988, IEEE T INFORMATION T, V44, P16
   JOHNSTON JD, 1988, IEEE J SEL AREA COMM, V6, P314, DOI 10.1109/49.608
   LU Z, 1998, P WORKSH MULT SIGN P
   Malvar H., 1992, SIGNAL PROCESSING LA
   MALVAR H, 2000, Patent No. 6029126
   MALVAR HS, 1989, IEEE T ACOUST SPEECH, V37, P553, DOI 10.1109/29.17536
   Malvar HS, 1996, COMPUT APPL MATH, V15, P111
   Malvar HS, 1998, IEEE T SIGNAL PROCES, V46, P1043, DOI 10.1109/78.668555
   MALVAR HS, 1998, P ISCAS 98 MONT CA J
   MALVAR HS, 1999, P ICASSP 99 PHOEN AZ
   Mihçak MK, 1999, IEEE SIGNAL PROC LET, V6, P300, DOI 10.1109/97.803428
   Moore B. C. J., 1997, INTRO PSYCHOL HEARIN
   Painter T, 2000, P IEEE, V88, P451, DOI 10.1109/5.842996
   PRINCEN J, 1987, P IEEE ICASSP DALL T
   PURAT M, 1996, P ICASSP 96 ATL GA M
   Shlien S, 1997, IEEE T SPEECH AUDI P, V5, P359, DOI 10.1109/89.593311
   SINHA DP, 1993, IEEE T SIGNAL PROCES, V41, P3463, DOI 10.1109/78.258086
   Xiong ZX, 1997, IEEE T CIRC SYST VID, V7, P433, DOI 10.1109/76.564123
   Xiong ZX, 2001, IEEE SIGNAL PROC LET, V8, P257, DOI 10.1109/97.948450
   Zwicker E., 1999, PSYCHOACOUSTICS FACT, V2nd, DOI DOI 10.1007/978-3-662-09562-1
NR 27
TC 3
Z9 3
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 817
EP 827
DI 10.1109/TMM.2005.854470
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900003
DA 2024-07-18
ER

PT J
AU Ebrahimi-Moghadam, A
   Shirani, S
AF Ebrahimi-Moghadam, A
   Shirani, S
TI Progressive scalable interactive region-of-interest image coding using
   vector quantization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE embedded coding; foveation; image compression lossy compression; region
   of interest; scalability; vector quantization
AB We have developed novel progressive scalable region-of-interest (ROI) image compression schemes with rate-distortion-complexity tradeoff based on vector quantization. Residual vector quantization (RVQ) equips the encoder with a multi-resolution apparatus which is useful for rate-distortion tradeoff. Having all advantages of RVQ, jointly suboptimized RVQ provides a distortion-complexity adjustment. The systems are unbalanced in the sense that the decoder has less computational requirements than the encoder. The proposed jointly suboptimized RVQ method provides an interactive tool for fast ROI-based browsing from image archives.
C1 McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
C3 McMaster University
RP McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
EM ebrabia@mcmaster.ca; Shirani@mcmaster.ca
RI Ebrahimi-Moghadam, Abbas/AAE-5526-2020
OI Ebrahimi-Moghadam, Abbas/0000-0002-3921-9814
CR AKODRAS A, 2001, IEEE SIGNAL PROCESS, V18, P36
   [Anonymous], THESIS U TEXAS AUSTI
   Barnes CF, 1996, IEEE T IMAGE PROCESS, V5, P226, DOI 10.1109/83.480761
   BARNES CF, 1989, THESIS BRIGHAM YOUNG
   BASU A, 1993, P IEEE RSJ INT C INT, V3, P1822
   Cormack L.K., 2000, HDB IMAGE VIDEO PROC
   Gersho A., 2003, Vector Quantization and Signal Compression
   In JH, 1999, IEEE T IMAGE PROCESS, V8, P1630, DOI 10.1109/83.799890
   Khan MAU, 2001, IEEE T COMMUN, V49, P937, DOI 10.1109/26.930620
   MOGHADAM AE, 2003, P IEEE DAT COMPR C M, P426
   MOUSA WAH, 2002, P IEEE INT C AC SPEE, V3, P2529
   Sayood K, 2017, Introduction to data compression
   Sheikh HR, 2001, INT CONF ACOUST SPEE, P1781, DOI 10.1109/ICASSP.2001.941286
   TAUBMAN D, 2003, P INT S VIS COMM IM
   Wang Z, 2001, IEEE T IMAGE PROCESS, V10, P1397, DOI 10.1109/83.951527
NR 15
TC 10
Z9 11
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 680
EP 687
DI 10.1109/TMM.2005.850967
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000009
DA 2024-07-18
ER

PT J
AU Yeung, SF
   Lui, JCS
   Yau, DKY
AF Yeung, SF
   Lui, JCS
   Yau, DKY
TI A multikey secure multimedia proxy using asymmetric reversible
   parametric sequences: Theory, design, and implementation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE asymmetric parametric sequence functions; multikey; RSA; security; video
   proxy
AB Because of limited server and network capacities for streaming applications, multimedia proxies are commonly used to cache multimedia objects such that, by accessing nearby proxies, clients can enjoy a smaller start-up latency and receive a better quality-of-service (QoS) guarantee-for example, reduced packet loss and delay jitters for their requests. However, the use of multimedia proxies increases the risk that multimedia data are exposed to unauthorized access by intruders. In this paper, we present a framework for implementing a secure multimedia proxy system for audio and video streaming applications. The framework employs a notion of asymmetric reversible parametric sequence (ARPS) to provide the following security properties: i) data confidentiality during transmission, ii) end-to-end data confidentiality, iii) data confidentiality against proxy intruders, and iv) data confidentiality against member collusion. Our framework is grounded on a multikey RSA technique such that system resilience against attacks is provably strong given standard computability, assumptions. One important feature of our proposed scheme is that clients only need to perform a single decryption operation to recover the original data even though the data packets may have been encrypted by multiple proxies along the delivery path. We also propose the use of a set of encryption configuration parameters (ECP) to trade off proxy encryption throughput against the presentation quality of audio/video obtained by unauthorized parties. Implementation results show that we can simultaneously achieve high encryption throughput and extremely low video quality (in terms of peak signal-to-noise ratio and visual quality of decoded video frames) for unauthorized access.
C1 Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
C3 Chinese University of Hong Kong; Purdue University System; Purdue
   University
RP Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM sfyeung@cse.cuhk.edu.hk; cslui@cse.cuhk.edu.hk; yau@cs.purdue.edu
RI Yau, David/AFT-0656-2022
OI Yau, David/0000-0001-9061-7423
CR [Anonymous], 1995, Advances in Cryptology-EUROCRYPT'94, DOI DOI 10.1007/BFB0053428
   Changgui Shi, 1998, Proceedings ACM Multimedia 98, P81
   Griwodz C., 1998, Proceedings ACM Multimedia 98, P21, DOI 10.1145/290747.290751
   GRUBER S, 2000, P 9 INT WORLD WID WE
   Guo Y., 2002, IEEE ICC
   Kangasharju J, 2001, IEEE INFOCOM SER, P1791, DOI 10.1109/INFCOM.2001.916677
   Molva R, 1999, 6TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P101, DOI 10.1145/319709.319723
   *R S INC, 1999, PKCS1 R S INC
   REJAIE R, 1999, P 4 INT WEB CACH WOR
   Schneier B., 2015, APPL CRYPTOGRAPHY, VSecond
   SEN S, 1999, IEEE INFOCOM     MAR
   Stinson D. R., 2018, Cryptography Theory and Practice
   TOSUN AS, 2002, COMPUTER INFORMATION
   YEUNG SF, 2003, MULTIKEY SECURE MULT
   [No title captured]
NR 15
TC 18
Z9 21
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 330
EP 338
DI 10.1109/TMM.2005.843361
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400014
DA 2024-07-18
ER

PT J
AU Shum, HY
   Ng, KT
   Chan, SC
AF Shum, HY
   Ng, KT
   Chan, SC
TI A virtual reality system using the concentric mosaic: Construction,
   rendering, and data compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE concentric mosaic; data compression; image-based rendering (IBR);
   MPEG-2; video coding; virtual reality (VR)
AB This paper proposes a new image-based rendering (IBR) technique called "concentric mosaic" for virtual reality applications. IBR using the plenoptic function is an efficient technique for rendering new views of a scene from a collection of sample images previously captured. It provides much better image quality and lower computational requirement for rendering than conventional three-dimensional (3-D) model-building approaches. The concentric mosaic is a 3-D plenoptic function with viewpoints constrained on a plane. Compared with other more sophisticated four-dimensional plenoptic functions such as the light field and the lumigraph, the file size of a concentric mosaic is much smaller. In contrast to a panorama, the concentric mosaic allows users to move freely in a circular region and observe significant parallax and lighting changes without recovering the geometric and photometric scene models. The rendering of concentric mosaics is very efficient, and involves the reordering and interpolating of previously captured slit images in the concentric mosaic. It typically consists of hundreds of high-resolution images which consume a significant amount of storage and bandwidth for transmission. An MPEG-like compression algorithm is therefore proposed in this paper taking into account the access patterns and redundancy of the mosaic images. The compression algorithms of two equivalent representations of the concentric mosaic, namely the multi perspective panoramas and the normal setup sequence, are investigated. A multiresolution representation of concentric mosaics using a nonlinear filter bank is also proposed.
C1 Microsoft Res Asia, Beijing, Peoples R China.
   Univ Hong Kong, Dept Elect & Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Microsoft Research Asia; Microsoft; University of Hong Kong
RP Microsoft Res Asia, Beijing, Peoples R China.
EM hshum@microsoft.com; ktng@eee.hku.hk; scchan@eee.hku.hk
RI Ng, To/C-1762-2009; Chan, Shing Chow/C-1844-2009
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   [Anonymous], 2012, VECTOR QUANTIZATION
   Antonini M, 1992, IEEE T IMAGE PROCESS, V1, P205, DOI 10.1109/83.136597
   Chanteau S, 2000, MICROBES INFECT, V2, P25, DOI 10.1016/S1286-4579(00)00289-6
   Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   CHUI JX, 2000, P ANN C COMP GRAPH S, P307
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   DEQUEIROZ RL, 1996, P 38 MIDW S CIRC SYS, V2, P1115
   Goebel M, 2001, IEEE COMPUT GRAPH, V21, P22, DOI 10.1109/38.963457
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Gupta R, 1997, IEEE T PATTERN ANAL, V19, P963, DOI 10.1109/34.615446
   *ITU T, 1994, H262 ITUT ISOIEC
   ITU-T, 1994, 138182 ITUT ISOIEC
   Jayant N.C., 1984, Digital Coding of Waveforms: Principles and Applications to Speech and Video
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   LI J, 2000, P IEEE INT C IM PROC, V2, P21
   LI J, 2001, INT J IMAGE GRAPHICS, V1, P45
   MACHOVER C, 1994, IEEE COMPUT GRAPH, V14, P15, DOI 10.1109/38.250913
   Magnor M, 2000, IEEE T CIRC SYST VID, V10, P338, DOI 10.1109/76.836278
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   NG KT, 2001, P IEEE INT C IM PROC, V2, P113
   Peleg S, 1997, PROC CVPR IEEE, P338, DOI 10.1109/CVPR.1997.609346
   PETER J, 1999, P IEEE VIS 99 OCT, P69
   Rademacher P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P199, DOI 10.1145/280814.280871
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Shum HY, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P644, DOI 10.1109/ICIP.2000.899536
   Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573
   Szeliski Richard., 1997, P SIGGRAPH 97 COMPUT, P251, DOI DOI 10.1145/258734.258861
   Tong X, 2000, INT CONF ACOUST SPEE, P1879, DOI 10.1109/ICASSP.2000.859194
   Wood D. N., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P243, DOI 10.1145/258734.258859
   ZHANG C, 2000, P SPIE VIS COMM IM P, V4067
   ZHANG C, 2000, P IEEE DAT COMPR C M, P253
NR 33
TC 32
Z9 37
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 85
EP 95
DI 10.1109/TMM.2004.840591
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300009
OA Green Published
DA 2024-07-18
ER

PT J
AU Yeasin, M
   Polat, E
   Sharma, R
AF Yeasin, M
   Polat, E
   Sharma, R
TI A multiobject tracking framework for interactive multimedia applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE detection of human body parts; interactive multimedia applications;
   multiple hypothesis tracking; path coherence and real-time systems
ID ALGORITHM; MOTION
AB Automatic initialization and tracking of multiple people and their body parts is one of the first steps in designing interactive multimedia applications. The key problems in this context are robust detection and tracking of people and their body parts in an unconstrained environment. This paper presents an integrated framework to address detection and tracking of multiple objects in a computationally efficient manner. In particular, a neural network-based face detector was employed to detect faces and compute person specific statistical model for skin color from the face regions. A probabilistic model was proposed to fuse the color and motion information to localize the moving body parts (hands). Multiple hypothesis tracking (MHT) algorithm was adopted to track face and hands. In real world scenes extracted features (face and hands) usually contain spurious measurements that create unconvincing trajectories and needless computations. To deal with this problem a path coherence function was incorporated along with MHT to reduce the number of hypotheses, which in turn reduces the computational cost and improves the structure of trajectories. The performance of the framework was validated using experiments on synthetic and real sequence of images.
C1 SUNYIT, ISET, Dept Comp Sci, Utica, NY 13501 USA.
   Kirikkale Univ, Dept Elect & Elect Engn, TR-71450 Kirikkale, Turkey.
   Penn State Univ, Dept Comp Sci & Engn, University Pk, PA 16802 USA.
C3 SUNY Polytechnic Institute; Kirikkale University; Pennsylvania
   Commonwealth System of Higher Education (PCSHE); Pennsylvania State
   University; Pennsylvania State University - University Park
RP SUNYIT, ISET, Dept Comp Sci, Utica, NY 13501 USA.
EM yeasinm@cs.sunyit.edu; exp128@psu.edu; rsharma@cse.psu.edu
CR Blackman S., 1999, Design and analysis of modern tracking systems
   BRADSKI GR, 1998, INTEL TECHNOL J, V2
   CHAM TJ, 1998, CRL988
   CHEN Y, 2002, P IEEE AUT FAC GEST
   Chen YQ, 2001, PROC CVPR IEEE, P543
   Cox IJ, 1996, IEEE T PATTERN ANAL, V18, P138, DOI 10.1109/34.481539
   COX IJ, 1993, INT J COMPUT VISION, V10, P53, DOI 10.1007/BF01440847
   FORTMANN TE, 1983, IEEE J OCEANIC ENG, V8, P173, DOI 10.1109/JOE.1983.1145560
   Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056
   GONCALVES L, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P764, DOI 10.1109/ICCV.1995.466861
   Haritaoglu I, 1999, SECOND IEEE WORKSHOP ON VISUAL SURVEILLANCE (VS'99), PROCEEDINGS, P6, DOI 10.1109/VS.1999.780263
   Huber E, 1996, THIRD IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV '96, PROCEEDINGS, P136, DOI 10.1109/ACV.1996.572020
   Intille SS, 1997, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.1997.609402
   Iwasawa S, 1997, PROC CVPR IEEE, P15, DOI 10.1109/CVPR.1997.609290
   Jain R., 1995, MACHINE VISION
   Kakadiaris IA, 1996, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.1996.517057
   Kurien T, 1990, Issues in the design of practical multitarget tracking algorithms
   LEUNG MK, 1995, IEEE T PATTERN ANAL, V17, P359, DOI 10.1109/34.385981
   MACCORMICK J, 1999, P IEEE C COMP VIS PA
   Medioni G, 2001, IEEE T PATTERN ANAL, V23, P873, DOI 10.1109/34.946990
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   RAJAGOPALAN AN, 1997, P TENCHON 97, P803
   Rasmussen C, 1998, PROC CVPR IEEE, P16, DOI 10.1109/CVPR.1998.698582
   REHG JM, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P612, DOI 10.1109/ICCV.1995.466882
   REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177
   ROSALES R, 1998, WORKSH INT VIS MOT C, P10
   SETHI IK, 1987, IEEE T PATTERN ANAL, V9, P56, DOI 10.1109/TPAMI.1987.4767872
   Shalom Y.B., 1988, Tracking and Data association
   Sharma R, 1998, P IEEE, V86, P853, DOI 10.1109/5.664275
   SMITH P, 1975, IEEE T AUTOMAT CONTR, VAC20, P101, DOI 10.1109/TAC.1975.1100851
   Wachter S, 1997, IEEE NONRIGID AND ARTICULATED MOTION WORKSHOP, PROCEEDINGS, P2, DOI 10.1109/NAMW.1997.609843
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Wu Y, 2002, IEEE T NEURAL NETWOR, V13, P948, DOI 10.1109/TNN.2002.1021895
   Yeasin M, 2000, PATTERN RECOGN, V33, P1805, DOI 10.1016/S0031-3203(99)00175-2
   Yeasin M, 2000, PROC CVPR IEEE, P168, DOI 10.1109/CVPR.2000.854770
NR 35
TC 8
Z9 9
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 398
EP 405
DI 10.1109/TMM.2004.827514
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200002
DA 2024-07-18
ER

PT J
AU Sehgal, A
   Jagmohan, A
   Ahuja, N
AF Sehgal, A
   Jagmohan, A
   Ahuja, N
TI Wyner-Ziv coding of video: An error-resilient compression framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID SIDE INFORMATION; BINARY SOURCES; CODES; DECODER
AB This paper addresses the problem of video coding in a joint source-channel setting. In particular, we propose a video encoding algorithm that prevents the indefinite propagation of errors in predictively encoded video-a problem that has received considerable attention over the last decade. This is accomplished by periodically transmitting a small amount of additional information, termed, coset information, to the decoder, as opposed to the popular approach of periodic insertion of intra-coded frames. Perhaps surprisingly, the coset information is capable of correcting for errors, without the encoder having a precise knowledge of the lost packets that resulted in the errors. In the context of real-time transmission,, the proposed approach entails a minimal loss in performance over conventional encoding in the absence of channel losses, while simultaneously Allowing error recovery in the event of channel losses. We demonstrate the efficacy of the proposed approach through experimental evaluation. In particular, the performance of the proposed framework is 34 dB superior to the conventional approach of periodic insertion of intra-coded frames, and 1.5-2 dB away from an ideal system, with infinite decoding delay, operating at Shannon capacity.
C1 Univ Illinois, Beckman Inst, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Univ Illinois, Beckman Inst, Urbana, IL 61801 USA.
EM anshul@vision.ai.uiuc.edu; jagmohan@vision.ai.uiuc.edu;
   ahuja@vision.ai.uiuc.edu
CR Chou PA, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P587, DOI 10.1109/MMSP.2001.962796
   FROSSARD P, 1998, SPIE INT S VOIC VID, V3528, P113
   Garcia-Frias J, 2001, IEEE COMMUN LETT, V5, P417, DOI 10.1109/4234.957380
   Hemami SS, 1997, IEEE T IMAGE PROCESS, V6, P523, DOI 10.1109/83.563318
   Jagmohan A, 2003, IEEE DATA COMPR CONF, P213
   JAGMOHAN A, 2003, P IEEE INT C MULT EX, V1, P569
   JAGMOHAN A, 2002, P IEEE INT C IM PROC, V2, P29
   JOHN S, 1998, PACKET VIDEO WORKSHO
   Liveris AD, 2002, IEEE COMMUN LETT, V6, P440, DOI 10.1109/LCOMM.2002.804244
   MacKay DJC, 1996, ELECTRON LETT, V32, P1645, DOI 10.1049/el:19961141
   Pradhan SS, 1999, IEEE DATA COMPR CONF, P158, DOI 10.1109/DCC.1999.755665
   PURI R, PRISM VIDEO CODING A
   Reibman AR, 2002, IEEE T CIRC SYST VID, V12, P193, DOI 10.1109/76.993440
   REIBMAN AR, 1999, P IEEE INT C IM PROC
   Rose K, 2001, IEEE T IMAGE PROCESS, V10, P965, DOI 10.1109/83.931091
   Sehgal A, 2003, IEEE IMAGE PROC, P605
   Sehgal A, 2003, IEEE DATA COMPR CONF, P103
   SEHGAL A, 2002, P IEEE INT C COMM SY
   SEHGAL A, WYNER ZIV CODING VID
   Stockhammer T, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P173
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   Turaga DS, 2002, IEEE T CIRC SYST VID, V12, P483, DOI 10.1109/TCSVT.2002.800318
   Wang A, 1999, IEEE DATA COMPR CONF, P404, DOI 10.1109/DCC.1999.755690
   Wang Y, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P441, DOI 10.1109/MMSP.2001.962773
   WIEGAND T, 2001, VCEGN83D1 ITUT VID C
   WYNER AD, 1976, IEEE T INFORM THEORY, V22, P1, DOI 10.1109/TIT.1976.1055508
   Zamir R, 2002, IEEE T INFORM THEORY, V48, P1250, DOI 10.1109/TIT.2002.1003821
   Zamir R, 1998, 1998 INFORMATION THEORY WORKSHOP - KILLARNEY, IRELAND, P92, DOI 10.1109/ITW.1998.706450
   ZHAO Y, 2002, P DCC 02 SNOWB UT AP
   [No title captured]
NR 30
TC 85
Z9 98
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 249
EP 258
DI 10.1109/TMM.2003.822995
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400004
DA 2024-07-18
ER

PT J
AU Stockhammer, T
   Jenkac, H
   Kuhn, G
AF Stockhammer, T
   Jenkac, H
   Kuhn, G
TI Streaming video over variable bit-rate wireless channels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE receiver buffer; streaming video; variable bit-rate (VBR); wireless
   video
ID VBR VIDEO
AB We consider streaming of video sequences over both constant and variable bit-rate (VBR) channels. Our goal is to enable decoding of each video unit before exceeding its displaying deadline and, hence, to guarantee successful sequence presentation even if the media rate does not match the channel rate. In this work, we will show that the separation between a delay jitter buffer and a decoder buffer is in general suboptimal for VBR video transmitted over VBR channels. We will specify the minimum initial delay and the minimum required buffer for a given video stream and a deterministic VBR channel. In addition, we provide some probabilistic statements in case that we observe a random behavior of the channel bit rate. A specific example tailored to wireless video streaming is discussed in greater details and bounds are derived which allow guaranteeing a certain quality-of-service even for random VBR channels in a wireless environment. Simulation results validate the findings.
C1 Tech Univ Munich, Inst Commun Engn, D-80290 Munich, Germany.
   Tech Univ Munich, Ctr Math Sci, D-80290 Munich, Germany.
C3 Technical University of Munich; Technical University of Munich
RP Stockhammer, T (corresponding author), Tech Univ Munich, Inst Commun Engn, D-80290 Munich, Germany.
RI Stockhammer, Thomas/AAC-8090-2022
OI Stockhammer, Thomas/0000-0003-2917-4234
CR *3GPP TSG SA, 2002, UMTS VID STREAM US C
   [Anonymous], MSRTR200135
   [Anonymous], 1991, WAHRSCHEINLICHKEITST
   FINDELI M, 2002, THESIS MUNICH U TECH
   Girod B., 2002, P INT C IM PROC ICIP
   Hsu CY, 1997, IEEE J SEL AREA COMM, V15, P1016, DOI 10.1109/49.611156
   *ISO IEC, 2000, 13818002 ISOIEC
   Lakshman TV, 1998, P IEEE, V86, P952, DOI 10.1109/5.664282
   LIANG YJ, 2002, P IEEE INT C MULT EX
   Pao IM, 2001, IEEE T CIRC SYST VID, V11, P199, DOI 10.1109/76.905985
   Reibman AR, 1992, IEEE T CIRC SYST VID, V2, P361, DOI 10.1109/76.168912
   RIBASCORBERA J, 2002, P INT C IM PROC ICIP
   STEINBACH EG, 2001, P INT C IM PROC ICIP
   Varsa V., 2003, 3GPP TR26 937, pV1
   VARSA V, 2001, P 11 INT PACK VID WO
NR 15
TC 74
Z9 89
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 268
EP 277
DI 10.1109/TMM.2003.822795
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400006
DA 2024-07-18
ER

PT J
AU Bao, QQ
   Liu, YM
   Gang, BW
   Yang, WM
   Liao, QM
AF Bao, Qiqi
   Liu, Yunmeng
   Gang, Bowen
   Yang, Wenming
   Liao, Qingmin
TI SCTANet: A Spatial Attention-Guided CNN-Transformer Aggregation Network
   for Deep Face Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Faces; Image reconstruction; Feature extraction; Face
   recognition; Task analysis; Superresolution; Face super-resolution;
   CNN-transformer aggregation; spatial attention; GAN; MLP
ID FIELDS
AB Numerous CNN-based algorithms have been proposed to reconstruct high-quality face images. However, the inability of convolution operation to model long-distance relationships limits the performance of the CNN-based methods. Moreover, in the high-resolution (HR) image reconstruction stage, with the well decoded feature representations, more efficient architecture design can be explored to synthesize pixel-level image details. In this work, we propose a spatial attention-guided CNN-Transformer aggregation network (SCTANet) for face image super-resolution (FSR) tasks. The core component in the deep feature extraction stage is the Hybrid Attention Aggregation (HAA) block. The HAA block has two parallel paths, one for the Residual Spatial Attention (RSA) block, the other for the Multi-scale Patch embedding and Spatial-attention Masked Transformer (MPSMT) block. The HAA block combines the strengths of CNN and transformer to effectively exploit both local and global information. For the reconstruction stage, we propose to use the Sub-pixel MLP-based Upsampling (SMU) module instead of the conventional CNN architecture. The SMU module promotes the reconstruction of pixel-level image details and reduces computational complexity. Extensive experiments on both synthetic and real-world face datasets demonstrate the superiority of our proposed SCTANet over state-of-the-art methods.
C1 [Bao, Qiqi; Yang, Wenming; Liao, Qingmin] Tsinghua Univ, Shenzhen Int Grad Sch, Dept Elect Engn, Shenzhen 518055, Peoples R China.
   [Liu, Yunmeng] Chinese Acad Sci, Shanghai Inst Tech Phys, Key Lab Infrared Syst Detect & Imaging Technol, Shanghai 200083, Peoples R China.
   [Gang, Bowen] Fudan Univ, Sch Management, Dept Stat & Data Sci, Shanghai 200433, Peoples R China.
C3 Tsinghua University; Chinese Academy of Sciences; Shanghai Institute of
   Technical Physics, CAS; Fudan University
RP Yang, WM (corresponding author), Tsinghua Univ, Shenzhen Int Grad Sch, Dept Elect Engn, Shenzhen 518055, Peoples R China.
EM bqq19@mails.tsinghua.edu.cn; lym_sitp@163.com; bgang@fudan.edu.cn;
   yang.wenming@sz.tsinghua.edu.cn; liaoqm@tsinghua.edu.cn
RI Bao, Qiqi/KSM-2412-2024
OI Gang, Bowen/0000-0002-7705-2584; Yang, Wenming/0000-0002-2506-1286
FU National Natural Science Foundation of China
FX No Statement Available
CR Bao Qiqi, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P6727, DOI 10.1145/3503161.3548437
   Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019
   Bulat A, 2018, LECT NOTES COMPUT SC, V11210, P187, DOI 10.1007/978-3-030-01231-1_12
   Chen CF, 2021, IEEE T IMAGE PROCESS, V30, P1219, DOI 10.1109/TIP.2020.3043093
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen SF, 2022, Arxiv, DOI arXiv:2107.10224
   Chen YB, 2021, PROC CVPR IEEE, P8624, DOI 10.1109/CVPR46437.2021.00852
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Ding CX, 2018, IEEE T PATTERN ANAL, V40, P1002, DOI 10.1109/TPAMI.2017.2700390
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gao GW, 2022, Arxiv, DOI arXiv:2204.08696
   Grgic M, 2011, MULTIMED TOOLS APPL, V51, P863, DOI 10.1007/s11042-009-0417-2
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hou H, 2022, Arxiv, DOI arXiv:2205.03777
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang JJ, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3485132
   Jiang K, 2022, IEEE T NEUR NET LEAR, V33, P378, DOI 10.1109/TNNLS.2020.3027849
   Jiang K, 2020, IEEE T MULTIMEDIA, V22, P2734, DOI 10.1109/TMM.2019.2960586
   Kim D, 2019, Arxiv, DOI arXiv:1908.08239
   Kingma D. P., 2014, arXiv
   Li MY, 2021, IEEE T MULTIMEDIA, V23, P468, DOI 10.1109/TMM.2020.2984092
   Li WB, 2022, Arxiv, DOI arXiv:2112.10175
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5501, DOI 10.1145/3474085.3475682
   Ludwiczuk B., 2016, Tech. Rep. CMU-CS-16-118
   Ma C, 2020, PROC CVPR IEEE, P5568, DOI 10.1109/CVPR42600.2020.00561
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Shi JG, 2015, IEEE SIGNAL PROC LET, V22, P554, DOI 10.1109/LSP.2014.2364262
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sitzmann V., 2020, ADV NEURAL INF PROCE, V33, P7462
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Touvron H, 2021, Arxiv, DOI [arXiv:2105.03404, DOI 10.48550/ARXIV.2105.03404]
   Tu Z., 2022, P IEEE CVF C COMP VI, P5769
   Tuzel O, 2016, Arxiv, DOI arXiv:1603.07235
   Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9
   Wang YZ, 2021, Arxiv, DOI arXiv:2109.08174
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Xiaobin Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P763, DOI 10.1007/978-3-030-58548-8_44
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14
   Yu X, 2020, INT J COMPUT VISION, V128, P500, DOI 10.1007/s11263-019-01254-5
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang X., 2022, arXiv
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao L., 2021, Advances in Neural Information Processing Systems, P18367
   Zheng WB, 2019, IEEE IMAGE PROC, P2851, DOI [10.1109/icip.2019.8803393, 10.1109/ICIP.2019.8803393]
   Zhou JT, 2019, IEEE T INF FOREN SEC, V14, P2537, DOI 10.1109/TIFS.2019.2900907
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 65
TC 10
Z9 10
U1 19
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8554
EP 8565
DI 10.1109/TMM.2023.3238522
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000001
DA 2024-07-18
ER

PT J
AU Chen, HY
   Teng, MG
   Shi, BX
   Wang, YZ
   Huang, TJ
AF Chen, Haoyu
   Teng, Minggui
   Shi, Boxin
   Wang, Yizhou
   Huang, Tiejun
TI A Residual Learning Approach to Deblur and Generate High Frame Rate
   Video With an Event Camera
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deblur; HFR video generation; event camera; residual learning
ID SPARSE REPRESENTATION; IMAGE
AB Event cameras are bio-inspired cameras that can measure the intensity change asynchronously with high temporal resolution. One of the advantages of event cameras is that they suffer less from motion blur than traditional frame cameras when recording daily scenes with fast-moving objects. In this paper, we formulate the deblurring task on traditional cameras directed by events to be a residual learning one, and propose corresponding network architectures for effective learning of deblurring and high frame rate video generation tasks. We first train a modified U-Net network to restore a sharp image from a blurry image using the corresponding events. Then we train another similar network by replacing the downsampling blocks with blocks of the convolutional long short-term memory (Conv-LSTM) to recurrently generate high frame rate video using the restored sharp image and part of the events. Benefitting from the blur-free events and the proposed learning strategy, the experimental results show that the proposed method outperforms state-of-the-art methods for generating sharp images and high frame rate videos.
C1 [Chen, Haoyu] Bilibili, Shanghai 200433, Peoples R China.
   [Chen, Haoyu] Peking Univ, Beijing 100871, Peoples R China.
   [Teng, Minggui; Shi, Boxin; Huang, Tiejun] Peking Univ, Sch Comp Sci, Natl Engn Res Ctr Visual Technol, Beijing 100871, Peoples R China.
   [Shi, Boxin; Huang, Tiejun] Peking Univ, Inst Artificial Intelligence, Beijing 100871, Peoples R China.
   [Shi, Boxin; Huang, Tiejun] Beijing Acad Artificial Intelligence, Beijing 100084, Peoples R China.
   [Wang, Yizhou] Peking Univ, Sch Comp Sci, Ctr Frontiers Comp, Beijing 100871, Peoples R China.
C3 Peking University; Peking University; Peking University; Peking
   University
RP Shi, BX (corresponding author), Peking Univ, Sch Comp Sci, Natl Engn Res Ctr Visual Technol, Beijing 100871, Peoples R China.
EM chenhaoyu02@bilibili.com; minggui_teng@pku.edu.cn; shiboxin@pku.edu.cn;
   yizhou.wang@pku.edu.cn; tjhuang@pku.edu.cn
RI Huang, Tiejun/D-6161-2011
FU National Key R&D Program of China [2021ZD0109803]; National Natural
   Science Foundation of China [62136001, 62088102]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021ZD0109803 and in part by the National Natural Science
   Foundation of China under Grants 62136001 and 62088102.
CR Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Brandli C, 2014, IEEE INT SYMP CIRC S, P686, DOI 10.1109/ISCAS.2014.6865228
   Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Girshick R., 2014, P 2014 IEEE C COMPUT, P580, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Jiang Z, 2020, PROC CVPR IEEE, P3317, DOI 10.1109/CVPR42600.2020.00338
   Jin MG, 2018, PROC CVPR IEEE, P6334, DOI 10.1109/CVPR.2018.00663
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kumar K, 2019, J VIS COMMUN IMAGE R, V58, P345, DOI 10.1016/j.jvcir.2018.12.009
   Kumar K, 2018, MULTIMED TOOLS APPL, V77, P7383, DOI 10.1007/s11042-017-4642-9
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Li JC, 2021, IEEE T MULTIMEDIA, V23, P2986, DOI 10.1109/TMM.2021.3068561
   Liang CH, 2022, IEEE T MULTIMEDIA, V24, P61, DOI 10.1109/TMM.2020.3045303
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Mehri A, 2021, IEEE WINT CONF APPL, P2703, DOI 10.1109/WACV48630.2021.00275
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Miyatani Y, 2016, IEEE WINT CONF APPL
   Nah S, 2019, PROC CVPR IEEE, P8094, DOI 10.1109/CVPR.2019.00829
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Pan LY, 2022, IEEE T PATTERN ANAL, V44, P2519, DOI 10.1109/TPAMI.2020.3036667
   Pan LY, 2019, PROC CVPR IEEE, P6813, DOI 10.1109/CVPR.2019.00698
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Purohit K, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00699
   Rebecq H, 2019, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2019.00398
   Rebecq H, 2021, IEEE T PATTERN ANAL, V43, P1964, DOI 10.1109/TPAMI.2019.2963386
   Reinbacher C., 2016, PROC BRIT MACH VIS C, P91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Scaramuzza D., 2018, P MACHINE LEARNING R, P969
   Scheerlinck C, 2019, IEEE COMPUT SOC CONF, P1684, DOI 10.1109/CVPRW.2019.00215
   Scheerlinck C, 2019, LECT NOTES COMPUT SC, V11365, P308, DOI 10.1007/978-3-030-20873-8_20
   Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418
   Shi XJ, 2015, ADV NEUR IN, V28
   Shoushun C., 2018, Patent, Patent No. [9,961,291, 9961291]
   Sironi A, 2018, PROC CVPR IEEE, P1731, DOI 10.1109/CVPR.2018.00186
   Songnan Lin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P695, DOI 10.1007/978-3-030-58598-3_41
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Wang Bishan, 2020, EUR C COMP VIS, P155
   Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032
   Xiao L, 2016, LECT NOTES COMPUT SC, V9907, P734, DOI 10.1007/978-3-319-46487-9_45
   Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Yu X, 2014, IEEE T MULTIMEDIA, V16, P1510, DOI 10.1109/TMM.2014.2321734
   Zhang HC, 2013, PROC CVPR IEEE, P1051, DOI 10.1109/CVPR.2013.140
   Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267
   Zhang KH, 2019, IEEE T IMAGE PROCESS, V28, P291, DOI 10.1109/TIP.2018.2867733
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhihang Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P191, DOI 10.1007/978-3-030-58539-6_12
   Zhou SC, 2019, IEEE I CONF COMP VIS, P2482, DOI 10.1109/ICCV.2019.00257
   Zhu AZ, 2019, PROC CVPR IEEE, P989, DOI 10.1109/CVPR.2019.00108
NR 61
TC 6
Z9 6
U1 4
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5826
EP 5839
DI 10.1109/TMM.2022.3199556
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500013
DA 2024-07-18
ER

PT J
AU Fang, H
   Xiong, PF
   Xu, LH
   Luo, WH
AF Fang, Han
   Xiong, Pengfei
   Xu, Luhui
   Luo, Wenhan
TI Transferring Image-CLIP to Video-Text Retrieval via Temporal Relations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image-text pretrained; temporal transformer; video-text retrieval
AB We present a novel network to transfer the image-language pre-trained model to video-text retrieval in an end-to-end manner. Leading approaches in the domain of video-and-language learning try to distill the spatio-temporal video features and multi-modal interaction between videos and language from a large-scale video-text dataset. Differently, we leverage the pre-trained image-language model, and simplify it as a two-stage framework including co-learning of image and text, and enhancing temporal relations between video frames and video-text respectively. Specifically, based on the spatial semantics captured by Contrastive Language-Image Pre-training (CLIP) model, our model involves a Temporal Difference Block (TDB) to capture motions at fine temporal video frames, and a Temporal Alignment Block (TAB) to re-align the tokens of video clips and phrases and enhance the cross-modal correlation. These two temporal blocks efficiently realize video-language learning and enable the proposed model to scale well on comparatively small datasets. We conduct extensive experimental studies including ablation studies and comparisons with existing SOTA methods, and our proposed approach outperforms them on the popularly-employed text-to-video and video-to-text retrieval benchmarks, including MSR-VTT, MSVD, LSMDC, and VATEX.
C1 [Fang, Han] China Telecom Corp Ltd, Digital Intelligence Technol Co, Beijing 100033, Peoples R China.
   [Xiong, Pengfei; Xu, Luhui] Tencent Technol, Beijing 100193, Peoples R China.
   [Luo, Wenhan] Sun Yat Sen Univ, Shenzhen Campus, Shenzhen 518107, Guangdong, Peoples R China.
C3 China Telecom Corp. Ltd.; Tencent; Sun Yat Sen University
RP Luo, WH (corresponding author), Sun Yat Sen Univ, Shenzhen Campus, Shenzhen 518107, Guangdong, Peoples R China.
EM anghan1996@outlook.com; xiongpengfei2019@gmail.com;
   luhuixu.cn@gmail.com; whluo.china@gmail.com
RI Luo, Wenhan/GZL-0535-2022
OI Luo, Wenhan/0000-0002-5697-4168
FU Shenzhen Science and Technology Program [JSGG20220831093004008,
   JCYJ20220818102012025]
FX This work was supported by the Shenzhen Science and Technology Program,
   under Grants JSGG20220831093004008 and JCYJ20220818102012025.
CR Amrani E, 2021, AAAI CONF ARTIF INTE, V35, P6644
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Bogolin SV, 2022, PROC CVPR IEEE, P5184, DOI 10.1109/CVPR52688.2022.00513
   Stroud JC, 2021, Arxiv, DOI arXiv:2007.14937
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen FY, 2021, IEEE T MULTIMEDIA, V23, P3073, DOI 10.1109/TMM.2020.3019710
   Chen S., 2020, P IEEE CVF C COMP VI, P10638
   Chen ZF, 2020, Arxiv, DOI arXiv:2001.09308
   Chen ZF, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1884
   Cheng X., 2021, arXiv
   Croitoru I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11563, DOI 10.1109/ICCV48922.2021.01138
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Dosovitskiy A., 2021, INT C LEARN REPRESEN, P1
   Dzabraev M, 2021, IEEE COMPUT SOC CONF, P3349, DOI 10.1109/CVPRW53098.2021.00374
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Fu X, 2020, IEEE T MULTIMEDIA, V22, P2354, DOI 10.1109/TMM.2019.2957948
   Gabeur Valentin, 2020, P EUR C COMP VIS, V12349, P214, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Gao ZJ, 2022, Arxiv, DOI arXiv:2111.05610
   Ghadiyaram D, 2019, PROC CVPR IEEE, P12038, DOI 10.1109/CVPR.2019.01232
   Gorti SK, 2022, PROC CVPR IEEE, P4996, DOI 10.1109/CVPR52688.2022.00495
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Lei Ba J., 2016, arXiv
   Lei J, 2021, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR46437.2021.00725
   Li JN, 2020, IEEE T MULTIMEDIA, V22, P554, DOI 10.1109/TMM.2019.2930041
   Li KP, 2023, IEEE T PATTERN ANAL, V45, P641, DOI 10.1109/TPAMI.2022.3148470
   Li LJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2046
   Li TH, 2021, Arxiv, DOI arXiv:2001.05691
   Li XR, 2021, IEEE T MULTIMEDIA, V23, P4351, DOI 10.1109/TMM.2020.3042067
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Liu S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11895, DOI 10.1109/ICCV48922.2021.01170
   Liu Y., 2019, P BRIT MACH VIS C, P279
   Luo H., 2021, CLIP4CLIP EMPIRICAL
   Luo HS, 2020, Arxiv, DOI arXiv:2002.06353
   Luo JJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5600, DOI 10.1145/3474085.3475703
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Malali N, 2022, IEEE T PATTERN ANAL, V44, P10252, DOI 10.1109/TPAMI.2021.3132163
   Miech Antoine, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9876, DOI 10.1109/CVPR42600.2020.00990
   Miech A, 2018, ARXIV
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Ning K, 2020, IEEE T MULTIMEDIA, V22, P2434, DOI 10.1109/TMM.2019.2957854
   Patrick Mandela, 2021, P INT C LEARN REPR
   Portillo-Quintero JA, 2021, LECT NOTES COMPUT SC, V12725, P3, DOI 10.1007/978-3-030-77004-4_1
   Radford A., 2019, LANGUAGE MODELS ARE
   Radford A, 2021, PR MACH LEARN RES, V139
   Rohrbach A, 2017, INT J COMPUT VISION, V123, P94, DOI 10.1007/s11263-016-0987-1
   Rohrbach A, 2015, LECT NOTES COMPUT SC, V9358, P209, DOI 10.1007/978-3-319-24947-6_17
   Sanh V, 2019, P NEUR INF PROC SYST, P1
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715
   Sharir G., 2021, arXiv
   Song G, 2021, IEEE T MULTIMEDIA, V23, P1708, DOI 10.1109/TMM.2020.3002177
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Tang HY, 2022, IEEE T MULTIMEDIA, V24, P1338, DOI 10.1109/TMM.2021.3063631
   Teng JY, 2022, IEEE T MULTIMEDIA, V24, P1141, DOI 10.1109/TMM.2021.3120545
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang L, 2019, IEEE I CONF COMP VIS, P8697, DOI 10.1109/ICCV.2019.00879
   Wang W, 2023, IEEE T MULTIMEDIA, V25, P2661, DOI 10.1109/TMM.2022.3149716
   Wang W, 2021, IEEE T MULTIMEDIA, V23, P2386, DOI 10.1109/TMM.2020.3011288
   Wang XH, 2021, PROC CVPR IEEE, P5075, DOI 10.1109/CVPR46437.2021.00504
   Wang X, 2019, IEEE I CONF COMP VIS, P4580, DOI 10.1109/ICCV.2019.00468
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P3362, DOI 10.1109/TMM.2020.3024822
   Wightman Ross, 2019, PYTORCH IMAGE MODELS
   Wu YL, 2021, IEEE T MULTIMEDIA, V23, P559, DOI 10.1109/TMM.2020.2985540
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xue HW, 2022, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR52688.2022.00498
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23
   Zhang D, 2018, P BRIT MACH VIS C, P1
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P1320, DOI 10.1109/TMM.2022.3141603
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zhang X, 2022, IEEE T MULTIMEDIA, V24, P2986, DOI 10.1109/TMM.2021.3091882
   Zhang ZJ, 2021, IEEE T MULTIMEDIA, V23, P3306, DOI 10.1109/TMM.2020.3023339
   Zhao N, 2017, IEEE T MULTIMEDIA, V19, P2080, DOI 10.1109/TMM.2017.2722687
   Zhao S, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P970, DOI 10.1145/3477495.3531950
   Zhu L., 2020, P IEEE CVF C COMP VI, DOI 10.1109/CVPR42600.2020.00877
NR 86
TC 1
Z9 1
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7772
EP 7785
DI 10.1109/TMM.2022.3227416
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400013
DA 2024-07-18
ER

PT J
AU Feng, YB
   Gao, JY
   Yang, SC
   Xu, CS
AF Feng, Yangbo
   Gao, Junyu
   Yang, Shicai
   Xu, Changsheng
TI Spatial-Temporal Exclusive Capsule Network for Open Set Action
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Task analysis; Routing; Deep learning; Three-dimensional
   displays; Image recognition; Bayes methods; Open set action recognition;
   spatial-temporal learning; capsule network
AB Open set action recognition (OSAR) is a rising research domain that simultaneously identifies all videos from known classes and rejects videos from unknown classes. Existing methods rarely consider the open set data distribution and the spatial-temporal relations of video subsequence. Recently proposed Capsule Network (CapsNet) has shown robust performance in many fields, especially image recognition. However, the current CapsNet has not been directly applied to the OSAR task since it cannot explicitly consider the data distribution of known and unknown classes along with the spatial-temporal relations for videos. This paper proposes the Spatial-Temporal Exclusive Capsule Network (STE-CapsNet) to solve the problems in the OSAR task. The STE-CapsNet designs the temporal-spatial routing mechanism to jointly capture the spatial-temporal information of the videos. Furthermore, the exclusive capsules are learned with dot product routing mechanism to limit the data distribution of closed set and open set and reduce the open set risk for OSAR. Extensive experimental results demonstrate that our proposed approach performs favorably compared with state-of-the-art methods on three standard datasets, which verifies its effectiveness and generalization ability.
C1 [Feng, Yangbo; Yang, Shicai] Tianjin Univ Technol, Tianjin 300382, Peoples R China.
   [Gao, Junyu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Gao, Junyu; Yang, Shicai; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
   [Yang, Shicai] Hikvis Res Inst, Hangzhou 310051, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Tianjin University of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
EM ybfeng6@gmail.com; gaojunyu2015@ia.ac.cn; yangshicai@hikvision.com;
   csxu@nlpr.ia.ac.cn
RI Gao, Junyu/HDO-5516-2022; xu, cj/HJZ-3488-2023
OI Gao, Junyu/0000-0002-8105-5497; xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Plan of China
FX No Statement Available
CR Algamdi AM, 2019, INT CONF ACOUST SPEE, P3867, DOI [10.1109/ICASSP.2019.8683720, 10.1109/icassp.2019.8683720]
   Bao W., 2021, IEEE CVF ICCV, P13349
   Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Busto PP, 2020, IEEE T PATTERN ANAL, V42, P413, DOI 10.1109/TPAMI.2018.2880750
   Cen J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15313, DOI 10.1109/ICCV48922.2021.01505
   Chen GY, 2022, IEEE T PATTERN ANAL, V44, P8065, DOI 10.1109/TPAMI.2021.3106743
   Chen KX, 2020, IEEE T NEUR NET LEAR, V31, P1747, DOI 10.1109/TNNLS.2019.2927224
   Chen YS, 2022, IEEE T CIRC SYST VID, V32, P2962, DOI 10.1109/TCSVT.2021.3104226
   Cui P, 2012, IEEE T MULTIMEDIA, V14, P102, DOI 10.1109/TMM.2011.2176110
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ditria B. J., 2020, ASIAN C COMPUT VIS
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Feng J., 2022, IEEE Trans. Multimedia, earlyaccess, DOI [10.1109/TMM.2022.3156938.[49]Y., DOI 10.1109/TMM.2022.3156938.[49]Y]
   Fu H, 2021, INT C PATT RECOG, P446, DOI 10.1109/ICPR48806.2021.9412057
   Gal Y, 2016, PR MACH LEARN RES, V48
   Gao JY, 2021, IEEE T MULTIMEDIA, V23, P3203, DOI 10.1109/TMM.2020.3021980
   Gao JY, 2021, IEEE T PATTERN ANAL, V43, P3476, DOI 10.1109/TPAMI.2020.2985708
   Ge S., 2017, BRIT MACH VIS C
   Guangyao Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P507, DOI 10.1007/978-3-030-58580-8_30
   Guo YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P103, DOI 10.1109/ICCV48922.2021.00017
   Hahn T, 2019, ADV NEUR IN, V32
   Hinton S., 2018, INT C LEARN REPR, V18
   Hu YF, 2023, IEEE T MULTIMEDIA, V25, P2061, DOI 10.1109/TMM.2022.3142413
   Hu YF, 2021, IEEE T MULTIMEDIA, V23, P4285, DOI 10.1109/TMM.2020.3039329
   Jain LP, 2014, LECT NOTES COMPUT SC, V8691, P393, DOI 10.1007/978-3-319-10578-9_26
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kong S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P793, DOI 10.1109/ICCV48922.2021.00085
   Kosiorek AR, 2019, ADV NEUR IN, V32
   Krishnan R, 2018, Arxiv, DOI arXiv:1811.03305
   Krishnan R, 2020, AAAI CONF ARTIF INTE, V34, P4477
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lei Ba J., 2016, arXiv
   Li F, 2005, IEEE T PATTERN ANAL, V27, P1686, DOI 10.1109/TPAMI.2005.224
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu Y, 2019, IEEE I CONF COMP VIS, P1232, DOI 10.1109/ICCV.2019.00132
   Luo MN, 2018, IEEE T CYBERNETICS, V48, P648, DOI 10.1109/TCYB.2017.2647904
   Ma D, 2021, PROC CVPR IEEE, P10943, DOI 10.1109/CVPR46437.2021.01080
   Mendes PR, 2017, MACH LEARN, V106, P359, DOI 10.1007/s10994-016-5610-8
   Monfort M, 2022, IEEE T PATTERN ANAL, V44, P9434, DOI 10.1109/TPAMI.2021.3126682
   Oza P, 2019, PROC CVPR IEEE, P2302, DOI 10.1109/CVPR.2019.00241
   Perera Pramuditha, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11811, DOI 10.1109/CVPR42600.2020.01183
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Sabour N., 2017, Adv. Neural Inf. Process. Syst.
   Scheirer WJ, 2014, IEEE T PATTERN ANAL, V36, P2317, DOI 10.1109/TPAMI.2014.2321392
   Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256
   Shao D, 2020, PROC CVPR IEEE, P727, DOI 10.1109/CVPR42600.2020.00081
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Shu Y, 2018, IEEE INT CON MULTI
   Shu Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-63649-6
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh M, 2019, IEEE I CONF COMP VIS, P340, DOI 10.1109/ICCV.2019.00043
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Subedar M, 2019, IEEE I CONF COMP VIS, P6310, DOI 10.1109/ICCV.2019.00640
   Sun X., 2020, P IEEE CVF C COMP VI, P13480
   Tsai Y.-H. H., 2020, P INT C LEARN REPR, P1
   Urooj A., 2021, P IEEE CVF C COMP VI, P8465
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wu XX, 2019, INT CONF ACOUST SPEE, P6695, DOI 10.1109/ICASSP.2019.8683163
   Xu YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9312, DOI 10.1109/ICCV48922.2021.00920
   Yang HM, 2022, IEEE T PATTERN ANAL, V44, P2358, DOI 10.1109/TPAMI.2020.3045079
   Yang HM, 2018, PROC CVPR IEEE, P3474, DOI 10.1109/CVPR.2018.00366
   Yang Y, 2019, PATTERN RECOGN, V85, P60, DOI 10.1016/j.patcog.2018.07.030
   Yoshihashi R, 2019, PROC CVPR IEEE, P4011, DOI 10.1109/CVPR.2019.00414
   Yu C, 2022, PROC CVPR IEEE, P4022, DOI 10.1109/CVPR52688.2022.00400
   Zhang DL, 2020, IEEE T CYBERNETICS, V50, P3033, DOI 10.1109/TCYB.2019.2905157
   Zhang H, 2017, IEEE T PATTERN ANAL, V39, P1690, DOI 10.1109/TPAMI.2016.2613924
   Zhang H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6954, DOI 10.1109/ICCV48922.2021.00689
   Zhao AT, 2022, IEEE T MULTIMEDIA, V24, P846, DOI 10.1109/TMM.2021.3060280
   Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110
   Zhou D.W., 2021, P IEEE CVF C COMP VI, P4401
NR 73
TC 2
Z9 2
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9464
EP 9478
DI 10.1109/TMM.2023.3252275
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200028
DA 2024-07-18
ER

PT J
AU He, KJ
   Zhang, XJ
   Xu, D
   Gong, J
   Xie, LSQ
AF He, Kangjian
   Zhang, Xuejie
   Xu, Dan
   Gong, Jian
   Xie, Lisiqi
TI Fidelity-driven Optimization Reconstruction and Details Preserving
   Guided Fusion for Multi-Modality Medical Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Details preserving; image fusion; low-rank approximation;
   Multi-modality; weighted mean curvature
ID MULTI-FOCUS IMAGE; COUPLED NEURAL-NETWORK; REPRESENTATION; INFORMATION;
   PERFORMANCE; MODEL
AB By integrating effective features of multi-modality medical images to provide richer information, multi-modality medical image fusion has been substantially used in computer-aided diagnosis applications. However, many existing fusion schemes do not consider how to eliminate the effects of the noise in source medical images and cannot provide enough details and textures for disease diagnosis. To address the problems above, we propose a new fidelity-driven optimization (FDO) reconstruction and details preserving guided-based fusion method for multi-modality medical images. To overcome the influence of noise in multi-modality medical images, a rank coefficient optimization method of low-rank approximation based on weighted mean curvature is proposed to reconstruct multi-modality medical image. Moreover, we propose an iterative detail preserving guided fusion (DPGF) method to integrate more textures and detail information of source multi-modality medical images, while ensuring high signal-to noise ratios. The experimental results show that the proposed method outperforms some of the state-of-the-art fusion methods. Specifically, the extensive experiments prove that our method has high robustness for noisy medical images, which also indicates the application prospects in diagnosis applications.
C1 [He, Kangjian; Zhang, Xuejie; Xu, Dan; Gong, Jian; Xie, Lisiqi] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Peoples R China.
C3 Yunnan University
RP Xu, D (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Peoples R China.
EM hekj@ynu.edu.cn; xjzhang@ynu.edu.cn; danxu@ynu.edu.cn; gon3d@live.com;
   shirleys.qi@hotmail.com
RI He, Kangjian/CAG-0300-2022; Xu, Dan/KPA-7396-2024; Zhang,
   Jianjun/KEJ-3941-2024
OI Xu, Dan/0000-0003-4602-3550; Zhang, Xuejie/0000-0002-6591-0916; He,
   Kangjian/0000-0001-6207-9728
FU National Natural Science Foundation of China [62162068, 61761049];
   Yunnan Province Ten Thousand Talents Program and Yunling Scholars
   Special Project [YNWR-YLXZ-2018-022]; Yunnan Provincial Science and
   Technology Department- Yunnan University "Double First Class"
   Construction Joint Fund Project [2019FY003012]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62162068 and 61761049, in part by the
   Yunnan Province Ten Thousand Talents Program and Yunling Scholars
   Special Project under Grant YNWR-YLXZ-2018-022, and in part by the
   Yunnan Provincial Science and Technology Department- Yunnan University
   "Double First Class" Construction Joint Fund Project under Grant
   2019FY003012 Theassociate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Ramanathan
   Subramanian.
CR Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Das S, 2012, MED BIOL ENG COMPUT, V50, P1105, DOI 10.1007/s11517-012-0943-3
   Deng C, 2020, IEEE T MULTIMEDIA, V22, P885, DOI 10.1109/TMM.2019.2934833
   Diwakar M, 2021, MATER TODAY-PROC, V37, P3411, DOI 10.1016/j.matpr.2020.09.278
   Dong ZK, 2018, NEUROCOMPUTING, V308, P172, DOI 10.1016/j.neucom.2018.04.066
   Eckart C, 1936, PSYCHOMETRIKA, V1, P211, DOI 10.1007/BF02288367
   Gao J, 2020, NEURAL COMPUT, V32, P829, DOI 10.1162/neco_a_01273
   Gong YH, 2019, SIGNAL PROCESS, V164, P329, DOI 10.1016/j.sigpro.2019.06.020
   Guo Q, 2016, IEEE T CIRC SYST VID, V26, P868, DOI 10.1109/TCSVT.2015.2416631
   Guo XJ, 2020, IEEE T PATTERN ANAL, V42, P694, DOI 10.1109/TPAMI.2018.2883553
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He KJ, 2019, SOFT COMPUT, V23, P4685, DOI 10.1007/s00500-018-3118-9
   He KJ, 2018, NEUROCOMPUTING, V320, P157, DOI 10.1016/j.neucom.2018.09.018
   He KJ, 2017, J APPL REMOTE SENS, V11, DOI 10.1117/1.JRS.11.015011
   Hermessi H, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.108036
   Hossny M, 2008, ELECTRON LETT, V44, P1066, DOI 10.1049/el:20081754
   Jagalingam P, 2015, AQUAT PR, V4, P133, DOI 10.1016/j.aqpro.2015.02.019
   James AP, 2014, INFORM FUSION, V19, P4, DOI 10.1016/j.inffus.2013.12.002
   Jin X, 2018, SOFT COMPUT, V22, P6395, DOI 10.1007/s00500-017-2694-4
   Kong WW, 2021, SIGNAL PROCESS, V181, DOI 10.1016/j.sigpro.2020.107921
   Kumar A, 2020, IEEE T MED IMAGING, V39, P204, DOI 10.1109/TMI.2019.2923601
   Li HG, 2019, IEEE SENS J, V19, P9755, DOI 10.1109/JSEN.2019.2928818
   Li LG, 2020, INT J EMBED SYST, V12, P1, DOI 10.1504/IJES.2020.105294
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li S, 2013, INFORM FUSION, V14, P147, DOI 10.1016/j.inffus.2011.07.001
   Li XS, 2021, INFORM SCIENCES, V569, P302, DOI 10.1016/j.ins.2021.04.052
   Li XX, 2020, IEEE T INSTRUM MEAS, V69, P6880, DOI 10.1109/TIM.2020.2975405
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Liu RS, 2021, IEEE T IMAGE PROCESS, V30, P1261, DOI 10.1109/TIP.2020.3043125
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   med, The Whole Brain Atlas
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Nie RC, 2022, IEEE T MULTIMEDIA, V24, P1460, DOI 10.1109/TMM.2021.3065496
   Ning ZY, 2021, IEEE T MED IMAGING, V40, P1632, DOI 10.1109/TMI.2021.3063150
   Ochotorena CN, 2020, IEEE T IMAGE PROCESS, V29, P1397, DOI 10.1109/TIP.2019.2941326
   Rajwade A, 2013, IEEE T PATTERN ANAL, V35, P849, DOI 10.1109/TPAMI.2012.140
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shi MW, 2021, COMPUT VIS IMAGE UND, V206, DOI 10.1016/j.cviu.2021.103173
   Shreyamsha Kumar BK, 2015, SIGNAL IMAGE VIDEO P, V9, P1193, DOI 10.1007/s11760-013-0556-9
   Tan W, 2020, NEURAL COMPUT APPL, DOI 10.1007/s00521-020-05173-2
   Tao DP, 2019, IEEE T NEUR NET LEAR, V30, P163, DOI 10.1109/TNNLS.2018.2836969
   TAYLOR JE, 1992, ACTA METALL MATER, V40, P1475, DOI 10.1016/0956-7151(92)90091-R
   Wang B, 2021, IEEE T MULTIMEDIA, V23, P3137, DOI 10.1109/TMM.2020.3020695
   Wang YY, 2020, IEEE T ULTRASON FERR, V67, P2363, DOI 10.1109/TUFFC.2020.3005426
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu WY, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107424
   Xu HY, 2022, IEEE T MULTIMEDIA, V24, P3738, DOI 10.1109/TMM.2021.3106789
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yin M, 2019, IEEE T INSTRUM MEAS, V68, P49, DOI 10.1109/TIM.2018.2838778
   Zeng M, 2019, IEEE SENS J, V19, P6335, DOI 10.1109/JSEN.2019.2910868
   Zhao F, 2021, IEEE T MULTIMEDIA, V23, P2745, DOI 10.1109/TMM.2020.3016123
   Zhu SJ, 2020, IET IMAGE PROCESS, V14, P2561, DOI 10.1049/iet-ipr.2019.1471
   Zhu S, 2017, NEUROCOMPUTING, V227, P149, DOI 10.1016/j.neucom.2016.07.068
NR 56
TC 7
Z9 7
U1 10
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4943
EP 4957
DI 10.1109/TMM.2022.3185887
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300025
DA 2024-07-18
ER

PT J
AU Hu, YF
   Gao, JY
   Xu, CS
AF Hu, Yufan
   Gao, Junyu
   Xu, Changsheng
TI Learning Scene-Aware Spatio-Temporal GNNs for Few-Shot Early Action
   Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot learning; early action prediction; scene graph; graph neural
   network
ID OBJECT AFFORDANCES
AB We aim to address a new task named few-shot early action prediction (FS-EAP) that learns classifiers for novel actions from only a few partially observed videos. We argue that the task is extremely challenging since the partially observed videos do not contain enough action information in a few-shot environment. To tackle this task, in this paper, we propose a scene-aware spatio-temporal graph neural network (SA-STGNN) by leveraging the fine-grained spatio-temporal interactions in the video scenes. Specifically, we first generate a spatio-temporal graph corresponding to the partially observed video to capture comprehensive spatio-temporal correlations. Then we utilize the spatio-temporal graph as the input of our SA-STGNN and predict the augmented video features corresponding to the complete video. The architecture uses several scene-aware learning blocks, which are a combination of edge fusion graph neural layers and temporal gated convolutional layers to jointly model spatial and temporal dependencies. Finally, we employ an early action predictor to exploit the learned video features for predicting actions in the few-shot setting. Extensive experimental results on two widely adopted video datasets demonstrate the effectiveness of our approach and its superior performance over the state-of-the-art approaches.
C1 [Hu, Yufan] Hefei Univ Technol, Hefei 230009, Peoples R China.
   [Hu, Yufan; Xu, Changsheng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Gao, Junyu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Gao, Junyu; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 Hefei University of Technology; Peng Cheng Laboratory; Chinese Academy
   of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM huyufanqaixuan@gmail.com; junyu.gao@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Gao, Junyu/HDO-5516-2022
OI xu, chang sheng/0000-0001-8343-9665; Gao, Junyu/0000-0002-8105-5497
FU National Key Research & Development Plan of China [2020AAA0106200];
   National Natural Science Foundation of China [62036012, 61721004,
   62102415, 62072286, 61720106006, 61832002, 62072455, 62002355, U1836220,
   U1705262]; Key Research Program of Frontier Sciences of CAS
   [QYZDJSS-WJSC039]; Beijing Natural Science Foundation [L201001];
   CCF-Hikvision Open Fund
FX This work was supported in part by the National Key Research &
   Development Plan of China under Grant 2020AAA0106200, in part by the
   National Natural Science Foundation of China under Grants 62036012,
   61721004, 62102415, 62072286, 61720106006, 61832002, 62072455, 62002355,
   U1836220, and U1705262, in part by the Key Research Program of Frontier
   Sciences of CAS under Grant QYZDJSS-WJSC039, in part by the Beijing
   Natural Science Foundation under Grant L201001, and in part by
   CCF-Hikvision Open Fund. The Associate Editor co-ordinating the review
   of this manuscript and approving it for publication was Dr. Vasileios
   Mezaris.
CR Abu Farha Y, 2018, PROC CVPR IEEE, P5343, DOI 10.1109/CVPR.2018.00560
   Aliakbarian MS, 2017, IEEE I CONF COMP VIS, P280, DOI 10.1109/ICCV.2017.39
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Baldassarre Federico, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P612, DOI 10.1007/978-3-030-58604-1_37
   Cheng M, 2022, IEEE T CIRC SYST VID, V32, P2158, DOI 10.1109/TCSVT.2021.3088545
   Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222
   Dauphin Y. N., 2015, P 4 INT C LEARN REPR
   Dauphin YN, 2017, PR MACH LEARN RES, V70
   Defferrard M, 2016, ADV NEUR IN, V29
   Dwivedi SK, 2019, IEEE INT CONF COMP V, P1308, DOI 10.1109/ICCVW.2019.00166
   Estrach J. B., 2014, PROC 2 INT C LEARN R
   Finn C, 2017, PR MACH LEARN RES, V70
   Gao JY, 2021, IEEE T MULTIMEDIA, V23, P3203, DOI 10.1109/TMM.2020.3021980
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Gardner MW, 1998, ATMOS ENVIRON, V32, P2627, DOI 10.1016/S1352-2310(97)00447-0
   Gori M, 2005, IEEE IJCNN, P729
   Hamilton WL, 2017, ADV NEUR IN, V30
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu JF, 2019, IEEE T PATTERN ANAL, V41, P2568, DOI 10.1109/TPAMI.2018.2863279
   Hu YF, 2021, IEEE T MULTIMEDIA, V23, P4285, DOI 10.1109/TMM.2020.3039329
   Huang YF, 2019, IEEE I CONF COMP VIS, P6281, DOI 10.1109/ICCV.2019.00637
   Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kingma D. P., 2014, arXiv
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Kong Y, 2017, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2017.390
   Kong Y, 2016, IEEE T PATTERN ANAL, V38, P1844, DOI 10.1109/TPAMI.2015.2491928
   Kong Y, 2014, LECT NOTES COMPUT SC, V8693, P596, DOI 10.1007/978-3-319-10602-1_39
   Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335
   Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lan T, 2014, LECT NOTES COMPUT SC, V8691, P689, DOI 10.1007/978-3-319-10578-9_45
   Li Y., 2016, P 4 INT C LEARNING R
   Liu Y, 2016, IEEE T MULTIMEDIA, V18, P351, DOI 10.1109/TMM.2016.2514848
   Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214
   Mohamed Abduallah, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14412, DOI 10.1109/CVPR42600.2020.01443
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Pang GL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P897
   Paszke Adam, 2017, NIPS 2017 WORKSH AUT
   Perez-Rua Juan-Manuel, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13843, DOI 10.1109/CVPR42600.2020.01386
   Perrett T, 2021, PROC CVPR IEEE, P475, DOI 10.1109/CVPR46437.2021.00054
   Ravi S., 2016, INT C LEARN REPR ICL
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349
   Ryoo MS, 2010, LECT NOTES COMPUT SC, V6388, P270, DOI 10.1007/978-3-642-17711-8_28
   Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361
   Santoro A, 2016, PR MACH LEARN RES, V48
   Satorras V.G., 2018, P INT C LEARN REPR V
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shi YG, 2018, LECT NOTES COMPUT SC, V11214, P305, DOI 10.1007/978-3-030-01249-6_19
   Snell J, 2017, ADV NEUR IN, V30
   Soomro K, 2019, IEEE T PATTERN ANAL, V41, P459, DOI 10.1109/TPAMI.2018.2797266
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Velickovic P, 2018, PROC INT C LEARN REP
   Vicol P, 2018, PROC CVPR IEEE, P8581, DOI 10.1109/CVPR.2018.00895
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XH, 2019, PROC CVPR IEEE, P3551, DOI 10.1109/CVPR.2019.00367
   Xiong Y, 2019, IEEE I CONF COMP VIS, P4591, DOI 10.1109/ICCV.2019.00469
   Xu B., 2019, PROC 7 INT C LEARN R
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Ying R, 2018, ADV NEUR IN, V31
   Yu B, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3634
   Yu G., 2012, Acm international conference on multimedia, P1049
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhu LC, 2018, LECT NOTES COMPUT SC, V11211, P782, DOI 10.1007/978-3-030-01234-2_46
NR 72
TC 5
Z9 5
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2061
EP 2073
DI 10.1109/TMM.2022.3142413
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100037
DA 2024-07-18
ER

PT J
AU Li, JF
   Li, YP
   Zhuo, L
   Kuang, LY
   Yu, TJ
AF Li, Jiafeng
   Li, Yaopeng
   Zhuo, Li
   Kuang, Lingyan
   Yu, Tianjian
TI USID-Net: Unsupervised Single Image Dehazing Network via Disentangled
   Representations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Disentangled representations; end-to-end; single image dehazing;
   unsupervised learning
AB Captured images of outdoor scenes usually exhibit low visibility in cases of severe haze, which interferes with optical imaging and degrades image quality. Most of the existing methods solve the single-image dehazing problem by applying supervised training on paired images; however, in practice, the pairing of real-world images is not viable. Additionally, the processing speed of individual dehazing models is important in practical applications. In this study, a novel unsupervised single image dehazing network (USID-Net) based on disentangled representations without paired training images is explored. Furthermore, considering the trade-off between performance and memory storage, a compact multi-scale feature attention (MFA) module is developed, integrating multi-scale feature representation and attention mechanism to facilitate feature representation. To effectively extract haze information, a mechanism referred to as OctEncoder is designed to include multi-frequency representations that can capture more global information. Extensive experiments show that USID-Net achieves competitive dehazing results and a relatively high processing speed compared to state-of-the-art methods. The source code is available at https://github.com/dehazing/USID-Net.
C1 [Li, Jiafeng; Li, Yaopeng; Zhuo, Li; Kuang, Lingyan] Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intelligen, Beijing 100124, Peoples R China.
   [Yu, Tianjian] Cent South Univ, Sch Traff & Transportat Engn, Changsha 410075, Hunan, Peoples R China.
C3 Beijing University of Technology; Central South University
RP Li, JF (corresponding author), Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intelligen, Beijing 100124, Peoples R China.
EM lijiafeng@bjut.edu.cn; lyp@emails.bjut.edu.cn; zhuoli@bjut.edu.cn;
   kuangly@emails.bjut.edu.cn; yutianjian@csu.edu.cn
RI li, jiafeng/KVY-4468-2024
OI Li, Jiafeng/0000-0001-6976-7275
FU Beijing Natural Science Foundation; General Program of Beijing Municipal
   Education Commission [L211017]; National Natural Science Foundation of
   China [KM202110005027, 61971016]; Beijing Municipal Education Commission
   Cooperation Beijing Natural Science Foundation [61701011]; 
   [KZ201910005077]
FX This work was supported in part by Beijing Natural Science Foundation
   under Grant L211017, in part by the General Program of Beijing Municipal
   Education Commission under Grant KM202110005027, in part by the National
   Natural Science Foundation of China under Grants 61971016 and 61701011,
   and in part by Beijing Municipal Education Commission Cooperation
   Beijing Natural Science Foundation under Grant KZ201910005077.& nbsp;
CR [Anonymous], 1976, Optics of the Atmosphere: Scattering by Molecules and Particles
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dudhane A, 2020, PROC CVPR IEEE, P4563, DOI 10.1109/CVPR42600.2020.00462
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gou Yuanbiao, 2020, P ADV NEUR INF PROC, V33, P17129, DOI DOI 10.5555/3495724.3497161
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu HM, 2020, IEEE T MULTIMEDIA, V22, P1485, DOI 10.1109/TMM.2019.2944260
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Ju MY, 2020, IEEE T IMAGE PROCESS, V29, P3104, DOI 10.1109/TIP.2019.2957852
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li BY, 2020, IEEE T IMAGE PROCESS, V29, P8457, DOI 10.1109/TIP.2020.3016134
   Li CY, 2020, IEEE T MULTIMEDIA, V22, P704, DOI 10.1109/TMM.2019.2933334
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Liu MY, 2017, ADV NEUR IN, V30
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wenchao Du, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14471, DOI 10.1109/CVPR42600.2020.01449
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhao SY, 2020, IEEE T IMAGE PROCESS, V29, P6947, DOI 10.1109/TIP.2020.2995264
   Zhu HY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1234
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 44
TC 30
Z9 30
U1 18
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3587
EP 3601
DI 10.1109/TMM.2022.3163554
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA S2QI4
UT WOS:001069663600001
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Li, M
   Liu, J
   Zheng, C
   Huang, XM
   Zhang, ZM
AF Li, Ming
   Liu, Jun
   Zheng, Ce
   Huang, Xinming
   Zhang, Ziming
TI Exploiting Multi-View Part-Wise Correlation via an Efficient Transformer
   for Vehicle Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Correlation; Feature extraction; Visualization; Training;
   Benchmark testing; Task analysis; Correlation exploiting; multi-view
   learning; transformer; vehicle re-identification
AB Image-based vehicle re-identification (ReID) has witnessed much progress in recent years. However, most of existing works struggled to extract robust but discriminative features from a single image to represent one vehicle instance. We argue that images taken from distinct viewpoints, e.g., front and back, have significantly different appearances and patterns for recognition. In order to identify each vehicle, these models have to capture consistent "ID codes " from totally different views, causing learning difficulties. Additionally, we claim that part-level correspondences among views, i.e., various vehicle parts observed from the identical image and the same part visible from different viewpoints, contribute to instance-level feature learning as well. Motivated by these, we propose to extract comprehensive vehicle instance representations from multiple views through modelling part-wise correlations. To this end, we present our efficient transformer-based framework to exploit both inner- and inter-view correlations for vehicle ReID. In specific, we first adopt a convnet encoder to condense a series of patch embeddings from each view. Then our efficient transformer, consisting of a distillation token and a noise token in addition to a regular classification token, is constructed for enforcing these patch embeddings to interact with each other regardless of whether they are taken from identical or different views. We conduct extensive experiments on widely used vehicle ReID benchmarks, and our approach achieves the state-of-the-art performance, showing the effectiveness of our method.
C1 [Li, Ming] Natl Univ Singapore, Inst Data Sci, Singapore 119077, Singapore.
   [Liu, Jun] Singapore Univ Technol & Design, Informat Syst Technol & Design, Singapore 487372, Singapore.
   [Zheng, Ce] Univ Cent Florida, Dept Comp Sci, Orlando, FL 32816 USA.
   [Huang, Xinming; Zhang, Ziming] Worcester Polytech Inst, Dept Elect & Comp Engn, Worcester, MA 01609 USA.
C3 National University of Singapore; Singapore University of Technology &
   Design; State University System of Florida; University of Central
   Florida; Worcester Polytechnic Institute
RP Zhang, ZM (corresponding author), Worcester Polytech Inst, Dept Elect & Comp Engn, Worcester, MA 01609 USA.
EM ming.li@u.nus.edu; jun_liu@sutd.edu.sg; cezheng@knights.ucf.edu;
   xhuang@wpi.edu; zzhang15@wpi.edu
RI Zheng, Ce/IXX-2617-2023; Zheng, Ce/AEF-2521-2022
OI Zheng, Ce/0000-0002-9033-0622; Liu, Jun/0000-0002-4365-4165; Li,
   Ming/0000-0002-7852-0159
FU NSF under Grant [CCF-2006738]
FX This work was supported by NSF under Grant CCF-2006738. Theassociate
   editor coordinating the review of this manuscript and approving it
   forpublication was Prof. Ling-Yu Duan.(
CR Alfasly SAS, 2019, IEEE IMAGE PROC, P3118, DOI [10.1109/icip.2019.8803366, 10.1109/ICIP.2019.8803366]
   Bai Y, 2022, IEEE T PATTERN ANAL, V44, P6854, DOI 10.1109/TPAMI.2021.3099253
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Chen GY, 2019, IEEE I CONF COMP VIS, P9546, DOI 10.1109/ICCV.2019.00964
   Chen TS, 2020, Arxiv, DOI arXiv:2008.11423
   Chu RH, 2019, IEEE I CONF COMP VIS, P8281, DOI 10.1109/ICCV.2019.00837
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Ge Y., 2020, P ADV NEUR INF PROC, V33, p11 309
   Girshick Ross, 2018, Detectron
   Gulcehre C, 2016, PR MACH LEARN RES, V48
   Guo HY, 2019, IEEE T IMAGE PROCESS, V28, P4328, DOI 10.1109/TIP.2019.2910408
   Guo HY, 2018, AAAI CONF ARTIF INTE, P6853
   Guo MH, 2021, Arxiv, DOI arXiv:2012.09688
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, Arxiv, DOI arXiv:2102.04378
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Jin X, 2020, AAAI CONF ARTIF INTE, V34, P11165
   Khorramshahi P, 2020, Arxiv, DOI arXiv:2004.06271
   Khorramshahi P, 2019, IEEE I CONF COMP VIS, P6131, DOI 10.1109/ICCV.2019.00623
   Kingma D. P., 2014, arXiv
   Kumar R, 2019, ANNU IEEE IND CONF, DOI 10.1109/indicon47234.2019.9030349
   Li M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P194, DOI 10.1109/ICCV48922.2021.00026
   Lin XM, 2021, IEEE T MULTIMEDIA, V23, P3968, DOI 10.1109/TMM.2020.3035279
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu XB, 2018, IEEE INT CON MULTI
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Lou YH, 2019, IEEE T IMAGE PROCESS, V28, P3794, DOI 10.1109/TIP.2019.2902112
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Meng DC, 2020, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR42600.2020.00713
   Neelakantan A, 2015, Arxiv, DOI arXiv:1511.06807
   Qian JJ, 2020, MEAS SCI TECHNOL, V31, DOI 10.1088/1361-6501/ab8b81
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Shen YT, 2017, IEEE I CONF COMP VIS, P1918, DOI 10.1109/ICCV.2017.210
   Solomon E, 2021, PR MACH LEARN RES, V130, P109
   Srinivas A, 2021, Arxiv, DOI [arXiv:2101.11605, DOI 10.48550/ARXIV.2101.11605]
   Sun Z., 2020, Rethinking Transformer-based Set Prediction for Object Detection
   Sun ZR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3523, DOI 10.1145/3394171.3413541
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang Z, 2019, IEEE I CONF COMP VIS, P211, DOI 10.1109/ICCV.2019.00030
   Tang Z, 2019, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2019.00900
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2021, Arxiv, DOI arXiv:2102.12122
   Wang Z., 2017, PROC INT C COMPUT VI, P3794, DOI [10.1109/TIP.2019.2902112, DOI 10.1109/TIP.2019.2902112]
   Wen Y., 2019, An empirical study of large-batch stochastic gradient descent with structured covariance noise
   Wu FY, 2018, INT C PATT RECOG, P278, DOI 10.1109/ICPR.2018.8545584
   Xiao L, 2021, Arxiv, DOI arXiv:2102.04450
   Yan K, 2017, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2017.68
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Yuan L, 2021, Arxiv, DOI arXiv:2101.11986
   Zhang XY, 2022, IEEE T INTELL TRANSP, V23, P3048, DOI 10.1109/TITS.2020.3030301
   Zhang YH, 2017, IEEE INT CON MULTI, P1386, DOI 10.1109/ICME.2017.8019491
   Zhang YD, 2021, Arxiv, DOI arXiv:2102.08005
   Zheng SX, 2021, Arxiv, DOI arXiv:2012.15840
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou HY, 2021, Arxiv, DOI arXiv:2012.07436
   Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679
NR 62
TC 15
Z9 16
U1 3
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 919
EP 929
DI 10.1109/TMM.2021.3134839
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900019
DA 2024-07-18
ER

PT J
AU Li, WH
   Huang, JW
   Wang, SQ
   Wu, CL
   Liu, S
   Wang, JX
AF Li, Weihe
   Huang, Jiawei
   Wang, Shiqi
   Wu, Chuliang
   Liu, Sen
   Wang, Jianxin
TI An Apprenticeship Learning Approach for Adaptive Video Streaming Based
   on Chunk Quality and User Preference
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Chunk quality; DASH; dynamic chunk; user preference; video streaming
ID ADAPTATION; EXPERIENCE; DASH
AB Video traffic has experienced an exponential increase in current years due to the growing ubiquity of mobile equipment and the constant network improvement. Most commercial players employ adaptive bitrate (ABR) algorithms to dynamically choose bitrate for each chunk based on perceived network capacity and buffer occupancy. Unluckily, even though improving the quality of chunks with dynamic scenes can achieve more QoE gain than static scenes, current ABR algorithms usually strive to maximize the average bitrate instead of perceptual quality, leading to the QoE degradation. To overcome this obstacle, we introduce a dynamic-chunk quality-aware adaptive bitrate algorithm through apprenticeship learning called DAVS (Dynamic-chunk quality Aware Video Streaming), where higher quality is selected for the dynamic chunks without reducing the quality of static chunks extravagantly. Furthermore, we take the user's viewing preference into account to make DAVS adapt to the QoE diversity. The experimental results demonstrate that DAVS ameliorates the quality of dynamic chunks and significantly enhances the QoE compared with several representative ABR algorithms.
C1 [Li, Weihe] Univ Edinburgh, Sch Informat, Edinburgh EH8 9YL, Scotland.
   [Li, Weihe; Huang, Jiawei; Wang, Shiqi; Wu, Chuliang; Wang, Jianxin] Cent South Univ, Sch Comp Sci & Engn, Changsha 410017, Peoples R China.
   [Liu, Sen] Fudan Univ, Sch Comp Sci, Shanghai 200437, Peoples R China.
C3 University of Edinburgh; Central South University; Fudan University
RP Huang, JW (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha 410017, Peoples R China.
EM weiheli@csu.edu.cn; jiaweihuang@csu.edu.cn; shiqi_wang@csu.edu.cn;
   204712275@csu.edu.cn; senliu@fudan.edu.cn; jxwang@csu.edu.cn
RI Wang, Shiqi/AAR-5013-2020; Wang, Jianxin/V-2800-2018
OI Wang, Shiqi/0000-0002-6338-1432; Wang, Jianxin/0000-0003-1516-0480;
   Huang, Jiawei/0000-0002-7578-4490; Liu, Sen/0000-0003-2230-7671
FU National Natural Science Foundation of China [62132022, 61872387]; Key
   Research and Development Program of Hunan [2022WK2005]; Natural Science
   Foundation of Hunan Province, China [2021JJ30867]; Project of Foreign
   Cultural and Educational Expert [G20190018003]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62132022 and 61872387, in part by the
   Key Research and Development Program of Hunan under Grant 2022WK2005, in
   part by the Natural Science Foundation of Hunan Province, China under
   Grant 2021JJ30867, and in part by the Project of Foreign Cultural and
   Educational Expert under Grant G20190018003. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Zhi Wang.& nbsp;
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Abbeel P., 2004, ICML 04, P1
   [Anonymous], 2017, White Paper
   [Anonymous], 2016, RAW DAT MEAS BROADB
   [Anonymous], 2013, P IEEE 20 INT PACK V
   [Anonymous], 1993, BT50012 ITU
   Batalla JM, 2016, IEEE J SEL AREA COMM, V34, P2154, DOI 10.1109/JSAC.2016.2577360
   Bentaleb A, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P13, DOI 10.1145/3204949.3204961
   Bonnineau C, 2022, IEEE T BROADCAST, V68, P246, DOI 10.1109/TBC.2022.3140710
   Bruneau-Queyreix J, 2017, IEEE INT CON MULTI, P1, DOI 10.1109/ICME.2017.8019378
   Bruneau-Queyreix J, 2018, IEEE MULTIMEDIA, V25, P65, DOI 10.1109/MMUL.2018.112142627
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Cover Thomas M, 1999, Elements of information theory
   Da Silva S, 2018, PROCEEDINGS OF THE 23TH ACM WORKSHOP ON PACKET VIDEO (PV'18), P54, DOI 10.1145/3210424.3210432
   Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361
   Duanmu Z., WATERLOO STREAMING Q
   Duanmu ZF, 2018, IEEE T BROADCAST, V64, P474, DOI 10.1109/TBC.2018.2822870
   Eilish B., 2020, NO TIME DIE
   Fang X, 2013, IEEE T KNOWL DATA EN, V25, P2302, DOI 10.1109/TKDE.2012.196
   Gadaleta M, 2017, IEEE T COGN COMMUN, V3, P703, DOI 10.1109/TCCN.2017.2755007
   Gao GY, 2018, IEEE T MULTIMEDIA, V20, P3399, DOI 10.1109/TMM.2018.2838330
   github, PREF DAT
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huang TC, 2022, IEEE T MULTIMEDIA, V24, P1350, DOI 10.1109/TMM.2021.3063620
   Huang TC, 2020, IEEE J SEL AREA COMM, V38, P2324, DOI 10.1109/JSAC.2020.3000363
   Huang TC, 2020, IEEE INFOCOM SER, P1967, DOI [10.1109/infocom41043.2020.9155411, 10.1109/INFOCOM41043.2020.9155411]
   Huang TC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P429, DOI 10.1145/3343031.3351014
   Huang TC, 2019, IEEE INT CON MULTI, P1678, DOI 10.1109/ICME.2019.00289
   Hubert B., TC8 LINUX MAN PAGE
   Jiang J., 2012, P 8 INT C EM NETW EX, P97
   Le Feuvre J., 2007, P 15 ACM INT C MULT, P1009, DOI [10.1145/1291233.1291452, DOI 10.1145/1291233.1291452]
   Li W, 2020, IEEE INT SYM BROADB, DOI 10.1109/BMSB49480.2020.9379821
   Li WH, 2022, IEEE WIREL COMMUN LE, V11, P513, DOI 10.1109/LWC.2021.3134491
   Li Z., Toward a practical perceptual video quality metric
   Li Z., 2014, Proceedings of the 5th ACM Multimedia Systems Conference, MMSys '14, P248
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Miller K, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2990505
   Mok R.K., 2011, PROC ACM SIGCOMM WOR, P31, DOI DOI 10.1145/2018602.2018611
   Narayanan A, 2021, SIGCOMM '21: PROCEEDINGS OF THE 2021 ACM SIGCOMM 2021 CONFERENCE, P610, DOI 10.1145/3452296.3472923
   Netflix Inc, 2015, PER TITL ENC OPT
   Qiao CY, 2021, IEEE ACM T NETWORK, V29, P289, DOI 10.1109/TNET.2020.3032416
   Qin YY, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P189, DOI 10.1145/3304109.3306231
   Qin YY, 2018, CONEXT'18: PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES, P366, DOI 10.1145/3281411.3281439
   Rached Z, 2004, IEEE T INFORM THEORY, V50, P917, DOI 10.1109/TIT.2004.826687
   Rao RRR, 2019, IEEE INT SYM MULTIM, P17, DOI [10.1109/ism46123.2019.00012, 10.1109/ISM46123.2019.00012]
   Ravi Netravali, 2015, 2015 USENIX ANN TECH, P417
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Ross S, 2011, P 14 INT C ART INT S, P627
   Schulman J, 2017, Arxiv, DOI arXiv:1707.06347
   Sengupta S, 2018, I C NETWORK PROTOCOL, P165, DOI 10.1109/ICNP.2018.00026
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P123, DOI 10.1145/3204949.3204953
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Stanford University, PRINC ROB AUT IM LEA
   tsinghua, V QUAL DAT
   Wang B, 2021, IEEE T MOBILE COMPUT, V20, P174, DOI 10.1109/TMC.2019.2939124
   Wang C, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P1, DOI 10.1145/2910017.2910593
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wisniewski P, 2015, IEEE ICC, P6867, DOI 10.1109/ICC.2015.7249420
   Yadav PK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1130, DOI 10.1145/3123266.3123390
   Yan FY, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P495
   Yanyuan Qin, 2017, INFOCOM 2017 - IEEE Conference on Computer Communications, DOI 10.1109/INFOCOM.2017.8057056
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Yu S, 2020, IEEE WIREL COMMUN, V27, P92, DOI 10.1109/MWC.001.1900232
   Yuan T., 2017, TF.Learn: TensorFlow's high-level module for distributed machine learning
   Zhan C, 2020, IEEE T MULTIMEDIA, V22, P795, DOI 10.1109/TMM.2019.2931441
   Zhou C, 2019, IEEE T CIRC SYST VID, V29, P198, DOI 10.1109/TCSVT.2017.2771246
NR 68
TC 9
Z9 9
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2488
EP 2502
DI 10.1109/TMM.2022.3147667
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600003
DA 2024-07-18
ER

PT J
AU Li, WH
   Wang, Y
   Su, YT
   Li, XY
   Liu, AA
   Zhang, YD
AF Li, Wenhui
   Wang, Yan
   Su, Yuting
   Li, Xuanya
   Liu, An-An
   Zhang, Yongdong
TI Multi-Scale Fine-Grained Alignments for Image and Sentence Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Visualization; Dogs; Mouth; Task analysis; Feature
   extraction; Bridges; Bi-directional aggregations; image and sentence
   matching; multi-scale alignments
AB Image and sentence matching is a critical task to bridge the visual and textual discrepancy due to the heterogeneous modalities. Great progress has been made by exploring the coarse-grained relationships between images and sentences or fine-grained relationships between regions and words. However, how to fully excavate and exploit corresponding relations between these two modalities is still challenging. In this work, we propose a novel Multi-scale Fine-grained Alignments Network (MFA), which can effectively explore multi-scale visual-textual correspondences to facilitate bridging cross-modal discrepancy. Specifically, word-scale matching module is firstly utilized to mine the basic but fundamental correspondences between a single word and independent region. Then, we propose a phrase-scale matching module to explore the relations between objects with the constraint of attribute and corresponding region, which can further reserve more associated information. To cope with the complex interactions among multiple phrases and images, we design the relation-scale matching module to capture high-order semantics between two modalities. Moreover, each matching module includes visual aggregation and textual aggregations, which can ensure the bi-directional coupling of multi-scale semantics. Extensive qualitative and quantitative experiments on two challenging datasets including Flickr30 K and MSCOCO, show that the proposed method achieves superior performance compared with the existing methods.
C1 [Li, Wenhui; Wang, Yan; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Liu, An-An] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230000, Peoples R China.
   [Liu, An-An] Chinese Acad Sci, Key Lab Electromagnet SpaceInformat, Beijing 100000, Peoples R China.
   [Li, Xuanya] Baidu Inc, Beijing 100000, Peoples R China.
   [Zhang, Yongdong] Univ Sci & Technol China, Hefei 230027, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; Baidu; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS
RP Wang, Y; Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Liu, AA (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230000, Peoples R China.
EM liwenhui@tju.edu.cn; wyleo7@tju.edu.cn; ytsu@tju.edu.cn;
   lixuanya@baidu.com; anan0422@gmail.com; zhyd73@ustc.edu.cn
RI wu, meng/JPK-1930-2023; LU, lpp pp/JFJ-9011-2023; Lu,
   Wang/JVO-0416-2024; Zeng, Yun/JFK-6190-2023; LI, Wenhui/JCD-9947-2023
FU National Key Research and Development Program of China [2020YFB1709201];
   National Nature Science Foundation of China [61772359, 61872267]; 2019
   Tianjin New Generation Artificial Intelligence Major Program
   [19ZXZNGX00110]; Open Funding Project of the Key Laboratory of
   Electromagnetic Space Information, Chinese Academy of Sciences
   (Theoretical Research on Video Semantic Analysis for Content Se-curity
   of Social Networks); Baidu Pinecone Program
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB1709201, in part by the
   National Nature Science Foundation of China under Grants 61772359 and
   61872267, in part by the grant of 2019 Tianjin New Generation Artificial
   Intelligence Major Program (19ZXZNGX00110), in part by the Open Funding
   Project of the Key Laboratory of Electromagnetic Space Information,
   Chinese Academy of Sciences (Theoretical Research on Video Semantic
   Analysis for Content Se-curity of Social Networks), and in part by the
   Baidu Pinecone Program. The Associate editor Coordinating the review of
   this manuscript and approving it for publication was Prof. Lamberto
   Ballan. (Corresponding authors: Yan Wang; An-An Liu.)
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bhunia Ayan Kumar, 2021, ICCV, P14940
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Fan HQ, 2018, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2018.00118
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang FR, 2019, IEEE T IMAGE PROCESS, V28, P2008, DOI 10.1109/TIP.2018.2882225
   Huang X, 2021, IEEE T CYBERNETICS, V51, P5692, DOI 10.1109/TCYB.2019.2956975
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Huang Y, 2020, IEEE T PATTERN ANAL, V42, P636, DOI 10.1109/TPAMI.2018.2883466
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Z, 2019, IEEE I CONF COMP VIS, P5753, DOI 10.1109/ICCV.2019.00585
   Jian YW, 2019, IEEE INT CON MULTI, P1810, DOI 10.1109/ICME.2019.00311
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li CC, 2020, IEEE T MULTIMEDIA, V22, P1634, DOI 10.1109/TMM.2019.2946477
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li S, 2017, IEEE I CONF COMP VIS, P1908, DOI 10.1109/ICCV.2017.209
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C, 2020, P IEEECVF C COMPUTER, p10 918
   Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869
   Liu TL, 2020, IEEE T MULTIMEDIA, V22, P1098, DOI 10.1109/TMM.2019.2936805
   Liu XC, 2019, PROC CVPR IEEE, P3561, DOI 10.1109/CVPR.2019.00368
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Luo JY, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P2309, DOI 10.1145/3357384.3358104
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Niu ZX, 2017, IEEE I CONF COMP VIS, P1899, DOI 10.1109/ICCV.2017.208
   Peng YX, 2021, INT J COMPUT VISION, V129, P921, DOI 10.1007/s11263-020-01392-1
   Peng YX, 2020, IEEE T CIRC SYST VID, V30, P4368, DOI 10.1109/TCSVT.2019.2953692
   Peng YX, 2020, IEEE T IMAGE PROCESS, V29, P2728, DOI 10.1109/TIP.2019.2952085
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Quan Cui, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P189, DOI 10.1007/978-3-030-58580-8_12
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Wang JP, 2021, AAAI CONF ARTIF INTE, V35, P10129
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang Q, 2020, IEEE T IMAGE PROCESS, V29, P7549, DOI 10.1109/TIP.2020.3004249
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wehrmann P, 2020, AAAI CONF ARTIF INTE, V34, P12313
   Wu H, 2019, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2019.00677
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Yongzhi Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12783, DOI 10.1109/CVPR42600.2020.01280
   You QZ, 2018, PROC CVPR IEEE, P5735, DOI 10.1109/CVPR.2018.00601
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Zhang LL, 2020, IEEE T MULTIMEDIA, V22, P775, DOI 10.1109/TMM.2019.2931352
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhao SC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P192, DOI 10.1145/3343031.3351062
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
   Zhou PL, 2020, IEEE T MULTIMEDIA, V22, P2684, DOI 10.1109/TMM.2019.2960594
   Zhou YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1245, DOI 10.1145/3394171.3413998
NR 65
TC 30
Z9 30
U1 14
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 543
EP 556
DI 10.1109/TMM.2021.3128744
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800016
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Li, XP
   Guo, XJ
AF Li, Xiaopeng
   Guo, Xiaojie
TI SPN2D-GAN: Semantic Prior Based Night-to-Day Image-to-Image Translation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image-to-image/night-to-day translation; generative adversarial network;
   semantic segmentation; weakly-supervised learning.
AB Existing image-to-image translation approaches can deal with simple scenes or styles effectively, such as summer-to-winter, horses-to-zebra, and photo-to-map. Although a great progress has been made by GAN-based methods recently, the performance of night-to-day (N2D) translation remains unsatisfactory due to imbalanced/poor visibility, and thus leading to translation ambiguity. To improve the quality of N2D translation, we propose an unpaired translation scheme based on a semantic prior generator, namely SPN2D-GAN, in a weakly- supervised manner with consideration of both image and semantic information. Specifically, we design a novel N2D generator, which can adopt the semantic information of images as prior knowledge to generate more reasonable and realistic results. Also, we suggest adjusting the brightness of nighttime images to boost the visibility, so that the generator can better extract content information. Moreover, the proposed SPN2D-GAN translates images by enforcing the distribution of daytime images in both image and semantic domains on final outputs. Besides, the cycle consistency is employed to preserve the fidelity between translations from two directions. Extensive experimental results are provided to reveal the effectiveness of our design, and demonstrate its superior performance over other state-of-the-art N2D translation approaches both quantitatively and qualitatively.
C1 [Li, Xiaopeng; Guo, Xiaojie] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Guo, XJ (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM lxpalliance@tju.edu.cn; xj.max.guo@gmail.com
RI Guo, Xiaojie/AAC-3114-2022
FU National Natural Science Foundation of China [62072327]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62072327.
CR Aakerberg A, 2022, IEEE WINT CONF APPL, P449, DOI 10.1109/WACVW54805.2022.00051
   Anoosheh A, 2019, IEEE INT CONF ROBOT, P5958, DOI [10.1109/ICRA.2019.8794387, 10.1109/icra.2019.8794387]
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Brock A., 2019, INT C LEARN REPR
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen R., 2020, PROC IEEECVF C COMPU, P8165, DOI DOI 10.1109/CVPR42600.2020.00819
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han JL, 2021, IEEE COMPUT SOC CONF, P746, DOI 10.1109/CVPRW53098.2021.00084
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim J., 2020, ICLR, P1
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   Li MJ, 2018, LECT NOTES COMPUT SC, V11213, P186, DOI 10.1007/978-3-030-01240-3_12
   Li Peilun, 2018, P BRIT MACH VIS C, P73
   Li X., 2022, P IEEE INT C MULT EX, P1
   Ma LY, 2023, IEEE T MULTIMEDIA, V25, P930, DOI 10.1109/TMM.2021.3134157
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Miyato T., 2018, INT C LEARN REPR ICL, P1
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Park Taesung, 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Paszke A, 2019, ADV NEUR IN, V32
   Rad MS, 2019, IEEE I CONF COMP VIS, P2710, DOI 10.1109/ICCV.2019.00280
   Radford A., 2015, ARXIV
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178
   Sakaridis C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10745, DOI 10.1109/ICCV48922.2021.01059
   Shaham TR, 2021, PROC CVPR IEEE, P14877, DOI 10.1109/CVPR46437.2021.01464
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang H, 2023, IEEE T NEUR NET LEAR, V34, P1972, DOI 10.1109/TNNLS.2021.3105725
   Tang H, 2019, PROC CVPR IEEE, P2412, DOI 10.1109/CVPR.2019.00252
   Venkatanath N, 2015, NATL CONF COMMUN
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zhou XR, 2021, PROC CVPR IEEE, P11460, DOI 10.1109/CVPR46437.2021.01130
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 52
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7621
EP 7634
DI 10.1109/TMM.2022.3224329
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400001
DA 2024-07-18
ER

PT J
AU Li, XH
   Li, MJ
   Li, XP
   Guo, XJ
AF Li, Xinhui
   Li, Mingjia
   Li, Xiaopeng
   Guo, Xiaojie
TI Learning Generalized Knowledge From a Single Domain on Urban-Scene
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain generalization; Generalized knowledge; Contrastive learning;
   Semantic segmentation
ID ADAPTATION; ALIGNMENT
AB Deep neural networks have made significant progress in various tasks under the assumption of the same distribution between training and testing data. However, the obtained domain-specific knowledge often suffers from performance degradation when facing out-of-distribution data. Towards addressing the degradation, a critical requirement of such networks is the generalization capability to unseen domains, which is the goal of domain generalization (DG). This paper attempts to learn generalized knowledge from a single synthetic domain and then apply it to real and unknown scenarios. Specifically, we propose a contour-aware instance normalization module to effectively learn domain-invariant features via a novel weight-updating strategy, which can largely exploit the generalized information from the observed data. In addition, a category-level contrastive learning mechanism is proposed through understanding the semantic discrepancy and relevance among samples to mitigate the interference of domain-specific features on classification. Extensive experiments together with ablation studies on widely-adopted datasets are conducted to demonstrate the effectiveness of our design and show the superiority of our method over other state-of-the-art schemes on the task of urban-scene segmentation.
C1 [Li, Xinhui; Li, Mingjia; Li, Xiaopeng; Guo, Xiaojie] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Guo, XJ (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM lixinhui@tju.edu.cn; mingjiali@tju.edu.cn; lxpalliance@tju.edu.cn;
   xj.max.guo@gmail.com
RI Guo, Xiaojie/AAC-3114-2022
OI Li, Xinhui/0000-0003-3327-1854
FU National Natural Science Foundation of China [62072327]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62072327.
CR Balaji Y, 2018, ADV NEUR IN, V31
   Chen C, 2019, AAAI CONF ARTIF INTE, P865
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P1042, DOI 10.1109/TMM.2021.3106095
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen W., 2021, ARXIV210211535, P1
   Chen YH, 2018, PROC CVPR IEEE, P7892, DOI 10.1109/CVPR.2018.00823
   Chen YC, 2019, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2019.00189
   Choi S, 2021, PROC CVPR IEEE, P11575, DOI 10.1109/CVPR46437.2021.01141
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Dou Q, 2019, ADV NEUR IN, V32
   Dou Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P691
   Du DP, 2022, INT J COMPUT VISION, V130, P2842, DOI 10.1007/s11263-022-01674-w
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fengchun Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12553, DOI 10.1109/CVPR42600.2020.01257
   Geng B, 2011, IEEE T IMAGE PROCESS, V20, P2980, DOI 10.1109/TIP.2011.2134107
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hong WX, 2018, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2018.00145
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jia J., 2019, P BRIT MACH VIS C
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Khosla Prannay, 2020, ADV NEURAL INFORM PR, V33, P18661
   Kouw WM, 2021, IEEE T PATTERN ANAL, V43, P766, DOI 10.1109/TPAMI.2019.2945942
   Lee S, 2022, PROC CVPR IEEE, P9926, DOI 10.1109/CVPR52688.2022.00970
   Lee S, 2021, AAAI CONF ARTIF INTE, V35, P8306
   Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153
   Li DQ, 2021, PROC CVPR IEEE, P8296, DOI 10.1109/CVPR46437.2021.00820
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Liu QD, 2021, PROC CVPR IEEE, P1013, DOI 10.1109/CVPR46437.2021.00107
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lu YW, 2022, IEEE T MULTIMEDIA, V24, P1871, DOI 10.1109/TMM.2021.3073258
   Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Matsuura T, 2020, AAAI CONF ARTIF INTE, V34, P11749
   Melas-Kyriazi L, 2021, PROC CVPR IEEE, P12430, DOI 10.1109/CVPR46437.2021.01225
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   Nam H, 2018, ADV NEUR IN, V31
   Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534
   Pan F, 2020, PROC CVPR IEEE, P3763, DOI 10.1109/CVPR42600.2020.00382
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Peng D, 2022, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR52688.2022.00262
   Peng D, 2021, IEEE T IMAGE PROCESS, V30, P6594, DOI 10.1109/TIP.2021.3096334
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Prakash A, 2019, IEEE INT CONF ROBOT, P7249, DOI [10.1109/icra.2019.8794443, 10.1109/ICRA.2019.8794443]
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Sakaridis C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10745, DOI 10.1109/ICCV48922.2021.01059
   Shankar S., 2018, INT C LEARN REPR
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Tjio G, 2022, IEEE WINT CONF APPL, P3849, DOI 10.1109/WACV51458.2022.00390
   Tobin J, 2017, IEEE INT C INT ROBOT, P23
   Tsai YH, 2019, IEEE I CONF COMP VIS, P1456, DOI 10.1109/ICCV.2019.00154
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Volpi R, 2018, ADV NEUR IN, V31
   Wang Jindong, 2021, P 30 INT JOINT C ART, P4627, DOI DOI 10.24963/IJCAI.2021/628
   Wang T, 2020, INT C MACH LEARN, P9929, DOI DOI 10.1109/CVPR.2019.00516
   Wang Z., 2021, P IEEE CVF INT C COM, P834
   Wu XY, 2021, PROC CVPR IEEE, P15764, DOI 10.1109/CVPR46437.2021.01551
   Wu ZX, 2018, LECT NOTES COMPUT SC, V11209, P535, DOI 10.1007/978-3-030-01228-1_32
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Yue XY, 2019, IEEE I CONF COMP VIS, P2100, DOI 10.1109/ICCV.2019.00219
   Zhang YX, 2020, AAAI CONF ARTIF INTE, V34, P6877
   Zhou K., 2021, P INT C LEARN REPR
   Zhu RH, 2021, IEEE DATA MINING, P1547, DOI 10.1109/ICDM51629.2021.00203
NR 74
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7635
EP 7646
DI 10.1109/TMM.2022.3224328
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400002
DA 2024-07-18
ER

PT J
AU Liu, GS
   Yue, HJ
   Wu, JM
   Yang, JY
AF Liu, Gaosheng
   Yue, Huanjing
   Wu, Jiamin
   Yang, Jingyu
TI Efficient Light Field Angular Super-Resolution With Sub-Aperture Feature
   Learning and Macro-Pixel Upsampling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Feature extraction; Three-dimensional displays;
   Spatial resolution; Correlation; Cameras; Superresolution; Light field;
   angular super-resolution; view synthesis; deep learning
ID NETWORK
AB The acquisition of densely-sampled light field (LF) images is costly, which hampers the applications of LF imaging technology in 3D reconstruction, digital refocusing, virtual reality, etc. To mitigate the obstacle, various approaches have been proposed to reconstruct densely-sampled LF images from sparsely-sampled ones. However, most existing methods still suffer from the non-Lambertian effect and large disparity issue. In this paper, we embrace the challenges by introducing a new paradigm for LF angular super-resolution (SR), which first explores the multi-scale spatial-angular correlations on the sparse sub-aperture images (SAIs) and then performs angular SR on macro-pixel features. In this way, we propose an efficient LF angular SR network, termed as EASR, with simple 3D (2D) CNNs and reshaping operations. The proposed EASR can extract effective feature representations on SAIs and can handle large disparities well by performing angular SR on macro-pixel features. Extensive comparisons with state-of-the-art methods demonstrate that our method achieves superior performance visually and quantitatively. Furthermore, our method achieves efficient angular SR by providing an excellent tradeoff between reconstruction performance and inference time.
C1 [Liu, Gaosheng; Yue, Huanjing; Yang, Jingyu] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Wu, Jiamin] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 Tianjin University; Tsinghua University
RP Yang, JY (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM gaoshengliu@tju.edu.cn; dayueer@tju.edu.cn; wujiamin@tsinghua.edu.cn;
   yjy@tju.edu.cn
RI Wu, Jiamin/ABD-5325-2021; Yang, jingyu/AAA-2088-2021
OI Wu, Jiamin/0000-0003-3479-1026; Liu, Gaosheng/0000-0003-3386-3390
FU National Natural Science Foundation of China [62231018, <LF>62072331<LF>
   62071272]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62231018, Grant 62072331, and Grant
   62071272. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Li Cheng.
CR Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Chen YB, 2021, PROC CVPR IEEE, P8624, DOI 10.1109/CVPR46437.2021.00852
   Fiss J, 2014, IEEE INT CONF COMPUT
   Ghassab VK, 2020, IEEE T MULTIMEDIA, V22, P1447, DOI 10.1109/TMM.2019.2946094
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167
   Jin J, 2020, AAAI CONF ARTIF INTE, V34, P11141
   Jin J, 2022, IEEE T PATTERN ANAL, V44, P1819, DOI 10.1109/TPAMI.2020.3026039
   Jin J, 2020, PROC CVPR IEEE, P2257, DOI 10.1109/CVPR42600.2020.00233
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kauvar I, 2015, ACM T GRAPHIC, V34, DOI [10.1145/2682631, 10.1145/2816795.2818070]
   Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926
   Ko K, 2021, IEEE T IMAGE PROCESS, V30, P4114, DOI 10.1109/TIP.2021.3069291
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Liu DY, 2020, IEEE T COMPUT IMAG, V6, P1507, DOI 10.1109/TCI.2020.3037413
   Liu GS, 2023, IEEE T MULTIMEDIA, V25, P256, DOI 10.1109/TMM.2021.3124385
   Long J, 2014, ADV NEUR IN, V27
   Mitra Kaushik, 2012, 2012 IEEE COMPUTER S, P22
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Pearson J, 2013, IEEE T IMAGE PROCESS, V22, P3405, DOI 10.1109/TIP.2013.2268939
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Vagharshakyan S, 2018, IEEE T PATTERN ANAL, V40, P133, DOI 10.1109/TPAMI.2017.2653101
   Vaish V, 2008, The (New) Stanford Light Field Archive
   Wang YQ, 2023, IEEE T PATTERN ANAL, V45, P425, DOI 10.1109/TPAMI.2022.3152488
   Wanner S., 2013, INT S VIS MOD VIS, P225
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Wu GC, 2017, PROC CVPR IEEE, P1638, DOI 10.1109/CVPR.2017.178
   Wu GC, 2021, IEEE T IMAGE PROCESS, V30, P8999, DOI 10.1109/TIP.2021.3122089
   Wu GC, 2022, IEEE T PATTERN ANAL, V44, P5430, DOI 10.1109/TPAMI.2021.3073739
   Wu GC, 2019, IEEE T IMAGE PROCESS, V28, P3261, DOI 10.1109/TIP.2019.2895463
   Wu GC, 2019, IEEE T PATTERN ANAL, V41, P1681, DOI 10.1109/TPAMI.2018.2845393
   Wu JM, 2021, CELL, V184, P3318, DOI 10.1016/j.cell.2021.04.029
   Yang J., 2021, Proc. NIPS, V34, P13304
   Yeung H. W. F., 2018, P EUR C COMP VIS, P137
   Yeung HWF, 2019, IEEE T IMAGE PROCESS, V28, P2319, DOI 10.1109/TIP.2018.2885236
   Yingqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P290, DOI 10.1007/978-3-030-58592-1_18
   Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17
   Yu JY, 2017, IEEE MULTIMEDIA, V24, P104, DOI 10.1109/MMUL.2017.24
   Zhang FL, 2017, IEEE T VIS COMPUT GR, V23, P1561, DOI 10.1109/TVCG.2016.2532329
   Zhang S, 2021, IEEE T COMPUT IMAG, V7, P799, DOI 10.1109/TCI.2021.3099636
   Zhang S, 2021, IEEE T IMAGE PROCESS, V30, P5956, DOI 10.1109/TIP.2021.3079805
   Zhang S, 2016, COMPUT VIS IMAGE UND, V145, P148, DOI 10.1016/j.cviu.2015.12.007
   Zhang YB, 2017, IEEE T CIRC SYST VID, V27, P739, DOI 10.1109/TCSVT.2016.2555778
   Zhang ZT, 2015, PROC CVPR IEEE, P3800, DOI 10.1109/CVPR.2015.7299004
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhu H, 2021, IEEE T VIS COMPUT GR, V27, P3019, DOI 10.1109/TVCG.2019.2957761
NR 47
TC 8
Z9 8
U1 5
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6588
EP 6600
DI 10.1109/TMM.2022.3211402
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500068
DA 2024-07-18
ER

PT J
AU Liu, JZ
   Zhou, W
   Li, X
   Xu, JH
   Chen, ZB
AF Liu, Jianzhao
   Zhou, Wei
   Li, Xin
   Xu, Jiahua
   Chen, Zhibo
TI LIQA: Lifelong Blind Image Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Task analysis; Image quality; Head; Training; Knowledge
   engineering; Adaptation models; Blind image quality assessment; lifelong
   learning; Split-and-Merge distillation; pseudo memory replay
ID NEURAL-NETWORKS; MEMORY
AB The image distortions are complex and dynamically changing in the real-world scenario, due to the fast development of the image processing system. The blind image quality assessment (BIQA) models may encounter the challenge of processing images with distortion types never seen before deployment. However, existing BIQA models generally cannot evolve with unseen distortion types adaptively, which greatly limits the deployment and application of BIQA models in real-world scenarios. To address this problem, we propose a novel Lifelong blind Image Quality Assessment (LIQA) approach, targeting to achieve the lifelong learning of BIQA. Without accessing to previous training data, our proposed LIQA can not only learn new knowledge, but also mitigate the catastrophic forgetting of learned knowledge. Specifically, we adopt the Split-and-Merge distillation strategy to train a single-head network that makes task-agnostic predictions. In the split stage, we first employ a distortion-specific generator to generate pseudo features of each previously seen distortion. Then, we utilize an auxiliary multi-head regression network to keep the response of each distortion. In the merge stage, we replay the pseudo features and use the pseudo labels generated by the auxiliary multi-head network to distill the knowledge of the multiple heads, which can build the final regression single head. Extensive experiments demonstrate that LIQA can perform well in handling both inner-dataset distortion shift and cross-dataset distortion shift. More importantly, our model can achieve stable performance even if the task sequences are long.
C1 [Liu, Jianzhao; Zhou, Wei; Li, Xin; Xu, Jiahua; Chen, Zhibo] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Chen, ZB (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
EM jianzhao@mail.ustc.edu.cn; weichou@mail.ustc.edu.cn;
   lixin666@mail.ustc.edu.cn; xujiahua@mail.ustc.edu.cn;
   chenzhibo@ustc.edu.cn
RI Zhou, Wei/AAG-8797-2020; Liu, Jianzhao/AAO-2359-2020
OI Liu, Jianzhao/0000-0001-9465-6075; Li, Xin/0000-0002-6352-6523; Zhou,
   Wei/0000-0003-3641-1429
FU NSFC [U1908209, 62021001]; National Key Research and Development Program
   of China [2018AAA0101400]
FX This work was supported in part by NSFC under Grants U1908209 and
   62021001 and in part by the National Key Research and Development
   Program of China under Grant 2018AAA0101400. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Federica Battisti.
CR Rusu AA, 2016, Arxiv, DOI arXiv:1606.04671
   Ans B, 2004, CONNECT SCI, V16, P71, DOI 10.1080/09540090412331271199
   Atkinson C, 2018, Arxiv, DOI arXiv:1802.03875
   Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33
   Chen ZB, 2020, IEEE J-STSP, V14, P103, DOI 10.1109/JSTSP.2020.2968182
   Chen ZB, 2017, IEEE T IMAGE PROCESS, V26, P5138, DOI 10.1109/TIP.2017.2736422
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   De Lange M, 2022, IEEE T PATTERN ANAL, V44, P3366, DOI 10.1109/TPAMI.2021.3057446
   D¡az-Rodr¡guez N, 2018, Arxiv, DOI arXiv:1810.13166
   Freitas PG, 2018, IEEE T MULTIMEDIA, V20, P3353, DOI 10.1109/TMM.2018.2839529
   FRENCH RM, 1994, PROCEEDINGS OF THE SIXTEENTH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P335
   Gao F, 2015, IEEE T NEUR NET LEAR, V26, P2275, DOI 10.1109/TNNLS.2014.2377181
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Grossberg Stephen T, 2012, STUDIES MIND BRAIN N, V70
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Goodfellow IJ, 2015, Arxiv, DOI arXiv:1312.6211
   Jianzhao Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P616, DOI 10.1007/978-3-030-58523-5_36
   Jung HC, 2016, Arxiv, DOI arXiv:1607.00122
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu JZ, 2022, IEEE COMPUT SOC CONF, P1794, DOI 10.1109/CVPRW56347.2022.00194
   Liu XL, 2020, IEEE COMPUT SOC CONF, P915, DOI [10.1061/9780784482742.105, 10.1109/CVPRW50498.2020.00121]
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Mai ZD, 2022, NEUROCOMPUTING, V469, P28, DOI 10.1016/j.neucom.2021.10.021
   Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810
   McCloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P109, DOI [10.1016/S0079-7421(08)60536-8, DOI 10.1016/S0079-7421(08)60536-8]
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Montgomery D. C., 2010, Applied Statistics and Probability for Engineers
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012
   RATCLIFF R, 1990, PSYCHOL REV, V97, P285, DOI 10.1037/0033-295X.97.2.285
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Rolnick D, 2019, 33 C NEURAL INFORM P, V32
   Rosenfeld A, 2020, IEEE T PATTERN ANAL, V42, P651, DOI 10.1109/TPAMI.2018.2884462
   Schwarz J, 2018, PR MACH LEARN RES, V80
   Serrà J, 2018, PR MACH LEARN RES, V80
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shin H, 2017, ADV NEUR IN, V30
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   van de Ven GM, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17866-2
   Xu JH, 2021, IEEE T CIRC SYST VID, V31, P1724, DOI 10.1109/TCSVT.2020.3015186
   Xu SP, 2017, IETE TECH REV, V34, P223, DOI 10.1080/02564602.2016.1151385
   Yang C, 2021, IEEE T MULTIMEDIA, V23, P1557, DOI 10.1109/TMM.2020.3001537
   Zenke F, 2017, PR MACH LEARN RES, V70
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang JT, 2020, IEEE WINT CONF APPL, P1120, DOI [10.1109/WACV45572.2020.9093365, 10.1109/wacv45572.2020.9093365]
   Zhang WX, 2023, IEEE T PATTERN ANAL, V45, P2864, DOI 10.1109/TPAMI.2022.3178874
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE IMAGE PROC, P111, DOI [10.1109/icip40778.2020.9191278, 10.1109/ICIP40778.2020.9191278]
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhou M, 2021, PROC CVPR IEEE, P4905, DOI 10.1109/CVPR46437.2021.00487
   Zhou W, 2021, INT WORK QUAL MULTIM, P61, DOI 10.1109/QoMEX51781.2021.9465479
   Zhou W, 2020, INFORM SCIENCES, V528, P205, DOI 10.1016/j.ins.2020.04.030
   Zhou W, 2019, IEEE T IMAGE PROCESS, V28, P3946, DOI 10.1109/TIP.2019.2902831
NR 61
TC 18
Z9 18
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5358
EP 5373
DI 10.1109/TMM.2022.3190700
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300053
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, NY
   Sun, X
   Yu, HF
   Yao, FL
   Xu, GL
   Fu, K
AF Liu, Nayu
   Sun, Xian
   Yu, Hongfeng
   Yao, Fanglong
   Xu, Guangluan
   Fu, Kun
TI Abstractive Summarization for Video: A Revisit in Multistage Fusion
   Network With Forget Gate
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Abstractive summarization; abstractive summarization for videos;
   multimodal; multimodal fusion; forget gate
AB Multimodal abstractive summarization for videos is an emerging task that aims to generate a summary from multi-source information (i.e., video, audio transcript). The challenge is how to merge multimodal long sequences to capture rich semantic information without allowing possible noise from either lengthy modal sequence to degrade the other modality and thus hurt the entire model. To address the issues, we propose a multistage fusion network with forget gate (MFFG), which selectively integrates multi-source information through the cross-fusion in encoding and hierarchical fusion in decoding between modalities, and design a fusion forget gate module to suppress the potential multimodal noise flow of multi-source long sequence. Meanwhile, considering that the source text in this task is lengthy and has the same distribution as the output summary text, we inherit the partial structure of the MFFG model and again propose its variant, single-stage fusion network with forget gate (SFFG), which simplifies the fusion schema, and leverages the long source text to enhance the representation of the target summary. Experimental results on How2 dataset and How2-300 dataset demonstrate the superiority of the two multimodal fusion methods. Further, we provide a version of ASR transcription data of How2 dataset to evaluate model performance under noisy scenarios, and experimental results show obvious advantages of our proposed models over prior systems.
C1 [Liu, Nayu; Sun, Xian; Yu, Hongfeng; Yao, Fanglong; Xu, Guangluan; Fu, Kun] Chinese Acad Sci, Aerosp Informat Res Inst, Beijing 100190, Peoples R China.
   [Liu, Nayu; Sun, Xian; Yu, Hongfeng; Yao, Fanglong; Fu, Kun] Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing 100190, Peoples R China.
   [Liu, Nayu; Sun, Xian; Yu, Hongfeng; Yao, Fanglong; Xu, Guangluan; Fu, Kun] Chinese Acad Sci, Aerosp Informat Res Inst, Key Lab Network Informat Syst Technol NIST, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Aerospace Information Research Institute,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Chinese Academy of Sciences; Aerospace Information
   Research Institute, CAS
RP Sun, X (corresponding author), Chinese Acad Sci, Aerosp Informat Res Inst, Beijing 100190, Peoples R China.; Sun, X (corresponding author), Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing 100190, Peoples R China.
EM 695704204@qq.com; sunxian@mail.ie.ac.cn; hfyu@mail.ie.ac.cn;
   yaofanglong17@mails.ucas.ac.cn; gluanxu@mail.ie.ac.cn;
   fukun@mail.ie.ac.cn
OI Sun, Xian/0000-0002-0038-9816; Yao, Fanglong/0000-0003-4187-9755; fu,
   kun/0000-0002-0450-6469; Liu, Nayu/0000-0002-7664-9856
FU National Science Fund for Distinguished Young Scholars of China
   [61725105]
FX This work was supported by the National Science Fund for Distinguished
   Young Scholars of China under Grant 61725105. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was DrXavier Alameda-Pineda.
CR Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Bian JW, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1807, DOI 10.1145/2505515.2505652
   Chen JQ, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4046
   Cho K., 2014, ARXIV14061078
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Gao JY, 2021, IEEE T MULTIMEDIA, V23, P3203, DOI 10.1109/TMM.2020.3021980
   Google, GOOGL SPEECH TEXT AP
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jin L, 2023, IEEE T NEUR NET LEAR, V34, P1838, DOI 10.1109/TNNLS.2020.2997020
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Khullar A, 2020, ARXIV
   Kingma D. P., 2014, arXiv
   Lei Ba J., 2016, arXiv
   Li H., 2017, P 2017 C EMP METH NA, P1092
   Li HR, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4152
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZC, 2017, SPRINGER THESES-RECO, P131, DOI 10.1007/978-981-10-3689-7_6
   Li ZC, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2624140
   Li ZF, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2807705
   Libovicky J., 2018, PROC WORKSHOP INT C, P1
   Libovicky J, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P196, DOI 10.18653/v1/P17-2031
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu NY, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1834
   Liu NY, 2021, NEUROCOMPUTING, V456, P179, DOI 10.1016/j.neucom.2021.04.072
   Liu Y, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3730
   Luong M.-T., 2015, P 2015 C EMPIRICAL M, DOI DOI 10.18653/V1/D15-1166
   Mei SH, 2021, IEEE T MULTIMEDIA, V23, P732, DOI 10.1109/TMM.2020.2987683
   Narayan Shashi, 2018, NAACL HLT, DOI DOI 10.18653/V1/N18-1158
   Palaskar S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6587
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Radford A., 2019, LANGUAGE MODELS ARE
   Rush Alexander M., 2015, P 2015 C EMPIRICAL M, P379, DOI [10.48550/arXiv.1509.00685, DOI 10.18653/V1/D15-1044]
   Sanabria Ramon, 2018, ARXIV181100347
   See A, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1073, DOI 10.18653/v1/P17-1099
   Shah RR, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P185, DOI 10.1145/2733373.2809932
   Shah RR, 2016, KNOWL-BASED SYST, V108, P102, DOI 10.1016/j.knosys.2016.05.022
   Sutskever I, 2014, ADV NEUR IN, V27
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2015, P ANN C N AM CHAPT A, P1494, DOI 10.3115/v1/N15-1173
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang William Yang, 2016, P 2016 C N AM CHAPT, P58
   Wu YH, 2016, Arxiv, DOI [arXiv:1609.08144, DOI 10.48550/ARXIV.1609.08144]
   Zhang J., 2020, P 37 INT C MACHINE L
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Zhu JN, 2020, AAAI CONF ARTIF INTE, V34, P9749
   Zhu JN, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4154
NR 49
TC 0
Z9 0
U1 5
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3296
EP 3310
DI 10.1109/TMM.2022.3157993
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200026
DA 2024-07-18
ER

PT J
AU Lu, W
   Li, DS
   Nie, LQ
   Jing, PG
   Su, YT
AF Lu, Wei
   Li, Desheng
   Nie, Liqiang
   Jing, Peiguang
   Su, Yuting
TI Learning Dual Low-Rank Representation for Multi-Label Micro-Video
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Micro-video; multi-label classification; multi-modality; low-rank
   representation
ID THRESHOLDING ALGORITHM; DICTIONARY; SELECTION; MODEL
AB Currently, with the rapid development of mobile Internet, micro-video has become a prevailing format of user-generated contents (UGCs) on various social media platforms. Several studies have been conducted towards to understanding high-level micro-video semantics, such as venue categorization, memorability, and popularity. However, these approaches supported tasks with only a single output, which exhibited limitations when attempting to use them to resolve tasks with multiple outputs, especially the multi-label micro-video classification. To tackle this problem, in this paper, we propose a dual multi-modal low-rank decomposition (DMLRD) method for multi-label micro-video classification tasks. To learn more comprehensive micro-video representations, we first learn the low-rank-regularized modality-specific and modality-shared components by considering the consistency and the complementarity among modalities simultaneously. Meanwhile, the less descriptive power of each modality aroused by inherent properties can be solved to a certain extent. To obtain unseen label representations, we next construct a sparsity-regularized multi-matrix normal estimation term to jointly encode the latent relationship structures among labels and dimensions. Experiments on two datasets demonstrate the effectiveness of our proposed method over the state-of-art methods.
C1 [Lu, Wei; Li, Desheng; Jing, Peiguang; Su, Yuting] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
C3 Tianjin University; Shandong University
RP Jing, PG (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM luwei@tju.edu.cn; lidesheng1996@tju.edu.cn; nieliqiang@gmail.com;
   pgjing@tju.edu.cn; ytsu@tju.edu.cn
OI Jing, Peiguang/0000-0003-2648-7358
FU National Natural Science Foundation of China [61802277]; Tianjin
   Municipal Natural Science Foundation [20JCQNJC01210]; China Postdoctoral
   Science Foundation [2019M651038]; Elite Scholar Program of Tianjin
   University [2020XRG-0104]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61802277, in part by Tianjin Municipal
   Natural Science Foundation under Grant 20JCQNJC01210, in part by China
   Postdoctoral Science Foundation Funded Project under Grant 2019M651038,
   and in part by the Elite Scholar Program of Tianjin University under
   Grant 2020XRG-0104.
CR [Anonymous], 2015, P IEEE C COMP VIS PA
   Banerjee O, 2008, J MACH LEARN RES, V9, P485
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cao D, 2018, ACM/SIGIR PROCEEDINGS 2018, P645, DOI 10.1145/3209978.3209998
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen CF, 2012, PROC CVPR IEEE, P2618, DOI 10.1109/CVPR.2012.6247981
   Chen GB, 2017, IEEE IJCNN, P2377, DOI 10.1109/IJCNN.2017.7966144
   Chen JY, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P898, DOI 10.1145/2964284.2964314
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen YD, 2018, IEEE T MULTIMEDIA, V20, P3212, DOI 10.1109/TMM.2018.2834867
   Cheng X, 2018, GENOMICS, V110, P50, DOI 10.1016/j.ygeno.2017.08.005
   Cheng X, 2017, BIOINFORMATICS, V33, P341, DOI 10.1093/bioinformatics/btw644
   COPPERSMITH D, 1990, J SYMB COMPUT, V9, P251, DOI 10.1016/S0747-7171(08)80013-2
   Dahl J, 2008, OPTIM METHOD SOFTW, V23, P501, DOI 10.1080/10556780802102693
   Ding ZM, 2016, AAAI CONF ARTIF INTE, P1181
   Ding Z, 2014, AAAI CONF ARTIF INTE, P1192
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fürnkranz J, 2008, MACH LEARN, V73, P133, DOI 10.1007/s10994-008-5064-8
   GARDINER JD, 1992, ACM T MATH SOFTWARE, V18, P223, DOI 10.1145/146847.146929
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Jia XD, 2021, IEEE T PATTERN ANAL, V43, P2496, DOI 10.1109/TPAMI.2020.2973634
   Jia ZL, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0137782
   Jing PG, 2018, IEEE T KNOWL DATA EN, V30, P1519, DOI 10.1109/TKDE.2017.2785784
   Jing XY, 2017, IEEE T IMAGE PROCESS, V26, P1363, DOI 10.1109/TIP.2017.2651364
   Jing XY, 2016, IEEE T IMAGE PROCESS, V25, P2712, DOI 10.1109/TIP.2016.2549459
   Li LY, 2014, IMAGE VISION COMPUT, V32, P814, DOI 10.1016/j.imavis.2014.02.007
   Li X, 2016, IEEE T MULTIMEDIA, V18, P474, DOI 10.1109/TMM.2016.2518478
   Lin Z., 2009, Technical Report (No. UILU-ENG-09-2215
   Liu AN, 2018, SIGNAL PROCESS, V152, P206, DOI 10.1016/j.sigpro.2018.06.001
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Liu JZ, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P115, DOI 10.1145/3077136.3080834
   Liu M, 2019, IEEE T IMAGE PROCESS, V28, P1235, DOI 10.1109/TIP.2018.2875363
   Liu S, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3020, DOI 10.1145/3308558.3313513
   LU TX, 1986, COMPUTING, V37, P351, DOI 10.1007/BF02251092
   Lyu F, 2019, IEEE T MULTIMEDIA, V21, P1971, DOI 10.1109/TMM.2019.2894964
   Ma L, 2012, PROC CVPR IEEE, P2586, DOI 10.1109/CVPR.2012.6247977
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Redi M, 2014, PROC CVPR IEEE, P4272, DOI 10.1109/CVPR.2014.544
   Sanches JM, 2008, IEEE T IMAGE PROCESS, V17, P1522, DOI 10.1109/TIP.2008.2001398
   Shi M, 2020, IEEE T NEUR NET LEAR, V31, P3682, DOI 10.1109/TNNLS.2019.2945869
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P2837, DOI 10.1109/TMM.2019.2909860
   Wang HY, 2017, IEEE T MULTIMEDIA, V19, P969, DOI 10.1109/TMM.2016.2638624
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wei YW, 2020, IEEE T IMAGE PROCESS, V29, P1, DOI 10.1109/TIP.2019.2923608
   Wu F, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107335
   Yeh CK, 2017, AAAI CONF ARTIF INTE, P2838
   Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang ML, 2006, IEEE T KNOWL DATA EN, V18, P1338, DOI 10.1109/TKDE.2006.162
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhang X, 2017, LECT NOTES COMPUT SC, V10177, P659, DOI 10.1007/978-3-319-55753-3_41
   Zhang YMZ, 2013, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2013.93
   Zhang Z, 2016, IEEE T IMAGE PROCESS, V25, P2429, DOI 10.1109/TIP.2016.2547180
   Zhou P, 2016, IEEE T NEUR NET LEAR, V27, P1080, DOI 10.1109/TNNLS.2015.2436951
   Zhu Y, 2018, IEEE T KNOWL DATA EN, V30, P1081, DOI 10.1109/TKDE.2017.2785795
NR 65
TC 6
Z9 6
U1 5
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 77
EP 89
DI 10.1109/TMM.2021.3121567
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400006
DA 2024-07-18
ER

PT J
AU Luo, DY
   Ye, M
   Li, S
   Zhu, C
   Li, X
AF Luo, Dengyan
   Ye, Mao
   Li, Shuai
   Zhu, Ce
   Li, Xue
TI Spatio-Temporal Detail Information Retrieval for Compressed Video
   Quality Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality enhancement; compressed video; multi frame; deformable
   convolution; deep learning
ID EFFICIENCY
AB The past few years have witnessed the great success of multi-frame quality enhancement for compressed video. Although the existing methods based on deformable alignment have achieved the state-of-the-art performance, they do not pay enough attention to the recovery of detail information. In this work, we propose a Spatio-Temporal Detail Retrieval (STDR) method to promote the recovery of detail information. To alleviate the problem of inaccurate deformable offsets caused by the fixed receptive field, motivated by multi-task learning, we design a plug-and-play Multi-path Deformable Alignment (MDA) module to generate more accurate offsets by integrating the alignment features of different receptive fields, so that the temporal detail information can be better recovered. For the spatial detail information restoration, several residual dense blocks with channel attention layer are utilized in the reconstruction module to explore valuable high-frequency spatial information from the fused multi-path alignment features. Meanwhile, a complementary loss function based on the Pearson correlation coefficient is developed to ameliorate the over-smoothing shortcoming caused by pixel-wise mean square or absolute value loss. Experimental results demonstrate that the proposed STDR network achieves superior performance compared with the state-of-the-art methods in both quantitative and qualitative evaluations.
C1 [Luo, Dengyan; Ye, Mao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Li, Shuai] Shandong Univ, Sch Control Sci & Engn, Jinan 250100, Peoples R China.
   [Li, Shuai; Zhu, Ce] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Li, Xue] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
C3 University of Electronic Science & Technology of China; Shandong
   University; University of Electronic Science & Technology of China;
   University of Queensland
RP Ye, M (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM dengyanluo@126.com; cvlab.uestc@gmail.com; shuaili@sdu.edu.cn;
   eczhu@uestc.edu.cn; xueli@itee.uq.edu.au
RI Ye, Mao/G-8559-2012
OI Li, Shuai/0000-0002-9938-0917; Luo, Dengyan/0000-0001-5482-4636; LI,
   Xue/0000-0002-4515-6792
FU National Key R&D Program of China [2018YFE0203900]; National Natural
   Science Foundation of China [62276048, 62020106011, U19A2052]; Sichuan
   Science and Technology Program [2020YFG0476]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018YFE0203900, in part by the National Natural Science
   Foundation of China under Grants 62276048, 62020106011, U19A2052, and in
   part by Sichuan Science and Technology Program under Grant 2020YFG0476.
CR Bampis CG, 2017, IEEE T IMAGE PROCESS, V26, P5217, DOI 10.1109/TIP.2017.2729891
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Cavigelli L, 2017, IEEE IJCNN, P752, DOI 10.1109/IJCNN.2017.7965927
   Chan KCK, 2022, PROC CVPR IEEE, P5962, DOI 10.1109/CVPR52688.2022.00588
   Chan KCK, 2021, AAAI CONF ARTIF INTE, V35, P973
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cui K., 2019, P CVPR WORKSH, P1
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   De Vito F, 2005, 2005 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY (ISSPIT), VOLS 1 AND 2, P612
   Deng JN, 2020, AAAI CONF ARTIF INTE, V34, P10696
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Ding Q, 2021, IEEE T IMAGE PROCESS, V30, P6459, DOI 10.1109/TIP.2021.3092949
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Du WB, 2017, IEEE I CONF COMP VIS, P3745, DOI 10.1109/ICCV.2017.402
   Fuoli Dario, 2020, P IEEE C COMP VIS PA, P476
   Gong YH, 2000, PROC CVPR IEEE, P174, DOI 10.1109/CVPR.2000.854772
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Guo J, 2016, LECT NOTES COMPUT SC, V9905, P628, DOI 10.1007/978-3-319-46448-0_38
   He XY, 2018, IEEE IMAGE PROC, P216, DOI 10.1109/ICIP.2018.8451086
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu SD, 2012, IEEE T IMAGE PROCESS, V21, P1911, DOI 10.1109/TIP.2011.2176347
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jin Z, 2021, IEEE T CIRC SYST VID, V31, P467, DOI 10.1109/TCSVT.2020.2982174
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Li K, 2017, IEEE INT CON MULTI, P1320, DOI 10.1109/ICME.2017.8019416
   Lin WY, 2020, IEEE T MULTIMEDIA, V22, P2749, DOI 10.1109/TMM.2019.2962310
   Luo DY, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675384
   Luo DY, 2022, IEEE SIGNAL PROC LET, V29, P543, DOI 10.1109/LSP.2022.3147441
   Luo Xiaotong, 2020, PROC IEEE C COMPUT V
   Meng XD, 2021, IEEE T CIRC SYST VID, V31, P2401, DOI 10.1109/TCSVT.2020.3019919
   Meng XD, 2019, IEEE IMAGE PROC, P1193, DOI [10.1109/icip.2019.8804469, 10.1109/ICIP.2019.8804469]
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Ouyang WL, 2015, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2015.7298854
   Park Park W.-S. W.-S., 2016, P IM VID MULT SIGN P, P1
   Paszke Adam, 2017, NIPS W
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tan TK, 2016, IEEE T CIRC SYST VID, V26, P76, DOI 10.1109/TCSVT.2015.2477916
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Wang TT, 2017, IEEE DATA COMPR CONF, P410, DOI 10.1109/DCC.2017.42
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Wulff J, 2015, PROC CVPR IEEE, P120, DOI 10.1109/CVPR.2015.7298607
   Xiao WH, 2020, IEEE T MULTIMEDIA, V22, P1680, DOI 10.1109/TMM.2020.2978664
   Xu Y, 2019, IEEE I CONF COMP VIS, P7042, DOI 10.1109/ICCV.2019.00714
   Yang R, 2019, IEEE INT CON MULTI, P532, DOI 10.1109/ICME.2019.00098
   Yang R, 2019, IEEE T CIRC SYST VID, V29, P2039, DOI 10.1109/TCSVT.2018.2867568
   Yang R, 2018, PROC CVPR IEEE, P6664, DOI 10.1109/CVPR.2018.00697
   Yang R, 2017, IEEE INT CON MULTI, P817, DOI 10.1109/ICME.2017.8019299
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao MY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5646, DOI 10.1145/3474085.3475710
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 63
TC 5
Z9 5
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6808
EP 6820
DI 10.1109/TMM.2022.3214775
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000008
DA 2024-07-18
ER

PT J
AU Madhu, A
   Suresh, K
AF Madhu, Aswathy
   Suresh, K.
TI RQNet: Residual Quaternion CNN for Performance Enhancement in Low
   Complexity and Device Robust Acoustic Scene Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks; Acoustics; Quaternions; Feature
   extraction; Complexity theory; Performance evaluation; Spectrogram; Low
   complexity ASC; device robust ASC; wavelet transform; quaternion CNN;
   residual CNN; DCASE
ID CONVOLUTIONAL NEURAL-NETWORKS
AB Acoustic Scene Classification aims to recognize the unique acoustic characteristics of an environment. Recently, Convolutional Neural Networks (CNNs) have boosted the accuracy of ASC algorithms. However, the focus of ASC system designers has shifted from improving accuracy to incorporating real-world considerations like device robustness and model complexity. In this paper, we address the problem of developing a low complexity system for ASC which can generalize across multiple recording devices. We propose to employ residual quaternion CNNs for low complexity, device-robust ASC. The proposed model RQNet uses quaternion encoding to increase the accuracy with fewer parameters. To further enhance the performance of RQNet, we employ a variant of log-mel spectrogram called multi-scale mel spectrogram (ms2) to represent the acoustic signal. Experiments on two benchmark ASC datasets indicate that RQNet outperforms a log-mel spectrum-based baseline by more than twofold. In addition, it has a good measure of separability between the individual classes, as indicated by an AUC (Area Under the ROC Curve) scores of 0.906 and 0.994. Furthermore, it reduces the model size by 82.19% and floating-point operations by 23.25%. Consequently, RQNet is suitable for deployment in context-aware devices.
C1 [Madhu, Aswathy] APJ Abdul Kalam Technol Univ, Coll Engn, Dept Elect, Trivandrum 695016, India.
   [Suresh, K.] APJ Abdul Kalam Technol Univ, Govt Engn Coll, Wayanad 670644, India.
C3 College of Engineering, Trivandrum
RP Madhu, A (corresponding author), APJ Abdul Kalam Technol Univ, Coll Engn, Dept Elect, Trivandrum 695016, India.
EM aswathymadhu@cet.ac.in; sureshk@cet.ac.in
OI Kumaraswamy, Suresh/0000-0002-4358-7732; Madhu,
   Aswathy/0000-0002-6392-3318
CR [Anonymous], 2019, P DET CLASS AC SCEN
   Bahmei B, 2022, IEEE SIGNAL PROC LET, V29, P682, DOI 10.1109/LSP.2022.3150258
   Basbug AM, 2019, IEEE INT C SEMANT CO, P128, DOI [10.1109/ICSC.2019.00029, 10.1109/ICOSC.2019.8665547]
   Cakir E., 2018, P WORKSH DET CLASS C, P138
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Clarkson B., 1998, P INT WORKSH PERC US, P46
   Comminiello D, 2019, INT CONF ACOUST SPEE, P8533, DOI [10.1109/icassp.2019.8682711, 10.1109/ICASSP.2019.8682711]
   Dang A, 2018, IEEE ICCE
   DCASE, 2019, Dcase2019 challenge, IEEE AASP challenge on detection and classification of acoustic scenes and events
   Demir F, 2020, IEEE ACCESS, V8, P66529, DOI 10.1109/ACCESS.2020.2984903
   Devalraju DV, 2021, EUR SIGNAL PR CONF, P21, DOI 10.23919/Eusipco47968.2020.9287875
   Drossos K, 2019, IEEE WORK APPL SIG, P259, DOI [10.1109/WASPAA.2019.8937231, 10.1109/waspaa.2019.8937231]
   Eghbal-Zadeh Hamid, 2016, DCASE2016 CHALLENGE
   Fang Z, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-10382-x
   Gosztolya G, 2016, J ELECTR ENG-SLOVAK, V67, P124, DOI 10.1515/jee-2016-0017
   Härmä A, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P634, DOI 10.1109/ICME.2005.1521503
   Heittola T., 2020, P DET CLASS AC SCEN, P56
   Hu H., 2020, Tech. Rep.
   Johnson DS, 2021, EUR SIGNAL PR CONF, P81, DOI 10.23919/Eusipco47968.2020.9287327
   Kim B., 2021, Tech. Rep.
   Kim Byeonggeun, 2021, P 6 DET CLASS AC SCE, P21
   Kosmider Michal, 2019, Tech. Rep, P25
   Koutini K., 2019, P DETECTION CLASSIFI, P124, DOI DOI 10.33682/CJD9-KC43
   Koutini K., 2021, Tech. Rep.
   Koutini K., 2019, DCASE2019 CHALLENGE
   Lee Y, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10040371
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   McDonnell MD, 2020, INT CONF ACOUST SPEE, P141, DOI [10.1109/ICASSP40776.2020.9053274, 10.1109/icassp40776.2020.9053274]
   Mesaros A., 2018, P DET CLASS AC SCEN, P9
   Mu WJ, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-01045-4
   Nguyen T., 2018, Tech. Rep
   Nguyen T, 2020, INT CONF ACOUST SPEE, P126, DOI [10.1109/icassp40776.2020.9053582, 10.1109/ICASSP40776.2020.9053582]
   Octave M., 2018, Proc. DCASE, P103
   Olvera M, 2021, EUR SIGNAL PR CONF, P281, DOI 10.23919/Eusipco47968.2020.9287436
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Parcollet T, 2018, INTERSPEECH, P22
   Petetin Y, 2015, EUR SIGNAL PR CONF, P125, DOI 10.1109/EUSIPCO.2015.7362358
   Pham L.., 2021, Tech. Rep
   Phan H, 2019, INTERSPEECH, P3845, DOI 10.21437/Interspeech.2019-3040
   Phan H, 2017, IEEE-ACM T AUDIO SPE, V25, P1278, DOI 10.1109/TASLP.2017.2690564
   Primus P., 2019, P DCASE, P204
   Ren Z., 2017, P WORKSH DET CLASS A, P113
   Ren Z, 2021, IEEE T MULTIMEDIA, V23, P4131, DOI 10.1109/TMM.2020.3037534
   Ren Z, 2019, INT CONF ACOUST SPEE, P56, DOI 10.1109/ICASSP.2019.8683434
   Seo S, 2021, INTERSPEECH, P576, DOI 10.21437/Interspeech.2021-1308
   Serizel R, 2016, IEEE IMAGE PROC, P948, DOI 10.1109/ICIP.2016.7532497
   Shen J., 2016, P INT C MULT MOD, P231
   Siderius M., 2021, J. Acoustical Soc. Amer, V150, pA314, DOI [10.1121/10.0008412, DOI 10.1121/10.0008412]
   Song HW, 2022, IEEE SIGNAL PROC LET, V29, P154, DOI 10.1109/LSP.2021.3130502
   Suh S., 2020, Technical Report
   Takeyama S, 2021, EUR SIGNAL PR CONF, P36, DOI 10.23919/Eusipco47968.2020.9287734
   Toffa OK, 2021, IEEE T MULTIMEDIA, V23, P3978, DOI 10.1109/TMM.2020.3035275
   Ullo SL, 2020, IEEE ACCESS, V8, P124055, DOI 10.1109/ACCESS.2020.3006082
   Ullrich K., 2017, INT C LEARNING REPRE, P1
   Valenti M, 2017, IEEE IJCNN, P1547, DOI 10.1109/IJCNN.2017.7966035
   Virtanen T., 2018, Computational Analysis of Sound Scenes and Events, P3, DOI [10.1007/978-3-319-63450-0_1, DOI 10.1007/978-3-319-63450-0, 10.1007/978-3-319-63450-0]
   Wang J, 2019, IEEE INT CONF MULTI, P390, DOI 10.1109/ICMEW.2019.00073
   Wang M, 2019, ASIAPAC SIGN INFO PR, P1511, DOI 10.1109/APSIPAASC47483.2019.9023236
   Wu B, 2022, IEEE INTERNET THINGS, V9, P3416, DOI 10.1109/JIOT.2021.3098464
   Wu YZ, 2019, INT CONF ACOUST SPEE, P815, DOI [10.1109/ICASSP.2019.8683490, 10.1109/icassp.2019.8683490]
   Yang C.-H. H., 2021, Tech. Rep.
   Yang H., 2021, P IEEE INT S BROADB, P1
   Yihao C., 2021, Tech. Rep.
   Zhang J., 2021, Tech. Rep
   Zhang T, 2020, EXPERT SYST APPL, V143, DOI 10.1016/j.eswa.2019.113067
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhu XY, 2018, LECT NOTES COMPUT SC, V11212, P645, DOI 10.1007/978-3-030-01237-3_39
NR 67
TC 3
Z9 3
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8780
EP 8792
DI 10.1109/TMM.2023.3241553
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000023
DA 2024-07-18
ER

PT J
AU Ou, ZH
   Chen, ZJ
   Shen, SY
   Fan, LN
   Yao, SY
   Song, MN
   Hui, P
AF Ou, Zhonghong
   Chen, Zhongjie
   Shen, Shengyi
   Fan, Lina
   Yao, Siyuan
   Song, Meina
   Hui, Pan
TI Free<SUP>3</SUP>Net: Gliding Free, Orientation Free, and Anchor Free
   Network for Oriented Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Detectors; Feature extraction; Location awareness;
   Task analysis; Convolution; Training; Oriented object detection;
   anchor-free detector; label assignment; oriented feature alignment
AB Object detection for aerial images has achieved remarkable progress in recent years. Nevertheless, most exiting studies do not differentiate oriented object detection from horizontal detection. Certain schemes ignore the ambiguity of oriented object representation and leverage label assignment designed for horizontal object detection directly. Consequently, it leads to unstable training and causes performance degradation, because high-quality samples surrounding the oriented bounding boxes can not be leveraged effectively. To address this problem, we propose a gliding Free, orientation Free, and anchor Free Network (Free(3)Net) with high-efficiency for oriented object detection. Specifically, we propose an unambiguous oriented object representation scheme, named FreeGliding, by gliding the projection points of samples on each edge of horizontal bounding boxes. It makes the detection largely free from representation ambiguity and multi-task dependency. To overcome the restrictions of label assignment, we put forward a novel Loss-aware Outer Sample Selection (LOSS) scheme, which takes into consideration spatial information and localization capability to retain high-quality samples surrounding the objects. Moreover, we introduce an Oriented Feature Fusion (OFF) scheme to tackle feature alignment by adjusting the receptive field and fusing oriented features dynamically. Experimental results on two large-scale remote sensing datasets HRSC2016 and DOTA demonstrate that Free(3)Net outperforms the state-of-the-art schemes with a large margin. We hope our work can inspire rethinking the design of anchor-free detectors, and serve as a strong baseline for oriented object detection.
C1 [Ou, Zhonghong; Chen, Zhongjie; Shen, Shengyi; Fan, Lina; Yao, Siyuan; Song, Meina] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
   [Hui, Pan] Hong Kong Univ Sci & Technol, Computat Media & Arts Trust Area, Hong Kong, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Hong Kong University
   of Science & Technology
RP Song, MN (corresponding author), Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
EM zhonghong.ou@bupt.edu.cn; zhongjie_chen@bupt.edu.cn;
   pmathsun@bupt.edu.cn; lina_fan@bupt.edu.cn; yaosiyuan@bupt.edu.cn;
   mnsong@bupt.edu.cn; panhui@cse.ust.hk
RI bai, yu/KHU-2608-2024
FU National Key Research and Development Program of China [2020AAA0107500];
   National Natural Science Foundation of China [62076035]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020AAA0107500 and in part by
   the National Natural Science Foundation of China under Grant 62076035.
CR Chen YT, 2019, Arxiv, DOI arXiv:1908.01570
   Chen Z., 2020, PROC EUR C COMPUT VI, P195, DOI DOI 10.1007/978-3-030-58558-7_12
   Ding J, 2019, PROC CVPR IEEE, P2844, DOI 10.1109/CVPR.2019.00296
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Han JM, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3062048
   He K., 2017, IEEE C COMP VIS PATT, P2961, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou LP, 2022, AAAI CONF ARTIF INTE, P923
   Hou LP, 2022, IEEE T IMAGE PROCESS, V31, P1545, DOI 10.1109/TIP.2022.3143690
   Jiang YY, 2017, Arxiv, DOI [arXiv:1706.09579, DOI 10.48550/ARXIV.1706.09579]
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin YT, 2021, Arxiv, DOI arXiv:1912.00969
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595
   Liu ZK, 2016, IEEE GEOSCI REMOTE S, V13, P1074, DOI 10.1109/LGRS.2016.2565705
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Ming Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3095186
   Ming Q, 2021, AAAI CONF ARTIF INTE, V35, P2355
   Pan X., 2020, P IEEE CVF C COMP VI, P11207
   Qian W, 2021, AAAI CONF ARTIF INTE, V35, P2458
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Xie XX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3500, DOI 10.1109/ICCV48922.2021.00350
   Xu YC, 2021, IEEE T PATTERN ANAL, V43, P1452, DOI [10.1109/TGRS.2020.3026387, 10.1109/TPAMI.2020.2974745]
   Xue Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P677, DOI 10.1007/978-3-030-58598-3_40
   Yang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13043, DOI 10.1109/CVPR42600.2020.01306
   Yang X, 2021, PROC CVPR IEEE, P15814, DOI 10.1109/CVPR46437.2021.01556
   Yang X, 2021, PR MACH LEARN RES, V139
   Yang X, 2021, AAAI CONF ARTIF INTE, V35, P3163
   Yang X, 2019, IEEE I CONF COMP VIS, P8231, DOI 10.1109/ICCV.2019.00832
   Yi JR, 2021, IEEE WINT CONF APPL, P2149, DOI 10.1109/WACV48630.2021.00220
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 40
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7089
EP 7100
DI 10.1109/TMM.2022.3217397
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000028
DA 2024-07-18
ER

PT J
AU Shim, BS
   Choe, JH
   Hou, JU
AF Shim, Bo Seok
   Choe, Jae Hong
   Hou, Jong-Uk
TI Source Identification of 3D Printer Based on Layered Texture Encoders
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Printers; Three-dimensional printing;
   Frequency division multiplexing; Forensics; Watermarking; Surface
   treatment; 3D printing; additive manufacturing; multimedia forensics;
   multitask learning; source identification
ID CONSUMER; PATTERN
AB With the rapid growth in the three-dimensional (3D) printing content market, various unprecedented criminal cases and copyright protection issues have emerged. In response to this imminent and emergent difficulty, we propose a forensic technique for identifying the source of 3D printed products based only on surface inspection features. The surface texture of 3D printed objects exhibits, inevitably, extremely fine periodic features during the additive manufacturing process. We propose a two-stream texture encoder, referred to as CFTNet, combined with fast Fourier transform and positional encoding of the transformer encoder to leverage inherent periodic features occurring during the additive manufacturing. As benchmarks, we define detailed scenarios for six source identification problems and present detailed verification procedures with a large-scale benchmark dataset SI3DP++ for forensic real-world scenarios. A certain level of performance was achieved using six benchmarks, including printer and device-level identification. Moreover, we extended the baseline study based on the benchmark set to forensic test scenarios from multiple perspectives in preparation for real situations. We reveal both the dataset and detailed experimental design to provide an opportunity to facilitate future in-depth studies related to forensics and protection of intellectual property.
C1 [Shim, Bo Seok; Choe, Jae Hong; Hou, Jong-Uk] Hallym Univ, Div Software, Chunchon 24252, Gangwon Do, South Korea.
C3 Hallym University
RP Hou, JU (corresponding author), Hallym Univ, Div Software, Chunchon 24252, Gangwon Do, South Korea.
EM bycicle55@naver.com; jae_apple@naver.com; juhou@hallym.ac.kr
OI Hou, Jong-Uk/0000-0002-7101-0244
FU National Research Foundation of Korea
FX No Statement Available
CR Akhoundi B, 2019, EXP MECH, V59, P883, DOI 10.1007/s11340-018-00467-y
   Alsoufi M.S., 2017, American Journal of Mechanical Engineering, V5, P211, DOI [10.12691/ajme-5-5-4, DOI 10.12691/AJME-5-5-4]
   Aronson A, 2021, J FORENSIC SCI, V66, P2405, DOI 10.1111/1556-4029.14825
   Arrizubieta JI, 2020, METALS-BASEL, V10, DOI 10.3390/met10020261
   Bai WY, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11188298
   Bayar B, 2018, IEEE T INF FOREN SEC, V13, P2691, DOI 10.1109/TIFS.2018.2825953
   Bayram S, 2005, IEEE IMAGE PROC, P2793
   Biggs M, 2019, J FORENSIC RADIOL IM, V18, P1, DOI 10.1016/j.jofri.2019.07.001
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Brinsko-Beckert K, 2020, J FORENSIC SCI, V65, P1480, DOI 10.1111/1556-4029.14486
   Carew RM, 2020, J FORENSIC SCI, V65, P1752, DOI 10.1111/1556-4029.14442
   Chase J. R., 2018, Nat. Inst. Justice J., V279, P49
   Chen A., 2019, 4 vital things on post-processing 3D printed parts
   Chua CK, 2010, RAPID PROTOTYPING: PRINCIPLES AND APPLICATIONS, 3RD EDITION, DOI 10.1142/6665
   Chua C. K., 2010, World Sci.
   Delmotte A, 2021, IEEE T MULTIMEDIA, V23, P3467, DOI 10.1109/TMM.2020.3025660
   Delmotte A, 2020, IEEE T MULTIMEDIA, V22, P2780, DOI 10.1109/TMM.2019.2962306
   Diehl E, 2003, ICCE: 2003 INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, DIGEST OF TECHNICAL PAPERS, P52
   Dizon JRC, 2021, TECHNOLOGIES, V9, DOI 10.3390/technologies9030061
   Dogan MD, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376202
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dudek P, 2013, ARCH METALL MATER, V58, P1415, DOI 10.2478/amm-2013-0186
   Durall R., 2020, P IEEE CVF C COMP VI, P7890, DOI DOI 10.1109/CVPR42600.2020.00791
   Favela C., 2020, Incredible prints: Can you 3D print a key?
   Fingas J., 2017, Security firm claims to thwart iphone X's face id with a mask
   Formlabs, How much does a 3D printer cost?
   Fung B., 2019, Facebook has shut down 5.4 billion fake accounts this year
   Gao Y, 2021, IEEE T INF FOREN SEC, V16, P2805, DOI 10.1109/TIFS.2021.3065225
   Groth Christian, 2014, J Clin Orthod, V48, P475
   Halassi S, 2019, INT J PHYS DISTR LOG, V49, P200, DOI 10.1108/IJPDLM-03-2018-0139
   Hou JU, 2018, IEEE ACCESS, V6, P44082, DOI 10.1109/ACCESS.2018.2864331
   Hou JU, 2017, IEEE T INF FOREN SEC, V12, P2712, DOI 10.1109/TIFS.2017.2718482
   Kubo Y, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382847
   Li DZY, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P449, DOI 10.1145/3126594.3126635
   Li ZX, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P1306, DOI 10.1145/3243734.3243735
   Lipp J., 2020, Defense Counsel J., V87, P1
   Liu C, 2019, IH&MMSEC '19: PROCEEDINGS OF THE ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P119, DOI 10.1145/3335203.3335725
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loukas A., 2020, IBER CONF INF SYST, DOI DOI 10.23919/cisti49556.2020.9141108
   Lukás J, 2006, IEEE T INF FOREN SEC, V1, P205, DOI 10.1109/TIFS.2006.873602
   Mahdian B, 2008, IEEE T INF FOREN SEC, V3, P529, DOI 10.1109/TIFS.2004.924603
   Maia HT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322960
   Malik A, 2022, IND ROBOT, V49, P491, DOI 10.1108/IR-10-2021-0247
   Mehrish A, 2020, IEEE T CIRC SYST VID, V30, P3000, DOI 10.1109/TCSVT.2019.2929561
   Mellin P, 2016, J CLEAN PROD, V139, P1224, DOI 10.1016/j.jclepro.2016.08.141
   Mikkilineni AK, 2005, IS&T'S NIP21: INTERNATIONAL CONFERENCE ON DIGITAL PRINTING TECHNOLOGIES, FINAL PROGRAM AND PROCEEDINGS, P223
   Mitsouras D, 2015, RADIOGRAPHICS, V35, P1966, DOI 10.1148/rg.2015140320
   Peng F, 2019, IEEE T RELIAB, V68, P342, DOI 10.1109/TR.2018.2869303
   Peng H, 2019, COMPUT AIDED DESIGN, V114, P91, DOI 10.1016/j.cad.2019.05.029
   Peng L, 2021, MULTIMEDIA SYST, V27, P363, DOI 10.1007/s00530-020-00697-y
   Popescu AC, 2005, IEEE T SIGNAL PROCES, V53, P758, DOI 10.1109/TSP.2004.839932
   Rao YM, 2021, Advances in neural information processing systems, V34
   Ruder S, 2017, Arxiv, DOI [arXiv:1706.05098, 10.48550/arXiv.1706.05098]
   Scott C., 2016, Criminals break through security? MSU researchers demonstrate it's possible
   Shim BS, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1721, DOI 10.1145/3474085.3475316
   Steenhuis HJ, 2016, J MANUF TECHNOL MANA, V27, P990, DOI 10.1108/JMTM-01-2016-0002
   Tan MX, 2021, PR MACH LEARN RES, V139, P7102
   Tan MX, 2019, PR MACH LEARN RES, V97
   Yamamoto H, 2018, PROCEEDINGS OF 2018 INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY AND ITS APPLICATIONS (ISITA2018), P321, DOI 10.23919/ISITA.2018.8664290
   Youn JS, 2019, RSC ADV, V9, P19606, DOI 10.1039/c9ra03248g
   Zhang GQ, 2021, IEEE T IMAGE PROCESS, V30, P8913, DOI 10.1109/TIP.2021.3120054
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhou M., 2018, 3d-printed gun controversy: Everything you need to know
NR 63
TC 2
Z9 2
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8240
EP 8252
DI 10.1109/TMM.2022.3233764
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000009
DA 2024-07-18
ER

PT J
AU Sun, QP
   Xiao, Y
   Zhang, J
   Zhou, SZ
   Leung, CS
   Su, X
AF Sun, Qingping
   Xiao, Yi
   Zhang, Jie
   Zhou, Shizhe
   Leung, Chi-Sing
   Su, Xin
TI A Local Correspondence-Aware Hybrid CNN-GCN Model for Single-Image Human
   Body Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D human reconstruction; monocular image; convolutional neural networks;
   graph convolutional neural networks; SMPL
ID SHAPE ESTIMATION; POSE; TRACKING
AB Reconstructing a 3D human body mesh from a monocular image is a challenging inverse problem because of occlusion and complicated human articulations. Recent deep learning-based methods have made significant progress in single-image human reconstruction. Most of these works are either model-based methods or model-free methods. However, model-based methods always suffer detail losses due to the limited parameter space, and model-free methods are hard to directly recover satisfactory results from images due to the use of a shared global feature for all vertices and the domain gap between 2D regular images and 3D irregular meshes. To resolve these issues, we propose a hybrid model, which combines the advantages of both model based approach and model-free approach to estimate a 3D human mesh in a coarse-to fine manner. Initially, we utilize a convolutional neural network (CNN) to estimate the parameters of a Skinned Multi-Person Linear Model (SMPL), which allows us to generate a coarse human mesh. After that, the vertex coordinates of the coarse human mesh are further refined by a graph convolutional neural network (GCN). Unlike previous GCN-based methods, whose vertex coordinates are recovered from a shared global feature, we propose a LOcal CorRespondence-Aware (LOCRA) module to extract local special features for each vertex. To make the local features related to the human pose, we also add a keypoint-related loss to supervise the training process of the LOCRA module. Experiments demonstrate that our hybrid model with the LOCRA module outperforms existing methods on multiple public benchmarks.
C1 [Sun, Qingping; Zhang, Jie; Zhou, Shizhe] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Xiao, Yi] Hunan Univ, Sch Design, Changsha 410006, Peoples R China.
   [Leung, Chi-Sing] City Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
   [Su, Xin] Hunan Police Acad, Hunan Prov Key Lab Network Invest Technol, Changsha 410138, Peoples R China.
C3 Hunan University; Hunan University; City University of Hong Kong
RP Xiao, Y (corresponding author), Hunan Univ, Sch Design, Changsha 410006, Peoples R China.
EM sunqingping@hnu.edu.cn; yixiao1984@gmail.com; jie_zhang@hnu.edu.cn;
   shizhe@hnu.edu.cn; eeleungc@cityu.edu.hk; suxin@hnu.edu.cn
FU National Key R&D Program of China [2021YFF0900604]; NSFC from PRC
   [61872137, 62076090]; science and technology innovation Program of Hunan
   Province [2020JJ4009]; Hunan NSF [2021RC3064]
FX The work was supported in part by the National Key R&D Program of China
   under Grant 2021YFF0900604), in part by NSFC from PRC under Grants
   61872137 and 62076090, in part by the science and technology innovation
   Program of Hunan Province under Grant 2021RC3064, and in part by Hunan
   NSF under Grant 2020JJ4009.
CR Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Arnab A, 2019, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2019.00351
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Choi H., 2020, COMPUTER VISION ECCV, P769
   Choutas Vasileios, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P20, DOI 10.1007/978-3-030-58607-2_2
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M., 2010, BMVC, V2, P5
   Gabeur V, 2019, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2019.00232
   Ge XH, 2017, IEEE T MULTIMEDIA, V19, P2345, DOI 10.1109/TMM.2017.2733461
   GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478
   Guan P, 2009, IEEE I CONF COMP VIS, P1381, DOI 10.1109/iccv.2009.5459300
   Güler RA, 2019, PROC CVPR IEEE, P10876, DOI 10.1109/CVPR.2019.01114
   Habermann M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3311970
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu PP, 2022, IEEE T MULTIMEDIA, V24, P2139, DOI 10.1109/TMM.2021.3076340
   Huang YH, 2017, INT CONF 3D VISION, P421, DOI 10.1109/3DV.2017.00055
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jiang HY, 2019, IEEE I CONF COMP VIS, P5430, DOI 10.1109/ICCV.2019.00553
   Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868
   Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kipf T. N., 2017, P INT C LEARN REPR, P11313
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Kundu Jogendra Nath, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P794, DOI 10.1007/978-3-030-58452-8_46
   Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Liang JB, 2019, IEEE I CONF COMP VIS, P4351, DOI 10.1109/ICCV.2019.00445
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Z, 2018, IEEE T MULTIMEDIA, V20, P1321, DOI 10.1109/TMM.2017.2767781
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lu Y, 2021, IEEE T MULTIMEDIA, V23, P3657, DOI 10.1109/TMM.2020.3029941
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062
   Osman Ahmed A. A., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P598, DOI 10.1007/978-3-030-58539-6_36
   Pavlakos G, 2019, IEEE I CONF COMP VIS, P803, DOI 10.1109/ICCV.2019.00089
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Pumarola A, 2019, IEEE I CONF COMP VIS, P2242, DOI [10.1109/ICCV.2019.00233, 10.1109/ICCV.2019.2019.00233]
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Rhodin H, 2016, LECT NOTES COMPUT SC, V9909, P509, DOI 10.1007/978-3-319-46454-1_31
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Rong Y, 2019, IEEE I CONF COMP VIS, P5339, DOI 10.1109/ICCV.2019.00544
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Sekhavat YA, 2017, IEEE T MULTIMEDIA, V19, P1041, DOI 10.1109/TMM.2016.2639380
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun Y, 2019, IEEE I CONF COMP VIS, P5348, DOI 10.1109/ICCV.2019.00545
   Tan Vince, 2017, BMVC
   Tang SC, 2019, IEEE I CONF COMP VIS, P7749, DOI 10.1109/ICCV.2019.00784
   Tiancheng Zhi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P492, DOI 10.1007/978-3-030-58607-2_29
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Tung HYF, 2017, ADV NEUR IN, V30
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   von Marcard T, 2017, COMPUT GRAPH FORUM, V36, P349, DOI 10.1111/cgf.13131
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Weiss A, 2011, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2011.6126465
   Wen C, 2019, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2019.00113
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xu HY, 2020, PROC CVPR IEEE, P6183, DOI 10.1109/CVPR42600.2020.00622
   Xu J, 2021, IEEE T MULTIMEDIA, V23, P2222, DOI 10.1109/TMM.2021.3070972
   Xu L, 2020, IEEE T PATTERN ANAL, V42, P2508, DOI 10.1109/TPAMI.2019.2915229
   Xu YL, 2019, IEEE I CONF COMP VIS, P7759, DOI 10.1109/ICCV.2019.00785
   Xu ZY, 2021, IEEE T MULTIMEDIA, V23, P1542, DOI 10.1109/TMM.2020.3001540
   Yu T, 2018, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR.2018.00761
   Yu T, 2017, IEEE I CONF COMP VIS, P910, DOI 10.1109/ICCV.2017.104
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229
   Zeng W, 2020, PROC CVPR IEEE, P7052, DOI 10.1109/CVPR42600.2020.00708
   Zhang HW, 2022, IEEE T PATTERN ANAL, V44, P2610, DOI 10.1109/TPAMI.2020.3042341
   Zhang JY, 2019, IEEE I CONF COMP VIS, P7113, DOI 10.1109/ICCV.2019.00721
   Zhao TH, 2019, IEEE T MULTIMEDIA, V21, P114, DOI 10.1109/TMM.2018.2844087
   Zheng ZR, 2018, LECT NOTES COMPUT SC, V11213, P389, DOI 10.1007/978-3-030-01240-3_24
   Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589
   Zhu H, 2019, PROC CVPR IEEE, P4486, DOI 10.1109/CVPR.2019.00462
   Zuo XX, 2021, IEEE T MULTIMEDIA, V23, P1617, DOI 10.1109/TMM.2020.3001506
NR 84
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4679
EP 4690
DI 10.1109/TMM.2022.3180218
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300006
DA 2024-07-18
ER

PT J
AU Tan, CC
   Gu, GH
   Ruan, T
   Wei, SK
   Zhao, Y
AF Tan, Chuangchuang
   Gu, Guanghua
   Ruan, Tao
   Wei, Shikui
   Zhao, Yao
TI Dual-Gradients Localization Framework With Skip-Layer Connections for
   Weakly Supervised Object Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Weakly supervised object localization; pixel-level class selection;
   class-aware enhanced maps; gradients of loss function; gradients of
   target class; skip-layer connections
ID NETWORK
AB This article focuses on generating object locations in a given image while only using image-level annotations. Towards this end, we present a simple and effective training-free framework, named Dual-Gradients Localization (DGL) framework. The key idea of the proposed DGL framework is to leverage two kinds of gradients to achieve precise localization on any convolutional layer of a classification model during the testing stage. Concretely, the DGL framework is developed based on two branches: 1) Pixel-level Class Selection, leveraging gradients of the target class to identify the correlation ratio of pixels to the target class within any convolutional feature maps, and 2) Class-aware Enhanced Maps, utilizing linear relationship in gradients of the classification loss function to mine entire target object regions. To further polish the details of objects, we apply the skip-layer connections to the classification model, which concatenates the high- and low-level layers to achieve classification. In such a case, DGL with Skip-layer Connections (DGL-SC) can capture more edge information on the high-level layer. In addition, we propose a Localization Maps Selection method to evaluate the quality of the localization map and provide a way for automatically selecting localization maps produced on different layers. Extensive experiments on public ILSVRC and CUB-200-2011 datasets show the effectiveness of the proposed DGL framework. Especially, our DGL-SC obtains a new state-of-the-art gt-known localization error of 27.35% on the ILSVRC benchmark.
C1 [Tan, Chuangchuang; Ruan, Tao; Wei, Shikui; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Tan, Chuangchuang; Ruan, Tao; Wei, Shikui; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Gu, Guanghua] Yanshan Univ, Sch Informat Sci & Engn, Hebei Key Lab Informat Transmiss & Signal Proc, Qinhuangdao 066004, Peoples R China.
C3 Beijing Jiaotong University; Yanshan University
RP Zhao, Y (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM chuangchuangtan@aliyun.com; guguanghua@ysu.edu.cn; 16112064@bjtu.edu.cn;
   shkwei@bjtu.edu.cn; yzhao@bjtu.edu.cn
OI Ruan, Tao/0000-0002-6718-7223; Zhao, Yao/0000-0002-8581-9554
FU National Key Research and Development of China [2018AAA0102100];
   National Natural Science Foundation of China [61972022, 62072394,
   U1936212, 62120106009]; Natural Science Foundation of Hebei province
   [F2021203019]
FX This work was supported in part by the National Key Research and
   Development of China under Grant 2018AAA0102100,in part by the National
   Natural Science Foundation of China under Grants 61972022, U1936212,
   62072394, and 62120106009, and in part by the Natural Science Foundation
   of Hebei province under Grant F2021203019
CR Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Choe J, 2020, PROC CVPR IEEE, P3130, DOI 10.1109/CVPR42600.2020.00320
   Choe J, 2019, PROC CVPR IEEE, P2214, DOI 10.1109/CVPR.2019.00232
   Dong XY, 2019, IEEE T PATTERN ANAL, V41, P1641, DOI 10.1109/TPAMI.2018.2844853
   Gao W, 2021, Arxiv, DOI arXiv:2103.14862
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang PT, 2021, IEEE T IMAGE PROCESS, V30, P5875, DOI 10.1109/TIP.2021.3089943
   Jiang PT, 2019, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2019.00216
   Jie ZQ, 2017, PROC CVPR IEEE, P4294, DOI 10.1109/CVPR.2017.457
   Li XY, 2019, IEEE I CONF COMP VIS, P9734, DOI 10.1109/ICCV.2019.00983
   Lin M, 2014, Arxiv, DOI [arXiv:1312.4400, DOI 10.48550/ARXIV.1312.4400]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mai J., 2020, P IEEE CVF C COMP VI, P8766
   Pan XJ, 2021, PROC CVPR IEEE, P11637, DOI 10.1109/CVPR46437.2021.01147
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sermanet P, 2014, Arxiv, DOI arXiv:1312.6229
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Su HS, 2021, IEEE T MULTIMEDIA, V23, P1503, DOI 10.1109/TMM.2020.2999184
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Tao QY, 2019, IEEE T MULTIMEDIA, V21, P1135, DOI 10.1109/TMM.2018.2875597
   Wah C, 2011, CALTECH UCSD BIRDS 2
   Wan F, 2019, PROC CVPR IEEE, P2194, DOI 10.1109/CVPR.2019.00230
   Wan F, 2019, IEEE T PATTERN ANAL, V41, P2395, DOI 10.1109/TPAMI.2019.2898858
   Wei J, 2021, PROC CVPR IEEE, P5989, DOI 10.1109/CVPR46437.2021.00593
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Weizeng Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P481, DOI 10.1007/978-3-030-58574-7_29
   Wu T, 2021, PROC CVPR IEEE, P16760, DOI 10.1109/CVPR46437.2021.01649
   Xie JH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P132, DOI 10.1109/ICCV48922.2021.00020
   Xue HL, 2019, IEEE I CONF COMP VIS, P6588, DOI 10.1109/ICCV.2019.00669
   Yang S, 2020, IEEE WINT CONF APPL, P2930, DOI 10.1109/WACV45572.2020.9093566
   Yang ZH, 2019, PROC CVPR IEEE, P2912, DOI 10.1109/CVPR.2019.00303
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang BF, 2020, AAAI CONF ARTIF INTE, V34, P12765
   Zhang HM, 2021, IEEE T MULTIMEDIA, V23, P2033, DOI 10.1109/TMM.2020.3007352
   Zhang SW, 2020, IEEE T MULTIMEDIA, V22, P2610, DOI 10.1109/TMM.2019.2959425
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang X, 2021, AUTOPHAGY, V17, P1519, DOI 10.1080/15548627.2020.1840796
   Zhang XL, 2020, Arxiv, DOI arXiv:2006.05220
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhang XL, 2018, LECT NOTES COMPUT SC, V11216, P610, DOI 10.1007/978-3-030-01258-8_37
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
   Zhu Y, 2017, IEEE I CONF COMP VIS, P1859, DOI 10.1109/ICCV.2017.204
NR 56
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4933
EP 4942
DI 10.1109/TMM.2022.3184486
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300024
DA 2024-07-18
ER

PT J
AU Tan, X
   Chen, HA
   Xu, K
   Jin, Y
   Zhu, CA
AF Tan, Xiao
   Chen, Huaian
   Xu, Kai
   Jin, Yi
   Zhu, Changan
TI Deep SR-HDR: Joint Learning of Super-Resolution and High Dynamic Range
   Imaging for Dynamic Scenes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic scene; high dynamic range image; super-resolution
ID RECONSTRUCTION; IMAGES
AB The visual quality of a single image captured by a digital camera usually suffers from limited spatial resolution and low dynamic range (LDR) due to sensor constraints. To address these problems, recent works have independently applied convolutional neural networks (CNNs) to super-resolution (SR) and high dynamic range (HDR) imaging and made significant improvements in visual quality. However, directly connecting SR and HDR networks is an inefficient way to enhance image quality, because these two tasks share most of the same processing steps. To this end, we propose a deep neural network for the joint task of SR and HDR imaging, termed Deep SR-HDR, which reconstructs a high-resolution (HR) HDR image from a set of differently exposed low-resolution (LR) LDR images of a dynamic scene. Specifically, we merge the shared processing steps, including feature extraction and alignment of these two tasks. In particular, to handle large-scale complex motions, we design a multi-scale deformable module (MSDM) that estimates the sampling location offsets in a coarse-to-fine manner and then flexibly integrates useful information to compensate for the missing content in the motion regions. Then, we divide the fusion stage into two branches for HDR generation and high-frequency information extraction. With the cooperation and interactions of these modules, the proposed network reconstructs high-quality HR HDR images. Extensive qualitative and quantitative experimental results demonstrate the superiority and high efficiency of the proposed network.
C1 [Tan, Xiao; Chen, Huaian; Xu, Kai] Univ Sci & Technol China, Sch Engn Sci, Hefei 230022, Anhui, Peoples R China.
   [Jin, Yi; Zhu, Changan] Univ Sci & Technol China, Sch Engn Sci, Hefei 230022, Anhui, Peoples R China.
   [Jin, Yi; Zhu, Changan] Univ Sci & Technol China, Sch Data Sci, Hefei 230022, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS
RP Jin, Y (corresponding author), Univ Sci & Technol China, Sch Engn Sci, Hefei 230022, Anhui, Peoples R China.; Jin, Y (corresponding author), Univ Sci & Technol China, Sch Data Sci, Hefei 230022, Anhui, Peoples R China.
EM tx2015@mail.ustc.edu.cn; anchen@mail.ustc.edu.cn;
   xukaikai@mail.ustc.edu.cn; jinyi08@ustc.edu.cn; changan@ustc.edu.cn
RI Zhu, ChangAn/KIL-0881-2024
OI Chen, Huaian/0000-0003-3999-2206; Xu, Kai/0000-0003-0191-0375
FU National Natural Science Foundation of China [61727809]; Special Fund
   for Key Program of Science and Technology of Anhui Province
   [201903c08020002]; National Key Research and Development Program of
   China [2019YFC0117800]
FX This work was supported in part by the National Natural Science
   Foundation of China 61727809, in part by the Special Fund for Key
   Program of Science and Technology of Anhui Province 201903c08020002, and
   in part by the National Key Research and Development Program of China
   2019YFC0117800.
CR Aydin TO, 2008, PROC SPIE, V6806, DOI 10.1117/12.765095
   Bengtsson T, 2013, INT CONF ACOUST SPEE, P2212, DOI 10.1109/ICASSP.2013.6638047
   Bengtsson T, 2012, INT CONF ACOUST SPEE, P1097, DOI 10.1109/ICASSP.2012.6288078
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Chen HA, 2021, IEEE T IND INFORM, V17, P5369, DOI 10.1109/TII.2020.3024187
   Choi JS, 2009, COMPUT J, V52, P114, DOI 10.1093/comjnl/bxm080
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Gunturk BK, 2006, IEEE SIGNAL PROC LET, V13, P197, DOI 10.1109/LSP.2005.863693
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heide F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661260
   Hu J, 2013, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2013.154
   Isobe Takashi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P645, DOI 10.1007/978-3-030-58610-2_38
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kalantari NK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073609
   Kang SB, 2003, ACM T GRAPHIC, V22, P319, DOI 10.1145/882262.882270
   Khan EA, 2006, IEEE IMAGE PROC, P2005, DOI 10.1109/ICIP.2006.312892
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim Seijoon, 2020, P AAAI C ART INT
   Kim SY, 2019, IEEE I CONF COMP VIS, P3116, DOI 10.1109/ICCV.2019.00321
   Kim Soo Ye, 2018, P AS C COMP VIS, P379
   Kingma D. P., 2014, arXiv
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu Ce, 2009, THESIS
   Liu D, 2017, IEEE I CONF COMP VIS, P2526, DOI 10.1109/ICCV.2017.274
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Niu YZ, 2021, IEEE T IMAGE PROCESS, V30, P3885, DOI 10.1109/TIP.2021.3064433
   Oh TH, 2015, IEEE T PATTERN ANAL, V37, P1219, DOI 10.1109/TPAMI.2014.2361338
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Prabhakar K. Ram, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P497, DOI 10.1007/978-3-030-58589-1_30
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Schubert F., 2009, WACV, P1
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Tocci MD, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964936
   Tomaszewska A, 2007, WSCG 2007, FULL PAPERS PROCEEDINGS I AND II, P49
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Traonmilin Y, 2014, SIAM J IMAGING SCI, V7, P1624, DOI 10.1137/130946903
   Tursun OT, 2016, COMPUT GRAPH FORUM, V35, P139, DOI 10.1111/cgf.12818
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wu SZ, 2018, LECT NOTES COMPUT SC, V11206, P120, DOI 10.1007/978-3-030-01216-8_8
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yan QS, 2020, IEEE T IMAGE PROCESS, V29, P4308, DOI 10.1109/TIP.2020.2971346
   Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185
   Zeng H., 2020, ADV NEUR IN, V8, p182 815
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang XY, 2021, IEEE T MULTIMEDIA, V23, P1924, DOI 10.1109/TMM.2020.3005025
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zheng JH, 2013, IEEE T IMAGE PROCESS, V22, P5190, DOI 10.1109/TIP.2013.2283401
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
   Zimmer H, 2011, COMPUT GRAPH FORUM, V30, P405, DOI 10.1111/j.1467-8659.2011.01870.x
NR 62
TC 7
Z9 7
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 750
EP 763
DI 10.1109/TMM.2021.3132165
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900006
DA 2024-07-18
ER

PT J
AU Verma, A
   Subramanyam, AV
   Wang, Z
   Satoh, S
   Shah, RR
AF Verma, Astha
   Subramanyam, A. V.
   Wang, Zheng
   Satoh, Shin'ichi
   Shah, Rajiv Ratn
TI Unsupervised Domain Adaptation for Person Re-Identification Via
   Individual-Preserving and Environmental-Switching Cyclic Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptation models; Cameras; Training; Data models; Task analysis;
   Generative adversarial networks; Feature extraction; Person
   re-identification; domain adaptation; GAN
ID NETWORK
AB Unsupervised domain adaptation for person re-identification (Re-ID) suffers severe domain discrepancies between source and target domains. To reduce the domain shift caused by the changes of context, camera style, or viewpoint, existing methods in this field fine-tune and adapt the Re-ID model with augmented samples, either translating source samples to the target style or assigning pseudo labels to the target. The former methods may lose identity details but keep redundant source background during translation. In contrast, the latter techniques may give noisy labels when the model meets the unseen background and person pose. We mitigate the domain shift in the former translation direction by cyclically decoupling environment and identity-related features. We propose a novel individual-preserving and environmental-switching cyclic generation network (IPES-GAN). Our network has the following distinct features: 1) Decoupled features instead of fused features: we encode the images into an individual part and an environmental part, which are proved beneficial to generation and adaptation; 2) Cyclic generation instead of one-step adaptive generation. We swap source and target environment features to generate cross-domain images with preserved identity-related features conditioned with source (target) background features and then changed again to generate back the input image so that cyclic generation runs in a self-supervised way. Experiments carried out on two significant benchmarks: Market-1501 and DukeMTMC-Reid, reveal state-of-the-art performance.
C1 [Verma, Astha; Subramanyam, A. V.] Dept Elect & Commun Engn, Delhi 110020, India.
   [Shah, Rajiv Ratn] Indraprastha Inst Informat Technol, Dept Comp Sci & Engn, Delhi 110020, India.
   [Wang, Zheng; Satoh, Shin'ichi] Natl Inst Informat, Digital Content & Media Sci Res Div, Chiyoda ku, Tokyo 1018430, Japan.
C3 Indraprastha Institute of Information Technology Delhi; Research
   Organization of Information & Systems (ROIS); National Institute of
   Informatics (NII) - Japan
RP Verma, A (corresponding author), Dept Elect & Commun Engn, Delhi 110020, India.
EM asthav@iiitd.ac.in; subramanyam@iiitd.ac.in; wangz@nii.ac.jp;
   satoh@nii.ac.jp; rajivratn@iiitd.ac.in
OI venkata, subramanyam/0000-0002-8873-4644; Satoh,
   Shin'ichi/0000-0001-6995-6447; Verma, Astha/0000-0003-3615-5373
FU NII International Internship Program; DST Govt. of India
   [ECR/2018/002449]; JSPS [18F18378]; JST CREST [JPMJCR1686]
FX This work was supported in part by NII International Internship Program,
   in part by DST Govt. of India under Grant ECR/2018/002449, in part by
   Grant-in-Aid for JSPS Fellows under Grant 18F18378, and in part by JST
   CREST under Grant JPMJCR1686.
CR Bai S, 2017, AAAI CONF ARTIF INTE, P1281
   Bak S, 2018, LECT NOTES COMPUT SC, V11217, P193, DOI 10.1007/978-3-030-01261-8_12
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Coifman B, 1998, TRANSPORT RES REC, P181
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Dong H., 2018, NeurIPS, P474
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ge YX, 2018, ADV NEUR IN, V31
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang Y, 2019, IEEE I CONF COMP VIS, P9526, DOI 10.1109/ICCV.2019.00962
   Huang YK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P365, DOI 10.1145/3343031.3350994
   Khatun A, 2021, IEEE T INF FOREN SEC, V16, P3803, DOI 10.1109/TIFS.2021.3088012
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1492, DOI 10.1145/3240508.3240674
   Li M., 2018, PROC EUR C COMPUT, P737
   Li YY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2115, DOI 10.1145/3343031.3350982
   Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin S, 2018, Arxiv, DOI arXiv:1807.01440
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lv JM, 2018, PROC CVPR IEEE, P7948, DOI 10.1109/CVPR.2018.00829
   Ma LQ, 2017, ADV NEUR IN, V30
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146
   Ren CX, 2020, IEEE T INF FOREN SEC, V15, P1290, DOI 10.1109/TIFS.2019.2939750
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tang H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2052, DOI 10.1145/3343031.3350980
   Tang YZ, 2020, IEEE T IMAGE PROCESS, V29, P5641, DOI 10.1109/TIP.2020.2985545
   Tzeng E, 2014, Arxiv, DOI [arXiv:1412.3474, 10.48550/arXiv.1412.3474, DOI 10.48550/ARXIV.1412.3474]
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang Z, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1267, DOI 10.1145/2733373.2806400
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu AC, 2019, IEEE I CONF COMP VIS, P6921, DOI 10.1109/ICCV.2019.00702
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xie S., 2018, P 35 INT C MACHINE L, P5423
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yao ZH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1842, DOI 10.1145/3343031.3350908
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao CR, 2021, IEEE T IMAGE PROCESS, V30, P4212, DOI 10.1109/TIP.2021.3070182
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zhao CR, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107014
   Zhao CR, 2019, PATTERN RECOGN LETT, V117, P161, DOI 10.1016/j.patrec.2018.04.029
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 69
TC 20
Z9 20
U1 3
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 364
EP 377
DI 10.1109/TMM.2021.3126404
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800003
DA 2024-07-18
ER

PT J
AU Wang, GA
   Wang, YZ
   Gu, RS
   Hu, WJ
   Hwang, JN
AF Wang, Gaoang
   Wang, Yizhou
   Gu, Renshu
   Hu, Weijie
   Hwang, Jenq-Neng
TI Split and Connect: A Universal Tracklet Booster for Multi-Object
   Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention; embedding; multi-object tracking
ID MULTITARGET TRACKING
AB Multi-object tracking (MOT) is an essential task in the computer vision field. With the fast development of deep learning technology in recent years, MOT has achieved great improvement. However, some challenges still remain, such as sensitiveness to occlusion, instability under different lighting conditions, and non-robustness to deformable objects, causing incorrect temporal associations. To address such common challenges in most of the existing trackers, in this paper, a tracklet booster (TBooster) algorithm is proposed to correct the association errors resulting from existing trackers. The correction of the association error from TBooster has two folds: split tracklets on potential ID-change positions and then connect multiple tracklets into one if they are from the same object. To achieve this goal, the TBooster consists of two components, i.e., Splitter and Connector. In Splitter, an architecture with stacked temporal dilated convolution blocks is employed for the splitting position prediction via label smoothing strategy with adaptive Gaussian kernels. In Connector, a multi-head self-attention-based encoder is exploited for the tracklet embedding, which is further used to connect tracklets into full tracks. We conduct sufficient experiments on MOT17 and MOT20 benchmark datasets and achieve promising results. Combined with the proposed tracklet booster, existing trackers can achieve large improvements on the IDF1 score, which shows the effectiveness of the proposed TBooster.
C1 [Wang, Gaoang] Zhejiang Univ, Univ Illinois Urbana Champaign Inst ZJU UIUC, Haining, Zhejiang, Peoples R China.
   [Wang, Gaoang] Zhejiang Univ, Coll Comp Sci & Technol ogy, Haining 310027, Zhejiang, Peoples R China.
   [Wang, Yizhou; Hwang, Jenq-Neng] Univ Washington, Seattle, WA 98195 USA.
   [Gu, Renshu] Hangzhou Dianzi Univ, Hangzhou 310018, Peoples R China.
   [Hu, Weijie] Guangdong Univ Petrochem Technol, Maoming 525000, Guangdong, Peoples R China.
C3 Zhejiang University; Zhejiang University; University of Washington;
   University of Washington Seattle; Hangzhou Dianzi University; Guangdong
   University of Petrochemical Technology
RP Wang, GA (corresponding author), Zhejiang Univ, Univ Illinois Urbana Champaign Inst ZJU UIUC, Haining, Zhejiang, Peoples R China.
EM gaoangwang@intl.zju.edu.cn; ywang26@uw.edu; renshugu@hdu.edu.cn;
   huweijie@gdupt.edu.cn; hwang@uw.edu
RI Wang, Yizhou/ACX-1036-2022; WANG, GAOANG/GNM-8993-2022
OI Wang, Yizhou/0000-0001-9692-6235; Hwang, Jenq-Neng/0000-0002-8877-2421;
   Wang, Gaoang/0000-0002-8403-1538; Gu, Renshu/0000-0002-3900-2148; Hu,
   Weijie/0000-0002-2392-3284
FU National Natural Science Foundation of China [62106219]; Fundamental
   Research Funds for the Central Universities [2021QN81017]; Guangdong
   Provincial Special Funding projects for Introducing Innovation Team and
   Industry University Research Cooperation [2019C002001]; Guangdong
   Maoming Science and Technology Special Industry-University Research
   Integration Project [2020576]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62106219, in part by the Fundamental
   Research Funds for the Central Universities under Grant 2021QN81017, in
   part by the Guangdong Provincial Special Funding projects for
   Introducing Innovation Team and Industry University Research Cooperation
   under Grant 2019C002001, and in part by the Guangdong Maoming Science
   and Technology Special Industry-University Research Integration Project
   under Grant 2020576.
CR Andriyenko A, 2012, PROC CVPR IEEE, P1926, DOI 10.1109/CVPR.2012.6247893
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bochinski E, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Brasó G, 2020, PROC CVPR IEEE, P6246, DOI 10.1109/CVPR42600.2020.00628
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chaabane M, 2021, Arxiv, DOI arXiv:2102.02267
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347
   Chu P, 2021, Arxiv, DOI arXiv:2104.00194
   Chuang MC, 2017, IEEE T SYST MAN CY-S, V47, P2467, DOI 10.1109/TSMC.2016.2523943
   Dai P, 2021, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR46437.2021.00247
   Dawkins M, 2017, IEEE WINT CONF APPL, P898, DOI 10.1109/WACV.2017.105
   Dendorfer P., 2020, arXiv
   Dosovitskiy Alexey, 2020, ABS201011929 CORR
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Feng WT, 2020, Arxiv, DOI arXiv:2009.10338
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Gu RS, 2021, INT C PATT RECOG, P8243, DOI 10.1109/ICPR48806.2021.9412107
   Gu RS, 2020, IEEE T CIRC SYST VID, V30, P4245, DOI 10.1109/TCSVT.2019.2953678
   Gu RS, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P163, DOI 10.1109/MIPR.2019.00036
   Guo M, 2018, LECT NOTES COMPUT SC, V11205, P673, DOI 10.1007/978-3-030-01246-5_40
   Han RZ, 2020, AAAI CONF ARTIF INTE, V34, P10917
   He JW, 2021, PROC CVPR IEEE, P5295, DOI 10.1109/CVPR46437.2021.00526
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hornakova A, 2020, PR MACH LEARN RES, V119
   Hsu H.M., 2019, P CVPR WORKSH LONG B, P416
   Hu HN, 2019, IEEE I CONF COMP VIS, P5389, DOI 10.1109/ICCV.2019.00549
   Jagadeesh B., 2018, INT J PURE APPL MATH, V118, P2637
   Jalal A, 2019, INT BHURBAN C APPL S, P371, DOI 10.1109/IBCAST.2019.8667145
   Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9
   Karthik S, 2020, Arxiv, DOI arXiv:2006.02609
   Kim DY, 2014, INFORM SCIENCES, V278, P641, DOI 10.1016/j.ins.2014.03.080
   Kumar R, 2015, LECT NOTES COMPUT SC, V9006, P445, DOI 10.1007/978-3-319-16817-3_29
   Lee B, 2016, LECT NOTES COMPUT SC, V9914, P68, DOI 10.1007/978-3-319-48881-3_6
   Lenz P, 2015, IEEE I CONF COMP VIS, P4364, DOI 10.1109/ICCV.2015.496
   Lu YY, 2017, IEEE I CONF COMP VIS, P2363, DOI 10.1109/ICCV.2017.257
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Meinhardt T, 2022, Arxiv, DOI arXiv:2101.02702
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Milan A, 2017, AAAI CONF ARTIF INTE, P4225
   Milan A, 2016, IEEE T PATTERN ANAL, V38, P2054, DOI 10.1109/TPAMI.2015.2505309
   Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103
   Narasimhan M, 2018, ADV NEUR IN, V31
   Pang B, 2020, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR42600.2020.00634
   Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023
   Papakis I, 2020, arXiv
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren WH, 2021, IEEE T IMAGE PROCESS, V30, P1439, DOI 10.1109/TIP.2020.3044219
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41
   Shan CB, 2020, Arxiv, DOI arXiv:2010.09015
   Shen JB, 2018, IEEE T INTELL TRANSP, V19, P162, DOI 10.1109/TITS.2017.2750082
   Sun PZ, 2021, Arxiv, DOI arXiv:2012.15460
   Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394
   Tang SY, 2015, PROC CVPR IEEE, P5033, DOI 10.1109/CVPR.2015.7299138
   Tang Z, 2019, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2019.00900
   Tang Z, 2018, IEEE COMPUT SOC CONF, P108, DOI 10.1109/CVPRW.2018.00022
   Valmadre J, 2021, Arxiv, DOI arXiv:2104.02631
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang G., 2019, P CVPR WORKSH, P382
   Wang GA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9856, DOI 10.1109/ICCV48922.2021.00973
   Wang GA, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P482, DOI 10.1145/3343031.3350853
   Wang GA, 2016, 2016 ICPR 2ND WORKSHOP ON COMPUTER VISION FOR ANALYSIS OF UNDERWATER IMAGERY (CVAUI 2016), P7, DOI [10.1109/CVAUI.2016.17, 10.1109/CVAUI.2016.014]
   Wang SF, 2017, INT J COMPUT VISION, V122, P484, DOI 10.1007/s11263-016-0960-z
   Wang YX, 2021, IEEE INT CONF ROBOT, P13708, DOI 10.1109/ICRA48506.2021.9561110
   Wen LY, 2014, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2014.167
   Weng XS, 2020, PROC CVPR IEEE, P6498, DOI 10.1109/CVPR42600.2020.00653
   Wu YP, 2020, IEEE T MULTIMEDIA, V22, P2177, DOI 10.1109/TMM.2019.2953380
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409
   Xu YL, 2017, AAAI CONF ARTIF INTE, P4299
   Xu YL, 2016, PROC CVPR IEEE, P4256, DOI 10.1109/CVPR.2016.461
   Yang F, 2016, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2016.234
   Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680
   Liu YP, 2020, NEUROCOMPUTING, V386, P18, DOI 10.1016/j.neucom.2019.12.037
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhang ZM, 2017, Arxiv, DOI [arXiv:1712.09531, DOI 10.48550/ARXIV.1712.09531, 10.48550/arXiv.1712.09531]
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7
   Zhou QQ, 2019, IEEE T MULTIMEDIA, V21, P1183, DOI 10.1109/TMM.2018.2875360
   Zhu X., 2021, PROC INT C LEARN REP
NR 83
TC 15
Z9 16
U1 2
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1256
EP 1268
DI 10.1109/TMM.2022.3140919
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, XS
   Jin, K
   Kong, Y
   Chen, CLP
   Cheng, YH
AF Wang, Xuesong
   Jin, Ke
   Kong, Yi
   Chen, C. L. Philip
   Cheng, Yuhu
TI Discriminator-Quality Evaluation GAN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Discriminator-quality; generative adversarial network; image quality;
   objective function
AB In existing generative adversarial networks (standard GAN and its variants), the discriminator is trained for recognizing the real data as positive while the generated data as negative. This kind of positive-negative classification criterion ignores the fact that the discriminator is a non-objective evaluator, which means that the image quality evaluated by the discriminator may fluctuate during the whole training progress. Considering this fact, we propose a novel GAN framework called Discriminator-Quality Evaluation GAN (DQE-GAN) by using the discriminator outputs to evaluate image quality. By dynamically classifying images into high discriminator-quality and low discriminator-quality samples, every adversarial iteration step can be more reasonable and objective. The convergence of DQE-GAN framework can be theoretically proved. Through extensive experiments, we demonstrate DQE-GANs' ability of achieving better generated images faster and more stable.
C1 [Wang, Xuesong; Jin, Ke; Kong, Yi; Cheng, Yuhu] China Univ Min & Technol, Engn ing Res Ctr Intelligent Control Underground S, Ministryof Educ, Xuzhou 221116, Peoples R China.
   [Jin, Ke] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
   [Kong, Yi] South China Univ Technol, Sch Comp Sci & Engn, Minist Educ, Guangzhou 510006, Peoples R China.
   [Chen, C. L. Philip] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
C3 China University of Mining & Technology; China University of Mining &
   Technology; South China University of Technology; South China University
   of Technology
RP Cheng, YH (corresponding author), China Univ Min & Technol, Engn ing Res Ctr Intelligent Control Underground S, Ministryof Educ, Xuzhou 221116, Peoples R China.
EM wangxuesongcumt@163.com; 584483121@qq.com; kongyicumt@163.com;
   philip.chen@ieee.org; chengyuhu@163.com
RI Chen, C. L. Philip/O-2657-2016
OI Ke, Jin/0000-0001-6513-3996
FU National Natural Science Foundation of China [61976215, 62176259,
   62006232]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61976215, 62176259, and 62006232. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Prof. Xiaochun Cao.(Corresponding author: Yuhu Cheng.)
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789
   Berthelot D, 2017, Arxiv, DOI arXiv:1703.10717
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Hoang T., 2019, P 7 INT C LEARN REPR
   Huang YF, 2021, IEEE T MULTIMEDIA, V23, P176, DOI 10.1109/TMM.2020.2981994
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jolicoeur-Martineau A., 2018, The relativistic discriminator: A key element missing from standard GAN
   Karnewar Animesh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7796, DOI 10.1109/CVPR42600.2020.00782
   Kingma D. P., 2014, arXiv
   Liu CX, 2021, IEEE T MULTIMEDIA, V23, P2843, DOI 10.1109/TMM.2020.3017924
   Liu S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P82, DOI 10.1145/3123266.3123431
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lucic M, 2018, ADV NEUR IN, V31
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Martin A., 2017, P 5 INT C LEARN REPR
   Miyato T, 2018, INT C LEARN REPR
   Paszke A., 2017, AUTOMATIC DIFFERENTI
   Radford A., 2015, ARXIV
   Sun YL, 2020, IEEE T INF FOREN SEC, V15, P2679, DOI 10.1109/TIFS.2020.2975921
   Tianyu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8382, DOI 10.1109/CVPR42600.2020.00841
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang Q, 2021, IEEE T MULTIMEDIA, V23, P429, DOI 10.1109/TMM.2020.2978633
   Xiangli Y., 2020, P INT C LEARN REPR
   Zhan HJ, 2021, IEEE T MULTIMEDIA, V23, P133, DOI 10.1109/TMM.2020.2978669
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhang T, 2021, IEEE T EVOLUT COMPUT, V25, P830, DOI 10.1109/TEVC.2021.3061466
   Zhang T, 2022, IEEE T KNOWL DATA EN, V34, P544, DOI 10.1109/TKDE.2020.2985365
   Zhang WW, 2008, LECT NOTES COMPUT SC, V5305, P802, DOI 10.1007/978-3-540-88693-8_59
   Zhao Junbo, 2017, ICLR
NR 32
TC 1
Z9 1
U1 8
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4081
EP 4093
DI 10.1109/TMM.2022.3171084
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W3PP1
UT WOS:001090784100001
DA 2024-07-18
ER

PT J
AU Wang, Y
   Chen, SW
AF Wang, Yu
   Chen, Shiwei
TI Multi-Agent Trajectory Prediction With Spatio-Temporal Sequence Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-modal trajectory prediction; saptio-temporal sequence fusion;
   generative adversarial networks; sequence-to-sequence
ID PEOPLE
AB Accurate trajectory prediction of surrounding agents is an important issue for building up an intelligent transportation system. Frequent interactions among agents have a major impact on their movement patterns. Current research mainly relies on agents' spatial structure associated with the last frame of the observation to model social interactions, while paying less attention to structure information from previous moments. In addition, existing methods merely consider temporal features of a single trajectory sequence, while neglecting temporal dependencies across multiple trajectories. In this work, we endeavor to capture comprehensively social interactions among agents with the proposed Spatio-Temporal Sequence Fusion Network (STSF-Net). Specifically, we construct a spatio-temporal sequence that encodes contextual information taking explicitly spatial distributions of agents during movement into account while capturing socially temporal dependencies across multiple trajectory sequences. Besides, a social recurrent mechanism is introduced to explicitly capture temporal correlations between interactions by concerning spatial structure at each time-step. Finally, our model is evaluated on datasets covering pedestrian, vehicle, and heterogeneous multi-agent trajectories. Experimental evidence manifests that our method achieves excellent performance.
C1 [Wang, Yu] Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Chen, Shiwei] Bilibili, Shanghai 200433, Peoples R China.
C3 Tongji University
RP Wang, Y (corresponding author), Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
EM yuwangtj@yeah.net; chenshiwei@bilibili.com
RI Wang, Yu/HLG-5572-2023
OI Wang, Yu/0000-0001-7099-4424
CR Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Amirian J, 2019, IEEE COMPUT SOC CONF, P2964, DOI 10.1109/CVPRW.2019.00359
   Bansal M, 2019, ROBOTICS: SCIENCE AND SYSTEMS XV
   Chen CG, 2019, IEEE INT CONF ROBOT, P6015, DOI [10.1109/ICRA.2019.8794134, 10.1109/icra.2019.8794134]
   Choi W, 2014, IEEE T PATTERN ANAL, V36, P1242, DOI 10.1109/TPAMI.2013.220
   Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16
   Colyar J., 2007, Tech. Rep. FHWA-HRT-07-030
   Dai SZ, 2019, IEEE ACCESS, V7, P38287, DOI 10.1109/ACCESS.2019.2907000
   Deo N, 2018, IEEE INT VEH SYM, P1179, DOI 10.1109/IVS.2018.8500493
   Deo N, 2018, IEEE COMPUT SOC CONF, P1549, DOI 10.1109/CVPRW.2018.00196
   Deo N, 2018, IEEE T INTELL VEHICL, V3, P129, DOI 10.1109/TIV.2018.2804159
   Dong X., 2018, EUR C COMPUT VIS ECC, P459
   Duh PJ, 2021, IEEE T MULTIMEDIA, V23, P1567, DOI 10.1109/TMM.2020.3001500
   Guan'an Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P275, DOI 10.1007/978-3-030-58598-3_17
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   Habibi G, 2020, IEEE COMPUT SOC CONF, P4411, DOI 10.1109/CVPRW50498.2020.00520
   Hasan I, 2021, IEEE T PATTERN ANAL, V43, P1267, DOI 10.1109/TPAMI.2019.2949414
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Huang YF, 2019, IEEE I CONF COMP VIS, P6281, DOI 10.1109/ICCV.2019.00637
   Kingma D. P., 2014, arXiv
   Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15
   Kosaraju V., 2019, Advances in Neural Information Processing Systems, P1
   Kuefler A, 2017, IEEE INT VEH SYM, P204, DOI 10.1109/IVS.2017.7995721
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Li Jiachen, 2020, NEURIPS
   Li SY, 2017, AAAI CONF ARTIF INTE, P4140
   Liang JW, 2019, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR.2019.00587
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P664, DOI 10.1109/TMM.2018.2863604
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Ma WC, 2017, PROC CVPR IEEE, P4636, DOI 10.1109/CVPR.2017.493
   Mohamed Abduallah, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14412, DOI 10.1109/CVPR42600.2020.01443
   Pellegrini S, 2010, LECT NOTES COMPUT SC, V6311, P452, DOI 10.1007/978-3-642-15549-9_33
   Robicquet A, 2016, LECT NOTES COMPUT SC, V9912, P549, DOI 10.1007/978-3-319-46484-8_33
   Sadeghian A, 2019, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2019.00144
   Sun JH, 2020, PROC CVPR IEEE, P657, DOI 10.1109/CVPR42600.2020.00074
   Tang YC, 2019, ADV NEUR IN, V32
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Yamaguchi K, 2011, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2011.5995468
   Zhang P, 2019, PROC CVPR IEEE, P12077, DOI 10.1109/CVPR.2019.01236
   Zhang SZ, 2021, IEEE T MULTIMEDIA, V23, P281, DOI 10.1109/TMM.2020.2977528
   Zhao DP, 2021, IEEE ROBOT AUTOM LET, V6, P628, DOI 10.1109/LRA.2020.3047771
   Zhao TY, 2019, PROC CVPR IEEE, P12118, DOI 10.1109/CVPR.2019.01240
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
NR 44
TC 6
Z9 6
U1 9
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 13
EP 23
DI 10.1109/TMM.2021.3120535
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400002
DA 2024-07-18
ER

PT J
AU Wu, FX
   Liu, L
   Hao, FS
   He, FX
   Cheng, J
AF Wu, Fuxiang
   Liu, Liu
   Hao, Fusheng
   He, Fengxiang
   Cheng, Jun
TI Language-Based Image Manipulation Built on Language-Guided Ranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Proposals; Task analysis; Generators; Visualization;
   Generative adversarial networks; Training; Text-based image
   manipulation; language-guided ranker; semantic mask
AB Text-based image manipulation is a popular subject and has many applications. However, it is a challenging task because there is no ground-truth edited dataset and textual descriptions have abstractive and ambiguous properties. To alleviate the difficult issues, we propose a manipulation framework consisting of the proposal attentional GANs, language-related semantic mask, and language-guided ranker. Specially, we construct an editing proposal generator to generate the suitable edited proposals with and without semantic conditions, which supports the reorganization of sub-generators to output proposals in various aspects as many as possible. To distinguish the text-relevant and the text-irrelevant regions, we introduce a language-related semantic mask based on the source image and target caption. Then, we exploit a language-guided ranker to retrieve the best edited result from the edited proposals through using the multi-modal similarity and the language-related semantic mask. Extensive experiments on widely-used datasets demonstrate that our model could manipulate images interactively and improve the editing quality effectively.
C1 [Wu, Fuxiang; Hao, Fusheng; Cheng, Jun] Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Prov Key Lab Robot & Intelligent Syst, Shenzhen, Guangdong, Peoples R China.
   [Wu, Fuxiang; Hao, Fusheng; Cheng, Jun] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [Liu, Liu] Univ Sydney, Sch Comp Sci, Fac Engn, Darlington, NSW 2008, Australia.
   [He, Fengxiang] JD Com Inc, JD Explore Acad, Beijing 100176, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese University of Hong Kong; University of Sydney
RP Cheng, J (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Prov Key Lab Robot & Intelligent Syst, Shenzhen, Guangdong, Peoples R China.
EM fx.wu1@siat.ac.cn; liu.liu1@sydney.edu.au; fs.hao@siat.ac.cn;
   fengxiang.f.he@gmail.com; jun.cheng@siat.ac.cn
OI liu, liu/0000-0002-8128-2788
FU National Natural Science Foundation of China [U21A20487]; Major Science
   and Technology Innovation 2030 "New Generation Artificial Intelligence"
   key Project [2021ZD0111700]; Australian Research Council [DP-180103424];
   Guangdong Technology Project [2016B010125003]; Shenzhen Technology
   Project [JCYJ20180507182610734, KCXFZ20201221173411032, Y795001001]; CAS
   Key Technology Talent Program
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant U21A20487, in part by Major Science and
   Technology Innovation 2030 "New Generation Artificial Intelligence" key
   Project 2021ZD0111700, in part by Australian Research Council Project
   under Grant DP-180103424, in part by Guangdong Technology Project under
   Grant 2016B010125003, in part by Shenzhen Technology Project under
   Grants JCYJ20180507182610734, KCXFZ20201221173411032, and Y795001001,
   and in part by CAS Key Technology Talent Program.
CR [Anonymous], 2015, P 14 INT C INT DES C, DOI [10.1145/2771839.2771910, DOI 10.1145/2771839.2771910]
   Barratt S., 2018, P INT C MACH LEARN W
   Bau D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323023
   Chen JB, 2018, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR.2018.00909
   Chen YC, 2019, PROC CVPR IEEE, P9851, DOI 10.1109/CVPR.2019.01009
   Cheng Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4383
   Collins E, 2020, PROC CVPR IEEE, P5770, DOI 10.1109/CVPR42600.2020.00581
   Dhamo H, 2020, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR42600.2020.00526
   Dorta G, 2020, PROC CVPR IEEE, P5355, DOI 10.1109/CVPR42600.2020.00540
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gunel M., 2018, P IEEE EUR C COMP VI
   Jun Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10908, DOI 10.1109/CVPR42600.2020.01092
   Jung H, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312773
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Lang YN, 2020, INT CONF ACOUST SPEE, P1968, DOI [10.1109/ICASSP40776.2020.9053880, 10.1109/icassp40776.2020.9053880]
   Ngo LM, 2022, IEEE T MULTIMEDIA, V24, P377, DOI 10.1109/TMM.2021.3050672
   Li B., 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P7880
   Li B., 2020, Advances in Neural Information Processing Systems, V33, P22020, DOI [10.48550/arxiv.2010.12136, DOI 10.48550/ARXIV.2010.12136]
   Li BW, 2019, ADV NEUR IN, V32
   Li BW, 2020, Arxiv, DOI arXiv:2002.05235
   Li K, 2019, IEEE I CONF COMP VIS, P4219, DOI 10.1109/ICCV.2019.00432
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Liang XD, 2018, LECT NOTES COMPUT SC, V11217, P574, DOI 10.1007/978-3-030-01261-8_34
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu BC, 2021, AAAI CONF ARTIF INTE, V35, P2082
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Liu YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1357, DOI 10.1145/3394171.3413505
   Liu ZH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P322, DOI 10.1145/3394171.3413777
   Nam S, 2018, ADV NEUR IN, V31
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Radford A, 2021, PR MACH LEARN RES, V139
   Reed S, 2016, PR MACH LEARN RES, V48
   Salimans T, 2016, ADV NEUR IN, V29
   Shi J, 2021, PROC CVPR IEEE, P13585, DOI 10.1109/CVPR46437.2021.01338
   Tang H, 2020, IEEE T IMAGE PROCESS, V29, P8916, DOI 10.1109/TIP.2020.3021789
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wu FX, 2022, IEEE T CYBERNETICS, V52, P568, DOI 10.1109/TCYB.2020.2979258
   Wu Y., 2019, Detectron 2
   Xihui Liu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P89, DOI 10.1007/978-3-030-58621-8_6
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang LS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1302, DOI 10.1145/3394171.3414017
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhou XR, 2019, PROC CVPR IEEE, P3658, DOI 10.1109/CVPR.2019.00378
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 47
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6219
EP 6231
DI 10.1109/TMM.2022.3207000
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500041
DA 2024-07-18
ER

PT J
AU Xu, CP
   Jia, WJ
   Cui, TC
   Wang, RM
   Zhang, YF
   He, XJ
AF Xu, Chengpei
   Jia, Wenjing
   Cui, Tingcheng
   Wang, Ruomei
   Zhang, Yuan-fang
   He, Xiangjian
TI Arbitrary-Shape Scene Text Detection via Visual-Relational Rectification
   and Contour Approximation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Arbitrary-shape scene text detection; bottom-up method; false
   positive/negative suppression; relational reasoning
ID NETWORK
AB One trend in the latest bottom-up approaches for arbitrary-shape scene text detection is to determine the links between text segments using Graph Convolutional Networks (GCNs). However, the performance of these bottom-up methods is still inferior to that of state-of-the-art top-down methods even with the help of GCNs. We argue that a cause of this is that bottom-up methods fail to make proper use of visual-relational features, which results in accumulated false detection, as well as the error-prone route-finding used for grouping text segments. In this paper, we improve classic bottom-up text detection frameworks by fusing the visual-relational features of text with two effective false positive/negative suppression (FPNS) mechanisms and developing a new shape-approximation strategy. First, dense overlapping text segments depicting the "characterness" and "streamline" properties of text are constructed and used in weakly supervised node classification to filter the falsely detected text segments. Then, relational features and visual features of text segments are fused with a novel Location-Aware Transfer (LAT) module and Fuse Decoding (FD) module to jointly rectify the detected text segments. Finally, a novel multiple-text-map-aware contour-approximation strategy is developed based on the rectified text segments, instead of the error-prone route-finding process, to generate the final contour of the detected text. Experiments conducted on five benchmark datasets demonstrate that our method outperforms the state-of-the-art performance when embedded in a classic text detection framework, which revitalizes the strengths of bottom-up methods.
C1 [Xu, Chengpei; Jia, Wenjing; Zhang, Yuan-fang; He, Xiangjian] Univ Technol Sydney, Fac Engn & IT, Sydney 2007, Australia.
   [He, Xiangjian] Univ Nottingham Ningbo China, Sch Comp Sci, Ningbo 315104, Peoples R China.
   [Cui, Tingcheng] Orbiseed Technol Inc, Toronto, ON, Canada.
   [Wang, Ruomei] Sun Yat sen Univ, Natl Engn Res Ctr Digital Life, Sch Data & Comp Sci, Guangzhou 510006, Peoples R China.
C3 University of Technology Sydney; University of Nottingham Ningbo China;
   Sun Yat Sen University
RP He, XJ (corresponding author), Univ Technol Sydney, Fac Engn & IT, Sydney 2007, Australia.; He, XJ (corresponding author), Univ Nottingham Ningbo China, Sch Comp Sci, Ningbo 315104, Peoples R China.
EM chengpei.xu@uts.edu.au; wenjing.jia@uts.edu.au; tingcheng@orbiseed.com;
   isswrm@mail.sysu.edu.cn; zyf.robinzhang@gmail.com;
   xiangjian.he@uts.edu.au
OI Jia, Wenjing/0000-0002-0940-3338; He, Xiangjian/0000-0001-8962-540X
FU National Key R&D Program of China [2018AAA0100300]; UTS FEIT Research
   Scholarship; China Natural Science Foundation [61976037]; Orbiseed
   Technology Inc.
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0100300, in part by UTS FEIT Research Scholarship, in
   part by China Natural Science Foundation under Grant 61976037, and in
   part by Orbiseed Technology Inc.
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Binmakhashen GM, 2020, ACM COMPUT SURV, V52, DOI 10.1145/3355610
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai PW, 2021, PROC CVPR IEEE, P7389, DOI 10.1109/CVPR46437.2021.00731
   Dai PW, 2022, IEEE T MULTIMEDIA, V24, P1883, DOI 10.1109/TMM.2021.3073575
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   Dikmen M, 2016, 8TH INTERNATIONAL CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS (AUTOMOTIVEUI 2016), P225, DOI 10.1145/3003715.3005465
   Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu H., 2020, Puzzlenet: scene text detection by segment context graph learning
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Ma CX, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107684
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Mishra A., 2019, 2019 INT C DOC AN RE, P947, DOI DOI 10.1109/ICDAR.2019.00156
   Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Shanyu Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P108, DOI 10.1007/978-3-030-58526-6_7
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K, 2015, IEEE INT C ICLR
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   Wang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P111, DOI 10.1145/3394171.3413819
   Wang FF, 2018, PROC CVPR IEEE, P1381, DOI 10.1109/CVPR.2018.00150
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P1316, DOI 10.1109/TMM.2020.2995290
   Wang ZD, 2019, PROC CVPR IEEE, P1117, DOI 10.1109/CVPR.2019.00121
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye J, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P516
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang S, 2021, IEEE T MULTIMEDIA, V23, P454, DOI 10.1109/TMM.2020.2978630
   Zhang Shi-Xue, 2020, IEEE C COMPUTER VISI, P9699
   Zhang WQ, 2021, LECT NOTES COMPUT SC, V12824, P79, DOI 10.1007/978-3-030-86337-1_6
   Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314
NR 49
TC 1
Z9 1
U1 6
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4052
EP 4066
DI 10.1109/TMM.2022.3171085
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200001
DA 2024-07-18
ER

PT J
AU Yang, Y
   Zheng, H
   Zeng, LL
   Shen, XJ
   Zhan, YZ
AF Yang, Yang
   Zheng, Hao
   Zeng, Lanling
   Shen, Xiangjun
   Zhan, Yongzhao
TI <i>L</i><sub>1</sub>-Regularized Reconstruction Model for
   Edge-Preserving Filtering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computational photography; edge-preserving filtering;
   L-1-regularization; warm start
ID VLSI ARCHITECTURE; IMAGE
AB Smoothing images while preserving salient edges is a crucial task in computational photography. Existing edge-preserving filters suffer from various artifacts, such as halos, gradient reversals, and intensity shifts. Observing that various artifacts are strongly related to salient edges with large gradients, we propose a continuous mapping function to process the gradients. The proposed function is literally edge-preserving, i.e., it keeps large gradients intact while attenuating small gradients. We propose an $L_{1}$-regularized reconstruction model based on the processed gradients for edge-preserving image filtering. The $L_{1}$-regularization facilitates the edge-preserving property in the reconstructed results. To solve the proposed $L_{1}$-regularized model, we implement an efficient algorithm based on the alternating direction method of multipliers (ADMM) and Fourier domain optimization. We have conducted qualitative and quantitative experiments to evaluate the proposed filter. The results demonstrate that our filter better handles various artifacts and delivers superior image quality on various applications. The proposed filter is highly efficient, our GPU implementation takes 70 ms to process a color image with 1 megapixel on an NVIDIA GTX 1070 GPU.
C1 [Yang, Yang; Zheng, Hao; Zeng, Lanling; Shen, Xiangjun; Zhan, Yongzhao] Jiangsu Univ, Dept Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China.
C3 Jiangsu University
RP Yang, Y (corresponding author), Jiangsu Univ, Dept Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China.
EM yyoung@ujs.edu.cn; 2221908064@stmail.ujs.edu.cn; lanling73@163.com;
   xjshen@ujs.edu.cn; yzzhan@ujs.edu.cn
OI shen, xiangjun/0000-0002-3359-8972; , Yang/0000-0001-8782-4819
FU National Natural Science Foundation of China [61402205, 61672268];
   Natural Science Foundation of Jiangsu Province [BK20170197]; China
   Postdoctoral Science Foundation [2015M571688]; Jiangsu Key Laboratory of
   Media Design and Soft-ware Technology (Jiangnan University) [20ST0206];
   Jiangsu University [13JDG085]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61402205 and 61672268,in part by the
   Natural Science Foundation of Jiangsu Province under Grant BK20170197,
   in part by China Postdoctoral Science Foundation under Grant
   2015M571688, in part by the Jiangsu Key Laboratory of Media Design and
   Sofware Technology (Jiangnan University) under Grant 20ST0206, and in
   part by Jiangsu University under Grant 13JDG085.
CR Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x
   Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629645
   Barron JT, 2015, PROC CVPR IEEE, P4466, DOI 10.1109/CVPR.2015.7299076
   Bhat P, 2008, LECT NOTES COMPUT SC, V5303, P114, DOI 10.1007/978-3-540-88688-4_9
   Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Dai LQ, 2020, IEEE T CIRC SYST VID, V30, P603, DOI 10.1109/TCSVT.2019.2893322
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Fan QN, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275081
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Feng YD, 2022, IEEE T NEUR NET LEAR, V33, P7223, DOI 10.1109/TNNLS.2021.3084473
   Gastal ESL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185529
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He SF, 2016, IEEE T CIRC SYST VID, V26, P891, DOI 10.1109/TCSVT.2015.2430671
   HOLLAND PW, 1977, COMMUN STAT A-THEOR, V6, P813, DOI 10.1080/03610927708827533
   Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156
   Kingma D. P., 2014, arXiv
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu W, 2020, AAAI CONF ARTIF INTE, V34, P11620
   Liu W, 2022, IEEE T PATTERN ANAL, V44, P6631, DOI 10.1109/TPAMI.2021.3097891
   Liu W, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3388887
   Liu W, 2020, IEEE T CIRC SYST VID, V30, P23, DOI 10.1109/TCSVT.2018.2890202
   Liu W, 2017, IEEE I CONF COMP VIS, pCP32, DOI 10.1109/ICCV.2017.624
   Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600
   Morel JM, 2012, PATTERN RECOGN LETT, V33, P342, DOI 10.1016/j.patrec.2011.10.010
   Mun H, 2022, IEEE T MULTIMEDIA, V24, P3823, DOI 10.1109/TMM.2021.3108401
   Nikolova M, 2004, J MATH IMAGING VIS, V20, P99, DOI 10.1023/B:JMIV.0000011920.58935.9c
   Pan XX, 2015, IEEE SIGNAL PROC LET, V22, P1806, DOI 10.1109/LSP.2015.2432466
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Tomasi C., 1998, 6 INT C COMP VIS IEE, P839
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197
   Wu L, 2019, IEEE T CIRC SYST VID, V29, P1868, DOI 10.1109/TCSVT.2018.2852336
   Xu L, 2015, PR MACH LEARN RES, V37, P1669
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yin H, 2019, PROC CVPR IEEE, P8750, DOI 10.1109/CVPR.2019.00896
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhang XC, 2018, IEEE T CIRC SYST VID, V28, P230, DOI 10.1109/TCSVT.2016.2605690
   Zhou ZQ, 2018, IEEE T MULTIMEDIA, V20, P1392, DOI 10.1109/TMM.2017.2772438
   Zhu FD, 2019, IEEE T IMAGE PROCESS, V28, P3556, DOI 10.1109/TIP.2019.2908778
NR 51
TC 10
Z9 10
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4148
EP 4162
DI 10.1109/TMM.2022.3171686
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200006
DA 2024-07-18
ER

PT J
AU Yi, J
   Zhu, YC
   Xie, JY
   Chen, ZZ
AF Yi, Jing
   Zhu, Yaochen
   Xie, Jiayi
   Chen, Zhenzhong
TI Cross-Modal Variational Auto-Encoder for Content-Based Micro-Video
   Background Music Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Visualization; Recommender systems; Semantics; Mood; Task
   analysis; Pattern matching; Cross-modal matching; variational
   auto-encoder; product-of-experts system; recommendation systems
ID GENERATION
AB In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for content-based micro-video background music recommendation. CMVAE is a hierarchical Bayesian generative model that matches relevant background music to a micro-video by projecting these two multimodal inputs into a shared low-dimensional latent space, where the alignment of two corresponding embeddings of a matched video-music pair is achieved by cross-generation. Moreover, the multimodal information is fused by the product-of-experts (PoE) principle, where the semantic information in visual and textual modalities of the micro-video are weighted according to their variance estimations such that the modality with a lower noise level is given more weights. Therefore, the micro-video latent variables contain less irrelevant information that results in a more robust model generalization. Furthermore, we establish a large-scale content-based micro-video background music recommendation dataset, TT-150k, composed of extracted features from approximately 3,000 different background music clips associated to 150,000 micro-videos from different users. Extensive experiments on the established TT-150k dataset demonstrate the effectiveness of the proposed method. A qualitative assessment of CMVAE by visualizing some recommendation results is also included.
C1 [Yi, Jing; Chen, Zhenzhong] Wuhan Univ, Sch Comp Sci, Hubei 430079, Peoples R China.
   [Zhu, Yaochen; Xie, Jiayi] Wuhan Univ, Sch Remote Sensing & Informat Engn, Hubei 430079, Peoples R China.
C3 Wuhan University; Wuhan University
RP Chen, ZZ (corresponding author), Wuhan Univ, Sch Comp Sci, Hubei 430079, Peoples R China.
EM yijing-v@whu.edu.cn; yaochenzhu@whu.edu.cn; xjyxie@whu.edu.cn;
   zzchen@ieee.org
RI Chen, Zhenzhong/C-2529-2015; Zhu, Yaochen/AAO-2023-2020
OI Zhu, Yaochen/0000-0001-6266-2788
FU National Natural Science Foundation of China [62036005]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62036005.
CR Abu-El-Haija Sami, 2016, arXiv, DOI [DOI 10.48550/ARXIV.1609.08675, DOI 10.48550/-ARXIV.1609.08675]
   Andjelkovic I, 2019, INT J HUM-COMPUT ST, V121, P142, DOI 10.1016/j.ijhcs.2018.04.004
   Baldi P., 2012, P INT C UNS TRANSF L, P37
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Cai R, 2007, INT CONF ACOUST SPEE, P737
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao J., 2011, PROC INT SEMANTIC WE
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen FY, 2021, IEEE T MULTIMEDIA, V23, P3073, DOI 10.1109/TMM.2020.3019710
   Chen H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1749, DOI 10.1145/3343031.3351055
   Chen YF, 2018, PROCEEDINGS OF THE 3RD WORKSHOP ON DEEP LEARNING FOR RECOMMENDER SYSTEMS (DLRS), P3, DOI 10.1145/3270323.3270326
   Cheng ZY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3654
   Cheng ZY, 2016, ACM T INFORM SYST, V34, DOI 10.1145/2846092
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Eyben Florian, 2010, P 18 ACM INT C MULT, P1459
   Fu X, 2020, IEEE T MULTIMEDIA, V22, P2354, DOI 10.1109/TMM.2019.2957948
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   He XT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1740, DOI 10.1145/3343031.3350974
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hong S, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P353, DOI 10.1145/3206025.3206046
   Huang PY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1758, DOI 10.1145/3343031.3350894
   JAYNES ET, 1957, PHYS REV, V108, P171, DOI 10.1103/PhysRev.108.171
   Ji ZY, 2019, IEEE ACCESS, V7, P40416, DOI 10.1109/ACCESS.2019.2897586
   Jing M., 2020, MM 20 P 28 ACM INT C, P3283, DOI DOI 10.1145/3394171.3413676
   KaiyeWang Qiyue Yin, 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06215
   Karamanolakis G, 2018, PROCEEDINGS OF THE 3RD WORKSHOP ON DEEP LEARNING FOR RECOMMENDER SYSTEMS (DLRS), P10, DOI 10.1145/3270323.3270329
   Kingma D. P., 2014, arXiv
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li Bochen, 2019, INT SOC MUSIC INFORM, P604
   Li XP, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P305, DOI 10.1145/3097983.3098077
   Liang DW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P689, DOI 10.1145/3178876.3186150
   Lin Jen-Chun, 2016, P ACM INT C MULT, P372
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu CL, 2018, KNOWL-BASED SYST, V159, P158, DOI 10.1016/j.knosys.2018.07.001
   Ma JX, 2019, ADV NEUR IN, V32
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Posner J, 2005, DEV PSYCHOPATHOL, V17, P715, DOI 10.1017/S0954579405050340
   Sasaki Shoto, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P299, DOI 10.1007/978-3-319-14442-9_33
   Schedl M, 2019, FRONT APPL MATH STAT, V5, DOI 10.3389/fams.2019.00044
   Semedo D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1152, DOI 10.1145/3394171.3413540
   Shah RR, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P607, DOI 10.1145/2647868.2654919
   Shang LY, 2020, PR I-A I C AD S N A, P425, DOI 10.1109/ASONAM49781.2020.9381371
   Shen DH, 2018, AAAI CONF ARTIF INTE, P5438
   Shi Y., 2019, P ADV NEURAL INFORM, VVolume 32
   Shin KH, 2017, INT CONF BIG DATA, P47, DOI 10.1109/BIGCOMP.2017.7881714
   Surís D, 2019, LECT NOTES COMPUT SC, V11132, P711, DOI 10.1007/978-3-030-11018-5_62
   Suzuki M., 2017, PROC INT C LEARN REP
   Tan HC, 2022, IEEE T MULTIMEDIA, V24, P832, DOI 10.1109/TMM.2021.3060291
   Thayer Robert E., 1990, The Biopsychology of Mood and Arousal
   Van den Oord A., 2013, ADV NEURAL INFORM PR, P2643, DOI [DOI 10.1109/MMUL.2011.34.VAN, 10.5555/2999792.2999907]
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Vystrcilova Michaela, 2020, WIMS 2020: Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics, P190, DOI 10.1145/3405962.3405963
   Wang JJ, 2007, IEEE T MULTIMEDIA, V9, P576, DOI 10.1109/TMM.2006.888013
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang PM, 2020, INTERFACING BIOELECTRONICS AND BIOMEDICAL SENSING, P1, DOI 10.1007/978-3-030-34467-2_1
   Wei Jiwei, 2020, P IEEE C COMP VIS PA, P13005
   Wei TX, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1791, DOI 10.1145/3447548.3467289
   Wu F., 2013, P ACM INT C MULT, P877
   Wu MK, 2018, ADV NEUR IN, V31
   Wu X., 2012, P 20 ACM INT C MULTI, P837
   Wu XX, 2016, IEEE T MULTIMEDIA, V18, P1305, DOI 10.1109/TMM.2016.2557722
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Yang YF, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020): SYSTEM DEMONSTRATIONS, P87
   Yi J, 2022, IEEE T MULTIMEDIA, V24, P1067, DOI 10.1109/TMM.2021.3111487
   Yin-Tzu Lin, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8326, P283, DOI 10.1007/978-3-319-04117-9_26
   Yu Wenhao, 2020, P 58 ANN M ASS COMPU
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zhang Y, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P11, DOI 10.1145/3404835.3462875
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zhu ZW, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2439, DOI 10.1145/3447548.3467376
NR 77
TC 11
Z9 11
U1 5
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 515
EP 528
DI 10.1109/TMM.2021.3128254
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, YZ
   Po, LM
   Yu, WY
   Rehman, YAU
   Liu, MY
   Zhang, YJ
   Ou, WF
AF Zhao, Yuzhi
   Po, Lai-Man
   Yu, Wing-Yin
   Rehman, Yasar Abbas Ur
   Liu, Mengyang
   Zhang, Yujia
   Ou, Weifeng
TI VCGAN: Video Colorization With Hybrid Generative Adversarial Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generative adversarial networks; placeholder feature extractor; video
   colorization
ID COLOR TRANSFER; IMAGE
AB We propose a Video Colorization with Hybrid Generative Adversarial Network (VCGAN), an improved approach to video colorization using end-to-end learning and recurrent architecture. The VCGAN addresses two prevalent issues in the video colorization domain: Temporal consistency and the unification of colorization network and refinement network into a single architecture. To enhance colorization quality and spatiotemporal consistency, the mainstream of the generator in VCGAN is assisted by two additional networks, i.e., global feature extractor and placeholder feature extractor, respectively. The global feature extractor encodes the global semantics of grayscale input to enhance colorization quality, whereas the placeholder feature extractor serves as a feedback connection to encode the semantics of the previous colorized frame in order to maintain spatiotemporal consistency. If changing the input for placeholder feature extractor as grayscale input, the hybrid VCGAN also has the potential to colorize single images. To improve the color consistency of far frames, we propose a dense long-term loss that minimizes the temporal disparity of every two remote frames. Trained with colorization and temporal losses jointly, VCGAN strikes a good balance between video color vividness and spatiotemporal continuity. Experimental results demonstrate that VCGAN produces higher-quality and temporally more consistent colorful videos than existing approaches.
C1 [Zhao, Yuzhi; Po, Lai-Man; Yu, Wing-Yin; Zhang, Yujia] City Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
   [Rehman, Yasar Abbas Ur] TCL Corp Res, Hong Kong, Peoples R China.
   [Liu, Mengyang] Tencent Holdings Ltd, Tencent Video, Shenzhen 518054, Peoples R China.
   [Ou, Weifeng] SenseTime Grp Ltd, Hong Kong, Peoples R China.
C3 City University of Hong Kong; TCL Inc.; Tencent
RP Zhao, YZ (corresponding author), City Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
EM yzzhao2-c@my.cityu.edu.hk; eelmpo@cityu.edu.hk;
   wingyinyu8-c@my.cityu.edu.hk; yaurehman2-c@my.cityu.edu.hk;
   mengyaliu7-c@my.cityu.edu.hk; yzhang2383-c@my.cityu.edu.hk;
   weifengou2-c@my.cityu.edu.hk
RI Zhao, Yuzhi/HJH-8107-2023; /ADN-5973-2022; Zhang, Yujia/K-5056-2016
OI /0000-0002-8908-3863; Zhao, Yuzhi/0000-0001-8561-2206; YU, Wing
   Yin/0000-0002-9559-1055; Zhang, Yujia/0000-0003-3991-7388; rehman,
   yasar/0000-0002-2945-7181
CR [Anonymous], 2017, Advances in neural information processing systems
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bonneel N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818107
   Cao Y, 2017, LECT NOTES ARTIF INT, V10534, P151, DOI 10.1007/978-3-319-71249-9_10
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   deoldify, About us
   Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Guadarrama S, 2017, ARXIV170507208
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Iizuka S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925974
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jampani V, 2017, PROC CVPR IEEE, P3154, DOI 10.1109/CVPR.2017.336
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim D, 2019, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR.2019.00594
   Kingma D. P., 2014, arXiv
   Kouzouglidis Panagiotis, 2019, Advances in Visual Computing. 14th International Symposium on Visual Computing, ISVC 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11844), P209, DOI 10.1007/978-3-030-33720-9_16
   Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Lei CY, 2019, PROC CVPR IEEE, P3748, DOI 10.1109/CVPR.2019.00387
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Liu XP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409105
   Maas A.L., 2013, P 30 INT C MACH LEAR, V30, P3
   Miyato T, 2018, INT C LEARN REPR
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Royer A., 2017, P BRIT MACH VIS C
   Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sheng B, 2014, IEEE T CIRC SYST VID, V24, P407, DOI 10.1109/TCSVT.2013.2276702
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su J.-W., 2020, P IEEE C COMP VIS PA, P7968
   Tai YW, 2005, PROC CVPR IEEE, P747
   Thasarathan H, 2019, 2019 16TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2019), P189, DOI 10.1109/CRV.2019.00033
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Video, US
   Vitoria P, 2020, IEEE WINT CONF APPL, P2434, DOI [10.1109/WACV45572.2020.9093389, 10.1109/wacv45572.2020.9093389]
   Wan SH, 2020, IEEE T MULTIMEDIA, V22, P1756, DOI 10.1109/TMM.2020.2976573
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Xie Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201304
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
   Zhang B, 2019, PROC CVPR IEEE, P8044, DOI 10.1109/CVPR.2019.00824
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao JJ, 2020, INT J COMPUT VISION, V128, P818, DOI 10.1007/s11263-019-01271-4
   Zhao YZ, 2021, IEEE T CIRC SYST VID, V31, P3062, DOI 10.1109/TCSVT.2020.3037688
NR 51
TC 7
Z9 8
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3017
EP 3032
DI 10.1109/TMM.2022.3154600
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, Q
   Yu, CH
AF Zhou, Qiang
   Yu, Chaohui
TI Object Detection Made Simpler by Eliminating Heuristic NMS
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE NMS-free; object detection; one-to-one label assignment; stop-gradient
AB It is valuable and promising to remove post-processing non-maximum suppression (NMS) for object detectors, making detectors simpler and purely end-to-end. Removing NMS is possible if the object detector can identify only one positive sample for prediction for each ground-truth object instance in an image. In this work, we propose a compact and plug-in head, named PSS head, which can be attached to any one-stage detectors to make them NMS-free. Specifically, the PSS head works by automatically selecting a positive sample for each instance to be detected, so that the detectors with our PSS head can directly remove NMS. The success of our PSS head lies in three aspects, namely one-to-one label assignment, stop-gradient operation for eliminating optimization conflicts, and the pss loss and ranking loss specifically designed for the PSS head. Experiments on the COCO dataset demonstrate the effectiveness of our method. In particular, when compared with stage-of-the-art NMS-free methods, our VFNETPSS (attaching PSS head to VFNET) achieves 44.0% mAP, which exceeds the 41.5% mAP of DeFCN with a large margin. When taking Res2Net-101-DCN as backbone network, our VFNETPSS achieves 50.3% mAP on the COCO test set, which is a promising performance even among NMS-based methods.
C1 [Zhou, Qiang; Yu, Chaohui] Alibaba Grp, DAMO, Hangzhou 310023, Peoples R China.
C3 Alibaba Group
RP Zhou, Q (corresponding author), Alibaba Grp, DAMO, Hangzhou 310023, Peoples R China.
EM zhouqiang@zju.edu.cn; huakun.ych@alibaba-inc.com
RI Zhou, Qiang/B-1568-2015
OI Yu, Chaohui/0000-0002-7852-4491
CR Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen L, 2021, IEEE T MULTIMEDIA, V23, P4297, DOI 10.1109/TMM.2020.3040539
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Han Qiu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P549, DOI 10.1007/978-3-030-58452-8_32
   Hao Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8570, DOI 10.1109/CVPR42600.2020.00860
   He K., 2017, IEEE C COMP VIS PATT, P2961, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Leng JX, 2022, IEEE T MULTIMEDIA, V24, P861, DOI 10.1109/TMM.2021.3060278
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002, DOI DOI 10.48550/ARXIV.2006.04388
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rufeng Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10223, DOI 10.1109/CVPR42600.2020.01024
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255
   Sun P., 2020, PROC INT C MACH LEAR, P9934
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tian Z, 2019, Arxiv, DOI arXiv:1911.07451
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang JF, 2021, PROC CVPR IEEE, P15844, DOI 10.1109/CVPR46437.2021.01559
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17
   Zhu X., 2020, PROC INT C LEARN REP
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 33
TC 4
Z9 4
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9254
EP 9262
DI 10.1109/TMM.2023.3248966
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, YX
   Zhao, PC
   Qi, MB
   Zhao, Y
   Jia, W
   Wang, RG
AF Chen, Yanxiang
   Zhao, Pengcheng
   Qi, Meibin
   Zhao, Yang
   Jia, Wei
   Wang, Ronggang
TI Audio Matters in Video Super-Resolution by Implicit Semantic Guidance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Visualization; Task analysis; Superresolution; Image
   reconstruction; Face recognition; Streaming media; Video
   super-resolution; audiovisual fusion; implicit semantic guidance
ID IMAGE SUPERRESOLUTION; NETWORK; ENHANCEMENT
AB Video super-resolution (VSR) aims to use multiple consecutive low-resolution frames to recover the corresponding high-resolution frames. However, existing VSR methods only consider videos as image sequences, ignoring another essential timing information-audio, while in fact, there is a semantic link between audio and vision, and extensive studies have shown that audio can provide supervisory information in visual networks. Meanwhile, the addition of semantic priors has been proven to be effective in super-resolution (SR) tasks, but a pretrained segmentation network is required to obtain semantic segmentation maps. By contrast, audio as the information contained in the video itself can be directly used. Therefore, in this study, we propose a novel and pluggable multiscale audiovisual fusion (MS-AVF) module to enhance VSR performance by exploiting the relevant audio information, which can be regarded as implicit semantic guidance compared with the kind of explicit segmentation priors. Specifically, we first fuse audiovisual features on the semantic feature maps of different granularities of the target frames, and then through a top-down multiscale fusion approach, feedback high-level semantics to the underlying global visual features layer by layer, thereby providing effective audio implicit semantic guidance for VSR. Experimental results show that audio can further improve the VSR effect. Moreover, by visualizing the learned attention mask, the proposed end-to-end model can automatically learn potential audiovisual semantic links, especially improving the accuracy and effectiveness of the SR of sound sources and their surrounding regions.
C1 [Chen, Yanxiang; Zhao, Pengcheng; Qi, Meibin; Zhao, Yang; Jia, Wei] Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230601, Peoples R China.
   [Chen, Yanxiang; Zhao, Pengcheng; Qi, Meibin; Zhao, Yang; Jia, Wei] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Intelligent Interconnected Syst Anhui Prov Lab, Hefei 230601, Peoples R China.
   [Zhao, Yang; Wang, Ronggang] Peng Cheng Natl Lab, Shenzhen 518000, Peoples R China.
   [Wang, Ronggang] Peking Univ Shenzhen Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology
RP Zhao, Y (corresponding author), Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230601, Peoples R China.; Zhao, Y (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Intelligent Interconnected Syst Anhui Prov Lab, Hefei 230601, Peoples R China.
EM chenyx@hfut.edu.cn; stevenzhao1001@gmail.com; qimeibin@163.com;
   yzhao@hfut.edu.cn; jiawei@hfut.edu.cn; rgwang@pkusz.edu.cn
OI Zhao, Pengcheng/0000-0003-3774-5113; Wang, Ronggang/0000-0003-0873-0465
FU National Natural Science Foundation ofChina [61972127, 61972129,
   61771180, 62076086]; Key Research and Development Program in Anhui
   Province [202004d07020008]
FX This work was supported in part by the National Natural Science
   Foundation ofChina underGrants 61972127, 61972129, 61771180, and
   62076086, and in part by the Key Research and Development Program in
   Anhui Province under Grant 202004d07020008.
CR Belin P, 2004, TRENDS COGN SCI, V8, P129, DOI 10.1016/j.tics.2004.01.008
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chung Joon Son, 2018, arXiv
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Ellis A.W., 1989, Handbook of research on face processing, P207
   Gao RH, 2020, PROC CVPR IEEE, P10454, DOI 10.1109/CVPR42600.2020.01047
   Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hu D, 2019, PROC CVPR IEEE, P9240, DOI 10.1109/CVPR.2019.00947
   Huang Y, 2018, IEEE T PATTERN ANAL, V40, P1015, DOI 10.1109/TPAMI.2017.2701380
   Isobe Takashi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P645, DOI 10.1007/978-3-030-58610-2_38
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kazakos E, 2019, IEEE I CONF COMP VIS, P5491, DOI 10.1109/ICCV.2019.00559
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim SY, 2019, IEEE IMAGE PROC, P2831, DOI [10.1109/icip.2019.8803297, 10.1109/ICIP.2019.8803297]
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li S, 2019, PROC CVPR IEEE, P10514, DOI 10.1109/CVPR.2019.01077
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu HY, 2022, Arxiv, DOI arXiv:2007.12928
   Luo ZW, 2021, IEEE COMPUT SOC CONF, P471, DOI 10.1109/CVPRW53098.2021.00058
   Ma S, 2021, Arxiv, DOI arXiv:2009.09805
   MARKS LE, 1993, J EXP PSYCHOL HUMAN, V19, P227, DOI 10.1037/0096-1523.19.2.227
   Meishvili G, 2020, PROC CVPR IEEE, P1361, DOI 10.1109/CVPR42600.2020.00144
   Michelsanti D, 2019, INT CONF ACOUST SPEE, P8077, DOI 10.1109/ICASSP.2019.8682790
   Noesselt T, 2010, J NEUROSCI, V30, P13609, DOI 10.1523/JNEUROSCI.4524-09.2010
   Oh TH, 2019, PROC CVPR IEEE, P7531, DOI 10.1109/CVPR.2019.00772
   Rouditchenko Andrew, 2019, ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings, P2357, DOI 10.1109/ICASSP.2019.8682467
   Rui Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P292, DOI 10.1007/978-3-030-58565-5_18
   Sadeghi M, 2020, INT CONF ACOUST SPEE, P7534, DOI [10.1109/ICASSP40776.2020.9053730, 10.1109/icassp40776.2020.9053730]
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   SHELTON BR, 1980, PERCEPT PSYCHOPHYS, V28, P589, DOI 10.3758/BF03198830
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Tivadar RI, 2018, NEUROIMAGE, V179, P480, DOI 10.1016/j.neuroimage.2018.06.070
   Wang WP, 2020, INT CONF ACOUST SPEE, P7529, DOI [10.1109/icassp40776.2020.9053033, 10.1109/ICASSP40776.2020.9053033]
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wang ZY, 2019, IEEE T IMAGE PROCESS, V28, P2530, DOI 10.1109/TIP.2018.2887017
   Wen Y., 2019, Advances in Neural Information Processing Systems, P5266
   Wu XY, 2019, EUR SIGNAL PR CONF, DOI 10.23919/eusipco.2019.8902987
   Wu Y, 2021, PROC CVPR IEEE, P1326, DOI 10.1109/CVPR46437.2021.00138
   Wu Y, 2019, IEEE I CONF COMP VIS, P6301, DOI 10.1109/ICCV.2019.00639
   Xuan HY, 2020, AAAI CONF ARTIF INTE, V34, P279
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yan YT, 2022, IEEE T MULTIMEDIA, V24, P1473, DOI 10.1109/TMM.2021.3065731
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yapeng Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P436, DOI 10.1007/978-3-030-58580-8_26
   Yi R, 2020, Arxiv, DOI arXiv:2002.10137
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhang X., 2020, P IEEE CVF C COMP VI, P12335
   Zhang XY, 2021, IEEE T MULTIMEDIA, V23, P1924, DOI 10.1109/TMM.2020.3005025
   Zhang Y., 2020, arXiv
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu H, 2021, INT J AUTOM COMPUT, V18, P351, DOI 10.1007/s11633-021-1293-0
NR 59
TC 1
Z9 1
U1 2
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4128
EP 4142
DI 10.1109/TMM.2022.3152941
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400033
DA 2024-07-18
ER

PT J
AU Gong, XW
   Yang, JH
   Yuan, D
   Bao, W
AF Gong, Xiuwen
   Yang, Jiahui
   Yuan, Dong
   Bao, Wei
TI Generalized Large Margin <i>k</i>NN for Partial Label Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Partial label; classification; k-nearest neighbor (kNN); metric learning
AB To deal with noises in partial label learning (PLL), existing approaches try to perform disambiguation either by identifying the ground-truth label or by averaging the candidate labels. However, these methods can be easily misled by the false-positive noisy labels in the candidate set, and fail to generalize well in testing. When labeling information is ambiguous, learning paradigms should depend more on underlying data structure. Large margin nearest neighbour (LMNN) is a popular strategy to consider instance and class correlations in supervised learning, but can not he directly used in weakly-supervised PLL due to the ambiguity of labeling information. In this paper, we first define similarly and differently labeled pairs as well as the similarity weight to evaluate the similarties between any two instances. We then propose a novel PLL method called Generalized Large Margin kNN for Partial Label Learning (GLMNN-PLL), which adapts the framework of LMNN to PLL by modifying the constraint from 'the same class' to 'similarly-labeled'. GLMNN-PLL aims to learn a new metric and perform disambiguation by reorganizing the underlying data structure, that is, making similarly labeled instances closer to each other while making differently labeled instances seperated by a large margin. As two close instances with shared labels do not necessarily belong to the same class, we put a weight on each instance pair. An efficient algorithm is designed to optimize the proposed method and the convergence is analyzed in this paper. Moreover, we present a theoretical analysis of the generalization error hound for GLMNN-PLL. Comprehensive experiments on controlled UCI datasets as well as real-world partial label datasets from various domains demonstrate the superiorities of the proposed method.
C1 [Gong, Xiuwen; Yuan, Dong; Bao, Wei] Univ Sydney, Fac Engn, Camperdown, NSW 2006, Australia.
   [Yang, Jiahui] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
C3 University of Sydney; University of New South Wales Sydney
RP Yuan, D (corresponding author), Univ Sydney, Fac Engn, Camperdown, NSW 2006, Australia.
EM xiuwen.gong@sydney.edu.au; jiahuiyang0@gmail.com;
   dong.yuan@sydney.edu.au; wei.bao@sydney.edu.au
RI Jiang, Jiaojiao/AAF-7525-2020; Bao, Wei/ACK-4153-2022
OI Jiang, Jiaojiao/0000-0001-7307-8114; Bao, Wei/0000-0003-1874-1766; Yuan,
   Dong/0000-0003-1130-0888
CR Alfaro JC, 2021, INT J INTELL SYST, V36, P890, DOI 10.1002/int.22325
   Briggs Forrest, 2012, P 18 ACM SIGKDD INT, P534
   Chai J, 2020, IEEE T NEUR NET LEAR, V31, P2594, DOI 10.1109/TNNLS.2019.2933530
   Chang XJ, 2016, IEEE T NEUR NET LEAR, V27, P1502, DOI 10.1109/TNNLS.2015.2441735
   Chen YC, 2014, IEEE T INF FOREN SEC, V9, P2076, DOI 10.1109/TIFS.2014.2359642
   Chen YL, 2020, APPL INTELL, V50, P3412, DOI 10.1007/s10489-020-01696-2
   Cour T, 2011, J MACH LEARN RES, V12, P1501
   Dhar S, 2019, ADV NEUR IN, V32
   Domeniconi C, 2005, IEEE T NEURAL NETWOR, V16, P899, DOI 10.1109/TNN.2005.849821
   Feng L, 2020, P INT C NEUR INF PRO
   Gong C, 2018, IEEE T CYBERNETICS, V48, P967, DOI 10.1109/TCYB.2017.2669639
   Flores DKG, 2020, COMPUT SIST, V24, P597, DOI [10.13053/CyS-24-2-3393, 10.13053/cys-24-2-3393]
   Guillaumin M, 2010, LECT NOTES COMPUT SC, V6311, P634, DOI 10.1007/978-3-642-15549-9_46
   Hiranandani G, 2019, ADV NEUR IN, V32
   Hüllermeier E, 2006, INTELL DATA ANAL, V10, P419, DOI 10.3233/IDA-2006-10503
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jin Rong, 2002, Advances in Neural Information Processing Systems, V15, P897
   Liu Liping, 2012, Adv. Neural Inf. Process. Syst., V25, P548
   Liu WW, 2017, J MACH LEARN RES, V18
   Liu WW, 2019, IEEE T PATTERN ANAL, V41, P408, DOI 10.1109/TPAMI.2018.2794976
   Liu WW, 2017, J MACH LEARN RES, V18
   Lv Jiaqi, 2020, P INT C MACH LEARN, P6500
   Lyu GY, 2021, IEEE T KNOWL DATA EN, V33, P521, DOI 10.1109/TKDE.2019.2933837
   Nguyen N., 2008, P 14 ACM SIGKDD INT, P551, DOI 10.1145/1401890.1401958
   Song HY, 2020, IEEE ACCESS, V8, P76411, DOI 10.1109/ACCESS.2020.2989200
   Su XP, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.3.033036
   Vasisht D, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P472, DOI 10.1145/2623330.2623759
   Wang DB, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P83, DOI 10.1145/3292500.3330840
   Weinberger K.Q., 2006, Advances in neural information processing systems, P1473
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Xiao SJ, 2015, IEEE T NEUR NET LEAR, V26, P2440, DOI 10.1109/TNNLS.2014.2386307
   Yu F, 2017, MACH LEARN, V106, P573, DOI 10.1007/s10994-016-5606-4
   Yuan D, 2021, IEEE T IMAGE PROCESS, V30, P976, DOI 10.1109/TIP.2020.3037518
   Zeng ZN, 2013, PROC CVPR IEEE, P708, DOI 10.1109/CVPR.2013.97
   Zhang ML, 2017, IEEE T KNOWL DATA EN, V29, P2155, DOI 10.1109/TKDE.2017.2721942
   Zhang ML, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1335, DOI 10.1145/2939672.2939788
   Zhang ML, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4048
   Zhang SW, 2020, IEEE T MULTIMEDIA, V22, P2610, DOI 10.1109/TMM.2019.2959425
   Zhixin Liu, 2016, Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data. 15th China National Conference, CCL 2016, and 4th International Symposium, NLP-NABD 2016. Proceedings: LNAI 10035, P424, DOI 10.1007/978-3-319-47674-2_35
NR 39
TC 14
Z9 14
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1055
EP 1066
DI 10.1109/TMM.2021.3109438
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800004
DA 2024-07-18
ER

PT J
AU Hu, W
   Pang, JH
   Liu, XM
   Tian, D
   Lin, CW
   Vetro, A
AF Hu, Wei
   Pang, Jiahao
   Liu, Xianming
   Tian, Dong
   Lin, Chia-Wen
   Vetro, Anthony
TI Graph Signal Processing for Geometric Data and Beyond: Theory and
   Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Geometry; Symmetric matrices; Manifolds;
   Laplace equations; Tools; Spectral analysis; Graph Signal Processing
   (GSP); geometric data; riemannian manifold; graph neural networks
   (GNNs); interpretability
ID FOURIER-TRANSFORM; POINT CLOUDS; IMAGE; CONVERGENCE; REGULARIZATION;
   COMPRESSION; LAPLACIAN; REPRESENTATION
AB Geometric data acquired from real-world scenes, e.g., 2D depth images, 3D point clouds, and 4D dynamic point clouds, have found a wide range of applications including immersive telepresence, autonomous driving, surveillance, etc. Due to irregular sampling patterns of most geometric data, traditional image/video processing methodologies are limited, while Graph Signal Processing (GSP)-a fast-developing field in the signal processing community-enables processing signals that reside on irregular domains and plays a critical role in numerous applications of geometric data from low-level processing to high-level analysis. To further advance the research in this field, we provide the first timely and comprehensive overview of GSP methodologies for geometric data in a unified manner by bridging the connections between geometric data and graphs, among the various geometric data modalities, and with spectral/nodal graph filtering techniques. We also discuss the recently developed Graph Neural Networks (GNNs) and interpret the operation of these networks from the perspective of GSP. We conclude with a brief discussion of open problems and challenges.
C1 [Hu, Wei] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100080, Peoples R China.
   [Pang, Jiahao; Tian, Dong] InterDigital, Imaging Sci Lab, Princeton, NJ 08540 USA.
   [Liu, Xianming] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 30013, Taiwan.
   [Lin, Chia-Wen] Ind Technol Res Inst, Elect & Optoelect Syst Res Labs, Hsinchu, Taiwan.
   [Vetro, Anthony] Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.
C3 Peking University; InterDigital; Harbin Institute of Technology;
   National Tsing Hua University; National Tsing Hua University; Industrial
   Technology Research Institute - Taiwan
RP Lin, CW (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.; Lin, CW (corresponding author), Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 30013, Taiwan.; Lin, CW (corresponding author), Ind Technol Res Inst, Elect & Optoelect Syst Res Labs, Hsinchu, Taiwan.
EM forhuwei@pku.edu.cn; jpang@connect.ust.hk; csxm@hit.edu.cn;
   dong.tian@gmail.com; cwlin@ee.nthu.edu.tw; avetro@merl.com
OI Tian, Dong/0000-0002-2310-0974
FU Ministry of Science and Technology, Taiwan [110-2634-F-007-015];
   National Key R&D project of China [2019YFF0302903]; Natural Science
   Foundation of China [61972009]
FX This work was supported in part by Ministry of Science and Technology,
   Taiwan, under Grant 110-2634-F-007-015, in part by the National Key R&D
   project of China under contract No. 2019YFF0302903, and in part by
   Natural Science Foundation of China (61972009).
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   Anis A, 2016, INT CONF ACOUST SPEE, P6360, DOI 10.1109/ICASSP.2016.7472901
   [Anonymous], 1979, Differential Geometry, Lie Groups, and Symmetric Spaces
   [Anonymous], 2014, PROC 2 INT C LEARN R
   [Anonymous], 2007, NIPS
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   AXELSSON O, 1986, NUMER MATH, V48, P499, DOI 10.1007/BF01389448
   Bai YC, 2019, IEEE T IMAGE PROCESS, V28, P1404, DOI 10.1109/TIP.2018.2874290
   Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939
   Benedek C, 2014, PATTERN RECOGN LETT, V50, P149, DOI 10.1016/j.patrec.2014.04.010
   Berger M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451246
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Burdea G. C., 2003, Virtual reality technology
   Cai Q, 2016, MICROSOFT VOXELIZED
   Chao YH, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P60, DOI 10.1109/PCS.2015.7170047
   Chen SH, 2021, IEEE SIGNAL PROC MAG, V38, P68, DOI 10.1109/MSP.2020.2984780
   Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3183, DOI 10.1109/TIP.2019.2957935
   Chen SH, 2018, IEEE T SIGNAL PROCES, V66, P666, DOI 10.1109/TSP.2017.2771730
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Cheung G, 2011, IEEE INT WORKSH MULT
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Chung F. R., 1997, Spectral Graph Theory, V92, DOI DOI 10.1090/CBMS/092
   Cohen RA, 2016, IEEE DATA COMPR CONF, P141, DOI 10.1109/DCC.2016.67
   Cohen RA, 2016, IEEE IMAGE PROC, P1374, DOI 10.1109/ICIP.2016.7532583
   Couprie C, 2013, SIAM J IMAGING SCI, V6, P1246, DOI 10.1137/120895068
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   cvlab, TSUKUBA DATASETS
   Dabov K, 2006, PROC SPIE, V6064, DOI 10.1117/12.643267
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3507, DOI 10.1109/TIP.2017.2699922
   Defferrard M, 2016, ADV NEUR IN, V29
   Dinesh C, 2020, IEEE T IMAGE PROCESS, V29, P4143, DOI 10.1109/TIP.2020.2969052
   Dong XW, 2020, IEEE SIGNAL PROC MAG, V37, P117, DOI 10.1109/MSP.2020.3014591
   Dong XW, 2019, IEEE SIGNAL PROC MAG, V36, P44, DOI 10.1109/MSP.2018.2887284
   Ebrahimi T, 2016, IEEE MULTIMEDIA, V23, P14, DOI 10.1109/MMUL.2016.64
   Egilmez HE, 2017, IEEE J-STSP, V11, P825, DOI 10.1109/JSTSP.2017.2726975
   Elmoataz A, 2008, IEEE T IMAGE PROCESS, V17, P1047, DOI 10.1109/TIP.2008.924284
   Eugene dEon T. M., 2017, 8I VOXELIZED FULL BO
   Franceschi L, 2019, PR MACH LEARN RES, V97
   Fu GJ, 2020, Arxiv, DOI arXiv:2006.04386
   Fu ZQ, 2021, IEEE T MULTIMEDIA, V23, P3022, DOI 10.1109/TMM.2021.3068606
   Gadde A, 2013, IEEE IMAGE PROC, P1222, DOI 10.1109/ICIP.2013.6738252
   Gao X., 2020, PROC IEEE INT C MULT, P1
   Gao X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P601, DOI 10.1145/3343031.3351170
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   github, WAYMO OPENDATASET
   Gripon V, 2018, 2018 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA)
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Guo Z., 2020, 2020 IEEE INT C MULT, P1
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Hein M, 2007, J MACH LEARN RES, V8, P1325
   Hein M, 2006, LECT NOTES ARTIF INT, V4005, P50, DOI 10.1007/11776420_7
   Henaff M, 2015, Arxiv, DOI arXiv:1506.05163
   Hu W, 2020, IEEE T SIGNAL PROCES, V68, P2841, DOI 10.1109/TSP.2020.2978617
   Hu W, 2019, IEEE T IMAGE PROCESS, V28, P4087, DOI 10.1109/TIP.2019.2906554
   Hu W, 2015, IEEE SIGNAL PROC LET, V22, P1913, DOI 10.1109/LSP.2015.2446683
   Hu W, 2015, IEEE T IMAGE PROCESS, V24, P419, DOI 10.1109/TIP.2014.2378055
   Hu W, 2013, IEEE INT WORKSH MULT, P1, DOI 10.1109/MMSP.2013.6659254
   Hu W, 2012, IEEE IMAGE PROC, P1297, DOI 10.1109/ICIP.2012.6467105
   informatik, FLYINGTHINGS3D DATAS
   Jin W, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P66, DOI 10.1145/3394486.3403049
   Kim WS, 2012, INT CONF ACOUST SPEE, P813, DOI 10.1109/ICASSP.2012.6288008
   Le Magoarou L, 2018, IEEE T SIGNAL INF PR, V4, P407, DOI 10.1109/TSIPN.2017.2710619
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levoy M, 2005, The stanford 3d scanning repository
   Li RY, 2018, AAAI CONF ARTIF INTE, P3546
   Li YZ, 2018, ADV NEUR IN, V31
   Liu XM, 2017, IEEE T IMAGE PROCESS, V26, P509, DOI 10.1109/TIP.2016.2627807
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Mateos G, 2019, IEEE SIGNAL PROC MAG, V36, P16, DOI 10.1109/MSP.2018.2890143
   Maugey T, 2015, IEEE T IMAGE PROCESS, V24, P1573, DOI 10.1109/TIP.2015.2400817
   middlebury, Middlebury stereo datasets
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Ortega A, 2018, P IEEE, V106, P808, DOI 10.1109/JPROC.2018.2820126
   Osher S, 2017, SIAM J IMAGING SCI, V10, P1669, DOI 10.1137/16M1058686
   Pang JH, 2017, IEEE T IMAGE PROCESS, V26, P1770, DOI 10.1109/TIP.2017.2651400
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Rosman G, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12139
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Rue Havard, 2005, Gaussian Markov Random Fields: Theory and Applications, DOI DOI 10.1201/9780203492024
   Sandryhaila A, 2013, IEEE T SIGNAL PROCES, V61, P1644, DOI 10.1109/TSP.2013.2238935
   Schmalstieg D., 2016, AUGMENTED REALITY PR
   Schnabel R., 2006, P S POINT BAS GRAPH, V6, P111, DOI DOI 10.2312/SPBG/SPBG06/111-120
   Schoenenberger Y, 2015, 3DTV CONF
   Shao YT, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Shen G., 2010, 2010 28th Picture Coding Symposium (PCS 2010), P566, DOI 10.1109/PCS.2010.5702565
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Singer A, 2006, APPL COMPUT HARMON A, V21, P128, DOI 10.1016/j.acha.2006.03.004
   Stankovic L, 2020, Arxiv, DOI arXiv:2001.00426
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tang JX, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2940, DOI 10.1109/ICASSP39728.2021.9414792
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506
   Tian D, 2014, IEEE INT CON MULTI
   Ting D., 2010, P INT C MACH LEARN, P1079
   Valsesia D, 2020, IEEE T IMAGE PROCESS, V29, P8226, DOI 10.1109/TIP.2020.3013166
   Velickovic Petar, 2018, INT C LEARN REPR
   Wahlberg B., 2012, IFAC P, V45, P83
   Wang YZ, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853724
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Welling Max., 2016, arXiv, DOI DOI 10.48550/ARXIV.1609.02907
   Wu F, 2019, PR MACH LEARN RES, V97
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xu Y., 2019, arXiv
   Xu YQ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1753, DOI 10.1109/ICASSP.2018.8462684
   Zeng J, 2019, IEEE COMPUT SOC CONF, P1759, DOI 10.1109/CVPRW.2019.00226
   Zeng J, 2020, IEEE T IMAGE PROCESS, V29, P3474, DOI 10.1109/TIP.2019.2961429
   Zhang C., 2015, Tech.Rep. MSR-TR-2015-31,
   Zhang C, 2014, IEEE IMAGE PROC, P2066, DOI 10.1109/ICIP.2014.7025414
   Zhang C, 2013, IEEE SIGNAL PROC LET, V20, P106, DOI 10.1109/LSP.2012.2230165
   Zhang XQ, 2011, J SCI COMPUT, V46, P20, DOI 10.1007/s10915-010-9408-8
   Zhang YX, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6279, DOI 10.1109/ICASSP.2018.8462291
   Ziko I., 2020, INT C MACH LEARN PML, P11660
NR 121
TC 21
Z9 23
U1 4
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3961
EP 3977
DI 10.1109/TMM.2021.3111440
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 7F8JC
UT WOS:000902085900002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, JL
   Jing, L
   Tan, ZF
   Kwong, S
AF Huang, Jialu
   Jing, Liao
   Tan, Zhifeng
   Kwong, Sam
TI Multi-Density Sketch-to-Image Translation Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Codes; Training; Faces; Decoding; Image edge detection; Task analysis;
   Image synthesis; Deep image synthesis; interactive editing; GAN;
   multi-scale disentangle
AB Sketch-to-image (S2I) translation plays an important role in image synthesis and manipulation tasks, such as photo editing and colorization. Some specific S2I translations, including sketch-to-photo and sketch-to-painting, can be used as powerful tools in the art design industry. However, previous methods only support S2I translation with a single level of density, which gives less flexibility to users for controlling the input sketches. In this work, we propose the first multi-level density sketch-to-image translation framework, which allows the input sketch to cover a wide range from rough object outlines to microstructures. Moreover, to tackle the problem of noncontinuous representation of multi-level density input sketches, we project the density level into a continuous latent space, which can then be linearly controlled by a parameter. This allows users to conveniently control the densities of input sketches and the generation of images. Moreover, our method has been successfully verified on various datasets for different applications, including face editing, multi-modal sketch-to-photo translation, and anime colorization, providing coarse-to-fine levels of controls to these applications.
C1 [Huang, Jialu; Jing, Liao] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 518000, Peoples R China.
   [Tan, Zhifeng] Brion ASMI, Santa Clar, CA USA.
C3 City University of Hong Kong; City University of Hong Kong; Shenzhen
   Research Institute, City University of Hong Kong
RP Kwong, S (corresponding author), City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 518000, Peoples R China.
EM jialhuang8c@my.cityu.edu.hk; jingliao@cityu.edu.hk; steven.tan@asml.com;
   cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261; HUANG, Jialu/0000-0002-0516-3736; LIAO,
   Jing/0000-0001-7014-5377
FU National Natural Science Foundation of China [61672443]; Hong Kong
   GRF-RGC General Research Fund [9042322 (CityU 11200116), 9042489 (CityU
   11206317), 9042816 (CityU 11209819)]; Hong Kong Research Grants Council
   (RGC) Early Career Scheme [9048148 (CityU 21209119)]; CityU of Hong Kong
   under APRC [9610488]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61672443, in part by Hong Kong GRF-RGC
   General Research Fund under Grants 9042322 (CityU 11200116), 9042489
   (CityU 11206317), and 9042816 (CityU 11209819), in part by Hong Kong
   Research Grants Council (RGC) Early Career Scheme under Grant 9048148
   (CityU 21209119), and in part by the CityU of Hong Kong under APRC Grant
   9610488.
CR Anonymous Gwern Branwen Aaron Gokaslan, 2019, DANBOORU2019 LARGE S
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen W, 2019, J MECH DESIGN, V141, DOI 10.1115/1.4044076
   Chen X, 2016, ADV NEUR IN, V29
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
   Ding LJ, 2001, PATTERN RECOGN, V34, P721, DOI 10.1016/S0031-3203(00)00023-6
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Han XH, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P266, DOI [10.1109/BigMM.2019.00049, 10.1109/BigMM.2019.00-13]
   Han XJ, 2020, IEEE T MULTIMEDIA, V22, P1619, DOI 10.1109/TMM.2019.2945197
   Hensel M, 2017, ADV NEUR IN, V30
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Kim J, 2020, Arxiv, DOI [arXiv:1907.10830, DOI 10.48550/ARXIV.1907.10830]
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li SC, 2019, IEEE INT CON MULTI, P1192, DOI 10.1109/ICME.2019.00208
   Li XY, 2019, IEEE INT CON MULTI, P652, DOI 10.1109/ICME.2019.00118
   Lin SB, 2019, IEEE INT CON MULTI, P1330, DOI 10.1109/ICME.2019.00231
   Liu MY, 2017, ADV NEUR IN, V30
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Shuai Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P601, DOI 10.1007/978-3-030-58555-6_36
   Simo-Serra E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132703
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soh JW, 2019, IEEE INT SYM MULTIM, P160, DOI 10.1109/ISM46123.2019.00037
   Wang C, 2019, IEEE INT CONF MULTI, P264, DOI 10.1109/ICMEW.2019.00-76
   Wang LD, 2018, IEEE INT CONF AUTOMA, P83, DOI 10.1109/FG.2018.00022
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu Y, 2019, IEEE INT CON MULTI, P1198, DOI 10.1109/ICME.2019.00209
NR 45
TC 7
Z9 7
U1 4
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4002
EP 4015
DI 10.1109/TMM.2021.3111501
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400023
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, HF
   Zhang, HF
   Lu, JF
   Tang, ZN
AF Liu, Huafeng
   Zhang, Haofeng
   Lu, Jianfeng
   Tang, Zhenmin
TI Exploiting Web Images for Fine-Grained Visual Recognition via Dynamic
   Loss Correction and Global Sample Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Training; Training data; Uncertainty; History;
   Feature extraction; Visualization; Fine-grained recognition; web
   datasets; global sample selection; uncertainly-based dynamic loss
   correction
ID CATEGORY
AB To distinguish subtle differences among fine-grained categories, a large amount of well-labeled images are typically required. However, acquiring manual annotations for fine-grained categories is an extremely difficult task as it usually has a high demand for professional knowledge. To this end, directly leveraging web images for learning fine-grained models becomes a natural choice. Nevertheless, due to the existence of label noise, this learning paradigm tends to have a poor performance. In this work, we propose an end-to-end approach by combining dynamic loss correction and global sample selection to alleviate the problem of label noise. Specifically, we leverage the network to predict all samples, record the predictions of recent several epochs, and calculate the uncertainly-based dynamic loss for global sample selection. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed approach. The source code of our approach has been released on the website: https://github.com/NUST-Machine-Intelligence-Laboratory/dlc.
C1 [Liu, Huafeng; Zhang, Haofeng; Lu, Jianfeng; Tang, Zhenmin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China.
C3 Nanjing University of Science & Technology
RP Tang, ZN (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China.
EM liu.hua.feng@outlook.com; zhanghf@njust.edu.cn; lujf@mail.njust.edu.cn;
   tzm.cs@njust.edu.cn
OI Lu, Jianfeng/0000-0002-9190-507X; Liu, Huafeng/0000-0001-5396-3183
FU National Natural Science Foundation of China [62102182, 61976116];
   Natural Science Foundation of Jiangsu Province [BK20210327]; Fundamental
   Research Funds for the Central Universities [30920021135]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62102182 and 61976116, in part by the
   Natural Science Foundation of Jiangsu Province under Grant BK20210327,
   and in part by the Fundamental Research Funds for the Central
   Universities under Grant 30920021135.
CR Anguelov D., 2015, P INT C LEARN REPR W
   [Anonymous], 2013, Tech. rep.
   [Anonymous], 2018, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00749
   Arpit D., 2017, P 34 INT C MACH LEAR, P233, DOI DOI 10.48550/ARXIV.1706.05394
   Branson S., 2014, P BRIT MACH VIS C, P190
   Chang HS, 2017, ADV NEUR IN, V30
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Cui Y, 2016, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2016.130
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gold JR, 2017, PLAN HIST ENVIRON SE, P1
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Huang L., 2020, P ADV NEUR INF PROC, P1
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jiang L., 2017, Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu HF, 2022, IEEE T MULTIMEDIA, V24, P546, DOI 10.1109/TMM.2021.3055024
   Malach E, 2017, ADV NEUR IN, V30
   Nettleton DF, 2010, ARTIF INTELL REV, V33, P275, DOI 10.1007/s10462-010-9156-z
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Ren MY, 2018, PR MACH LEARN RES, V80
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Song H, 2019, PR MACH LEARN RES, V97
   Sun Z., 2021, P IEEE INT C COMP VI
   Sun ZR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P92, DOI 10.1145/3394171.3413978
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wang YS, 2018, PROC CVPR IEEE, P8688, DOI 10.1109/CVPR.2018.00906
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xu Z, 2018, IEEE T PATTERN ANAL, V40, P1100, DOI 10.1109/TPAMI.2016.2637331
   Yao Y., 2020, ACM MM, P1735
   Yao Y., 2016, P 24 ACM INT C MULT, P212
   Yao YZ, 2021, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR46437.2021.00515
   Yao YZ, 2021, PROC CVPR IEEE, P2623, DOI 10.1109/CVPR46437.2021.00265
   Yao YZ, 2020, IEEE T NEUR NET LEAR, V31, P2348, DOI 10.1109/TNNLS.2020.2966644
   Yao YZ, 2020, IEEE T KNOWL DATA EN, V32, P1199, DOI 10.1109/TKDE.2019.2903036
   Yao YZ, 2019, IEEE T MULTIMEDIA, V21, P184, DOI 10.1109/TMM.2018.2847248
   Yao YZ, 2019, IEEE T IMAGE PROCESS, V28, P436, DOI 10.1109/TIP.2018.2869721
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Zhang C., 2020, ACM MM, P2372
   Zhang C., 2021, P ACM INT C MULT
   Zhang C., 2016, COMMUN ACM, V64, P107
   Zhang CY, 2020, AAAI CONF ARTIF INTE, V34, P12781
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhu XQ, 2004, ARTIF INTELL REV, V22, P177, DOI 10.1007/s10462-004-0751-8
NR 52
TC 7
Z9 7
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1105
EP 1115
DI 10.1109/TMM.2021.3118216
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800008
DA 2024-07-18
ER

PT J
AU Reddy, MKK
   Rochan, M
   Lu, YW
   Wang, Y
AF Reddy, Mahesh Kumar Krishna
   Rochan, Mrigank
   Lu, Yiwei
   Wang, Yang
TI AdaCrowd: Unlabeled Scene Adaptation for Crowd Counting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptation models; Cameras; Data models; Computational modeling;
   Backpropagation; Training data; Training; Computer vision; crowd
   counting; deep learning; scene adaptation
AB We address the problem of image-based crowd counting. In particular, we propose a new problem called unlabeled scene-adaptive crowd counting. Given a new target scene, we would like to have a crowd counting model specifically adapted to this particular scene based on the target data that capture some information about the new scene. In this paper, we propose to use one or more unlabeled images from the target scene to perform the adaptation. In comparison with the existing problem setups (e.g. fully supervised), our proposed problem setup is closer to the real-world applications of crowd counting systems. We introduce a novel AdaCrowd framework to solve this problem. Our framework consists of a crowd counting network and a guiding network. The guiding network predicts some parameters in the crowd counting network based on the unlabeled images from a particular scene. This allows our model to adapt to different target scenes. The experimental results on several challenging benchmark datasets demonstrate the effectiveness of our proposed approach compared with other alternative methods. Code is available at https://github.com/maheshkkumar/adacrowd
C1 [Reddy, Mahesh Kumar Krishna] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
   [Rochan, Mrigank; Wang, Yang] Univ Manitoba, Dept Comp Sci, Winnipeg, MB R3T 2N2, Canada.
   [Lu, Yiwei] Univ Waterloo, Dept Comp Sci, Waterloo, ON, Canada.
   [Wang, Yang] Huawei Technol Canada, Winnipeg, MB R3T 2N2, Canada.
C3 Simon Fraser University; University of Manitoba; University of Waterloo;
   Huawei Technologies
RP Reddy, MKK (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
EM mahesh_reddy@sfu.ca; mrochan@cs.umanitoba.ca; y485lu@uwaterloo.ca;
   ywang@cs.umanitoba.ca
RI wang, yitian/JFA-6804-2023; Reddy, Mahesh Kumar Krishna/AAP-6909-2021;
   Zhang, Han/JMR-0670-2023
OI Krishna Reddy, Mahesh Kumar/0000-0001-5645-4931
CR [Anonymous], 2012, P ICML WORKSHOP UNSU
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569
   Chattopadhyay P, 2017, PROC CVPR IEEE, P4428, DOI 10.1109/CVPR.2017.471
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625
   de Vries H, 2017, ADV NEUR IN, V30
   Deng J., 2009, IEEE C COMP VIS PATT
   Fang YY, 2019, IEEE INT CON MULTI, P814, DOI 10.1109/ICME.2019.00145
   Ferris J, 2009, PALG STUD THEAT PERF, P1
   Finn C, 2017, PR MACH LEARN RES, V70
   Gao JY, 2020, IEEE T CIRC SYST VID, V30, P3486, DOI 10.1109/TCSVT.2019.2919139
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hossain M. A., 2019, P BRIT MACH VIS C
   Ioffe S., 2015, P INT C LEARN REPR S
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Jiang XH, 2021, IEEE T MULTIMEDIA, V23, P443, DOI 10.1109/TMM.2020.2980945
   Jiang XH, 2020, IEEE T NEUR NET LEAR, V31, P2705, DOI 10.1109/TNNLS.2019.2933920
   Kang D., 2018, P BRIT MACH VIS C, P1
   Kang Di, 2017, NIPS, P3870
   Kingma D. P., 2014, arXiv
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Liu L, 2021, IEEE T MULTIMEDIA, V23, P1060, DOI 10.1109/TMM.2020.2992979
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Liu X., EUR C COMP VIS
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Loy CC, 2013, IEEE I CONF COMP VIS, P2256, DOI 10.1109/ICCV.2013.270
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Marana AN, 1998, SIBGRAPI '98 - INTERNATIONAL SYMPOSIUM ON COMPUTER GRAPHICS, IMAGE PROCESSING, AND VISION, PROCEEDINGS, P354, DOI 10.1109/SIBGRA.1998.722773
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Nichol A., 2018, ARXIV180302999
   Paragios N, 2001, PROC CVPR IEEE, P1034
   Ravi S., 2016, INT C LEARNING REPRE
   Reddy MKK, 2020, IEEE WINT CONF APPL, P2803, DOI [10.1109/WACV45572.2020.9093409, 10.1109/wacv45572.2020.9093409]
   Sam DB, 2019, AAAI CONF ARTIF INTE, P8868
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Santoro A, 2016, PR MACH LEARN RES, V48
   Shang C, 2016, IEEE IMAGE PROC, P1215, DOI 10.1109/ICIP.2016.7532551
   Simonyan K., 2015, P ICLR
   Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Snell J, 2017, ADV NEUR IN, V30
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Pham VQ, 2015, IEEE I CONF COMP VIS, P3253, DOI 10.1109/ICCV.2015.372
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Walach E, 2016, LECT NOTES COMPUT SC, V9906, P660, DOI 10.1007/978-3-319-46475-6_41
   Wan J, 2019, IEEE I CONF COMP VIS, P1130, DOI 10.1109/ICCV.2019.00122
   Wang C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1299, DOI 10.1145/2733373.28063370-12345-67-8/90/01
   Wang Q, 2022, IEEE T NEUR NET LEAR, V33, P3238, DOI 10.1109/TNNLS.2021.3051371
   Wang Q, 2021, IEEE T PATTERN ANAL, V43, P2141, DOI 10.1109/TPAMI.2020.3013269
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Yu F., 2015, ARXIV
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang Q, 2019, PROC CVPR IEEE, P8289, DOI 10.1109/CVPR.2019.00849
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
NR 60
TC 14
Z9 14
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1008
EP 1019
DI 10.1109/TMM.2021.3062481
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100038
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shen, XB
   Dong, GH
   Zheng, YH
   Lan, L
   Tsang, I
   Sun, QS
AF Shen, Xiaobo
   Dong, Guohua
   Zheng, Yuhui
   Lan, Long
   Tsang, Ivor
   Sun, Quan-Sen
TI Deep Co-Image-Label Hashing for Multi-Label Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Codes; Semantics; Image retrieval; Hash functions; Prototypes;
   Quantization (signal); Hamming distance; Hashing; multi-label; image
   retrieval
AB Deep supervised hashing has greatly improved retrieval performance with the powerful learning capability of deep neural network. In multi-label image retrieval, existing deep hashing simply indicates whether two images are similar by constructing a similarity matrix. However, it ignores the dependency among multiple labels that has been shown important in multi-label application. To fulfill this gap, this paper proposes Deep Co-Image-Label Hashing (DCILH) to discover label dependency. Specifically, DCILH regards image and label as two views, and maps the two views into a common deep Hamming space. DCILH proposes to learn prototype for each label, and preserve similarity among images, labels, and prototypes. To exploit label dependency, DCILH further employs the label-correlation aware loss on the predicted labels, such that predicted output on positive label is enforced to be larger than that on negative label. Extensive experiments on several multi-label benchmarks demonstrate the proposed DCILH outperforms state-of-the-art deep supervised hashing on large-scale multi-label image retrieval.
C1 [Shen, Xiaobo; Sun, Quan-Sen] Nanjing Univ Sci & Technol, Sch Comp & Engn, Nanjing 210094, Peoples R China.
   [Dong, Guohua] Beijing Inst Basic Med Sci, Ctr Computat Biol, Beijing 100850, Peoples R China.
   [Zheng, Yuhui] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
   [Lan, Long] Natl Univ Def Technol, Inst Quantum Informat, Changsha 410073, Peoples R China.
   [Lan, Long] Natl Univ Def Technol, State Key Lab High Performance Comp, Changsha 410073, Peoples R China.
   [Tsang, Ivor] Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW 2006, Australia.
C3 Nanjing University of Science & Technology; Academy of Military Medical
   Sciences - China; Nanjing University of Information Science &
   Technology; National University of Defense Technology - China; National
   University of Defense Technology - China; University of Technology
   Sydney
RP Zheng, YH (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
EM njust.shenxiaobo@gmail.com; dgh1991.learn@gmail.com;
   zheng_yuhui@nuist.edu.cn; long.lan@nudt.edu.cn; ivor.tsang@uts.edu.au;
   sunquansen@njust.edu.cn
RI Tsang, Ivor W/E-8653-2011
OI Lan, Long/0000-0002-4238-8985; Tsang, Ivor/0000-0001-8095-4637; Shen,
   Xiaobo/0000-0001-8494-4532
FU National Natural Science Foundation of China [62176126, 61906091,
   61972206, U20B2065, 61906210]; National Grand R, and D Plan
   [2020AAA0103501]; Natural Science Foundation of Jiangsu Province, China
   [BK20190440, BK20211539]; Fundamental Research Funds for the Central
   Universities [30921011210]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62176126, 61906091, 61972206, U20B2065,
   and 61906210, in part by National Grand R, and D Plan under Grant
   2020AAA0103501, in part by the Natural Science Foundation of Jiangsu
   Province, China under Grants BK20190440 and BK20211539, and in part by
   the Fundamental Research Funds for the Central Universities under Grant
   30921011210. The Guest Editor coordinating the review of this manuscript
   and approving it for publication was Dr. Dan Zeng.
CR [Anonymous], 2009, NIPS
   Cai JJ, 2015, IEEE T IMAGE PROCESS, V24, P261, DOI 10.1109/TIP.2014.2372616
   Cao ZJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1653, DOI 10.1145/3240508.3240516
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li SY, 2020, IEEE T MULTIMEDIA, V22, P1542, DOI 10.1109/TMM.2019.2946096
   Li WJ, 2016, IJCAI, P1711
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HM, 2019, INT J COMPUT VISION, V127, P1217, DOI 10.1007/s11263-019-01174-4
   Liu W., 2020, IEEE T PATTERN ANAL
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Ma C, 2020, IEEE T MULTIMEDIA, V22, P760, DOI 10.1109/TMM.2019.2931808
   Norouzi M.E., 2011, ICML
   Paszke A, 2019, ADV NEUR IN, V32
   Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen YM, 2020, INFORM SCIENCES, V539, P145, DOI 10.1016/j.ins.2020.05.114
   Song JK, 2020, INT J COMPUT VISION, V128, P2243, DOI 10.1007/s11263-020-01305-2
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yang HC, 2014, PROC CVPR IEEE, P1955, DOI 10.1109/CVPR.2014.251
   Yeh CK, 2017, AAAI CONF ARTIF INTE, P2838
   Zha ZJ, 2008, PROC CVPR IEEE, P333
   Zha ZJ, 2009, J VIS COMMUN IMAGE R, V20, P97, DOI 10.1016/j.jvcir.2008.11.009
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhong CL, 2017, LECT NOTES COMPUT SC, V10366, P169, DOI 10.1007/978-3-319-63579-8_14
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 43
TC 27
Z9 27
U1 7
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1116
EP 1126
DI 10.1109/TMM.2021.3119868
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800009
DA 2024-07-18
ER

PT J
AU Wang, DJ
   Zhang, X
   Wan, Y
   Yu, DJ
   Xu, GD
   Deng, SG
AF Wang, Dongjing
   Zhang, Xin
   Wan, Yao
   Yu, Dongjin
   Xu, Guandong
   Deng, Shuiguang
TI Modeling Sequential Listening Behaviors With Attentive Temporal Point
   Process for Next and Next New Music Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Music; Recommender systems; Task analysis; Context modeling; Computer
   science; Collaboration; Real-time systems; Recommender system; temporal
   point process; attention mechanism; user profiling; music recommendation
ID EMOTION; NETWORK
AB Recommender systems, which aim to provide personalized suggestions for users, have proven to be an effective approach to cope with the information overload problem existing in many online applications and services. In this paper, we target two specific sequential recommendation tasks, next music recommendation and next new music recommendation, to predict the next (new) music piece that users would like based on their historical listening records. In current music recommender systems, various kinds of auxiliary/side information, e.g., item contents and users' contexts, have been taken into account to facilitate user/item preference modeling and have yielded comparable performance improvement. Despite the gained benefits, it is still a challenging and important problem to fully exploit sequential music listening records due to the complexity and diversity of interactions and temporal contexts among users and music, as well as the dynamics of users' preferences. To this end, this paper proposes a novel Attentive Temporal Point Process (ATPP) approach for sequential music recommendation, which is mainly composed of a temporal point process model and an attention mechanism. Our ATPP can effectively capture the long- and short-term preferences from the sequential behaviors of users for sequential music recommendation. Specifically, ATPP is able to discover the complex sequential patterns from the interaction between users and music with the temporal point process, as well as model the dynamic impact of historical music listening records on next (new) music pieces adaptively with an attention mechanism. Comprehensive experiments on four real-world music datasets demonstrate that the proposed approach ATPP outperforms state-of-the-art baselines in both next and next new music recommendation tasks.
C1 [Wang, Dongjing; Zhang, Xin; Yu, Dongjin] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Wan, Yao] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
   [Xu, Guandong] Univ Technol Sydney, Adv Analyt Inst, Sydney, NSW 2007, Australia.
   [Deng, Shuiguang] Zhejiang Univ, Sch Comp Sci & Technol, Hangzhou 310027, Peoples R China.
C3 Hangzhou Dianzi University; Huazhong University of Science & Technology;
   University of Technology Sydney; Zhejiang University
RP Zhang, X (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
EM dongjing.wang@hdu.edu.cn; zhangxin@hdu.edu.cn; wanyao@hust.edu.cn;
   yudj@hdu.edu.cn; guandong.xu@uts.edu.au; dengsg@zju.edu.cn
RI Wang, Dongjing/JMJ-5616-2023; zhou, xuan/GZA-8157-2022; Xu,
   Guandong/HSH-3463-2023
OI Xu, Guandong/0000-0003-4493-6663; Yu, Dongjin/0000-0001-8919-1613; Wang,
   Dongjing/0000-0003-2152-0446
FU Natural Science Foundation of Zhejiang Province [LQ20F020015,
   LQ21F020015]; Fundamental Research Funds for the Provincial University
   of Zhejiang [GK199900299012-017]; Key Science and Technology Project of
   Zhejiang Province [2020C01165]
FX This work was supported in part by the Natural Science Foundation of
   Zhejiang Province under Grants LQ20F020015 and LQ21F020015, in part by
   the Fundamental Research Funds for the Provincial University of Zhejiang
   under Grant GK199900299012-017, and in part by the Key Science and
   Technology Project of Zhejiang Province under Grant 2020C01165.
CR Adamic LA, 2000, SCIENCE, V287, DOI 10.1126/science.287.5461.2115a
   Ayata D, 2018, IEEE T CONSUM ELECTR, V64, P196, DOI 10.1109/TCE.2018.2844736
   Celma O, 2010, MUSIC RECOMMENDATION AND DISCOVERY, P1, DOI 10.1007/978-3-642-13287-2
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Cheng Derek, 2020, P INT SOC MUS INF RE, P583
   Cheng ZY, 2016, ACM T INFORM SYST, V34, DOI 10.1145/2846092
   Deng SG, 2015, EXPERT SYST APPL, V42, P9284, DOI 10.1016/j.eswa.2015.08.029
   Dong MQ, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P688, DOI 10.1145/3394486.3403113
   Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875
   Dutta HS, 2020, IEEE T INF FOREN SEC, V15, P2667, DOI 10.1109/TIFS.2020.2970601
   Gao XY, 2020, IEEE T MULTIMEDIA, V22, P1647, DOI 10.1109/TMM.2019.2945180
   Han JY, 2020, IEEE T NEUR NET LEAR, V31, P737, DOI 10.1109/TNNLS.2019.2909432
   HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83
   Hidasi B., 2016, PROC 4 INT C LEARN R
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Kang WC, 2018, IEEE DATA MINING, P197, DOI 10.1109/ICDM.2018.00035
   Kingma D. P., 2014, arXiv
   Lee J H., 2019, Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, P663
   Li JC, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P322, DOI 10.1145/3336191.3371786
   Li J, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1419, DOI 10.1145/3132847.3132926
   Liang DW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P689, DOI 10.1145/3178876.3186150
   Liu SW, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P2029, DOI 10.1145/3397271.3401252
   Luo X, 2021, IEEE T BIG DATA, V7, P227, DOI 10.1109/TBDATA.2019.2916868
   Ma C, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P825, DOI 10.1145/3292500.3330984
   Manolovitz Brian, 2020, ISMIR, P633
   Mei H., 2017, Advances in Neural Information Processing Systems, P6754
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Paradarami TK, 2017, EXPERT SYST APPL, V83, P300, DOI 10.1016/j.eswa.2017.04.046
   Quadrana M, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3190616
   Rendle S., 2010, P 19 INT C WORLD WID, P811, DOI DOI 10.1145/1772690.1772773
   Ricci F, 2011, RECOMMENDER SYSTEMS HANDBOOK, P1, DOI 10.1007/978-0-387-85820-3_1
   Rotondi R, 2019, STOCH ENV RES RISK A, V33, P709, DOI 10.1007/s00477-019-01663-5
   Sachdeva N, 2018, 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS), P417, DOI 10.1145/3240323.3240397
   Schedl M., 2015, Recommender Systems Handbook, V2nd, P453, DOI DOI 10.1007/978-1-4899-7637-6_13
   Schedl M, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P103, DOI 10.1145/2911996.2912004
   Shakirova E, 2017, IEEE NW RUSS YOUNG, P548, DOI 10.1109/EIConRus.2017.7910613
   Shen TC, 2020, AAAI CONF ARTIF INTE, V34, P206
   Song J, 2017, IEEE T KNOWL DATA EN, V29, P1888, DOI 10.1109/TKDE.2017.2700392
   Sun F, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1441, DOI 10.1145/3357384.3357895
   Tang JX, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P565, DOI 10.1145/3159652.3159656
   Turrin R., 2015, RecSys posters, P75
   Vall A., 2018, PROC JOINT EUR C MAC, P639
   Vall A, 2019, USER MODEL USER-ADAP, V29, P527, DOI 10.1007/s11257-018-9215-8
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang DJ, 2021, IEEE T NEUR NET LEAR, V32, P1375, DOI 10.1109/TNNLS.2020.2984665
   Wang DJ, 2018, WORLD WIDE WEB, V21, P1399, DOI 10.1007/s11280-017-0521-6
   Wang DJ, 2018, INFORM RETRIEVAL J, V21, P230, DOI 10.1007/s10791-017-9317-7
   Wang PF, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P403, DOI 10.1145/2766462.2767694
   Xiao J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3119
   Xu HT, 2017, IEEE T KNOWL DATA EN, V29, P157, DOI 10.1109/TKDE.2016.2618925
   Yang LX, 2020, IEEE T MULTIMEDIA, V22, P2111, DOI 10.1109/TMM.2019.2949434
   Yang YJ, 2018, IEEE T MULTIMEDIA, V20, P1888, DOI 10.1109/TMM.2017.2779043
   Ying HC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3926
   Yuan FJ, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P582, DOI 10.1145/3289600.3290975
   Zangerle E, 2021, IEEE T AFFECT COMPUT, V12, P78, DOI 10.1109/TAFFC.2018.2846596
   Zhang J, 2019, IEEE T SYST MAN CY-S, V49, P1141, DOI 10.1109/TSMC.2017.2738151
   Zhang Q., 2020, 2017 IEEE INT C PROG, P183
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhao PP, 2022, IEEE T KNOWL DATA EN, V34, P2512, DOI 10.1109/TKDE.2020.3007194
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zheng E, 2018, EXPERT SYST APPL, V106, P244, DOI 10.1016/j.eswa.2018.04.014
   Zheng HT, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9040703
   Zhou YP, 2018, IEEE T MULTIMEDIA, V20, P2153, DOI 10.1109/TMM.2017.2781364
   Zhu Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3602
NR 64
TC 6
Z9 6
U1 3
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4170
EP 4182
DI 10.1109/TMM.2021.3114545
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 4K8TP
UT WOS:000852215000002
DA 2024-07-18
ER

PT J
AU Wang, SR
   Wang, SQ
   Yang, WH
   Zhang, XF
   Wang, SS
   Ma, SW
   Gao, W
AF Wang, Shurun
   Wang, Shiqi
   Yang, Wenhan
   Zhang, Xinfeng
   Wang, Shanshe
   Ma, Siwei
   Gao, Wen
TI Towards Analysis-Friendly Face Representation With Scalable Feature and
   Texture Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Visualization; Standards; Transform coding; Feature
   extraction; Image reconstruction; Deep learning; Feature compression;
   texture compression; scalable coding; multitask learning
ID VIDEO
AB Compactly representing visual information plays a fundamental role in optimizing the ultimate utility of myriad visual data-centered applications. Numerous approaches have been proposed to efficiently compress the texture and visual features for human visual perception and machine intelligence, respectively; however, much less work has been dedicated to studying the interactions between them. Here, we investigate the integration of feature and texture compression and show that a universal and collaborative visual information representation can be achieved in a hierarchical way. In particular, we study feature and texture compression in a scalable coding framework, where the base layer serves as the deep learning feature and the enhancement layer targets to perfectly reconstruct the texture. Based on the strong generative capability of deep neural networks, the gap between the base feature layer and enhancement layer is further filled with feature-level texture reconstruction, with the goal of further constructing texture representations from features. As such, the residuals between the original and reconstructed texture could be further conveyed in the enhancement layer. To improve the efficiency of the proposed framework, the base layer neural network is trained in a multitask manner such that the learned features enjoy both high-quality reconstruction and high-accuracy analysis. The framework and optimization strategies are further applied in face image compression, and promising coding performance has been achieved in terms of both rate-fidelity and rate-accuracy evaluations.
C1 [Wang, Shurun; Wang, Shiqi; Yang, Wenhan] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Zhang, Xinfeng] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100190, Peoples R China.
   [Wang, Shanshe; Ma, Siwei; Gao, Wen] Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
C3 City University of Hong Kong; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Peking University
RP Wang, SQ (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.; Zhang, XF (corresponding author), Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100190, Peoples R China.
EM srwang3-c@my.cityu.edu.hk; shiqwang@cityu.edu.hk; wyang34@cityu.edu.hk;
   xfzhang@ucas.ac.cn; sswang@pku.edu.cn; swma@pku.edu.cn; wgao@pku.edu.cn
RI Zhang, Xinfeng/X-8148-2019
OI Shurun, WANG/0000-0002-0758-9175
FU National Natural Science Foundation of China [62022002]; Hong Kong
   Research grants Council, Early Career Scheme (RGC ECS) [21211018];
   General Research Fund (GRF) [11203220]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62022002, in part by the Hong Kong
   Research grants Council, Early Career Scheme (RGC ECS) under Grant
   21211018, and in part by General Research Fund (GRF) under Grant
   11203220.
CR Abdelazim A, 2010, INT ARCH PHOTOGRAMM, V38, P1
   [Anonymous], 2014, COMPUT NOW
   [Anonymous], 2007, Technical Report 07-49
   Balle J, 2017, 5 INT C LEARN REPR I
   Balle J., 2018, INT C LEARN REPR ICL, P1
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   BT, 2012, 2020 BT INT TEL UN, P1
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chakraborty S, 2018, IEEE T CIRC SYST VID, V28, P171, DOI 10.1109/TCSVT.2016.2603535
   Chandrasekhar V, 2009, VIS COMMUN IMAGE PRO
   Chao JS, 2015, IEEE T CIRC SYST VID, V25, P958, DOI 10.1109/TCSVT.2014.2367354
   Chen FD, 2017, IEEE T CIRC SYST VID, V27, P2639, DOI 10.1109/TCSVT.2016.2593599
   Chen X, 2015, POULTRY SCI, V94, P984, DOI [10.3382/ps/pev043, 10.1109/PESGM.2015.7286027]
   Chen Z, 2020, IEEE T IMAGE PROCESS, V29, P2230, DOI 10.1109/TIP.2019.2941660
   Ding L, 2020, IEEE T IMAGE PROCESS, V29, P3734, DOI 10.1109/TIP.2020.2965306
   Duan LY, 2019, IEEE MULTIMEDIA, V26, P44, DOI 10.1109/MMUL.2018.2873844
   Duan LY, 2016, IEEE T IMAGE PROCESS, V25, P179, DOI 10.1109/TIP.2015.2500034
   Duan LY, 2020, IEEE T IMAGE PROCESS, V29, P8680, DOI 10.1109/TIP.2020.3016485
   Duan LY, 2019, IEEE MULTIMEDIA, V26, P8, DOI 10.1109/MMUL.2018.2873564
   Edmundson D, 2012, ELMAR PROC, P75
   Experts Group, 1995, PROC ISOIEC JTC 1SC, P1
   Gao W, 2014, IEEE INTELL SYST, V29, P30, DOI 10.1109/MIS.2013.101
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   Glorot X., 2010, P INT C ART INT STAT, P249
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jun An, 2015, 2015 IEEE Power & Energy Society General Meeting, P1, DOI 10.1109/PESGM.2015.7286363
   Kingma D.P., 2014, ARXIV14126980
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Li Y, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM)
   Lin S, 2015, 2015 IEEE 81 VEHICUL, P1
   Liu D, 2018, LECT NOTES COMPUT SC, V10705, P61, DOI 10.1007/978-3-319-73600-6_6
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lou YH, 2020, IEEE T MULTIMEDIA, V22, P3002, DOI 10.1109/TMM.2020.2966885
   Lou YH, 2019, IEEE J SEL AREA COMM, V37, P1489, DOI 10.1109/JSAC.2019.2916488
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Ma SW, 2019, IEEE T CIRC SYST VID, V29, P3095, DOI 10.1109/TCSVT.2018.2873102
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Mohan A, 2017, IEEE INT SYMP CIRC S, P286
   Paul M, 2011, IEEE T CIRC SYST VID, V21, P1242, DOI 10.1109/TCSVT.2011.2138750
   Pfaff J., 2019, 14 M JOINT VIDEO EXP, P1
   Rabbani M, 2002, J ELECTRON IMAGING, V11, P286, DOI 10.1117/1.1469618.00000
   Redondi A, 2013, IEEE INT WORKSH MULT, P278, DOI 10.1109/MMSP.2013.6659301
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shi ZB, 2014, IEEE J EM SEL TOP C, V4, P17, DOI 10.1109/JETCAS.2014.2298291
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Toderici G., 2016, P INT C LEARN REPR I
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang M, 2020, IEEE T IMAGE PROCESS, V29, P2931, DOI 10.1109/TIP.2019.2955238
   Wang SL, 2021, ASIA PAC J MANAG, V38, P1305, DOI 10.1007/s10490-020-09714-0
   Wang SR, 2019, IEEE IMAGE PROC, P2691, DOI [10.1109/ICIP.2019.8803255, 10.1109/icip.2019.8803255]
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wong CK, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1687
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zhang XG, 2014, IEEE T IMAGE PROCESS, V23, P769, DOI 10.1109/TIP.2013.2294549
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhao L, 2018, IEEE T CIRC SYST VID, V28, P1346, DOI 10.1109/TCSVT.2016.2645616
   Zhao X, 2015, PALG STUD CHIN ED GL, P1, DOI [10.1109/PVSC.2015.7356358, 10.1007/978-1-137-47941-9]
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 65
TC 19
Z9 20
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3169
EP 3181
DI 10.1109/TMM.2021.3094300
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000038
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, XY
   Guo, YD
   Yang, ZQ
   Zhang, JY
AF Wang, Xueying
   Guo, Yudong
   Yang, Zhongqi
   Zhang, Juyong
TI Prior-Guided Multi-View 3D Head Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Three-dimensional displays; Hair; Solid modeling;
   Semantics; Faces; Rendering (computer graphics); 3D head reconstruction;
   multi-view stereo; prior guidance; neural rendering
ID FACE RECONSTRUCTION; HAIR; SEGMENTATION; GEOMETRY; CAPTURE; STEREO
AB Recovery of a 3D head model including the complete face and hair regions is still a challenging problem in computer vision and graphics. In this paper, we consider this problem using only a few multi-view portrait images as input. Previous multi-view stereo methods that have been based, either on optimization strategies or deep learning techniques, suffer from low-frequency geometric structures such as unclear head structures and inaccurate reconstruction in hair regions. To tackle this problem, we propose a prior-guided implicit neural rendering network. Specifically, we model the head geometry with a learnable signed distance field (SDF) and optimize it via an implicit differentiable renderer with the guidance of some human head priors, including the facial prior knowledge, head semantic segmentation information and 2D hair orientation maps. The utilization of these priors can improve the reconstruction accuracy and robustness, leading to a high-quality integrated 3D head model. Extensive ablation studies and comparisons with state-of-the-art methods demonstrate that our method can generate high-fidelity 3D head geometries with the guidance of these priors.
C1 [Wang, Xueying; Guo, Yudong; Yang, Zhongqi; Zhang, Juyong] Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhang, JY (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Anhui, Peoples R China.
EM WXY17719@mail.ustc.edu.cn; gyd2011@mail.ustc.edu.cn;
   ivanov@mail.ustc.edu.cn; juyong@ustc.edu.cn
RI chen, ying/HHS-8254-2022
OI Guo, Yudong/0000-0002-4788-9594
FU National Natural Science Foundation of China [62122071]; Youth
   Innovation Promotion Association CAS [2018495]; Fundamental Research
   Funds for the Central Universities [WK3470000021]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62122071, in part by the Youth
   Innovation Promotion Association CAS under Grant 2018495, and in part by
   the Fundamental Research Funds for the Central Universities under Grant
   WK3470000021. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhu Liu.
CR 2K sports, NBA 2K22
   Aanæs H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9
   Agrawal S, 2020, IEEE WINT CONF APPL, P81, DOI 10.1109/WACV45572.2020.9093455
   Anbarjafari G, 2019, B POL ACAD SCI-TECH, V67, P125, DOI 10.24425/bpas.2019.127341
   Tran AT, 2018, PROC CVPR IEEE, P3935, DOI 10.1109/CVPR.2018.00414
   [Anonymous], Agisoft Metashape
   [Anonymous], 2015, P 3 INT C LEARN REPR
   Bai ZQ, 2020, PROC CVPR IEEE, P5849, DOI 10.1109/CVPR42600.2020.00589
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14
   Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Chen AP, 2019, IEEE I CONF COMP VIS, P9428, DOI 10.1109/ICCV.2019.00952
   Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162
   Cheng S, 2020, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR42600.2020.00260
   Dai YC, 2019, INT CONF 3D VISION, P1, DOI 10.1109/3DV.2019.00010
   Dou PF, 2018, IMAGE VISION COMPUT, V80, P80, DOI 10.1016/j.imavis.2018.09.004
   Fan X, 2021, IEEE T MULTIMEDIA, V23, P1252, DOI 10.1109/TMM.2020.2994506
   Ferrari C, 2017, IEEE T MULTIMEDIA, V19, P2666, DOI 10.1109/TMM.2017.2707341
   Furukawa Y, 2013, FOUND TRENDS COMPUT, V9, P1, DOI 10.1561/0600000052
   Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106
   Gerig T, 2018, IEEE INT CONF AUTOMA, P75, DOI 10.1109/FG.2018.00021
   Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257
   Guo YD, 2019, IEEE T PATTERN ANAL, V41, P1294, DOI 10.1109/TPAMI.2018.2837742
   Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084
   He HY, 2019, COMPUT GRAPH-UK, V80, P85, DOI 10.1016/j.cag.2019.03.008
   Huang BC, 2021, IEEE IMAGE PROC, P3163, DOI 10.1109/ICIP42928.2021.9506469
   JAIN AK, 1991, PATTERN RECOGN, V24, P1167, DOI 10.1016/0031-3203(91)90143-S
   Jianfeng Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P674, DOI 10.1007/978-3-030-58548-8_39
   Jiang L, 2018, IEEE T IMAGE PROCESS, V27, P4756, DOI 10.1109/TIP.2018.2845697
   Jiang Y, 2020, PROC CVPR IEEE, P1248, DOI 10.1109/CVPR42600.2020.00133
   Karras T., 2018, INT CONFLEARN REPRES
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Khot T., 2019, PROC IEEECVF C COMPU
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Konami Digital Entertainment, EFOOTBALL
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Liang S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275020
   Lifkooee M. Z., 2018, PROC INT C COMPUT SC, P1
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Lou JW, 2020, IEEE T MULTIMEDIA, V22, P730, DOI 10.1109/TMM.2019.2933338
   Luo LJ, 2012, PROC CVPR IEEE, P1490, DOI 10.1109/CVPR.2012.6247838
   Mapillary, OPENSFM
   Mayost D., 2014, THESIS U TORONTO
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Nam G, 2019, PROC CVPR IEEE, P155, DOI 10.1109/CVPR.2019.00024
   Paris S, 2004, ACM T GRAPHIC, V23, P712, DOI 10.1145/1015706.1015784
   Paszke A., 2017, AUTOMATIC DIFFERENTI
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Pillai RK, 2019, IEEE INT CONF COMP V, P3082, DOI 10.1109/ICCVW.2019.00371
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Smith WAP, 2020, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR42600.2020.00506
   Song SL, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392491
   Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XY, 2020, PROC CVPR IEEE, P737, DOI 10.1109/CVPR42600.2020.00082
   Wei YC, 2005, ACM T GRAPHIC, V24, P816, DOI 10.1145/1073204.1073267
   Wenninger S., 2020, PROC ACM S VIRTUAL R, P1
   Wu FZ, 2019, PROC CVPR IEEE, P959, DOI 10.1109/CVPR.2019.00105
   Wu PF, 2017, IEEE T MULTIMEDIA, V19, P266, DOI 10.1109/TMM.2016.2612761
   Xu HB, 2021, AAAI CONF ARTIF INTE, V35, P3030
   Yang JY, 2020, PROC CVPR IEEE, P4876, DOI 10.1109/CVPR42600.2020.00493
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yao Y, 2020, PROC CVPR IEEE, P1787, DOI 10.1109/CVPR42600.2020.00186
   Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567
   Yariv Lior, 2020, NeurIPS, V33
   Zhou JY, 2018, VISUAL COMPUT, V34, P1177, DOI 10.1007/s00371-018-1553-3
NR 68
TC 11
Z9 11
U1 5
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4028
EP 4040
DI 10.1109/TMM.2021.3111485
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400025
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xing, MT
   Xie, HT
   Tan, QF
   Fang, SC
   Wang, YX
   Zha, ZJ
   Zhang, YD
AF Xing, Mengting
   Xie, Hongtao
   Tan, Qingfeng
   Fang, Shancheng
   Wang, Yuxin
   Zha, Zhengjun
   Zhang, Yongdong
TI Boundary-Aware Arbitrary-Shaped Scene Text Detector With Learnable
   Embedding Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Task analysis; Proposals; Noise measurement;
   Detectors; Shape; Noise reduction; Scene text detection; boundary
   representation; false positive suppression
AB Benefiting from the popularity of deep learning theory, scene text detection algorithms have developed rapidly in recent years. Methods representing text region by text segmentation map are proved to capture arbitrary-shaped text in a more flexible and accurate way. However, such segmentation-based methods are prone to be disturbed by the text-like background patterns (like the fence, grass, etc.), which generally suffer from imprecise boundary detail problem. In this paper, LEMNet is proposed to handle the imprecise boundary problem by guiding the generation of text boundary based on a priori constraint. In the training stage, Boundary Segmentation Branch is firstly constructed to predict coarse boundary mask for each text instance. Then, through mapping pixels into an embedding space, the proposed Pixel Embedding Branch makes the embedding representation of boundary points learn to be more similar, meanwhile enlarging the characteristic distance between background points and boundary points. During inference, noise in the coarse boundary segmentation map can be effectively suppressed by a Noisy Point Suppression Algorithm among pixel embedding vectors. In this way, LEMNet can generate a more precise boundary description of text regions. To further enhance the distinguishability of boundary features, we propose a Context Enhancement Module to capture feature interactions in different representation subspaces, in which features are parallelly performed attention and concatenated to generate enhanced features. Extensive experiments are conducted over four challenging datasets, which demonstrate the effectiveness of LEMNet. Specifically, LEMNet achieves F-measure of 85.2%, 87.6% and 85.2% on CTW1500, Total-Text and MSRA-TD500 respectively, which is the latest SOTA.
C1 [Xing, Mengting; Xie, Hongtao; Fang, Shancheng; Wang, Yuxin; Zha, Zhengjun; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230052, Anhui, Peoples R China.
   [Tan, Qingfeng] Guangzhou Univ, Cyberspace Inst Adv Technol, Guangzhou 511442, Guangdong, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Guangzhou University
RP Xie, HT; Tan, QF (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230052, Anhui, Peoples R China.
EM metingx@mail.ustc.edu.cn; htxie@ustc.edu.cn; tqf528@gzhu.edu.cn;
   fangsc@ustc.edu.cn; wangyx58@mail.ustc.edu.cn; zhazj@ustc.edu.cn;
   zhyd73@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020
OI Wang, Yuxin/0000-0002-0228-6220; Xing, Mengting/0000-0002-7285-6671
FU National Nature Science Foundation of China [62022076, U1936210,
   61972105]; Fundamental Research Funds for the Central Universities
   [WK3480000011]
FX This work was supported by the National Nature Science Foundation of
   China (62022076, U1936210, and 61972105), the Fundamental Research Funds
   for the Central Universities under Grant WK3480000011.
CR [Anonymous], 2015, ARXIV151104377
   Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Chen HT, 2020, PROC CVPR IEEE, P1638, DOI 10.1109/CVPR42600.2020.00171
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   De Brabandere Bert, 2017, Semantic Instance Segmentation with a Discriminative Loss Function
   Deli Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12110, DOI 10.1109/CVPR42600.2020.01213
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Kingma D.P., 2014, ARXIV14126980
   Kong S, 2018, PROC CVPR IEEE, P9018, DOI 10.1109/CVPR.2018.00940
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu YL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3052
   Liu YL, 2019, PROC CVPR IEEE, P9604, DOI 10.1109/CVPR.2019.00984
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Newell A, 2017, ADV NEUR IN, V30
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, PROC INT CONF DOC, P1429, DOI 10.1109/ICDAR.2017.233
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P111, DOI 10.1145/3394171.3413819
   Wang H, 2020, AAAI CONF ARTIF INTE, V34, P12160
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P1316, DOI 10.1109/TMM.2020.2995290
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xu WQ, 2019, IEEE I CONF COMP VIS, P5167, DOI 10.1109/ICCV.2019.00527
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yuliang L., 2017, ARXIV171202170
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhou K, 2016, DESTECH TRANS COMP
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhou Y., 2020, P 28 ACM INT C MULT, P2571
NR 58
TC 5
Z9 5
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3129
EP 3143
DI 10.1109/TMM.2021.3093727
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000035
DA 2024-07-18
ER

PT J
AU Ye, YL
   He, YK
   Pan, TJ
   Li, JJ
   Shen, HT
AF Ye, Yalan
   He, Yukun
   Pan, Tongjie
   Li, Jingjing
   Shen, Heng Tao
TI Alleviating Domain Shift via Discriminative Learning for Generalized
   Zero-Shot Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generalized zero-shot learning; domain shift problem; generative
   adversarial networks; latent attributes; seen/unseen classifier
AB In zero-shot learning (ZSL) tasks, especially in generalized zero-shot learning (GZSL), the model tends to classify unseen test samples into seen categories, which is well known as the domain shift problem, because the model is trained from seen samples without unseen samples. Recently, generative adversarial network (GAN) based methods have achieved good performance in GZSL, which replace real unseen features by synthesizing fake ones to mitigate the domain shift. However, the domain shift problem is still not well solved, due to the lacking of unseen samples in the training progress of the GAN generator. In this paper, we propose a generative model named discriminative learning GAN (DL-GAN) to alleviate the domain shift in GZSL. Specifically, the DL-GAN is designed with three novel components: a dual-stream embedding model that aligns features to the ground-truth attributes to extract discriminative latent attributes from features, an attribute-based generative model that generates high-quality unseen features from semantic attributes to guarantee inter-class discriminability and semantic consistency, and a seen/unseen classifier that leverages validation samples to distinguish seen samples from unseen ones. Experimental results on four widely used datasets verify that our proposed approach significantly outperforms the state-of-the-art methods under the GZSL protocol.
C1 [Ye, Yalan; He, Yukun; Pan, Tongjie; Li, Jingjing; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Li, JJ (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM yalanye@uestc.edu.cn; juekun55@163.com; tongjiepan@foxmail.com;
   lijin117@yeah.net; shenhengtao@hotmail.com
RI LI, Jing/HNB-5575-2023; li, jian/IAQ-2794-2023; Wang,
   Jing/IQW-3496-2023; li, jy/HTT-1535-2023; Shen, Heng Tao/ABD-5331-2021;
   Li, Jing/GYU-5036-2022; ye, yalan/JWP-9553-2024
OI Wang, Jing/0000-0002-8296-2961; Pan, Tongjie/0000-0002-1538-5789
FU National Natural Science Foundation of China [61976047, 61872061,
   61806039]; Science, and Technology Department of Sichuan Province of
   China [2019YFG0122, 2020YFG0087, 2020YFG0080, 2020YFG0326]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61976047, 61872061, and 61806039 and in
   part by the Science, and Technology Department of Sichuan Province of
   China under Grants 2019YFG0122, 2020YFG0087, 2020YFG0080, and
   2020YFG0326. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Ramazan S. Aygun.
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Atzmon Y., 2019, C COMP VIWS PATT REC, p11 671
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Fletcher Roger., 1980, PRACTICAL METHODS OP
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo JC, 2021, IEEE T MULTIMEDIA, V23, P524, DOI 10.1109/TMM.2020.2984091
   Guo YC, 2018, AAAI CONF ARTIF INTE, P6870
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang HJ, 2018, LECT NOTES COMPUT SC, V11214, P121, DOI 10.1007/978-3-030-01249-6_8
   Kingma DP, 2013, ARXIV
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Lee H, 2018, IEEE INT CONF COMM
   Li JJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1348, DOI 10.1145/3394171.3413503
   Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050
   Li JJ, 2022, IEEE T CYBERNETICS, V52, P8167, DOI 10.1109/TCYB.2021.3050803
   Li JJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1587, DOI 10.1145/3343031.3350901
   Li JJ, 2019, IEEE T IMAGE PROCESS, V28, P6103, DOI 10.1109/TIP.2019.2924174
   Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779
   Liu CB, 2020, IEEE T MULTIMEDIA, V22, P1785, DOI 10.1109/TMM.2019.2954747
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Min S., 2020, P IEEE C COMP VIS PA, p12 661
   Min SB, 2020, IEEE T IMAGE PROCESS, V29, P4996, DOI 10.1109/TIP.2020.2977457
   Ni J, 2019, ADV NEUR IN, V32
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Norouzi M, 2013, ARXIV13125650
   OSHERSON DN, 1991, COGNITIVE SCI, V15, P251, DOI 10.1207/s15516709cog1502_3
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Peng PX, 2016, LECT NOTES COMPUT SC, V9908, P336, DOI 10.1007/978-3-319-46493-0_21
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang X., 2020, INT SOC THROMBOSIS H
   Wang YD, 2020, KNOWL-BASED SYST, V196, DOI 10.1016/j.knosys.2020.105796
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Zhang HF, 2020, IEEE T NEUR NET LEAR, V31, P2361, DOI 10.1109/TNNLS.2019.2955157
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
NR 46
TC 16
Z9 16
U1 0
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1325
EP 1337
DI 10.1109/TMM.2021.3063616
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200007
DA 2024-07-18
ER

PT J
AU Zhang, X
   Zhang, FF
   Xu, CS
AF Zhang, Xi
   Zhang, Feifei
   Xu, Changsheng
TI Explicit Cross-Modal Representation Learning for Visual Commonsense
   Reasoning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognition; Video recording; Syntactics; Visualization; Task analysis;
   Semantics; Linguistics; Visual Commonsense Reasoning; explicit
   reasoning; syntactic structure; interpretability
AB Given a question about an image, Visual Commonsense Reasoning (VCR) needs to provide not only a correct answer, but also a rationale to justify the answer. VCR is a challenging task due to the requirement of proper semantic alignment and reasoning between the image and linguistic expression. Recent approaches offer a great promise by exploring holistic attention mechanisms or graph-based networks, but most of them do implicit reasoning and ignore the semantic dependencies among the linguistic expression. In this paper, we propose a novel explicit cross-modal representation learning network for VCR by incorporating syntactic information into the visual reasoning and natural language understanding. The proposed method enjoys several merits. First, based on a two-branch neural module network, we can do explicit cross-modal reasoning guided by the high-level syntactic structure of linguistic expression. Second, the semantic structure of the linguistic expression is incorporated into a syntactic GCN to facilitate language understanding. Third, our explicit cross-modal representation learning network can provide a traceable reasoning-flow, which offers visible fine-grained evidence of the answer and rationale. Quantitative and qualitative evaluations on the public VCR dataset demonstrate that our approach performs favorably against state-of-the-art methods.
C1 [Zhang, Xi; Zhang, Feifei; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Zhang, Xi; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.; Xu, CS (corresponding author), Peng Cheng Lab, Shenzhen 518066, Peoples R China.
EM zhangxi2019@ia.ac.cn; feifei.zhang@ia.ac.cn; csxu@nlpr.ia.ac.cn
RI zhang, fei/KHU-5230-2024; Zhang, Feifei/A-3199-2015; zhang,
   xi/KSL-6747-2024; Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023
OI xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2018AAA0100604];
   National Natural Science Foundation of China [61720106006, 62002355,
   61721004, 61832002, 61532009, 61751211, 62072455, U1705262, U1836220];
   Key Research Program of Frontier Sciences of CAS [QYZDJSSW-JSC039];
   National Postdoctoral Program for Innovative Talents [BX20190367];
   Beijing Natural Science Foundation [L201001]; Jiangsu Province key
   research, and development plan [BE2020036]
FX This work was supported by National Key Research and Development Program
   of China under Grant 2018AAA0100604, in part by National Natural Science
   Foundation of China underGrants 61720106006, 62002355, 61721004,
   61832002, 61532009, 61751211, 62072455, U1705262, and U1836220, in part
   by Key Research Program of Frontier Sciences of CAS under Grant
   QYZDJSSW-JSC039, in part by National Postdoctoral Program for Innovative
   Talents (BX20190367), in part by Beijing Natural Science Foundation
   (L201001), and in part by Jiangsu Province key research, and development
   plan (BE2020036).
CR Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   [Anonymous], 2017, ARXIV161004325
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bastings J., 2017, P 2017 C EMP METH NA, P1957, DOI 10.18653/v1/d17-1209
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Chen Q, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1657, DOI 10.18653/v1/P17-1152
   Chen S., 2020, PROC IEEECVF C COMPU, p10 638
   Dan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10052, DOI 10.1109/CVPR42600.2020.01007
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Eyzaguirre, 2020, PROC IEEECVF C COMPU, p12 817
   Gan C, 2017, IEEE I CONF COMP VIS, P1829, DOI 10.1109/ICCV.2017.201
   Gao Difei, 2020, CVPR, P12746
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Guo WY, 2020, AAAI CONF ARTIF INTE, V34, P91
   Gupta N., 2019, PROC INT C LEARN REP
   Han C, 2019, ADV NEUR IN, V32
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hudson Drew A, 2019, P IEEE CVF C COMP VI, P6700, DOI DOI 10.1109/CVPR.2019.00686
   Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44
   Jiang YC, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4474
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Kingma D. P., 2014, arXiv
   Li Q, 2020, P 37 INT C MACH LEAR, P5884
   Lin J., 2019, PROC C WORKSHOP NEUR, p15 615
   Liu F., 2020, IEEE TRAN POWER ELEC, P1, DOI DOI 10.1109/TMM.2020.3026892
   Liu Y., 2020, PROC ASS ADV ARTIF I, p11 645
   Lu JS, 2019, ADV NEUR IN, V32
   Mao J., 2018, PROC INT C LEARN REP
   Niu YL, 2019, PROC CVPR IEEE, P6672, DOI 10.1109/CVPR.2019.00684
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Schuster S., 2015, P 4 WORKSHOP VISION, P70
   Selvaraju R. R., 2020, CVPR
   Shi JX, 2019, PROC CVPR IEEE, P8368, DOI 10.1109/CVPR.2019.00857
   Singh A, 2019, PROC CVPR IEEE, P8309, DOI 10.1109/CVPR.2019.00851
   Wang, 2020, PROC IEEECVF C COMPU, p10 760
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Wang X., 2020, CVPR, P10126, DOI DOI 10.1109/CVPR42600.2020.01014
   Wen Z, 2021, IEEE T CIRC SYST VID, V31, P1042, DOI 10.1109/TCSVT.2020.2991866
   Wu L., 2019, Advances in Neural Information Processing Systems, P5669
   Xinzhe Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P553, DOI 10.1007/978-3-030-58545-7_32
   Yang Jianwei, 2020, ARXIV201211587
   Yang S., 2020, P IEEE CVF C COMP VI, P9952
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yi K, 2019, PROC INT C LEARN REP
   Yi KX, 2018, ADV NEUR IN, V31
   Yu Fei, 2020, ARXIV200616934
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu WJ, 2019, ADV NEUR IN, V32
   Yuan ZQ, 2021, IEEE T MULTIMEDIA, V23, P1744, DOI 10.1109/TMM.2020.3002667
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zhang HM, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1025
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
   Zhu ZH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1097
NR 59
TC 13
Z9 13
U1 2
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2986
EP 2997
DI 10.1109/TMM.2021.3091882
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000024
DA 2024-07-18
ER

PT J
AU Fu, ZQ
   Hu, W
AF Fu, Zeqing
   Hu, Wei
TI Dynamic Point Cloud Inpainting via Spatial-Temporal Graph Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Coherence; Dynamics; Optimization; Laplace
   equations; Measurement; Geometry; 3D dynamic point clouds; inpainting;
   inter-frame coherence; spatial-temporal graph
ID FILLING HOLES; SURFACES
AB The maturity of depth sensors and laser scanning techniques has enabled the convenient acquisition of 3D dynamic point clouds-one natural representation of 3D objects/scenes in motion, leading to a wide range of applications such as immersive tele-presence, autonomous driving, augmented and virtual reality. Nevertheless, dynamic point clouds usually exhibit holes of missing data, thus inpainting is crucial to the subsequent rendering or downstream understanding tasks. Dynamic point cloud inpainting has been largely overlooked so far, which is also quite challenging due to the irregular sampling patterns both in the spatial domain and temporal domain. To this end, we propose an efficient dynamic point cloud inpainting method based on a learnable spatial-temporal graph representation, exploiting both the second-order inter-frame coherence and the intra-frame self-similarity. The key is the second-order inter-frame coherence that enforces the consistent flow in 3D motion over time, for which we search the temporal correspondence in consecutive frames for the same underlying surface by the point-to-plane distance and represent the correlation between them via temporal edge weights in the graph. Based on the second-order inter-frame coherence and intra-frame self-similarity, we formulate dynamic point cloud inpainting as a joint optimization problem of the desired point cloud and underlying spatial-temporal graph, which is regularized by consistency in the temporal edge weights and smoothness in the spatial domain. We analyze and reformulate the optimization, leading to an efficient alternating minimization algorithm. Experimental results show that the proposed approach outperforms several competing methods significantly, both on synthetic holes and real holes.
C1 [Fu, Zeqing; Hu, Wei] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Hu, W (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
EM zeqing_fu@pku.edu.cn; forhuwei@pku.edu.cn
FU National Key R&D project of China [2019YFF0302903]; Beijing Natural
   Science Foundation [19L2053]; National Natural Science Foundation of
   China [61972009]
FX This work was supported in part by the National Key R&D project of China
   under Contract 2019YFF0302903, in part by Beijing Natural Science
   Foundation [19L2053], and in part by National Natural Science Foundation
   of China [61972009].
CR Andrew G., 2007, P 24 INT C MACH LEAR, V24, P33, DOI [10.1145/1273496.1273501, DOI 10.1145/1273496.1273501]
   [Anonymous], 2016, document ISO/IEC JTC1/SC29/WG1 N16331
   Baingana B, 2017, IEEE T SIGNAL PROCES, V65, P985, DOI 10.1109/TSP.2016.2628354
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Biyikogu T., 2007, LECT NOTES MATH, V1915
   Chalmoviansky P, 2003, LECT NOTES COMPUT SC, V2768, P196
   Chen C, 2016, ISPRS J PHOTOGRAMM, V119, P90, DOI 10.1016/j.isprsjprs.2016.05.007
   Chetverikov D, 2002, INT C PATT RECOG, P545, DOI 10.1109/ICPR.2002.1047997
   Chung F. R. K., 1997, Spectral graph theory
   Cignoni P., 2008, EUR IT CHAPT C, DOI [DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136, 10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136]
   de Amorim CC, 2019, LECT NOTES COMPUT SC, V11731, P646, DOI 10.1007/978-3-030-30493-5_59
   dEon E., 2017, Standard ISO/IEC JTC1/SC29 Joint WG11/WG1 (MPEG/JPEG)input document WG11M40059/WG1M74006
   Dinesh C., 2017, P 2017 IEEE VISUAL C, P1, DOI [10.1109/VCIP.2017.8305070, DOI 10.1109/VCIP.2017.8305070]
   Dinesh C, 2018, IEEE SIGNAL PROC LET, V25, P878, DOI 10.1109/LSP.2018.2831621
   Dong XW, 2019, IEEE SIGNAL PROC MAG, V36, P44, DOI 10.1109/MSP.2018.2887284
   Ebner T., 2018, JTC1SC29WG11 ISOIEC
   FRIEDMAN S., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1
   Fu ZQ, 2018, IEEE IMAGE PROC, P2137, DOI 10.1109/ICIP.2018.8451550
   Gao X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P601, DOI 10.1145/3343031.3351170
   Grant M., 2011, CVX USERS GUIDE
   Guo SN, 2019, AAAI CONF ARTIF INTE, P922
   Guo Z., 2020, 2020 IEEE INT C MULT, P1
   Hallac D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P205, DOI 10.1145/3097983.3098037
   He J, 2019, IEEE IMAGE PROC, P4385, DOI [10.1109/icip.2019.8803497, 10.1109/ICIP.2019.8803497]
   Hu W., 2020, GRAPH SIGNAL PROCESS
   Hu W, 2020, IEEE T SIGNAL PROCES, V68, P2841, DOI 10.1109/TSP.2020.2978617
   Hu W, 2019, IEEE T IMAGE PROCESS, V28, P4087, DOI 10.1109/TIP.2019.2906554
   Hu W, 2015, IEEE T IMAGE PROCESS, V24, P419, DOI 10.1109/TIP.2014.2378055
   Hu W, 2012, IEEE IMAGE PROC, P1297, DOI 10.1109/ICIP.2012.6467105
   Javaheri Alireza, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P1, DOI 10.1109/ICMEW.2017.8026263
   Kalofolias V, 2017, INT CONF ACOUST SPEE, P2826, DOI 10.1109/ICASSP.2017.7952672
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Li L, 2021, IEEE T MULTIMEDIA, V23, P2806, DOI 10.1109/TMM.2020.3016894
   Lin HB, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P402, DOI 10.1109/SIPROCESS.2016.7888293
   Loop C., 2016, JTC1SC29 ISOIEC
   Lozes F, 2015, IEEE SIGNAL PROC MAG, V32, P103, DOI 10.1109/MSP.2015.2408631
   Lozes F, 2014, IEEE T IMAGE PROCESS, V23, P3896, DOI 10.1109/TIP.2014.2336548
   Mateos G, 2019, IEEE SIGNAL PROC MAG, V36, P16, DOI 10.1109/MSP.2018.2890143
   Muraki Y, 2017, COMPUT SCI RES NOTES, V2701, P65
   Ortega A, 2018, P IEEE, V106, P808, DOI 10.1109/JPROC.2018.2820126
   Pang JH, 2017, IEEE T IMAGE PROCESS, V26, P1770, DOI 10.1109/TIP.2017.2651400
   Quinsat Y, 2015, INT J ADV MANUF TECH, V81, P411, DOI 10.1007/s00170-015-7185-0
   Sahay Pratyush, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301388
   Sahay P, 2012, INT C PATT RECOG, P101
   Sandryhaila A, 2013, IEEE T SIGNAL PROCES, V61, P1644, DOI 10.1109/TSP.2013.2238935
   Setty S, 2018, MACH VISION APPL, V29, P329, DOI 10.1007/s00138-017-0886-7
   Setty Shankar, 2015, 2015 5 NAT C COMP VI, P1
   Shen G., 2010, 2010 28th Picture Coding Symposium (PCS 2010), P566, DOI 10.1109/PCS.2010.5702565
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Song C, 2020, AAAI CONF ARTIF INTE, V34, P914
   Spielman DA, 2007, ANN IEEE SYMP FOUND, P29, DOI 10.1109/FOCS.2007.56
   Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Wang HN, 2007, IMAGE VISION COMPUT, V25, P103, DOI 10.1016/j.imavis.2005.12.006
   Wu XJ, 2014, 2014 11TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P5329, DOI 10.1109/WCICA.2014.7053624
   Yamada K., 2020, TIME VARYING GRAPH L
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zeng J, 2020, IEEE T IMAGE PROCESS, V29, P3474, DOI 10.1109/TIP.2019.2961429
NR 58
TC 9
Z9 9
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3022
EP 3034
DI 10.1109/TMM.2021.3068606
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000006
DA 2024-07-18
ER

PT J
AU Guo, JC
   Guo, S
AF Guo, Jingcai
   Guo, Song
TI A Novel Perspective to Zero-Shot Learning: Towards an Alignment of
   Manifold Structures via Semantic Feature Expansion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Zero-shot learning; manifold; autoencoder; semantic feature; alignment
ID DEEP; REPRESENTATION; CLASSIFICATION
AB Zero-shot learning aims at recognizing unseen classes (no training example) with knowledge transferred from seen classes. This is typically achieved by exploiting a semantic feature space shared by both seen and unseen classes, i.e., attribute or word vector, as the bridge. One common practice in zero-shot learning is to train a projection between the visual and semantic feature spaces with labeled seen classes examples. When inferring, this learned projection is applied to unseen classes and recognizes the class labels by some metrics. However, the visual and semantic feature spaces are mutually independent and have quite different manifold structures. Under such a paradigm, most existing methods easily suffer from the domain shift problem and weaken the performance of zero-shot recognition. To address this issue, we propose a novel model called AMS-SFE. It considers the alignment of manifold structures by semantic feature expansion. Specifically, we build upon an autoencoder-based model to expand the semantic features from the visual inputs. Additionally, the expansion is jointly guided by an embedded manifold extracted from the visual feature space of the data. Our model is the first attempt to align both feature spaces by expanding semantic features and derives two benefits: first, we expand some auxiliary features that enhance the semantic feature space; second and more importantly, we implicitly align the manifold structures between the visual and semantic feature spaces; thus, the projection can be better trained and mitigate the domain shift problem. Extensive experiments show significant performance improvement, which verifies the effectiveness of our model.
C1 [Guo, Jingcai; Guo, Song] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University
RP Guo, S (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
EM cscjguo@comp.polyu.edu.hk; song.guo@polyu.edu.hk
RI Guo, Jingcai/JAO-0302-2023; Guo, Jingcai/HRD-6940-2023; Guo,
   Song/AAZ-4542-2020
OI Guo, Song/0000-0001-9831-2202; Guo, Jingcai/0000-0002-0449-4525
FU National Natural Science Foundation of China [61872310]; Innovation and
   Technology Commission of the HKSAR [BBV2]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61872310 and in part by the Innovation
   and Technology Commission of the HKSAR to the Hong Kong Branch of
   National Rail Transit Electrification and Automation Engineering
   Technology Research Center (no. BBV2).
CR Akata Z, 2016, PROC CVPR IEEE, P59, DOI 10.1109/CVPR.2016.14
   Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   [Anonymous], 2009, Advances in neural information processing systems
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483
   Baldi P., 2012, P ICML WORKSH UNS TR, P37
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Bucher M, 2016, LECT NOTES COMPUT SC, V9909, P730, DOI 10.1007/978-3-319-46454-1_44
   Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chonavel T, 2003, SIGNAL PROCESS, V83, P307, DOI 10.1016/S0165-1684(02)00417-6
   Chu WQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1561
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Ding X, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2327
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fu YW, 2016, PROC CVPR IEEE, P5337, DOI 10.1109/CVPR.2016.576
   Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354
   Fu ZY, 2015, PROC CVPR IEEE, P2635, DOI 10.1109/CVPR.2015.7298879
   Goodfellow IJ, 2014, ADV NEUR IN, P2672, DOI DOI 10.1145/3422622
   Greenspan H, 2016, IEEE T MED IMAGING, V35, P1153, DOI 10.1109/TMI.2016.2553401
   Guo JC, 2019, IEEE INT CON MULTI, P73, DOI 10.1109/ICME.2019.00021
   Guo JC, 2019, INT CONF ACOUST SPEE, P3287, DOI 10.1109/ICASSP.2019.8682869
   Guo JC, 2019, INT CONF ACOUST SPEE, P3517, DOI [10.1109/ICASSP.2019.8682891, 10.1109/icassp.2019.8682891]
   Guo J, 2016, 2016 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING, APPLICATIONS AND TECHNOLOGIES (BDCAT), P149, DOI 10.1145/3006299.3006307
   Hsu Chin-Wei, 2010, Technical Report
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kingma D. P., 2014, arXiv
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Krizhevsky A., 2012, ADV NEURAL INF PROCE, V25, P1097
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li YN, 2017, PROC CVPR IEEE, P5207, DOI 10.1109/CVPR.2017.553
   Liu RS, 2018, INT J DIGIT MULTIMED, V2018, DOI [10.1155/2018/7543875, 10.1109/TMM.2018.2812605]
   Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653
   Lu Y., 2016, P 25 INT JOINT C ART, P3432
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z
   Rahman S, 2018, IEEE T IMAGE PROCESS, V27, P5652, DOI 10.1109/TIP.2018.2861573
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Romera-Paredes Bernardino, 2015, ICML
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Sener O, 2018, 32 C NEURAL INFORM P, V31
   Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9
   Socher R., 2013, P 2013 C EMP METH NA, V2013, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791
   Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tong B., 2018, ADVERSARIAL ZERO SHO
   Torrey L., 2010, IGI Global, P242, DOI 10.4018/978-1-60566-766-9.CH011
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang Q, 2017, AAAI CONF ARTIF INTE, P4292
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wenchao Yu, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P208, DOI 10.1007/978-3-642-40994-3_14
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xu X, 2017, PROC CVPR IEEE, P2007, DOI 10.1109/CVPR.2017.217
   Yang Yongxin, 2014, ARXIV14127489
   Yu YL, 2019, IEEE T CYBERNETICS, V49, P3755, DOI 10.1109/TCYB.2018.2850750
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang ZM, 2016, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2016.649
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
NR 75
TC 23
Z9 23
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 524
EP 537
DI 10.1109/TMM.2020.2984091
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kim, H
   Lee, EC
   Seo, Y
   Im, DH
   Lee, IK
AF Kim, Hayeon
   Lee, Eun-Cheol
   Seo, Yongseok
   Im, Dong-Hyuck
   Lee, In-Kwon
TI Character Detection in Animated Movies Using Multi-Style Adaptation and
   Visual Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Motion pictures; Animation; Streaming media; Adaptation models;
   Visualization; Task analysis; Detectors; Image analysis; video content
   analysis; object detection; multi-domain learning; visual attention
ID REPRESENTATION; PREDICTION; DISCOVERY; NETWORKS; SCENE
AB Automatic identification of fictional characters is one of the primary analysis techniques for video content. A common approach to detect characters in live-action movies involves detecting human faces; however, this approach cannot be used in non-realistic domains, such as animated movies. Detection of characters in animated movies presents two major challenges: the same subject of character can be expressed in various unique styles, and there are no stylistic or other restrictions on the nature and design of character objects. To address these challenges, we introduce the "animation adaptive region-based convolutional neural network" model to detect characters in animated movies and determine whether the detected characters are human or non-human types. Our model extends the Faster R-CNN model, which is a two-stage object detector, in the following manner: 1) we add a hierarchical animation adaptation module to learn the variety of unique styles from animated movies using a single model; 2) we incorporate a double-detector architecture to focus on the regions that are visually important in determining the character class. We build a new dataset for the animated character detection task. Experiments on this dataset show that our model outperforms other existing representative object detector models in terms of character detection. Furthermore, our model achieves significant performance improvements compared with previous state-of-the-art methods used for the character dictionary generation task. Our model is robust for a variety of animation styles and can find common visual representations of all types of characters, providing an effective way to detect animated characters.
C1 [Kim, Hayeon; Lee, Eun-Cheol; Lee, In-Kwon] Yonsei Univ, Dept Comp Sci, Seoul 03722, South Korea.
   [Seo, Yongseok; Im, Dong-Hyuck] Elect & Telecommun Res Inst, Daejeon 34129, South Korea.
C3 Yonsei University; Electronics & Telecommunications Research Institute -
   Korea (ETRI)
RP Lee, IK (corresponding author), Yonsei Univ, Dept Comp Sci, Seoul 03722, South Korea.
EM qoocrab@gmail.com; dl4636@naver.com; yongseok@etri.re.kr;
   iammoni@etri.re.kr; iklee@yonsei.ac.kr
RI Lee, In-Kwon/AGP-6124-2022; Kim, Hayeon/AAY-5003-2021
OI Kim, Hayeon/0000-0002-6529-0921; Seo, Yongseok/0000-0001-8964-3739; Lee,
   In-Kwon/0000-0002-1534-1882
FU Ministry of Culture, Sports, and Tourism (MCST); Korea Copyright
   Commission (KCC) in the Copyright Technology Research & Development
   Program 2020; Ministry of Science and ICT (MSIT), Korea, under the
   Information Technology Research Center (ITRC) support program
   [IITP-2018-0-01419]
FX Manuscript received January 27, 2020; revised May 8, 2020; accepted June
   21, 2020. Date of publication July 3, 2020; date of current version June
   25, 2021. This work was supported in part by theMinistry of Culture,
   Sports, and Tourism (MCST) and Korea Copyright Commission (KCC) in the
   Copyright Technology Research & Development Program 2020, in part by the
   Ministry of Science and ICT (MSIT), Korea, under the Information
   Technology Research Center (ITRC) support program under Grant
   IITP-2018-0-01419 supervised by the Institute for Information and
   Communications Technology Promotion (IITP). The associate editor
   coordinating the reviewof this manuscript and approving it for
   publication was Dr. Wen-Huang Cheng.
CR [Anonymous], 2006, P 14 ACM INT C MULT
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Bilen Hakan, 2017, ARXIV170107275
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Chu WT, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P417, DOI 10.1145/3078971.3079031
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Cui P, 2012, IEEE T MULTIMEDIA, V14, P102, DOI 10.1109/TMM.2011.2176110
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gao GY, 2016, IEEE T MULTIMEDIA, V18, P1749, DOI 10.1109/TMM.2016.2579305
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Hsu HK, 2020, IEEE WINT CONF APPL, P738, DOI [10.1109/WACV45572.2020.9093358, 10.1109/wacv45572.2020.9093358]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525
   Jiang HZ, 2017, IEEE INT CONF AUTOMA, P650, DOI [10.1109/FG.2017.82, 10.1109/MWSYM.2017.8058653]
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Khodabandeh M, 2019, IEEE I CONF COMP VIS, P480, DOI 10.1109/ICCV.2019.00057
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kurzhals K, 2016, IEEE T MULTIMEDIA, V18, P2149, DOI 10.1109/TMM.2016.2614184
   Liao HS, 2016, IEEE T MULTIMEDIA, V18, P2196, DOI 10.1109/TMM.2016.2614227
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YJ, 2016, IEEE T MULTIMEDIA, V18, P1269, DOI 10.1109/TMM.2016.2557061
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Nguyen N.-V., 2019, MULTIMEDIA MODELING, P637
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Ortiz EG, 2013, PROC CVPR IEEE, P3531, DOI 10.1109/CVPR.2013.453
   Qin XR, 2017, PROC INT CONF DOC, P1074, DOI 10.1109/ICDAR.2017.178
   Rebuffi SA, 2017, ADV NEUR IN, V30
   Rebuffi SA, 2018, PROC CVPR IEEE, P8119, DOI 10.1109/CVPR.2018.00847
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P586, DOI 10.1109/TMM.2012.2188784
   Schoenauer-Sebag A, 2019, P INT C LEARN REPR
   Schutze H, 2009, INTRO INFORM RETRIEV, P356
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Somandepalli K, 2018, IEEE T MULTIMEDIA, V20, P539, DOI 10.1109/TMM.2017.2745712
   Sommer LW, 2017, IEEE WINT CONF APPL, P311, DOI 10.1109/WACV.2017.41
   Springenberg J.T., 2014, STRIVING SIMPLICITY
   Summerfield C, 2016, TRENDS COGN SCI, V20, P401, DOI 10.1016/j.tics.2016.03.008
   Tarvainen J, 2014, IEEE T MULTIMEDIA, V16, P2085, DOI 10.1109/TMM.2014.2357688
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xudong Wang, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7281, DOI 10.1109/CVPR.2019.00746
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang XS, 2015, IEEE T MULTIMEDIA, V17, P1562, DOI 10.1109/TMM.2015.2449660
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou HJ, 2017, IEEE INT CONF COMP V, P760, DOI 10.1109/ICCVW.2017.95
NR 66
TC 1
Z9 1
U1 4
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1990
EP 2004
DI 10.1109/TMM.2020.3006372
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100012
DA 2024-07-18
ER

PT J
AU Li, K
   Wu, YX
   Xue, Y
   Qian, XM
AF Li, Ke
   Wu, Yuxia
   Xue, Yao
   Qian, Xueming
TI Viewpoint Recommendation Based on Object-Oriented 3D Scene
   Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Object detection; Cameras; Feature
   extraction; Image reconstruction; Social networking (online); Two
   dimensional displays; 3D reconstruction; social media; aesthetics
   evaluation; viewpoint recommendation
ID SOCIAL MEDIA; MODEL
AB Viewpoint recommendation can recommend several viewpoints for taking aesthetic photographs of a place-of-interest (POI) and is of great importance for photography assistance. In this paper, we propose a system that can assist a user in choosing good viewpoints for taking high-quality photographs. Our system is based on social media and 3D reconstruction. To reduce the time cost and improve the quality of 3D reconstruction, we propose a weakly supervised object detection method that is used before 3D reconstruction. The camera pose of images is recovered by the subsequent 3D reconstruction pipeline. We use a convolutional neural network (CNN) to extract 2D image features, and we fuse them with 3D camera pose features to learn their relationships to image aesthetics. The trained model is utilized to evaluate the aesthetics of images. Finally, the 3D space of all possible camera poses is divided into 3D grids, and the aesthetics score of each grid is evaluated. We combine the aesthetics and diversity of all viewpoints and recommend several high-quality viewpoints. Experimental results indicate that our approach can help users choose viewpoints that will result in high-quality photographs while maintaining diversity.
C1 [Li, Ke] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
   [Wu, Yuxia; Xue, Yao] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Smiles Lab, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong
   University; Xi'an Jiaotong University
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, Smiles Lab, Xian 710049, Peoples R China.
EM likely@stu.xjtu.edu.cn; wuyuxia@stu.xjtu.edu.cn; yxue2@ualberta.ca;
   qianxm@mail.xjtu.edu.cn
FU NSFC [61732008, 61772407]; Guangdong Provincial Science and Technology
   Plan [2016A010101005]; Microsoft Research Asia
FX This work was supported in part by the NSFC under Grants 61732008 and
   61772407, in part by the Guangdong Provincial Science and Technology
   Plan under Grant 2016A010101005, and in part by Microsoft Research Asia.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sebastian Knorr.
CR [Anonymous], 2004, P ACM C INT US INT
   [Anonymous], 2014, P INT C INT MULT COM
   [Anonymous], 2010, ACM MULTIMEDIA
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Guangjun Shi, 2013, 2013 5th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC), P474, DOI 10.1109/IHMSC.2013.119
   He JW, 2019, IEEE T VIS COMPUT GR, V25, P2636, DOI 10.1109/TVCG.2018.2853751
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Khan I, 2018, IEEE T MULTIMEDIA, V20, P841, DOI 10.1109/TMM.2017.2758740
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Liu J, 2014, 2014 11TH INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (FSKD), P766, DOI 10.1109/FSKD.2014.6980933
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Liu W., 2016, LECT NOTES COMPUT SC, P21, DOI DOI 10.1007/978-3-319-46448-0_2
   Qian XM, 2021, IEEE T MULTIMEDIA, V23, P378, DOI 10.1109/TMM.2020.2977478
   Qian XM, 2019, KNOWL-BASED SYST, V164, P107, DOI 10.1016/j.knosys.2018.10.028
   Qian XM, 2018, IEEE T IMAGE PROCESS, V27, P1178, DOI 10.1109/TIP.2017.2769454
   Qian XM, 2017, P IEEE, V105, P1937, DOI 10.1109/JPROC.2017.2731600
   Qian XM, 2015, IEEE T CIRC SYST VID, V25, P1857, DOI 10.1109/TCSVT.2014.2369731
   Rawat YS, 2017, IEEE T CIRC SYST VID, V27, P149, DOI 10.1109/TCSVT.2016.2555658
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Saleh FS, 2018, IEEE T PATTERN ANAL, V40, P1382, DOI 10.1109/TPAMI.2017.2713785
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shi XX, 2019, KNOWL-BASED SYST, V186, DOI 10.1016/j.knosys.2019.104955
   Simon I, 2007, IEEE I CONF COMP VIS, P274
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Tang P, 2018, LECT NOTES COMPUT SC, V11215, P370, DOI 10.1007/978-3-030-01252-6_22
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304
   Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326
   Wang SG, 2018, IEEE T MULTIMEDIA, V20, P3148, DOI 10.1109/TMM.2018.2829602
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P171, DOI 10.1109/TMM.2014.2384396
NR 35
TC 4
Z9 4
U1 3
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 257
EP 267
DI 10.1109/TMM.2020.2981237
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600020
DA 2024-07-18
ER

PT J
AU Lu, ZZ
   Yang, GA
   Yang, JJ
   Wang, YH
AF Lu, Zhengzhi
   Yang, Guoan
   Yang, Junjie
   Wang, Yuhao
TI An Adaptive Arbitrary Multiresolution Decomposition for Multiscale
   Geometric Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transforms; Image resolution; Image denoising; Image reconstruction;
   Noise reduction; Two dimensional displays; Laplace equations; Arbitrary
   multiresolution decomposition; image denoising; multiscale geometric
   analysis; nonsubsampled Laplacian pyramid; pseudopolar Fourier
   transformation
ID DIRECTIONAL FILTER BANKS; CONTOURLET TRANSFORM; DESIGN; REPRESENTATION;
   FEATURES
AB The nonsubsampled Laplacian pyramid (NSLP) is widely used as a common multiresolution decomposition method in various nonsubsampled image transforms. However, the NSLP has a fixed spectrum partition, and thus cannot represent images accurately, and flexibly. We propose a new adaptive arbitrary multiresolution decomposition to solve these problems. First, using the affine characteristics of a pseudopolar Fourier transform (PPFT), we apply a 1-D nonuniform filter bank to the modulated PPFT to obtain a 2-D arbitrary resolution filter bank. This filter surpasses the limitation of fixed spectrum partitioning of the traditional tree structure. We then demonstrate that the proposed method satisfies the compact frame condition, and has translation invariance, and a linear phase. Furthermore, we propose an adaptive spectrum division approach at various scales based on image spectrum information based on a 2-D filter bank of arbitrary multiresolution, so our method can capture important visual information more accurately. Finally, we combine our method with a nonsubsampled directional filter bank of the nonsubsampled contourlet transform to create a new multiscale geometric analysis (MGA) method, and verify that the new method can have perfect reconstruction properties. The new MGA also performs better in image denoising, and recognition experiments than state-of-the-art MGA methods with translation invariance.
C1 [Lu, Zhengzhi; Yang, Guoan; Yang, Junjie; Wang, Yuhao] Xi An Jiao Tong Univ, Sch Automat Sci & Engn, Xian 710049, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University
RP Yang, GA (corresponding author), Xi An Jiao Tong Univ, Sch Automat Sci & Engn, Xian 710049, Shaanxi, Peoples R China.
EM lu947867114@stu.xjtu.edu.cn; gayang@mail.xjtu.edu.cn; nappoo@163.com;
   Yuhao_wang@foxmail.com
OI Yang, Guoan/0000-0003-2414-2719; Lu, Zhengzhi/0000-0001-7392-6320
FU National Natural Science Foundation of China [61673314, 61573273,
   11504297]; National Key R&D Program Project of China [2018YFB1700104]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61673314, 61573273, and 11504297 and in
   part by the National Key R&D Program Project of China under Grant
   2018YFB1700104.
CR Abbadeni N, 2011, IEEE T IMAGE PROCESS, V20, P236, DOI 10.1109/TIP.2010.2060345
   Allili MS, 2014, IEEE T MULTIMEDIA, V16, P772, DOI 10.1109/TMM.2014.2298832
   [Anonymous], 2015, Sparse image and signal processing: Wavelets and related geometric multiscale analysis
   Averbuch A, 2006, APPL COMPUT HARMON A, V21, P145, DOI 10.1016/j.acha.2005.11.003
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Bindima T, 2018, IEEE T CIRCUITS-II, V65, P2057, DOI 10.1109/TCSII.2018.2817592
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Dong YY, 2016, IEEE T MULTIMEDIA, V18, P549, DOI 10.1109/TMM.2016.2522639
   Easley G, 2008, APPL COMPUT HARMON A, V25, P25, DOI 10.1016/j.acha.2007.09.003
   Fang L, 2018, J VIS COMMUN IMAGE R, V51, P23, DOI 10.1016/j.jvcir.2017.12.013
   Fang L, 2016, IET SIGNAL PROCESS, V10, P106, DOI 10.1049/iet-spr.2015.0075
   Fang LL, 2020, NEUROCOMPUTING, V396, P266, DOI 10.1016/j.neucom.2018.10.094
   Fearnhead P, 2019, J AM STAT ASSOC, V114, P169, DOI 10.1080/01621459.2017.1385466
   Firoiu I, 2009, IEEE T INSTRUM MEAS, V58, P2410, DOI 10.1109/TIM.2009.2016382
   Killick R, 2012, J AM STAT ASSOC, V107, P1590, DOI 10.1080/01621459.2012.737745
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Liang LL, 2011, IEEE T IMAGE PROCESS, V20, P283, DOI 10.1109/TIP.2010.2052267
   Liu BH, 2010, NAT NEUROSCI, V13, P89, DOI 10.1038/nn.2443
   Liu JC, 2020, IEEE GEOSCI REMOTE S, V17, P1637, DOI 10.1109/LGRS.2019.2949806
   Liu SQ, 2017, IEEE T GEOSCI REMOTE, V55, P2985, DOI 10.1109/TGRS.2017.2657602
   Madhe SP, 2017, CIRC SYST SIGNAL PR, V36, P4441, DOI 10.1007/s00034-017-0519-4
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Muramatsu S, 2012, IEEE T IMAGE PROCESS, V21, P2434, DOI 10.1109/TIP.2011.2182055
   Muramatsu S, 2012, IEEE T IMAGE PROCESS, V21, P2314, DOI 10.1109/TIP.2011.2181527
   Nguyen TT, 2008, IEEE T SIGNAL PROCES, V56, P4651, DOI 10.1109/TSP.2007.912897
   Ram MR, 2013, IEEE T INSTRUM MEAS, V62, P2639, DOI 10.1109/TIM.2013.2259114
   Sadreazami H, 2019, IEEE T CIRCUITS-II, V66, P151, DOI 10.1109/TCSII.2018.2846547
   Sadreazami H, 2016, IEEE T MULTIMEDIA, V18, P196, DOI 10.1109/TMM.2015.2508147
   Selesnick IW, 2005, IEEE SIGNAL PROC MAG, V22, P123, DOI 10.1109/MSP.2005.1550194
   Shi GM, 2009, IEEE T SIGNAL PROCES, V57, P4936, DOI 10.1109/TSP.2009.2027737
   Starck JL, 2002, IEEE T IMAGE PROCESS, V11, P670, DOI [10.1109/TIP.2002.1014998, 10.1117/12.408568]
   Xie XH, 2010, PATTERN RECOGN, V43, P4177, DOI 10.1016/j.patcog.2010.06.019
   Yan Chunman, 2014, [自动化学报, Acta Automatica Sinica], V40, P757
   Yang GA, 2019, IEEE ACCESS, V7, P88243, DOI 10.1109/ACCESS.2019.2924674
   Yang HY, 2014, NEURAL NETWORKS, V57, P152, DOI 10.1016/j.neunet.2014.06.007
   Yi S, 2009, IEEE T IMAGE PROCESS, V18, P929, DOI 10.1109/TIP.2009.2013082
   Zhao H, 2017, IEEE SIGNAL PROC LET, V24, P843, DOI 10.1109/LSP.2017.2696886
   Zhong W, 2010, IEEE T SIGNAL PROCES, V58, P3390, DOI 10.1109/TSP.2010.2045424
NR 39
TC 0
Z9 0
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2883
EP 2893
DI 10.1109/TMM.2020.3017921
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600027
DA 2024-07-18
ER

PT J
AU Min, SB
   Yao, HT
   Xie, HT
   Zha, ZJ
   Zhang, YD
AF Min, Shaobo
   Yao, Hantao
   Xie, Hongtao
   Zha, Zheng-Jun
   Zhang, Yongdong
TI Domain-Oriented Semantic Embedding for Zero-Shot Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Visualization; Image recognition; Image reconstruction;
   Training; Gallium nitride; Search problems; Zero-shot learning;
   multi-modality embedding; recognition
AB Zero-Shot Learning (ZSL) targets to recognize images from new classes. Existing methods focus on learning a projection function to associate the visual features and category descriptions in the seen domain, which is directly transferred to the unseen domain. However, due to the inherent domain shift, a single shared projection cannot fully capture the domain difference and similarity, thereby making the unseen samples tend to be recognized as seen categories. In this paper, we propose a novel Domain-Oriented Semantic Embedding (DOSE) network that learns specific projections for different domains to better capture the domain characteristics for unbiased ZSL. Besides a domain-shared projection, DOSE learns two auxiliary domain-specific sub-projections to model the semantic-visual association in respective seen and unseen domains. Specifically, the domain-specific projections are learned in a cycle consistency way to capture domain characteristics, and a domain division constraint is developed to penalize the margin between two domain embeddings. Furthermore, to boost semantic-visual association, a semantic-visual dual attention module is designed to automatically remove trivial information in both visual and semantic embeddings under a co-guidance learning manner. Experiments on four public benchmarks prove that the proposed DOSE is robust to the domain shift problem in ZSL and obtains an averaged 5.6% improvement in terms of harmonic mean.
C1 [Min, Shaobo; Xie, Hongtao; Zha, Zheng-Jun; Zhang, Yongdong] Univ Sci & Technol China, Natl Engn Lab Brain Inspired Intelligence Technol, Hefei 230026, Peoples R China.
   [Yao, Hantao] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100864, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Institute of Automation, CAS
RP Xie, HT; Zhang, YD (corresponding author), Univ Sci & Technol China, Natl Engn Lab Brain Inspired Intelligence Technol, Hefei 230026, Peoples R China.
EM mbobo@mail.ustc.edu.cn; hantao.yao@nlpr.ia.ac.cn; htxie@ustc.edu.cn;
   zhazj@ustc.edu.cn; zhyd73@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020
OI Yao, Hantao/0000-0001-8125-2864
FU National Key Research, and Development Program of China
   [2017YFC0820600]; National Nature Science Foundation of China [61525206,
   62022076, U1936210]; National Postdoctoral Programme for Innovative
   Talents [BX20180358]; Youth Innovation Promotion Association Chinese
   Academy of Sciences [2017209]
FX This work was supported in part by the National Key Research, and
   Development Program of China under Grant 2017YFC0820600, in part by the
   National Nature Science Foundation of China under Grants 61525206,
   62022076, and U1936210, in part by the National Postdoctoral Programme
   for Innovative Talents under Grant BX20180358, in part by the Youth
   Innovation Promotion Association Chinese Academy of Sciences (2017209).
   The associate editor coordinating the reviewof this manuscript and
   approving it for publication was Dr. L. Zhang.
CR Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   [Anonymous], 2018, P EUROPEAN C COMPUTE
   Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483
   Bucher M, 2017, IEEE INT CONF COMP V, P2666, DOI 10.1109/ICCVW.2017.308
   Cacheux Y. L., 2019, P IEEE INT C COMP VI, p10 333
   CatherineWah PeterWelinder PietroPerona, 2011, Tech Rep CNS-TR- 2011-001
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Ding Zhengming., 2019, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P6191
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Guo YC, 2016, AAAI CONF ARTIF INTE, P3494
   Ji Z, 2020, IEEE T IMAGE PROCESS, V29, P6549, DOI 10.1109/TIP.2020.2991527
   Ji Z, 2020, IEEE T NEUR NET LEAR, V31, P321, DOI 10.1109/TNNLS.2019.2904991
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Lazaridou A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P270
   Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758
   Li K, 2019, IEEE I CONF COMP VIS, P3582, DOI 10.1109/ICCV.2019.00368
   Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779
   Liu H, 2020, IEEE T MULTIMEDIA, V22, P1808, DOI 10.1109/TMM.2020.2969793
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long T, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1802, DOI 10.1145/3240508.3240715
   Min SB, 2020, IEEE T IMAGE PROCESS, V29, P4996, DOI 10.1109/TIP.2020.2977457
   Min SB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2070, DOI 10.1145/3343031.3351092
   Mishra A, 2018, IEEE COMPUT SOC CONF, P2269, DOI 10.1109/CVPRW.2018.00294
   Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Paul A, 2019, PROC CVPR IEEE, P7049, DOI 10.1109/CVPR.2019.00722
   Qiao RZ, 2016, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2016.247
   Radovanovic M, 2010, J MACH LEARN RES, V11, P2487
   Rahman S, 2020, IEEE T MULTIMEDIA, V22, P242, DOI 10.1109/TMM.2019.2924511
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shaobo Min, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12661, DOI 10.1109/CVPR42600.2020.01268
   Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9
   Socher R., 2013, ADV NEURAL INFORM PR, V26, P935, DOI DOI 10.1007/978-3-319-46478-7
   Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113
   Tomasev N, 2014, IEEE T KNOWL DATA EN, V26, P739, DOI 10.1109/TKDE.2013.25
   Tong B, 2019, PROC CVPR IEEE, P11459, DOI 10.1109/CVPR.2019.01173
   Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450
   Wan ZY, 2019, ADV NEUR IN, V32
   Wang C., 2019, INT C MACHINE LEARNI, P6515
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei K, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P551
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Yang ML, 2019, PATTERN RECOGN, V88, P236, DOI 10.1016/j.patcog.2018.11.015
   Yang Muli, 2020, CVPR
   Yang X, 2019, PROC CVPR IEEE, P4061, DOI 10.1109/CVPR.2019.00419
   Yang Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1286, DOI 10.1145/2964284.2964319
   Yao HT, 2018, IEEE T IMAGE PROCESS, V27, P10, DOI 10.1109/TIP.2017.2751960
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   Ye M, 2019, PROC CVPR IEEE, P11720, DOI 10.1109/CVPR.2019.01200
   Yu YL, 2019, IEEE T CYBERNETICS, V49, P3755, DOI 10.1109/TCYB.2018.2850750
   Zhang HG, 2018, PROC CVPR IEEE, P7670, DOI 10.1109/CVPR.2018.00800
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zheng F, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3697
   Zhu PK, 2019, PROC CVPR IEEE, P2990, DOI 10.1109/CVPR.2019.00311
   Zhu Y., 2019, P 33 INT C NEURAL IN, p14 917
NR 71
TC 8
Z9 8
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3919
EP 3930
DI 10.1109/TMM.2020.3033124
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100038
DA 2024-07-18
ER

PT J
AU Wang, Z
   Wang, LZ
   Wan, J
   Huang, H
AF Wang, Zhan
   Wang, Lizhi
   Wan, Jun
   Huang, Hua
TI Shared Low-Rank Correlation Embedding for Multiple Feature Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Kernel; Task analysis; Fuses; Noise measurement; Laplace
   equations; Dictionaries; Common subspace; low-rank representation;
   multiple feature fusion; canonical correlation analysis
ID CANONICAL CORRELATION-ANALYSIS; ALGORITHM; KERNEL; RECOGNITION
AB The diversity of multimedia data in the real world usually forms heterogeneous types of feature sets. How to explore the structure information and the relationships among multiple features is still an open problem. In this paper, we propose an unsupervised subspace learning method, named the shared low-rank correlation embedding (SLRCE) for multiple feature fusion. First, in the learned subspace, we implement the low-rank representation on each feature set and enforce a shared low-rank constraint to uncover the common structure information of multiple features. Second, we develop an enhanced correlation analysis in the learned subspace for simultaneously removing the redundancy of each feature set and exploring the correlation of multiple features. Finally, we incorporate the shared low-rank representation and the correlation analysis into a unified framework. The shared low-rank constraint not only depicts the data distribution consistency among multiple features, but also assists robust subspace learning. Our method is robust to noise in practice and can be extended to the kernel case to handle the nonlinear feature fusion. Experimental results on several typical datasets demonstrate the superior performance of the proposed methods.
C1 [Wang, Zhan; Wang, Lizhi; Huang, Hua] Beijing Inst Technol, Beijing Lab Intelligent Informat Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
   [Wan, Jun] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Wan, Jun] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
C3 Beijing Institute of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Huang, H (corresponding author), Beijing Inst Technol, Beijing Lab Intelligent Informat Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
EM 7520180008@bit.edu.cn; lzwang@bit.edu.cn; jun.wan@nlpr.ia.ac.cn;
   huahuang@bit.edu.cn
RI wu, jd/IST-2336-2023; wang, zhan/HGD-6796-2022; Huang, Hua/M-9684-2013;
   wu, jun/ISB-8607-2023
OI Huang, Hua/0000-0003-2587-1702; Wang, Lizhi/0000-0002-1953-3339; Wang,
   Zhan/0009-0007-1426-062X; wan, jun/0000-0002-4735-2885
FU National Key Research and Development Program of China [2017YFB1002203]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2017YFB1002203. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Guo-Jun Qi.
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], 2008, COMP VIS PATT REC 20
   [Anonymous], 2013, Advances in Neural Information Processing Systems
   [Anonymous], 2011, Advances in Neural Information Processing Systems
   [Anonymous], 2013, Proc. 30th Int. Conf. Mach. Learning
   BARTELS RH, 1972, COMMUN ACM, V15, P820, DOI 10.1145/361573.361582
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cai J, 2018, NEURAL NETWORKS, V98, P178, DOI 10.1016/j.neunet.2017.11.013
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen J, 2017, KNOWL-BASED SYST, V127, P46, DOI 10.1016/j.knosys.2017.02.031
   Chen JK, 2018, IEEE T AFFECT COMPUT, V9, P38, DOI 10.1109/TAFFC.2016.2593719
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Ding ZM, 2014, IEEE DATA MINING, P110, DOI 10.1109/ICDM.2014.29
   Ding ZM, 2015, IEEE T IMAGE PROCESS, V24, P4322, DOI 10.1109/TIP.2015.2462023
   Gao SH, 2013, IEEE T PATTERN ANAL, V35, P92, DOI 10.1109/TPAMI.2012.63
   Gu Q., 2011, Proceedings of the Twenty-Second international joint conference on Artificial Intelligence, V2, P1294
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hasan Mohammed A., 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1128, DOI 10.1109/IJCNN.2009.5178958
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Jin L, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P959, DOI 10.1145/2733373.2806374
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai PL, 2000, IEEE IJCNN, P614
   Lee CS, 2016, PATTERN RECOGN, V50, P74, DOI 10.1016/j.patcog.2015.08.024
   Li CG, 2017, IEEE T IMAGE PROCESS, V26, P2988, DOI 10.1109/TIP.2017.2691557
   Li YM, 2019, IEEE T KNOWL DATA EN, V31, P1863, DOI 10.1109/TKDE.2018.2872063
   Li ZH, 2018, IEEE T NEUR NET LEAR, V29, P6323, DOI 10.1109/TNNLS.2018.2829867
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Lopez-Paz D, 2014, PR MACH LEARN RES, V32, P1359
   Lu YW, 2018, IEEE T IMAGE PROCESS, V27, P5248, DOI 10.1109/TIP.2018.2855433
   Mizutani T, 2018, MACH LEARN, V107, P643, DOI 10.1007/s10994-017-5673-1
   Nielsen AA, 2002, IEEE T IMAGE PROCESS, V11, P293, DOI 10.1109/83.988962
   Parkhomenko E, 2009, STAT APPL GENET MOL, V8, DOI 10.2202/1544-6115.1406
   Qi G., 2011, Proc. ACM International Conference on World Wide Web, P297
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Qi GJ, 2012, P IEEE, V100, P2688, DOI 10.1109/JPROC.2012.2201909
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Schuller B, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P2798
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130
   Sun L., 2010, P 16 ACM SIGKDD INT, P313, DOI DOI 10.1145/1835804.1835846
   Sun QS, 2005, PATTERN RECOGN, V38, P2437, DOI 10.1016/j.patcog.2004.12.013
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   van Breukelen M, 1998, KYBERNETIKA, V34, P381
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang YJ, 2012, IEEE T MULTIMEDIA, V14, P597, DOI 10.1109/TMM.2012.2189550
   Wang Z., 2020, PROC IEEE INT C MULT, P1
   Wang Z, 2020, NEUROCOMPUTING, V388, P324, DOI 10.1016/j.neucom.2020.01.017
   Yang JF, 2009, SIAM J IMAGING SCI, V2, P569, DOI 10.1137/080730421
   Yin M, 2016, IEEE T PATTERN ANAL, V38, P504, DOI 10.1109/TPAMI.2015.2462360
   Yin QY, 2017, PATTERN RECOGN, V67, P313, DOI 10.1016/j.patcog.2017.01.035
   Zhang CQ, 2017, IEEE T IMAGE PROCESS, V26, P648, DOI 10.1109/TIP.2016.2627806
   Zhang CJ, 2018, IEEE T MULTIMEDIA, V20, P903, DOI 10.1109/TMM.2017.2759500
   Zhang MJ, 2020, IEEE T NEUR NET LEAR, V31, P2623, DOI 10.1109/TNNLS.2019.2933590
   Zhao J, 2017, INFORM FUSION, V38, P43, DOI 10.1016/j.inffus.2017.02.007
   Zhou Dengyong, 2006, 19 INT C NEURAL INFO, V19, P1601
   Zhu XK, 2017, AAAI CONF ARTIF INTE, P2970
   Zhuang LS, 2015, IEEE T IMAGE PROCESS, V24, P3717, DOI 10.1109/TIP.2015.2441632
NR 64
TC 6
Z9 6
U1 3
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1855
EP 1867
DI 10.1109/TMM.2020.3003747
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XG0UQ
UT WOS:000724477100004
DA 2024-07-18
ER

PT J
AU Yang, H
   Liu, L
   Min, WD
   Yang, XS
   Xiong, X
AF Yang, Hao
   Liu, Li
   Min, Weidong
   Yang, Xiaosong
   Xiong, Xin
TI Driver Yawning Detection Based on Subtle Facial Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Driver fatigue; yawning detection; keyframe selection; subtle facial
   action; 3D deep learning network
ID FATIGUE; DROWSINESS; REPRESENTATION; NETWORK
AB Various investigations have shown that driver fatigue is the main cause of traffic accidents. Research on the use of computer vision techniques to detect signs of fatigue from facial actions, such as yawning, has demonstrated good potential. However, accurate and robust detection of yawning is difficult because of the complicated facial actions and expressions of drivers in the real driving environment. Several facial actions and expressions have the same mouth deformation as yawning. Thus, a novel approach to detecting yawning based on subtle facial action recognition is proposed in this study to alleviate the abovementioned problems. A 3D deep learning network with a low time sampling characteristic is proposed for subtle facial action recognition. This network uses 3D convolutional and bidirectional long short-term memory networks for spatiotemporal feature extraction and adopts SoftMax for classification. A keyframe selection algorithm is designed to select the most representative frame sequence from subtle facial actions. This algorithm rapidly eliminates redundant frames using image histograms with low computation cost and detects outliers by median absolute deviation. A series of experiments are also conducted on YawDD benchmark and self-collected datasets. Compared with several state-of-the-art methods, the proposed method has high yawning detection rates and can effectively distinguish yawning from similar facial actions.
C1 [Yang, Hao; Liu, Li; Xiong, Xin] Nanchang Univ, Sch Informat Engn, Nanchang 330031, Jiangxi, Peoples R China.
   [Min, Weidong] Nanchang Univ, Sch Software, Nanchang 330047, Jiangxi, Peoples R China.
   [Yang, Xiaosong] Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
C3 Nanchang University; Nanchang University; Bournemouth University
RP Min, WD (corresponding author), Nanchang Univ, Sch Software, Nanchang 330047, Jiangxi, Peoples R China.
EM 15079072351@163.com; liuli_033@163.com; minweidong@ncu.edu.cn;
   xyang@bournemouth.ac.uk; 15070017693@163.com
RI ; Min, Weidong/D-4585-2017
OI Xiong, Xin/0000-0003-2998-6494; Min, Weidong/0000-0003-2526-2181
FU National Natural Science Foundation of China [61762061, 61603256];
   Natural Science Foundation of Jiangxi Province, China [20161ACB20004];
   Jiangxi Key Laboratory of Smart City [20192BCD40002]
FX This work was supported in part by the National Natural Science
   Foundation of China underGrants 61762061 and 61603256, in part by the
   Natural Science Foundation of Jiangxi Province, China under Grant
   20161ACB20004, and in part by the Jiangxi Key Laboratory of Smart City
   under Grant 20192BCD40002.
CR Abtahi S., 2014, P 5 ACM MULT SYST C, P24, DOI DOI 10.1145/2557642.2563678
   Abtahi S, 2011, IEEE IMTC P, P1606
   Akrout Belhassen, 2016, P INT IMAGE PROCESSI, P1, DOI DOI 10.1109/IPAS.2016.7880127
   Anitha C, 2016, PROCEDIA COMPUT SCI, V92, P63, DOI 10.1016/j.procs.2016.07.324
   [Anonymous], 2015, INT JOINT C NEUR NET
   Azouji N, 2013, IRAN CONF MACH, P219, DOI 10.1109/IranianMVIP.2013.6779982
   Choi IH, 2016, INT CONF BIG DATA, P143, DOI 10.1109/BIGCOMP.2016.7425813
   Ding WY, 2013, ADV MATER RES-SWITZ, V605-607, P2227, DOI 10.4028/www.scientific.net/AMR.605-607.2227
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du Y, 2011, FUND INFORM, V111, P65, DOI 10.3233/FI-2011-554
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Graves A, 2005, LECT NOTES COMPUT SC, V3697, P799
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Han H, 2016, IETE TECH REV, V33, P518, DOI 10.1080/02564602.2015.1118362
   Haque MA, 2016, IET COMPUT VIS, V10, P323, DOI 10.1049/iet-cvi.2015.0215
   Hefenbrock D, 2010, ANN IEEE SYM FIELD P, P11, DOI 10.1109/FCCM.2010.12
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   HUANG TS, 1979, IEEE T ACOUST SPEECH, V27, P13, DOI 10.1109/TASSP.1979.1163188
   Ibrahim MM, 2015, BIOMED SIGNAL PROCES, V18, P360, DOI 10.1016/j.bspc.2015.02.006
   Ibrahim MM, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING APPLICATIONS (IEEE ICSIPA 2013), P89, DOI 10.1109/ICSIPA.2013.6707983
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kholerdi HA, 2016, CONNECT SCI, V28, P27, DOI 10.1080/09540091.2015.1130019
   Li L., 2009, PROCEEDING 17 IEEEAC, P1, DOI DOI 10.1109/BMEI.2009.5305169
   Liu L, 2013, PATTERN RECOGN, V46, P1810, DOI 10.1016/j.patcog.2012.10.004
   Lux M., 2009, CEUR WORKSHOP PROC, V441, P19
   Mandal B, 2017, IEEE T INTELL TRANSP, V18, P545, DOI 10.1109/TITS.2016.2582900
   Martinez CM, 2018, IEEE T INTELL TRANSP, V19, P666, DOI 10.1109/TITS.2017.2706978
   McDonald AD, 2014, HUM FACTORS, V56, P986, DOI 10.1177/0018720813515272
   Min WD, 2019, IET COMPUT VIS, V13, P165, DOI 10.1049/iet-cvi.2018.5586
   [闵卫东 Min Weidong], 2017, [光学精密工程, Optics and Precision Engineering], V25, P779
   Omidyeganeh M., 2011, IEEE International Confer- ence on Virtual Environments, Human-Computer Interfaces and Measurement Systems, P1, DOI DOI 10.1109/VECIMS.2011.6053857
   Pan R, 2015, P IEEE TEN CON 2015, P1
   Rau P.S., 2005, DROWSY DRIVER DETECT, P5
   Simonyan K, 2014, ADV NEUR IN, V27
   Su-Gang M. A, 2018, COMPUT SCI
   Sun W, 2017, IEEE T INTELL TRANSP, V18, P3408, DOI 10.1109/TITS.2017.2690914
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang P, 2017, IEEE T CIRC SYST VID, V27, P2613, DOI 10.1109/TCSVT.2016.2576761
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Zhang LF, 2018, IEEE T CYBERNETICS, V48, P16, DOI 10.1109/TCYB.2016.2605044
   Zhang LF, 2015, PATTERN RECOGN, V48, P3102, DOI 10.1016/j.patcog.2014.12.016
   Zhang Y, 2015, OPTIK, V126, P4501, DOI 10.1016/j.ijleo.2015.08.185
   Zhao L, 2018, IET INTELL TRANSP SY, V12, P127, DOI 10.1049/iet-its.2017.0183
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhu Y., 2018, LNCS, P363, DOI DOI 10.1007/978-3-030-03596-926
NR 45
TC 58
Z9 67
U1 13
U2 113
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 572
EP 583
DI 10.1109/TMM.2020.2985536
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200005
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Yang, XC
   Feng, S
   Wang, DL
   Zhang, YF
AF Yang, Xiaocui
   Feng, Shi
   Wang, Daling
   Zhang, Yifei
TI Image-Text Multimodal Emotion Classification via Multi-View Attentional
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sentiment analysis; Feature extraction; Task analysis; Analytical
   models; Visualization; Semantics; Social networking (online); Memory
   network; multi-view attention mechanism; social media; multimodal
   emotion analysis
ID SENTIMENT ANALYSIS
AB Compared with single-modal content, multimodal data can express users' feelings and sentiments more vividly and interestingly. Therefore, multimodal sentiment analysis has become a popular research topic. However, most existing methods either learn modal sentiment feature independently, without considering their correlations, or they simply integrate multimodal features. In addition, most publicly available multimodal datasets are labeled by sentiment polarities, while the emotions expressed by users are specific. Based on this observation, in this paper, we build a large-scale image-text emotion dataset (i.e., labeled by different emotions), called TumEmo, with more than 190,000 instances from Tumblr.(1) We further propose a novel multimodal emotion analysis model based on the Multi-view Attentional Network (MVAN), which utilizes a memory network that is continually updated to obtain the deep semantic features of image-text. The model includes three stages: feature mapping, interactive learning, and feature fusion. In the feature mapping stage, we leverage image features from an object viewpoint and a scene viewpoint to capture effective information for multimodal emotion analysis. Then, an interactive learning mechanism is adopted that uses the memory network; this mechanism extracts single-modal emotion features and interactively models the cross-view dependencies between the image and text. In the feature fusion stage, multiple features are deeply fused using a multilayer perceptron and a stacking-pooling module. The experimental results on the MVSA-Single, MVSA-Multiple, and TumEmo datasets show that the proposed MVAN outperforms strong baseline models by large margins.
C1 [Yang, Xiaocui; Feng, Shi; Wang, Daling; Zhang, Yifei] Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110169, Peoples R China.
C3 Northeastern University - China
RP Feng, S (corresponding author), Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110169, Peoples R China.
EM 2378211I48@qq.com; fengshi@cse.neu.edu.cn; wangdaling@cse.neu.edu.cn;
   zhangyifei@cse.neu.edu.cn
RI Zhang, Yifei/GRO-3001-2022
OI Yang, Xiaocui/0000-0002-9488-6716
FU National Key R&D Program of China [2018YFB1004700]; National Natural
   Science Foundation of China [61872074, 61772122]
FX The work was supported in part by the National Key R&D Program of China
   under Grant 2018YFB1004700 and in part by the National Natural Science
   Foundation of China under Grant 61872074 and 61772122.
CR Abdi A, 2019, INFORM PROCESS MANAG, V56, P1245, DOI 10.1016/j.ipm.2019.02.018
   [Anonymous], 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   [Anonymous], 2013, ACL
   [Anonymous], 2015, P 2015 C EMP METH NA, DOI [10.18653/v1/d15-1303, 10.18653/v1/D15-1303]
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Chen XM, 2019, DATA SCI ENG, V4, P109, DOI 10.1007/s41019-019-0094-8
   Duke B, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P39, DOI 10.1109/CRV.2018.00016
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Hamouda A., 2011, WORLD C COMP SCI INF, V2, P2090
   Hu A, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P350, DOI 10.1145/3219819.3219853
   Huang F, 2019, IEEE T CYBERNETICS, P1
   Huang FR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1874, DOI 10.1145/3240508.3240614
   Huang FR, 2019, KNOWL-BASED SYST, V167, P26, DOI 10.1016/j.knosys.2019.01.019
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Kaur R., 2022, Research Anthology on Implementing Sentiment Analysis Across Multiple Disciplines, V10, P1846, DOI DOI 10.4018/IJSSMET.2019040103
   Kim J.-H., 2016, arXiv
   Lai SW, 2015, AAAI CONF ARTIF INTE, P2267
   Liu Y, 2018, PATTERN RECOGN, V84, P51, DOI 10.1016/j.patcog.2018.07.001
   Majumder N, 2018, KNOWL-BASED SYST, V161, P124, DOI 10.1016/j.knosys.2018.07.041
   Mao L., 2019, Multiview Machine Learning
   Niu T., 2016, MULTIMEDIA MODELING, P15, DOI [DOI 10.1007/978-3-319-27674-82, 10.1007/978-3-319-27674-8_2]
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79, DOI 10.3115/1118693.1118704
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Poria S, 2017, IEEE DATA MINING, P1033, DOI 10.1109/ICDM.2017.134
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   Siersdorfer S., 2010, ACM MM, P715
   Simonyan K, 2015, IEEE INT C ICLR
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Song KK, 2018, NEUROCOMPUTING, V312, P218, DOI 10.1016/j.neucom.2018.05.104
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Taboada M, 2011, COMPUT LINGUIST, V37, P267, DOI 10.1162/COLI_a_00049
   Tang DY, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1014
   Wang X., 2011, Proceedings of the 20th ACM International Conference on Information and Knowledge Management, CIKM'11, P1031
   Wu FZ, 2016, DECIS SUPPORT SYST, V87, P39, DOI 10.1016/j.dss.2016.04.007
   Xu C., 2014, Visual sentiment prediction with deep convolutional neural networks
   Xu C., 2013, arXiv
   Xu J, 2019, KNOWL-BASED SYST, V178, P61, DOI 10.1016/j.knosys.2019.04.018
   Xu N, 2018, ACM/SIGIR PROCEEDINGS 2018, P929, DOI 10.1145/3209978.3210093
   Xu N, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2399, DOI 10.1145/3132847.3133142
   Xu N, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS (ISI), P152, DOI 10.1109/ISI.2017.8004895
   You QZ, 2017, AAAI CONF ARTIF INTE, P231
   You QZ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1008, DOI 10.1145/2964284.2964288
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   Yue L, 2019, KNOWL INF SYST, V60, P617, DOI 10.1007/s10115-018-1236-4
   Zadeh Amir, 2017, P 2017 C EMP METH NA, P1103
   Zhang L, 2018, WIRES DATA MIN KNOWL, V8, DOI 10.1002/widm.1253
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou P, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2016), VOL 2, P207, DOI 10.18653/v1/p16-2034
NR 50
TC 72
Z9 73
U1 20
U2 102
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4014
EP 4026
DI 10.1109/TMM.2020.3035277
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900008
DA 2024-07-18
ER

PT J
AU Yao, XX
   Zhao, SC
   Lai, YK
   She, DY
   Liang, J
   Yang, JF
AF Yao, Xingxu
   Zhao, Sicheng
   Lai, Yu-Kun
   She, Dongyu
   Liang, Jie
   Yang, Jufeng
TI APSE: Attention-Aware Polarity-Sensitive Embedding for Emotion-Based
   Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Psychology; Image retrieval; Feature
   extraction; Image color analysis; Emotion recognition; Convolutional
   neural networks; emotion-based image retrieval; visual sentiment
   analysis
ID RECOGNITION
AB With the popularity of social media, an increasing number of people are accustomed to expressing their feelings and emotions online using images and videos. An emotion-based image retrieval (EBIR) system is useful for obtaining visual contents with desired emotions from a massive repository. Existing EBIR methods mainly focus on modeling the global characteristics of visual content without considering the crucial role of informative regions of interest in conveying emotions. Further, they ignore the hierarchical relationships between coarse polarities and fine categories of emotions. In this paper, we design an attention-aware polarity-sensitive embedding (APSE) network to address these issues. First, we develop a hierarchical attention mechanism to automatically discover and model the informative regions of interest. Specifically, both polarity- and emotion-specific attended representations are aggregated for discriminative feature embedding. Second, we propose a generated emotion-pair (GEP) loss to simultaneously consider the inter- and intra-polarity relationships of the emotion labels. Moreover, we adaptively generate negative examples of different hard levels in the feature space guided by the attention module to further improve the performance of feature embedding. Extensive experiments on four popular benchmark datasets demonstrate that the proposed APSE method outperforms the state-of-the-art EBIR approaches by a large margin.
C1 [Yao, Xingxu; Yang, Jufeng] Nankai Univ, Coll Comp Sci, Tianjin 300071, Peoples R China.
   [Zhao, Sicheng] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94704 USA.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, Wales.
   [She, Dongyu] Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
   [Liang, Jie] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong 999077, Peoples R China.
C3 Nankai University; University of California System; University of
   California Berkeley; Cardiff University; Tsinghua University; Hong Kong
   Polytechnic University
RP Yang, JF (corresponding author), Nankai Univ, Coll Comp Sci, Tianjin 300071, Peoples R China.
EM yxx_hbgd@163.com; schzhao@gmail.com; laiy4@cardiff.ac.uk;
   sherry6656@163.com; liang27jie@163.com; yangjufeng@nankai.edu.cn
RI Lai, Yu-Kun/D-2343-2010
OI Liang, Jie/0000-0003-2822-5466; Lai, Yukun/0000-0002-2094-5680
FU Major Project for New Generation of AI [2018AAA0100403]; NSFC [61876094
   andU1933114]; Natural Science Foundation of Tianjin, China
   [20JCJQJC00020, 18JCYBJC15400, 18ZXZNGX00110]; Fundamental Research
   Funds for the Central Universities
FX This work was supported in part by the Major Project for New Generation
   of AI under Grant 2018AAA0100403, NSFC(No. 61876094 andU1933114), in
   part by the Natural Science Foundation of Tianjin, China (No.
   20JCJQJC00020, 18JCYBJC15400, and 18ZXZNGX00110), and in part by the
   Fundamental Research Funds for the Central Universities. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Raouf Hamzaoui.
CR Al-Fuqaha, 2020, ARXIV200203773
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Cakir F, 2019, PROC CVPR IEEE, P1861, DOI 10.1109/CVPR.2019.00196
   Calvo MG, 2004, MOTIV EMOTION, V28, P221, DOI 10.1023/B:MOEM.0000040153.26156.ed
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Chen T., 2014, DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks
   Chen T, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P367, DOI 10.1145/2647868.2654935
   Chen TL, 2018, LECT NOTES COMPUT SC, V11214, P527, DOI 10.1007/978-3-030-01249-6_32
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Compton Rebecca J, 2003, Behav Cogn Neurosci Rev, V2, P115, DOI 10.1177/1534582303002002003
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Duan YQ, 2018, PROC CVPR IEEE, P2780, DOI 10.1109/CVPR.2018.00294
   Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Ge WF, 2018, LECT NOTES COMPUT SC, V11210, P272, DOI 10.1007/978-3-030-01231-1_17
   Guntuku S. C., 2019, P INT AAAI C WEB SOC, P236, DOI DOI 10.1609/ICWSM.V13I01.3225
   Guo X, 2020, IEEE WINT CONF APPL, P2910, DOI [10.1109/WACV45572.2020.9093547, 10.1109/wacv45572.2020.9093547]
   Hanbury, 2010, P 18 ACM INT C MULT, P25
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Zhang, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P166, DOI 10.1007/978-3-642-42051-1_22
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Jou B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P159, DOI 10.1145/2733373.2806246
   Jou B, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P213, DOI 10.1145/2647868.2656408
   Kim HR, 2018, IEEE T MULTIMEDIA, V20, P2980, DOI 10.1109/TMM.2018.2827782
   Kim S, 2019, PROC CVPR IEEE, P2283, DOI 10.1109/CVPR.2019.00239
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lang P.J., 1997, INT AFFECTIVE PICTUR, P39, DOI DOI 10.1027/0269-8803/A000147
   Li L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3359753
   Liao LZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1571, DOI 10.1145/3240508.3240646
   Lin C, 2020, AAAI CONF ARTIF INTE, V34, P2661
   Lin Honghuang., 2014, Proceedings of the 51st Design Automation Conference, P1, DOI [10.1109/ICME.2014.6890213, DOI 10.1109/ICME.2014.6890213]
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu TL, 2020, IEEE T MULTIMEDIA, V22, P1098, DOI 10.1109/TMM.2019.2936805
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Lu Xin, 2012, Proc ACM Int Conf Multimed, V2012, P229, DOI 10.1145/2393347.2393384
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Olkiewicz K. A., 2010, Proceedings 2010 International Multiconference on Computer Science and Information Technology (IMCSIT 2010), P89
   Pan S, 2014, TOURISM MANAGE, V40, P59, DOI 10.1016/j.tourman.2013.05.007
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   Rao Tianrong, 2016, NEURAL PROCESSING LE, P1
   Robertson, 2018, ARXIV181101459
   Sartori A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P311, DOI 10.1145/2733373.2806250
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   She DY, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3326335
   Shin H., 2009, P DES AUT C, P1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohn K, 2016, ADV NEUR IN, V29
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Song KK, 2018, NEUROCOMPUTING, V312, P218, DOI 10.1016/j.neucom.2018.05.104
   Sun M., 2017, P ASS ADV ART INT
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tu GY, 2020, IEEE T MULTIMEDIA, V22, P148, DOI 10.1109/TMM.2019.2922129
   VALDEZ P, 1994, J EXP PSYCHOL GEN, V123, P394, DOI 10.1037/0096-3445.123.4.394
   Vaswani A, 2017, ADV NEUR IN, V30
   Vuilleumier P, 2005, TRENDS COGN SCI, V9, P585, DOI 10.1016/j.tics.2005.10.011
   Wang D, 2018, AAAI CONF ARTIF INTE, P7388
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Jingwen., 2016, IJCAI, P3484
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang WN, 2008, IEEE IMAGE PROC, P117, DOI 10.1109/ICIP.2008.4711705
   Wei-Ning W, 2006, IEEE SYS MAN CYBERN, P3534, DOI 10.1109/ICSMC.2006.384667
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xing BX, 2015, NEUROCOMPUTING, V148, P619, DOI 10.1016/j.neucom.2014.08.007
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yang JF, 2018, AAAI CONF ARTIF INTE, P491
   Yang JF, 2018, PROC CVPR IEEE, P7584, DOI 10.1109/CVPR.2018.00791
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yang P, 2010, PROC CVPR IEEE, P2638, DOI 10.1109/CVPR.2010.5539978
   Yang ZY, 2019, IEEE INT CON MULTI, P1090, DOI 10.1109/ICME.2019.00191
   Yao XX, 2019, IEEE I CONF COMP VIS, P1140, DOI 10.1109/ICCV.2019.00123
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   YOU Q, 2016, ASS ADVANCEMENT ARTI
   You QZ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1008, DOI 10.1145/2964284.2964288
   You Quanzeng, 2018, ARXIV180110121
   Zhang HM, 2018, IEEE T MULTIMEDIA, V20, P2824, DOI 10.1109/TMM.2018.2808760
   Zhang SX, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON MEDICAL IMAGING PHYSICS AND ENGINEERING (ICMIPE), P1, DOI 10.1109/ICMIPE.2013.6864491
   Zhao SC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5534
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1025, DOI 10.1145/2647868.2655035
   Zheng WZ, 2019, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.2019.00016
   Zhu XG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3595
NR 93
TC 5
Z9 5
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4469
EP 4482
DI 10.1109/TMM.2020.3042664
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800006
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhang, JS
   Du, J
   Yang, YX
   Song, YZ
   Dai, LR
AF Zhang, Jianshu
   Du, Jun
   Yang, Yongxin
   Song, Yi-Zhe
   Dai, Lirong
TI SRD: A Tree Structure Based Decoder for Online Handwritten Mathematical
   Expression Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Decoding; Mathematical model; Handwriting recognition; Vegetation;
   Grammar; Image recognition; Task analysis; Handwritten mathematical
   expression recogni- tion; tree structure; decoder; attention
AB Recently, recognition of online handwritten mathe- matical expression has been greatly improved by employing encoder-decoder based methods. Existing encoder-decoder models use string decoders to generate LaTeX strings for mathematical expression recognition. However, in this paper, we importantly argue that string representations might not be the most natural for mathematical expressions - mathematical expressions are inherently tree structures other than flat strings. For this purpose, we propose a novel sequential relation decoder (SRD) that aims to decode expressions into tree structures for online handwritten mathematical expression recognition. At each step of tree construction, a sub-tree structure composed of a relation node and two symbol nodes is computed based on previous sub-tree structures. This is the first work that builds a tree structure based decoder for encoder-decoder based mathematical expression recognition. Compared with string decoders, a decoder that better understands tree structures is crucial for mathematical expression recognition as it brings a more reasonable learning objective and improves overall generalization ability. We demonstrate how the proposed SRD outperforms state-of-the-art string decoders through a set of experiments on CROHME database, which is currently the largest benchmark for online handwritten mathematical expression recognition.
C1 [Zhang, Jianshu; Du, Jun; Dai, Lirong] Univ Sci & Technol China, Natl Engn Lab Speech & Language Informat Proc, Hefei 230052, Peoples R China.
   [Yang, Yongxin; Song, Yi-Zhe] Univ Surrey, Guildford GU2 7XH, Surrey, England.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Surrey
RP Du, J (corresponding author), Univ Sci & Technol China, Natl Engn Lab Speech & Language Informat Proc, Hefei 230052, Peoples R China.
EM xysszjs@mail.ustc.edu.cn; jundu@ustc.edu.cn; yongxin.yang@surrey.ac.uk;
   y.song@surrey.ac.uk; lrdai@ustc.edu.cn
RI DAI, Jinjia/KCL-5110-2024; yang, yongxin/HNB-4716-2023
OI Song, Yi-Zhe/0000-0001-5908-3275
FU National Key R&D Program of China [2017YFB1002202]; National Natural
   Science Foundation of China [61671422, U1613211]; Fundamental Research
   Funds for the Central Universities; MOE-Microsoft Key Laboratory of USTC
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFB1002202, in part by the National Natural Science
   Foundation of China under Grants 61671422 and U1613211, in part by the
   Fundamental Research Funds for the Central Universities, in part by the
   MOE-Microsoft Key Laboratory of USTC, and in part by the iFLYTEK-Surrey
   Joint Research Centre on Artificial Intelligence.
CR Alvaro F, 2016, PATTERN RECOGN, V51, P135, DOI 10.1016/j.patcog.2015.09.013
   Alvaro F, 2014, PATTERN RECOGN LETT, V35, P58, DOI 10.1016/j.patrec.2012.09.023
   Awal AM, 2014, PATTERN RECOGN LETT, V35, P68, DOI 10.1016/j.patrec.2012.10.024
   Bandanau D, 2016, INT CONF ACOUST SPEE, P4945, DOI 10.1109/ICASSP.2016.7472618
   BELAID A, 1984, IEEE T PATTERN ANAL, V6, P105, DOI 10.1109/TPAMI.1984.4767483
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y., 2014, TECHNICAL REPORT
   Berger J., 2010, Proceedings of the Python for Scientific Computing Conference (SciPy), number Scipy, P1
   Chan KF, 2001, PATTERN RECOGN, V34, P1671, DOI 10.1016/S0031-3203(00)00102-3
   Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621
   Chen X., 2018, ADV NEURAL INFORM PR, P2547
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Chung Junyoung, 2014, ARXIV14123555
   Deng YT, 2017, 34 INT C MACHINE LEA, V70
   Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hirata NST, 2015, PATTERN RECOGN, V48, P837, DOI 10.1016/j.patcog.2014.09.015
   Hu L, 2016, INT CONF FRONT HAND, P337, DOI [10.1109/ICFHR.2016.0070, 10.1109/ICFHR.2016.65]
   Kam-Fai Chan, 2000, International Journal on Document Analysis and Recognition, V3, P3, DOI 10.1007/PL00013549
   Klakow D, 2002, SPEECH COMMUN, V38, P19, DOI 10.1016/S0167-6393(01)00041-3
   Kosmala A, 1998, INT C PATT RECOG, P1306, DOI 10.1109/ICPR.1998.711941
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   MacLean S, 2013, INT J DOC ANAL RECOG, V16, P139, DOI 10.1007/s10032-012-0184-x
   Mahdavi Mahshad, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1533, DOI 10.1109/ICDAR.2019.00247
   Mouchère H, 2016, INT CONF FRONT HAND, P607, DOI [10.1109/ICFHR.2016.0116, 10.1109/ICFHR.2016.108]
   Mouchère H, 2014, INT CONF FRONT HAND, P791, DOI 10.1109/ICFHR.2014.138
   Mouchère H, 2016, INT J DOC ANAL RECOG, V19, P173, DOI 10.1007/s10032-016-0263-5
   Rhee TH, 2009, PATTERN RECOGN, V42, P3192, DOI 10.1016/j.patcog.2008.10.036
   Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556
   Tu Z., 2016, M ASS COMP LING
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu J.-W., 2018, JOINT EUR C MACH LEA, P18
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Zanibbi R, 2002, IEEE T PATTERN ANAL, V24, P1455, DOI 10.1109/TPAMI.2002.1046157
   Zanibbi R., 2013, P DOCUMENT RECOGNIT
   Zeiler Matthew D, 2012, ARXIV12125701
   Zhang JS, 2017, PROC INT CONF DOC, P902, DOI 10.1109/ICDAR.2017.152
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang JS, 2018, INT C PATT RECOG, P2245, DOI 10.1109/ICPR.2018.8546031
   Zhang JS, 2017, PATTERN RECOGN, V71, P196, DOI 10.1016/j.patcog.2017.06.017
   Zhang T, 2017, PROC INT CONF DOC, P914, DOI 10.1109/ICDAR.2017.154
   Zhang XY, 2018, IEEE T PATTERN ANAL, V40, P849, DOI 10.1109/TPAMI.2017.2695539
   Zhao N, 2017, IEEE T MULTIMEDIA, V19, P2080, DOI 10.1109/TMM.2017.2722687
NR 45
TC 10
Z9 11
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2471
EP 2480
DI 10.1109/TMM.2020.3011316
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800024
DA 2024-07-18
ER

PT J
AU Yu, HK
   Zheng, K
   Fang, JW
   Guo, H
   Wang, S
AF Yu, Hongkai
   Zheng, Kang
   Fang, Jianwu
   Guo, Hao
   Wang, Song
TI A New Method and Benchmark for Detecting Co-Saliency Within a Single
   Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency detection; Proposals; Benchmark testing; Object recognition;
   Streaming media; Image segmentation; Feature extraction; Within-image
   co-saliency; convex optimization; low-rank based fusion
ID OBJECT DETECTION; SEGMENTATION; CONSTRAINT; GRAPH
AB Recently, saliency detection in a single image and co-saliency detection in multiple images have drawn extensive research interest in the vision and multimedia communities. In this paper, we investigate a new problem of co-saliency detection within a single image, i.e., detecting within-image co-saliency. By identifying common saliency within an image, e.g., highlighting multiple occurrences of an object class with similar appearance, this work can benefit many important applications, such as the detection of objects of interest, more robust object recognition, reduction of information redundancy, and animation synthesis. We propose a new bottom-up method to address this problem. Specifically, a large number of object proposals are first detected from the image. Then we develop an optimization algorithm to derive a set of proposal groups, each of which contains multiple proposals showing good common saliency in the image. For each proposal group, we calculate a co-saliency map and then use a low-rank based algorithm to fuse the maps calculated from all the proposal groups for the final co-saliency map in the image. In the experiment, we collect a new benchmark dataset of 664 color images (two subsets) for within-image co-saliency detection. Experiment results show that the proposed method can better detect the within-image co-saliency than existing algorithms. The experimental results also show that the proposed method can be applied to detect the repetitive patterns in a single image and detect the co-saliency in multiple images.
C1 [Yu, Hongkai] Cleveland State Univ, Dept Elect Engn & Comp Sci, Cleveland, OH 44115 USA.
   [Zheng, Kang; Guo, Hao; Wang, Song] Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.
   [Fang, Jianwu] Changan Univ, Sch Elect & Control Engn, Xian 710064, Peoples R China.
   [Wang, Song] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
C3 University System of Ohio; Cleveland State University; University of
   South Carolina System; University of South Carolina Columbia; Chang'an
   University; Tianjin University
RP Wang, S (corresponding author), Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.
EM h.yu19@csuohio.edu; zheng37@email.sc.edu; j.w.fangit@gmail.com;
   hguo@email.sc.edu; songwang@cec.sc.edu
RI 房, 建武/IWD-9461-2023
OI Wang, Song/0000-0003-4152-5295
FU National Natural Science Foundation of China [NSFC-U1803264,
   NSFC-61671325, 61672376, 61603057]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants NSFC-U1803264, NSFC-61671325, 61672376,
   and 61603057. This article was presented in part at the 32nd Conference
   on Artificial Intelligence, New Orleans, LA, USA, February 2018 [1]. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. E. Ricci.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Cong RM, 2019, IEEE T CYBERNETICS, V49, P233, DOI 10.1109/TCYB.2017.2771488
   Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819
   Dai JF, 2013, IEEE I CONF COMP VIS, P1305, DOI 10.1109/ICCV.2013.165
   Faktor A, 2013, IEEE I CONF COMP VIS, P1297, DOI 10.1109/ICCV.2013.164
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616
   Ge CJ, 2016, SIGNAL PROCESS-IMAGE, V44, P69, DOI 10.1016/j.image.2016.03.005
   Grant M., 2014, CVX MATLAB SOFTWARE
   He XM, 2014, PROC CVPR IEEE, P296, DOI 10.1109/CVPR.2014.45
   Hou YT, 2007, IEEE INFOCOM SER, P1, DOI 10.1109/INFCOM.2007.9
   Huang R, 2017, IEEE SIGNAL PROC LET, V24, P569, DOI 10.1109/LSP.2017.2681687
   Jerripothula KR, 2018, IEEE T MULTIMEDIA, V20, P2466, DOI 10.1109/TMM.2018.2798294
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Joulin A, 2014, LECT NOTES COMPUT SC, V8694, P253, DOI 10.1007/978-3-319-10599-4_17
   Kanan C, 2009, VIS COGN, V17, P979, DOI 10.1080/13506280902771138
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3_36
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li HL, 2014, IEEE T IMAGE PROCESS, V23, P3545, DOI 10.1109/TIP.2014.2330759
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin X, 2019, IEEE T MULTIMEDIA, V21, P1646, DOI 10.1109/TMM.2018.2884474
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mahadevan V, 2009, PROC CVPR IEEE, P1007, DOI 10.1109/CVPRW.2009.5206573
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253
   Rui Huang, 2015, 2015 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT). Proceedings, P1, DOI 10.1109/ISGT.2015.7131826
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Tan ZY, 2013, INT CONF ACOUST SPEE, P2114, DOI 10.1109/ICASSP.2013.6638027
   Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190
   Wang C, 2017, IEEE T IMAGE PROCESS, V26, P5825, DOI 10.1109/TIP.2017.2750410
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang Q, 2013, IEEE T CYBERNETICS, V43, P660, DOI 10.1109/TSMCB.2012.2214210
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Xu M, 2018, IEEE T MULTIMEDIA, V20, P1335, DOI 10.1109/TMM.2017.2767784
   Xu XM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409070
   Yu HK, 2018, AAAI CONF ARTIF INTE, P7509
   Yu HK, 2017, IEEE IMAGE PROC, P3335, DOI 10.1109/ICIP.2017.8296900
   Yu HK, 2014, IEEE IMAGE PROC, P4412, DOI 10.1109/ICIP.2014.7025895
   Zhang DW, 2016, IEEE T NEUR NET LEAR, V27, P1163, DOI 10.1109/TNNLS.2015.2495161
   Zhang DW, 2015, IEEE I CONF COMP VIS, P594, DOI 10.1109/ICCV.2015.75
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 60
TC 6
Z9 7
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3051
EP 3063
DI 10.1109/TMM.2020.2972165
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700003
DA 2024-07-18
ER

PT J
AU Luo, H
   Jiang, W
   Fan, X
   Zhang, C
AF Luo, Hao
   Jiang, Wei
   Fan, Xing
   Zhang, Chi
TI STNReID: Deep Convolutional Networks With Pairwise Spatial Transformer
   Networks for Partial Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Task analysis; Training; Deep learning;
   Computational modeling; Fans; Image reconstruction; Partial person ReID;
   STN; occlusion; deep learning
ID NEURAL-NETWORK
AB Partial person re-identification (ReID) is a challenging task because only partial information of person images is available for matching target persons. Few studies, especially on deep learning, have focused on matching partial person images with holistic person images. This study presents a novel deep partial ReID framework based on pairwise spatial transformer networks (STNReID), which can be trained on existing holistic person datasets. STNReID includes a spatial transformer network (STN) module and a ReID module. The STN module samples an affined image (a semantically corresponding patch) from the holistic image to match the partial image. The ReID module extracts the features of the holistic, partial, and affined images. Competition (or confrontation) is observed between the STN module and the ReID module, and two-stage training is applied to acquire a strong STNReID for partial ReID. Experimental results show that our STNReID obtains 66.7% and 54.6% rank-1 accuracies on Partial-ReID and Partial-iLIDS datasets, respectively. These values are at par with those obtained with state-of-the-art methods.
C1 [Luo, Hao; Jiang, Wei; Fan, Xing] Zhejiang Univ, Coll Control Sci & Enginneering, State Key Lab Ind Control Technol, Hangzhou 310027, Peoples R China.
   [Zhang, Chi] Megvii Inc, Beijing Res Inst, Beijing 100089, Peoples R China.
C3 Zhejiang University
RP Jiang, W (corresponding author), Zhejiang Univ, Coll Control Sci & Enginneering, State Key Lab Ind Control Technol, Hangzhou 310027, Peoples R China.
EM haoluocsc@zju.edu.cn; jiangwei_zju@zju.edu.cn; xfanplus@zju.edu.cn;
   zhangchi@megvii.com
RI Luo, Hao/AAG-2570-2020; jiang, wei/J-6317-2018; Fan, Xing/AAY-6418-2020
OI Luo, Hao/0000-0002-6405-4011; jiang, wei/0000-0002-9240-5851; Fan,
   Xing/0000-0003-2622-2210
FU National Natural Science Foundation of China [61633019]; Science
   Foundation of Chinese Aerospace Industry [JCKY2018204B053]; Autonomous
   Research Project of the State Key Laboratory of Industrial Control
   Technology, China [ICT1917]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61633019, in part by the Science
   Foundation of Chinese Aerospace Industry under Grant JCKY2018204B053 and
   in part by the Autonomous Research Project of the State Key Laboratory
   of Industrial Control Technology, China under Grant ICT1917.
CR Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Donahue J, 2014, PR MACH LEARN RES, V32
   Du J, 2018, ICCSP 2018: PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON CRYPTOGRAPHY, SECURITY AND PRIVACY, P1, DOI 10.1145/3199478.3199511
   Fan X, 2019, J VIS COMMUN IMAGE R, V60, P51, DOI 10.1016/j.jvcir.2019.01.010
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   He L., 2018, ARXIV181007399
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   Hermans Alexander, 2017, ARXIV170307737
   Iodice Sara, 2018, ASIAN C COMPUT VIS, P101
   Jaderberg Max, 2015, ADV NEURAL INFORM PR, P8
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Luo H., 2019, P IEEE CVF C COMP VI
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo H, 2019, PATTERN RECOGN, V94, P53, DOI 10.1016/j.patcog.2019.05.028
   Ristani E, 2018, PROC CVPR IEEE, P6036, DOI 10.1109/CVPR.2018.00632
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xiao Q., 2017, ARXIV171000478
   Zhang Z, 2018, IEEE ACCESS, V6, P36887, DOI 10.1109/ACCESS.2018.2852712
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2017, P IEEE C COMP VIS PA, P3346, DOI DOI 10.1109/CVPR.2017.357
   Zheng Liang, 2016, arXiv preprint arXiv
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
NR 44
TC 74
Z9 78
U1 4
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2905
EP 2913
DI 10.1109/TMM.2020.2965491
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yuan, X
   Haimi-Cohen, R
AF Yuan, Xin
   Haimi-Cohen, Raziel
TI Image Compression Based on Compressive Sensing: End-to-End Comparison
   With JPEG
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Transform coding; Quantization (signal); Sensors; Image
   reconstruction; Reconstruction algorithms; Bit rate; Compressive
   sensing; image compression; quantization; entropy coding; sparse coding;
   reconstruction; JPEG; JPEG2000
ID SPARSE REPRESENTATION; VIDEO; ALGORITHM
AB We present an end-to-end image compression system based on compressive sensing. The presented system integrates the conventional scheme of compressive sampling (on the entire image) and reconstruction with quantization and entropy coding. The compression performance, in terms of decoded image quality versus data rate, is shown to be comparable with JPEG and significantly better at the low rate range. We study the parameters that influence the system performance, including (i) the choice of sensing matrix, (ii) the trade-off between quantization and compression ratio, and (iii) the reconstruction algorithms. We propose an effective method to select, among all possible combinations of quantization step and compression ratio, the ones that yield the near-best quality at any given bit rate. Furthermore, our proposed image compression system can be directly used in the compressive sensing camera, e.g., the single pixel camera, to construct a hardware compressive sampling system.
C1 [Yuan, Xin] Bell Labs, 600 Mt Ave, Murray Hill, NJ 07974 USA.
   [Haimi-Cohen, Raziel] Verizon Labs, Bedminster, NJ 07921 USA.
C3 AT&T
RP Yuan, X (corresponding author), Bell Labs, 600 Mt Ave, Murray Hill, NJ 07974 USA.
EM xyuan@bell-labs.com; Raziel.Haimi-Cohen@verizon.com
RI Haimi-Cohen, Raziel/U-7141-2019; YUAN, XIN/AGH-6764-2022; Yuan,
   Xin/KCZ-2239-2024
OI Haimi-Cohen, Raziel/0000-0001-8417-8807; YUAN, XIN/0000-0002-8311-7524; 
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Ahn JH, 2016, IEEE T IMAGE PROCESS, V25, P4796, DOI 10.1109/TIP.2016.2598651
   [Anonymous], 1992, INFORM TECHNOLOGY DI
   [Anonymous], 2002, INFORM TECHNOLOGY JP
   [Anonymous], 2012, VECTOR QUANTIZATION
   [Anonymous], 2017, CS VS JPEG
   Baig Yousuf, 2010, 2010 17th International Conference on Telecommunications (ICT 2010), P935, DOI 10.1109/ICTEL.2010.5478657
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Candès EJ, 2011, IEEE T INFORM THEORY, V57, P7235, DOI 10.1109/TIT.2011.2161794
   Cao X, 2016, IEEE SIGNAL PROC MAG, V33, P95, DOI 10.1109/MSP.2016.2582378
   Chen Z, 2018, IEEE T MULTIMEDIA, V20, P1610, DOI 10.1109/TMM.2017.2774004
   Cui WX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1777, DOI 10.1145/3240508.3240706
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai W, 2009, IEEE INT SYMP INFO, P11, DOI 10.1109/ISIT.2009.5206032
   Dai W, 2009, ITW: 2009 IEEE INFORMATION THEORY WORKSHOP ON NETWORKING AND INFORMATION THEORY, P171, DOI 10.1109/ITWNIT.2009.5158565
   Deng CW, 2012, IEEE T MULTIMEDIA, V14, P278, DOI 10.1109/TMM.2011.2181491
   Do TT, 2008, INT CONF ACOUST SPEE, P3369, DOI 10.1109/ICASSP.2008.4518373
   Do TT, 2012, IEEE T SIGNAL PROCES, V60, P139, DOI 10.1109/TSP.2011.2170977
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Duarte MF, 2012, IEEE T IMAGE PROCESS, V21, P494, DOI 10.1109/TIP.2011.2165289
   Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1
   FINO BJ, 1976, IEEE T COMPUT, V25, P1142, DOI 10.1109/TC.1976.1674569
   Ginesu G, 2012, SIGNAL PROCESS-IMAGE, V27, P867, DOI 10.1016/j.image.2012.01.011
   Goyal VK, 2008, IEEE SIGNAL PROC MAG, V25, P48, DOI 10.1109/MSP.2007.915001
   Haimi-Cohen R, 2016, SIGNAL PROCESS, V120, P71, DOI 10.1016/j.sigpro.2015.07.023
   Hannuksela MM, 2015, IEEE SIGNAL PROC MAG, V32, P150, DOI 10.1109/MSP.2015.2419292
   Huang G, 2013, IEEE IMAGE PROC, P2101, DOI 10.1109/ICIP.2013.6738433
   Jalali S, 2019, IEEE T INFORM THEORY, V65, P8005, DOI 10.1109/TIT.2019.2940666
   Kipnis A, 2018, IEEE T INFORM THEORY, V64, P6013, DOI 10.1109/TIT.2018.2857822
   Kipnis A, 2017, IEEE INT SYMP INFO, P2148, DOI 10.1109/ISIT.2017.8006909
   Kipnis A, 2016, IEEE T INFORM THEORY, V62, P401, DOI 10.1109/TIT.2015.2485271
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Lainema J, 2016, IEEE IMAGE PROC, P71, DOI 10.1109/ICIP.2016.7532321
   Laska JN, 2012, IEEE T SIGNAL PROCES, V60, P3496, DOI 10.1109/TSP.2012.2194710
   Laska JN, 2011, IEEE T SIGNAL PROCES, V59, P5289, DOI 10.1109/TSP.2011.2162324
   Laska JN, 2011, APPL COMPUT HARMON A, V31, P429, DOI 10.1016/j.acha.2011.02.002
   Liu HX, 2014, IEEE T MULTIMEDIA, V16, P1549, DOI 10.1109/TMM.2014.2328324
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P2990, DOI 10.1109/TPAMI.2018.2873587
   Liu Y, 2016, IEEE T MULTIMEDIA, V18, P351, DOI 10.1109/TMM.2016.2514848
   Llull P, 2013, OPT EXPRESS, V21, P10526, DOI 10.1364/OE.21.010526
   Lucas A, 2018, IEEE SIGNAL PROC MAG, V35, P20, DOI 10.1109/MSP.2017.2760358
   Ma JW, 2019, IEEE I CONF COMP VIS, P10222, DOI 10.1109/ICCV.2019.01032
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Miao X, 2019, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2019.00416
   PRATT WK, 1969, P IEEE, V57, P58, DOI 10.1109/PROC.1969.6869
   RISSANEN J, 1979, IBM J RES DEV, V23, P149, DOI 10.1147/rd.232.0149
   Romberg J, 2008, IEEE SIGNAL PROC MAG, V25, P14, DOI 10.1109/MSP.2007.914729
   Shi WZ, 2017, IEEE INT CON MULTI, P877, DOI 10.1109/ICME.2017.8019428
   Song XD, 2017, IEEE T MULTIMEDIA, V19, P1351, DOI 10.1109/TMM.2017.2654123
   Venkatraman D, 2009, INT CONF ACOUST SPEE, P3513, DOI 10.1109/ICASSP.2009.4960383
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu M, 2014, IEEE T CIRC SYST VID, V24, P1743, DOI 10.1109/TCSVT.2014.2317886
   Yang JB, 2015, IEEE T IMAGE PROCESS, V24, P106, DOI 10.1109/TIP.2014.2365720
   Yang JB, 2014, IEEE T IMAGE PROCESS, V23, P4863, DOI 10.1109/TIP.2014.2344294
   Yuan X., 2015, ARXIV150803498
   Yuan X, 2018, SIGNAL PROCESS, V152, P273, DOI 10.1016/j.sigpro.2018.06.002
   Yuan X, 2018, OPT EXPRESS, V26, P1962, DOI 10.1364/OE.26.001962
   Yuan X, 2016, IEEE IMAGE PROC, P2539, DOI 10.1109/ICIP.2016.7532817
   Yuan X, 2016, IEEE SENS J, V16, P8091, DOI 10.1109/JSEN.2016.2609201
   Yuan X, 2015, IEEE J-STSP, V9, P964, DOI 10.1109/JSTSP.2015.2411575
   Yuan X, 2014, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2014.424
   Yuan Xin, 2015, ARXIV150806901
   Yuan XL, 2016, IEEE T MULTIMEDIA, V18, P2002, DOI 10.1109/TMM.2016.2602758
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P3254, DOI 10.1109/TIP.2019.2958309
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang LY, 2016, IEEE T MULTIMEDIA, V18, P1720, DOI 10.1109/TMM.2016.2581593
   Zhang X, 2018, IEEE T IMAGE PROCESS, V27, P3753, DOI 10.1109/TIP.2018.2823546
   Zhang X, 2017, IEEE DATA COMPR CONF, P380, DOI 10.1109/DCC.2017.63
   Zhou WX, 2018, INT POW ELEC APPLICA, P2318
   Zymnis A, 2010, IEEE SIGNAL PROC LET, V17, P149, DOI 10.1109/LSP.2009.2035667
NR 77
TC 21
Z9 22
U1 6
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2889
EP 2904
DI 10.1109/TMM.2020.2967646
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Avola, D
   Cascio, M
   Cinque, L
   Foresti, GL
   Massaroni, C
   Rodolà, E
AF Avola, Danilo
   Cascio, Marco
   Cinque, Luigi
   Foresti, Gian Luca
   Massaroni, Cristiano
   Rodola, Emanuele
TI 2-D Skeleton-Based Action Recognition via Two-Branch Stacked LSTM-RNNs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; 2D skeleton; recurrent neural networks (RNNs); long
   short-term memory (LSTM)
ID RECOGNIZING HUMAN ACTIONS; NEURAL-NETWORKS; CONTEXTUAL KNOWLEDGE;
   INFORMATION; LANGUAGE; DEPTH
AB Action recognition in video sequences is an interesting field for many computer vision applications, including behavior analysis, event recognition, and video surveillance. In this article, a method based on 2D skeleton and two-branch stacked Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) cells is proposed. Unlike 3D skeletons, usually generated by RGB-D cameras, the 2D skeletons adopted in this article are reconstructed starting from RGB video streams, therefore allowing the use of the proposed approach in both indoor and outdoor environments. Moreover, any case of missing skeletal data is managed by exploiting 3D-Convolutional Neural Networks (3D-CNNs). Comparative experiments with several key works on KTH and Weizmann datasets show that the method described in this paper outperforms the current state-of-the-art. Additional experiments on UCF Sports and IXMAS datasets demonstrate the effectiveness of our method in the presence of noisy data and perspective changes, respectively. Further investigations on UCF Sports, HMDB51, UCF101, and Kinetics400 highlight how the combination between the proposed two-branch stacked LSTM and the 3D-CNN-based network can manage missing skeleton information, greatly improving the overall accuracy. Moreover, additional tests on KTH and UCF Sports datasets also show the robustness of our approach in the presence of partial body occlusions. Finally, comparisons on UT-Kinect and NTU-RGB+D datasets show that the accuracy of the proposed method is fully comparable to that of works based on 3D skeletons.
C1 [Avola, Danilo; Cascio, Marco; Cinque, Luigi; Massaroni, Cristiano; Rodola, Emanuele] Sapienza Univ, Dept Comp Sci, I-00198 Rome, Italy.
   [Foresti, Gian Luca] Univ Udine, Dept Math Comp Sci & Phys, I-33100 Udine, Italy.
C3 Sapienza University Rome; University of Udine
RP Massaroni, C (corresponding author), Sapienza Univ, Dept Comp Sci, I-00198 Rome, Italy.
EM avola@di.uniroma1.it; cascio@di.uniroma1.it; cinque@di.uniroma1.it;
   gianluca.foresti@uniud.it; massaroni@di.uniroma1.it;
   rodola@di.uniroma1.it
RI Cascio, Marco/AAW-2240-2021
OI Cascio, Marco/0000-0003-4370-8140; Avola, Danilo/0000-0001-9437-6217;
   Massaroni, Cristiano/0000-0002-6942-4851
FU MIUR under grant "Departments of Excellence 2018-2022" of the Department
   of Computer Science of Sapienza University
FX Thisworkwas supported in part by the MIUR under grant "Departments of
   Excellence 2018-2022" of the Department of Computer Science of Sapienza
   University. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Xiaochun Cao.
CR Anirudh R, 2015, PROC CVPR IEEE, P3147, DOI 10.1109/CVPR.2015.7298934
   [Anonymous], 2017, ABS170506950 ARXIV
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2016, Adv. NeuralInf. Process. Syst.
   [Anonymous], 2014, ADV COMPUT VIS PATTE, DOI 10.1007/978-3-319-09396-3_9
   [Anonymous], 2008, P IEEE INT C COMP VI
   [Anonymous], 2016, COMPUT VISUAL MEDIA
   [Anonymous], 2013, P ICML
   Avola D, 2019, MULTIMED TOOLS APPL, V78, P5919, DOI 10.1007/s11042-018-6875-7
   Avola D, 2019, J BIOMED INFORM, V89, P81, DOI 10.1016/j.jbi.2018.11.012
   Avola D, 2019, IEEE T MULTIMEDIA, V21, P234, DOI 10.1109/TMM.2018.2856094
   Avola D, 2018, MULTIMED TOOLS APPL, V77, P24955, DOI 10.1007/s11042-018-5730-1
   Avola D, 2017, PATTERN RECOGN LETT, V100, P110, DOI 10.1016/j.patrec.2017.10.029
   Avola D, 2017, PATTERN RECOGN LETT, V96, P96, DOI 10.1016/j.patrec.2016.10.015
   Bregonzio M, 2012, PATTERN RECOGN, V45, P1220, DOI 10.1016/j.patcog.2011.08.014
   Brendel W, 2011, IEEE I CONF COMP VIS, P778, DOI 10.1109/ICCV.2011.6126316
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cippitelli E, 2016, INT C INTELL COMP CO, P19, DOI 10.1109/ICCP.2016.7737116
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   del Rincón JM, 2013, PATTERN RECOGN LETT, V34, P1849, DOI 10.1016/j.patrec.2012.10.020
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Evangelidis G, 2014, INT C PATT RECOG, P4513, DOI 10.1109/ICPR.2014.772
   Gao J, 2010, LECT NOTES ENG COMP, P88
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hand DJ, 2001, INT STAT REV, V69, P385, DOI 10.1111/j.1751-5823.2001.tb00465.x
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hermans M., 2013, Advances in neural information processing systems, V26
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang ZW, 2017, PROC CVPR IEEE, P1243, DOI 10.1109/CVPR.2017.137
   Pham HH, 2018, COMPUT VIS IMAGE UND, V170, P51, DOI 10.1016/j.cviu.2018.03.003
   Ibrahim MS, 2016, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2016.217
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia J, 2019, IEEE T MULTIMEDIA, V21, P1853, DOI 10.1109/TMM.2018.2887016
   Jing CJ, 2017, I C MECH MACH VIS PR, P149
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lan T, 2011, IEEE I CONF COMP VIS, P2003, DOI 10.1109/ICCV.2011.6126472
   Lavee G, 2013, IEEE T CIRC SYST VID, V23, P337, DOI 10.1109/TCSVT.2012.2203742
   Laxton BM, 2007, PROC CVPR IEEE, P799
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Li CK, 2017, IEEE SIGNAL PROC LET, V24, P624, DOI 10.1109/LSP.2017.2678539
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li Kang, 2014, IEEE Trans Pattern Anal Mach Intell, V36, P1644, DOI 10.1109/TPAMI.2013.2297321
   Li Y, 2016, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2016.215
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Lo Presti L, 2015, LECT NOTES COMPUT SC, V9005, P529, DOI 10.1007/978-3-319-16811-1_35
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   Mogren O., 2016, CORR
   Tu NA, 2019, IEEE T CIRC SYST VID, V29, P800, DOI 10.1109/TCSVT.2018.2816960
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Onofri L, 2016, EXPERT SYST APPL, V63, P97, DOI 10.1016/j.eswa.2016.06.011
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Parisi GI, 2017, NEURAL NETWORKS, V96, P137, DOI 10.1016/j.neunet.2017.09.001
   Pei MT, 2011, IEEE I CONF COMP VIS, P487, DOI 10.1109/ICCV.2011.6126279
   Poppe R, 2007, COMPUT VIS IMAGE UND, V108, P4, DOI 10.1016/j.cviu.2006.10.016
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Qi J, 2018, IEEE T HUM-MACH SYST, V48, P637, DOI 10.1109/THMS.2018.2850301
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Rahmani H, 2018, IEEE T PATTERN ANAL, V40, P667, DOI 10.1109/TPAMI.2017.2691768
   Ribeiro M, 2018, PATTERN RECOGN LETT, V105, P13, DOI 10.1016/j.patrec.2017.07.016
   Roy D, 2018, PATTERN RECOGN LETT, V108, P56, DOI 10.1016/j.patrec.2018.03.004
   Sak H, 2014, INTERSPEECH, P338
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro K., 2012, ABS12120402 ARXIV
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su ZX, 2009, SMI 2009: IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P1, DOI 10.1109/SMI.2009.5170156
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sundermeyer M, 2015, IEEE-ACM T AUDIO SPE, V23, P517, DOI 10.1109/TASLP.2015.2400218
   Tran SD, 2008, LECT NOTES COMPUT SC, V5303, P610, DOI 10.1007/978-3-540-88688-4_45
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang DG, 2018, LECT NOTES COMPUT SC, V11213, P457, DOI 10.1007/978-3-030-01240-3_28
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang XY, 2012, INT C PATT RECOG, P3378
   Weinland D, 2007, IEEE I CONF COMP VIS, P170
   Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362
   Wu YQ, 2018, IEEE T SERV COMPUT, V11, P341, DOI 10.1109/TSC.2015.2501981
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yan C., IEEE T MULTIMEDIA
   Yan CG, 2019, IEEE T MULTIMEDIA, V21, P2675, DOI 10.1109/TMM.2019.2903448
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Yao HM, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17040805
   Zang JL, 2018, IFIP ADV INF COMM TE, V519, P97, DOI 10.1007/978-3-319-92007-8_9
   Zhang BW, 2018, IEEE T IMAGE PROCESS, V27, P2326, DOI 10.1109/TIP.2018.2791180
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
   Zhao MM, 2018, PROC CVPR IEEE, P7356, DOI 10.1109/CVPR.2018.00768
   Zhou Q, 2019, IEEE T VIS COMPUT GR, V25, P2040, DOI 10.1109/TVCG.2019.2898742
NR 103
TC 44
Z9 47
U1 4
U2 68
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2481
EP 2496
DI 10.1109/TMM.2019.2960588
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000001
DA 2024-07-18
ER

PT J
AU Jiang, K
   Wang, ZY
   Yi, P
   Wang, GC
   Gu, K
   Jiang, JJ
AF Jiang, Kui
   Wang, Zhongyuan
   Yi, Peng
   Wang, Guangcheng
   Gu, Ke
   Jiang, Junjun
TI ATMFN: Adaptive-Threshold-Based Multi-Model Fusion Network for
   Compressed Face Hallucination
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Threshold-based fusion network; attention mechanism; ensemble learning;
   compressed face hallucination
ID IMAGE SUPERRESOLUTION
AB Although tremendous strides have been recently made in face hallucination, exiting methods based on a single deep learning framework can hardly satisfactorily provide fine facial features from tiny faces under complex degradation. This article advocates an adaptive-threshold-based multi-model fusion network (ATMFN) for compressed face hallucination, which unifies different deep learning models to take advantages of their respective learning merits. First of all, we construct CNN-, GAN- and RNN-based underlying super-resolvers to produce candidate SR results. Further, the attention subnetwork is proposed to learn the individual fusion weight matrices capturing the most informative components of the candidate SR faces. Particularly, the hyper-parameters of the fusion matrices and the underlying networks are optimized together in an end-to-end manner to drive them for collaborative learning. Finally, a threshold-based fusion and reconstruction module is employed to exploit the candidates' complementarity and thus generate high-quality face images. Extensive experiments on benchmark face datasets and real-world samples show that our model outperforms the state-of-the-art SR methods in terms of quantitative indicators and visual effects. The code and configurations are released at https://github.com/kuihua/ATMFN.
C1 [Jiang, Kui; Wang, Zhongyuan; Yi, Peng; Wang, Guangcheng] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Engn Res Ctr Intelligent Percept & Autonomous Con, Minist Educ,Beijing Artificial Intelligence Inst, Beijing 100124, Peoples R China.
   [Jiang, Junjun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Jiang, Junjun] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Wuhan University; Beijing University of Technology; Harbin Institute of
   Technology; Peng Cheng Laboratory
RP Wang, ZY (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM kaijiang_1994@163.com; wzy_hope@163.com; 2017202110008@whu.edu.cn;
   wangguangcheng0428@163.com; guke.doctor@gmail.com;
   jiangjunjun@hit.edu.cn
RI Jiang, Junjun/L-7087-2019; Wang, Zhongyuan/ABD-2189-2020; Gu,
   Ke/AAJ-9684-2021; Jiang, Kui/Z-2573-2019
OI Jiang, Junjun/0000-0002-5694-505X; Jiang, Kui/0000-0002-4055-7503; Yi,
   Peng/0000-0001-9366-951X; Wang, Zhongyuan/0000-0002-9796-488X; Wang,
   Guangcheng/0000-0001-8277-797X
FU National Key RD Project [2016YFE0202300]; National Natural Science
   Foundation of China [U1903214, 61671332, U1736206, 41771452, 41771454,
   61971165]; Hubei Province Technological Innovation Major Project
   [2019AAA049, 2018CFA024]
FX This work was supported by National Key R&D Project (2016YFE0202300), in
   part by National Natural Science Foundation of China under Grants
   U1903214, 61671332, U1736206, 41771452, 41771454, and 61971165, and in
   part by Hubei Province Technological Innovation Major Project
   (2019AAA049, 2018CFA024).
CR [Anonymous], 2015, NATURE, DOI DOI 10.1038/NATURE14539
   [Anonymous], 2010, Fddb: A benchmark for face detection in unconstrained settings
   [Anonymous], 2017, ADV INTELL SYST, DOI DOI 10.1007/978-3-319-52941-7_1
   [Anonymous], 2016, ARXIV160307235
   Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616
   Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338
   Chen C, 2019, LECT NOTES COMPUT SC, V11766, P184, DOI 10.1007/978-3-030-32248-9_21
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Cheng Z., 2018, ARXIV180409691
   Dahl R, 2017, IEEE I CONF COMP VIS, P5449, DOI 10.1109/ICCV.2017.581
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gers FA, 2001, IEEE T NEURAL NETWOR, V12, P1333, DOI 10.1109/72.963769
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hao ZK, 2017, PROC CVPR IEEE, P1913, DOI 10.1109/CVPR.2017.207
   Hsu CC, 2019, IEEE T IMAGE PROCESS, V28, P6225, DOI 10.1109/TIP.2019.2924554
   Hsu WN, 2016, IEEE W SP LANG TECH, P467, DOI 10.1109/SLT.2016.7846305
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang JJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P771
   Jiang JJ, 2020, IEEE T CYBERNETICS, V50, P4694, DOI 10.1109/TCYB.2018.2890149
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Jiang K, 2019, IEEE T GEOSCI REMOTE, V57, P5799, DOI 10.1109/TGRS.2019.2902431
   Jiang K, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111700
   Jung CK, 2011, IEEE SIGNAL PROC LET, V18, P367, DOI 10.1109/LSP.2011.2140370
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960
   Liao RJ, 2015, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2015.68
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma X, 2010, PATTERN RECOGN, V43, P2224, DOI 10.1016/j.patcog.2009.12.019
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Shi JG, 2019, IEEE T MULTIMEDIA, V21, P2223, DOI 10.1109/TMM.2019.2898752
   Shi JG, 2018, IEEE T IMAGE PROCESS, V27, P2980, DOI 10.1109/TIP.2018.2813163
   Shi JG, 2015, IEEE SIGNAL PROC LET, V22, P1189, DOI 10.1109/LSP.2015.2390972
   Shi JG, 2014, PATTERN RECOGN, V47, P3520, DOI 10.1016/j.patcog.2014.04.023
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sollich P, 1996, ADV NEUR IN, V8, P190
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang GC, 2020, IEEE T IMAGE PROCESS, V29, P1802, DOI 10.1109/TIP.2019.2945675
   Wang LF, 2017, PATTERN RECOGN, V68, P191, DOI 10.1016/j.patcog.2017.02.027
   Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Wang ZY, 2019, IEEE T IMAGE PROCESS, V28, P2530, DOI 10.1109/TIP.2018.2887017
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yi P, 2020, IEEE T CIRC SYST VID, V30, P2503, DOI 10.1109/TCSVT.2019.2925844
   Yu X, 2020, IEEE T PATTERN ANAL, V42, P2926, DOI 10.1109/TPAMI.2019.2916881
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang W, 2018, IEEE CONF COMPUT
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou EJ, 2015, AAAI CONF ARTIF INTE, P3871
   Zhou LG, 2019, IEEE T NEUR NET LEAR, V30, P3275, DOI 10.1109/TNNLS.2018.2890550
   Zhou ZH, 2002, ARTIF INTELL, V137, P239, DOI 10.1016/S0004-3702(02)00190-X
   Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37
NR 70
TC 83
Z9 85
U1 2
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2734
EP 2747
DI 10.1109/TMM.2019.2960586
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000017
DA 2024-07-18
ER

PT J
AU Liang, YQ
   Li, X
AF Liang, Yongqing
   Li, Xin
TI Reassembling Shredded Document Stripes Using Word-Path Metric and Greedy
   Composition Optimal Matching Solver
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Optical character recognition software; Optimal matching;
   Image reconstruction; Traveling salesman problems; Reliability;
   Semantics; Shredded document reassembly; sequence compatibility
   measurement; global reconstruction from local alignments
ID JIGSAW PUZZLES; RECONSTRUCTION; ALGORITHM
AB This paper develops a shredded document reassembly algorithm based on character/word detection. A new word compatibility estimation metric and a searching strategy called Greedy Composition and Optimal Matching (GCOM) are proposed to compose documents from their vertically shredded stripes. We reduce the stripe puzzle reassembly problem to the traveling salesman problem (TSP) on a sparse graph. The word-path compatibility metric takes advantages of the optical character recognition (OCR) to compute the compatibility score among a group of stripes. The global composition strategy, based on an integration of greedy composition and optimal matching, is proposed to search for a maximal Hamiltonian path and the final global reassembly. We demonstrate that our solver outperforms the state-of-the-art puzzle solvers on reassembling stripe shredded documents.
C1 [Liang, Yongqing; Li, Xin] Louisiana State Univ, Sch Elect Engn Comp Sci, Baton Rouge, LA 70808 USA.
C3 Louisiana State University System; Louisiana State University
RP Li, X (corresponding author), Louisiana State Univ, Sch Elect Engn Comp Sci, Baton Rouge, LA 70808 USA.
EM yqliang@cct.lsu.edu; xinli@cct.lsu.edu
RI Li, Xin/AAS-5967-2021; Liang, Yongqing/ABG-7122-2020
OI Liang, Yongqing/0000-0002-7282-0476; Li, Xin/0000-0002-0144-9489
FU National Science Foundation [IIS-1320959]; National Natural Science
   Foundation of China [61728206]
FX This work was supported in part by the National Science Foundation under
   Grant IIS-1320959 and in part by the National Natural Science Foundation
   of China under Grant 61728206. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Engin Erzin.
CR Andalo Fernanda A., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P63, DOI 10.1109/SIBGRAPI.2012.18
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], 2003, IEEE CVPR WORKSH, DOI DOI 10.1109/CVPRW.2003.10008
   Atallah AS, 2015, INT CONF COMM SYST, P345, DOI 10.1109/CSNT.2015.69
   Butler P, 2012, IEEE CONF VIS ANAL, P113, DOI 10.1109/VAST.2012.6400560
   Chen JH, 2018, MULTIMED TOOLS APPL, V77, P19281, DOI 10.1007/s11042-017-5389-z
   Cho TS, 2010, PROC CVPR IEEE, P183, DOI 10.1109/CVPR.2010.5540212
   Choi S, 2015, PROC CVPR IEEE, P5556, DOI 10.1109/CVPR.2015.7299195
   Christofides N., 1976, 388 C MELL U MAN SCI
   Deever A, 2012, IEEE IMAGE PROC, P233, DOI 10.1109/ICIP.2012.6466838
   FREEMAN H, 1964, IEEE T COMPUT, VEC13, P118, DOI 10.1109/PGEC.1964.263781
   Gallagher AC, 2012, PROC CVPR IEEE, P382, DOI 10.1109/CVPR.2012.6247699
   Ge YF, 2015, GECCO'15: PROCEEDINGS OF THE 2015 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P847, DOI 10.1145/2739480.2754677
   Gong YJ, 2016, APPL SOFT COMPUT, V45, P163, DOI 10.1016/j.asoc.2016.03.024
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   Justino E, 2006, FORENSIC SCI INT, V160, P140, DOI 10.1016/j.forsciint.2005.09.001
   Le CY, 2019, IEEE T IMAGE PROCESS, V28, P4000, DOI 10.1109/TIP.2019.2903298
   Le CY, 2018, COMPUT AIDED DESIGN, V102, P33, DOI 10.1016/j.cad.2018.04.018
   Leitao HCD, 2002, IEEE T PATTERN ANAL, V24, P1239, DOI 10.1109/TPAMI.2002.1033215
   Li HS, 2014, IEEE T MULTIMEDIA, V16, P571, DOI 10.1109/TMM.2013.2291968
   Li XF, 2011, IEEE INT C INT ROBOT, P2879, DOI 10.1109/IROS.2011.6048529
   Li X, 2019, COMPUT AIDED GEOM D, V71, P220, DOI 10.1016/j.cagd.2019.04.015
   MARQUES M., 2009, Proceedings of the 2009 ACM symposium on Applied Computing SAC 09, P893
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Nielsen TR, 2008, PATTERN RECOGN LETT, V29, P1924, DOI 10.1016/j.patrec.2008.05.027
   Paixao TM, 2018, SIBGRAPI, P87, DOI 10.1109/SIBGRAPI.2018.00018
   Phienthrakul T, 2015, 2015 11TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P623, DOI 10.1109/SITIS.2015.13
   Pöhler D, 2015, INT SYMP IMAGE SIG, P143, DOI 10.1109/ISPA.2015.7306048
   Pomeranz D, 2011, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2011.5995331
   Richter F, 2013, IEEE T MULTIMEDIA, V15, P582, DOI 10.1109/TMM.2012.2235415
   Richter W, 2014, INT CONF CLOUD ENG, P7, DOI 10.1109/IC2E.2014.36
   Shang SZ, 2014, IEEE IMAGE PROC, P5537, DOI 10.1109/ICIP.2014.7026120
   Sholomon D, 2013, PROC CVPR IEEE, P1767, DOI 10.1109/CVPR.2013.231
   Smith R, 2007, PROC INT CONF DOC, P629, DOI 10.1109/icdar.2007.4376991
   Son K, 2016, PROC CVPR IEEE, P1193, DOI 10.1109/CVPR.2016.134
   Son K, 2014, LECT NOTES COMPUT SC, V8694, P32, DOI 10.1007/978-3-319-10599-4_3
   Tsamoura E, 2010, IEEE T IMAGE PROCESS, V19, P680, DOI 10.1109/TIP.2009.2035840
   Xing N, 2017, MULTIMED TOOLS APPL, V76, P12871, DOI 10.1007/s11042-016-3685-7
   Zhang K, 2015, IEEE I CONF COMP VIS, P2138, DOI 10.1109/ICCV.2015.247
   Zhang K, 2014, GRAPH MODELS, V76, P484, DOI 10.1016/j.gmod.2014.03.001
   Zhao B, 2014, PROC INT C TOOLS ART, P1016, DOI 10.1109/ICTAI.2014.154
   Zhu LJ, 2008, IEEE T PATTERN ANAL, V30, P1, DOI 10.1109/TPAMI.2007.1163
NR 44
TC 6
Z9 7
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1168
EP 1181
DI 10.1109/TMM.2019.2941777
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200005
OA hybrid
DA 2024-07-18
ER

PT J
AU Deng, C
   Yang, X
   Nie, FP
   Tao, DP
AF Deng, Cheng
   Yang, Xu
   Nie, Feiping
   Tao, Dapeng
TI Saliency Detection via a Multiple Self-Weighted Graph-Based Manifold
   Ranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency detection; Feature extraction; Manifolds; Image color analysis;
   Task analysis; Image reconstruction; Convolutional neural nets; Saliency
   detection; multiple graphs manifold learning; self-adaptive weight
ID OBJECT DETECTION; MODEL
AB As an important task in the process of image understanding and analysis, saliency detection has recently received increasing attention. In this paper, we propose an efficient multiple self-weighted graph-based manifold ranking method to construct salient maps. First, we extract several different views of features from superpixels, and generate original salient regions as foreground and background cues using boundary information via multiple graph-based manifold ranking. Furthermore, a set of hyperparameters is learned to distinguish the importance between different graphs, which can be viewed as an adaptive weighting of each graph, and then a centroid graph is generated by using these self-weighted multiple graphs. An iterative algorithm is proposed to simultaneously optimize the hyperparameters as well as the centroid graph connection. Thus, an ideal centroid graph can be obtained, offering a more clear profile of the separated structure. Finally, the saliency maps can be produced with an approximate binary image from the manifold ranking. Extensive experiments have demonstrated our method consistently achieves superior detection performance than several state-of-the-arts.
C1 [Deng, Cheng; Yang, Xu] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Nie, Feiping] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Nie, Feiping] Northwestern Polytech Univ, Ctr OPT IMagery Anal & Learning, Xian 710072, Peoples R China.
   [Tao, Dapeng] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Peoples R China.
C3 Xidian University; Northwestern Polytechnical University; Northwestern
   Polytechnical University; Yunnan University
RP Tao, DP (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Peoples R China.
EM chdeng.xd@gmail.com; xuyang.xd@gmail.com; feipingnie@gmail.com;
   dapeng.tao@gmail.com
RI Nie, Feiping/B-3039-2012; Yang, Xu/ACW-5509-2022; Tao,
   Dapeng/E-8649-2013
OI Yang, Xu/0000-0002-0405-6816; Deng, Cheng/0000-0003-2620-3247; Nie,
   Feiping/0000-0002-0871-6519; Tao, Dapeng/0000-0003-0783-5273
FU National Natural Science Foundation of China [61572388, 61703327,
   61772455]; Key RAMP;D Program-The Key Industry Innovation Chain of
   Shaanxi [2017ZDCXL-GY-05-04-02, 2017ZDCXL-GY-05-02, 2018ZDXM-GY-176];
   National Key RAMP;D Program of China [2017YFE0104100]; Yunnan Natural
   Science Funds [2018FY001 (-013)]; Program for Excellent Young Talents of
   National Natural Science Foundation of Yunnan University [2018YDJQ004]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61572388, 61703327, and 61772455, in
   part by the Key R&D Program-The Key Industry Innovation Chain of Shaanxi
   under Grants 2017ZDCXL-GY-05-04-02, 2017ZDCXL-GY-05-02, and
   2018ZDXM-GY-176, in part by the National Key R&D Program of China under
   Grant 2017YFE0104100, in part by Yunnan Natural Science Funds under
   Grant 2018FY001 (-013), and in part by the Program for Excellent Young
   Talents of National Natural Science Foundation of Yunnan University
   under Grant 2018YDJQ004.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Daubechies I, 2010, COMMUN PUR APPL MATH, V63, P1, DOI 10.1002/cpa.20303
   Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661
   Deng C, 2018, PATTERN RECOGN, V77, P306, DOI 10.1016/j.patcog.2017.10.007
   Deng C, 2014, IEEE T MULTIMEDIA, V16, P785, DOI 10.1109/TMM.2014.2298841
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Dong YY, 2016, IEEE T MULTIMEDIA, V18, P549, DOI 10.1109/TMM.2016.2522639
   Duchi J., 2008, P 25 INT C MACH LEAR, P272
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Hu XW, 2018, AAAI CONF ARTIF INTE, P6943
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kim J, 2016, IEEE T IMAGE PROCESS, V25, P9, DOI 10.1109/TIP.2015.2495122
   Kim J, 2016, LECT NOTES COMPUT SC, V9908, P455, DOI 10.1007/978-3-319-46493-0_28
   Lang CY, 2012, IEEE T IMAGE PROCESS, V21, P1327, DOI 10.1109/TIP.2011.2169274
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li J, 2018, IEEE T IMAGE PROCESS, V27, P349, DOI 10.1109/TIP.2017.2762594
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu WW, 2019, IEEE T PATTERN ANAL, V41, P408, DOI 10.1109/TPAMI.2018.2794976
   Liu XL, 2017, IEEE T IMAGE PROCESS, V26, P5324, DOI 10.1109/TIP.2017.2729896
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nie F., 2010, ADV NEURAL INFORM PR, P1813
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1969
   Nie FP, 2014, PR MACH LEARN RES, V32, P1062
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Peng X, 2018, IEEE T IMAGE PROCESS, V27, P5076, DOI 10.1109/TIP.2018.2848470
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Quan R, 2018, IEEE T MULTIMEDIA, V20, P1101, DOI 10.1109/TMM.2017.2763780
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   SU J, 2018, ARXIV181210066
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang M, 2011, PROC CVPR IEEE, P417, DOI 10.1109/CVPR.2011.5995743
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang TT, 2016, LECT NOTES COMPUT SC, V9912, P450, DOI 10.1007/978-3-319-46484-8_27
   Wei Y., 2012, Proceedings of the 12th European Conference on Computer Vision - Volume Part III, ECCV'12, P29
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Xu XY, 2019, SIGNAL PROCESS, V165, P186, DOI 10.1016/j.sigpro.2019.06.026
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yang ML, 2019, PATTERN RECOGN, V88, P236, DOI 10.1016/j.patcog.2018.11.015
   Yang X, 2019, PROC CVPR IEEE, P4061, DOI 10.1109/CVPR.2019.00419
   Yang X, 2018, AAAI CONF ARTIF INTE, P4374
   Zhang LH, 2018, IEEE T IMAGE PROCESS, V27, P987, DOI 10.1109/TIP.2017.2766787
   Zhang LH, 2017, IEEE T PATTERN ANAL, V39, P1892, DOI 10.1109/TPAMI.2016.2609426
   Zhang P., 2018, ARXIV180802373
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 66
TC 46
Z9 48
U1 1
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 885
EP 896
DI 10.1109/TMM.2019.2934833
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400005
DA 2024-07-18
ER

PT J
AU Rodríguez, P
   Velazquez, D
   Cucurull, G
   Gonfaus, JM
   Roca, EX
   González, J
AF Rodriguez, Pau
   Velazquez, Diego
   Cucurull, Guillem
   Gonfaus, Josep M.
   Roca, E. Xavier
   Gonzalez, Jordi
TI Pay Attention to the Activations: A Modular Attention Mechanism for
   Fine-Grained Image Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer architecture; Computational modeling; Image recognition; Task
   analysis; Proposals; Logic gates; Clutter; Image Retrieval Deep Learning
   Convolutional Neural Networks Attention-based Learning
ID VISUAL-ATTENTION; MODEL; AGE
AB Fine-grained image recognition is central to many multimedia tasks such as search, retrieval, and captioning. Unfortunately, these tasks are still challenging since the appearance of samples of the same class can be more different than those from different classes. This issue is mainly due to changes in deformation, pose, and the presence of clutter. In the literature, attention has been one of the most successful strategies to handle the aforementioned problems. Attention has been typically implemented in neural networks by selecting the most informative regions of the image that improve classification. In contrast, in this paper, attention is not applied at the image level but to the convolutional feature activations. In essence, with our approach, the neural model learns to attend to lower-level feature activations without requiring part annotations and uses those activations to update and rectify the output likelihood distribution. The proposed mechanism is modular, architecture-independent, and efficient in terms of both parameters and computation required. Experiments demonstrate that well-known networks such as wide residual networks and ResNeXt, when augmented with our approach, systematically improve their classification accuracy and become more robust to changes in deformation and pose and to the presence of clutter. As a result, our proposal reaches state-of-the-art classification accuracies in CIFAR-10, the Adience gender recognition task, Stanford Dogs, and UEC-Food100 while obtaining competitive performance in ImageNet, CIFAR-100, CUB200 Birds, and Stanford Cars. In addition, we analyze the different components of our model, showing that the proposed attention modules succeed in finding the most discriminative regions of the image. Finally, as a proof of concept, we demonstrate that with only local predictions, an augmented neural network can successfully classify an image before reaching any fully connected layer, thus reducing the computational amount up to 10.
C1 [Rodriguez, Pau; Cucurull, Guillem] Element AI, Montreal, PQ H2S 3G9, Canada.
   [Velazquez, Diego; Roca, E. Xavier; Gonzalez, Jordi] Univ Autonoma Barcelona, Comp Vis Ctr, Bellaterra 08193, Spain.
   [Gonfaus, Josep M.] Univ Autonoma Barcelona, Visual Tagging Serv, Parc Recerca, Bellaterra 08193, Spain.
C3 Centre de Visio per Computador (CVC); Autonomous University of
   Barcelona; Autonomous University of Barcelona
RP Rodríguez, P (corresponding author), Element AI, Montreal, PQ H2S 3G9, Canada.
EM pau.rodriguez@elementai.com; diegovd0296@gmail.com;
   gcucurull@cvc.uab.es; pep.gonfaus@visual-tagging.com; xavir@cvc.uab.es;
   jordi.gonzalez@cvc.uab.cat
RI Gonzàlez, Jordi/I-1812-2015; Rodríguez López, Pau/I-2762-2015; Roca
   Marva, F. Xavier/I-2013-2015
OI Gonzàlez, Jordi/0000-0001-8033-0306; Rodríguez López,
   Pau/0000-0002-1689-8084; Roca Marva, F. Xavier/0000-0002-7043-7334;
   Velazquez, Diego/0000-0001-6782-4336
FU Spanish project (MINECO/FEDER) [TIN2015-65464-R]; Generalitat de
   Catalunya [2016FI B 01163]; COST Action [IC1307iVL Net]
FX This work was supported in part by the Spanish project TIN2015-65464-R
   (MINECO/FEDER), in part by the 2016FI B 01163 Grant of Generalitat de
   Catalunya, and in part by the COST Action IC1307iV&L Net. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Xiaochun Cao.
CR [Anonymous], 2015, P INT C LEARN REPR
   [Anonymous], P COMP VIS PATT REC
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], 2017, PYTORCH
   [Anonymous], 2016, ADAPTIVE COMPUTATION
   [Anonymous], 2005, International Conference on Machine Learning ICML, DOI DOI 10.1145/1102351.1102433
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Rodríguez AMB, 2018, ESTUDIOS CRITICOS DE JURISPRUDENCIA TRIBUTARIA Y ADUANERA, TOMO VII, OBSERVATORIO DE JURISPRUDENCIA TRIBUTARIA Y ADUANERA, P349
   Boulos F, 2009, IEEE IMAGE PROC, P3109, DOI 10.1109/ICIP.2009.5414458
   Butko NJ, 2008, INT C DEVEL LEARN, P139, DOI 10.1109/DEVLRN.2008.4640819
   Cavaro-Ménard C, 2010, PROC SPIE, V7627, DOI 10.1117/12.844505
   Chamaret C, 2010, P SOC PHOTO-OPT INS, V7524, DOI 10.1117/12.837532
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312
   Dhondt Y, 2006, IEEE IMAGE PROC, P829, DOI 10.1109/ICIP.2006.312530
   Doulamis N, 1998, IEEE T CIRC SYST VID, V8, P928, DOI 10.1109/76.736718
   Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Hassannejad H, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P41, DOI 10.1145/2986035.2986042
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jetley S., 2018, P INT C LEARN REPR
   Kastner S, 2000, ANNU REV NEUROSCI, V23, P315, DOI 10.1146/annurev.neuro.23.1.315
   Khosla A., 2011, P CVPR WORKSHOP FINE, V2, P1
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Callet P, 2013, P IEEE, V101, P2058, DOI 10.1109/JPROC.2013.2265801
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li ZC, 2011, IMAGE VISION COMPUT, V29, P1, DOI 10.1016/j.imavis.2010.07.001
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu HT, 2011, IEEE T CIRC SYST VID, V21, P971, DOI 10.1109/TCSVT.2011.2133770
   Loffe R, 2015, International Workshop on OpenCL 2015, DOI 10.1145/2791321.2791324
   Matsuda Y., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P25, DOI 10.1109/ICME.2012.157
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Ranzato M., 2014, ARXIV14055488
   Recasens A, 2018, LECT NOTES COMPUT SC, V11213, P52, DOI 10.1007/978-3-030-01240-3_4
   Rodríguez P, 2017, PATTERN RECOGN, V72, P563, DOI 10.1016/j.patcog.2017.06.028
   Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vaswani A., 2017, P ADV NEUR INF PROC, V2017, P5998
   Veit A, 2018, LECT NOTES COMPUT SC, V11205, P3, DOI 10.1007/978-3-030-01246-5_1
   Venjakob A, 2012, PROC SPIE, V8318, DOI 10.1117/12.913611
   Wah C, 2011, CALTECH UCSD BIRDS 2
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu L, 2019, IEEE T CYBERNETICS, V49, P1791, DOI 10.1109/TCYB.2018.2813971
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zilly JG, 2017, PR MACH LEARN RES, V70
NR 75
TC 53
Z9 59
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 502
EP 514
DI 10.1109/TMM.2019.2928494
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, SJ
   Li, L
   Wang, SH
   Zhang, WG
   Huang, QM
   Tian, Q
AF Yang, Shijie
   Li, Liang
   Wang, Shuhui
   Zhang, Weigang
   Huang, Qingming
   Tian, Qi
TI SkeletonNet: A Hybrid Network With a Skeleton-Embedding Process for
   Multi-View Image Representation Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Correlation; Visualization; Skeleton; Matrix decomposition;
   Kernel; Laplace equations; Unsupervised multi-view subspace learning;
   semantic inconsistency; tensor factorization; deep auto-encoders
ID MATRIX
AB Multi-view representation learning plays a fundamental role in multimedia data analysis. Some specific inter-view alignment principles are adopted in conventional models, where there is an assumption that different views share a common latent subspace. However, when dealing views on diverse semantic levels, the view-specific characteristics are neglected, and the divergent inconsistency of similarity measurements hinders sufficient information sharing. This paper proposes a hybrid deep network by introducing tensor factorization into the multi-view deep auto-encoder. The network adopts skeleton-embedding process for unsupervised multi-view subspace learning. It takes full consideration of view-specific characteristics, and leverages the strength of both shallow and deep architectures for modeling low- and high-level views, respectively. We first formulate the high-level-view semantic distribution as the underlying skeleton structure of the learned subspace, and then infer the local tangent structures according to the affinity propagation of low-level-view geometric correlations. As a consequence, more discriminative subspace representation can be learned from global semantic pivots to local geometric details. Experimental comparisons on three benchmark image datasets show the promising performance and flexibility of our model.
C1 [Yang, Shijie; Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Yang, Shijie; Huang, Qingming] Univ Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing 101408, Peoples R China.
   [Li, Liang; Wang, Shuhui; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Zhang, Weigang] Harbin Inst Technol, Sch Comp Sci & Technol, Weihai 264209, Peoples R China.
   [Zhang, Weigang] Chinese Acad Sci, Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
   [Tian, Qi] Huawei Noahs Ark Lab, Comp Vis, Shenzhen 518129, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Chinese Academy of Sciences; Institute of Computing
   Technology, CAS; Harbin Institute of Technology; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS; University of
   Texas System; University of Texas at San Antonio (UTSA); Huawei
   Technologies
RP Li, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM shijie.yang@vipl.ict.ac.cn; liang.li@ict.ac.cn; wangshuhui@ict.ac.cn;
   wgzhang@hit.edu.cn; qmhuang@ucas.ac.cn; qi.tian@utsa.edu
RI Zhang, Weigang/GZA-9095-2022
OI Zhang, Weigang/0000-0003-0042-7074; Li, Liang/0000-0002-1943-8219
FU National Natural Science Foundation of China [61836002, 61771457,
   61620106009, 61732007, 61672497, U1636214, 61572488, 61772494,
   61472389]; National Basic Research Program of China (973 Program)
   [2015CB351800]; Key Research Program of Frontier Sciences [CAS:
   QYZDJ-SSW-SYS013]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 61836002, 61771457, 61620106009, 61732007,
   61672497, U1636214, 61572488, 61772494 and 61472389, in part by National
   Basic Research Program of China (973 Program): 2015CB351800, and in part
   by Key Research Program of Frontier Sciences, CAS: QYZDJ-SSW-SYS013.
CR Andrew G., 2013, ICML, P1247
   [Anonymous], 2015, 3 INT C LEARN REPR I
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247923
   CAO XC, 2015, PROC CVPR IEEE, P586, DOI DOI 10.1109/CVPR.2015.7298657
   Conneau A., 2017, P 2017 C EMPIRCAL ME, P670, DOI [DOI 10.18653/V1/D17-1070, 10.18653/v1/d17-1070]
   Ding ZM, 2016, AAAI CONF ARTIF INTE, P1181
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang HC, 2012, PROC CVPR IEEE, P773, DOI 10.1109/CVPR.2012.6247748
   Kalayeh MM, 2014, PROC CVPR IEEE, P184, DOI 10.1109/CVPR.2014.31
   Krompass D., 2013, P EUR C MACH LEARN
   Kumar D, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1413
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Li L, 2015, J VIS COMMUN IMAGE R, V31, P231, DOI 10.1016/j.jvcir.2015.06.008
   Li L, 2013, IEEE MULTIMEDIA, V20, P13, DOI 10.1109/MMUL.2013.15
   Li L, 2012, IEEE T MULTIMEDIA, V14, P1401, DOI 10.1109/TMM.2012.2194993
   Li L, 2018, PROGNOST SYST HEALT, P1094, DOI 10.1109/PHM-Chongqing.2018.00193
   Li YQ, 2015, AAAI CONF ARTIF INTE, P2750
   Li YN, 2010, IEEE T MULTIMEDIA, V12, P814, DOI 10.1109/TMM.2010.2066960
   Li ZC, 2015, IEEE T PATTERN ANAL, V37, P2085, DOI 10.1109/TPAMI.2015.2400461
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu HF, 2012, IEEE T PATTERN ANAL, V34, P1299, DOI 10.1109/TPAMI.2011.217
   Liu J., 2013, P 2013 SIAM INT C DA, P252, DOI DOI 10.1137/1.9781611972832.28
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Nickel M., 2011, P 28 INT C MACH LEAR, V11, P3104482, DOI 10.5555/3104482.3104584
   Pang JB, 2016, IEEE T MULTIMEDIA, V18, P2482, DOI 10.1109/TMM.2016.2598439
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Salakhutdinov R., 2009, ARTIFICIAL INTELLIGE, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh AK, 2011, IEEE COMP SOC ANN, P339, DOI 10.1109/ISVLSI.2011.44
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Tang W, 2009, IEEE DATA MINING, P1016, DOI 10.1109/ICDM.2009.125
   Tao ZQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2843
   Tian H, 2018, OCEANS 2018 MTS/IEEE CHARLESTON
   Tian Y, 2015, IEEE MULTIMEDIA, V22, P93, DOI 10.1109/MMUL.2015.61
   Tzortzis G, 2012, IEEE DATA MINING, P675, DOI 10.1109/ICDM.2012.43
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vincent Pascal, 2008, P 25 INT C MACHINE L, DOI DOI 10.1145/1390156.1390294
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang AR, 2015, IEEE T IMAGE PROCESS, V24, P4459, DOI 10.1109/TIP.2015.2465133
   Wang H., 2013, INT C MACHINE LEARNI, P352
   Wang HX, 2014, PROC CVPR IEEE, P4106, DOI 10.1109/CVPR.2014.523
   Wang S, 2015, INT CONF MACH LEARN, P883, DOI 10.1109/ICMLC.2015.7340670
   Wang W, 2014, PROC VLDB ENDOW, V7, P649, DOI 10.14778/2732296.2732301
   Xiao Cai, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1977, DOI 10.1109/CVPR.2011.5995740
   Xu JL, 2016, PROC CVPR IEEE, P5356, DOI 10.1109/CVPR.2016.578
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang SJ, 2017, PROC CVPR IEEE, P7053, DOI 10.1109/CVPR.2017.746
   Yang SJ, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P121, DOI 10.1109/BigMM.2017.33
   Yang SY, 2017, IEEE ANN INT CONF CY, P1220, DOI 10.1109/CYBER.2017.8446483
   Yang YM, 2018, IEEE T MULTIMEDIA, V20, P1024, DOI 10.1109/TMM.2017.2760623
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang XC, 2015, AAAI CONF ARTIF INTE, P3174
   Zhang XC, 2014, IEEE DATA MINING, P1103, DOI 10.1109/ICDM.2014.19
   Zhu XG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3595
NR 60
TC 38
Z9 38
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2916
EP 2929
DI 10.1109/TMM.2019.2912735
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000018
DA 2024-07-18
ER

PT J
AU Liu, SG
   Zhang, XL
AF Liu, Shiguang
   Zhang, Xiaoli
TI Image Decolorization Combining Local Features and Exposure Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image decolorization; convolution neural network; color contrast
   preservation; exposure adjustment
ID COLOR
AB Image decolorization is a task aiming to transform a color image to a grayscale one and is a dimension reduction process which inevitably suffers from information loss. The general goal of image decolorization is to preserve the color contrast of the color image. According to human visual study, exposure affects the human visual perception, and low-exposure areas or overexposure areas will first attract the sense of sight. In addition, exposure also affects the contrast of the image, the contrast of low-exposure areas and over-exposure areas often cannot be well shown. Thus, the exposure should be taken into account in the process of image decolorization. Traditional local methods are not accurate enough to process local pixel blocks which may tend to cause local artifacts, while traditional global methods cannot greatly deal with local color blocks, which are usually time consuming too. Besides, the traditional image decolorization method usually uses the low-level features of an image. In this paper, the convolutional neural network is used to learn high-level abstract features of the image. We design a new convolutional neural-network framework with a local feature network and a rough classifier, which can learn the local semantic features and distinguish the different exposure conditions of color images. It is possible to learn the mapping model between input-output image pairs, which can generate better results in terms of color contrast preservation and exposure adjustment. Experiments indicate that our method does better in terms of color contrast preservation and exposure adjustment than the state of the art.
C1 [Liu, Shiguang] Tianjin Univ, Sch Comp Sci & Technol, Div Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Liu, Shiguang] Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300350, Peoples R China.
   [Zhang, Xiaoli] Tianjin Univ, Sch Software, Div Intelligence & Comp, Tianjin 300350, Peoples R China.
C3 Tianjin University; Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Div Intelligence & Comp, Tianjin 300350, Peoples R China.
EM lsg@tju.edu.cn; zxl218063@tju.edu.cn
FU Natural Science Foundation of China [61672375, 61170118]
FX Thisworkwas supported in part by the Natural Science Foundation of China
   under Grants 61672375 and 61170118.
CR Ancuti CO, 2011, LECT NOTES COMPUT SC, V6492, P79, DOI 10.1007/978-3-642-19315-6_7
   Ancuti CO, 2011, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2011.5995414
   Ancuti C, 2016, IEEE IMAGE PROC, P4107, DOI 10.1109/ICIP.2016.7533132
   [Anonymous], 2007, PROC 3 EUR C COMPUTA
   [Anonymous], 2010, P AS C COMP VIS
   Bala R, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P82
   Cadík M, 2008, COMPUT GRAPH FORUM, V27, P1745
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   Grundland M, 2007, PATTERN RECOGN, V40, P2891, DOI 10.1016/j.patcog.2006.11.003
   Hou X. X., ARXIV170709482
   HUNTER RS, 1958, J OPT SOC AM, V48, P985, DOI 10.1364/JOSA.48.000985
   IIZUKA S, 2016, ACM T GRAPHIC, V35, P1, DOI DOI 10.1145/2897824.2925974
   Ji Z. P., 2016, VISUAL COMPUT, V32, P1
   Ke P, 2017, MULTIMEDIA SYST, V23, P239, DOI 10.1007/s00530-015-0480-7
   Kim Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618507
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Li ZG, 2017, IEEE T IMAGE PROCESS, V26, P1243, DOI 10.1109/TIP.2017.2651366
   Lin Z. C., 2008, MSRTR2008189
   Liu Q. G., 2016, MULTIMED TOOLS APPL, V76, P1
   Liu QG, 2015, IEEE T IMAGE PROCESS, V24, P2889, DOI 10.1109/TIP.2015.2423615
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu CW, 2014, INT J COMPUT VISION, V110, P222, DOI 10.1007/s11263-014-0732-6
   Lu C, 2012, IEEE POW ENER SOC GE
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P4673, DOI 10.1109/TIP.2015.2460015
   MA MD, 2012, SIGGRAPH ASIA 2012 T, P1
   Nakajima M., 2015, COLOR RES APPL, V22, P385
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Smith K, 2008, COMPUT GRAPH FORUM, V27, P193, DOI 10.1111/j.1467-8659.2008.01116.x
   Song ML, 2010, IEEE T PATTERN ANAL, V32, P1537, DOI 10.1109/TPAMI.2009.74
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Song YJ, 2013, SPRINGERPLUS, V2, DOI 10.1186/2193-1801-2-204
   Sowmya V., 2016, SIGNAL IMAGE VIDEO P, V11, P1
   Wu TR, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.4.043004
   Wyszecki G., 1968, PHYS TODAY, V21, P83, DOI DOI 10.1063/1.3035025
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Zhang HC, 2017, LECT NOTES COMPUT SC, V10667, P560, DOI 10.1007/978-3-319-71589-6_49
   Zhang XL, 2018, VISUAL COMPUT, V34, P1099, DOI 10.1007/s00371-018-1524-8
NR 38
TC 22
Z9 23
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2461
EP 2472
DI 10.1109/TMM.2019.2903413
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400003
DA 2024-07-18
ER

PT J
AU Hu, B
   Li, LD
   Liu, HT
   Lin, WS
   Qian, JS
AF Hu, Bo
   Li, Leida
   Liu, Hantao
   Lin, Weisi
   Qian, Jiansheng
TI Pairwise-Comparison-Based Rank Learning for Benchmarking Image
   Restoration Algorithms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pairwise comparison; rank learning; image restoration; image structure
ID REFERENCE QUALITY ASSESSMENT; REPRESENTATION; CLASSIFICATION;
   STATISTICS; FRAMEWORK; SCALE
AB Image restoration has attracted substantial attention recently and many image restoration algorithms have been proposed for restoring latent clear images from degraded images. However, determining how to objectively evaluate the performances of these algorithms remains an open problem, which may hinder the further development of advanced image restoration techniques. Most image restoration-quality metrics are designed for specific restoration applications; hence, their generalization ability is limited. For benchmarking image restoration algorithms, the ranking of restored images that are generated via various algorithms, is the most heavily considered factor. Inspired by this, this paper presents a pairwise-comparison-based rank learning framework for benchmarking the performances of image restoration algorithms, which focuses on the relative quality ranking of restored images. Under the proposed framework, we further propose a general image restoration quality metric by integrating quality-aware features in both the spatial and frequency domains. The proposed metric exhibits good generalization performance, and it is applicable to various restoration applications. The results of extensive experiments that were conducted on eight public databases of five restoration scenarios demonstrate the superior performance of the proposed method over the existing quality metrics. Moreover, the proposed framework is used to improve the existing quality metrics for benchmarking image restoration algorithms and highly encouraging results are obtained.
C1 [Hu, Bo; Li, Leida; Qian, Jiansheng] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Jiangsu, Peoples R China.
   [Liu, Hantao] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, S Glam, Wales.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 China University of Mining & Technology; Cardiff University; Nanyang
   Technological University
RP Li, LD (corresponding author), China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Jiangsu, Peoples R China.
EM Hubo0523@cumt.edu.cn; lileida@cumt.edu.cn; hantao.liu@cs.cardiff.ac.uk;
   wslin@ntu.edu.sg; qianzhangiqa@163.com
RI Lin, Weisi/A-8011-2012; Li, Li/AEM-3636-2022; li, li/HII-4157-2022; Lin,
   Weisi/A-3696-2011
OI Lin, Weisi/0000-0001-9866-1947
FU National Natural Science Foundation of China [61771473, 61379143];
   Natural Science Foundation of Jiangsu Province [BK20181354]; Six Talent
   Peaks High-Level Talents in Jiangsu Province [XYDXX-063]; Qing Lan
   Project
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61771473 and 61379143, in part by the
   Natural Science Foundation of Jiangsu Province under Grant BK20181354,
   in part by the Six Talent Peaks High-Level Talents in Jiangsu Province
   under Grant XYDXX-063, and in part by the Qing Lan Project. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Raouf Hamzaoui. (Corresponding
   author: Leida Li.)
CR Amini S, 2014, INT GEOSCI REMOTE SE, DOI 10.1109/IGARSS.2014.6947074
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Chen ZY, 2014, PROC CVPR IEEE, P3003, DOI 10.1109/CVPR.2014.384
   Cho SI, 2018, IEEE T MULTIMEDIA, V20, P1738, DOI 10.1109/TMM.2017.2781371
   Dong WS, 2011, IEEE T IMAGE PROCESS, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Dong XS, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P19, DOI 10.1109/ITSC.2016.7795525
   Egiazarian K, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6752, DOI 10.1109/ICASSP.2018.8462294
   ENROTHCUGELL C, 1966, J PHYSIOL-LONDON, V187, P517, DOI 10.1113/jphysiol.1966.sp008107
   Gao F, 2015, IEEE T NEUR NET LEAR, V26, P2275, DOI 10.1109/TNNLS.2014.2377181
   Gu K, 2018, IEEE T VIS COMPUT GR, V24, P2689, DOI 10.1109/TVCG.2017.2771284
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Ham J, 2005, IEEE T GEOSCI REMOTE, V43, P492, DOI 10.1109/TGRS.2004.842481
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He H, 2011, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2011.5995713
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Hu B, 2018, IET COMPUT VIS, V12, P796, DOI 10.1049/iet-cvi.2017.0478
   Hu B, 2017, SIGNAL PROCESS-IMAGE, V58, P165, DOI 10.1016/j.image.2017.08.003
   Joshi P, 2018, VISUAL COMPUT, V34, P1739, DOI 10.1007/s00371-017-1460-z
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Lee JS, 2013, MULTIMED TOOLS APPL, V67, P31, DOI 10.1007/s11042-012-1011-6
   Li LD, 2017, IEEE ACCESS, V5, P2163, DOI 10.1109/ACCESS.2017.2661858
   Li LD, 2016, NEUROCOMPUTING, V177, P572, DOI 10.1016/j.neucom.2015.11.063
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Lia LD, 2016, SIGNAL PROCESS-IMAGE, V48, P81, DOI 10.1016/j.image.2016.09.005
   Liang HY, 2016, IEEE IMAGE PROC, P3106, DOI 10.1109/ICIP.2016.7532931
   Liang HY, 2016, IEEE T IMAGE PROCESS, V25, P5118, DOI 10.1109/TIP.2016.2601783
   Liang H, 2015, 2015 49TH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS AND COMPUTERS, P755, DOI 10.1109/ACSSC.2015.7421235
   Liu L, 2016, IEEE T IMAGE PROCESS, V25, P1368, DOI 10.1109/TIP.2016.2522378
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu YM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508391
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Ma KD, 2015, IEEE IMAGE PROC, P3600, DOI 10.1109/ICIP.2015.7351475
   MA L, 2016, IEEE T MULTIMEDIA, V18, P31
   MARCELJA S, 1980, J OPT SOC AM, V70, P1297, DOI 10.1364/JOSA.70.001297
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ni ZK, 2018, IEEE T IMAGE PROCESS, V27, P4516, DOI 10.1109/TIP.2018.2839890
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pan JS, 2016, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2016.306
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Ponomarenko N, 2011, LECT NOTES COMPUT SC, V6915, P459, DOI 10.1007/978-3-642-23687-7_42
   Qian JS, 2014, DIGIT SIGNAL PROCESS, V33, P125, DOI 10.1016/j.dsp.2014.06.009
   RODIECK RW, 1965, J NEUROPHYSIOL, V28, P833, DOI 10.1152/jn.1965.28.5.833
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Siahaan E, 2018, SIGNAL PROCESS-IMAGE, V60, P237, DOI 10.1016/j.image.2017.10.009
   Wang GC, 2017, IEEE IMAGE PROC, P3145, DOI 10.1109/ICIP.2017.8296862
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu JJ, 2015, IEEE T IMAGE PROCESS, V24, P4602, DOI 10.1109/TIP.2015.2460467
   Xu L, 2017, IEEE T CIRC SYST VID, V27, P1833, DOI 10.1109/TCSVT.2016.2543099
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zeng K, 2013, CONF REC ASILOMAR C, P1351, DOI 10.1109/ACSSC.2013.6810514
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang J, 2012, IEEE J EM SEL TOP C, V2, P380, DOI 10.1109/JETCAS.2012.2220391
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang W, 2016, PATTERN RECOGN, V59, P176, DOI 10.1016/j.patcog.2016.01.034
   Zhang Y. P., 2013, TSINGHUA U ED RES, V4, P22, DOI DOI 10.1117/1.JEI.22.4.043025
   Zhu X, 2010, IEEE T IMAGE PROCESS, V19, P3116, DOI 10.1109/TIP.2010.2052820
NR 68
TC 28
Z9 29
U1 5
U2 62
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 2042
EP 2056
DI 10.1109/TMM.2019.2894958
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700012
DA 2024-07-18
ER

PT J
AU Qiu, Y
   Liu, Y
   Arteaga-Falconi, J
   Dong, HW
   El Saddik, A
AF Qiu, Ying
   Liu, Yang
   Arteaga-Falconi, Juan
   Dong, Haiwei
   El Saddik, Abdulmotaleb
TI EVM-CNN: Real-Time Contactless Heart Rate Estimation From Facial Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Spatial decomposition; temporal filtering; convolutional neural network
ID NONCONTACT; ROBUST
AB With the increase in health consciousness, noninvasive body monitoring has aroused interest among researchers. As one of the most important pieces of physiological information, researchers have remotely estimated the heart rate (HR) from facial videos in recent years. Although progress has been made over the past few years, there are still some limitations, like the processing time increasing with accuracy and the lack of comprehensive and challenging datasets for use and comparison. Recently, it was shown that HR information can be extracted from facial videos by spatial decomposition and temporal filtering. Inspired by this, a new framework is introduced in this paper to remotely estimate the HR under realistic conditions by combining spatial and temporal filtering and a convolutional neural network. Our proposed approach shows better performance compared with the benchmark on the MMSE-HR dataset in terms of both the average HR estimation and short-time HR estimation. High consistency in short-time HR estimation is observed between our method and the ground truth.
C1 [Qiu, Ying; Liu, Yang; Arteaga-Falconi, Juan; Dong, Haiwei; El Saddik, Abdulmotaleb] Univ Ottawa, Sch Elect Engn & Comp Sci, Multimedia Comp Res Lab, Ottawa, ON K1N 6N5, Canada.
C3 University of Ottawa
RP Dong, HW (corresponding author), Univ Ottawa, Sch Elect Engn & Comp Sci, Multimedia Comp Res Lab, Ottawa, ON K1N 6N5, Canada.
EM yqiu059@uottawa.ca; yliu344@uottawa.ca; jarte060@uottawa.ca;
   hdong@uottawa.ca; elsaddik@uottawa.ca
RI Dong, Haiwei/I-1273-2014; /D-4159-2009
OI Dong, Haiwei/0000-0003-1437-7805; /0000-0002-7690-8547; Qiu,
   Ying/0000-0002-7851-4488
FU NSERC [RGPIN/2018-05600]
FX This work was supported by NSERC Grant RGPIN/2018-05600 on digital
   twins.
CR Alghoul K, 2017, IEEE ACCESS, V5, P4711, DOI 10.1109/ACCESS.2017.2678521
   Allen J, 2007, PHYSIOL MEAS, V28, pR1, DOI 10.1088/0967-3334/28/3/R01
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.263
   [Anonymous], 2017, Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications
   Balakrishnan G, 2013, PROC CVPR IEEE, P3430, DOI 10.1109/CVPR.2013.440
   de Haan G, 2013, IEEE T BIO-MED ENG, V60, P2878, DOI 10.1109/TBME.2013.2266196
   Hoon Y, 2016, C IND ELECT APPL, P1, DOI 10.1109/IEACON.2016.8067346
   Lam A, 2015, IEEE I CONF COMP VIS, P3640, DOI 10.1109/ICCV.2015.415
   Li XB, 2014, PROC CVPR IEEE, P4264, DOI 10.1109/CVPR.2014.543
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Montero AS, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P587, DOI 10.1109/ICCVW.2015.80
   Poh MZ, 2011, IEEE T BIO-MED ENG, V58, P7, DOI 10.1109/TBME.2010.2086456
   Poh MZ, 2010, OPT EXPRESS, V18, P10762, DOI 10.1364/OE.18.010762
   Prakash SKA, 2018, BIOMED OPT EXPRESS, V9, P873, DOI 10.1364/BOE.9.000873
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Verkruysse W, 2008, OPT EXPRESS, V16, P21434, DOI 10.1364/OE.16.021434
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
   Yang C, 2017, IEEE T MULTIMEDIA, V19, P1625, DOI 10.1109/TMM.2017.2672198
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374
NR 22
TC 96
Z9 100
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1778
EP 1787
DI 10.1109/TMM.2018.2883866
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Rodrigues, F
   Ascenso, J
   Rodrigues, A
   Queluz, MP
AF Rodrigues, Fabio
   Ascenso, Joao
   Rodrigues, Antonio
   Queluz, Maria Paula
TI Blind Quality Assessment of 3-D Synthesized Views Based on Hybrid
   Feature Classes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality assessment; image features; depth-image-based rendering;
   multiview video-plus-depth; view synthesis; synthesized image dataset
ID IMAGE; GENERATION
AB In this paper, a novel quality metric to evaluate depth-based synthesized views is proposed. This metric relies on a hybrid approach that uses features extracted in different phases of the image synthesis procedure, namely from the bitstream, from intermediate data produced during the synthesis process, and from the final synthesized view; these features are then combined using support vector regression. A new data set of synthesized images, with compression and rendering artifacts, was built and used to develop and assess the proposed metric. The metric performance is compared with conventional full-reference two-dimensional image quality assessment metrics and with quality assessment metrics developed specifically for synthesized images. The experimental results showed that the proposed solution outperforms the considered benchmark metrics, being able to predict the subjective quality scores of the synthesized images with a Pearson correlation coefficient close to 0.9.
C1 [Rodrigues, Fabio; Ascenso, Joao; Rodrigues, Antonio; Queluz, Maria Paula] Univ Lisbon, Inst Telecomunicacoes, Inst Super Tecn, P-1040001 Lisbon, Portugal.
C3 Universidade de Lisboa; Instituto de Telecomunicacoes
RP Ascenso, J (corresponding author), Univ Lisbon, Inst Telecomunicacoes, Inst Super Tecn, P-1040001 Lisbon, Portugal.
EM rodrigues@ist.utl.pt; joao.ascenso@lx.it.pt; ar@lx.it.pt;
   paula.queluz@lx.it.pt
RI Rodrigues, Fábio Ribeiro/I-8167-2015; Ascenso, Joao/B-9024-2008;
   Rodrigues, Antonio/B-5234-2016
OI Ascenso, Joao/0000-0001-9902-5926; Rodrigues,
   Antonio/0000-0003-2115-7245; Queluz, Maria Paula/0000-0003-0266-4022
FU FCT [UID/EEA/50008/2019]
FX This work was supported by FCT through the project UID/EEA/50008/2019.
CR [Anonymous], 2012, ITUR Recommendation BT. 500-13
   [Anonymous], 23001102015 ISOIEC
   [Anonymous], JTC1SC29WG11 ISOIEC
   Battisti F, 2015, SIGNAL PROCESS-IMAGE, V30, P78, DOI 10.1016/j.image.2014.10.005
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   Bose E, 2013, PROC SPIE, V8648, DOI 10.1117/12.2002410
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Conze PH, 2012, PROC SPIE, V8288, DOI 10.1117/12.908762
   DUBUISSON MP, 1994, INT C PATT RECOG, P566, DOI 10.1109/ICPR.1994.576361
   Ekmekcioglu E., 2010, P INT C US CENTR MED, P76
   ELYAMANY NA, 2010, P 3DTV C TRUE VIS CA, P1
   Farid MS, 2017, IEEE INT CON MULTI, P505, DOI 10.1109/ICME.2017.8019307
   Farid MS, 2013, IEEE INT WORKSH MULT, P135, DOI 10.1109/MMSP.2013.6659277
   Gonzalez RC, 2009, DIGITAL IMAGE PROCES, DOI 10.1117/1.3115362
   International Organisation For Standardization, 2012, JTC1SC29WG11N12559 I
   ITU-T, 2008, ITU T P910 SUBJECTIV
   Kovesi P., 1999, Videre, V1
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Lie WN, 2018, IEEE T MULTIMEDIA, V20, P1075, DOI 10.1109/TMM.2017.2763319
   Lipski C, 2014, IEEE T CIRC SYST VID, V24, P942, DOI 10.1109/TCSVT.2014.2302379
   Liu XK, 2015, IEEE T IMAGE PROCESS, V24, P4847, DOI 10.1109/TIP.2015.2469140
   Muddala SM, 2016, J VIS COMMUN IMAGE R, V38, P351, DOI 10.1016/j.jvcir.2016.02.017
   Narwaria M, 2010, IEEE T NEURAL NETWOR, V21, P515, DOI 10.1109/TNN.2010.2040192
   NYA PN, 2011, IEEE T MULTIMED, V13, P453
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Rodrigues F., 2017, THESIS
   Senoh T., 2016, JTC1SC29WG11M38979 I
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Tsai CT, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP 2013)
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 32
TC 9
Z9 10
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1737
EP 1749
DI 10.1109/TMM.2018.2888830
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700010
DA 2024-07-18
ER

PT J
AU Li, CH
   An, P
   Shen, LQ
   Li, K
AF Li, Chunhua
   An, Ping
   Shen, Liquan
   Li, Kai
TI A Modified Just Noticeable Depth Difference Model Built in Perceived
   Depth Space
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Just noticeable depth difference; blurring effect; 3D display; depth
   perception; binocular disparity
ID SENSATION ENHANCEMENT; ACCOMMODATION; RESPONSES; IMAGES
AB This paper proposes a modified just noticeable depth difference (JNDD) (JNDiD) model in perceived depth space. The JNDiD model improves the accuracy of current JNDD models by also taking into account the blurriness caused by the change in accommodation when a 3-D object is perceived far from the screen. This change in accommodation is a result of the convergence-accommodation conflict, in which convergence plays a leading role. In the JNDiD model described in this paper, the JNDD threshold is the addition between a base threshold and an additional threshold. Adapted to different blurring effects in three depth regions, the additional threshold is defined as a three-piecewise linear function. The proposed model also attempts to separate the JNDD modeling from the display modeling and viewing conditions in perceived depth space, making it applicable for different types of displays given their specific model parameters. With the help of model parameter transfer functions, the JNDiD model in perceived depth space and the corresponding one in any specific stimulated depth space can be easily transformed to each other. Experimental results demonstrate the effectiveness and superiority of the JNDiD model in perceived depth space.
C1 [Li, Chunhua] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Li, Chunhua] Hebei Univ Sci & Technol, Sch Informat Sci & Engn, Shijiazhuang 050051, Hebei, Peoples R China.
   [An, Ping; Shen, Liquan; Li, Kai] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 270002, Peoples R China.
C3 Shanghai University; Hebei University of Science & Technology; Shanghai
   University
RP Li, CH (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM jinhao@shu.edu.cn; anping@shu.edu.cn; jsslq@shu.edu.cn;
   kailee@shu.edu.cn
RI li, mengyang/JWO-9551-2024; Shen, Liquan/D-4832-2012
OI Shen, Liquan/0000-0002-2148-6279
FU National Natural Science Foundation of China [U1301257, 61172096,
   61571285, 61422111, 61171084, 61601278]
FX This work was supported by the National Natural Science Foundation of
   China under Grants U1301257, 61172096, 61571285, 61422111, 61171084, and
   61601278. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Balakrishnan
   Prabhakaran.
CR Alpaslan Z. Y., 2004, 2004 IEEE 6th Workshop on Multimedia Signal Processing, P256
   [Anonymous], 2002, BT50011 ITUR
   [Anonymous], 2006, N8038 ISOIEC JTC 1SC
   [毕家瑜 BI Jia-yu], 2009, [光学技术, Optical Technology], V35, P575
   Bruce V., 2003, VISUAL PERCEPTION
   Chen Y, 2014, J VIS COMMUN IMAGE R, V25, P679, DOI 10.1016/j.jvcir.2013.03.013
   Cutting J., 1995, HDB PERCEIVE COGNITI, V5
   Davis H, 2002, J AAPOS, V6, P377, DOI 10.1067/mpa.2002.127916
   De Silva DVSX, 2010, IEEE INT CON MULTI, P1219, DOI 10.1109/ICME.2010.5582582
   De Silva D.V.S.X., 2011, Selected topics in Signal Processing, IEEE Journal on, V5, P335
   De Silva V, 2011, IEEE T MULTIMEDIA, V13, P498, DOI 10.1109/TMM.2011.2129500
   Ding H, 2015, SENSYS'15: PROCEEDINGS OF THE 13TH ACM CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P141, DOI 10.1145/2809695.2809708
   FINCHAM EF, 1951, BRIT J OPHTHALMOL, V35, P381, DOI 10.1136/bjo.35.7.381
   Foley J. M., 1978, HDB SENSORY PHYSL
   FRY G A, 1955, Br J Physiol Opt, V12, P130
   HIRUMA N, 1993, SMPTE J, V102, P1137, DOI 10.5594/J01656
   Inoue T, 1997, APPL OPTICS, V36, P4509, DOI 10.1364/AO.36.004509
   JULESZ B, 1986, VISION RES, V26, P1601, DOI 10.1016/0042-6989(86)90178-1
   Jung SW, 2013, IEEE T IMAGE PROCESS, V22, P3892, DOI 10.1109/TIP.2013.2263150
   Jung SW, 2012, IEEE T IMAGE PROCESS, V21, P3624, DOI 10.1109/TIP.2012.2191569
   Koh LH, 1998, OPHTHAL PHYSL OPT, V18, P279, DOI 10.1016/S0275-5408(97)00089-6
   KOTULAK JC, 1995, VISION RES, V35, P791, DOI 10.1016/0042-6989(94)00157-H
   Lei JJ, 2015, IEEE T MULTIMEDIA, V17, P457, DOI 10.1109/TMM.2015.2400823
   Li C., 2016, P 13 INT FOR DIG TV, P63
   Li C., 2016, P VIS COMM IM PROC, P1
   Ma J, 2017, APPL OPTICS, V56, P8291, DOI 10.1364/AO.56.008291
   PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940
   REICHELT S, 2010, P SPIE 3D DISPLAYS, P1
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Shibata T, 2005, J SOC INF DISPLAY, V13, P665, DOI 10.1889/1.2039295
   Son JY, 2006, J DISP TECHNOL, V2, P359, DOI 10.1109/JDT.2006.884694
   WESTHEIMER G, 1956, ARCH OPHTHALMOL-CHIC, V55, P848
   Wu X., 1984, CHIN OPHTHALMIC RES, V2, P209
   Zhou TS, 2007, COMPUT GRAPH FORUM, V26, P15, DOI 10.1111/j.1467-8659.2007.00935.x
NR 34
TC 2
Z9 3
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1464
EP 1475
DI 10.1109/TMM.2018.2882085
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400010
DA 2024-07-18
ER

PT J
AU Xia, XJ
   Togneri, R
   Sohel, F
   Huang, DF
AF Xia, Xianjun
   Togneri, Roberto
   Sohel, Ferdous
   Huang, Defeng
TI Auxiliary Classifier Generative Adversarial Network With Soft Labels in
   Imbalanced Acoustic Event Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Acoustic event detection; auxiliary classifier generative adversarial
   networks; quantized confidence measure
ID NEURAL-NETWORKS
AB In acoustic event detection, the training data size of some acoustic events is often small and imbalanced. To deal with this, this paper proposes generating the virtual training data categorically using the auxiliary classifier generative adversarial networks. Soft labels of acoustic events are first calculated to represent the acoustic event localization information. The closer the current frame is to the middle of the manually labeled acoustic event, the higher the soft label will be, which makes the soft labels positively correlated with the acoustic event localization. Then, the acoustic event class and the quantized soft labels are used as the input condition to the auxiliary classifier generative adversarial networks to generate an arbitrary number of training samples. Experimental results on the TUT Sound Event 2016 under the home environment and TUT Sound Event 2017 under the street environment demonstrate the improved performance of the proposed technique compared to existing acoustic event detection systems.
C1 [Xia, Xianjun; Togneri, Roberto; Huang, Defeng] Univ Western Australia, Sch Elect Elect & Comp Engn, Perth, WA 6009, Australia.
   [Sohel, Ferdous] Murdoch Univ, Sch Engn & Informat Technol, Murdoch, WA 6150, Australia.
C3 University of Western Australia; Murdoch University
RP Xia, XJ (corresponding author), Univ Western Australia, Sch Elect Elect & Comp Engn, Perth, WA 6009, Australia.
EM xianjun.xia@research.uwa.edu.au; roberto.togneri@uwa.edu.au;
   F.Sohel@murdoch.edu.au; david.huang@uwa.edu.au
RI Togneri, Roberto/C-2466-2013; Sohel, Ferdous/C-2428-2013; Huang,
   David/JTS-8033-2023; Huang, David/H-5081-2014
OI Sohel, Ferdous/0000-0003-1557-4907; Togneri,
   Roberto/0000-0002-3778-4633; Huang, David/0000-0002-1431-8859
FU International Postgraduate Research Scholarship from the University of
   Western Australia
FX This work was supported by the International Postgraduate Research
   Scholarship from the University of Western Australia.
CR [Anonymous], P DCASE2017 CHALL SE
   [Anonymous], 2015, P IEEE INT JOINT C N, DOI DOI 10.1109/IJCNN.2015.7280624
   [Anonymous], 2017, P 31 INT C NEURAL IN, DOI DOI 10.5555/3295222.3295408
   [Anonymous], P DCASE2017 CHALL SE
   [Anonymous], 2016, ARXIV160407160
   [Anonymous], 2017, Detection and Classification of Acoustic Scenes and Events (DCASE)
   [Anonymous], P IEEE AASP CHALL DE
   [Anonymous], 2016, ARXIV161009585
   [Anonymous], 2017, ARXIV171002997
   [Anonymous], 2017, ARXIV170602293
   [Anonymous], ARXIV171000343
   [Anonymous], 2014, Proc. of ICML
   [Anonymous], P DCASE2017 CHALL SE
   [Anonymous], P WORKSH DET CLASS A
   [Anonymous], 2016, ADV NEURAL INFORM PR
   Antoniou Antreas, 2017, ARXIV171104340
   Çakir E, 2017, IEEE-ACM T AUDIO SPE, V25, P1291, DOI 10.1109/TASLP.2017.2690575
   Chou S.Y., 2017, RECALL, V14, P55
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Denton E.L., 2015, CoRR, P1486
   Ding XM, 2016, IEEE T MULTIMEDIA, V18, P1616, DOI 10.1109/TMM.2016.2572000
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Phan H, 2015, IEEE-ACM T AUDIO SPE, V23, P20, DOI 10.1109/TASLP.2014.2367814
   King DB, 2015, ACS SYM SER, V1214, P1
   Kingma D. P., 2013, ARXIV13126114
   Lin XD, 2016, IEEE T MULTIMEDIA, V18, P1480, DOI 10.1109/TMM.2016.2571999
   Mesaros A, 2016, APPL SCI-BASEL, V6, DOI 10.3390/app6060162
   Minotto Vicente P., 2014, IEEE Transactions on Multimedia, V16, P1032, DOI 10.1109/TMM.2014.2305632
   Mun Seongkyu, 2017, DCASE, P93
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Niessen ME, 2013, IEEE WORK APPL SIG
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Parascandolo G, 2016, INT CONF ACOUST SPEE, P6440, DOI 10.1109/ICASSP.2016.7472917
   Radford A., 2015, ARXIV
   Salamon J, 2017, IEEE WORK APPL SIG, P344, DOI 10.1109/WASPAA.2017.8170052
   Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Schröder J, 2013, IEEE WORK APPL SIG, DOI 10.1109/WASPAA.2013.6701868
   Smith JBL, 2014, IEEE T MULTIMEDIA, V16, P1219, DOI 10.1109/TMM.2014.2310706
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Temko A., 2006, Multimodal Technologies for Perception of Humans. First International Evaluation Workshop on Classification of Events, Activities and Relationships, CLEAR 2006. Revised Selected Papers (Lecture Notes in Computer Science Vol.4122), P311
   Wu Yonghui, 2016, P C ASS MACH TRANSL
   Xia XJ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P306, DOI 10.1109/ICASSP.2018.8461845
   Xia XJ, 2018, PATTERN RECOGN, V81, P1, DOI 10.1016/j.patcog.2018.03.025
   Xia XJ, 2017, IEEE INT CON MULTI, P163, DOI 10.1109/ICME.2017.8019452
   Xia XJ, 2017, IEEE INT CON MULTI, P157, DOI 10.1109/ICME.2017.8019418
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhu X., 2017, arXiv preprint arXiv:1711.00648
   Zhuang XD, 2009, INT CONF ACOUST SPEE, P69, DOI 10.1109/ICASSP.2009.4959522
NR 53
TC 31
Z9 33
U1 3
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1359
EP 1371
DI 10.1109/TMM.2018.2879750
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400002
DA 2024-07-18
ER

PT J
AU Yuan, XX
   Cai, ZC
AF Yuan, Xixi
   Cai, Zhanchuan
TI An Adaptive Triangular Partition Algorithm for Digital Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image processing; image adaptive triangular partition (IATP);
   least-squares method; image steganography; information encryption
ID COMPRESSION; VIDEO
AB The partition algorithm as a digital image processing technique is significant to many applications, such as data encryption, image denoising, and 3-D reconstruction. In order to achieve well partition that can availably reduce the distortion phenomenon, a novel approach named image adaptive triangular partition (IATP) is proposed, which considers the grayscale distribution of the image and removes the shared edges between the adjacent triangles in the partition mesh. The least-squares method is used to fit the sampled position-associated gray value of the image to determine whether further partition should be performed, that is, if the sum of squared residuals is bigger than the preselected control value, the current area will be divided into four separated sub-triangles by using the self-similar method, and then preparing the next fitting on each of them in recursion; otherwise, the terminal operation is reached. When the recursive partition of the image is done, the triangular partition mesh with the quaternary notations is obtained. The experimental results demonstrate that the performance of the IATP algorithm proposed in this paper is better than the existing state-of-the-art nonuniform partitions, and it solves the redundant coding problem and reduces the image quality losses. In addition, two applications-image steganography and information encryption-are selected to verify that the proposed algorithm has good feasibility and robustness.
C1 [Yuan, Xixi; Cai, Zhanchuan] Macau Univ Sci & Technol, Fac Informat Technol, Taipa 999078, Macau, Peoples R China.
C3 Macau University of Science & Technology
RP Cai, ZC (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Taipa 999078, Macau, Peoples R China.
EM yuanx-ixi.ok@foxmail.com; zccai@must.edu.mo
FU National Basic Research Program of China (973 Program) [2011CB302400];
   Science and Technology Development Fund of Macau [048/2016/A2,
   0012/2018/A1, 0069/2018/A2]; Open Project of State Key Laboratory of
   Virtual Reality Technology and Systems, Beihang University
   [VRLAB2019C02]; Open Project Program of the State Key Laboratory of
   CAD&CG of Zhejiang University [A1910]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2011CB302400, in part by the Science
   and Technology Development Fund of Macau under Grant 048/2016/A2,
   0012/2018/A1, and 0069/2018/A2, in part by the Open Project of State Key
   Laboratory of Virtual Reality Technology and Systems, Beihang University
   under Grant VRLAB2019C02, and in part by the Open Project Program of the
   State Key Laboratory of CAD&CG of Zhejiang University under Grant A1910.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sanjeev Mehrotra.
CR Cohen-Or D., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P67
   Computer Vision Group, TEST IM
   Costa L.Da Fontoura., 2010, Shape Analysis and Classification: Theory and Practice
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Hu KK, 2016, COMPUT METHOD APPL M, V305, P405, DOI 10.1016/j.cma.2016.03.021
   Islam MB, 2018, IEEE T MULTIMEDIA, V20, P2964, DOI 10.1109/TMM.2018.2820324
   Jiang H., 2012, THESIS
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Kim GH, 1996, P SOC PHOTO-OPT INS, V2660, P262, DOI 10.1117/12.234708
   KinTak U., 2010, Proceedings 2010 International Conference on Multimedia Communications (Mediacom 2010), P9, DOI 10.1109/MEDIACOM.2010.25
   LEGALL D, 1991, COMMUN ACM, V34, P46, DOI 10.1145/103085.103090
   Lertrattanapanich S, 2002, IEEE T IMAGE PROCESS, V11, P1427, DOI 10.1109/TIP.2002.806234
   LI H, 1995, IEEE T IMAGE PROCESS, V4, P320, DOI 10.1109/83.366480
   Li PY, 2018, IEEE T MULTIMEDIA, V20, P1960, DOI 10.1109/TMM.2017.2786860
   LIU HC, 1990, PATTERN RECOGN, V23, P51, DOI 10.1016/0031-3203(90)90048-P
   Ma J., 2011, IEEE T MULTIMEDIA, V20, P3111
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Qi D., 2011, DISCONTINUOUS ORTHOG, P280
   Queluz MP, 2001, SIGNAL PROCESS-IMAGE, V16, P461, DOI 10.1016/S0923-5965(00)00010-2
   ShengDun Hu, 2011, Proceedings of the 2011 IEEE 14th International Conference on Computational Science and Engineering (CSE 2011). 11th International Symposium on Pervasive Systems, Algorithms, Networks (I-SPAN 2011). 10th IEEE International Conference on Ubiquitous Computing and Communications (IUCC 2011), P57, DOI 10.1109/CSE.2011.24
   Shewchuk J., 2016, Delaunay Mesh Generation
   Tak U. Kin, 2009, 2009 International Conference on Information and Automation (ICIA), P995, DOI 10.1109/ICINFA.2009.5205063
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577
   Trisiripisal P., 2003, THESIS
   Wan L., 2009, THESIS
   [余建德 Yu Jiande], 2009, [计算机研究与发展, Journal of Computer Research and Development], V46, P1432
   Yu WY, 2015, PROCEDIA ENGINEER, V124, P44, DOI 10.1016/j.proeng.2015.10.121
NR 27
TC 3
Z9 3
U1 2
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1372
EP 1383
DI 10.1109/TMM.2018.2881069
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400003
DA 2024-07-18
ER

PT J
AU Wu, YW
   Gao, F
   Huang, YC
   Lin, J
   Chandrasekhar, V
   Yuan, JS
   Duan, LY
AF Wu, Yuwei
   Gao, Feng
   Huang, Yicheng
   Lin, Jie
   Chandrasekhar, Vijay
   Yuan, Junsong
   Duan, Ling-Yu
TI Codebook-Free Compact Descriptor for Scalable Visual Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual Search; Compact Descriptor; CDVS; CDVA; Codebook free; Feature
   Descriptor Aggregation
ID LEARNING BINARY-CODES; PRODUCT QUANTIZATION; VECTOR
AB The MPEG compact descriptors for visual search (CDVS) is a standard toward image matching and retrieval. To achieve high retrieval accuracy over a large scale image/video dataset, recent research efforts have demonstrated that employing extremely high-dimensional descriptors such as the Fisher vector (FV) and the vector of locally aggregated descriptors (VLAD) can yield good performance. Since the FV (or VLAD) possesses high discriminability but small visual vocabulary, it has been adopted by CDVS to construct a global compact descriptor. In this paper, we study the development of global compact descriptors in the completed CDVS standard and the emerging compact descriptors for video analysis (CDVA) standard, in which we formulate the FV (or VLAD) compression as a resource-constrained optimization problem. Accordingly, we propose a codebook-free aggregation method via dual selection to generate a global compact visual descriptor, which supports fast and accurate feature matching free of large visual codebooks, fulfilling the low memory requirement of mobile visual search at significantly reduced latency. Specifically, we investigate both sample-specific Gaussian component redundancy and bit dependency within a binary aggregated descriptor to produce compact binary codes. Our technique contributes to the scalable compressed Fisher vector (SCFV) adopted by the CDVS standard. Moreover, the SCFV descriptor is currently serving as the frame-level hand-crafted video feature, which inspires the inheritance of CDVS descriptors for the emerging CDVA standard. Furthermore, we investigate the positive complementary effect of our standard compliant compact descriptor and deep learning based features extracted from convolutional neural networks with significant mean average precision gains. Extensive evaluation over benchmark databases shows the significant merits of the codebook-free binary codes for scalable visual search.
C1 [Wu, Yuwei] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Wu, Yuwei; Gao, Feng] PKU NTU Joint Res Inst, Singapore 639798, Singapore.
   [Gao, Feng] Tsinghua Univ, Future Lab, Beijing 100084, Peoples R China.
   [Huang, Yicheng; Duan, Ling-Yu] Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100080, Peoples R China.
   [Lin, Jie; Chandrasekhar, Vijay] Inst Infocomm Res, Singapore 138634, Singapore.
   [Yuan, Junsong] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
C3 Beijing Institute of Technology; Tsinghua University; Peking University;
   Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); State University of New York (SUNY) System;
   State University of New York (SUNY) Buffalo
RP Duan, LY (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100080, Peoples R China.
EM wuyuwei@bit.edu.cn; gaofeng2018@tsinghua.edu.cn;
   anorange0409@pku.edu.cn; lin-j@i2r.a-star.edu.sg;
   vijay@i2r.a-star.edu.sg; jsyuan@buffalo.edu; lingyu@pku.edu.cn
RI Yuan, Junsong/R-4352-2019; Huang, Yicheng/AFV-4905-2022
OI Huang, Yicheng/0000-0002-0293-6844; Yuan, Junsong/0000-0002-7901-8793
FU National Natural Science Foundation of China [61661146005, U1611461,
   61702037]; National Key Research and Development Program of China
   [2016YFB1001501]; Key Research and Development Program of Beijing
   Municipal Science & Technology Commission [D171100003517002]
FX This work is supported in part by the National Natural Science
   Foundation of China under Grants 61661146005, U1611461, and 61702037, in
   part by the National Key Research and Development Program of China under
   Grant 2016YFB1001501, and in part by the Key Research and Development
   Program of Beijing Municipal Science & Technology Commission under Grant
   D171100003517002. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Zhu Li. (Yuwei Wu
   and Feng Gao are co-first authors.)
CR [Anonymous], 2014, PR MACH LEARN RES
   [Anonymous], ARXIV170408141
   [Anonymous], 2016, P INT JOINT C ART IN
   [Anonymous], THESIS
   [Anonymous], 2009, NEURIPS
   Azizpour Hossein, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P36, DOI 10.1109/CVPRW.2015.7301270
   Bäuml M, 2013, PROC CVPR IEEE, P3602, DOI 10.1109/CVPR.2013.462
   Cao Y, 2017, PROC CVPR IEEE, P916, DOI 10.1109/CVPR.2017.104
   Chandrasekhar V, 2014, IEEE DATA COMPR CONF, P3, DOI 10.1109/DCC.2014.50
   Chen D, 2013, SIGNAL PROCESS, V93, P2316, DOI 10.1016/j.sigpro.2012.06.005
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P1996, DOI 10.1109/TMM.2017.2705918
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Duan LY, 2018, IEEE T IMAGE PROCESS, V27, DOI 10.1109/TIP.2018.2818008
   Duan LY, 2016, IEEE T IMAGE PROCESS, V25, P179, DOI 10.1109/TIP.2015.2500034
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   Duan LY, 2014, IEEE MULTIMEDIA, V21, P30, DOI 10.1109/MMUL.2013.66
   Esmaeili MM, 2012, IEEE T PATTERN ANAL, V34, P2481, DOI 10.1109/TPAMI.2012.170
   Ge TZ, 2013, PROC CVPR IEEE, P2946, DOI 10.1109/CVPR.2013.379
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Guo YC, 2017, IEEE T IMAGE PROCESS, V26, P1344, DOI 10.1109/TIP.2017.2652730
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Ji RR, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2597181
   Jiang QY, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2248
   Kong W., 2012, P 26 AAAI C ART INT, P634, DOI DOI 10.5555/2900728.2900819
   Li J, 2017, IEEE T MULTIMEDIA, V19, P559, DOI 10.1109/TMM.2016.2617089
   Li Y, 2015, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2015.7299108
   Lin J, 2017, IEEE T MULTIMEDIA, V19, P1968, DOI 10.1109/TMM.2017.2713410
   Lin J, 2014, IEEE SIGNAL PROC LET, V21, P195, DOI 10.1109/LSP.2013.2296532
   Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu L, 2017, IEEE T IMAGE PROCESS, V26, P1289, DOI 10.1109/TIP.2017.2651390
   Liu SC, 2017, IEEE T MULTIMEDIA, V19, P1785, DOI 10.1109/TMM.2017.2692181
   Lou Y., 2017, P DAT COMPR C, P73
   Müller H, 2017, IEEE T MULTIMEDIA, V19, P2093, DOI 10.1109/TMM.2017.2729400
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Norouzi M, 2012, PROC CVPR IEEE, P3108, DOI 10.1109/CVPR.2012.6248043
   PARIKH N., 2014, FDN TRENDS OPTIM, V1, P3, DOI DOI 10.1561/2400000003
   Parkhi OM, 2014, PROC CVPR IEEE, P1693, DOI 10.1109/CVPR.2014.219
   Paschalakis S., 2015, ISOIEC1593813
   PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009
   Redondi A, 2013, IEEE INT WORKSH MULT, P278, DOI 10.1109/MMSP.2013.6659301
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tao R, 2015, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2015.7298613
   Tolias G., 2016, ARXIV151105879V2
   Wang JF, 2015, IEEE T KNOWL DATA EN, V27, P180, DOI 10.1109/TKDE.2014.2324592
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   XIA Y, 2015, PROC CVPR IEEE, P3332
   Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795
   Yan Li, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163089
   Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812
   Yu FX, 2014, PR MACH LEARN RES, V32, P946
   Zhang T, 2015, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2015.7299085
   Zhou N, 2016, PATTERN RECOGN, V53, P87, DOI 10.1016/j.patcog.2015.12.008
   Zhu XF, 2017, IEEE T MULTIMEDIA, V19, P2033, DOI 10.1109/TMM.2017.2703636
NR 64
TC 6
Z9 6
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 388
EP 401
DI 10.1109/TMM.2018.2856628
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400010
DA 2024-07-18
ER

PT J
AU Yang, M
   Zheng, NN
AF Yang, Meng
   Zheng, Nanning
TI SynBF: A New Bilateral Filter for Postremoval of Noise From Synthesis
   Views in 3-D Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D video system; synthesis view; local filter; edge preservation;
   bilateral filter
ID IMAGE; ENHANCEMENT; DISTORTION; PHASE
AB In 3-D video systems, noise in the texture and depth videos of reference views may not be removed (Scenario 1) or not fully removed (Scenario 2) by prefiltering methods before the view synthesis procedure. In these scenarios, the noise is transferred to the generated synthesis view. After investigating the noise model of the synthesis view, we conclude that the noise in the synthesis view not only causes fluctuation in the photometric values of pixels in the range domain but also additionally shifts the positions of neighbor pixels in the spatial domain compared to that in natural images. It consequently damages the textural content near edges in the synthesis view, for which the popular local filters of natural images, that is, the bilateral filter (BF) and the guided filter, do not work well. In this paper, we develop a new local filter for the synthesis view (named SynBF) after it has been generated, which has a similar expression as that of the BF but not the exact same weight terms. On one hand, the spatial term in the classical BF is directly reused due to its robustness to noise, which gives high weights to spatially closed pixels of to-be-filtered pixels. On the other hand, a reliability term is designed that gives high weights to pixels that are unlikely to be affected by noise. It is inspired by the finding that not all pixels are significantly affected by noise in the synthesis view. In this way, true edge profiles are protected in the filtering process. Experiments are conducted on a set of synthesis views for both scenarios above and compared to the two local filters, which verifies its effectiveness in removing noise and protecting edge profiles. The proposed method can be considered as a supplement to prefiltering methods of texture/depth videos in 3-D video systems.
C1 [Yang, Meng; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
   [Yang, Meng; Zheng, Nanning] Xi An Jiao Tong Univ, Natl Engn Lab Visual Informat Proc & Applicat, Xian 710049, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Yang, M (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.; Yang, M (corresponding author), Xi An Jiao Tong Univ, Natl Engn Lab Visual Informat Proc & Applicat, Xian 710049, Shaanxi, Peoples R China.
EM mengyang@xjtu.edu.cn; nnzheng@xjtu.edu.cn
FU NSFC key project [61627811]; national Key R&D Programm of China
   [2017YFB1302200]; NSFC [61503294]
FX This work was supported in part by the NSFC key project under Grant
   61627811, the national Key R&D Programm of China under Grant
   2017YFB1302200, and the NSFC project under Grant 61503294.
CR [Anonymous], 2008, WORKSH MULT MULT SEN
   [Anonymous], 2007, PROC IEEE C COMPUT V
   Caraffa L, 2015, IEEE T IMAGE PROCESS, V24, P1199, DOI 10.1109/TIP.2015.2389617
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Domanski M, 2009, ISO/IEC JTC1/SC29/WG11 MPEG/M17050
   Dong WS, 2017, IEEE T MULTIMEDIA, V19, P293, DOI 10.1109/TMM.2016.2613824
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Ham B, 2014, IEEE T IMAGE PROCESS, V23, P870, DOI 10.1109/TIP.2013.2295716
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang ZH, 2017, IEEE IMAGE PROC, P953, DOI 10.1109/ICIP.2017.8296422
   Jin J, 2016, IEEE T MULTIMEDIA, V18, P953, DOI 10.1109/TMM.2016.2539825
   Jung SW, 2013, IEEE T CIRC SYST VID, V23, P269, DOI 10.1109/TCSVT.2012.2203734
   Köppel M, 2013, IEEE IMAGE PROC, P1356, DOI 10.1109/ICIP.2013.6738279
   Lei JJ, 2017, IEEE T MULTIMEDIA, V19, P1442, DOI 10.1109/TMM.2017.2660440
   Lei JJ, 2015, IEEE T MULTIMEDIA, V17, P457, DOI 10.1109/TMM.2015.2400823
   Li K, 2017, IEEE INT CON MULTI, P1320, DOI 10.1109/ICME.2017.8019416
   LIU S, 2010, P SOC PHOTO-OPT INS, V7744
   Liu W, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2612826
   Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329
   Moving Picture Experts Group Geneva Switzerland, 2009, JTC1SC29WG11 ISOIEC
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Oh KJ, 2011, IEEE T CIRC SYST VID, V21, P350, DOI 10.1109/TCSVT.2011.2116590
   Schmeing M, 2015, IEEE T MULTIMEDIA, V17, P2160, DOI 10.1109/TMM.2015.2476372
   Shao F, 2016, IEEE T MULTIMEDIA, V18, P2104, DOI 10.1109/TMM.2016.2594142
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Sheng H, 2017, IEEE T IMAGE PROCESS, V26, P5758, DOI 10.1109/TIP.2017.2745100
   Smolic A, 2011, P IEEE, V99, P607, DOI 10.1109/JPROC.2010.2098350
   Storath M, 2018, IEEE T PATTERN ANAL, V40, P639, DOI 10.1109/TPAMI.2017.2692779
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Urey H, 2011, P IEEE, V99, P540, DOI 10.1109/JPROC.2010.2098351
   Wu N, 2006, IEEE GEOSCI REMOTE S, V3, P73, DOI 10.1109/LGRS.2005.856703
   Yan B, 2012, IEEE T MULTIMEDIA, V14, P936, DOI 10.1109/TMM.2012.2184743
   Yang D, 2018, IEEE SIGNAL PROC LET, V25, P55, DOI 10.1109/LSP.2017.2768660
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Yang M, 2017, ASIAPAC SIGN INFO PR, P228, DOI 10.1109/APSIPA.2017.8282032
   Yang M, 2017, IEEE T IMAGE PROCESS, V26, P5122, DOI 10.1109/TIP.2017.2723731
   Yuan H, 2016, IEEE T BROADCAST, V62, P134, DOI 10.1109/TBC.2015.2492461
   Yuan H, 2012, IEEE T BROADCAST, V58, P558, DOI 10.1109/TBC.2012.2187612
   Zhang B, 2008, IEEE T IMAGE PROCESS, V17, P664, DOI 10.1109/TIP.2008.919949
   Zhang W, 2015, IEEE T POWER SYST, V30, P35, DOI 10.1109/TPWRS.2014.2319315
   Zhang Y, 2014, IEEE T IMAGE PROCESS, V23, P4879, DOI 10.1109/TIP.2014.2355715
   Zhao Y, 2011, IEEE T IMAGE PROCESS, V20, P2221, DOI 10.1109/TIP.2011.2118218
   Zhu C., 2013, 3D-TV System with Depth-Image-Based Rendering - Architectures, Techniques and Challenges
   Zhu C, 2016, IEEE T BROADCAST, V62, P82, DOI 10.1109/TBC.2015.2475697
NR 44
TC 4
Z9 5
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 15
EP 28
DI 10.1109/TMM.2018.2849605
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700002
DA 2024-07-18
ER

PT J
AU Yao, YZ
   Shen, FM
   Zhang, J
   Liu, L
   Tang, ZM
   Shao, L
AF Yao, Yazhou
   Shen, Fumin
   Zhang, Jian
   Liu, Li
   Tang, Zhenmin
   Shao, Ling
TI Extracting Multiple Visual Senses for Web Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple visual senses; visual polysemy; polysemous words
AB Labeled image datasets have played a critical role in high-level image understanding. However, the process of manual labeling is both time consuming and labor intensive. To reduce the dependence on manually labeled data, there have been increasing research efforts on learning visual classifiers by directly exploiting web images. One issue that limits their performance is the problem of polysemy. Existing unsupervised approaches attempt to reduce the influence of visual polysemy by filtering out irrelevant images, but do not directly address polysemy. To this end, in this paper, we present a multimodal framework that solves the problem of polysemy by allowing sense-specific diversity in search results. Specifically, we first discover a list of possible semantic senses from untagged corpora to retrieve sense-specific images. Then, we merge visual similar semantic senses and prune noise by using the retrieved images. Finally, we train one visual classifier for each selected semantic sense and use the learned sense-specific classifiers to distinguish multiple visual senses. Extensive experiments on classifying images into sense-specific categories and reranking search results demonstrate the superiority of our proposed approach.
C1 [Yao, Yazhou; Zhang, Jian] Univ Technol Sydney, Global Big Data Technol Ctr, Ultimo, NSW 2007, Australia.
   [Shen, Fumin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Tang, Zhenmin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
   [Liu, Li; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi 999041, U Arab Emirates.
C3 University of Technology Sydney; University of Electronic Science &
   Technology of China; Nanjing University of Science & Technology
RP Zhang, J (corresponding author), Univ Technol Sydney, Global Big Data Technol Ctr, Ultimo, NSW 2007, Australia.
EM yazhou.yao@outlook.com; fumin.shen@gmail.com; jian.zhang@uts.edu.au;
   liuli1213@gmail.com; Tzm.cs@njust.edu.cn; ling.shao@ieee.org
RI Shao, Ling/D-3535-2011; liu, li/ADL-2178-2022; Tang,
   Zhenmin/AAY-6058-2020; ARSLAN, Okan/AAA-3232-2020
OI liu, li/0000-0002-6669-9382; Tang, Zhenmin/0000-0001-6708-2205; Zhang,
   Jian/0000-0002-7240-3541; Yao, Yazhou/0000-0002-0337-9410; Shao,
   Ling/0000-0002-8264-6117
FU National Natural Science Foundation of China [61473154]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61473154.
CR [Anonymous], 2002, P 8 ACM SIGKDD INT C, DOI DOI 10.1145/775047.775138
   [Anonymous], THESIS
   [Anonymous], 2013, People's Web Meets NLP
   [Anonymous], 1992, the 14th International Conference on Computational Linguistics
   Barnard K, 2005, ARTIF INTELL, V167, P13, DOI 10.1016/j.artint.2005.04.009
   Batra D, 2012, LECT NOTES COMPUT SC, V7576, P1, DOI 10.1007/978-3-642-33715-4_1
   Berg TamaraL., 2006, CVPR, DOI DOI 10.1109/CVPR.2006.57
   Boyd S., 2004, CONVEX OPTIMIZATION
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chatterjee N, 2008, LECT NOTES COMPUT SC, V4919, P299, DOI 10.1007/978-3-540-78135-6_25
   Chen XL, 2015, PROC CVPR IEEE, P5298, DOI 10.1109/CVPR.2015.7299167
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Collins B, 2008, LECT NOTES COMPUT SC, V5302, P86, DOI 10.1007/978-3-540-88682-2_8
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Divvala SK, 2014, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2014.412
   Fergus R, 2005, IEEE I CONF COMP VIS, P1816
   Fergus R, 2004, LECT NOTES COMPUT SC, V3021, P242
   Hua XS, 2015, AAAI CONF ARTIF INTE, P137
   Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078
   KELLEY JE, 1960, J SOC IND APPL MATH, V8, P703, DOI 10.1137/0108053
   KIWIEL KC, 1990, MATH PROGRAM, V46, P105, DOI 10.1007/BF01585731
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li LJ, 2010, INT J COMPUT VISION, V88, P147, DOI 10.1007/s11263-009-0265-6
   Li Y., 2009, P INT C ARTIFICIAL I, P344
   Lin Y., 2012, P ACL 2012 SYSTEM DE, P169, DOI [DOI 10.1007/978-1-4614-2311-9_8, 10.2307/j.ctv18pgr3b.13]
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Loeff N., 2006, P COLINGACL, P547
   Mansur A, 2008, LECT NOTES COMPUT SC, V5359, P851, DOI 10.1007/978-3-540-89646-3_84
   Michel JB, 2011, SCIENCE, V331, P176, DOI 10.1126/science.1199644
   Mihalcea R., 2007, P N AM ASS COMP LING, P196
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Niu L, 2017, IEEE T NEUR NET LEAR, V28, P1985, DOI 10.1109/TNNLS.2016.2557349
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Saenko Kate., 2009, Advances in Neural Information Processing Systems, P1393
   Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Snow R., 2007, P C EMPIRICAL METHOD, P1005
   Veronis J., 1990, Proceedings of the 13th conference on Computational linguistics, V2, P389
   Vijayanarasimhan S, 2014, INT J COMPUT VISION, V108, P97, DOI 10.1007/s11263-014-0721-9
   Wan K.-W., 2009, P BRIT MACH VIS C, P2
   Yao YZ, 2018, AAAI CONF ARTIF INTE, P523
   Yao YZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1085
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yao YZ, 2016, IEEE INT CON MULTI, DOI 10.1109/ICME.2016.7552988
   Yao YX, 2016, ADV SOC SCI EDUC HUM, V64, P212
   Yarowsky D., 1995, 33 ANN M ASS COMPUTA, P189, DOI DOI 10.3115/981658.981684
   Yu Chi Jen, 2011, THESIS
   Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958
NR 49
TC 27
Z9 28
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 184
EP 196
DI 10.1109/TMM.2018.2847248
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700016
DA 2024-07-18
ER

PT J
AU Dong, L
   He, L
   Mao, MD
   Kong, GP
   Wu, X
   Zhang, QN
   Cao, XC
   Izquierdo, E
AF Dong, Le
   He, Ling
   Mao, Mengdie
   Kong, Gaipeng
   Wu, Xi
   Zhang, Qianni
   Cao, Xiaochun
   Izquierdo, Ebroul
TI CUNet: A Compact Unsupervised Network For Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Unsupervised learning; convolutional network; image classification;
   K-means
AB In this paper, we propose a compact network called compact unsupervised network (CUNet) to address the image classification challenge. Contrasting the usual learning approach of convolutional neural networks, learning is achieved by the simple K-means on diverse image patches. This approach performs well even with scarcely labeled training images, greatly reducing the computational cost, while maintaining high discriminative power. Furthermore, we propose a new weighted pooling method in which different weighting values of adjacent neurons are considered. This strategy leads to improved classification since the network becomes more robust against small image distortions. In the output layer, CUNet integrates feature maps obtained in the last hidden layer, and straightforwardly computes histograms in nonoverlapped blocks. To reduce feature redundancy, we also implement the max-pooling operation on adjacent blocks to select the most competitive features. Comprehensive experiments on well-established databases are conducted to validate the classification performances of the introduced CUNet approach.
C1 [Dong, Le; He, Ling; Mao, Mengdie; Kong, Gaipeng] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Wu, Xi] Chengdu Univ Informat Technol, Sch Comp Sci, Chengdu 610225, Sichuan, Peoples R China.
   [Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, Beijing 100864, Peoples R China.
   [Zhang, Qianni; Izquierdo, Ebroul] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.
C3 University of Electronic Science & Technology of China; Chengdu
   University of Information Technology; Chinese Academy of Sciences;
   Institute of Information Engineering, CAS; University of London; Queen
   Mary University London
RP Dong, L (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
EM ledong@uestc.edu.cn; aleenheling@163.com; maomengdie0910@163.com;
   konggaipeng@163.com; xi.wu@cuit.edu.cn; qianni.zhang@qmul.ac.uk;
   caoxiaochun@iie.ac.cn; ebroul.izquierdo@qmul.ac.uk
OI Zhang, Qianni/0000-0001-7685-2187
FU National Natural Science Foundation of China [61370149, 61772114];
   Fundamental Research Funds for the Central Universities [ZYGX2016J077]; 
   [61650202];  [U1605252]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61370149 and 61772114; in part by the
   Fundamental Research Funds for the Central Universities under Grant
   ZYGX2016J077; and in part by the Funds under Grants 61650202 and
   U1605252. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Dong Xu.
CR [Anonymous], 2012, P 26 INT C NEUR INF
   [Anonymous], 2013, ICML
   [Anonymous], P INT C LEARN REPR
   [Anonymous], 2010, ADV NEUR INF PROC SY
   [Anonymous], COMPUT SCI
   [Anonymous], COMPUT SCI
   [Anonymous], P INT C LEARN REPR
   [Anonymous], 2014, P ICML 13
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2011, P 28 INT C MACH LEAR, DOI DOI 10.5555/3104482.3104528
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Getoor, 2011, P 28 INT C MACH LEAR, P833
   Ji RR, 2013, IEEE T MULTIMEDIA, V15, P153, DOI 10.1109/TMM.2012.2225035
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124
   Kavukcuoglu K., 2010, Advances in neural information processing systems, P1
   Krizhevsky A., 2014, CUDA Convnet
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee H., 2009, P 26 ANN INT C MACH, P609
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Liu S, 2012, IEEE T MULTIMEDIA, V14, P361, DOI 10.1109/TMM.2011.2174780
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ngiam J., 2010, ADV NEURAL INFORM PR, P1279, DOI DOI 10.1561/2200000006
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Qi GJ, 2010, IEEE T MULTIMEDIA, V12, P278, DOI 10.1109/TMM.2010.2046270
   Ren XF, 2013, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR.2013.417
   Sifre L, 2013, PROC CVPR IEEE, P1233, DOI 10.1109/CVPR.2013.163
   Sohn K., 2012, P 29 INT C INT C MAC, P1339
   Sohn Kihyuk., 2013, Proceedings of the 30th International Conference on Machine Learning, P217
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yang Y, 2010, IEEE T IMAGE PROCESS, V19, P2761, DOI 10.1109/TIP.2010.2049235
   Yu Kai, 2010, ICML, P1215
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zou W., 2012, ADV NEURAL INFORM PR, V25, P3203, DOI DOI 10.5555/2999325.2999492
NR 40
TC 14
Z9 15
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2012
EP 2021
DI 10.1109/TMM.2017.2788205
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600008
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Min, XK
   Gu, K
   Zhai, GT
   Liu, J
   Yang, XK
   Chen, CW
AF Min, Xiongkuo
   Gu, Ke
   Zhai, Guangtao
   Liu, Jing
   Yang, Xiaokang
   Chen, Chang Wen
TI Blind Quality Assessment Based on Pseudo-Reference Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind image quality assessment (BIQA); pseudo-reference image (PRI);
   blockiness; sharpness; noisiness
ID SHARPNESS ASSESSMENT; BLOCKING ARTIFACTS; NATURAL SCENE; INDEX;
   SIMILARITY; DEVIATION; EFFICIENT
AB Traditional full-reference image quality assessment (IQA) metrics generally predict the quality of the distorted image by measuring its deviation from a perfect quality image called reference image. When the reference image is not fully available, the reduced-reference and no-reference IQA metrics may still be able to derive some characteristics of the perfect quality images, and then measure the distorted image's deviation from these characteristics. In this paper, contrary to the conventional IQA metrics, we utilize a new "reference" called pseudo-reference image (PRI) and a PRI-based blind IQA (BIQA) framework. Different from a traditional reference image, which is assumed to have a perfect quality, PRI is generated from the distorted image and is assumed to suffer from the severest distortion for a given application. Based on the PRI-based BIQA framework, we develop distortion-specific metrics to estimate blockiness, sharpness, and noisiness. The PRI-based metrics calculate the similarity between the distorted image's and the PRI's structures. An image suffering from severer distortion has a higher degree of similarity with the corresponding PRI. Through a two-stage quality regression after a distortion identification framework, we then integrate the PRI-based distortion-specific metrics into a general-purpose BIQA method named blind PRI-based (BPRI) metric. The BPRI metric is opinion-unaware (OU) and almost training-free except for the distortion identification process. Comparative studies on five large IQA databases show that the proposed BPRI model is comparable to the state-of-the-art opinion-aware- and OU-BIQA models. Furthermore, BPRI not only performs well on natural scene images, but also is applicable to screen content images. The MATLAB source code of BPRI and other PRI-based distortion-specific metrics will be publicly available.
C1 [Min, Xiongkuo; Zhai, Guangtao; Yang, Xiaokang] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai Key Lab Digital Media Proc & Transmiss, Shanghai 200240, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
   [Liu, Jing] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Chen, Chang Wen] SUNY Buffalo, Buffalo, NY 14260 USA.
C3 Shanghai Jiao Tong University; Beijing University of Technology; Tianjin
   University; State University of New York (SUNY) System; State University
   of New York (SUNY) Buffalo
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai Key Lab Digital Media Proc & Transmiss, Shanghai 200240, Peoples R China.
EM minxiongkuo@gmail.com; guke.doctor@gmail.com; zhaiguangtao@sjtu.edu.cn;
   jliu_tju@tju.edu.cn; xkyang@sjtu.edu.cn; chencw@buffalo.edu
RI Yang, Xiaokang/C-6137-2009; Gu, Ke/AAJ-9684-2021; Liu-Zeng,
   Jing/F-8582-2011; Zhai, Guangtao/X-5949-2019; Min, Xiongkuo/A-7097-2019
OI Yang, Xiaokang/0000-0003-4029-3322; Zhai, Guangtao/0000-0001-8165-9322;
   Chen, Chang Wen/0000-0002-6720-234X; Min, Xiongkuo/0000-0001-5693-0416
FU National Natural Science Foundation of China [61422112, 61371146,
   61521062, 61527804]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61422112, 61371146, 61521062, and 61527804. Part of
   this work was presented at the 2016 IEEE International Conference on
   Multimedia and Expo [1]. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Wolfgang Hurst.
CR [Anonymous], 2006, 2006 IEEE COMP SOC C, DOI DOI 10.1109/CVPR.2006.207
   [Anonymous], 2014, 2014 IEEEIAS 50 IND
   [Anonymous], IEEE T VIS COMPUT GR
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Bovik AC, 2001, INT CONF ACOUST SPEE, P1725, DOI 10.1109/ICASSP.2001.941272
   Crete F, 2007, PROC SPIE, V6492, DOI 10.1117/12.702790
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Golestaneh S. Alireza, 2014, IEEE Signal Processing Letters, V21, P155, DOI 10.1109/LSP.2013.2296038
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li C, 2011, ELECTRON LETT, V47, P962, DOI 10.1049/el.2011.0921
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P918, DOI 10.1109/LSP.2014.2320743
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu H, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/263540
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Min XK, 2018, SIGNAL PROCESS, V145, P127, DOI 10.1016/j.sigpro.2017.10.025
   Min XK, 2017, INFORM SCIENCES, V420, P417, DOI 10.1016/j.ins.2017.08.040
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Min XK, 2016, IEEE INT CON MULTI
   Min XK, 2014, IEEE INT SYMP CIRC S, P894, DOI 10.1109/ISCAS.2014.6865280
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Nafchi HZ, 2016, IEEE ACCESS, V4, P5579, DOI 10.1109/ACCESS.2016.2604042
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh H.R., Live Image Quality Assessment Database
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Tang CW, 2015, IEEE T CIRC SYST VID, V25, P1283, DOI 10.1109/TCSVT.2014.2380196
   Vu CT, 2012, IEEE T IMAGE PROCESS, V21, P934, DOI 10.1109/TIP.2011.2169974
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE IMAGE PROC, P477
   Wang ZJ, 2011, IEEE SIGNAL PROC MAG, V28, P2, DOI 10.1109/MSP.2011.940297
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Wu JJ, 2015, IEEE T IMAGE PROCESS, V24, P4602, DOI 10.1109/TIP.2015.2460467
   Wu QB, 2015, IEEE IMAGE PROC, P339, DOI 10.1109/ICIP.2015.7350816
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhai GT, 2015, APSIPA TRANS SIGNAL, V4, DOI 10.1017/ATSIP.2015.8
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhan YB, 2017, IEEE SIGNAL PROC LET, V24, P760, DOI 10.1109/LSP.2017.2688371
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zoran D, 2009, IEEE I CONF COMP VIS, P2209, DOI 10.1109/ICCV.2009.5459476
NR 59
TC 217
Z9 222
U1 6
U2 64
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2049
EP 2062
DI 10.1109/TMM.2017.2788206
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600011
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Hou, JY
   Wu, XX
   Sun, YC
   Jia, YD
AF Hou, Jingyi
   Wu, Xinxiao
   Sun, Yuchao
   Jia, Yunde
TI Content-Attention Representation by Factorized Action-Scene Network for
   Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep neural network; multi-label action recognition; complex event
   detection
AB During action recognition in videos, irrelevant motions in the background can greatly degrade the performance of recognizing specific actions with which we actually concern ourself here. In this paper, a novel deep neural network, called factorized action-scene network (FASNet), is proposed to encode and fuse the most relevant and informative semantic cues for action recognition. Specifically, we decompose the FASNet into two components. One is a newly designed encoding network, named content attention network (CANet), which encodes local spatial-temporal features to learn the action representations with good robustness to the noise of irrelevant motions. The other is a fusion network, which integrates the pretrained CANet to fuse the encoded spatial-temporal features with contextual scene feature extracted from the same video, for learning more descriptive and discriminative action representations. Moreover, different from the existing deep learning based tasks for generic action recognition, which applies softmax loss function as the training guidance, we formulate two loss functions for guiding the proposed model to accomplish more specific action recognition tasks, i.e., the multilabel correlation loss for multilabel action recognition and the triplet loss for complex event detection. Extensive experiments on the Hollywood2 dataset and the TRECVID MEDTest 14 dataset show that our method achieves superior performance compared with the state-of-the-art methods.
C1 [Hou, Jingyi; Wu, Xinxiao; Sun, Yuchao; Jia, Yunde] Beijing Inst Technol, Beijing Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Wu, XX (corresponding author), Beijing Inst Technol, Beijing Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
EM houjingyi@bit.edu.cn; wuxinxiao@bit.edu.cn; sunyuchao@bit.edu.cn;
   jiayunde@bit.edu.cn
OI Wu, Xinxiao/0000-0002-2056-6947
FU Natural Science Foundation of China [61673062, 61472038]
FX This work was supported by the Natural Science Foundation of China under
   Grants 61673062 and 61472038.
CR [Anonymous], ARXIV12120402
   [Anonymous], 2012, ARXIV12115590
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong Z, 2016, AAAI CONF ARTIF INTE, P3471
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fernando B, 2016, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR.2016.212
   Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919
   Graves Alex, 2014, Generating sequences with recurrent neural networks
   Han D, 2009, IEEE I CONF COMP VIS, P1933, DOI 10.1109/ICCV.2009.5459427
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ikizler-Cinbis N, 2010, LECT NOTES COMPUT SC, V6311, P494, DOI 10.1007/978-3-642-15549-9_36
   Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Liu CW, 2016, IEEE T CYBERNETICS, V46, P2596, DOI 10.1109/TCYB.2015.2482970
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Ma SG, 2013, IEEE I CONF COMP VIS, P2744, DOI 10.1109/ICCV.2013.341
   Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557
   Moore D. J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P80, DOI 10.1109/ICCV.1999.791201
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Raptis M, 2012, PROC CVPR IEEE, P1242, DOI 10.1109/CVPR.2012.6247807
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Samanta S, 2014, IEEE T MULTIMEDIA, V16, P1525, DOI 10.1109/TMM.2014.2326734
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sharma S, 2016, ACTION RECOGNITION U
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun J, 2009, PROC CVPR IEEE, P2004, DOI 10.1109/CVPRW.2009.5206721
   Sun L, 2014, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2014.336
   Varma M., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P1065, DOI DOI 10.1145/1553374.1553510
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang K., ARXIV160706215
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Zha Shengxin., 2015, BMVC
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
NR 54
TC 40
Z9 42
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1537
EP 1547
DI 10.1109/TMM.2017.2771462
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400020
DA 2024-07-18
ER

PT J
AU Long, CC
   Cao, Y
   Jiang, T
   Zhang, Q
AF Long, Changchun
   Cao, Yang
   Jiang, Tao
   Zhang, Qian
TI Edge Computing Framework for Cooperative Video Processing in Multimedia
   IoT Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Edge computing; multimedia IoT; cooperative video processing; human
   detection accuracy
AB Multimedia Internet-of-Things (IoT) systems have been widely used in surveillance, automatic behavior analysis and event recognition, which integrate image processing, computer vision, and networking capabilities. In conventional multimedia IoT systems, videos captured by surveillance cameras are required to be delivered to remote IoT servers for video analysis. However, the long-distance transmission of a large volume of video chunks may cause congestions and delays due to limited network bandwidth. Nowadays, mobile devices, e.g., smart phones and tablets, are resource-abundant in computation and communication capabilities. Thus, these devices have the potential to extract features from videos for the remote IoT servers. By sending back only a few video features to the remote servers, the bandwidth starvation of delivering original video chunks can be avoided. In this paper, we propose an edge computing framework to enable cooperative processing on resource-abundant mobile devices for delay-sensitive multimedia IoT tasks. We identify that the key challenges in the proposed edge computing framework are to optimally form mobile devices into video processing groups and to dispatch video chunks to proper video processing groups. Based on the derived optimal matching theorem, we put forward a cooperative video processing scheme formed by two efficient algorithms to tackle above challenges, which achieves suboptimal performance on the human detection accuracy. The proposed scheme has been evaluated under diverse parameter settings. Extensive simulation confirms the superiority of the proposed scheme over other two baseline schemes.
C1 [Long, Changchun; Cao, Yang; Jiang, Tao] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
   [Zhang, Qian] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
C3 Huazhong University of Science & Technology; Hong Kong University of
   Science & Technology
RP Cao, Y (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
EM ChangchunLong@hust.edu.cn; ycao@hust.edu.cn; tao.jiang@ieee.org;
   qianzh@cse.ust.hk
RI Zhang, Qian/B-9058-2009; Jiang, Tao/IWM-7503-2023; Cao,
   Yang/HGD-6463-2022; jiang, tao/GWC-7108-2022
OI Zhang, Qian/0000-0001-9205-1881; 
FU National Science Foundation of China [61601193, 61729101, 61631015,
   61720106001]; Major Program of National Natural Science Foundation of
   Hubei in China [2016CFA009]; Independent Innovation Research Fund of
   Huazhong University of Science and Technology [2016YXMS305]
FX This work was supported in part by the National Science Foundation of
   China under Grant 61601193, Grant 61729101, Grant 61631015, and Grant
   61720106001, in part by the Major Program of National Natural Science
   Foundation of Hubei in China under Grant 2016CFA009, and in part the
   Independent Innovation Research Fund of Huazhong University of Science
   and Technology under Grant 2016YXMS305.
CR [Anonymous], CLUSTER COMPUTING
   [Anonymous], 2013, P 18 IEEE INT C DIG
   Atzori L, 2010, COMPUT NETW, V54, P2787, DOI 10.1016/j.comnet.2010.05.010
   Bharadwaj V., 2003, Cluster Computing, V6, P7, DOI 10.1023/A:1020958815308
   Cao Y, 2016, IEEE INTERNET THINGS, V3, P1246, DOI 10.1109/JIOT.2016.2582540
   Cao Y, 2016, IEEE WIREL COMMUN, V23, P52, DOI 10.1109/MWC.2016.7553026
   Cao Y, 2016, IEEE T MOBILE COMPUT, V15, P1528, DOI 10.1109/TMC.2015.2461214
   Chen X, 2017, IEEE T CIRC SYST VID, V27, P19, DOI 10.1109/TCSVT.2016.2539758
   Eriksson E, 2016, IEEE T MOBILE COMPUT, V15, P1743, DOI 10.1109/TMC.2015.2465390
   Escudero LF, 2009, DECIS SUPPORT SYST, V46, P649, DOI 10.1016/j.dss.2008.10.009
   Floris A, 2015, IEEE INT CONF COMM, P1747, DOI 10.1109/ICCW.2015.7247433
   Lopez PG, 2015, ACM SIGCOMM COMP COM, V45, P37, DOI 10.1145/2831347.2831354
   Halpern M, 2016, INT S HIGH PERF COMP, P64, DOI 10.1109/HPCA.2016.7446054
   Kyong Y, 2012, IEEE T AERO ELEC SYS, V48, P3027, DOI 10.1109/TAES.2012.6324675
   Lehmann B, 2006, GAME ECON BEHAV, V55, P270, DOI 10.1016/j.geb.2005.02.006
   Li XL, 2010, CLUSTER COMPUT, V13, P31, DOI 10.1007/s10586-009-0103-1
   Lin X, 2007, RTAS 2007: 13TH REAL-TIME AND EMBEDDED TECHNOLOGY AND APPLICATIONS SYMPOSIUM, PROCEEDINGS, P303
   Mamat Anwar, 2010, Proceedings of the 16th IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS 2010), P323, DOI 10.1109/RTAS.2010.29
   Sheng ZG, 2018, IEEE T CLOUD COMPUT, V6, P114, DOI 10.1109/TCC.2015.2458272
   Varghese B, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SMART CLOUD (SMARTCLOUD), P20, DOI 10.1109/SmartCloud.2016.18
   Zhang XS, 2015, IEEE T MULTIMEDIA, V17, P1562, DOI 10.1109/TMM.2015.2449660
NR 21
TC 153
Z9 166
U1 2
U2 56
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1126
EP 1139
DI 10.1109/TMM.2017.2764330
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400009
DA 2024-07-18
ER

PT J
AU Li, JN
   Liang, XD
   Shen, SM
   Xu, TF
   Feng, JS
   Yan, SC
AF Li, Jianan
   Liang, Xiaodan
   Shen, Shengmei
   Xu, Tingfa
   Feng, Jiashi
   Yan, Shuicheng
TI Scale-Aware Fast R-CNN for Pedestrian Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pedestrian detection; scale-aware; deep learning
AB In this paper, we consider the problem of pedestrian detection in natural scenes. Intuitively, instances of pedestrians with different spatial scales may exhibit dramatically different features. Thus, large variance in instance scales, which results in undesirable large intracategory variance in features, may severely hurt the performance of modern object instance detection methods. We argue that this issue can be substantially alleviated by the divide-and-conquer philosophy. Taking pedestrian detection as an example, we illustrate how we can leverage this philosophy to develop a Scale-Aware Fast R-CNN (SAF R-CNN) framework. The model introduces multiple built-in subnetworks which detect pedestrians with scales from disjoint ranges. Outputs from all of the subnetworks are then adaptively combined to generate the final detection results that are shown to be robust to large variance in instance scales, via a gate function defined over the sizes of object proposals. Extensive evaluations on several challenging pedestrian detection datasets well demonstrate the effectiveness of the proposed SAF R-CNN. Particularly, our method achieves state-of-the-art performance on Caltech, and obtains competitive results on INRIA, ETH, and KITTI.
C1 [Li, Jianan; Xu, Tingfa] Beijing Inst Technol, Sch Opt Engn, Beijing 100081, Peoples R China.
   [Liang, Xiaodan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Shen, Shengmei] Panasonic R&D Ctr Singapore, Singapore 469332, Singapore.
   [Feng, Jiashi; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
C3 Beijing Institute of Technology; Carnegie Mellon University; Panasonic;
   National University of Singapore
RP Xu, TF (corresponding author), Beijing Inst Technol, Sch Opt Engn, Beijing 100081, Peoples R China.
EM 20090964@bit.edu.cn; xdliang328@gmail.com;
   shengmei.shen@sg.panasonic.com; 15210538723@163.com; jshfeng@gmail.com;
   eleyans@nus.odu.sg
RI Feng, Jiashi/AGX-6209-2022; Yan, Shuicheng/HCI-1431-2022
FU China Scholarship Council [201506030045]
FX This work was supported in part by China Scholarship Council under Grant
   201506030045. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xavier Giro-i-Nieto.
CR [Anonymous], IEEE T CYBE IN PRESS
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.141
   [Anonymous], 2009, BMVC
   [Anonymous], 2016, ARXIV160704441
   [Anonymous], 2011, 2011 17 KOR JAP JOIN
   [Anonymous], 2014, 2 INT C LEARN REPR I
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P 14 INT C COMP VAN
   [Anonymous], PROC CVPR IEEE
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2015, LECT NOTES BUS INF P
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Benenson R, 2015, LECT NOTES COMPUT SC, V8926, P613, DOI 10.1007/978-3-319-16181-5_47
   Benenson R, 2012, PROC CVPR IEEE, P2903, DOI 10.1109/CVPR.2012.6248017
   Cai ZW, 2015, IEEE I CONF COMP VIS, P3361, DOI 10.1109/ICCV.2015.384
   Campmany V, 2016, PROCEDIA COMPUT SCI, V80, P2377, DOI 10.1016/j.procs.2016.05.455
   Cao JL, 2016, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2016.147
   Chen G, 2013, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2013.235
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Ess A., 2007, PROCINT C COMPUT VIS, P1
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   González A, 2015, IEEE INT VEH SYM, P356, DOI 10.1109/IVS.2015.7225711
   Hariharan B, 2012, LECT NOTES COMPUT SC, V7575, P459, DOI 10.1007/978-3-642-33765-9_33
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hosang J, 2015, PROC CVPR IEEE, P4073, DOI 10.1109/CVPR.2015.7299034
   Huang C, 2016, IEEE T MULTIMEDIA, V18, P2372, DOI 10.1109/TMM.2016.2602060
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Marín J, 2013, IEEE I CONF COMP VIS, P2592, DOI 10.1109/ICCV.2013.322
   Marín J, 2014, IEEE T CYBERNETICS, V44, P342, DOI 10.1109/TCYB.2013.2255271
   Nam W., 2014, P 27 INT C NEURAL IN, V27
   Ouyang WL, 2016, INT J COMPUT VISION, V120, P14, DOI 10.1007/s11263-016-0890-9
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Ouyang WL, 2012, PROC CVPR IEEE, P3258, DOI 10.1109/CVPR.2012.6248062
   Paisitkriangkrai S, 2014, LECT NOTES COMPUT SC, V8692, P546, DOI 10.1007/978-3-319-10593-2_36
   Park D, 2010, LECT NOTES COMPUT SC, V6314, P241, DOI 10.1007/978-3-642-15561-1_18
   Ranzato MA., 2007, CVPR, DOI [10.1109/cvpr.2007.383157, 10.1109/CVPR.2007.383157]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Simonyan K., 2014, Very Deep Convolutional Networks for Large-Scale Image Recognition
   Tang JH, 2015, IEEE T MULTIMEDIA, V17, P1899, DOI 10.1109/TMM.2015.2476660
   Tian YL, 2015, IEEE I CONF COMP VIS, P1904, DOI 10.1109/ICCV.2015.221
   Tian YL, 2015, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2015.7299143
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Viola P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P734
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655
   Wang XG, 2014, IEEE T PATTERN ANAL, V36, P361, DOI 10.1109/TPAMI.2013.124
   Wang XY, 2013, IEEE I CONF COMP VIS, P17, DOI 10.1109/ICCV.2013.10
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Xu Yichong, 2014, ARXIV14116369
   Yan JJ, 2013, PROC CVPR IEEE, P3033, DOI 10.1109/CVPR.2013.390
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhang SS, 2015, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2015.7298784
   Zhang SS, 2014, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2014.126
   Zhu YK, 2014, IEEE T MULTIMEDIA, V16, P1585, DOI 10.1109/TMM.2014.2321534
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 64
TC 505
Z9 564
U1 17
U2 255
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 985
EP 996
DI 10.1109/TMM.2017.2759508
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000017
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Fujihashi, T
   Koike-Akino, T
   Watanabe, T
   Orlik, PV
AF Fujihashi, Takuya
   Koike-Akino, Toshiaki
   Watanabe, Takashi
   Orlik, Philip V.
TI High-Quality Soft Video Delivery With GMRF-Based Overhead Reduction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaussian Markov random field (GMRF); overhead reduction; soft video
   delivery
AB Soft video delivery, i.e., analog video transmission, has been proposed to provide high video quality in unstable wireless channels. However, existing analog schemes need to transmit a significant amount of metadata to a receiver for power allocation and decoding operations causing large overhead and quality degradation due to rate and power losses. To reduce the overhead while keeping the video quality high, we propose a new analog transmission scheme. Our scheme exploits a Gaussian Markov random field for modeling video sequences to significantly reduce the required amount of metadata, which are obtained by fitting into the Lorentzian function. Our scheme achieves not only reduced overhead but also improved video quality, by using the fitting function and parameters for metadata. Evaluations using several test video sequences demonstrate that the proposed scheme reduces overhead by 99.7% with 1.2-dB improvement of video quality (in terms of peak signal-to-noise ratio) compared to the existing analog video transmission scheme. We also investigate the impact of bandwidth limitation, showing a significant gain up to 2.7 dB for narrow-band systems.
C1 [Fujihashi, Takuya] Ehime Univ, Grad Sch Sci & Engn, Matsuyama, Ehime 7908577, Japan.
   [Koike-Akino, Toshiaki; Orlik, Philip V.] Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.
   [Watanabe, Takashi] Osaka Univ, Grad Sch Informat Sci & Technol, Suita, Osaka 5650871, Japan.
C3 Ehime University; Osaka University
RP Fujihashi, T (corresponding author), Ehime Univ, Grad Sch Sci & Engn, Matsuyama, Ehime 7908577, Japan.
EM fujihashi@cs.ehime-u.ac.jp; koike@merl.com; watanabe@ist.osaka-u.ac.jp;
   porlik@merl.com
RI Koike-Akino, Toshiaki/T-2062-2019; Fujihashi, Takuya/Y-1527-2019
OI Koike-Akino, Toshiaki/0000-0002-2578-5372; Fujihashi,
   Takuya/0000-0002-6960-0122; WATANABE, TAKASHI/0000-0002-3227-9048
FU JSPS [15K12018, 14J09461, 17K12672]; Grants-in-Aid for Scientific
   Research [15K12018, 17K12672, 14J09461] Funding Source: KAKEN
FX This work was partly supported by JSPS KAKENHI under Grant 15K12018,
   Grant 14J09461, and Grant 17K12672. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Jun Wu. (Corresponding author: Takuya Fujihashi.)
CR Almowuena S, 2016, IEEE T MULTIMEDIA, V18, P102, DOI 10.1109/TMM.2015.2502067
   [Anonymous], 2016, VISUAL NETWORKING IN
   [Anonymous], P INT C MOB MULT COM
   [Anonymous], ACM HOTNETS
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Cui H, 2016, IEEE T CIRC SYST VID, V26, P992, DOI 10.1109/TCSVT.2015.2430651
   Cui H, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2338279
   Cuiling Lan, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457915
   Fan XP, 2015, IEEE T CIRC SYST VID, V25, P1801, DOI 10.1109/TCSVT.2015.2402831
   Fan Xiaopeng, 2011, P INT C MOB UB MULT, P226
   Fujihashi T., 2015, IEEE GLOBAL COMMUNIC, P1
   Fujihashi T, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511548
   Gao W, 2016, IEEE T CIRC SYST VID, V26, P139, DOI 10.1109/TCSVT.2015.2444671
   Ghandi MM, 2006, J VIS COMMUN IMAGE R, V17, P451, DOI 10.1016/j.jvcir.2005.05.005
   Grois D, 2013, PICT COD SYMP, P394, DOI 10.1109/PCS.2013.6737766
   Guo ZL, 2015, IEEE T MULTIMEDIA, V17, P1335, DOI 10.1109/TMM.2015.2438718
   Hu SD, 2012, IEEE T IND ELECTRON, V59, P1673, DOI 10.1109/TIE.2011.2157282
   Huang Y, 2008, ACM SIGCOMM COMP COM, V38, P375, DOI 10.1145/1402946.1403001
   Jakubczak S., 2011, PROC MOBICOM, P289
   Li CL, 2015, IEEE T CIRC SYST VID, V25, P1665, DOI 10.1109/TCSVT.2015.2397232
   Li MD, 2013, IEEE T MULTIMEDIA, V15, P1519, DOI 10.1109/TMM.2013.2267207
   Li X, 2009, IEEE T CIRC SYST VID, V19, P193, DOI 10.1109/TCSVT.2008.2009255
   Liu XL, 2014, IEEE T MULTIMEDIA, V16, P2038, DOI 10.1109/TMM.2014.2331616
   Liu XL, 2014, SIGNAL PROCESS-IMAGE, V29, P361, DOI 10.1016/j.image.2014.01.005
   Persson D, 2009, IEEE T IMAGE PROCESS, V18, P1048, DOI 10.1109/TIP.2009.2014261
   Rombaut J, 2009, IEEE T IMAGE PROCESS, V18, P783, DOI 10.1109/TIP.2008.2011388
   Rue Havard, 2005, Gaussian Markov Random Fields: Theory and Applications, DOI DOI 10.1201/9780203492024
   Stockhammer T, 2004, IEEE T MULTIMEDIA, V6, P268, DOI 10.1109/TMM.2003.822795
   Wang AH, 2014, SIGNAL PROCESS-IMAGE, V29, P599, DOI 10.1016/j.image.2014.03.002
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu J, 2016, IEEE T MULTIMEDIA, V18, P893, DOI 10.1109/TMM.2016.2535727
   Yu L, 2014, IEEE T CIRC SYST VID, V24, P331, DOI 10.1109/TCSVT.2013.2273675
   Zhang C, 2013, IEEE SIGNAL PROC LET, V20, P106, DOI 10.1109/LSP.2012.2230165
NR 34
TC 23
Z9 23
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 473
EP 483
DI 10.1109/TMM.2017.2743984
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200017
DA 2024-07-18
ER

PT J
AU Zhang, L
   Ma, BP
   Li, GR
   Huang, QM
   Tian, Q
AF Zhang, Liang
   Ma, Bingpeng
   Li, Guorong
   Huang, Qingming
   Tian, Qi
TI Generalized Semi-supervised and Structured Subspace Learning for
   Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; documents and images; semi-supervised learning
ID DIMENSIONALITY; IMAGES
AB Motivated by the fact that unlabeled data can be easily collected and help to exploit the correlations among different modalities, this paper proposes a novel method named generalized semi-supervised structured subspace learning (GSS-SL) for the task of cross-modal retrieval. First, to predict more relevant class labels for unlabeled data, we propose a label graph constraint that ensures the intrinsic geometric structures of different feature spaces consistent with that of label space. Second, considering that class labels directly reveal the semantic information of multimedia data, GSS-SL takes the label space as a linkage to model the correlations among different modalities. Concretely, the label graph constraint, label-linked loss function, and regularization are integrated into a joint minimization formulation to learn a discriminative common subspace. Finally, an efficient optimization algorithm is designed to alternately optimize multiple linear transformations for different modalities and update the class indicator matrices for unlabeled data. Furthermore, an arbitrary number of modalities can be solved in the proposed framework. Extensive experiments on three standard benchmark datasets demonstrate that GSS-SL outperforms previous methods on exploiting the correlations among different modalities.
C1 [Zhang, Liang; Ma, Bingpeng; Li, Guorong; Huang, Qingming] Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 100190, Peoples R China.
   [Zhang, Liang; Ma, Bingpeng; Li, Guorong; Huang, Qingming] Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing 100190, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; University of Texas System; University
   of Texas at San Antonio (UTSA)
RP Ma, BP (corresponding author), Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 100190, Peoples R China.
EM zhangliang14@mails.ucas.ac.cn; bpma@ucas.ac.cn; liguorong@ucas.ac.cn;
   qmhuang@ucas.ac.cn; qitian@cs.utsa.edu
RI Li, Guorong/AAG-1594-2020
FU National Natural Science Foundation of China [61332016, 61620106009,
   61572465, 61429201, U1636214, 61650202, 61303153]; National Basic
   Research Program of China (973 Program) [2015CB351800]; Research Program
   of Frontier Sciences, CAS [QYZDJ-SSW-SYS013]; ARO Grant
   [W911NF-15-1-0290]; NEC Laboratories of America; Blippar
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61332016, Grant 61620106009, Grant
   61572465, Grant 61429201, Grant U1636214, Grant 61650202, and Grant
   61303153, in part by the National Basic Research Program of China (973
   Program) under Grant 2015CB351800, and in part by the Key Research
   Program of Frontier Sciences, CAS under Grant QYZDJ-SSW-SYS013. The work
   of Q. Tian was supported in part by the ARO Grant W911NF-15-1-0290 and
   in part by the Faculty Research Gift Awards by the NEC Laboratories of
   America and Blippar. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Marco Bertini.
CR [Anonymous], 2013, P INT C MACH LEARN
   [Anonymous], P ADV NEUR INF PROC
   [Anonymous], P INT C MULT RETR
   [Anonymous], 2011, Advances in Neural Information Processing Systems
   [Anonymous], 2004, P ADV NEUR INF PROC
   [Anonymous], P INT C MACH LEAR
   [Anonymous], 2009, P ACM INT C IM VID R
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cai X, 2013, IEEE I CONF COMP VIS, P1737, DOI 10.1109/ICCV.2013.218
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Hwang SJ, 2012, INT J COMPUT VISION, V100, P134, DOI 10.1007/s11263-011-0494-3
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Katsurai M, 2014, IEEE T MULTIMEDIA, V16, P1059, DOI 10.1109/TMM.2014.2306655
   Li ZC, 2015, IEEE T PATTERN ANAL, V37, P2085, DOI 10.1109/TPAMI.2015.2400461
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Nie F., 2010, ADV NEURAL INFORM PR, P1813
   Peng YX, 2016, IEEE T CIRC SYST VID, V26, P583, DOI 10.1109/TCSVT.2015.2400779
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Rasiwasia N, 2014, JMLR WORKSH CONF PRO, V33, P823
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wu F., 2013, P ACM INT C MULT, P877
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Zhai X., 2013, PROC 27 AAAI C ARTIF, P1198
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhang L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1355, DOI 10.1145/2964284.2964336
NR 38
TC 103
Z9 110
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 128
EP 141
DI 10.1109/TMM.2017.2723841
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700011
DA 2024-07-18
ER

PT J
AU Karimi, M
   Nejati, M
   Soroushmehr, SMR
   Samavi, S
   Karimi, N
   Najarian, K
AF Karimi, Maryam
   Nejati, Mansour
   Soroushmehr, S. M. Reza
   Samavi, Shadrokh
   Karimi, Nader
   Najarian, Kayvan
TI Blind Stereo Quality Assessment Based on Learned Features From Binocular
   Combined Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D perception; binocular combination; no-reference (NR) image quality
   assessment (IQA); sparse representation; stereo image quality assessment
   (SIQA); unsupervised feature learning
ID COMBINATION; MODELS
AB Quality assessment of stereo images confronts more challenges than its 2D counterparts. Direct use of 2D assessment methods is not sufficient to deal with the challenges of 3D perception. In this paper, an efficient general-purpose no-reference stereo image quality assessment, based on unsupervised feature learning, is presented. The proposed method extracts features without any prior knowledge about the types and levels of distortions. This property enables our method to be adaptable for different applications. The perceived contrast and phase of the binocular combination of original stereo images are utilized to learn individual dictionaries. For each distorted stereo image, two feature vectors are pooled, in a hierarchical manner, over all sparse representation vectors of phase and contrast blocks by their corresponding dictionaries. Performance results of learning a regression model by the features acknowledge the superiority of the proposed method to state-of-the-art algorithms.
C1 [Karimi, Maryam; Nejati, Mansour; Samavi, Shadrokh; Karimi, Nader] Isfahan Univ Technol, Dept Elect & Comp Engn, Esfahan 8415683111, Iran.
   [Soroushmehr, S. M. Reza] Univ Michigan, Michigan Ctr Integrat Res Crit Care, Ann Arbor, MI 48109 USA.
   [Samavi, Shadrokh] McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4L8, Canada.
   [Najarian, Kayvan] Univ Michigan, Michigan Ctr Integrat Res Crit Care, Dept Computat Med & Bioinformat, Ann Arbor, MI 48109 USA.
   [Najarian, Kayvan] Univ Michigan, Dept Emergency Med, Ann Arbor, MI 48109 USA.
C3 Isfahan University of Technology; University of Michigan System;
   University of Michigan; McMaster University; University of Michigan
   System; University of Michigan; University of Michigan System;
   University of Michigan
RP Soroushmehr, SMR (corresponding author), Univ Michigan, Michigan Ctr Integrat Res Crit Care, Ann Arbor, MI 48109 USA.
EM Maryam.karimi@ec.iut.ac.ir; mansour.nejati@ec.iut.ac.ir;
   ssoroush@umich.edu; samavi@mcmaster.ca; nader.karimi@cc.iut.ac.ir;
   kayvan@umich.edu
RI karimi, maryam/AAZ-9303-2021; nejati, mansour/HJP-0888-2023; Karimi,
   Nader/HWP-4206-2023
OI Karimi, Nader/0000-0001-8904-1607; Soroushmehr,
   S.M.Reza/0000-0001-8417-9260; karimi, maryam/0000-0002-7597-0680
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], P SPIE
   Appina B, 2016, SIGNAL PROCESS-IMAGE, V43, P1, DOI 10.1016/j.image.2016.02.001
   Benoit A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/659024
   Bensalma R, 2010, IEEE IMAGE PROC, P4037, DOI 10.1109/ICIP.2010.5649390
   Campisi Patrizio, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P2110
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   Ding J, 2013, J VISION, V13, DOI 10.1167/13.2.13
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Engan K, 1999, INT CONF ACOUST SPEE, P2443, DOI 10.1109/ICASSP.1999.760624
   Fleet DJ, 1996, VISION RES, V36, P1839, DOI 10.1016/0042-6989(95)00313-4
   Hachicha W, 2013, IEEE IMAGE PROC, P113, DOI 10.1109/ICIP.2013.6738024
   Hewage C.T. E. R., 2010, Future Network MobileSummit 2010 Conference Proceedings, P1
   Hewage CTER, 2009, IEEE J-STSP, V3, P304, DOI 10.1109/JSTSP.2009.2014805
   Jin L., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2521, DOI 10.1109/ICIP.2011.6116175
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lin YH, 2014, IEEE T IMAGE PROCESS, V23, P1527, DOI 10.1109/TIP.2014.2302686
   Maalouf A, 2011, INT CONF ACOUST SPEE, P1161
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2013, SIGNAL PROCESS-IMAGE, V28, P870, DOI 10.1016/j.image.2012.08.004
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ren XF, 2013, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR.2013.417
   Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551
   Ryu S, 2014, IEEE T CIRC SYST VID, V24, P591, DOI 10.1109/TCSVT.2013.2279971
   Ryu S, 2012, IEEE IMAGE PROC, P609, DOI 10.1109/ICIP.2012.6466933
   Saad MA, 2010, IEEE SIGNAL PROC LET, V17, P583, DOI 10.1109/LSP.2010.2045550
   Sazzad Z. M. Parvez, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P106, DOI 10.1109/QOMEX.2010.5517772
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Shao F, 2015, IEEE T IMAGE PROCESS, V24, P2971, DOI 10.1109/TIP.2015.2436332
   Shao F, 2015, IEEE SIGNAL PROC LET, V22, P1548, DOI 10.1109/LSP.2015.2413946
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Skretting K, 2010, IEEE T SIGNAL PROCES, V58, P2121, DOI 10.1109/TSP.2010.2040671
   Solh M., 2011, IEEE International Conference on Multimedia and Expo (ICME), P1
   Su CC, 2015, IEEE T IMAGE PROCESS, V24, P1685, DOI 10.1109/TIP.2015.2409558
   Wang JH, 2015, IEEE T IMAGE PROCESS, V24, P3400, DOI 10.1109/TIP.2015.2446942
   Wang X, 2015, NEUROCOMPUTING, V151, P683, DOI 10.1016/j.neucom.2014.05.090
   Wang Z, 2005, PROC SPIE, V5666, P149, DOI 10.1117/12.597306
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Yang JC, 2015, J VIS COMMUN IMAGE R, V31, P138, DOI 10.1016/j.jvcir.2015.06.002
   Yang JC, 2010, INT J IMAG SYST TECH, V20, P301, DOI 10.1002/ima.20246
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhou WJ, 2016, IEEE T MULTIMEDIA, V18, P1077, DOI 10.1109/TMM.2016.2542580
NR 48
TC 15
Z9 17
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2475
EP 2489
DI 10.1109/TMM.2017.2699082
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200010
DA 2024-07-18
ER

PT J
AU Ma, C
   Miao, ZJ
   Zhang, XP
   Li, M
AF Ma, Cong
   Miao, Zhenjiang
   Zhang, Xiao-Ping
   Li, Min
TI A Saliency Prior Context Model for Real-Time Object Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Context model; fast Fourier transforms; object tracking; saliency
ID VISUAL TRACKING
AB Real-time object tracking has wide applications in time-critical multimedia processing areas such as motion analysis and human-computer interaction. It remains a hard problem to balance between accuracy and speed. In this paper, we present a fast real-time context-based visual tracking algorithm with a new saliency prior context (SPC) model. Based on the probability formulation, the tracking problem is solved by sequentially maximizing the computed confidence map of target location in each video frame. To handle the various cases of feature distributions generated from different targets and their contexts, we exploit low-level features as well as fast spectral analysis for saliency to build a new prior context model. Then, based on this model and a spatial context model learned online, a confidence map is computed and the target location is estimated. In addition, under this framework, the tracking procedure can be accelerated by the fast Fourier transform. Therefore, the new method generally achieves a real-time running speed. Extensive experiments show that our tracking algorithm based on the proposed SPC model achieves real-time computation efficiency with overall best performance comparing with other state-of-the-art methods.
C1 [Ma, Cong; Miao, Zhenjiang; Li, Min] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Zhang, Xiao-Ping] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 Beijing Jiaotong University; Toronto Metropolitan University
RP Ma, C (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM 13112063@bjtu.edu.cn; zjmiao@bjtu.edu.cn; xzhang@ee.ryerson.ca;
   16112066@bjtu.edu.cn
RI Zhang, Xiao-Ping (Steven)/B-1436-2016
OI Zhang, Xiao-Ping (Steven)/0000-0001-5241-0069
FU National Natural Science Foundation of China [61672089, 61273274,
   61572064]; Fund of Beijing Collaborative Innovation Center
   [PXM2016_014219_000025]; National Key Technology R&D Program of China
   [2012BAH01F03]; Natural Science Foundation of Beijing [NSFB4123104];
   Natural Sciences and Engineering Research Council of Canada
   [RGPIN239031]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61672089, Grant 61273274, and Grant 61572064, by the
   Fund of Beijing Collaborative Innovation Center under Grant
   PXM2016_014219_000025, by the National Key Technology R&D Program of
   China under Grant 2012BAH01F03, by the Natural Science Foundation of
   Beijing under Grant NSFB4123104, and by the Natural Sciences and
   Engineering Research Council of Canada under Grant RGPIN239031. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Shu-Ching Chen. (Corresponding
   author: Cong Ma.)
CR [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247881
   Collins RT, 2003, PROC CVPR IEEE, P234
   Cui P, 2009, IEEE T MULTIMEDIA, V11, P333, DOI 10.1109/TMM.2008.2009722
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Doran MM, 2010, ATTEN PERCEPT PSYCHO, V72, P33, DOI 10.3758/APP.72.1.33
   Fan JL, 2010, LECT NOTES COMPUT SC, V6311, P480
   Grabner H, 2010, PROC CVPR IEEE, P1285, DOI 10.1109/CVPR.2010.5539819
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Luo Y., 2013, P 21 ACM INT C MULTI, P509, DOI [10.1145/2502081.2502135, DOI 10.1145/2502081.2502135]
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma LL, 2010, LECT NOTES COMPUT SC, V6298, P483, DOI 10.1007/978-3-642-15696-0_45
   Mahadevan V., 2012, NEURAL INFORM PROCES, P1664
   Makovski T, 2009, VIS COGN, V17, P180, DOI 10.1080/13506280802211334
   Pérez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Wen LY, 2012, LECT NOTES COMPUT SC, V7575, P716, DOI 10.1007/978-3-642-33765-9_51
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu Y, 2012, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2012.6247878
   Yang M, 2009, IEEE T PATTERN ANAL, V31, P1195, DOI 10.1109/TPAMI.2008.146
   Yuan Y, 2015, IEEE T MULTIMEDIA, V17, P1125, DOI 10.1109/TMM.2015.2440996
   Zhang HL, 2015, MULTIMED TOOLS APPL, V74, P1021, DOI 10.1007/s11042-013-1709-0
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang SP, 2013, PATTERN RECOGN, V46, P1772, DOI 10.1016/j.patcog.2012.10.006
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
NR 32
TC 55
Z9 58
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2415
EP 2424
DI 10.1109/TMM.2017.2694219
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200005
DA 2024-07-18
ER

PT J
AU Su, Z
   Xu, QC
   Hou, F
   Yang, Q
   Qi, QF
AF Su, Zhou
   Xu, Qichao
   Hou, Fen
   Yang, Qing
   Qi, Qifan
TI Edge Caching for Layered Video Contents in Mobile Social Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Caching; edge computing; future networks; mobile video delivery
ID ALLOCATION; SECURITY; FAIRNESS; PRIVACY
AB To improve the performance of mobile video delivery, caching layered videos at a site near to mobile end users (e.g., at the edge of mobile service provider's backbone) was advocated because cached videos can be delivered to mobile users with a high quality of experience, e.g., a short latency. How to optimally cache layered videos based on caching price, the available capacity of cache nodes, and the social features of mobile users, however, is still a challenging issue. In this paper, we propose a novel edge caching scheme to cache layered videos. First, a framework to cache layered videos is presented in which a cache node stores layered videos for multiple social groups, formed by mobile users based on their requests. Due to the limited capacity of the cache node, these social groups compete with each other for the number of layers they request to cache, aiming at maximizing their utilities while all mobile users in each group share the cost involved in the cache of video contents. Second, a Stackelberg game model is developed to study the interaction among multiple social groups and the cache node, and a noncooperative game model is introduced to analyze the competition among mobile users in different social groups. Third, leveraging the backward induction method, the optimal strategy of each player in the game model is proposed. Finally, simulation results show that the proposed method outperforms the exiting counterparts with a higher hit ratio and lower delay of delivering video contents.
C1 [Su, Zhou; Xu, Qichao; Qi, Qifan] Shanghai Univ, Sch Mechatron Engn & Automat, Shanghai 200072, Peoples R China.
   [Hou, Fen] Univ Macau, Dept Elect & Comp Engn, Taipa, Macau, Peoples R China.
   [Yang, Qing] Univ North Texas, Comp Sci & Engn Dept, Denton, TX 76203 USA.
C3 Shanghai University; University of Macau; University of North Texas
   System; University of North Texas Denton
RP Su, Z (corresponding author), Shanghai Univ, Sch Mechatron Engn & Automat, Shanghai 200072, Peoples R China.
EM zhousu@ieee.org; xqc690926910@shu.edu.cn; fenhou@umac.mo;
   qing.yang@montana.edu; 287533791@qq.com
FU National Natural Science Foundation of China [61571286, 61633016];
   Shanghai Key Laboratory of Power Station Automation Technology; Macau
   Science and Technology Development Fund [FDCT 121/2014/A3]; Research
   Committee of the University of Macau [MYRG2016-00171-FST]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61571286 and Grant 61633016, in part by
   the Shanghai Key Laboratory of Power Station Automation Technology, the
   Macau Science and Technology Development Fund, under Grant FDCT
   121/2014/A3, and in part by the Research Committee of the University of
   Macau under Grant MYRG2016-00171-FST. The guest editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Shiwen Mao. (Corresponding author: Zhou Su)
CR Alay Ö, 2010, IEEE T CIRC SYST VID, V20, P1095, DOI 10.1109/TCSVT.2010.2056951
   [Anonymous], P MIL COMM C MILCOM
   [Anonymous], CISC VIS NETW IND GL
   [Anonymous], COMPUTATIONAL MATH M, DOI DOI 10.1155/2015/536943
   [Anonymous], 2015, PROC INT C RENEWABLE
   [Anonymous], 2016, IEEE INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communications
   Azimi SM, 2015, IEEE COMMUN LETT, V19, P34, DOI 10.1109/LCOMM.2014.2373369
   Cao Y, 2016, IEEE WIREL COMMUN, V23, P52, DOI 10.1109/MWC.2016.7553026
   He WB, 2011, INT CON DISTR COMP S, P740, DOI 10.1109/ICDCS.2011.42
   Hu H, 2011, IEEE T CIRC SYST VID, V21, P1013, DOI 10.1109/TCSVT.2011.2129290
   Hui L, 2016, IEEE ACCESS, V4, P5607, DOI 10.1109/ACCESS.2016.2605918
   Hui YL, 2019, IEEE T EMERG TOP COM, V7, P337, DOI 10.1109/TETC.2017.2674023
   Ji W, 2012, IEEE T MULTIMEDIA, V14, P443, DOI 10.1109/TMM.2011.2177645
   Kazemi M, 2017, IEEE T MULTIMEDIA, V19, P54, DOI 10.1109/TMM.2016.2607342
   Li J, 2016, IEEE T VEH TECHNOL, V65, P8744, DOI 10.1109/TVT.2015.2511806
   Li SH, 2016, IEEE T MULTIMEDIA, V18, P2503, DOI 10.1109/TMM.2016.2596042
   Li Z, 2014, IEEE INFOCOM SER, P1941, DOI 10.1109/INFOCOM.2014.6848134
   Liang XH, 2013, IEEE J SEL AREA COMM, V31, P641, DOI 10.1109/JSAC.2013.SUP.0513056
   Liu Z, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2330332
   Niyato D, 2011, IEEE T VEH TECHNOL, V60, P1812, DOI 10.1109/TVT.2011.2116816
   Qi C, 2016, ELECTRON LETT, V52, DOI 10.1049/el.2016.2670
   Qiao J, 2016, IEEE T WIREL COMMUN, V15, P7187, DOI 10.1109/TWC.2016.2598748
   Ridong Fei, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P63, DOI 10.1109/INFCOMW.2011.5928891
   Su Z, 2020, IEEE T EMERG TOP COM, V8, P92, DOI 10.1109/TETC.2017.2671843
   Su Z, 2016, IEEE T MULTIMEDIA, V18, P1650, DOI 10.1109/TMM.2016.2566584
   Su Z, 2016, IEEE NETWORK, V30, P52, DOI 10.1109/MNET.2016.7389831
   Su Z, 2015, IEEE NETWORK, V29, P62, DOI 10.1109/MNET.2015.7166192
   Su Z, 2015, IEEE COMMUN MAG, V53, P66, DOI 10.1109/MCOM.2015.7120047
   Sun Y, 2014, IEEE CONF COMPUT, P163, DOI 10.1109/INFCOMW.2014.6849206
   Wang B, 2004, IEEE T MULTIMEDIA, V6, P366, DOI 10.1109/TMM.2003.822788
   Wang D., IEEE SYST J IN PRESS
   Wang Y, 2002, IEEE IMAGE PROC, P21
   Wang YS, 2013, IEEE T WIREL COMMUN, V12, P6043, DOI 10.1109/TWC.2013.102313.121508
   Xu QC, 2015, IEEE T EMERG TOP COM, V3, P399, DOI 10.1109/TETC.2015.2414792
   Xu QC, 2016, IEEE T VEH TECHNOL, V65, P6692, DOI 10.1109/TVT.2015.2472289
   Yang K., 2016, IEEE COMMUN MAG, V53, P75
   Yao-Jen Chang, 2007, 2007 International Conference on Convergence Information Technology - ICCIT '07, P151, DOI 10.1109/ICCIT.2007.132
   Yuan XL, 2016, IEEE J SEL AREA COMM, V34, P2077, DOI 10.1109/JSAC.2016.2577301
   Zhang JL, 2016, IEEE ACCESS, V4, P3887, DOI 10.1109/ACCESS.2016.2591782
   Zhang K, 2015, IEEE TRANS COMPUT SO, V2, P41, DOI 10.1109/TCSS.2016.2519819
   Zhang K, 2015, IEEE WIREL COMMUN, V22, P104, DOI 10.1109/MWC.2015.7224734
   Zhu HJ, 2013, IEEE T EMERG TOP COM, V1, P192, DOI 10.1109/TETC.2013.2279541
NR 42
TC 59
Z9 62
U1 1
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2210
EP 2221
DI 10.1109/TMM.2017.2733338
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600007
DA 2024-07-18
ER

PT J
AU Gao, LL
   Guo, Z
   Zhang, HW
   Xu, X
   Shen, HT
AF Gao, Lianli
   Guo, Zhao
   Zhang, Hanwang
   Xu, Xing
   Shen, Heng Tao
TI Video Captioning With Attention-Based LSTM and Semantic Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention mechanism; embedding; long short-term memory (LSTM); video
   captioning
AB Recent progress in using long short-term memory (LSTM) for image captioning has motivated the exploration of their applications for video captioning. By taking a video as a sequence of features, an LSTM model is trained on video-sentence pairs and learns to associate a video to a sentence. However, most existing methods compress an entire video shot or frame into a static representation, without considering attention mechanism which allows for selecting salient features. Furthermore, existing approaches usually model the translating error, but ignore the correlations between sentence semantics and visual content. To tackle these issues, we propose a novel end-to-end framework named aLSTMs, an attention-based LSTM model with semantic consistency, to transfer videos to natural sentences. This framework integrates attention mechanism with LSTM to capture salient structures of video, and explores the correlation between multimodal representations (i.e., words and visual content) for generating sentences with rich semantic content. Specifically, we first propose an attention mechanism that uses the dynamic weighted sum of local two-dimensional convolutional neural network representations. Then, an LSTM decoder takes these visual features at time t and the word-embedding feature at time t-1 to generate important words. Finally, we use multimodal embedding to map the visual and sentence features into a joint space to guarantee the semantic consistence of the sentence description and the video visual content. Experiments on the benchmark datasets demonstrate that our method using single feature can achieve competitive or even better results than the state-of-the-art baselines for video captioning in both BLEU and METEOR.
C1 [Gao, Lianli; Guo, Zhao; Xu, Xing; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Ctr Future Media, Chengdu 611731, Sichuan, Peoples R China.
   [Zhang, Hanwang] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
C3 University of Electronic Science & Technology of China; Columbia
   University
RP Shen, HT (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Ctr Future Media, Chengdu 611731, Sichuan, Peoples R China.
EM lianli.gao@uestc.edu.cn; guozhao@std.uestc.edu.cn;
   hanwangzhang@gmail.com; xing.xu@uestc.edu.cn; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021
OI Zhang, Hanwang/0000-0001-7374-8739
FU National Natural Science Foundation of China [61502080, 61632007];
   Fundamental Research Funds for the Central Universities [ZYGX2016J085,
   ZYGX2014Z007]
FX This work was supported in part by the National Natural Science
   Foundation of China under Project 61502080 and Project 61632007, and in
   part by the Fundamental Research Funds for the Central Universities
   under Project ZYGX2016J085 and Project ZYGX2014Z007. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Mr. Jingkuan Song. (Corresponding author: Heng Tao
   Shen.)
CR [Anonymous], 2014, CORR
   [Anonymous], N AM CHAPTER ASS COM
   [Anonymous], 2017, CVPR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2015, ICLR 2015
   [Anonymous], 2016, CORR
   [Anonymous], 2015, P 2015 C EMP METH NA, DOI [10.18653/v1/D15-1166, DOI 10.48550/ARXIV.1508.04025]
   [Anonymous], 2017, CVPR
   [Anonymous], 2012, MACH LEARN
   [Anonymous], PATTERN RECOG
   Auli Michael, 2013, P 2013 C EMPIRICAL M, P1044
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Gan Zhe, 2017, CVPR
   Gao LL, 2016, AAAI CONF ARTIF INTE, P1188
   Guo Z, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P357, DOI 10.1145/2964284.2967242
   Hihn J, 2016, AEROSP CONF PROC
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Li G, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1191, DOI 10.1145/2733373.2806314
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Pan ZQ, 2016, J VIS COMMUN IMAGE R, V40, P516, DOI 10.1016/j.jvcir.2016.07.018
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Rohrbach A, 2015, LECT NOTES COMPUT SC, V9358, P209, DOI 10.1007/978-3-319-24947-6_17
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Song JK, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2964284.2964295
   Song J, 2016, IEEE T IMAGE PROCESS, V25, P4999, DOI 10.1109/TIP.2016.2601260
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tian Q, 2017, NEUROCOMPUTING, V238, P286, DOI 10.1016/j.neucom.2017.01.064
   Tran K., 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, P49
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu Qingqing., 2015, CoRR
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K., 2015, COMPUTER SCI, P2048
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zhu XF, 2013, IEEE T MULTIMEDIA, V15, P633, DOI 10.1109/TMM.2012.2233723
NR 52
TC 413
Z9 436
U1 11
U2 184
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 2045
EP 2055
DI 10.1109/TMM.2017.2729019
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200008
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Xu, WR
   Miao, ZJ
   Zhang, XP
   Tian, Y
AF Xu, Wanru
   Miao, Zhenjiang
   Zhang, Xiao-Ping
   Tian, Yi
TI A Hierarchical Spatio-Temporal Model for Human Activity Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Activity recognition; hidden conditional random field (HCRF);
   hierarchical structure; spatio-temporal dependencies
ID HIDDEN MARKOV MODEL; FRAMEWORK
AB There are two key issues in human activity recognition: spatial dependencies and temporal dependencies. Most recent methods focus on only one of them, and thus do not have sufficient descriptive power to recognize complex activity. In this paper, we propose a hierarchical spatio-temporal model (HSTM) to solve the problem by modeling spatial and temporal constraints simultaneously. The new HSTM is a two-layer hidden conditional random field (HCRF), where the bottom-layer HCRF aims at describing spatial relations in each frame and learning more discriminative representations, and the top-layer HCRF utilizes these high-level features to characterize temporal relations in the whole video sequence. The new HSTM takes advantage of the bottom layer as the building blocks for the top layer and it aggregates evidence from local to global level. A novel learning algorithm is derived to train all model parameters efficiently and its effectiveness is validated theoretically. Experimental results show that the HSTM can successfully classify human activities with higher accuracies on single-person actions (UCF) than other existing methods. More importantly, the HSTM also achieves superior performance on more practical interactions, including human-human interactional activities (UT-Interaction, BIT-Interaction, and CASIA) and human-object interactional activities (Gupta video dataset).
C1 [Xu, Wanru; Miao, Zhenjiang; Tian, Yi] Beijing Jiaotong Univ, Beijing 100044, Peoples R China.
   [Zhang, Xiao-Ping] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 Beijing Jiaotong University; Toronto Metropolitan University
RP Xu, WR (corresponding author), Beijing Jiaotong Univ, Beijing 100044, Peoples R China.
EM 11112063@bjtu.edu.cn; zjmiao@bjtu.edu.cn; xzhang@ee.ryerson.ca;
   11112062@bjtu.edu.cn
RI Zhang, Xiao-Ping (Steven)/B-1436-2016; TIAN, YI/KHU-9704-2024
OI Zhang, Xiao-Ping (Steven)/0000-0001-5241-0069; 
FU Natural Science Foundation of China [61672089, 61273274, 61572064,
   PXM2016_014219_000025]; (973 Program) [2011CB302203]; National Key
   Technology R&D Program of China [2012BAH01F03, NSFB4123104]; Engineering
   Research Council of Canada [RGPIN239031]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61672089, Grant 61273274, Grant 61572064, Grant
   PXM2016_014219_000025, and (973 Program) Grant 2011CB302203, in part by
   the National Key Technology R&D Program of China under Grant
   2012BAH01F03 and Grant NSFB4123104, and in part by the Engineering
   Research Council of Canada under Grant RGPIN239031. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Shu-Ching Chen.
CR [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2008, P 2008 IEEE C COMPUT, DOI DOI 10.1109/CVPR.2008.4587735
   [Anonymous], CASIA ACTION DATABAS
   [Anonymous], 2010, Will person detection help bag-of-features action recognition?
   [Anonymous], CORR
   [Anonymous], INT CONF ACOUST SPEE
   Ballan L, 2012, IEEE T MULTIMEDIA, V14, P1234, DOI 10.1109/TMM.2012.2191268
   Brand M, 1997, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.1997.609450
   Bregonzio M, 2009, PROC CVPR IEEE, P1948, DOI 10.1109/CVPRW.2009.5206779
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Guo P, 2012, IEEE T CIRC SYST VID, V22, P1306, DOI 10.1109/TCSVT.2012.2199390
   Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83
   Hasan M, 2015, IEEE T MULTIMEDIA, V17, P1909, DOI 10.1109/TMM.2015.2477242
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Kong Y, 2014, LECT NOTES COMPUT SC, V8693, P596, DOI 10.1007/978-3-319-10602-1_39
   Kong Y, 2012, LECT NOTES COMPUT SC, V7572, P300, DOI 10.1007/978-3-642-33718-5_22
   Kong Y, 2014, IEEE T PATTERN ANAL, V36, P1775, DOI 10.1109/TPAMI.2014.2303090
   Kong Yu., 2014, Computer Vision-ECCV 2014 Workshops. Springer, P29
   Kosmopoulos DI, 2012, COMPUT VIS IMAGE UND, V116, P422, DOI 10.1016/j.cviu.2011.09.006
   Kovashka A, 2010, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2010.5539881
   Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248
   Lafferty J., 2001, CONDITIONAL RANDOM F, V2001, P282
   Lan T, 2014, LECT NOTES COMPUT SC, V8691, P689, DOI 10.1007/978-3-319-10578-9_45
   Lan T, 2012, IEEE T PATTERN ANAL, V34, P1549, DOI 10.1109/TPAMI.2011.228
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Liang YM, 2009, IEEE T SYST MAN CY B, V39, P268, DOI 10.1109/TSMCB.2008.2005643
   Liu CH, 2016, PATTERN RECOGN, V59, P213, DOI 10.1016/j.patcog.2016.03.019
   Liu DT, 2015, INT J MULTIMED DATA, V6, P1, DOI 10.4018/ijmdem.2015010101
   Morris BT, 2011, IEEE T PATTERN ANAL, V33, P2287, DOI 10.1109/TPAMI.2011.64
   Nematollahi M, 2014, IEEE IMAGE PROC, P5876, DOI 10.1109/ICIP.2014.7026187
   Nguyen NT, 2005, PROC CVPR IEEE, P955
   Ning HZ, 2008, LECT NOTES COMPUT SC, V5303, P419, DOI 10.1007/978-3-540-88688-4_31
   Prest A, 2013, IEEE T PATTERN ANAL, V35, P835, DOI 10.1109/TPAMI.2012.175
   Qixing Huang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1953, DOI 10.1109/CVPR.2011.5995571
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Rodriguez M. D., 2008, PROC IEEE C COMPUT V, P1
   Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361
   Song Y, 2013, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR.2013.457
   Turaga P, 2008, IEEE T CIRC SYST VID, V18, P1473, DOI 10.1109/TCSVT.2008.2005594
   Varol G, 2015, EXPERT SYST APPL, V42, P8274, DOI 10.1016/j.eswa.2015.06.013
   Wang HW, 2009, PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE AND ENGINEERING MANAGEMENT, P124
   Wang L, 2015, IEEE T IMAGE PROCESS, V24, P1424, DOI 10.1109/TIP.2015.2403231
   Wang XF, 2012, MULTIMED TOOLS APPL, V57, P131, DOI 10.1007/s11042-010-0722-9
   Wang XF, 2009, IEEE INT CON MULTI, P642, DOI 10.1109/ICME.2009.5202578
   Wang XY, 2015, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2015.7299071
   Wang XH, 2009, PROCEEDINGS OF THE FIBER SOCIETY 2009 SPRING CONFERENCE, VOLS I AND II, P25, DOI 10.1145/1631024.1631031
   Wang Y., 2009, ADV NEURAL INFORM PR, P1721
   Wang Y, 2009, PROC CVPR IEEE, P872, DOI 10.1109/CVPRW.2009.5206709
   Wen JJ, 2016, PATTERN RECOGN, V60, P515, DOI 10.1016/j.patcog.2016.06.006
   Xiaofeng Wang, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P562, DOI 10.1109/ICCVW.2009.5457653
   Xu WR, 2015, IEEE IMAGE PROC, P1245, DOI 10.1109/ICIP.2015.7350999
   Yang S, 2015, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2015.7298769
   Yu G, 2011, IEEE T MULTIMEDIA, V13, P507, DOI 10.1109/TMM.2011.2128301
   Yuan Y, 2016, IMAGE VISION COMPUT, V55, P77, DOI 10.1016/j.imavis.2016.04.001
   Zhang JG, 2010, PATTERN RECOGN, V43, P197, DOI 10.1016/j.patcog.2009.05.015
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2401, DOI 10.1109/TIP.2011.2128332
   Zhou J, 2008, IEEE T CIRC SYST VID, V18, P1576, DOI 10.1109/TCSVT.2008.2005614
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
NR 62
TC 33
Z9 33
U1 0
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1494
EP 1509
DI 10.1109/TMM.2017.2674622
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800008
DA 2024-07-18
ER

PT J
AU Feng, PM
   Wang, WW
   Dlay, S
   Naqvi, SM
   Chambers, J
AF Feng, Pengming
   Wang, Wenwu
   Dlay, Satnam
   Naqvi, Syed Mohsen
   Chambers, Jonathon
TI Social Force Model-Based MCMC-OCSVM Particle PHD Filter for Multiple
   Human Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple human tracking; Markov chain Monte Carlo (MCMC) resampling; one
   class support vector machine (OCSVM); probable hypothesis density (PHD)
   filter; social force model
ID ROBUST VISUAL TRACKING; DATA ASSOCIATION; VARIABLE NUMBER
AB Video-based multiple human tracking often involves several challenges, including target number variation, object occlusions, and noise corruption in sensor measurements. In this paper, we propose a novel method to address these challenges based on probability hypothesis density (PHD) filtering with a Markov chain Monte Carlo (MCMC) implementation. More specifically, a novel social force model (SFM) for describing the interaction between the targets is used to calculate the likelihood within the MCMC resampling step in the prediction step of the PHD filter, and a one class support vector machine (OCSVM) is then used in the update step to mitigate the noise in the measurements, where the SVM is trained with features from both color and oriented gradient histograms. The proposed method is evaluated and compared with state-of-the-art techniques using sequences from the CAVIAR, TUD, and PETS2009 datasets based on the mean Euclidean tracking error on each frame, the optimal subpattern assignment metric, and the multiple object tracking precision metric. The results show improved performance of the proposed method over the baseline algorithms, including the traditional particle PHD filtering method, the traditional SFM-based particle filtering method, multi-Bernoulli filtering, and an online-learningbased tracking method.
C1 [Feng, Pengming; Dlay, Satnam; Naqvi, Syed Mohsen; Chambers, Jonathon] Newcastle Univ, Sch Elect & Elect Engn, Commun Sensors Signal & Informat Proc Grp, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England.
   [Wang, Wenwu] Univ Surrey, Ctr Vis Speech & Signal Proc, Dept Elect & Elect Engn, Surrey GU2 7XH, England.
C3 Newcastle University - UK; University of Surrey
RP Feng, PM (corresponding author), Newcastle Univ, Sch Elect & Elect Engn, Commun Sensors Signal & Informat Proc Grp, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England.
EM P.Feng2@newcastle.ac.uk; w.wang@surrey.ac.uk;
   Satnam.Dlay@newcastle.ac.uk; Mohsen.Naqvi@newcastle.ac.uk;
   Jonathon.Chambers@newcastle.ac.uk
RI wang, wenwu/HOF-4371-2023
OI Naqvi, Syed Mohsen/0000-0002-1547-0908; Wang, Wenwu/0000-0002-8393-5703;
   Chambers, Jonathon/0000-0002-5820-6509
FU Engineering and Physical Sciences Research Council [EP/K014307]; MOD
   University Defence Research Collaboration in Signal Processing; EPSRC
   [EP/K014307/2] Funding Source: UKRI
FX This work was supported in part by the Engineering and Physical Sciences
   Research Council under Grant EP/K014307, and in part by the MOD
   University Defence Research Collaboration in Signal Processing. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Changsheng Xu.
CR Andriluka M, 2010, PROC CVPR IEEE, P623, DOI 10.1109/CVPR.2010.5540156
   [Anonymous], 2011, VIDEO TRACKING
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], CORR
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Barnard M, 2014, IEEE T MULTIMEDIA, V16, P864, DOI 10.1109/TMM.2014.2301977
   Ben Shitrit H, 2014, IEEE T PATTERN ANAL, V36, P1614, DOI 10.1109/TPAMI.2013.210
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bluman G. A., 2013, ELEMENTARY STAT
   Boukhatem N., 2014, 17 IEEE INT C INF FU, P1, DOI DOI 10.1109/WOWMOM.2014.6918949
   Chu CT, 2013, IEEE T MULTIMEDIA, V15, P1602, DOI 10.1109/TMM.2013.2266634
   Clark DE, 2006, IEEE T SIGNAL PROCES, V54, P2652, DOI 10.1109/TSP.2006.874845
   Cui P, 2009, IEEE T MULTIMEDIA, V11, P333, DOI 10.1109/TMM.2008.2009722
   Feng P., 2016, P IEEE INT C AC SPEE, P1
   Feng P X L, 2015, P EUR SIGN PROC C, P1, DOI DOI 10.1109/ICICDT.2015.7165878
   Feng PM, 2016, IEEE SIGNAL PROC LET, V23, P1592, DOI 10.1109/LSP.2016.2611138
   Feng PM, 2014, INT CONF DIGIT SIG, P12, DOI 10.1109/ICDSP.2014.6900806
   Fisher R., 2003, CAVIAR CASE SCENARIO
   Gilks WR, 2001, J ROY STAT SOC B, V63, P127, DOI 10.1111/1467-9868.00280
   Goldberg I., 2009, P 9 INT S
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   Helbing D, 2002, PEDESTRIAN AND EVACUATION DYNAMICS, P21
   Hoseinnezhad R, 2013, IEEE T SIGNAL PROCES, V61, P392, DOI 10.1109/TSP.2012.2222389
   Huang C, 2013, IEEE T PATTERN ANAL, V35, P898, DOI 10.1109/TPAMI.2012.159
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Khan Z, 2005, IEEE T PATTERN ANAL, V27, P1805, DOI 10.1109/TPAMI.2005.223
   Kiliç V, 2015, IEEE T MULTIMEDIA, V17, P186, DOI 10.1109/TMM.2014.2377515
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Li SH, 2006, Proceedings of the 8th Biennial Conference on Engineering Systems Design and Analysis, Vol 2, P1
   Li X, 2016, IEEE T PATTERN ANAL, V38, P931, DOI 10.1109/TPAMI.2015.2469276
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Li X, 2013, IEEE T IMAGE PROCESS, V22, P3028, DOI 10.1109/TIP.2013.2253478
   Li X, 2013, IEEE T PATTERN ANAL, V35, P863, DOI 10.1109/TPAMI.2012.166
   Liu S, 2016, PROC CVPR IEEE, P4312, DOI 10.1109/CVPR.2016.467
   Luber M, 2010, IEEE INT CONF ROBOT, P464, DOI 10.1109/ROBOT.2010.5509779
   Maggio E, 2008, IEEE T CIRC SYST VID, V18, P1016, DOI 10.1109/TCSVT.2008.928221
   Maggio E, 2007, INT CONF ACOUST SPEE, P1101
   Panta K, 2007, IEEE T AERO ELEC SYS, V43, P556, DOI 10.1109/TAES.2007.4285353
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Polat E, 2006, IEEE T MULTIMEDIA, V8, P1156, DOI 10.1109/TMM.2006.884624
   Ristic B., 2010, Thirteenth Conference on Information Fusion, P1
   Ristic B., 2003, Beyond the Kalman Filter: Particle Filters for Tracking Applications
   Schölkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   Schuhmacher D, 2008, IEEE T SIGNAL PROCES, V56, P3447, DOI 10.1109/TSP.2008.920469
   Ur-Rehman A, 2016, IEEE T SIGNAL PROCES, V64, P1320, DOI 10.1109/TSP.2015.2504340
   Wang L, 2014, IEEE T INTELL TRANSP, V15, P1886, DOI 10.1109/TITS.2014.2303196
   Wang YD, 2006, INT C PATT RECOG, P1127
   Yang B, 2012, PROC CVPR IEEE, P1918, DOI 10.1109/CVPR.2012.6247892
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhang TZ, 2014, PROC CVPR IEEE, P1258, DOI 10.1109/CVPR.2014.164
   Zhang TZ, 2015, INT J COMPUT VISION, V111, P171, DOI 10.1007/s11263-014-0738-0
   Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z
   Zinan Zhao, 2012, 2012 15th International Conference on Information Fusion (FUSION 2012), P1676
NR 53
TC 36
Z9 41
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 725
EP 739
DI 10.1109/TMM.2016.2638206
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500005
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Krüger, B
   Vögele, A
   Willig, T
   Yao, A
   Klein, R
   Weber, A
AF Krueger, Bjoern
   Voegele, Anna
   Willig, Tobias
   Yao, Angela
   Klein, Reinhard
   Weber, Andreas
TI Efficient Unsupervised Temporal Segmentation of Motion Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporal segmentation; time series clustering; human motion analysis
ID CAPTURE DATA; RECOGNITION; VISUALIZATION; VIEW
AB We introduce a method for automated temporal segmentation of human motion data into distinct actions and compositing motion primitives based on self-similar structures in the motion sequence. We use neighborhood graphs for the partitioning and the similarity information in the graph is further exploited to cluster the motion primitives into larger entities of semantic significance. The method requires no assumptions about the motion sequences at hand and no user interaction is required for the segmentation or clustering. In addition, we introduce a feature bundling preprocessing technique to make the segmentation more robust to noise, as well as a notion of motion symmetry for more refined primitive detection. We test our method on several sensor modalities, including markered and markerless motion capture as well as on electromyograph and accelerometer recordings. The results highlight our system's capabilities for both segmentation and for analysis of the finer structures of motion data, all in a completely unsupervised manner.
C1 [Krueger, Bjoern] Gokhale Method Inst, Stanford, CA 94305 USA.
   [Voegele, Anna; Willig, Tobias; Yao, Angela; Klein, Reinhard; Weber, Andreas] Univ Bonn, Dept Comp Sci, D-53113 Bonn, Germany.
C3 University of Bonn
RP Krüger, B (corresponding author), Gokhale Method Inst, Stanford, CA 94305 USA.
EM kruegerb@cs.uni-bonn.de; voegele@cs.uni-bonn.de; twillimail@gmail.com;
   yao@cs.uni-bonn.de; rk@cs.uni-bonn.de; weber@cs.uni-bonn.de
RI Krüger, Björn/AAA-6944-2022
OI Kruger, Bjorn/0000-0002-1596-6487
FU Deutsche Forschungsgemeinschaft [KR 4309/2-1]
FX This work was supported in part by Deutsche Forschungsgemeinschaft under
   Research Grant KR 4309/2-1. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Dong Xu.
CR Alexiadis DS, 2014, IEEE T MULTIMEDIA, V16, P1391, DOI 10.1109/TMM.2014.2317311
   Anna gele., 2012, Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'12, P53
   [Anonymous], 2012, Tech. Rep. MSR-TR-2012-68
   [Anonymous], 2009, P 2009 ACM SIGGRAPH
   [Anonymous], 2010, SCA'10: proceedings of the 2010 ACM SIGGRAPH/Eurographics symposium on computer animation, DOI [10.2312/SCA/SCA10/001-010, DOI 10.2312/SCA/SCA10/001-010]
   [Anonymous], 2014, P 2014 ACM SIGGRAPHE
   [Anonymous], 2009, Advances in Neural Information Processing Systems
   Araujo R, 2014, 2014 13TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P123, DOI 10.1109/ICMLA.2014.25
   Barbic J, 2004, PROC GRAPH INTERF, P185
   Beaudoin P., 2008, P 2008 ACM SIGGRAPHE, P117
   Bernard J, 2013, IEEE T VIS COMPUT GR, V19, P2257, DOI 10.1109/TVCG.2013.178
   Böttger J, 2014, IEEE T VIS COMPUT GR, V20, P471, DOI 10.1109/TVCG.2013.114
   Chen YM, 2011, IEEE T MULTIMEDIA, V13, P421, DOI 10.1109/TMM.2011.2127464
   Chew BS, 2011, IEEE T MULTIMEDIA, V13, P40, DOI 10.1109/TMM.2010.2082512
   Chowdhury RH, 2013, SENSORS-BASEL, V13, P12431, DOI 10.3390/s130912431
   Del Vecchio D, 2003, AUTOMATICA, V39, P2085, DOI 10.1016/S0005-1098(03)00250-4
   Desobry F, 2005, IEEE T SIGNAL PROCES, V53, P2961, DOI 10.1109/TSP.2005.851098
   Fearnhead P, 2006, STAT COMPUT, V16, P203, DOI 10.1007/s11222-006-8450-8
   Fothergill S., 2012, P SIGCHI C HUM FACT, P1737, DOI DOI 10.1145/2207676.2208303
   Guerra G, 2007, COMPUTER, V40, P42, DOI 10.1109/MC.2007.154
   Harchaoui Zaid, 2009, Advances in neural information processing systems, V21
   Heck R, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P129
   Holten D, 2006, IEEE T VIS COMPUT GR, V12, P741, DOI 10.1109/TVCG.2006.147
   Hou JH, 2015, IEEE T VIS COMPUT GR, V21, P848, DOI 10.1109/TVCG.2015.2403328
   Jenkins OdestChadwicke., 2004, P INT C MACHINE LEAR, P441
   Jones S, 2014, IEEE WINT CONF APPL, P816, DOI 10.1109/WACV.2014.6836019
   Junejo IN, 2011, IEEE T PATTERN ANAL, V33, P172, DOI 10.1109/TPAMI.2010.68
   Kadu H, 2014, IEEE T MULTIMEDIA, V16, P2191, DOI 10.1109/TMM.2014.2360793
   Keogh E, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P289, DOI 10.1109/ICDM.2001.989531
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Kruger B., 2008, J VIRTUAL REALITY BR, V5, P1
   Lan RY, 2015, VISUAL COMPUT, V31, P35, DOI 10.1007/s00371-013-0902-5
   Li CJ, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1236471.1236475
   Li S, 2015, IEEE I CONF COMP VIS, P4453, DOI 10.1109/ICCV.2015.506
   Lin CD, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, VOLS 1 AND 2, P1
   Liu Guodong., 2006, SCA 06, P127
   López-Méndez A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.49
   Lu CM, 2004, IEEE T PATTERN ANAL, V26, P258, DOI 10.1109/TPAMI.2004.1262196
   Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172
   Minh Hoai, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3265, DOI 10.1109/CVPR.2011.5995470
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Muller M., 2012, P ITG C SPEECH COMM, P1
   Muller M., 2009, P 10 INT SOC MUS INF, P735
   Okwechime D, 2011, IEEE T MULTIMEDIA, V13, P255, DOI 10.1109/TMM.2010.2096410
   Osmanlioglu Y., 2015, UNSUPERVISED MOTION, P133
   Ostendorf M, 1996, IEEE T SPEECH AUDI P, V4, P360, DOI 10.1109/89.536930
   Pratzlich T., 2014, P 15 INT C MUS INF R, P307
   Rui Y, 2000, PROC CVPR IEEE, P111, DOI 10.1109/CVPR.2000.855807
   Salamah S, 2015, LECT NOTES COMPUT SC, V8844, P169, DOI 10.1007/978-3-319-17043-5_10
   Sattler M., 2005, P 2005 ACM SIGGRAPH, P209
   Scott D. W., 1992, WILEY SER PROB MATH
   Tang NC, 2014, IEEE T MULTIMEDIA, V16, P47, DOI 10.1109/TMM.2013.2283844
   Vejdemo-Johansson M, 2015, APPL ALGEBR ENG COMM, V26, P5, DOI 10.1007/s00200-015-0251-x
   Xuan Xiang, 2007, P 24 INT C MACH LEAR, P1055, DOI DOI 10.1145/1273496.1273629
   Yao A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.67
   Yu G, 2015, LECT NOTES COMPUT SC, V9007, P50, DOI 10.1007/978-3-319-16814-2_4
   Zelnik-Manor L, 2006, IEEE T PATTERN ANAL, V28, P1530, DOI 10.1109/TPAMI.2006.194
   Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137
NR 61
TC 49
Z9 49
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 797
EP 812
DI 10.1109/TMM.2016.2635030
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Pan, Y
   Liu, RK
   Guan, BS
   Du, QC
   Xiong, ZX
AF Pan, Yu
   Liu, Rongke
   Guan, Boshen
   Du, Qiuchen
   Xiong, Zixiang
TI Accurate Depth Extraction Method for Multiple Light-Coding-Based Depth
   Cameras
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth camera; depth extraction; depth accuracy; interference reduction;
   subpixel
ID SYSTEM
AB Using multiple depth cameras can extend the field of view, which is beneficial to three-dimensional (3D) reconstruction. However, multiple active depth cameras would interfere with each other, which degrades the quality of depth images significantly. In this paper, we first present a depth extraction method based on sampling of the space at camera viewpoints to address the interference problem. Compared with the previous plane-sweeping method based on structured-light stereo and multiview stereo, depth images with less depth errors and better shape of objects are generated. Then, we analyze factors affecting depth accuracy for light-coding-based depth cameras and demonstrate the proposed depth extraction method also improves the accuracy of depth images by utilizing multiple projected patterns. Based on the analysis, subpixel-based matching (SPM) is finally applied to further improve the depth accuracy obtained by the depth extraction method. In the image matching process of our method, nearest neighbor interpolation is utilized to each projector's image plane to increase their resolution, which leads to the improved depth accuracy at subpixel level and avoids the drawbacks of changing parameters of light-coding-based depth cameras. Experimental results under simulated and real-world examples validate our analysis and show that the proposed method not only mitigates the impacts of interference effectively but also further improves the accuracy of depth images by using SPM.
C1 [Pan, Yu; Liu, Rongke; Guan, Boshen; Du, Qiuchen] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
   [Xiong, Zixiang] Texas A&M Univ, College Stn, TX 77843 USA.
C3 Beihang University; Texas A&M University System; Texas A&M University
   College Station
RP Liu, RK (corresponding author), Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
EM panyukop@gmail.com; rongke_liu@buaa.edu.cn; iceguan@msn.com;
   QC-Du@hotmail.com; zx@ece.tamu.edu
RI Pan, Yu/JQO-5094-2023
FU National Natural Science Foundation of China [61231010]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61231010. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof.
   Balakrishnan Prabhakaran. (Corresponding author: Rongke Liu.)
CR [Anonymous], 2011, P 12 INT C COMP SYST
   [Anonymous], 3DTV C 2007
   BERGER K, 2011, P VIS MOD VIS, P317
   Collins R. T., 1998, P IEEE COMP SOC C CO, P358
   Daras P, 2012, IEEE T MULTIMEDIA, V14, P374, DOI 10.1109/TMM.2011.2176111
   Duan Y., 2012, P 2012 5 INT C INT C, P25
   Faion F, 2012, IEEE INT C INT ROBOT, P3993, DOI 10.1109/IROS.2012.6386007
   FREDRICKSEN H, 1977, J COMB THEORY A, V22, P17, DOI 10.1016/0097-3165(77)90059-0
   Guan L., 2008, P 4 INT S 3D DAT PRO, P339
   Kang YS, 2010, IEEE INT CON MULTI, P1405, DOI 10.1109/ICME.2010.5583208
   Khoshelham K, 2011, INT ARCH PHOTOGRAMM, V38-5, P133
   Kordelas GA, 2016, IEEE T MULTIMEDIA, V18, P155, DOI 10.1109/TMM.2015.2505905
   Maimone A, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P51, DOI 10.1109/VR.2012.6180879
   Maimone A, 2012, COMPUT GRAPH-UK, V36, P791, DOI 10.1016/j.cag.2012.04.011
   Morano RA, 1998, IEEE T PATTERN ANAL, V20, P322, DOI 10.1109/34.667888
   Park MG, 2015, ELECTRON LETT, V51, P238, DOI 10.1049/el.2014.3770
   Pathak K, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3519, DOI 10.1109/IROS.2008.4650841
   Scharstein D, 2003, PROC CVPR IEEE, P195
   WANG JF, 2012, P IEEE INT C AC SPEE, P5429
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiang S, 2015, SIGNAL PROCESS-IMAGE, V31, P34, DOI 10.1016/j.image.2014.11.004
   Yan ZQ, 2014, APPL OPTICS, V53, P3621, DOI 10.1364/AO.53.003621
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zhu JJ, 2010, IEEE T PATTERN ANAL, V32, P899, DOI 10.1109/TPAMI.2009.68
NR 24
TC 13
Z9 15
U1 3
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 685
EP 701
DI 10.1109/TMM.2016.2646179
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500002
DA 2024-07-18
ER

PT J
AU Feng, SH
   Lang, CY
   Feng, JS
   Wang, T
   Luo, JB
AF Feng, Songhe
   Lang, Congyan
   Feng, Jiashi
   Wang, Tao
   Luo, Jiebo
TI Human Facial Age Estimation by Cost-Sensitive Label Ranking and Trace
   Norm Regularization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cost-sensitive label ranking; facial age estimation; kernel extension;
   low-rank; matrix recovery; trace norm
ID THRESHOLDING ALGORITHM; IMAGE; RECOGNITION; REGRESSION
AB Human facial age estimation has attracted much attention due to its potential applications in forensics, security, and biometrics. In contrast to existing approaches that cast facial age estimation as either a multiclass classification or regression problem, in this work, we propose a novel approach that combines the strength of cost-sensitive label ranking methods with the power of low-rank matrix recovery theories. Instead of having to make a binary decision for each age label, our approach ranks age labels in a descending order in terms of their predicted relevance to the given facial image. In addition, the proposed approach aggregates the linear prediction functions for different ages into a matrix, and introduces the matrix trace norm regularization to explicitly capture the correlations among different age labels and control the model complexity as well. Furthermore, motivated by nonlinear generalization performance of kernel methods, we extend the trace norm regularization from a finite dimensional space to an infinite dimensional space. We also provide theoretical analysis on the efficiency of the proposed kernelized trace normalization, which guarantees the feasibility of the proposed method for solving large-scale prediction problems. Comprehensive experiments on multiple well-known facial image datasets demonstrate the effectiveness of the proposed framework for age estimation compared to the state-of-the-arts.
C1 [Feng, Songhe; Lang, Congyan; Wang, Tao] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Feng, Songhe; Lang, Congyan; Wang, Tao] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Feng, Jiashi] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 638075, Singapore.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
C3 Beijing Jiaotong University; Beijing Jiaotong University; National
   University of Singapore; University of Rochester
RP Lang, CY (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM shfeng@bjtu.edu.cn; cylang@bjtu.edu.cn; elefjia@nus.edu.sg;
   twang@bjtu.edu.cn; jluo@cs.rochester.edu
RI Luo, Jiebo/AAI-7549-2020; Feng, Jiashi/AGX-6209-2022
OI Luo, Jiebo/0000-0002-4516-9729
FU National Natural Science Foundation of China [61472028, 61673048,
   61272352, 61372148]; Beijing Natural Science Foundation [4162048,
   4163075, 4142045]; Beijing Higher Education Young Elite Teacher Project
   [YETP0547]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61472028, Grant 61673048, Grant
   61272352, and Grant 61372148, in part by the Beijing Natural Science
   Foundation under Grant 4162048, Grant 4163075, and Grant 4142045, and in
   part by the Beijing Higher Education Young Elite Teacher Project under
   Grant YETP0547. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Lexing Xie.
   (Corresponding author: Congyan Lang.)
CR [Anonymous], 2010, P INT C PATT REC ICP, DOI DOI 10.1109/ICPR.2010.829
   [Anonymous], FG NET AGING DATABAS
   [Anonymous], 200776 U CATH LOUV C
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 1983, SOV MATH DOKL
   [Anonymous], 2006, P 14 ANN ACM INT C M
   [Anonymous], 2011, Advances in Neural Information Processing Systems
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chang KY, 2015, IEEE T IMAGE PROCESS, V24, P785, DOI 10.1109/TIP.2014.2387379
   Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Chen K, 2013, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2013.319
   Chen YL, 2013, IEEE T INF FOREN SEC, V8, P2164, DOI 10.1109/TIFS.2013.2286265
   Fan N, 2011, IEEE I CONF COMP VIS, P249, DOI 10.1109/ICCV.2011.6126249
   Fei Wu, 2015, IEEE Transactions on Big Data, V1, P109, DOI 10.1109/TBDATA.2015.2497270
   Feng SH, 2015, IEEE T IMAGE PROCESS, V24, P1223, DOI 10.1109/TIP.2015.2395816
   Fu Y, 2008, IEEE T MULTIMEDIA, V10, P578, DOI 10.1109/TMM.2008.921847
   Fu Y, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1383
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Geng X, 2010, AAAI CONF ARTIF INTE, P451
   Guo G., 2010, IEEE COMP SOC C COMP, P71, DOI DOI 10.1109/CVPRW.2010.5543609
   Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280
   Guo GD, 2014, PROC CVPR IEEE, P4257, DOI 10.1109/CVPR.2014.542
   Guo GD, 2011, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2011.5995404
   Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681
   Han H, 2015, IEEE T PATTERN ANAL, V37, P1148, DOI 10.1109/TPAMI.2014.2362759
   Harchaoui Z, 2012, PROC CVPR IEEE, P3386, DOI 10.1109/CVPR.2012.6248078
   Hong RC, 2015, IEEE T MULTIMEDIA, V17, P1980, DOI 10.1109/TMM.2015.2476657
   KWON YH, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P762, DOI 10.1109/CVPR.1994.323894
   Kwon YH, 1999, COMPUT VIS IMAGE UND, V74, P1, DOI 10.1006/cviu.1997.0549
   Lanitis A, 2004, IEEE T SYST MAN CY B, V34, P621, DOI 10.1109/TSMCB.2003.817091
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Li CS, 2015, IEEE T CYBERNETICS, V45, P2522, DOI 10.1109/TCYB.2014.2376517
   Li CS, 2015, IEEE T NEUR NET LEAR, V26, P1551, DOI 10.1109/TNNLS.2014.2339100
   Li CS, 2012, PROC CVPR IEEE, P2570, DOI 10.1109/CVPR.2012.6247975
   Liu KH, 2015, IEEE T INF FOREN SEC, V10, P2408, DOI 10.1109/TIFS.2015.2462732
   Liu LQ, 2016, IEEE T MULTIMEDIA, V18, P64, DOI 10.1109/TMM.2015.2500730
   Liu X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P258, DOI 10.1109/ICCVW.2015.42
   Lu JW, 2015, IEEE T IMAGE PROCESS, V24, P5356, DOI 10.1109/TIP.2015.2481327
   Ni B., 2009, Proceedings of the 17th ACM international conference on Multimedia, P85
   Ni BB, 2011, IEEE T MULTIMEDIA, V13, P1217, DOI 10.1109/TMM.2011.2167317
   Peng Yang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3404, DOI 10.1109/ICPR.2010.831
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Schölkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27
   Tawari A, 2013, IEEE T MULTIMEDIA, V15, P1543, DOI 10.1109/TMM.2013.2266635
   Toh KC, 2010, PAC J OPTIM, V6, P615
   Wang JF, 2009, PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS (ICIG 2009), P457, DOI 10.1109/ICIG.2009.29
   Wang SZ, 2016, IEEE T CYBERNETICS, V46, P827, DOI 10.1109/TCYB.2015.2416321
   Yan S., 2008, PROC IEEE INT C COMP, P1
   Yan SC, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P96
   Yang X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P344, DOI 10.1109/ICCVW.2015.53
   Zhang Y, 2010, PROC CVPR IEEE, P2622, DOI 10.1109/CVPR.2010.5539975
   Zhu Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P267, DOI 10.1109/ICCVW.2015.43
NR 59
TC 31
Z9 32
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 136
EP 148
DI 10.1109/TMM.2016.2608786
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200011
DA 2024-07-18
ER

PT J
AU Dong, FH
   Han, H
   Gong, XW
   Wang, JC
   Li, HJ
AF Dong, Feihong
   Han, Han
   Gong, Xiangwu
   Wang, Jingchao
   Li, Hongjun
TI A Constellation Design Methodology Based on QoS and User Demand in
   High-Altitude Platform Broadband Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Constellation design; cost efficient; high-altitude platforms (HAPs)
   broadband network; improved artificial immune algorithm; quality of
   service (QoS) metrics; user demand model
ID ARTIFICIAL IMMUNE-SYSTEMS; WIRELESS COMMUNICATIONS; SERVICES;
   OPTIMIZATION; SCHEME
AB An investigation into the constellation design methodology of high-altitude platform (HAP) broadband networks is presented. The quality of service (QoS) metrics and user demand model are established in the HAP broadband networks. Each HAP is seen as a mobile base station, and a methodology of mobile base station placement is proposed with QoS and user demand guarantee for the first time. The multiple HAP coverage is analyzed with link geometry based on the single HAP coverage model. Besides, a cost-efficient optimization design framework for the HAP constellation is established by optimizing the design vector to maximize the network capacity per cost under the constraints of QoS metrics. In addition, the optimization model and improved artificial immune algorithm based on immune review are designed, and performance of the proposed method is demonstrated through in-depth numerical simulations. Simulation results show that the proposed constellation design method can effectively solve the problem of the mobile base station layout in the HAP broadband networks.
C1 [Dong, Feihong; Wang, Jingchao; Li, Hongjun] Inst China Elect Syst Engn Corp, Beijing 100141, Peoples R China.
   [Dong, Feihong; Li, Hongjun] PLA Univ Sci & Technol, Coll Commun Engn, Nanjing 210007, Jiangsu, Peoples R China.
   [Han, Han] iBit Commun Co, Lanzhou 730050, Peoples R China.
   [Gong, Xiangwu] Equipment Acad, Beijing 101416, Peoples R China.
C3 Army Engineering University of PLA; Aerospace Engineering University
RP Dong, FH (corresponding author), Inst China Elect Syst Engn Corp, Beijing 100141, Peoples R China.
EM dfhsinlab@hotmail.com; hhhd20042008@163.com; xiangwugong@hotmail.com;
   wangjc_61@163.com; xclhj1985@163.com
OI Dong, Feihong/0000-0002-7404-6840
FU National Natural Science Foundation of China [61231011, 61032004,
   91338201]; National High Technology Research and Development Program of
   China (863 Program) [2012AA121605]
FX This work was supported in part by the National Natural Science
   Foundation of China ubder Grant 61231011, Grant 61032004, and Grant
   91338201, and by in part the National High Technology Research and
   Development Program of China (863 Program) under Grant 2012AA121605. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Shahram Shirani.
CR Alonso FR, 2015, IEEE T POWER SYST, V30, P840, DOI 10.1109/TPWRS.2014.2330628
   [Anonymous], 2000, F1500 ITUR
   [Anonymous], 1999, THESIS MIT CAMBRIDGE
   Araniti G, 2005, WIRELESS PERS COMMUN, V32, P195, DOI 10.1007/s11277-005-0742-3
   Biswas PK, 2010, IEEE SYST J, V4, P353, DOI 10.1109/JSYST.2010.2050364
   Cianca E, 2005, IEEE COMMUN MAG, V43, pS33, DOI 10.1109/MCOM.2005.1561921
   Dasgupta D, 2006, IEEE COMPUT INTELL M, V1, P40, DOI 10.1109/MCI.2006.329705
   De Rango F, 2006, INT J WIREL INF NETW, V13, P77, DOI 10.1007/s10776-005-0020-z
   De Rango F, 2009, IEEE T VEH TECHNOL, V58, P4447, DOI 10.1109/TVT.2009.2019281
   Denton S, 2010, IEEE AERO EL SYS MAG, V25, P30, DOI 10.1109/MAES.2010.5467654
   Djuknic GM, 1997, IEEE COMMUN MAG, V35, P128, DOI 10.1109/35.620534
   Dong FH, 2015, INT J DISTRIB SENS N, DOI 10.1155/2015/512321
   Dong FH, 2015, SENSORS-BASEL, V15, P22266, DOI 10.3390/s150922266
   [董飞鸿 Dong Feihong], 2014, [通信学报, Journal on Communications], V35, P50
   Dong Feihong, 2015, J DONGHUA U ENGLISH, P100
   Dovis F, 2002, IEEE J SEL AREA COMM, V20, P641, DOI 10.1109/49.995523
   Grace D, 2005, IEEE T WIREL COMMUN, V4, P700, DOI 10.1109/TWC.2004.842972
   Grace D., 2005, 14 IST MOB WIR COMM
   Ha Yoon Song, 2008, 2008 International Multiconference on Computer Science and Information Technology, P869, DOI 10.1109/IMCSIT.2008.4747344
   High Altitude Airship (HAA), 2011, PERS COMM ISR JOINT, V15, P15
   Holis J, 2010, IEEE T AERO ELEC SYS, V46, P1468, DOI 10.1109/TAES.2010.5545201
   Jilla C., 2002, 9 AIAA ISSMO S MULT, DOI [10.2514/6.2002-5491, DOI 10.2514/6.2002-5491]
   Karapantazis S, 2005, IEEE WIREL COMMUN, V12, P33, DOI 10.1109/MWC.2005.1561943
   Kashitani  T., 2002, THESIS
   Kumar N, 2015, IEEE SYST J, V9, P675, DOI 10.1109/JSYST.2014.2355113
   Lee YG, 2006, INT J WIREL INF NETW, V13, P31, DOI 10.1007/s10776-005-0018-6
   Li SF, 2011, IEEE T AERO ELEC SYS, V47, P2911, DOI 10.1109/TAES.2011.6034673
   Liu Y. J., 2008, RADIO ENG CHINA, V38, P29
   Luglio M, 2009, IEEE T AERO ELEC SYS, V45, P885, DOI 10.1109/TAES.2009.5259172
   Pace P, 2004, VTC2004-SPRING: 2004 IEEE 59TH VEHICULAR TECHNOLOGY CONFERENCE, VOLS 1-5, PROCEEDINGS, P2872
   Piro G, 2011, IEEE T MULTIMEDIA, V13, P1052, DOI 10.1109/TMM.2011.2152381
   Rappaport T. S., 1996, WIRELESS COMMUNICATI, P280
   Sano I, 2003, ADV SPACE RES-SERIES, V32, P2159, DOI 10.1016/S0273-1177(03)90538-2
   Souppouris A., 2013, BILL GATES SAYS GOOG
   Thornton J, 2001, ELECTRON COMMUN ENG, V13, P138, DOI 10.1049/ecej:20010304
   Tozer TC, 2001, ELECTRON COMMUN ENG, V13, P127, DOI 10.1049/ecej:20010303
   Tsujii T., 2001, P 14 INT TECH M SAT, P1017
   Zhang WW, 2014, IEEE T CYBERNETICS, V44, P185, DOI 10.1109/TCYB.2013.2250956
   Zhou L, 2013, IEEE T MULTIMEDIA, V15, P946, DOI 10.1109/TMM.2013.2237895
   [朱志良 Zhu Zhiliang], 2011, [电子与信息学报, Journal of Electronics & Information Technology], V33, P915
   Zong Ru, 2013, Journal of Xidian University, V40, P188, DOI 10.3969/j.issn.1001-2400.2013.05.030
   Zvanovec S, 2008, EURASIP J WIREL COMM, DOI 10.1155/2008/734216
NR 42
TC 29
Z9 31
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2384
EP 2397
DI 10.1109/TMM.2016.2595260
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200006
DA 2024-07-18
ER

PT J
AU Kilic, V
   Barnard, M
   Wang, WW
   Hilton, A
   Kittler, J
AF Kilic, Volkan
   Barnard, Mark
   Wang, Wenwu
   Hilton, Adrian
   Kittler, Josef
TI Mean-Shift and Sparse Sampling-Based SMC-PHD Filtering for Audio
   Informed Visual Speaker Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual tracking; mean-shift; multi-speaker tracking; probability
   hypothesis density (PHD) filter; sequential Monte Carlo (SMC)
   implementation; sparse particles
ID TIME-VARYING NUMBER; PARTICLE-FILTER; PERFORMANCE EVALUATION; TARGET
   TRACKING; LOCALIZATION; FUSION
AB The probability hypothesis density (PHD) filter based on sequential Monte Carlo (SMC) approximation (also known as SMC-PHD filter) has proven to be a promising algorithm for multispeaker tracking. However, it has a heavy computational cost as surviving, spawned, and born particles need to be distributed in each frame to model the state of the speakers and to estimate jointly the variable number of speakers with their states. In particular, the computational cost is mostly caused by the born particles as they need to be propagated over the entire image in every frame to detect the new speaker presence in the view of the visual tracker. In this paper, we propose to use the audio data to improve the visual SMC-PHD (V-SMC-PHD) filter by using the direction of arrival angles of the audio sources to determine when to propagate the born particles and reallocate the surviving and spawned particles. The tracking accuracy of the audio-visual SMC-PHD (AV-SMC-PHD) algorithm is further improved by using a modified mean-shift algorithm to search and climb density gradients iteratively to find the peak of the probability distribution, and the extra computational complexity introduced by mean-shift is controlled with a sparse sampling technique. These improved algorithms, named as AVMS-SMC-PHD and sparse-AVMS-SMC-PHD, respectively, are compared systematically with AV-SMC-PHD and V-SMC-PHD based on the AV16.3, AMI, and CLEAR datasets.
C1 [Kilic, Volkan] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
   [Kilic, Volkan] Izmir Katip Celebi Univ, Dept Elect & Elect Engn, TR-35620 Cigli Izmir, Turkey.
   [Barnard, Mark; Wang, Wenwu; Hilton, Adrian; Kittler, Josef] Univ Surrey, Dept Elect & Elect Engn, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 University of Surrey; Izmir Katip Celebi University; University of
   Surrey
RP Kilic, V (corresponding author), Izmir Katip Celebi Univ, Dept Elect & Elect Engn, TR-35620 Cigli Izmir, Turkey.
EM volkan.kilic@ikc.edu.tr; mark.barnard@surrey.ac.uk; w.wang@surrey.ac.uk;
   a.hilton@surrey.ac.uk; j.kittler@surrey.ac.uk
RI wang, wenwu/HOF-4371-2023; Kılıç, Volkan/JZE-2927-2024; Hilton, Adrian D
   M/N-3736-2014
OI Kılıç, Volkan/0000-0002-3164-1981; 
FU Engineering and Physical Sciences Research Council of the U.K.
   [EP/K014307/1, EP/L000539/1]; EPSRC [EP/L000539/1, EP/H050000/1,
   EP/K014307/1, EP/K014307/2] Funding Source: UKRI
FX This work was supported by the Engineering and Physical Sciences
   Research Council of the U.K. under Grant EP/K014307/1 and Grant
   EP/L000539/1. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Sen-Ching Samson
   Cheung.
CR [Anonymous], 2014, P INT C INF FUS
   [Anonymous], 2007, Surveillance performance evaluation initiative (SPEVI) audiovisual people dataset
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Barnard M, 2014, IEEE T MULTIMEDIA, V16, P864, DOI 10.1109/TMM.2014.2301977
   BARSHALOM Y, 1998, TRACKING DATA ASS
   Bluman A.G., 2013, ELEMENTARY STAT
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   Cevher V, 2007, IEEE T MULTIMEDIA, V9, P715, DOI 10.1109/TMM.2007.893340
   Chang C, 2005, IEEE SIGNAL PROC LET, V12, P242, DOI 10.1109/LSP.2004.842254
   Chen KW, 2011, IEEE T MULTIMEDIA, V13, P625, DOI 10.1109/TMM.2011.2131639
   Chu CT, 2013, IEEE T MULTIMEDIA, V15, P1602, DOI 10.1109/TMM.2013.2266634
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cui P, 2009, IEEE T MULTIMEDIA, V11, P333, DOI 10.1109/TMM.2008.2009722
   Czyz J, 2005, INT CONF ACOUST SPEE, P217
   Deguchi K, 2004, INT C PATT RECOG, P506, DOI 10.1109/ICPR.2004.1334577
   Fallon MF, 2012, IEEE T AUDIO SPEECH, V20, P1409, DOI 10.1109/TASL.2011.2178402
   Fox D, 2003, INT J ROBOT RES, V22, P985, DOI 10.1177/0278364903022012001
   Gatica-Perez D, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P25
   Hampapur I, 2005, IEEE SIGNAL PROC MAG, V22, P38
   Hoffman JR, 2004, IEEE T SYST MAN CY A, V34, P327, DOI 10.1109/TSMCA.2004.824848
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Isard M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P34, DOI 10.1109/ICCV.2001.937594
   Kilic V., 2015, THESIS
   Kilic V., 2013, 21 EUR SIGN PROC C, P1
   Kilic V., 2015, P INT C MULT EXP JUL, P715
   Kiliç V, 2015, IEEE T MULTIMEDIA, V17, P186, DOI 10.1109/TMM.2014.2377515
   Kiliç V, 2013, INT CONF ACOUST SPEE, P3627, DOI 10.1109/ICASSP.2013.6638334
   Lanz O, 2006, IEEE T PATTERN ANAL, V28, P1436, DOI 10.1109/TPAMI.2006.177
   Lathoud G, 2005, LECT NOTES COMPUT SC, V3361, P182
   Lathoud G, 2005, INT CONF ACOUST SPEE, P265
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma WK, 2006, IEEE T SIGNAL PROCES, V54, P3291, DOI 10.1109/TSP.2006.877658
   Maggio E, 2005, INT CONF ACOUST SPEE, P221
   Mahler R., 2007, STAT MULTISOURCE MUL
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Naphade MR, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P536, DOI 10.1109/ICIP.1998.999041
   Nickel K., 2005, Proceedings of the ACM International Conference on Multimodal Interfaces, P61
   Pham NT, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1511
   Polat E, 2006, IEEE T MULTIMEDIA, V8, P1156, DOI 10.1109/TMM.2006.884624
   Potamianos G., 2003, INT C AUD VIS SPEECH
   Qiong Liu, 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P442
   Ristic B, 2011, IEEE T SIGNAL PROCES, V59, P3452, DOI 10.1109/TSP.2011.2140111
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Schuhmacher D, 2008, IEEE T SIGNAL PROCES, V56, P3447, DOI 10.1109/TSP.2008.920469
   Shan C, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P669
   Shan CF, 2007, PATTERN RECOGN, V40, P1958, DOI 10.1016/j.patcog.2006.12.012
   Shivappa ST, 2010, IEEE J-STSP, V4, P882, DOI 10.1109/JSTSP.2010.2057890
   Stiefelhagen R, 2008, LECT NOTES COMPUT SC, V4625, P3
   Suau X, 2012, IEEE T MULTIMEDIA, V14, P575, DOI 10.1109/TMM.2012.2189853
   Sullivan J, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P323, DOI 10.1109/ICCV.2001.937536
   Talantzis F, 2008, IEEE T SYST MAN CY B, V38, P799, DOI 10.1109/TSMCB.2008.922063
   Vo BN, 2006, IEEE T SIGNAL PROCES, V54, P4091, DOI 10.1109/TSP.2006.881190
   Vo BN, 2005, IEEE T AERO ELEC SYS, V41, P1224, DOI 10.1109/TAES.2005.1561884
   Vo BN, 2004, INT CONF ACOUST SPEE, P357
   Wang JQ, 2009, IEEE T SYST MAN CY B, V39, P1578, DOI 10.1109/TSMCB.2009.2021482
   Wolfel M., 2005, P INT LISB SEP, P3149
   Zhong ST, 2008, FCST: 2008 JAPAN-CHINA JOINT WORKSHOP ON FRONTIER OF COMPUTER SCIENCE AND TECHNOLOGY, PROCEEDINGS, P163, DOI 10.1109/FCST.2008.9
   Zhou SHK, 2004, IEEE T IMAGE PROCESS, V13, P1491, DOI 10.1109/TIP.2004.836152
   Zotkin DN, 2002, EURASIP J APPL SIG P, V2002, P1154, DOI 10.1155/S1110865702206058
NR 60
TC 27
Z9 28
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2417
EP 2431
DI 10.1109/TMM.2016.2599150
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200009
OA Green Accepted, Green Published
DA 2024-07-18
ER

PT J
AU Liang, HR
   Liang, RH
   Sun, GD
AF Liang, Haoran
   Liang, Ronghua
   Sun, Guodao
TI Looking Into Saliency Model via Space-Time Visualization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency model; spatiotemporal analysis; visualization
ID VISUAL ANALYTICS; GAZE
AB We introduce a visual analytics method to analyze eye-tracking data and saliency models for dynamic stimuli, such as video or animated graphics. The focus lies on the analysis of the different performance of saliency models in contrast to human observers to identify trends in the general viewing behavior, including time sequences of attentional synchrony and objects with a strong attentional focus. By using a space-time cube visualization in combination with clustering, the dynamic stimuli and associated eye gazes as well as the attentionmaps from saliencymodels can be analyzed in a static three-dimensional representation. We propose algorithms to keep the appearance of the computer's attention data in line with the human's eye-tracking data. The analytical process is supported by multiple coordinated views that allow the user to focus on different aspects of spatial and temporal information in eye gaze data and saliency map. By comparing attention data from both human and computer incorporated with the spatiotemporal characteristics, we are able to find the different patterns within human and computer algorithms. We list our key findings to help developing better saliency detection algorithms.
C1 [Liang, Haoran; Liang, Ronghua; Sun, Guodao] Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310013, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology
RP Liang, RH (corresponding author), Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310013, Zhejiang, Peoples R China.
EM yanakz@zjut.edu.cn; rhliang@zjut.edu.cn; guodao@zjut.edu.cn
RI Sun, Guodao/AAN-4428-2021; liang, ronghua/H-4463-2012
FU National Science Foundation of China [61527808, 61379076, 61602409];
   Outstanding Youth of Zhejiang Provincial Natural Science Foundation
   [LR14F020002]
FX This work was supported in part by the National Science Foundation of
   China under Grant 61527808, Grant 61379076, and Grant 61602409; and in
   part by the Outstanding Youth of Zhejiang Provincial Natural Science
   Foundation under Grant LR14F020002. The guest editor coordinating the
   review of this manuscript and approving it for publication was Dr. Nan
   Cao.
CR Andrienko G, 2012, IEEE T VIS COMPUT GR, V18, P2889, DOI 10.1109/TVCG.2012.276
   [Anonymous], 2012, Proceedings of the Symposium on Eye Tracking Research and Applications, DOI DOI 10.1145/2168556.2168623
   [Anonymous], 1987, Shifts in selective visual attention: Towards the underlying neural circuitry. matters of intelligence
   [Anonymous], 2004, OPT SCI TECHNOL
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   Bojko A, 2009, LECT NOTES COMPUT SC, V5610, P30, DOI 10.1007/978-3-642-02574-7_4
   Borji A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.85
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706
   Borji A, 2011, MACH VISION APPL, V22, P61, DOI 10.1007/s00138-009-0192-0
   Botchen RP, 2008, IEEE T VIS COMPUT GR, V14, P885, DOI 10.1109/TVCG.2008.40
   Bruce N., 2005, ADV NEURAL INFORM PR, V18, P155
   Cerf M, 2009, J VISION, V9, DOI 10.1167/9.12.10
   Chen M, 2006, IEEE T VIS COMPUT GR, V12, P1093, DOI 10.1109/TVCG.2006.194
   Coutrot A, 2014, J VISION, V14, DOI 10.1167/14.8.5
   Duchowski A.T., 2012, P S EYE TRACKING RES, P13, DOI 10.1145/2168556.2168558
   Duchowski AT, 1998, P SOC PHOTO-OPT INS, V3299, P318, DOI 10.1117/12.320122
   Einhauser W., 2008, J VIS, V8, P341
   Gao F, 2007, PR IEEE COMP DESIGN, P3
   Garcia-Diaz A, 2012, IMAGE VISION COMPUT, V30, P51, DOI 10.1016/j.imavis.2011.11.007
   Gatalsky P, 2004, IEEE INFOR VIS, P145, DOI 10.1109/IV.2004.1320137
   Goldstein RB, 2007, COMPUT BIOL MED, V37, P957, DOI 10.1016/j.compbiomed.2006.08.018
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang M, 2014, LECT NOTES COMPUT SC, V8695, P17, DOI 10.1007/978-3-319-10584-0_2
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kenneth HolmqvistMarcus Nystrom., 2011, Eye Tracking: A Comprehensive Guide to Methods and Measures
   Kienzle W., 2007, Advances in neural information pro cessing systems, V19, P689
   Kim H, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425544
   Kurzhals K, 2013, IEEE T VIS COMPUT GR, V19, P2129, DOI 10.1109/TVCG.2013.194
   Le Meur O, 2006, IEEE T PATTERN ANAL, V28, P802, DOI 10.1109/TPAMI.2006.86
   Li J, 2010, INT J COMPUT VISION, V90, P150, DOI 10.1007/s11263-010-0354-6
   Li X, 2010, LECT NOTES COMPUT SC, V6292, P295, DOI 10.1007/978-3-642-15300-6_21
   Marchant P, 2009, DIGIT CREAT, V20, P153, DOI 10.1080/14626260903083611
   Mital PK, 2011, COGN COMPUT, V3, P5, DOI 10.1007/s12559-010-9074-z
   Nuthmann A., 2010, J VIS, V10, P381
   Peters R.J., 2007, P IEEE C COMPUTER VI, P1
   Raschke Michael., 2012, P 2012 S EYE TRACKIN, P165
   Roberts JC, 2007, CMV 2007: FIFTH INTERNATIONAL CONFERENCE ON COORDINATED & MULTIPLE VIEWS IN EXPLORATORY VISUALIZATION, PROCEEDINGS, P61, DOI 10.1109/CMV.2007.20
   Santella A., 2004, P S EYE TRACKING RES, P27, DOI DOI 10.1145/968363.968368
   Sawahata Y, 2008, PATTERN RECOGN, V41, P1610, DOI 10.1016/j.patcog.2007.10.010
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Shen CY, 2014, LECT NOTES COMPUT SC, V8695, P33, DOI 10.1007/978-3-319-10584-0_3
   Smith T.J., 2008, J. Vis., V8, P773, DOI DOI 10.1167/8.6.773
   Torralba A, 2003, J OPT SOC AM A, V20, P1407, DOI 10.1364/JOSAA.20.001407
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Tsang HY, 2010, IEEE T VIS COMPUT GR, V16, P953, DOI 10.1109/TVCG.2010.149
   Wooding D. S., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P31, DOI 10.1145/507072.507078
   Xu J, 2014, J VISION, V14, DOI 10.1167/14.1.28
   Zhang L., 2008, J VIS, V8, P621
   Zhao Q., 2011, J VIS, V11, P161
NR 53
TC 9
Z9 10
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2271
EP 2281
DI 10.1109/TMM.2016.2613681
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900014
OA Bronze
DA 2024-07-18
ER

PT J
AU Ma, CX
   Liu, YJ
   Zhao, GZ
   Wang, HA
AF Ma, Cui-Xia
   Liu, Yong-Jin
   Zhao, Guozhen
   Wang, Hong-An
TI Visualizing and Analyzing Video Content With Interactive Scalable Maps
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interaction; map metaphor; multi-scale representation; video
   visualization and analysis
AB Visualizing and communicating insights through maps offers an intuitive and familiar way to explore large-scale dynamic relational data. In this paper, we present VideoMap, which is a novel approach for presenting and interacting with relational video content by taking advantage of the map metaphor. VideoMap employs a metaphor to visualize video content by elements of a map with the aim of enabling exploration of video content as if reading a map. Video content is visualized in a hierarchal structure from a very large scale to a small scale of finely detailed representation. VideoMap recognizes a small set of sketch gestures for semantic zooming in and out, annotating the map, and automatically completing path navigation. To achieve this, VideoMap synthesizes map-derived visuals and binds them to the underlying data by operating the map with sketch interaction to facilitate interactive exploration. Extensive user studies were conducted to evaluate VideoMap, and the results demonstrated the effectiveness of VideoMap for facilitating the exploration and understanding of large video content.
C1 [Ma, Cui-Xia; Wang, Hong-An] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
   [Ma, Cui-Xia] Minist Educ, Engn Res Ctr Digital Media Technol, Jinan 100190, Peoples R China.
   [Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Zhao, Guozhen] Chinese Acad Sci, Inst Psychol, State Key Lab Brain & Cognit Sci, Beijing 100101, Peoples R China.
   [Wang, Hong-An] Chinese Acad Sci, Inst Software, Beijing Key Lab Human Comp Interact, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Tsinghua
   University; Chinese Academy of Sciences; Institute of Psychology, CAS;
   Chinese Academy of Sciences; Institute of Software, CAS
RP Liu, YJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM cuixia@iscas.ac.cn; liuyongjin@tsinghua.edu.cn; zhaogz@psych.ac.cn;
   hon-gan@iscas.ac.cn
RI Liu, Yong/GWQ-6163-2022
OI Zhao, Guozhen/0000-0003-4438-5320; ma, cui xia/0000-0003-3999-7429
FU National Key Research and Development Plan [2016YFB1001200]; Natural
   Science Foundation of China [U1435220, 61232013, 61661130156, 61322206,
   61272228]; Royal Society-Newton Advanced Fellowship
FX This work was supported in part by the National Key Research and
   Development Plan under Grant 2016YFB1001200, in part by the Natural
   Science Foundation of China under Grant U1435220, Grant 61232013, Grant
   61661130156, Grant 61322206, and Grant 61272228, and in part by the
   Royal Society-Newton Advanced Fellowship. The guest editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   David Gotz. (Cui-Xia Ma and Yong-Jin Liu contributed equally to this
   work.) (Corresponding author: Yong-Jin Liu.)
CR [Anonymous], P 16 ANN ACM S US IN
   [Anonymous], IEEE COMPUT SOC
   [Anonymous], P SIGCHI C HUM FACT
   [Anonymous], 2007, P 15 ACM INT C MULTI
   [Anonymous], 2011, AUSTR UNIV POWER ENG
   [Anonymous], 2015, Comput. Visual Media
   [Anonymous], 2010, P 18 ACM INT C MULTI
   Bach B, 2016, IEEE T VIS COMPUT GR, V22, P559, DOI 10.1109/TVCG.2015.2467851
   Benfold B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3457, DOI 10.1109/CVPR.2011.5995667
   Gansner Emden R., 2013, Graph Drawing. 20th International Symposium, GD 2012. Revised Selected Papers, P439, DOI 10.1007/978-3-642-36763-2_39
   Gansner ER, 2010, IEEE PAC VIS SYMP, P201, DOI 10.1109/PACIFICVIS.2010.5429590
   Goldman DB, 2006, ACM T GRAPHIC, V25, P862, DOI 10.1145/1141911.1141967
   Hu WM, 2007, IEEE T IMAGE PROCESS, V16, P1168, DOI 10.1109/TIP.2006.891352
   Huang SC, 2011, IEEE T CIRC SYST VID, V21, P1, DOI 10.1109/TCSVT.2010.2087812
   Jing GM, 2015, IEEE T MULTIMEDIA, V17, P2122, DOI 10.1109/TMM.2015.2474263
   Katz L., 1953, Psychometrika, V18, P39, DOI [10.1007/BF02289026, DOI 10.1007/BF02289026]
   Kim J, 2015, COMPUT GRAPH FORUM, V34, P167, DOI 10.1111/cgf.12550
   KORF RE, 1993, ARTIF INTELL, V62, P41, DOI 10.1016/0004-3702(93)90045-D
   Kurdoglu E, 2016, IEEE T MULTIMEDIA, V18, P90, DOI 10.1109/TMM.2015.2496872
   Ma CX, 2012, IEEE T MULTIMEDIA, V14, P1153, DOI 10.1109/TMM.2012.2190389
   Mashima D, 2012, IEEE T VIS COMPUT GR, V18, P1424, DOI 10.1109/TVCG.2011.288
   Nguyen C., 2012, P SIGCHI C HUM FACT, P647, DOI [DOI 10.1145/2207676.22077672, DOI 10.1145/2207676.2207767, 10.1145/2207676.2207767]
   Parry ML, 2011, IEEE T VIS COMPUT GR, V17, P1747, DOI 10.1109/TVCG.2011.208
   Piciarelli C, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P40
   Pimentel MG, 2007, IEEE INT SYM MULTIM, P207, DOI 10.1109/ISM.Workshops.2007.43
   SAMMON JW, 1969, IEEE T COMPUT, VC 18, P401, DOI 10.1109/T-C.1969.222678
   Shah R, 2013, IEEE T CIRC SYST VID, V23, P1565, DOI 10.1109/TCSVT.2013.2248972
   Tanahashi Y, 2012, IEEE T VIS COMPUT GR, V18, P2679, DOI 10.1109/TVCG.2012.212
   Taniguchi Y, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P427
   Tapaswi M, 2014, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.2014.111
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233
   Xie LX, 2002, INT CONF ACOUST SPEE, P4096
   Yang J, 2016, IEEE T IMAGE PROCESS, V25, P503, DOI 10.1109/TIP.2015.2500820
   Ye GG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2733373.2806221
   Yeung MM, 1997, IEEE T CIRC SYST VID, V7, P771, DOI 10.1109/76.633496
   Yu MJ, 2016, NEUROCOMPUTING, V173, P2041, DOI 10.1016/j.neucom.2015.09.046
NR 37
TC 5
Z9 7
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2171
EP 2183
DI 10.1109/TMM.2016.2614229
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900005
DA 2024-07-18
ER

PT J
AU Cicalò, S
   Mazzotti, M
   Moretti, S
   Tralli, V
   Chiani, M
AF Cicalo, Sergio
   Mazzotti, Matteo
   Moretti, Simone
   Tralli, Velio
   Chiani, Marco
TI Multiple Video Delivery in m-Health Emergency Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Camera ranking; content/context-aware; m-health; scalable video coding
   (SVC); video adaptation
ID RESOURCE-ALLOCATION; QUALITY ASSESSMENT; TRANSMISSION; ADAPTATION;
   H.264/AVC; NETWORKS
AB M-health services are expected to become increasingly relevant in the management of emergency situations by enabling real-time support of remote medical experts. In this context, the transmission of multiple health-related video streams from an ambulance to a remote hospital can improve the efficacy of the teleconsultation service, but requires a large bandwidth to meet the desired quality, not always guaranteed by the mobile network. In order to deliver the multiple streams over a single bandwidth-limited wireless access channel, in this paper we propose a novel optimization framework that enables to classify the available video sources and to automatically select and adapt the best streams to transmit. The camera ranking algorithm jointly works with a cross-layer adaptation strategy for multiple scalable streams to achieve different objectives and/or tradeoffs in terms of number and target quality of the transmitted videos. The final goal of the optimization is to dynamically adjust the overall transmitted throughput to meet the actual available bandwidth, while being able to provide high quality to diagnostic video sequences and lower quality to less critical ambient videos. Numerical simulations considering a realistic emergency scenario with long term evolution advanced (LTE-A) connectivity show that the proposed content/context-aware solution is able to automatically select the best sources of information from a visual point of view and to achieve optimal end-to-end video quality for both the diagnostic and the ambient videos.
C1 [Cicalo, Sergio; Tralli, Velio] Univ Ferrara, Dept Engn, CNIT, I-44124 Ferrara, Italy.
   [Mazzotti, Matteo] OCEM Airfield Technol, I-40056 Crespellano, Italy.
   [Moretti, Simone; Chiani, Marco] Univ Bologna, CNIT, Dept Elect Elect & Informat Engn DEI, I-40136 Bologna, Italy.
C3 University of Ferrara; University of Bologna
RP Cicalò, S (corresponding author), Univ Ferrara, Dept Engn, CNIT, I-44124 Ferrara, Italy.
EM sergio.cicalo@unife.it; matteo.mazzotti@ocem.com;
   simone.moretti6@unibo.it; velio.tralli@unife.it; marco.chiani@unibo.it
OI CHIANI, MARCO/0000-0001-8782-8318
CR Alinejad A, 2012, IEEE T INF TECHNOL B, V16, P31, DOI 10.1109/TITB.2011.2154384
   Alinejad A, 2010, IEEE ENG MED BIO, P3471, DOI 10.1109/IEMBS.2010.5627833
   [Anonymous], 2012, 23203 3GPP TS
   [Anonymous], P IEEE INT C COMP TO
   [Anonymous], 2011, JSVM 9 19 11 REFEREN
   Antoniou Z, 2015, IEEE ENG MED BIO, P173, DOI 10.1109/EMBC.2015.7318328
   Cho Y, 2010, IEEE T CONSUM ELECTR, V56, P1997, DOI 10.1109/TCE.2010.5606357
   Cicalo Sergio, 2013, 2013 IEEE 15th International Conference on e-Health Networking, Applications and Services (Healthcom 2013), P212, DOI 10.1109/HealthCom.2013.6720669
   Cicalo S., IEEE T CIRC IN PRESS
   Cicalò S, 2015, 2015 EUROPEAN CONFERENCE ON NETWORKS AND COMMUNICATIONS (EUCNC), P180, DOI 10.1109/EuCNC.2015.7194064
   Cicalò S, 2014, IEEE T MULTIMEDIA, V16, P848, DOI 10.1109/TMM.2014.2300442
   Dardari D, 2015, IEEE T VEH TECHNOL, V64, P1263, DOI 10.1109/TVT.2015.2403868
   DOUKAS C, 2008, EURASIP J WIREL COMM, V2008, P428
   Evolved Universal Terrestrial Radio Access (E-UTRA), 2010, 36104 3GPP TS
   Eysenbach G, 2001, J MED INTERNET RES, V3, DOI 10.2196/jmir.3.4.e34
   Ferrús R, 2013, IEEE COMMUN MAG, V51, P154, DOI 10.1109/MCOM.2013.6619579
   Gállego JR, 2005, IEEE T INF TECHNOL B, V9, P13, DOI 10.1109/TITB.2004.838362
   Glanzer G., 2012, P UB POS IND NAV LOC, P1
   Hu H, 2013, IEEE T MULTIMEDIA, V15, P1638, DOI 10.1109/TMM.2013.2266092
   Iacobelli L, 2015, 2015 EUROPEAN CONFERENCE ON NETWORKS AND COMMUNICATIONS (EUCNC), P118, DOI 10.1109/EuCNC.2015.7194052
   Istepanian RSH, 2009, IEEE J SEL AREA COMM, V27, P566, DOI 10.1109/JSAC.2009.090517
   Konda K. R., 2013, 2013 7 INT C DISTR S, P1
   Kyriacou E, 2007, IEEE ANTENN PROPAG M, V49, P216, DOI 10.1109/MAP.2007.371030
   Markarian G, 2012, IEEE T INF TECHNOL B, V16, P24, DOI 10.1109/TITB.2011.2174157
   Martini MG, 2007, P ANN INT IEEE EMBS, P3082, DOI 10.1109/IEMBS.2007.4352979
   Martini MG, 2015, IEEE ENG MED BIO, P7324, DOI 10.1109/EMBC.2015.7320083
   Martini MG, 2010, IEEE T MOBILE COMPUT, V9, P5, DOI 10.1109/TMC.2009.78
   Mazzotti M., 2011, 73rd IEEE Vehicular Technology Conference, P1
   Mazzotti M, 2012, IEEE T COMMUN, V60, P2915, DOI 10.1109/TCOMM.2012.081412.110113
   Moretti S, 2014, PROCEDIA COMPUT SCI, V40, P206, DOI 10.1016/j.procs.2014.12.028
   Panayides AS, 2013, IEEE ENG MED BIO, P7253, DOI 10.1109/EMBC.2013.6611232
   Panayides A, 2013, IEEE J BIOMED HEALTH, V17, P619, DOI 10.1109/TITB.2012.2232675
   Paradiso R, 2008, IEEE ENG MED BIO, P1699, DOI 10.1109/IEMBS.2008.4649503
   Perakis K, 2014, TELEMEDICINE TECHNOL
   Razaak M, 2014, IEEE J BIOMED HEALTH, V18, P1552, DOI 10.1109/JBHI.2014.2326891
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shaeffer DK, 2013, IEEE COMMUN MAG, V51, P100, DOI 10.1109/MCOM.2013.6495768
   Skorin-Kapov L, 2010, INT J TELEMED APPL, V2010, DOI 10.1155/2010/628086
   Sobhani B, 2014, IEEE J-STSP, V8, P125, DOI 10.1109/JSTSP.2013.2286771
   Soro S, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P81, DOI 10.1109/AVSS.2007.4425290
   Soro S, 2009, ADV MULTIMED, V2009, DOI 10.1155/2009/640386
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
NR 43
TC 15
Z9 17
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 1988
EP 2000
DI 10.1109/TMM.2016.2597001
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, XX
   Qiao, Y
   Wang, XG
   Tang, XO
AF Wu, Xixuan
   Qiao, Yu
   Wang, Xiaogang
   Tang, Xiaoou
TI Bridging Music and Image via Cross-Modal Ranking Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal; feature embedding; lyric-based image attribute; music-image
   matching; ordinal regression
AB Human perceptions of music and image are closely related to each other, since both can inspire similar human sensations, such as emotion, motion, and power. This paper aims to explore whether and how music and image can be automatically matched by machines. The main contributions are three aspects. First, we construct a benchmark dataset composed of more than 45 000 music-image pairs. Human labelers are recruited to annotate whether these pairs are well-matched or not. The results show that they generally agree with each other on the matching degree of music-image pairs. Secondly, we investigate suitable semantic representations of music and image for this cross-modal matching task. In particular, we adopt lyrics as a middle-media to connect music and image, and design a set of lyric-based attributes for image representation. Thirdly, we propose cross-modal ranking analysis (CMRA) to learn the semantic similarity between music and image with ranking labeling information. CMRA aims to find the optimal embedding spaces for both music and image in the sense of maximizing the ordinal margin between music-image pairs. The proposed method is able to learn the non-linear relationship between music and image, and to integrate heterogeneous ranking data from different modalities into a unified space. Experimental results demonstrate that the proposed method outperforms state-of-the-art cross-modal methods in the music-image matching task, and achieves a consistency rate of 91.5% with human labelers.
C1 [Wu, Xixuan; Tang, Xiaoou] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
   [Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518000, Peoples R China.
   [Wang, Xiaogang] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS; Chinese University of Hong Kong
RP Wu, XX (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
EM xixuan1124@gmail.com; yu.qiao@siat.ac.cn; xgwang@ee.cuhk.edu.hk;
   xtang@ie.cuhk.edu.hk
RI Wang, Xiaogang/L-4369-2014; Qiao, Yu/ABD-5787-2021; Yu,
   Qiao/IAP-6999-2023
FU SenseTime Group Limited; General Research Fund - Research Grants Council
   of the Hong Kong SAR [CUHK 416713]; Sino-Dutch Joint Scientific Thematic
   Research Program [JSTP172644KYSB20150019]; Guangdong Innovative Research
   Program [2015B010129013, 2014B050505017]; Shenzhen Research Program
   [KQCX2015033117354153]
FX This work was supported in part by SenseTime Group Limited, in part by
   the General Research Fund sponsored by the Research Grants Council of
   the Hong Kong SAR under Grant CUHK 416713, in part by the Sino-Dutch
   Joint Scientific Thematic Research Program under Grant
   JSTP172644KYSB20150019, in part by the Guangdong Innovative Research
   Program under Grant 2015B010129013 and Grant 2014B050505017, and in part
   by the Shenzhen Research Program under Grant KQCX2015033117354153. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. K. Selcuk Candan.
CR Agarwal Sameer, 2007, Artificial Intelligence and Statistics, P11
   Alameda-Pineda Xavier, 2011, P 13 INT C MULT INT, P247
   [Anonymous], 2008, P ADV NEUR INF PROC
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], 2000, ISMIR
   [Anonymous], 2011, Advances in Neural Information Processing Systems
   [Anonymous], 1983, J MENTAL IMAG
   [Anonymous], 1995, IEEE INT WORKSH AUT
   Barrington L, 2010, IEEE T AUDIO SPEECH, V18, P602, DOI 10.1109/TASL.2009.2036306
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Chao J., 2011, PROC INT SEMANTIC WE
   Chu W, 2007, NEURAL COMPUT, V19, P792, DOI 10.1162/neco.2007.19.3.792
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dunker Peter, 2008, P ACM INT C MULT INF, P97
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Endres I., 2010, Proceedings IEEE 2010 CVPR, P1, DOI DOI 10.1109/CVPRW.2010.5543183
   Essid S., 2005, P 6 INT C MUSIC INFO, P324
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Globerson A, 2006, ADV NEURAL INFORM PR, P451
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Imura J., 2011, P 19 ACM INT C MULT, P1085
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Juslin PN, 2008, BEHAV BRAIN SCI, V31, P559, DOI 10.1017/S0140525X08005293
   Kidron E, 2005, PROC CVPR IEEE, P88
   Klein D, 2003, 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P423, DOI 10.3115/1075096.1075150
   Kliegr T., 2008, Workshop on Neural Networks for Signal Processing at Proc. ACM SIG International Conference Knowledge Discovery and Data Mining, P8
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Libeks J, 2011, IEEE MULTIMEDIA, V18, P30, DOI 10.1109/MMUL.2011.1
   Liu C, 2012, PROC INT CONF DATA, P546, DOI 10.1109/ICDE.2012.13
   McFee B, 2011, J MACH LEARN RES, V12, P491
   McKinney M.F., 2003, Proc. ISMIR, V3, P151
   Meyer LeonardB., 1961, EMOTION MEANING MUSI
   Osborne J., 1981, Journal of Mental Imagery, V5, P133
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Pham T., 2007, Proc. ACM International Conference on Information and Knowledge Management, P439
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rubner L. J., 1997, P ARPA IM UND WORKSH, P661
   Saenz M, 2008, CURR BIOL, V18, pR650, DOI 10.1016/j.cub.2008.06.014
   Schütt M, 2004, PEPTIDE REVOLUTION: GENOMICS, PROTEOMICS & THERAPEUTICS, P41
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shivappa S., 2002, P IEEE, V98, P1692
   Slaney M, 2002, INT CONF ACOUST SPEE, P4108
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Tingle D., 2010, P ISMIR, P55
   Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273
   Turnbull D, 2008, IEEE T AUDIO SPEECH, V16, P467, DOI 10.1109/TASL.2007.913750
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Wang G, 2009, PROC CVPR IEEE, P1367, DOI 10.1109/CVPRW.2009.5206816
   Wang J C, 2012, P 20 ACM INT C MULT, P1379, DOI [10.1145/2393347.2396494, DOI 10.1145/2393347.2396494]
   Wang XG, 2011, PROC CVPR IEEE, P857, DOI 10.1109/CVPR.2011.5995399
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Westerveld T., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P437
   Yao BP, 2011, IEEE I CONF COMP VIS, P1331, DOI 10.1109/ICCV.2011.6126386
   Zhang H., 2007, P 15 INT C MULTIMEDI, P273
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 59
TC 17
Z9 18
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1305
EP 1318
DI 10.1109/TMM.2016.2557722
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600007
DA 2024-07-18
ER

PT J
AU Liang, XD
   Lin, L
   Yang, W
   Luo, P
   Huang, JS
   Yan, SC
AF Liang, Xiaodan
   Lin, Liang
   Yang, Wei
   Luo, Ping
   Huang, Junshi
   Yan, Shuicheng
TI Clothes Co-Parsing Via Joint Image Segmentation and Labeling With
   Application to Clothing Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothes recognition; fashion understanding; human-centric computing;
   image parsing
ID OBJECT; REPRESENTATION; COLOR
AB This paper aims at developing an integrated system for clothing co-parsing (CCP), in order to jointly parse a set of clothing images (unsegmented but annotated with tags) into semantic configurations. A novel data-driven system consisting of two phases of inference is proposed. The first phase, referred as "image cosegmentation," iterates to extract consistent regions on images and jointly refines the regions over all images by employing the exemplar-SVM technique [1]. In the second phase (i.e., "region colabeling"), we construct a multiimage graphical model by taking the segmented regions as vertices, and incorporating several contexts of clothing configuration (e.g., item locations and mutual interactions). The joint label assignment can be solved using the efficient Graph Cuts algorithm. In addition to evaluate our framework on the Fashionista dataset [2], we construct a dataset called the SYSU-Clothes dataset consisting of 2098 high-resolution street fashion photos to demonstrate the performance of our system. We achieve 90.29%/88.23% segmentation accuracy and 65.52%/63.89% recognition rate on the Fashionista and the SYSU-Clothes datasets, respectively, which are superior compared with the previous methods. Furthermore, we apply our method on a challenging task, i.e., cross-domain clothing retrieval: given user photo depicting a clothing image, retrieving the same clothing items from online shopping stores based on the fine-grained parsing results.
C1 [Liang, Xiaodan; Lin, Liang] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
   [Yang, Wei; Luo, Ping] Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
   [Huang, Junshi; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
C3 Sun Yat Sen University; Chinese University of Hong Kong; National
   University of Singapore
RP Lin, L (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
EM xdliang328@gmail.com; linliang@ieee.org; platero.yang@gmail.com;
   pluo.lhi@gmail.com; junshi.huang@gmail.com; eleyans@nus.edu.sg
RI Young, Wei/ABI-8482-2020; Lin, L/HKO-8213-2023; LU, LU/JEZ-4760-2023; l,
   j/JVZ-8480-2024; l, j/HNC-5728-2023; Yan, Shuicheng/HCI-1431-2022; L,
   J/JEF-9564-2023; Luo, Ping/HGE-7623-2022; Luo, Ping/GPG-2707-2022
OI Luo, Ping/0000-0002-6685-7950; 
FU Guangdong Natural Science Foundation [S2013050014548, 2014A030313201];
   Program of Guangzhou Zhujiang Star of Science and Technology
   [2013J2200067]; Guangdong Science and Technology Program
   [2015B010128009]
FX This work was supported in part by the Guangdong Natural Science
   Foundation under Grant S2013050014548 and Grant 2014A030313201, in part
   by Program of Guangzhou Zhujiang Star of Science and Technology under
   Grant 2013J2200067, and in part by Guangdong Science and Technology
   Program under Grant 2015B010128009. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Sen-Ching Samson Cheung. (Corresponding author: Liang Lin.)
CR [Anonymous], P INT C REPR LEARN
   [Anonymous], 2012, PROC ACM INT C MULTI
   [Anonymous], IEEE T IMAGE PROCESS
   [Anonymous], 2009, PROC 17 ACM INT C MU
   [Anonymous], P INT C REPR LEARN
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2013, P 3 ACM C INT C MULT
   [Anonymous], 2006, P CVPR, DOI 10.1109/CVPR.2006.81
   [Anonymous], P IEEE INT C COMP VI
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Borràs A, 2003, LECT NOTES COMPUT SC, V2652, P108
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Dong J, 2013, IEEE I CONF COMP VIS, P3408, DOI 10.1109/ICCV.2013.423
   Gallagher A., 2008, PROC IEEE C COMPUT V, P1
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Jagadeesh V, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1925, DOI 10.1145/2623330.2623332
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Kuettel D, 2012, LECT NOTES COMPUT SC, V7578, P459, DOI 10.1007/978-3-642-33786-4_34
   Leibe Bastian, 2004, WORKSH STAT LEARN CO
   Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360
   Lin L, 2012, PATTERN RECOGN, V45, P3648, DOI 10.1016/j.patcog.2012.03.017
   Lin LA, 2010, IEEE T PATTERN ANAL, V32, P1426, DOI 10.1109/TPAMI.2009.150
   Lin L, 2009, PATTERN RECOGN, V42, P1297, DOI 10.1016/j.patcog.2008.10.033
   Liu S, 2015, PROC CVPR IEEE, P1419, DOI 10.1109/CVPR.2015.7298748
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu XB, 2011, IEEE T CIRC SYST VID, V21, P393, DOI 10.1109/TCSVT.2010.2087570
   Luo P, 2013, IEEE I CONF COMP VIS, P2648, DOI 10.1109/ICCV.2013.329
   Luo P, 2012, PROC CVPR IEEE, P2480, DOI 10.1109/CVPR.2012.6247963
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   Nowozin S., 2009, ICML
   Shotton J, 2008, PROC CVPR IEEE, P1245
   Simo-Serra E, 2015, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2015.7298688
   Simo-Serra E, 2015, LECT NOTES COMPUT SC, V9005, P64, DOI 10.1007/978-3-319-16811-1_5
   Simonyan K., 2014, CORR
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang N, 2011, IEEE I CONF COMP VIS, P1535, DOI 10.1109/ICCV.2011.6126412
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wang Xianwang., 2011, Proceedings of the 19th ACM International Conference on Multimedia, MM'11, P1353
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Yamaguchi K, 2015, IEEE T PATTERN ANAL, V37, P1028, DOI 10.1109/TPAMI.2014.2353624
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Yihang Bo, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2265, DOI 10.1109/CVPR.2011.5995609
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zhang N, 2014, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2014.212
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
NR 50
TC 96
Z9 108
U1 0
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1175
EP 1186
DI 10.1109/TMM.2016.2542983
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100019
DA 2024-07-18
ER

PT J
AU Zhao, LP
   Lin, T
   Zhou, KL
   Wang, SH
   Chen, XY
AF Zhao, Liping
   Lin, Tao
   Zhou, Kailun
   Wang, Shuhui
   Chen, Xianyi
TI Pseudo 2D String Matching Technique for High Efficiency Screen Content
   Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Block matching; hash-table; high efficiency video coding (HEVC); screen
   content coding (SCC); string matching
AB This paper proposes a pseudo 2D string matching (P2SM) technique for high efficiency screen content coding (SCC). The technique uses a primary reference buffer (PRB) and a secondary reference buffer (SRB) for string matching and string copying. In the encoder, optimal reference string searching is performed in both PRB and SRB, and either a PRB or an SRB string is selected as an optimal reference string on a string-by-string basis. If no reference string of at least one pixel is founded for a current pixel, then the current pixel is coded as an unmatched pixel. Compared with HM-16.4+SCM-4.0 reference software, the proposed P2SM technique achieves up to 37.7% Y BD-rate reduction for a screen snapshot of a spreadsheet. On average, using HEVC SCC common test condition and YUV test sequences in text and graphics with motion category, the proposed technique achieves Y BD-rate reduction of 7.7%, 5.0%, 2.6% for all intra (AI), random access (RA) and low-delay B (LB) configurations, respectively in lossy coding with both intra block copy (IBC) and P2SM having the same 4 coding tree units (CTUs) searching range, and bit-rate saving of 6.0%, 3.9%, 3.1% for AI, RA, LB configurations, respectively in lossless coding with IBC having full frame searching range while P2SM having only 2 CTUs searching range, at very low additional encoding and decoding complexity.
C1 [Zhao, Liping; Lin, Tao; Zhou, Kailun; Wang, Shuhui; Chen, Xianyi] Tongji Univ, Coll Elect & Informat Engn, Inst VLSI, Shanghai 200092, Peoples R China.
C3 Tongji University
RP Lin, T (corresponding author), Tongji Univ, Coll Elect & Informat Engn, Inst VLSI, Shanghai 200092, Peoples R China.
EM Lintao@tongji.edu.cn
RI Lin, Tao/JUU-9060-2023; Zhao, Liping/N-4269-2017
OI Zhao, Liping/0000-0002-5538-2064
FU National Natural Science Foundation of China [61201226, 61271096];
   Specialized Research Fund for the Doctoral Program of Higher Education
   [20130072110054]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61201226 and Grant 61271096 and in part
   by the Specialized Research Fund for the Doctoral Program of Higher
   Education under Grant 20130072110054. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Zhu Li.
CR [Anonymous], 2014, JCTVCS0027
   [Anonymous], 2015, JCTVCT0126
   [Anonymous], 2008, VCEGAI11 ITUT SG16 Q
   [Anonymous], 2001, VCEGM33 ITUT SG16 Q6
   [Anonymous], 2011, JCTVCE145
   [Anonymous], 2014, JCTVCS1015
   [Anonymous], 2015, JCTVCU0064
   [Anonymous], 2015, JCTVCU0173
   [Anonymous], 2014, JCTVCS0250
   [Anonymous], 2015, JCTVCU0189
   [Anonymous], 2015, JCTVCT0139
   [Anonymous], 2014, JCTVCQ0124
   [Anonymous], 2015, JCTVCU1005
   [Anonymous], 2014, JCTVCR0060
   [Anonymous], 2013, JCTVCN0256
   [Anonymous], 2015, JCTVCU0116
   [Anonymous], 2015, JCTVCU0186
   [Anonymous], 2014, JCTVCS0010
   [Anonymous], 2013, JCTVCN0206
   [Anonymous], 2015, JCTVCT0136
   Guo L., 2013, P IEEE INT C IM PROC, P5556
   Li B, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P530, DOI 10.1109/VCIP.2014.7051623
   Lin T., 2013, P IEEE INT C MULT EX, P1
   Lin T, 2013, PICT COD SYMP, P369, DOI 10.1109/PCS.2013.6737760
   Lin T, 2013, INT J AD HOC UBIQ CO, V13, P96, DOI 10.1504/IJAHUC.2013.054174
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Lin T, 2009, IEEE INT CON MULTI, P1801
   Lin T, 2009, IEEE SIGNAL PROC LET, V16, P323, DOI 10.1109/LSP.2009.2014285
   Lu Y, 2011, IEEE MULTIMEDIA, V18, P4, DOI 10.1109/MMUL.2011.33
   RAMABADRAN TV, 1988, IEEE MICRO, V8, P62, DOI 10.1109/40.7773
   Shuhui Wang, 2010, Proceedings of the 2010 3rd International Congress on Image and Signal Processing (CISP 2010), P566, DOI 10.1109/CISP.2010.5647270
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wang SQ, 2009, PROCEEDINGS OF THE 2009 CHINESE CONFERENCE ON PATTERN RECOGNITION AND THE FIRST CJK JOINT WORKSHOP ON PATTERN RECOGNITION, VOLS 1 AND 2, P1, DOI 10.1109/PLASMA.2009.5227540
   Wang SH, 2014, MULTIMED TOOLS APPL, V71, P1263, DOI 10.1007/s11042-012-1274-y
   Wang SH, 2013, IET IMAGE PROCESS, V7, P484, DOI 10.1049/iet-ipr.2012.0439
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
   Zhu WJ, 2013, PICT COD SYMP, P373, DOI 10.1109/PCS.2013.6737761
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
NR 39
TC 25
Z9 25
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 339
EP 350
DI 10.1109/TMM.2015.2512539
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600003
DA 2024-07-18
ER

PT J
AU Yu, JG
   Xia, GS
   Gao, CX
   Samal, A
AF Yu, Jin-Gang
   Xia, Gui-Song
   Gao, Changxin
   Samal, Ashok
TI A Computational Model for Object-Based Visual Saliency: Spreading
   Attention Along Gestalt Cues
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gestalt principles; object-based visual saliency; perceptual grouping;
   personalized power iteration clustering (PPIC); salient object
   detection; sensory enhancement theory
ID REGION DETECTION; IMAGE; SEGMENTATION
AB The past few years have witnessed impressive progress on the research of salient object detection. Nevertheless, existing approaches still cannot perform satisfactorily in the case of complex scenes, particularly when the salient objects have non-uniform appearance or complicated shapes, and the background is complexly structured. One important reason for such limitations may be that these approaches commonly ignore the factor of perceptual grouping in saliency modeling. To address this issue, this paper presents a novel computational model for object-based visual saliency, which explicitly takes into consideration the connections between attention and perceptual grouping, and incorporates Gestalt grouping cues into saliency computation. Inspired by the sensory enhancement theory, we suggest a paradigm for object-based saliency modeling, that is, object-based saliency stems from spreading attention along Gestalt grouping cues. Computationally, three typical Gestalt cues, including proximity, similarity, and closure, are respectively extracted from the given image, which are then integrated by constructing a unified Gestalt graph. A new algorithm named personalized power iteration clustering is developed to effectively fulfill the spreading of attention information across the Gestalt graph. Intensive experiments have been carried out to demonstrate the superior performance of the proposed model in comparison to the state-of-the-art.
C1 [Yu, Jin-Gang; Samal, Ashok] Univ Nebraska, Dept Comp Sci & Engn, Lincoln, NE 68503 USA.
   [Xia, Gui-Song] Wuhan Univ, State Key Lab LIESMARS, Wuhan 430079, Peoples R China.
   [Gao, Changxin] Huazhong Univ Sci & Technol, Sch Automat, Wuhan 430074, Peoples R China.
C3 University of Nebraska System; University of Nebraska Lincoln; Wuhan
   University; Huazhong University of Science & Technology
RP Gao, CX (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Wuhan 430074, Peoples R China.
EM jgang.yu@gmail.com; guisong.xia@whu.edu.cn; cgao@hust.edu.cn;
   samal@cse.unl.edu
RI Gao, Changxin/L-4841-2016; Xia, Gui-Song/HII-9232-2022
OI Gao, Changxin/0000-0003-2736-3920; Xia, Gui-Song/0000-0001-7660-6090
FU National Natural Science Foundation of China [61401170, 61502164,
   91338113, 41501462]
FX This work was supported by National Natural Science Foundation of China
   under Grant 61401170, Grant 61502164, Grant 91338113, and Grant
   41501462. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Changsheng Xu.
   (Corresponding author: Changxin Gao.)
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   [Anonymous], 2012, PASCAL VISUAL OBJECT
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Aziz MZ, 2008, IEEE T IMAGE PROCESS, V17, P633, DOI 10.1109/TIP.2008.919365
   Bai X, 2010, IEEE T PATTERN ANAL, V32, P861, DOI 10.1109/TPAMI.2009.85
   Borji A., 2014, CoRR, Vabs/1411.5878
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2011, IEEE INT CONF ROBOT, P1902
   Bruce N., 2005, ADV NEURAL INFORM PR, V18, P155
   Carreira J, 2010, PROC CVPR IEEE, P3241, DOI 10.1109/CVPR.2010.5540063
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Chen Z, 2012, ATTEN PERCEPT PSYCHO, V74, P784, DOI 10.3758/s13414-012-0322-z
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Donoser M, 2013, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2013.174
   Driver J, 2001, COGNITION, V80, P61, DOI 10.1016/S0010-0277(00)00151-7
   DUNCAN J, 1984, J EXP PSYCHOL GEN, V113, P501, DOI 10.1037/0096-3445.113.4.501
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Feng SH, 2010, SIGNAL PROCESS, V90, P1, DOI 10.1016/j.sigpro.2009.05.017
   FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Li L, 2013, IEEE MULTIMEDIA, V20, P13, DOI 10.1109/MMUL.2013.15
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin Frank, 2010, ICML
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Lu Y, 2011, IEEE I CONF COMP VIS, P233, DOI 10.1109/ICCV.2011.6126247
   Peng H., 2015, SALIENT OBJECT DETEC
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Roelfsema PR, 2006, ANNU REV NEUROSCI, V29, P203, DOI 10.1146/annurev.neuro.29.051605.112939
   Roelfsema PR, 1998, NATURE, V395, P376, DOI 10.1038/26475
   Rutishauser U, 2004, PROC CVPR IEEE, P37
   Scholl BJ, 2001, COGNITION, V80, P1, DOI 10.1016/S0010-0277(00)00152-9
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Sun YR, 2003, ARTIF INTELL, V146, P77, DOI 10.1016/S0004-3702(02)00399-5
   van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wannig A, 2011, NAT NEUROSCI, V14, P1243, DOI 10.1038/nn.2910
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wertheimer M, 1923, PSYCHOL FORSCH, V4, P301, DOI 10.1007/BF00410640
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yanulevskaya V, 2013, J VISION, V13, DOI 10.1167/13.13.27
   Yu JG, 2014, IEEE T CYBERNETICS, V44, P1661, DOI 10.1109/TCYB.2013.2292054
   Yu YL, 2010, IEEE T SYST MAN CY B, V40, P1398, DOI 10.1109/TSMCB.2009.2038895
   Zhou D., 2004, P ADV NEUR INF PROC, P168
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 62
TC 30
Z9 32
U1 1
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2016
VL 18
IS 2
BP 273
EP 286
DI 10.1109/TMM.2015.2505908
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DB3HX
UT WOS:000368402400011
DA 2024-07-18
ER

PT J
AU Xiong, J
   Li, HL
   Meng, FM
   Wu, QB
   Ngan, KN
AF Xiong, Jian
   Li, Hongliang
   Meng, Fanman
   Wu, Qingbo
   Ngan, King Ngi
TI Fast HEVC Inter CU Decision Based on Latent SAD Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264; high efficiency video coding (HEVC); inter prediction; motion
   estimation; video coding
ID MODE DECISION; SIZE DECISION; EFFICIENCY; ALGORITHM; BLOCK
AB The emerging high efficiency video coding (HEVC) standard has improved compression performance significantly in comparison with H.264/AVC. However, more intensive computational complexity has been introduced by adopting a number of new coding tools. In this paper, a fast inter CU decision is proposed based on the latent sum of absolute differences (SAD) estimation. Firstly, a two-layer motion estimation (ME) method is designed to take advantage of the latent SAD cost. The new ME method can obtain the SAD costs for both the upper CU and its sub-CUs. Secondly, a concept of motion compensation rate-distortion (R-D) cost is defined, and an exponential model is proposed to express the relationship between the motion compensation R-D cost and the SAD cost. Then, a fast CU decision approach is designed based on the exponential model. The fast CU decision is implemented by comparing a derived threshold with the SAD cost difference between the upper and sub SAD costs. Experimental results show that the proposed algorithm achieves an average of 52% and 58.4% reductions of the coding time at the cost of 1.61% and 2% bit-rate increases under the low delay and random access conditions, respectively.
C1 [Xiong, Jian; Li, Hongliang; Meng, Fanman; Wu, Qingbo] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
   [Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese
   University of Hong Kong
RP Xiong, J (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
EM jxiong219@gmail.com; hlli@uestc.edu.cn; fmmeng@uestc.edu.cn;
   wqb.uestc@gmail.com; knngan@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014; Wu, Qingbo/AAF-6872-2019
OI Ngan, N/0000-0003-1946-3235; Wu, Qingbo/0000-0003-2936-6340; Xiong,
   Jian/0000-0002-4720-4102; Li, Hongliang/0000-0002-7481-095X; Xiong,
   Jian/0000-0002-8346-178X
FU National Natural Science Foundation of China [61525102, 61271289];
   Program for Science and Technology Innovative Research Team for Young
   Scholars in Sichuan Province, China [2014TD0006]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61525102 and 61271289, and in part by
   the Program for Science and Technology Innovative Research Team for
   Young Scholars in Sichuan Province, China under Grant 2014TD0006. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Adrian Munteanu.
CR Ahn S, 2015, IEEE T CIRC SYST VID, V25, P422, DOI 10.1109/TCSVT.2014.2360031
   Ahn S, 2013, PICT COD SYMP, P113, DOI 10.1109/PCS.2013.6737696
   [Anonymous], 2001, SC16Q6 ITUT
   BLOCH D, 1966, J AM STAT ASSOC, V61, P852, DOI 10.2307/2282794
   Cassa MB, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P493, DOI 10.1109/PCS.2012.6213262
   Chiang PT, 2013, IEEE INT SYMP CIRC S, P1640, DOI 10.1109/ISCAS.2013.6572177
   Correa G, 2015, IEEE T CIRC SYST VID, V25, P660, DOI 10.1109/TCSVT.2014.2363753
   Correa G, 2014, EUR SIGNAL PR CONF, P276
   Correa G, 2013, IEEE DATA COMPR CONF, P43, DOI 10.1109/DCC.2013.12
   Han WJ, 2010, IEEE T CIRC SYST VID, V20, P1709, DOI 10.1109/TCSVT.2010.2092612
   Jian Xiong, 2012, 2012 International Symposium on Intelligent Signal Processing and Communications Systems (ISPACS 2012), P366, DOI 10.1109/ISPACS.2012.6473512
   Jie Leng, 2011, 2011 International Conference on Multimedia and Signal Processing (CMSP), P56, DOI 10.1109/CMSP.2011.167
   Kim I.-K., 2012, P 9 JCT VC M APR, P6
   Kim J, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P449, DOI 10.1109/PCS.2012.6213251
   Lee J, 2015, IEEE T CIRC SYST VID, V25, P411, DOI 10.1109/TCSVT.2014.2339612
   Lee JH, 2013, IEEE IMAGE PROC, P1982, DOI 10.1109/ICIP.2013.6738408
   Ma SW, 2012, IEEE IMAGE PROC, P181, DOI 10.1109/ICIP.2012.6466825
   Pan ZQ, 2014, IEEE T BROADCAST, V60, P405, DOI 10.1109/TBC.2014.2321682
   ROTHENBERG TJ, 1964, J AM STAT ASSOC, V59, P460, DOI 10.2307/2282999
   Rousseeuw PJ, 2002, COMPUT STAT DATA AN, V40, P741, DOI 10.1016/S0167-9473(02)00078-6
   Shen LQ, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700298
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2014, IEEE T IMAGE PROCESS, V23, P4232, DOI 10.1109/TIP.2014.2341927
   Shen LQ, 2013, IEEE T CONSUM ELECTR, V59, P207, DOI 10.1109/TCE.2013.6490261
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen XL, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P453, DOI 10.1109/PCS.2012.6213252
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tan HL, 2012, INT CONF ACOUST SPEE, P825, DOI 10.1109/ICASSP.2012.6288011
   Ugur K, 2010, IEEE T CIRC SYST VID, V20, P1688, DOI 10.1109/TCSVT.2010.2092613
   Vanne J, 2014, IEEE T CIRC SYST VID, V24, P1579, DOI 10.1109/TCSVT.2014.2308453
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiong J, 2014, IEEE IMAGE PROC, P3715, DOI 10.1109/ICIP.2014.7025754
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P2141, DOI 10.1109/TMM.2014.2356795
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Xiong J, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.7.071504
   Zhang YF, 2013, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.2013.13
   Zhang Y, 2015, IEEE T IMAGE PROCESS, V24, P2225, DOI 10.1109/TIP.2015.2417498
NR 37
TC 69
Z9 71
U1 0
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2147
EP 2159
DI 10.1109/TMM.2015.2491018
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500004
DA 2024-07-18
ER

PT J
AU Toni, L
   Maugey, T
   Frossard, P
AF Toni, Laura
   Maugey, Thomas
   Frossard, Pascal
TI Optimized Packet Scheduling in Multiview Video Navigation Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlated sources; interactive multiview streaming; multiview
   navigation; packet scheduling
ID TRANSMISSION
AB We study coding and transmission strategies in multicamera systems, where correlated sources send data through a bottleneck channel to a central server, which eventually transmits views to different interactive users. We propose a dynamic navigation-path aware packet scheduling optimization under delay, bandwidth, and interactivity constraints aimed at optimizing the quality-of-experience of interactive users. In particular, the scene distortion is minimized jointly with the distortion variations along most likely navigation paths. The optimization relies both on a novel rate-distortion model, which captures the importance of each view in the scene reconstruction, and on an objective function that optimizes resources based on a client navigation model. The latter takes into account the distortion experienced by interactive clients as well as the distortion variations that might be observed by clients during multiview navigation. We solve the scheduling problem with a novel trellis-based solution, which permits to formally decompose the multivariate optimization problem, thereby significantly reducing the computation complexity. Simulation results show the PSNR quality gain offered by the proposed algorithm compared to baseline scheduling policies. Finally, we show that the best scheduling policy consistently adapts to the most likely user navigation path and that it minimizes distortion variations that can be very disturbing for users in traditional navigation systems.
C1 [Toni, Laura; Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
   [Maugey, Thomas] INRIA Rennes, Natl Inst Res Comp Sci & Control, F-35042 Rennes, France.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Universite de Rennes
RP Toni, L (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
EM laura.toni@epfl.ch; thomas.maugey@inria.fr; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019
OI Toni, Laura/0000-0002-8441-8791
FU Swiss National Science Foundation (SNSF) under the CHISTERA project
   CONCERT; Hasler Foundation under Project NORIA;  [FNS 20CH21 151569]
FX This work was supported in part by the Swiss National Science Foundation
   (SNSF) under the CHISTERA project CONCERT, by Project FNS 20CH21 151569,
   and by the Hasler Foundation under Project NORIA. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Yonggang Wen.
CR [Anonymous], 2001, Introduction to algorithms
   Arefin A, 2012, INT CON DISTR COMP S, P82, DOI 10.1109/ICDCS.2012.58
   Chakareski J, 2014, IEEE T IMAGE PROCESS, V23, P931, DOI 10.1109/TIP.2013.2293419
   Chen C, 2014, IEEE T IMAGE PROCESS, V23, P2206, DOI 10.1109/TIP.2014.2312613
   Chen ZB, 2009, IEEE INT SYMP CIRC S, P1795, DOI 10.1109/ISCAS.2009.5118125
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P3179, DOI 10.1109/TIP.2011.2158230
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P744, DOI 10.1109/TIP.2010.2070074
   Cheung NM, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P269
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Daribo Ismael, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P167, DOI 10.1109/MMSP.2010.5662013
   Dieber B, 2011, IEEE T CIRC SYST VID, V21, P1424, DOI 10.1109/TCSVT.2011.2162770
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Fu FW, 2012, IEEE T CIRC SYST VID, V22, P727, DOI 10.1109/TCSVT.2011.2177940
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Kurutepe E, 2008, IEEE IMAGE PROC, P3088, DOI 10.1109/ICIP.2008.4712448
   Li Z., 2014, Proceedings of the 5th ACM Multimedia Systems Conference, MMSys '14, P248
   Liu YW, 2010, J VIS COMMUN IMAGE R, V21, P523, DOI 10.1016/j.jvcir.2010.02.004
   Lou JG, 2007, ADV MULTIMED, V2007, DOI 10.1155/2007/97535
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Pan  Z., 2011, P IEEE ICC, P1
   Sutton RS., 2020, REINFORCEMENT LEARNI
   Toni L., 2012, CORR
   Toni L, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P338, DOI 10.1109/VCIP.2014.7051575
   Toni L, 2014, IEEE T MULTIMEDIA, V16, P496, DOI 10.1109/TMM.2013.2291531
   Wang HS, 2009, IEEE T IMAGE PROCESS, V18, P225, DOI 10.1109/TIP.2008.2008743
   Wang P., 2011, P IEEE INT C COMP CO, P746
   Yang ZY, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1671962.1671963
   Yu C, 2010, IEEE T IMAGE PROCESS, V19, P2042, DOI 10.1109/TIP.2010.2046794
NR 29
TC 9
Z9 9
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1604
EP 1616
DI 10.1109/TMM.2015.2450020
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lv, YM
   Ng, WWY
   Zeng, ZQ
   Yeung, DS
   Chan, PPK
AF Lv, Yueming
   Ng, Wing W. Y.
   Zeng, Ziqian
   Yeung, Daniel S.
   Chan, Patrick P. K.
TI Asymmetric Cyclical Hashing for Large Scale Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Asymmetric hashing with different code lengths; hashing; large scale
   image retrieval
ID LEARNING BINARY-CODES; ITERATIVE QUANTIZATION; PROCRUSTEAN APPROACH;
   NEAREST-NEIGHBOR
AB This paper addresses a problem in the hashing technique for large scale image retrieval: learn a compact hash code to reduce the storage cost with performance comparable to that of the long hash code. A longer hash code yields a better precision rate of retrieved images. However, it also requires a larger storage, which limits the number of stored images. Current hashing methods employ the same code length for both queries and stored images. We propose a new hashing scheme using two hash codes with different lengths for queries and stored images, i.e., the asymmetric cyclical hashing. A compact hash code is used to reduce the storage requirement, while a long hash code is used for the query image. The image retrieval is performed by computing the Hamming distance of the long hash code of the query and the cyclically concatenated compact hash code of the stored image to yield a high precision and recall rate. Experiments on benchmarking databases consisting up to one million images show the effectiveness of the proposed method.
C1 [Lv, Yueming; Ng, Wing W. Y.; Zeng, Ziqian; Yeung, Daniel S.; Chan, Patrick P. K.] S China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
C3 South China University of Technology
RP Ng, WWY (corresponding author), S China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM wingng@ieee.org
RI Zeng, Ziqian/GXM-5147-2022
OI Ng, Wing W. Y./0000-0003-0783-3585
FU National Natural Science Foundation of China [61272201]; Program for New
   Century Excellent Talents in University of China [NCET-11-0162]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61272201 and the Program for New Century Excellent
   Talents in University of China under Grant NCET-11-0162. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Cees Snoek. (Corresponding author: Wing W. Y. Ng.)
CR Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   [Anonymous], 2009, NIPS
   [Anonymous], CORR
   [Anonymous], THESIS U MARYLAND CO
   [Anonymous], 2009, NEURIPS
   [Anonymous], 2013, NeurIPS
   [Anonymous], 2008, Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
   Bawa M, 2005, Proceedings of the 14th International Conference on World Wide Web-WWW'05, P651, DOI [DOI 10.1145/1060745.1060840, 10.1145/1060745.1060840]
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong Y., 2012, NIPS, P1205
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Gordo A, 2014, IEEE T PATTERN ANAL, V36, P33, DOI 10.1109/TPAMI.2013.101
   Gordo A, 2011, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2011.5995505
   Grauman K., 2007, CVPR, P1
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu Wei, 2011, Reports in Parasitology, V1, P1
   Lv Q, 2007, P 33 INT C VER LARG, P950
   Norouzi M.E., 2011, ICML
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Norouzi M, 2012, PROC CVPR IEEE, P3108, DOI 10.1109/CVPR.2012.6248043
   Panigrahy R, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1186, DOI 10.1145/1109557.1109688
   Rahimi Ali, 2007, NIPS, V3, P5
   SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451
   Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950
   Wang JF, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P517, DOI 10.1145/2647868.2654898
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Weiss Y., 2008, NIPS, V9, P6
NR 34
TC 38
Z9 40
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1225
EP 1235
DI 10.1109/TMM.2015.2437712
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000009
DA 2024-07-18
ER

PT J
AU Li, W
   Wang, CH
   Zhang, L
   Rui, Y
   Zhang, B
AF Li, Wei
   Wang, Changhu
   Zhang, Lei
   Rui, Yong
   Zhang, Bo
TI Partial-Duplicate Clustering and Visual Pattern Discovery on Web Scale
   Image Database
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Local features; parallel algorithms; partial-duplicate images; visual
   patterns
ID OBJECT; RETRIEVAL
AB In this paper, we study the problem of discovering visual patterns and partial-duplicate images, which is fundamental to visual concept representation and image parsing, but very challenging when the database is extremely large, such as billions of images indexed by a commercial search engine. Although extensive research with sophisticated algorithms has been conducted for either partial-duplicate clustering or visual pattern discovery, most of them can not be easily extended to this scale, since both are clustering problems in nature and require pairwise comparisons. To tackle this computational challenge, we introduce a novel and highly parallelizable framework to discover partial-duplicate images and visual patterns in a unified way in distributed computing systems. We emphasize the nested property of local features, and propose the generalized nested feature (GNF) as a mid-level representation for regions and local patterns. Initial coarse clusters are then discovered by GNFs, upon which - gram GNF is defined to represent co-occurrent visual patterns. After that, efficient merging and refining algorithms are used to get the partial-duplicate clusters, and logical combinations of probabilistic GNF models are leveraged to represent the visual patterns of partially duplicate images. Extensive experiments show the parallelizable property and effectiveness of the algorithms on both partial-duplicate clustering and visual pattern discovery. With 2000 machines, it costs about eight and 400 minutes to process one million and 40 million images respectively, which is quite efficient compared to previous methods.
C1 [Li, Wei; Zhang, Bo] Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
   [Wang, Changhu; Rui, Yong] Microsoft Res, Beijing 100080, Peoples R China.
   [Zhang, Lei] Microsoft Res, Redmond, WA 98052 USA.
C3 Tsinghua University; Microsoft; Microsoft
RP Wang, CH (corresponding author), Microsoft Res, Beijing 100080, Peoples R China.
EM liwei2658@gmail.com; chw@microsoft.com; leizhang@microsoft.com;
   yongrui@microsoft.com; dcszb@mail.tsinghua.edu.cn
FU National Basic Research Program of China (973 Program) [2013CB329403,
   2012CB316301]; National Natural Science Foundation of China [91120011,
   61273023]; Tsinghua University Initiative Scientific Research Program
   [20121088071]
FX The work of W. Li and B. Zhang was supported in part by the National
   Basic Research Program of China (973 Program) under Grant 2013CB329403
   and Grant 2012CB316301, in part by the National Natural Science
   Foundation of China under Grant 91120011 and Grant 61273023, and in part
   by the Tsinghua University Initiative Scientific Research Program
   20121088071. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Lexing Xie.
   (Corresponding author: Changhu Wang.)
CR [Anonymous], 2007, IEEE C COMP VIS PATT
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2012, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2012.6248039
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   Chum O, 2010, IEEE T PATTERN ANAL, V32, P371, DOI 10.1109/TPAMI.2009.166
   Hua X.-S., 2013, Proceedings of the 21st ACM International Conference on Multimedia, Oct. 21-25, ACM, P243
   Kang H, 2012, LECT NOTES COMPUT SC, V7577, P794, DOI 10.1007/978-3-642-33783-3_57
   Kang HW, 2011, IEEE I CONF COMP VIS, P762, DOI 10.1109/ICCV.2011.6126314
   Kennedy L., 2008, ACM International Conference on Multimedia (MM), P349
   Lee DC, 2010, LECT NOTES COMPUT SC, V6311, P648, DOI 10.1007/978-3-642-15549-9_47
   Liu JC, 2013, PROC CVPR IEEE, P2003, DOI 10.1109/CVPR.2013.261
   Liu XY, 2011, 2011 SECOND INTERNATIONAL CONFERENCE ON EDUCATION AND SPORTS EDUCATION (ESE 2011), VOL II, P1
   Liu Z, 2015, IEEE T MULTIMEDIA, V17, P538, DOI 10.1109/TMM.2015.2399851
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ngo CW, 2013, IEEE MULTIMEDIA, V20, P10, DOI 10.1109/MMUL.2013.45
   Nister David, 2006, CVPR
   Romberg S., 2011, P 1 ACM INT C MULT R, P1
   Romberg S., 2013, Proceedings of the 3rd ACM conference on International conference on multimedia retrieval, P113
   Shen XH, 2012, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR.2012.6248031
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang HX, 2014, WIRES DATA MIN KNOWL, V4, P24, DOI 10.1002/widm.1110
   Wang XJ, 2013, IEEE COMPUT SOC CONF, P429, DOI 10.1109/CVPRW.2013.71
   Wang XJ, 2010, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2010.5540046
   Winder SAJ, 2007, PROC CVPR IEEE, P17
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Xu PF, 2013, IEEE MULTIMEDIA, V20, P34, DOI 10.1109/MMUL.2013.18
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   Zhang W, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P297, DOI 10.1145/2647868.2654942
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   [郑宇 ZHENG Yu], 2008, [火工品, Initiators & Pyrotechnics], P1
NR 32
TC 7
Z9 8
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 967
EP 980
DI 10.1109/TMM.2015.2428996
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300005
DA 2024-07-18
ER

PT J
AU Hefeeda, M
   ElGamal, T
   Calagari, K
   Abdelsadek, A
AF Hefeeda, Mohamed
   ElGamal, Tarek
   Calagari, Kiana
   Abdelsadek, Ahmed
TI Cloud-Based Multimedia Content Protection System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D video; cloud applications; depth signatures; video copy detection;
   video fingerprinting
AB We propose a new design for large-scale multimedia content protection systems. Our design leverages cloud infrastructures to provide cost efficiency, rapid deployment, scalability, and elasticity to accommodate varying workloads. The proposed system can be used to protect different multimedia content types, including 2-D videos, 3-D videos, images, audio clips, songs, and music clips. The system can be deployed on private and/or public clouds. Our system has two novel components: (i) method to create signatures of 3-D videos, and (ii) distributed matching engine for multimedia objects. The signature method creates robust and representative signatures of 3-D videos that capture the depth signals in these videos and it is computationally efficient to compute and compare as well as it requires small storage. The distributed matching engine achieves high scalability and it is designed to support different multimedia objects. We implemented the proposed system and deployed it on two clouds: Amazon cloud and our private cloud. Our experiments with more than 11,000 3-D videos and 1 million images show the high accuracy and scalability of the proposed system. In addition, we compared our system to the protection system used by YouTube and our results show that the YouTube protection system fails to detect most copies of 3-D videos, while our system detects more than 98% of them. This comparison shows the need for the proposed 3-D signature method, since the state-of-the-art commercial system was not able to handle 3-D videos.
C1 [Hefeeda, Mohamed; ElGamal, Tarek] Qatar Comp Res Inst, Doha 5825, Qatar.
   [Calagari, Kiana; Abdelsadek, Ahmed] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Qatar Foundation (QF); Hamad Bin Khalifa University-Qatar; Qatar
   Computing Research Institute; Simon Fraser University
RP Hefeeda, M (corresponding author), Qatar Comp Res Inst, Doha 5825, Qatar.
EM mhefeeda@qf.org.qa
CR Abdelsadek A., 2014, THESIS S FRASER U BU
   Abdelsadek A., 2014, P ACM MULT SYST C MM, P115
   Aly M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.40
   [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], 2010, LARGE SCALE DISTRIB
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Cano P, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P169
   Capeto U., 2013, DEPTH MAP AUTOMATIC
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Hampapur A, 2002, P SOC PHOTO-OPT INS, V4676, P194
   Haojun Liao, 2010, Proceedings of the 2010 IEEE International Conference on Networking, Architecture, and Storage (NAS 2010), P240, DOI 10.1109/NAS.2010.44
   Ioffe S., 2012, U.S. Patent, Patent No. 8229219
   Kahng AB, 1998, 1998 DESIGN AUTOMATION CONFERENCE, PROCEEDINGS, P776, DOI 10.1109/DAC.1998.724576
   Khodabakhshi N, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422963
   Lee S, 2008, IEEE T CIRC SYST VID, V18, P983, DOI 10.1109/TCSVT.2008.920739
   Liu Z., 2010, Proc. of the international conference on Multimedia information retrieval (MIR'10), P119
   Lu J., 2009, P SOC PHOTO-OPT INS, V7254
   Lu W, 2012, PROC VLDB ENDOW, V5, P1016, DOI 10.14778/2336664.2336674
   Metois E., 2011, U.S. Patent, Patent No. 7925044
   Müller H, 2001, PATTERN RECOGN LETT, V22, P593, DOI 10.1016/S0167-8655(00)00118-5
   Ram P., 2013, Advances in Neural Information Processing Systems, P656
   Ramachandra Vikas, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P81, DOI 10.1109/3DTV.2008.4547813
   Tasdemir Kasim, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3134, DOI 10.1109/ICPR.2010.767
NR 24
TC 15
Z9 15
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 420
EP 433
DI 10.1109/TMM.2015.2389628
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700013
DA 2024-07-18
ER

PT J
AU Mao, K
   Shou, LD
   Fan, J
   Chen, G
   Kankanhalli, MS
AF Mao, Kuang
   Shou, Lidan
   Fan, Ju
   Chen, Gang
   Kankanhalli, Mohan S.
TI Competence-Based Song Recommendation: Matching Songs to One's Singing
   Skill
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Learning-to-rank; singing competence; song recommendation
ID VOICE RANGE PROFILES; MUSIC RETRIEVAL; QUALITY; RATIO
AB Singing is a popular social activity and a pleasant way of expressing one's feelings. One important reason for unsuccessful singing performance is because the singer fails to choose a suitable song. In this paper, we propose a novel competence-based song recommendation framework for the purpose of singing. It is distinguished from most existing music recommendation systems which rely on the computation of listeners' interests or similarity. We model a singer's vocal competence as a singer profile, which takes voice pitch, intensity, and quality into consideration. Then we propose techniques to acquire singer profiles. We also present a song profile model which is used to construct a human annotated song database. Then we propose a learning-to-rank scheme for recommending songs by a singer profile. Finally, we introduce a reduced singer profile which can greatly simplify the vocal competence modelling process. The experimental study on real singers demonstrates the effectiveness of our approach and its advantages over two baseline methods.
C1 [Mao, Kuang; Chen, Gang] Zhejiang Univ, Coll Comp Sci, Database Lab, Hangzhou 310027, Zhejiang, Peoples R China.
   [Shou, Lidan] Zhejiang Univ, Coll Comp Sci, CAD & CG Lab, Hangzhou 310027, Zhejiang, Peoples R China.
   [Fan, Ju; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
C3 Zhejiang University; Zhejiang University; National University of
   Singapore
RP Mao, K (corresponding author), Zhejiang Univ, Coll Comp Sci, Database Lab, Hangzhou 310027, Zhejiang, Peoples R China.
EM mbill@zju.edu.cn; should@zju.edu.cn; fanj@comp.nus.edu.sg;
   cg@zju.edu.cn; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
FU National Basic Research Program (973 Program) [2015CB352400]; Singapore
   NRF under its IRC@SG Funding Initiative; National Science Foundation of
   China [61170034, 61472348]; National High Technology Research and
   Development Program of China [SS2013AA040601]; National Key Technology
   R&D Program of the Ministry of Science and Technology of China
   [2013BAG06B01]; Fundamental Research Funds for the Central Universities
FX This work was supported by the National Basic Research Program (973
   Program) under Grant 2015CB352400, the Singapore NRF under its IRC@SG
   Funding Initiative and administered by the IDMPO, the National Science
   Foundation of China under Grant 61170034 and Grant 61472348, the
   National High Technology Research and Development Program of China under
   Grant SS2013AA040601, the National Key Technology R&D Program of the
   Ministry of Science and Technology of China under Grant 2013BAG06B01,
   and the Fundamental Research Funds for the Central Universities. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Gokhan Tur.
CR [Anonymous], 2009, P 17 ACM INT C MULT
   Baeza-Yates R., 1999, MODERN INF RETRIEVAL
   Benetos E., 2012, ISMIR, P379
   Boersma P., 2006, VOICE
   Boersma P., Praat: doing phonetics by computer
   Cao Z., 2007, P 24 INT C MACHINE L, P129, DOI DOI 10.1145/1273496.1273513
   Deliyski D.D., 1993, P 3 C SPEECH COMMUNI, P1969
   Ghias A., 1995, P 3 ACM INT C MULT, P231, DOI DOI 10.1145/217279.215273
   GOLDBERG D, 1992, COMMUN ACM, V35, P61, DOI 10.1145/138859.138867
   Heylen L, 2002, J VOICE, V16, P1
   Heylen L. G., 1996, Acta Oto-Rhino-Laryngologica Belgica, V50, P299
   Hoashi K., 2003, MULTIMEDIA 03, P110
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Mao K, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P127, DOI 10.1145/2647868.2654921
   Mao K, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1009, DOI 10.1145/2348283.2348434
   Maryn Y, 2010, J VOICE, V24, P540, DOI 10.1016/j.jvoice.2008.12.014
   PABON JPH, 1991, J VOICE, V5, P203, DOI 10.1016/S0892-1997(05)80188-2
   PABON JPH, 1988, J SPEECH HEAR RES, V31, P710, DOI 10.1044/jshr.3104.710
   Peeters G., 2004, Tech. Rep.
   Quinlan J. R., 1993, PROGRAMS MACHINE LEA
   Schneider B, 2010, J VOICE, V24, P153, DOI 10.1016/j.jvoice.2008.07.007
   SCHUTTE HK, 1983, FOLIA PHONIATR, V35, P286, DOI 10.1159/000265703
   Shen JL, 2006, IEEE T MULTIMEDIA, V8, P1179, DOI 10.1109/TMM.2006.884618
   Shou LD, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P423
   Speyer R, 2003, J VOICE, V17, P544, DOI 10.1067/S0892-1997(03)00079-1
   SULTER AM, 1995, J VOICE, V9, P363, DOI 10.1016/S0892-1997(05)80198-5
   Tsai WH, 2012, IEEE T AUDIO SPEECH, V20, P1233, DOI 10.1109/TASL.2011.2174224
   Watts C, 2006, J VOICE, V20, P82, DOI 10.1016/j.jvoice.2004.12.003
   Wolf SK, 1935, J ACOUST SOC AM, V6, P255, DOI 10.1121/1.1915745
   Yu Y, 2013, IEEE T MULTIMEDIA, V15, P1969, DOI 10.1109/TMM.2013.2269313
   YUMOTO E, 1982, J ACOUST SOC AM, V71, P1544, DOI 10.1121/1.387808
   Zhang LM, 2015, IEEE T IND ELECTRON, V62, P1309, DOI 10.1109/TIE.2014.2336639
   Zhang LM, 2015, IEEE T IND ELECTRON, V62, P564, DOI 10.1109/TIE.2014.2327558
   Zhu Y, 2006, IEEE T MULTIMEDIA, V8, P575, DOI 10.1109/TMM.2006.870727
NR 34
TC 11
Z9 14
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 396
EP 408
DI 10.1109/TMM.2015.2392562
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700011
DA 2024-07-18
ER

PT J
AU Lou, ZY
   Gevers, T
AF Lou, Zhongyu
   Gevers, Theo
TI Extracting Primary Objects by Video Co-Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaussian mixture models (GMMs); graphical model; object proposal; video
   co-segmentation
AB Video object segmentation is a challenging problem. Without human annotation or other prior information, it is hard to select a meaningful primary object from a single video, so extracting the primary object across videos is a more promising approach. However, existing algorithms consider the problem as foreground/background segmentation. Therefore, we propose an algorithm that learns the model of the primary object by representing the frames/videos as a graphical model. The probabilistic graphical model is built across a set of videos based on an object proposal algorithm. Our approach considers appearance, spatial, and temporal consistency of the primary objects. A new dataset is created to evaluate the proposed method and to compare it to the state-of-the-art on video object co-segmentation. The experiments show that our method obtains state-of-the-art results, outperforming other algorithms by 1.5% (pixel accuracy) on the MOViCS dataset and 9.6% (pixel accuracy) on the new dataset.
C1 [Lou, Zhongyu; Gevers, Theo] Univ Amsterdam, Inst Informat, Intelligent Syst Lab Amsterdam, NL-1098 XH Amsterdam, Netherlands.
   [Gevers, Theo] Univ Autonoma Barcelona, Comp Vis Ctr, E-08193 Barcelona, Spain.
C3 University of Amsterdam; Autonomous University of Barcelona; Centre de
   Visio per Computador (CVC)
RP Lou, ZY (corresponding author), Univ Amsterdam, Inst Informat, Intelligent Syst Lab Amsterdam, NL-1098 XH Amsterdam, Netherlands.
EM fz.lou@uva.nll; th.geversg@uva.nl
CR Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   Arbeláez P, 2009, PROC CVPR IEEE, P2294, DOI 10.1109/CVPRW.2009.5206707
   BESAG J, 1986, J R STAT SOC B, V48, P259
   Chen D.-J., 2012, P 20 ACM INT C MULTI, P805, DOI [10.1145/2393347.2396317, DOI 10.1145/2393347.2396317]
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Chiu WC, 2013, PROC CVPR IEEE, P321, DOI 10.1109/CVPR.2013.48
   Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Kolmogorov V., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Ma TY, 2012, PROC CVPR IEEE, P670, DOI 10.1109/CVPR.2012.6247735
   Ochs P, 2011, IEEE I CONF COMP VIS, P1583, DOI 10.1109/ICCV.2011.6126418
   Rubio J.C., 2012, ASIAN C COMPUTER VIS, P13, DOI DOI 10.1007/978-3-642-37444-9
   Rudoy D, 2013, PROC CVPR IEEE, P1147, DOI 10.1109/CVPR.2013.152
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Valenti R, 2009, IEEE I CONF COMP VIS, P2185, DOI 10.1109/ICCV.2009.5459240
   Vazquez-Reina A, 2010, LECT NOTES COMPUT SC, V6315, P268, DOI 10.1007/978-3-642-15555-0_20
   Vicente S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2217, DOI 10.1109/CVPR.2011.5995530
   Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87
NR 21
TC 18
Z9 19
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2110
EP 2117
DI 10.1109/TMM.2014.2363936
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300003
DA 2024-07-18
ER

PT J
AU Liu, XL
   Hu, WJ
   Luo, C
   Pu, QF
   Wu, F
   Zhang, YG
AF Liu, Xiao Lin
   Hu, Wenjun
   Luo, Chong
   Pu, Qifan
   Wu, Feng
   Zhang, Yongguang
TI ParCast plus : Parallel Video Unicast in MIMO-OFDM WLANs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 8-MSAT - Multimedia Streaming and Transport; 8-WMMM - Wireless/Mobile
   Multimedia
ID ARCHITECTURE; SYSTEMS
AB We have observed two trends, growing wireless capability at the physical layer powered by MIMO-OFDM and growing video traffic as the dominant application traffic. Both the video source and MIMO-OFDM channel components exhibit nonuniform energy distribution. This has motivated us to leverage the source data redundancy at the channel to achieve high video recovery performance. We propose ParCast+ that first separates the source and the channel into independent components, matches the more important source components with higher-gain channel components, allocates power weights with joint consideration to the source and the channel, and uses pseudo-analog modulation for transmission. Such a scheme achieves fine-grained unequal error protection across source components. We implemented ParCast+ in Matlab and on Sora. Extensive evaluation has shown that our scheme outperforms competing schemes by notable margins, sometimes up to 6.4 dB in PSNR for challenging scenarios.
C1 [Liu, Xiao Lin] Univ Sci & Technol China, Hefei 230027, Peoples R China.
   [Hu, Wenjun; Zhang, Yongguang] Microsoft Res Asia, Wireless & Networking Grp, Beijing 100080, Peoples R China.
   [Luo, Chong; Wu, Feng] Microsoft Res Asia, Internet Media Grp, Beijing 100080, Peoples R China.
   [Pu, Qifan] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft Research Asia; Microsoft; Microsoft Research Asia;
   Microsoft; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Liu, XL (corresponding author), Article Numbering Ctr China, Beijing 100011, Peoples R China.
EM lin717@mail.ustc.edu.cn; wenjun.hu@yale.edu; cluo@microsoft.com;
   qifan@cs.berkeley.edu; fengwu@ustc.edu.cn; ygz@microsoft.com
RI Liu, Xiaolin/JXL-2314-2024; Hu, Wenjun/I-9392-2018; Wu,
   Feng/KCY-3017-2024
OI Liu, Xiaolin/0000-0003-2157-265X; 
CR Aditya Siripuram., 2011, ACM MOBICOM, P277
   [Anonymous], 2012, P 2012 VIS COMM IM P
   [Anonymous], H 264 MPEG 4 VIDEO C
   [Anonymous], 80211N2009 IEEE
   Chan RKW, 1997, INTERNATIONAL CONFERENCE ON VIRTUAL SYSTEMS AND MULTIMEDIA - VSMM'97, PROCEEDINGS, P188, DOI 10.1109/VSMM.1997.622346
   Chen CY, 2006, IEEE T SIGNAL PROCES, V54, P4004, DOI 10.1109/TSP.2006.880202
   Chen YH, 2008, IEEE T CIRC SYST VID, V18, P98, DOI 10.1109/TCSVT.2007.913759
   Halperin D, 2010, ACM SIGCOMM COMP COM, V40, P159, DOI 10.1145/1851275.1851203
   Hardy G., 1989, Inequalities, V2nd
   Jakubczak S., 2011, PROC MOBICOM, P289
   Ji Z., 2003, P IEEE ICC, P2298
   LEE KH, 1976, IEEE T COMMUN, V24, P1283
   Li MD, 2013, IEEE T MULTIMEDIA, V15, P1519, DOI 10.1109/TMM.2013.2267207
   Liu QA, 2010, IEEE INT CON MULTI, P968, DOI 10.1109/ICME.2010.5582986
   Liu XL, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P233
   Park G., 2005, SG16O ISOIEC JTC1SC2, V6
   PODILCHUK CI, 1995, IEEE T IMAGE PROCESS, V4, P125, DOI 10.1109/83.342187
   Rahul H, 2010, ACM SIGCOMM COMP COM, V40, P171, DOI 10.1145/1851275.1851204
   Rahult H, 2009, FIFTEENTH ACM INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM 2009), P193
   Sampath H., 1999, Conference Record of the Thirty-Third Asilomar Conference on Signals, Systems, and Computers (Cat. No.CH37020), P215, DOI 10.1109/ACSSC.1999.832325
   Song D, 2007, IEEE T CIRC SYST VID, V17, P1218, DOI 10.1109/TCSVT.2007.905531
   Song D, 2008, J VIS COMMUN IMAGE R, V19, P520, DOI 10.1016/j.jvcir.2008.06.008
   Sullivan G.J., 2010, SPIE OPT ENG APPL
   Tan K., 2009, Proceedings of the 6th USENIX Symposium on Networked Systems Design and Implementation, NSDI'09, P75
   Tse D., 2005, Fundementals of Wireless Communications
   VEMBU S, 1995, IEEE T INFORM THEORY, V41, P44, DOI 10.1109/18.370119
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   XIONG R, 2004, P PCS SAN FRANC CA D, P237
   Yahui Hu, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P206, DOI 10.1109/ICME.2012.1
   Yang Z, 2013, IEEE T CIRC SYST VID, V23, P212, DOI 10.1109/TCSVT.2012.2203216
   Zhao S.-Z., 2010, IEEE INT C ULR ICUWB, P1
NR 31
TC 40
Z9 41
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 2038
EP 2051
DI 10.1109/TMM.2014.2331616
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300020
DA 2024-07-18
ER

PT J
AU Egilmez, HE
   Tekalp, AM
AF Egilmez, Hilmi E.
   Tekalp, A. Murat
TI Distributed QoS Architectures for Multimedia Streaming Over Software
   Defined Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia streaming; network routing; openflow; quality-of-service
   (QoS); scalable video; Software Defined Network (SDN)
AB This paper presents novel QoS extensions to distributed control plane architectures for multimedia delivery over large-scale, multi-operator Software Defined Networks (SDNs). We foresee that large-scale SDNs shall be managed by a distributed control plane consisting of multiple controllers, where each controller performs optimal QoS routing within its domain and shares summarized (aggregated) QoS routing information with other domain controllers to enable inter-domain QoS routing with reduced problem dimensionality. To this effect, this paper proposes (i) topology aggregation and link summarization methods to efficiently acquire network topology and state information, (ii) a general optimization framework for flow-based end-to-end QoS provision over multi-domain networks, and (iii) two distributed control plane designs by addressing the messaging between controllers for scalable and secure inter-domain QoS routing. We apply these extensions to streaming of layered videos and compare the performance of different control planes in terms of received video quality, communication cost and memory overhead. Our experimental results show that the proposed distributed solution closely approaches the global optimum (with full network state information) and nicely scales to large networks.
C1 [Egilmez, Hilmi E.] Univ So Calif, Dept Elect Engn, Signal & Image Proc Inst, Los Angeles, CA 90089 USA.
   [Tekalp, A. Murat] Koc Univ, Coll Engn, TR-34450 Istanbul, Turkey.
C3 University of Southern California; Koc University
RP Egilmez, HE (corresponding author), Univ So Calif, Dept Elect Engn, Signal & Image Proc Inst, Los Angeles, CA 90089 USA.
EM hegilmez@usc.edu; mtekalp@ku.edu.tr
RI Tekalp, Murat/AAW-1060-2020
OI Tekalp, Ahmet Murat/0000-0003-1465-8121
FU TUBITAK [113E254]
FX This work was supported in part by the TUBITAK project 113E254. An
   earlier version of this paper was presented at the 19th IEEE
   International Conference on Image Processing, Orlando, FL, USA, October
   2012. The associate editor coordinating the review of this paper and
   approving it for publication was Dr. Shahram Shirani.
CR [Anonymous], 2012, P IEEE AS PAC SIGN I
   [Anonymous], 2012, ONF White Paper
   Blake S., 1998, RFC 2475 INTERNET EN
   Braden R., 1994, RFC 1633 INTERNET EN
   Dixit A., 2013, COMPUT COMMUN REV, V43, P7, DOI [DOI 10.1145/2491185.2491193, DOI 10.1145/2534169.2491193]
   Egilmez H. E., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2241, DOI 10.1109/ICIP.2011.6116083
   Egilmez HE, 2013, IEEE T MULTIMEDIA, V15, P710, DOI 10.1109/TMM.2012.2232645
   Egilmez HE, 2012, IEEE IMAGE PROC, P2237, DOI 10.1109/ICIP.2012.6467340
   Feamster N., 2013, OPEN NETW SUMMIT
   FROST VS, 1994, IEEE COMMUN MAG, V32, P70, DOI 10.1109/35.267444
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Guerin RA, 1997, GLOB TELECOMM CONF, P1903, DOI 10.1109/GLOCOM.1997.644603
   Heller Brandon., 2012, HOTSDN, P7
   Jüttner A, 2001, IEEE INFOCOM SER, P859, DOI 10.1109/INFCOM.2001.916277
   Kalman M, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P2015, DOI 10.1109/ICME.2004.1394659
   Kaune S, 2009, EUROMICRO WORKSHOP P, P301, DOI [10.1109/.43, 10.1109/PDP.2009.44]
   Kim W., 2010, INM/WREN, V10, P1
   Koponen T., 2010, P 9 USENIX C OP SYST, P351
   Kotronis V, 2012, PROCEEDINGS OF THE 11TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS-XI), P55
   Kuipers F, 2002, IEEE COMMUN MAG, V40, P50, DOI 10.1109/MCOM.2002.1106159
   Levin D., 2012, Proceedings of the first workshop on Hot topics in software defined networks, P1, DOI [DOI 10.1145/2342441.2342443, 10.1145/2342441.2342443]
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Open Networking Foundation (ONF) Palo Alto CA USA, OP FLOW SWITCH SPEC
   Raghavan B, 2012, PROCEEDINGS OF THE 11TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS-XI), P43
   Rekhter Y., 2006, RFC 4271 INTERNET EN
   Rosen E., 1999, RFC 2547 INTERNET EN
   SALTZER JH, 1984, ACM T COMPUT SYST, V2, P277, DOI 10.1145/357401.357402
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Tootoonchian A., 2010, HYPERFLOW DISTRIBUTE, P3
   Wang Z, 1996, IEEE J SEL AREA COMM, V14, P1228, DOI 10.1109/49.536364
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiao L, 2004, IEEE J SEL AREA COMM, V22, P1949, DOI 10.1109/JSAC.2004.836004
   Xiao Y, 2005, LECT NOTES COMPUT SC, V3827, P92, DOI 10.1007/11602613_11
   Zhang T., 2006, ICNS INT C NETW SERV, P80
NR 34
TC 91
Z9 95
U1 1
U2 49
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1597
EP 1609
DI 10.1109/TMM.2014.2325791
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200009
DA 2024-07-18
ER

PT J
AU Paisitkriangkrai, S
   Shen, CH
   van den Hengel, A
AF Paisitkriangkrai, Sakrapee
   Shen, Chunhua
   van den Hengel, Anton
TI Asymmetric Pruning for Learning Cascade Detectors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Asymmetric classification; asymmetric pruning; boosting; cascade
   classifier; feature selection; object detection
ID PEDESTRIAN DETECTION; OBJECT DETECTION; SPARSE; CLASSIFICATION;
   REGRESSION; TRACKING
AB Cascade classifiers are one of the most important contributions to real-time object detection. Nonetheless, there are many challenging problems arising in training cascade detectors. One common issue is that the node classifier is trained with a symmetric classifier. Having a low misclassification error rate does not guarantee an optimal node learning goal in cascade classifiers, i.e., an extremely high detection rate with a moderate false positive rate. In this work, we present a new approach to train an effective node classifier in a cascade detector. The algorithm is based on two key observations: 1) Redundant weak classifiers can be safely discarded; 2) The final detector should satisfy the asymmetric learning objective of the cascade architecture. To achieve this, we separate the classifier training into two steps: finding a pool of discriminative weak classifiers/features and training the final classifier by pruning weak classifiers which contribute little to the asymmetric learning criterion (asymmetric classifier construction). Our model reduction approach helps accelerate the learning time while achieving the pre-determined learning objective. Experimental results on both face and car data sets verify the effectiveness of the proposed algorithm. On the FDDB face data sets, our approach achieves the state-of-the-art performance, which demonstrates the advantage of our approach.
C1 [Paisitkriangkrai, Sakrapee; Shen, Chunhua] Univ Adelaide, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia.
   [van den Hengel, Anton] Univ Adelaide, Australian Ctr Visual Technol, Interdisciplinary Res Ctr, Adelaide, SA 5005, Australia.
C3 University of Adelaide; University of Adelaide
RP Paisitkriangkrai, S (corresponding author), Univ Adelaide, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia.
EM paul.pais@adelaide.edu.au; chunhua.shen@adelaide.edu.au;
   anton.vandenhengel@adelaide.edu.au
OI van den Hengel, Anton/0000-0003-3027-8364
FU ARC [FT120100969]
FX This work was supported in part by ARC Grant FT120100969. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Shin'ichi Satoh.
CR Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2005, PROC INT C MACH LEAR
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], NEURAL INFORM PROCES
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2007, P ADV NEUR INF PROC
   Bourdev L, 2005, PROC CVPR IEEE, P236, DOI 10.1109/cvpr.2005.310
   Chen YT, 2008, IEEE T IMAGE PROCESS, V17, P1452, DOI 10.1109/TIP.2008.926152
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092
   Destrero A, 2009, IEEE T IMAGE PROCESS, V18, P188, DOI 10.1109/TIP.2008.2007610
   Dundar M.M., 2007, 2007 IEEE Conference on Computer Vision and Pattern Recognition, P1
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Feris RS, 2012, IEEE T MULTIMEDIA, V14, P28, DOI 10.1109/TMM.2011.2170666
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Freund Y, 2001, MACH LEARN, V43, P293, DOI 10.1023/A:1010852229904
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Golub G.H., 2013, Matrix Computations, DOI DOI 10.56021/9781421407944
   Hastie T., 2009, The Elements of Statistical Learning
   Jain V., 2010, Fddb: A benchmark for face detection in unconstrained settings
   Li SZ, 2004, IEEE T PATTERN ANAL, V26, P1112, DOI 10.1109/TPAMI.2004.68
   Lienhart R, 2003, LECT NOTES COMPUT SC, V2781, P297
   Luo H., 2005, P IEEE C COMP VIS PA
   Marcellin M., 2000, P IEEE DAT COMPR C
   Masnadi-Shirazi H, 2011, IEEE T PATTERN ANAL, V33, P294, DOI 10.1109/TPAMI.2010.71
   Mikolajczyk K, 2004, P EUR C COMP VIS
   MOGHADDAM BABACK, 2006, P 23 INT C MACH LEAR, P641, DOI DOI 10.1145/1143844.1143925
   Nascimento JC, 2006, IEEE T MULTIMEDIA, V8, P761, DOI 10.1109/TMM.2006.876287
   Ozuysal M., 2009, P IEEE C COMP VIS PA
   Paisitkriangkcrai S, 2008, IEEE T CIRC SYST VID, V18, P1140, DOI 10.1109/TCSVT.2008.928213
   Paisitkriangkrai S, 2013, IEEE I CONF COMP VIS, P1057, DOI 10.1109/ICCV.2013.135
   Pallavi V, 2008, IEEE T MULTIMEDIA, V10, P794, DOI 10.1109/TMM.2008.922869
   Pham M.T., 2008, P IEEE C COMP VIS PA
   Porikli F, 2005, PROC CVPR IEEE, P829, DOI 10.1109/CVPR.2005.188
   Ross D, 2004, LECT NOTES COMPUT SC, V3022, P470
   Schapire RE, 1999, LECT NOTES ARTIF INT, V1720, P13
   Shen CH, 2011, IEEE T IMAGE PROCESS, V20, P22, DOI 10.1109/TIP.2010.2055880
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Venkatesh B. S., 2010, P EUR C COMP VIS WOR
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Viola P, 2002, ADV NEUR IN, V14, P1311
   Wang P., 2010, P AS C COMP VIS NZ
   Wang P, 2012, IEEE T NEUR NET LEAR, V23, P33, DOI 10.1109/TNNLS.2011.2178324
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu JX, 2008, IEEE T PATTERN ANAL, V30, P369, DOI 10.1109/TPAMI.2007.1181
   Wu JX, 2011, IEEE INT CONF ROBOT, P860
NR 48
TC 3
Z9 4
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1254
EP 1267
DI 10.1109/TMM.2014.2308723
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, XP
   Ren, YL
   Shen, LQ
   Qian, ZX
   Feng, GR
AF Zhang, Xinpeng
   Ren, Yanli
   Shen, Liquan
   Qian, Zhenxing
   Feng, Guorui
TI Compressing Encrypted Images With Auxiliary Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compression ratio-distortion performance; image compression; image
   encryption
AB This paper proposes a novel scheme of compressing encrypted images with auxiliary information. The content owner encrypts the original uncompressed images and also generates some auxiliary information, which will be used for data compression and image reconstruction. Then, the channel provider who cannot access the original content may compress the encrypted data by a quantization method with optimal parameters that are derived from a part of auxiliary information and a compression ratio-distortion criteria, and transmit the compressed data, which include an encrypted sub-image, the quantized data, the quantization parameters and another part of auxiliary information. At receiver side, the principal image content can be reconstructed using the compressed encrypted data and the secret key. Experimental result shows the ratio-distortion performance of the proposed scheme is better than that of previous techniques.
C1 [Zhang, Xinpeng; Ren, Yanli; Shen, Liquan; Qian, Zhenxing; Feng, Guorui] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 Shanghai University
RP Zhang, XP (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM xzhang@shu.edu.cn
RI Shen, Liquan/D-4832-2012; Qian, Zhenxing/AHC-9176-2022
FU National Natural Science Foundation of China [611038891, 61202367,
   61373151]; Research Fund for the Doctoral Program of Higher Education of
   China [20113108110010]; Program for Professor of Special Appointment
   (Eastern Scholar) at Shanghai Institutions of Higher Learning; Shanghai
   Pujiang Program [13PJ1403200]; Shanghai Rising-Star Program
   [14QA1401900]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 611038891, 61202367 and 61373151, the Research Fund
   for the Doctoral Program of Higher Education of China under Grant
   20113108110010, the Program for Professor of Special Appointment
   (Eastern Scholar) at Shanghai Institutions of Higher Learning, Shanghai
   Pujiang Program under Grant 13PJ1403200, and Shanghai Rising-Star
   Program under Grant 14QA1401900. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Alessandro Piva.
CR Anil Kumar A., 2008, 2008 IEEE 10th Workshop on Multimedia Signal Processing (MMSP), P760, DOI 10.1109/MMSP.2008.4665176
   [Anonymous], 2009, PROC TENCON 2009 IEE
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Erkin Z., 2007, EURASIP Journal on Information Security, V7, P1
   Jakimoski G, 2007, CONFERENCE RECORD OF THE FORTY-FIRST ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1-5, P901
   Johnson M, 2004, IEEE T SIGNAL PROCES, V52, P2992, DOI 10.1109/TSP.2004.833860
   Klinc Demijan, 2009, 2009 Data Compression Conference. DCC 2009, P213, DOI 10.1109/DCC.2009.71
   Kulkarni NS, 2009, STUD COMPUT INTELL, V231, P417
   Lazzeretti R., 2008, P 16 EUR SIGN PROC C
   Liu W, 2010, IEEE T IMAGE PROCESS, V19, P1097, DOI 10.1109/TIP.2009.2038773
   Schonberg D., 2005, 43 ANN ALL C ALL IL
   Schonberg D, 2008, IEEE T INF FOREN SEC, V3, P749, DOI 10.1109/TIFS.2008.2007244
   Siu-Wai Ho, 2011, IEEE Information Theory Workshop (ITW 2011), P653, DOI 10.1109/ITW.2011.6089527
   Xinpeng Zhang, 2011, 2011 Seventh International Conference on Intelligent Information Hiding and Multimedia Signal Processing, P222, DOI 10.1109/IIHMSP.2011.12
   Zhang XP, 2012, IEEE T IMAGE PROCESS, V21, P3108, DOI 10.1109/TIP.2012.2187671
   Zhang XP, 2011, IEEE T INF FOREN SEC, V6, P53, DOI 10.1109/TIFS.2010.2099114
NR 17
TC 40
Z9 44
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1327
EP 1336
DI 10.1109/TMM.2014.2315974
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600014
DA 2024-07-18
ER

PT J
AU Xu, YD
   Altman, E
   El-Azouzi, R
   Haddad, M
   Elayoubi, S
   Jimenez, T
AF Xu, Yuedong
   Altman, Eitan
   El-Azouzi, Rachid
   Haddad, Majed
   Elayoubi, Salaheddine
   Jimenez, Tania
TI Analysis of Buffer Starvation With Application to Objective QoE
   Optimization of Streaming Services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Starvation; start-up delay; quality of experience; ballot theorem
ID LOSS PROBABILITIES; REDUNDANT PACKETS
AB Our purpose in this paper is to characterize buffer starvations for streaming services. The buffer is modeled as a FIFO queue with exponential service time and Poisson arrivals. When the buffer is empty, the service restarts after a certain amount of packets are prefetched. With this goal, we propose two approaches to obtain exact distribution of the number of buffer starvations, one of which is based on Ballot theorem, and the other uses recursive equations. The Ballot theorem approach gives an explicit result. We extend this approach to the scenario with a constant playback rate using Takacs Ballot theorem. The recursive approach, though not offering an explicit result, allows us to obtain the distribution of starvations with non-independent and identically distributed (i.i.d.) arrival process in which an ON/OFF bursty arrival process is considered. We further compute the starvation probability as a function of the amount of prefetched packets for a large number of files via a fluid analysis. Among many potential applications of starvation analysis, we show how to apply it to optimize objective quality of experience (QoE) of media streaming, by exploiting the tradeoff between startup/rebuffering delay and starvations.
C1 [Xu, Yuedong] Fudan Univ, Dept Elect Engn, Shanghai 200433, Peoples R China.
   [Altman, Eitan; Haddad, Majed] INRIA Sophia Antipolis, Sophia Antipolis, France.
   [El-Azouzi, Rachid; Jimenez, Tania] Univ Avignon, Avignon, France.
   [Elayoubi, Salaheddine] Orange Labs, Paris, France.
C3 Fudan University; Avignon Universite; Orange SA
RP Xu, YD (corresponding author), Fudan Univ, Dept Elect Engn, Shanghai 200433, Peoples R China.
EM ydxu@fudan.edu.cn; eitan.altman@inria.fr;
   rachid.elazouzi@univ-avignon.fr; haddadmajed@yahoo.fr;
   salaheddine.elayoubi@orange.com; tania.jimenez@univ-avignon.fr
RI Jimenez, Tania/JJD-3074-2023
OI Elazouzi, Rachid/0000-0002-4756-0887
CR Abramowitz M., 1964, HDB MATH FUNCTIONS F
   Alcock S, 2011, ACM SIGCOMM COMP COM, V41, P25, DOI 10.1145/1971162.1971166
   Altman E, 1998, IEEE J SEL AREA COMM, V16, P778, DOI 10.1109/49.700912
   Altman E., 1995, P IEEE INFOCOM
   BACCELLI F, 1989, J APPL PROBAB, V26, P418, DOI 10.2307/3214049
   Box GEP, 1978, STAT EXPT, P130
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   CIDON I, 1993, IEEE T INFORM THEORY, V39, P98, DOI 10.1109/18.179347
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Dube P, 2003, PERFORM EVALUATION, V53, P147, DOI 10.1016/S0166-5316(03)00060-9
   Golub GH., 1986, MATRIX COMPUTATIONS, V2nd
   Gurewitz O, 2000, IEEE T INFORM THEORY, V46, P2588, DOI 10.1109/18.887866
   He JF, 2001, IEEE INFOCOM SER, P1075, DOI 10.1109/INFCOM.2001.916301
   Humblet P, 1993, IEEE ACM T NETWORK, V1, P81, DOI 10.1109/90.222909
   Krishnan S., 2012, P ACM USENIX INT MEA
   LEDERMANN W, 1954, PHILOS TR R SOC S-A, V246, P321, DOI 10.1098/rsta.1954.0001
   Liang GF, 2008, IEEE T MULTIMEDIA, V10, P1128, DOI 10.1109/TMM.2008.2001364
   Liu LM, 1996, J APPL PROBAB, V33, P815, DOI 10.2307/3215361
   Liu Y., P IEEE INF 2012
   Luan TH, 2010, IEEE T MULTIMEDIA, V12, P64, DOI 10.1109/TMM.2009.2036294
   Papoulis A., 1984, PROBABILITY RANDOM V, P360
   ParandehGheibi A, 2011, IEEE J SEL AREA COMM, V29, P1064, DOI 10.1109/JSAC.2011.110516
   Privalov AY, 2007, PROBL INF TRANSM, V43, P143, DOI 10.1134/S003294600702007X
   Stockhammer T, 2004, IEEE T MULTIMEDIA, V6, P268, DOI 10.1109/TMM.2003.822795
   Takacs L, 1962, PROBAB THEORY REL, V1, P154
   Xu Y. D., 2012, P IEEE INF 2012
   Xu Y. D., 2012, P IFIP NETW 2012
   Xu Y. D., 2013, P IEEE INF 2013
   Xu YD, 2009, WIRELESS PERS COMMUN, V51, P565, DOI 10.1007/s11277-009-9754-8
NR 29
TC 37
Z9 42
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 813
EP 827
DI 10.1109/TMM.2014.2300041
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, HS
   Zheng, YJ
   Zhang, ST
   Cheng, J
AF Li, Hongsheng
   Zheng, Yuanjie
   Zhang, Shaoting
   Cheng, Jian
TI Solving a Special Type of Jigsaw Puzzles: Banknote Reconstruction From a
   Large Number of Fragments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Agglomerative clustering; banknote reconstruction; jigsaw puzzle
   assembly; spectral embedding
AB In this paper, we propose a method to solve a special type of jigsaw puzzles, reconstructing banknotes from a large number of fragments based on fragments' images. Existing jigsaw puzzle assembly algorithms have difficulty solving this problem effectively. A main limitation of these methods is that they do not leverage the following important observations: 1) an intact banknote's image is known and thus can be used as prior information; 2) if two aligned fragments overlap each other, they must not be from a same banknote. Based on these two important observations, a three-step method is proposed to reconstruct banknotes from their fragments. Each fragment is first aligned to its original position on the banknote by a RANSAC method. After evaluating every two aligned fragments' relationships, all fragments are embedded into a lower dimensional space and then clustered into small groups using a modified agglomerative clustering method. Fragments in a same cluster are likely to be from a same banknote. Experiments on both synthetic and real data demonstrate the effectiveness of our proposed method.
C1 [Li, Hongsheng; Cheng, Jian] Univ Elect Sci & Technol China, Dept Informat Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Zheng, Yuanjie] Univ Penn, Dept Radiol, Philadelphia, PA 19014 USA.
   [Zhang, Shaoting] Univ N Carolina, Dept Comp Sci, Charlotte, NC 28223 USA.
C3 University of Electronic Science & Technology of China; University of
   Pennsylvania; University of North Carolina; University of North Carolina
   Charlotte
RP Cheng, J (corresponding author), Univ Elect Sci & Technol China, Dept Informat Engn, Chengdu 611731, Sichuan, Peoples R China.
EM lihongsheng@gmail.com; yuanjie.zheng@uphs.upenn.edu;
   shaoting@cs.rutgers.edu; chengjian@uestc.edu.cn
RI Li, Hongsheng/AES-5328-2022; li, hs/HNI-7007-2023; li, he/ISB-4278-2023
OI Li, Hongsheng/0000-0002-2664-7975; Zhang, Shaoting/0000-0002-8719-448X
FU National Science Foundation of China [61301269, 61201271]; Research Fund
   for the Doctoral Program of Higher Education [20100185120021];
   Scientific and Technological Cooperation Program; Academy of China and
   Sichuan Province [2012JZ0001]
FX This work was supported in part by the National Science Foundation of
   China (No. 61301269 and 61201271), in part by the Research Fund for the
   Doctoral Program of Higher Education (No. 20100185120021), and in part
   by the Scientific and Technological Cooperation Program with the Academy
   of China and Sichuan Province (No. 2012JZ0001). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Cees Snoek.
CR Alajlan Naif, 2009, American Journal of Applied Sciences, V6, P1941, DOI 10.3844/ajassp.2009.1941.1947
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P ACM SIAM S DISCR A
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   Chung MG, 1998, ICSP '98: 1998 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, PROCEEDINGS, VOLS I AND II, P877, DOI 10.1109/ICOSP.1998.770751
   Fränti P, 2006, IEEE T PATTERN ANAL, V28, P1875, DOI 10.1109/TPAMI.2006.227
   FREEMAN H, 1964, IEEE T COMPUT, VEC13, P118, DOI 10.1109/PGEC.1964.263781
   Goldberg D, 2004, COMP GEOM-THEOR APPL, V28, P165, DOI 10.1016/j.comgeo.2004.03.007
   Justino E, 2006, FORENSIC SCI INT, V160, P140, DOI 10.1016/j.forsciint.2005.09.001
   KOSIBA DA, 1994, INT C PATT RECOG, P616, DOI 10.1109/ICPR.1994.576377
   Liu HR, 2011, IEEE T MULTIMEDIA, V13, P1154, DOI 10.1109/TMM.2011.2160845
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Makridis M, 2006, IEEE IMAGE PROC, P2001, DOI 10.1109/ICIP.2006.312891
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nielsen TR, 2008, PATTERN RECOGN LETT, V29, P1924, DOI 10.1016/j.patrec.2008.05.027
   Richter F, 2013, IEEE T MULTIMEDIA, V15, P582, DOI 10.1109/TMM.2012.2235415
   Sagiroglu MS, 2006, INT C PATT RECOG, P1036
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   WEBSTER RW, 1991, IEEE T SYST MAN CYB, V21, P1271, DOI 10.1109/21.120080
   Wolfson H., 1988, Annals of Operations Research, V12, P51, DOI 10.1007/BF02186360
   Yao FH, 2003, PATTERN RECOGN LETT, V24, P1819, DOI 10.1016/S0167-8655(03)00006-0
NR 26
TC 11
Z9 11
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 571
EP 578
DI 10.1109/TMM.2013.2291968
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800024
DA 2024-07-18
ER

PT J
AU Wu, F
   Yu, Z
   Yang, Y
   Tang, SL
   Zhang, Y
   Zhuang, YT
AF Wu, Fei
   Yu, Zhou
   Yang, Yi
   Tang, Siliang
   Zhang, Yin
   Zhuang, Yueting
TI Sparse Multi-Modal Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dictionary learning; multi-modal hashing; sparse coding
ID NEAREST-NEIGHBOR; QUANTIZATION; SHRINKAGE
AB Learning hash functions across heterogenous high-dimensional features is very desirable for many applications involving multi-modal data objects. In this paper, we propose an approach to obtain the sparse codesets for the data objects across different modalities via joint multi-modal dictionary learning, which we call sparse multi-modal hashing (abbreviated as). In, both intra-modality similarity and inter-modality similarity are first modeled by a hypergraph, then multi-modal dictionaries are jointly learned by Hypergraph Laplacian sparse coding. Based on the learned dictionaries, the sparse codeset of each data object is acquired and conducted for multi-modal approximate nearest neighbor retrieval using a sensitive Jaccard metric. The experimental results show that outperforms other methods in terms of mAP and Percentage on two real-world data sets.
C1 [Wu, Fei; Yu, Zhou; Tang, Siliang; Zhang, Yin; Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310003, Zhejiang, Peoples R China.
   [Yang, Yi] Univ Queensland, ITEE, Brisbane, Qld, Australia.
C3 Zhejiang University; University of Queensland
RP Wu, F (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310003, Zhejiang, Peoples R China.
EM wufei@cs.zju.edu.cn; yuz@zju.edu.cn; yi.yang@uq.edu.au;
   siliang.tang@gmail.com; zhangyin98@zju.edu.cn; yzhuang@cs.zju.edu.cn
RI yang, yang/HGT-7999-2022; yang, yang/GWB-9426-2022; Lang,
   Ming/HIK-0758-2022; yang, yang/GVT-5210-2022; Yang, Yi/B-9273-2017
OI Yang, Yi/0000-0002-0512-880X; Yu, Zhou/0000-0001-8407-1137
FU 973 Program [2012CB316400]; NSFC [61070068, 90920303, 61103099]; 863
   program [2012AA012505]; Chinese Knowledge Center of Engineering Science
   and Technology (CKCEST); China Academic Digital Associative Library
   (CADAL)
FX This work was supported by 973 Program (No. 2012CB316400), NSFC
   (61070068, 90920303, 61103099), 863 program (2012AA012505), Chinese
   Knowledge Center of Engineering Science and Technology (CKCEST), and
   China Academic Digital Associative Library (CADAL). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Chong-Wah Ngo.
CR Ai LF, 2013, J ZHEJIANG U-SCI C, V14, P505, DOI 10.1631/jzus.CIDE1304
   Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   [Anonymous], ACM MULTIMEDIA
   [Anonymous], 2008, Advances in Neural Information Processing Systems
   [Anonymous], 2011, MALSAR: Multi-Task Learning via Structural Regularization
   Anoop Cherian V. M., 2012, P INT C IM PROC
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Berchtold S., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), P577, DOI 10.1109/ICDE.2000.839456
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Cao LL, 2009, IEEE I CONF COMP VIS, P1095, DOI 10.1109/ICCV.2009.5459401
   Chum O., 2008, BMVC, P812, DOI DOI 10.5244/C.22.50
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Gao SH, 2013, IEEE T PATTERN ANAL, V35, P92, DOI 10.1109/TPAMI.2012.63
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Huang YC, 2010, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2010.5540012
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jia K, 2013, IEEE T PATTERN ANAL, V35, P367, DOI 10.1109/TPAMI.2012.95
   Jia Y., 2010, Adv. Neural Inf. Process. Syst., P982
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Joly A, 2011, PROC CVPR IEEE, P873, DOI 10.1109/CVPR.2011.5995709
   Kong WH, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P45, DOI 10.1145/2348283.2348293
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Liu W, 2012, ARXIV12064618
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu Wei, 2011, Reports in Parasitology, V1, P1
   Liu Y, 2012, IEEE T IMAGE PROCESS, V21, P4480, DOI 10.1109/TIP.2012.2207394
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu XY, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P433
   Lv Q, 2007, P 33 INT C VER LARG, P950
   Monaci G, 2007, IEEE T IMAGE PROCESS, V16, P2272, DOI 10.1109/TIP.2007.901813
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Wang C., 2010, P 18 ACM INT C MULT, P391, DOI [10.1145/ 1873951.1874005, DOI 10.1145/1873951.1874005]
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu F., 2010, Proceedings of the International Conference on Multimedia (MM'10), P15, DOI DOI 10.1145/1873951.1873957
   Wu F, 2006, IEEE IMAGE PROC, P1465, DOI 10.1109/ICIP.2006.312707
   Yang M, 2010, IEEE IMAGE PROC, P1601, DOI 10.1109/ICIP.2010.5652363
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zhang D, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P18
   Zhen Y., 2012, P ACM SIGKDD INT C K
   Zhou D., 2006, ADV NEURAL INFORM PR, P1601, DOI DOI 10.7551/MITPRESS/7503.003.0205
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
   Zhuang Yueting., 2011, ACM MM, P1457
NR 51
TC 114
Z9 123
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 427
EP 439
DI 10.1109/TMM.2013.2291214
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800012
DA 2024-07-18
ER

PT J
AU Momcilovic, S
   Ilic, A
   Roma, N
   Sousa, L
AF Momcilovic, Svetislav
   Ilic, Aleksandar
   Roma, Nuno
   Sousa, Leonel
TI Dynamic Load Balancing for Real-Time Video Encoding on Heterogeneous CPU
   plus GPU Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video Coding; GPGPU; Hybrid CPU plus GPU System; Load Balancing
ID MOTION ESTIMATION; DESIGN; IMPLEMENTATION
AB The high computational demands and overall encoding complexity make the processing of high definition video sequences hard to be achieved in real-time. In this manuscript, we target an efficient parallelization and RD performance analysis of H.264/AVC inter-loop modules and their collaborative execution in hybrid multi-core CPU and multi-GPU systems. The proposed dynamic load balancing algorithm allows efficient and concurrent video encoding across several heterogeneous devices by relying on realistic run-time performance modeling and module-device execution affinities when distributing the computations. Due to an online adjustment of load balancing decisions, this approach is also self-adaptable to different execution scenarios. Experimental results show the proposed algorithm's ability to achieve real-time encoding for different resolutions of high-definition sequences in various heterogeneous platforms. Speed-up values of up to 2.6 were obtained when compared to the video inter-loop encoding on a single GPU device, and up to 8.5 when compared to a highly optimized multi-core CPU execution. Moreover, the proposed algorithm also provides an automatic tuning of the encoding parameters, in order to meet strict encoding constraints.
C1 [Momcilovic, Svetislav; Ilic, Aleksandar; Roma, Nuno; Sousa, Leonel] INESC ID IST TU, P-1000029 Lisbon, Portugal.
C3 Universidade de Lisboa; INESC-ID
RP Momcilovic, S (corresponding author), INESC ID IST TU, P-1000029 Lisbon, Portugal.
EM Svetislav.Momcilovic@inesc-id.pt; Aleksandar.Ilic@inesc-id.pt;
   Nuno.Roma@inesc-id.pt; Leonel.Sousa@inesc-id.pt
RI Roma, Nuno/C-5586-2008; Roma, Nuno/AAD-1997-2022; Ilic,
   Aleksandar/AAF-3831-2021; Sousa, Leonel/B-2749-2009; Momcilovic,
   Svetislav/L-7591-2015
OI Roma, Nuno/0000-0003-2491-4977; Roma, Nuno/0000-0003-2491-4977; Ilic,
   Aleksandar/0000-0002-8594-3539; Sousa, Leonel/0000-0002-8066-221X;
   Momcilovic, Svetislav/0000-0002-4529-7632
FU FCT-Fundacao para a Ciencia e a Tecnologia [PEst-OE/EEI/LA0021/2013,
   PTDC/EEI-ELC/3152/2012, PTDC/EEA-ELC/117329/2010]; Fundação para a
   Ciência e a Tecnologia [PTDC/EEI-ELC/3152/2012,
   PTDC/EEA-ELC/117329/2010] Funding Source: FCT
FX This work was supported by national funds through FCT-Fundacao para a
   Ciencia e a Tecnologia, under projects PEst-OE/EEI/LA0021/2013,
   PTDC/EEI-ELC/3152/2012 and PTDC/EEA-ELC/117329/2010. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Yen-Kuang Chen.
CR [Anonymous], 2011, OPENCL SPEC V1 2
   [Anonymous], 2013, REFERENCE SOFTWARE X
   [Anonymous], J REAL TIME IMAGE PR
   [Anonymous], TECHNICAL REPORT
   [Anonymous], RES REPORT
   [Anonymous], J REAL TIME IMAGE PR
   Ates HF, 2005, INT CONF ACOUST SPEE, P905, DOI 10.1109/ICASSP.2005.1415552
   Augonnet C, 2011, CONCURR COMP-PRACT E, V23, P187, DOI 10.1002/cpe.1631
   Barlas G, 2011, EUROMICRO WORKSHOP P, P247, DOI 10.1109/PDP.2011.51
   Bharadwaj V., 2003, Cluster Computing, V6, P7, DOI 10.1023/A:1020958815308
   Bjontegaard G., 2001, VCEGM33 ITU TEL STAN
   Chen D., 2013, P INT S COMP COMM CO, P6
   Chen WN, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P697
   Chen Z., 2005, J VISUAL COMMUNICATI, P264
   Chen Z, 2012, J COMPUT, V7, P341, DOI [10.4304/jcp.7.2.341-348, 10.4304/jcp.7.2.341-48]
   Cheung NM, 2010, IEEE SIGNAL PROC MAG, V27, P79, DOI 10.1109/MSP.2009.935416
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Fung WWL, 2007, INT SYMP MICROARCH, P407, DOI 10.1109/MICRO.2007.30
   Gao Y., 2012, MULTIMED TOOLS APPL, P1
   Garland M, 2008, IEEE MICRO, V28, P13, DOI 10.1109/MM.2008.57
   Huang Y.-L., 2009, P 17 INT C MULTIMEDI, P361
   Ilic A., 2012, 2012 IEEE 10th International Symposium on Parallel and Distributed Processing with Applications (ISPA), P683, DOI 10.1109/ISPA.2012.101
   Ilic A, 2012, LECT NOTES COMPUT SC, V7155, P491, DOI 10.1007/978-3-642-29737-3_54
   Lastovetsky A, 2010, LECT NOTES COMPUT SC, V6043, P91
   Lee CY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1603
   Li P, 2005, IEEE T CIRC SYST VID, V15, P1098, DOI 10.1109/TCSVT.2005.852627
   Marchal L, 2006, INT J HIGH PERFORM C, V20, P365, DOI 10.1177/1094342006067474
   Momcilovic S., 2012, P 10 INT WORKSH ALG, P165
   Momcilovic S, 2009, IEEE WRK SIG PRO SYS, P291, DOI 10.1109/SIPS.2009.5336269
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Pieters B., 2009, P SOC PHOTOOPTICAL I, V7443, P12
   Pieters B., 2007, P 8 INT WORKSH IM AN, P69
   Pieters B, 2011, IEEE T CIRC SYST VID, V21, P96, DOI 10.1109/TCSVT.2011.2105553
   Rodríguez-Sánchez R, 2012, LECT NOTES COMPUT SC, V7131, P551
   Schwalb M, 2009, IEEE T MULTIMEDIA, V11, P1, DOI 10.1109/TMM.2008.2008873
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tan T., 2008, VCEGAI10 ITU TEL STA
   Tourapis A. M., 2009, JVTAD010 JOINT VID T
   Tourapis AM, 2002, PROC SPIE, V4671, P1069, DOI 10.1117/12.453031
   Wang B., 2013, P EUR 2012 WORKSH, V7640, P155
   Wei HT, 2010, COMPUT SCI INF SYST, V7, P189, DOI 10.2298/CSIS1001189W
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu N, 2012, COMM COM INF SC, V346, P420
   Zhang JL, 2012, IEEE I C EMBED SOFTW, P41, DOI 10.1109/HPCC.2012.16
   Zhang LY, 2012, 2012 INTERNATIONAL CONFERENCE ON INDUSTRIAL CONTROL AND ELECTRONICS ENGINEERING (ICICEE), P767, DOI 10.1109/ICICEE.2012.205
   Zhao Z., 2006, P INT S CIRC SYST IS
NR 46
TC 27
Z9 29
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 108
EP 121
DI 10.1109/TMM.2013.2284892
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100010
DA 2024-07-18
ER

PT J
AU Belyaev, E
   Egiazarian, K
   Gabbouj, M
AF Belyaev, Evgeny
   Egiazarian, Karen
   Gabbouj, Moncef
TI A Low-Complexity Bit-Plane Entropy Coding and Rate Control for 3-D DWT
   Based Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Entropy coding; rate control; 3-D DWT.
AB This paper is dedicated to fast video coding based on three-dimensional discrete wavelet transform. First, we propose a novel low-complexity bit-plane entropy coding of wavelet subbands based on Levenstein zero-run coder for low entropy contexts and adaptive binary range coder for other contexts. Second, we propose a rate-distortion efficient criterion for skipping 2-D wavelet transforms and entropy encoding based on parent-child subband tree. Finally, we propose one pass rate control which uses virtual buffer concept for adaptive Lagrange multiplier selection. Simulations results show that the proposed video codec has a much lower computational complexity (from 2 to 6 times) for the same quality level compared to the H. 264/AVC standard in the low complexity mode.
C1 [Belyaev, Evgeny; Egiazarian, Karen; Gabbouj, Moncef] Tampere Univ, Dept Signal Proc, FIN-33101 Tampere, Finland.
C3 Tampere University
RP Belyaev, E (corresponding author), Tampere Univ, Dept Signal Proc, FIN-33101 Tampere, Finland.
EM evgeny.belyaev@tut.fi; karen.egiazarian@tut.fi; moncef.gabbouj@tut.fi
RI Egiazarian, Karen Onikovich/D-9466-2014; Belyaev, Evgeny/C-9132-2017;
   Eguiazarian, Karen/G-4299-2014; Gabbouj, Moncef/G-4293-2014
OI Belyaev, Evgeny/0000-0003-1245-0140; Gabbouj,
   Moncef/0000-0002-9788-2323; , Karen/0000-0002-8135-1085
CR Andreopoulos Y, 2004, SIGNAL PROCESS-IMAGE, V19, P653, DOI 10.1016/j.image.2004.05.007
   [Anonymous], 2009, 2000 JPEG
   Belyaev E., 2011, P 11 INT C NEXT GEN
   Belyaev E., 2010, P INT C ULTR TEL
   Belyaev E., 2012, P INT S SPIE EL IM
   Chen PS, 2004, IEEE T CIRC SYST VID, V14, P1183, DOI 10.1109/TCSVT.2004.833165
   Eeckhaut H, 2007, IEEE T MULTIMEDIA, V9, P1508, DOI 10.1109/TMM.2007.906606
   Gu ZY, 2012, IEEE T IMAGE PROCESS, V21, P674, DOI 10.1109/TIP.2011.2166969
   Hua JP, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P569, DOI 10.1109/MMSP.2001.962793
   ITU-T and ISO/IEC JTC 1, 2000, JPEG 2000 1
   Kim BJ, 2000, IEEE T CIRC SYST VID, V10, P1374, DOI 10.1109/76.889025
   Li X, 2011, IEEE T CIRC SYST VID, V21, P957, DOI 10.1109/TCSVT.2011.2133750
   López O, 2011, SIGNAL PROCESS-IMAGE, V26, P358, DOI 10.1016/j.image.2011.01.008
   Marpe D., 2003, P IEEE INT C IM PROC
   Mehrseresht N, 2006, IEEE T IMAGE PROCESS, V15, P740, DOI 10.1109/TIP.2005.860619
   Munteanu A, 2010, IEEE T VIS COMPUT GR, V16, P513, DOI 10.1109/TVCG.2009.90
   Penna B, 2006, IEEE GEOSCI REMOTE S, V3, P125, DOI 10.1109/LGRS.2005.859942
   Reibman AR, 1992, IEEE T CIRC SYST VID, V2, P361, DOI 10.1109/76.168912
   Schuster G., 1997, RATE DISTORTION BASE
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   TAUBMAN D, 1994, IEEE T IMAGE PROCESS, V3, P572, DOI 10.1109/83.334984
   Tham JY, 1998, IEEE J SEL AREA COMM, V16, P12, DOI 10.1109/49.650917
   Turaga DS, 2005, IEEE T CIRC SYST VID, V15, P982, DOI 10.1109/TCSVT.2005.852399
   van der Schaar M, 2006, IEEE T MOBILE COMPUT, V5, P755, DOI 10.1109/TMC.2006.81
   Xiong RQ, 2007, IEEE T CIRC SYST VID, V17, P1311, DOI 10.1109/TCSVT.2007.903550
   Xiong RQ, 2007, IEEE T CIRC SYST VID, V17, P1256, DOI 10.1109/TCSVT.2007.905507
   Xu J., 2001, Journal of Applied and Computational Harmonic Analysis, V10, P290
   Xuesong J., 2011, P 2011 3 INT C ADV C, P309
   YANG F, 2007, P IEEE INT C IM PROC, P309
   Zhang DD, 2008, IEEE T CIRC SYST VID, V18, P516, DOI 10.1109/TCSVT.2008.918536
NR 31
TC 16
Z9 17
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1786
EP 1799
DI 10.1109/TMM.2013.2269315
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900006
DA 2024-07-18
ER

PT J
AU Deena, S
   Hou, SB
   Galata, A
AF Deena, Salil
   Hou, Shaobo
   Galata, Aphrodite
TI Visual Speech Synthesis Using a Variable-Order Switching Shared Gaussian
   Process Dynamical Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Artificial Talking Head; speech-driven facial animation; visual speech
   synthesis.
ID ANIMATION; ALGORITHMS; FACE
AB In this paper, we present a novel approach to speech-driven facial animation using a non-parametric switching state space model based on Gaussian processes. The model is an extension of the shared Gaussian process dynamical model, augmented with switching states. Two talking head corpora are processed by extracting visual and audio data from the sequences followed by a parameterization of both data streams. Phonetic labels are obtained by performing forced phonetic alignment on the audio. The switching states are found using a variable length Markov model trained on the labelled phonetic data. The audio and visual data corresponding to phonemes matching each switching state are extracted and modelled together using a shared Gaussian process dynamical model. We propose a synthesis method that takes into account both previous and future phonetic context, thus accounting for forward and backward coarticulation in speech. Both objective and subjective evaluation results are presented. The quantitative results demonstrate that the proposed method outperforms other state-of-the-art methods in visual speech synthesis and the qualitative results reveal that the synthetic videos are comparable to ground truth in terms of visual perception and intelligibility.
C1 [Deena, Salil; Hou, Shaobo; Galata, Aphrodite] Univ Manchester, Sch Comp Sci, Manchester, Lancs, England.
C3 University of Manchester
RP Deena, S (corresponding author), Univ Manchester, Sch Comp Sci, Manchester, Lancs, England.
OI Galata, Aphrodite/0000-0002-9229-7811
CR [Anonymous], THESIS U UTAH SALT L
   [Anonymous], 2005, Nonlinear Signal Processing: A Statistical Approach
   Blanz Volker, 1999, SIGGRAPH 99 SIGGRAPH 99
   Brand M., 1999, SIGGRAPH 99
   BREGLER C, 1997, SIGGRAPH 97
   C H Ek, 2008, MLMI 08
   Cao Y, 2005, ACM T GRAPHIC, V24, P1283, DOI 10.1145/1095878.1095881
   Cao Y., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P225
   Cao Y., 2004, SCA 04
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cosatto E, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P619, DOI 10.1109/ICME.2000.871439
   COSKER D, 2005, ACM T APPL PERCEPT, V2, P270
   Deena S., 2009, ISVC 09
   Deena S., 2010, ICMI MLMI 10
   Deng ZG, 2006, IEEE T VIS COMPUT GR, V12, P1523, DOI 10.1109/TVCG.2006.90
   Deng Zhigang., 2007, DATA DRIVEN 3D FACIA
   Edge JD, 2009, EURASIP J AUDIO SPEE, DOI 10.1155/2009/597267
   Ek C. H., 2007, MLMI 07
   Englebienne G., 2007, P NIPS 07 ADV NEUR I
   Ezzat T., 2002, SIGGRAPH 02
   Galata A, 2001, COMPUT VIS IMAGE UND, V81, P398, DOI 10.1006/cviu.2000.0894
   Geiger G., 2003, 224AI CBCL, P2003
   Govokhina O., 2006, P INTERSPEECH
   Guyon I., 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P454, DOI 10.1109/ICDAR.1995.599034
   Hardcastle WilliamJ., 2000, COARTICULATION THEOR
   Hilder S., 2009, AVSP 09
   Hollander M., 2014, Nonparametric Statistical Methods, Solutions Manual, Vthird
   Hyvärinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722
   Jixu Chen, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2655, DOI 10.1109/CVPRW.2009.5206580
   Kshirsagar S., 2003, P EUR COMP GRAPH FOR, V22
   Lawrence N. D., 2007, AISTATS 07
   Lehn-Schioler T., 2005, MLMI 04
   Lehn-Schioler T., 2005, THESIS TU DENMARK LY
   Ma JY, 2006, IEEE T VIS COMPUT GR, V12, P266, DOI 10.1109/TVCG.2006.18
   Mariooryad S, 2012, IEEE T AUDIO SPEECH, V20, P2329, DOI 10.1109/TASL.2012.2201476
   Mattheyses W., 2010, P INTERSPEECH
   Mattheyses Wesley., 2010, AVSP 10 PROC INT C A, P148
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Ron D, 1996, MACH LEARN, V25, P117, DOI 10.1007/BF00114008
   Rothweiler J. H., 1983, Proceedings of ICASSP 83. IEEE International Conference on Acoustics, Speech and Signal Processing, P1280
   Salzmann Mathieu, 2010, P 13 INT C ARTIFICIA, V9, P701
   Shannon M., 2011, P INTERSPEECH
   Tekalp AM, 2000, SIGNAL PROCESS-IMAGE, V15, P387, DOI 10.1016/S0923-5965(99)00055-7
   THEOBALD B, 2003, THESIS U E ANGLIA NO
   Theobald B.-J., 2008, P INTERSPEECH
   Theobald BJ, 2012, IEEE T AUDIO SPEECH, V20, P2378, DOI 10.1109/TASL.2012.2202651
   Tokuda K, 2000, INT CONF ACOUST SPEE, P1315, DOI 10.1109/ICASSP.2000.861820
   Urtasun R., 2006, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2006.15
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Wang LJ, 2011, INT CONF ACOUST SPEE, P4580
   Wang LJ, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P446
   Weid H., CMU PRONUNCIATION DI
   Xie L, 2007, PATTERN RECOGN, V40, P2325, DOI 10.1016/j.patcog.2006.12.001
   Xie L, 2007, IEEE T MULTIMEDIA, V9, P500, DOI 10.1109/TMM.2006.888009
   Xuan Huang, 2021, 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), P867, DOI 10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00144
   Young S.J., 1993, HTK HIDDEN MARKOV MO
   Zhou ZH, 2012, IEEE T CIRC SYST VID, V22, P1420, DOI 10.1109/TCSVT.2012.2199399
   Zhuang XD, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P1736
NR 59
TC 9
Z9 9
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1755
EP 1768
DI 10.1109/TMM.2013.2279659
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900004
DA 2024-07-18
ER

PT J
AU Evangelopoulos, G
   Zlatintsi, A
   Potamianos, A
   Maragos, P
   Rapantzikos, K
   Skoumas, G
   Avrithis, Y
AF Evangelopoulos, Georgios
   Zlatintsi, Athanasia
   Potamianos, Alexandros
   Maragos, Petros
   Rapantzikos, Konstantinos
   Skoumas, Georgios
   Avrithis, Yannis
TI Multimodal Saliency and Fusion for Movie Summarization Based on Aural,
   Visual, and Textual Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention; audio saliency; fusion; movie summarization; multimodal
   saliency; multistream processing; text saliency; video summarization;
   visual saliency
ID AUDITORY ATTENTION; VIDEO ABSTRACTION; MODEL; FRAMEWORK; IMAGE;
   MECHANISMS; SEPARATION; FREQUENCY; TRACKING; FEATURES
AB Multimodal streams of sensory information are naturally parsed and integrated by humans using signal-level feature extraction and higher level cognitive processes. Detection of attention-invoking audiovisual segments is formulated in this work on the basis of saliency models for the audio, visual, and textual information conveyed in a video stream. Aural or auditory saliency is assessed by cues that quantify multifrequency waveform modulations, extracted through nonlinear operators and energy tracking. Visual saliency is measured through a spatiotemporal attention model driven by intensity, color, and orientation. Textual or linguistic saliency is extracted from part-of-speech tagging on the subtitles information available with most movie distributions. The individual saliency streams, obtained from modality-depended cues, are integrated in a multimodal saliency curve, modeling the time-varying perceptual importance of the composite video stream and signifying prevailing sensory events. The multimodal saliency representation forms the basis of a generic, bottom-up video summarization algorithm. Different fusion schemes are evaluated on a movie database of multimodal saliency annotations with comparative results provided across modalities. The produced summaries, based on low-level features and content-independent fusion and selection, are of subjectively high aesthetic and informative quality.
C1 [Evangelopoulos, Georgios; Zlatintsi, Athanasia; Maragos, Petros; Rapantzikos, Konstantinos; Skoumas, Georgios; Avrithis, Yannis] Natl Tech Univ Athens, Sch Elect & Comp Engn, GR-15773 Athens, Greece.
   [Potamianos, Alexandros] Tech Univ Crete, Dept Elect & Comp Engn, GR-73100 Khania, Greece.
C3 National Technical University of Athens; Technical University of Crete
RP Evangelopoulos, G (corresponding author), Natl Tech Univ Athens, Sch Elect & Comp Engn, GR-15773 Athens, Greece.
EM gevag@cs.ntua.gr; nzlat@cs.ntua.gr; potam@telecom.tuc.gr;
   maragos@cs.ntua.gr; rap@image.ntua.gr; gskoumas@dblab.ece.ntua.gr;
   iavr@image.ntua.gr
RI Rapantzikos, Konstantinos E/B-3755-2009; Evangelopoulos,
   Georgios/B-8471-2015
OI Evangelopoulos, Georgios/0000-0003-2240-1801
FU project COGN-IMUSE; European Social Fund (ESF); EU project DIRHA
   [FP7-ICT-2011-7-288121]; Greek national funds through the Operational
   Program Education and Lifelong Learning of the National Strategic
   Reference Framework-Research Funding Program: Heracleitus II; National
   Resources
FX This work was supported in part by the project COGN-IMUSE which is
   implemented under the ARISTEIA Action of the Operational Program
   Education and Lifelong Learning and is cofunded by the European Social
   Fund (ESF) and National Resources and in part by the EU project DIRHA
   with Grant FP7-ICT-2011-7-288121. The work of A. Zlatintsi was supported
   by the ESF and Greek national funds through the Operational Program
   Education and Lifelong Learning of the National Strategic Reference
   Framework-Research Funding Program: Heracleitus II. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Christophe De Vleeschouwer.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Amari SI, 2007, NEURAL COMPUT, V19, P2780, DOI 10.1162/neco.2007.19.10.2780
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   BOVIK AC, 1993, IEEE T SIGNAL PROCES, V41, P3245, DOI 10.1109/78.258071
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Carlyon RP, 2004, TRENDS COGN SCI, V8, P465, DOI 10.1016/j.tics.2004.08.008
   Chuang W. T., 2000, SIGIR Forum, V34, P152
   Coensel B. D., 2010, P 20 INT C AC SYDN A, P887
   Cristea D, 2005, LECT NOTES COMPUT SC, V3406, P632
   DENG L, 2000, P INT C SPOK LANG PR
   Derpanis KG, 2005, IEEE IMAGE PROC, P2777
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Dimitriadis D, 2005, IEEE SIGNAL PROC LET, V12, P621, DOI 10.1109/LSP.2005.853050
   Elhilali M, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000129
   Ellouze M, 2010, J VIS COMMUN IMAGE R, V21, P283, DOI 10.1016/j.jvcir.2010.01.007
   Erkan G, 2004, J ARTIF INTELL RES, V22, P457, DOI 10.1613/jair.1523
   Evangelopoulos G, 2009, INT CONF ACOUST SPEE, P3553, DOI 10.1109/ICASSP.2009.4960393
   Evangelopoulos G, 2006, IEEE T AUDIO SPEECH, V14, P2024, DOI 10.1109/TASL.2006.872625
   Fritz JB, 2007, CURR OPIN NEUROBIOL, V17, P437, DOI 10.1016/j.conb.2007.07.011
   Gales MJF, 1998, COMPUT SPEECH LANG, V12, P75, DOI 10.1006/csla.1998.0043
   Gao DS, 2009, IEEE T PATTERN ANAL, V31, P989, DOI 10.1109/TPAMI.2009.27
   Gkioulekas I, 2010, IEEE IMAGE PROC, P1081, DOI 10.1109/ICIP.2010.5650991
   Gong YH, 2000, PROC CVPR IEEE, P174, DOI 10.1109/CVPR.2000.854772
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Hafter Ervin R., 2008, V29, P115
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   Hauptmann AG, 2005, P 4 INT C IM VID RET
   Hennig Leonhard, 2008, 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Workshops, P291, DOI 10.1109/WIIAT.2008.175
   Hering E., 1964, OUTLINES THEORY LIGH
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jurafsky D., 2021, SPEECH LANGUAGE PROC
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Kalinli O, 2009, IEEE T AUDIO SPEECH, V17, P1009, DOI 10.1109/TASL.2009.2014795
   Kayser C, 2005, CURR BIOL, V15, P1943, DOI 10.1016/j.cub.2005.09.040
   Kim DK, 2011, IEEE T AUDIO SPEECH, V19, P315, DOI 10.1109/TASL.2010.2047756
   Kipp M., 2012, Multimedia Information Extraction: Advances in Video, Audio, and Imagery Analysis for Search, Data Mining, Surveillance, and Authoring, P351, DOI DOI 10.1002/9781118219546.CH21
   Knudsen EI, 2007, ANNU REV NEUROSCI, V30, P57, DOI 10.1146/annurev.neuro.30.051606.094256
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Kupiec J., 1995, SIGIR Forum, P68
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li Y, 2006, IEEE SIGNAL PROC MAG, V23, P79
   Lienhart R, 1997, COMMUN ACM, V40, P54, DOI 10.1145/265563.265572
   Lioma C, 2009, LECT NOTES COMPUT SC, V5478, P412, DOI 10.1007/978-3-642-00958-7_37
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Malandrakis N, 2011, INT CONF ACOUST SPEE, P2376
   MARAGOS P, 1993, IEEE T SIGNAL PROCES, V41, P3024, DOI 10.1109/78.277799
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Neto JL, 2002, LECT NOTES ARTIF INT, V2507, P205
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Orriols X, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P335, DOI 10.1109/ICCV.2001.937645
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   Pellom B., 2001, TRCSLR200101 U COL
   PONCELEON D, 1999, P 7 ACM INT C MULT O, P199
   POSNER MI, 1990, ANNU REV NEUROSCI, V13, P25, DOI 10.1146/annurev.ne.13.030190.000325
   Potamianos A, 1996, J ACOUST SOC AM, V99, P3795, DOI 10.1121/1.414997
   Rapantzikos K, 2011, COGN COMPUT, V3, P167, DOI 10.1007/s12559-011-9097-0
   Rapantzikos K, 2009, PROC CVPR IEEE, P1454, DOI 10.1109/CVPRW.2009.5206525
   Rautiainen M, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P751, DOI 10.1109/ICME.2004.1394309
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   SCHMID LA, 1994, INT CONF ACOUST SPEE, P41
   Smith MA, 1997, PROC CVPR IEEE, P775, DOI 10.1109/CVPR.1997.609414
   Toet A, 2011, IEEE T PATTERN ANAL, V33, P2131, DOI 10.1109/TPAMI.2011.53
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P12, DOI 10.1109/79.888862
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Wrigley SN, 2004, IEEE T NEURAL NETWOR, V15, P1151, DOI 10.1109/TNN.2004.832710
   You JY, 2007, IEEE T CIRC SYST VID, V17, P273, DOI 10.1109/TCSVT.2007.890857
   Yuille A.L., 1996, Perception as Bayesian inference, P123
   Zacks JM, 2010, FRONT HUM NEUROSCI, V4, DOI 10.3389/fnhum.2010.00168
   Zhuang XD, 2010, PATTERN RECOGN LETT, V31, P1543, DOI 10.1016/j.patrec.2010.02.005
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
   Zlatintsi A, 2012, EUR SIGNAL PR CONF, P1294
NR 75
TC 173
Z9 192
U1 3
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1553
EP 1568
DI 10.1109/TMM.2013.2267205
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800008
OA Green Published
DA 2024-07-18
ER

PT J
AU Bisio, I
   Lavagetto, F
   Marchese, M
   Sciarrone, A
AF Bisio, Igor
   Lavagetto, Fabio
   Marchese, Mario
   Sciarrone, Andrea
TI GPS/HPS-and Wi-Fi Fingerprint-Based Location Recognition for Check-In
   Applications Over Smartphones in Cloud-Based LBSs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Check-in applications; cloud computing; GPS/HPS receivers; smartphone
   terminals; Wi-Fi fingerprint
ID POSITIONING SYSTEM; SIGNAL STRENGTH
AB This paper proposes a new location recognition algorithm for automatic check-in applications (LRACI), suited to be implemented within Smartphones, integrated in the Cloud platform and representing a service for Cloud end users. The algorithm, the performance of which is independent of the employed device, uses both global and hybrid positioning systems (GPS/HPS) and, in an opportunistic way, the presence of Wi-Fi access points (APs), through a new definition of Wi-Fi FingerPrint (FP), which is proposed in this paper. This FP definition considers the order relation among the received signal strength (RSS) rather than the absolute values. This is one of the main contributions of this paper. LRACI is designed to be employed where traditional approaches, usually based only on GPS/HPS, fail, and is aimed at finding user location, with a room-level resolution, in order to estimate the overall time spent in the location, called Permanence, instead of the simple presence. LRACI allows automatic check-in in a given location only if the users' Permanence is larger than a minimum amount of time, called Stay Length (SL), and may be exploited in the Cloud. For example, if many people check-in in a particular location (e. g., a supermarket or a post office), it means that the location is crowded. Using LRACI-based data, collected by smartphones in the Cloud and made available in the Cloud itself, end users can manage their daily activities (e. g., buying food or paying a bill) in a more efficient way. The proposal, practically implemented over Android operating system-based Smartphones, has been extensively tested. Experimental results have shown a location recognition accuracy of about 90%, opening the door to real LRACI employments. In this sense, a preliminary study of its application in the Cloud, obtained through simulation, has been provided to highlight the advantages of the LRACI features.
C1 [Bisio, Igor; Lavagetto, Fabio; Marchese, Mario; Sciarrone, Andrea] Univ Genoa, Dept Telecommun Elect Elect Engn & Naval Architec, I-16145 Genoa, Italy.
C3 University of Genoa
RP Bisio, I (corresponding author), Univ Genoa, Dept Telecommun Elect Elect Engn & Naval Architec, I-16145 Genoa, Italy.
EM igor.bisio@unige.it; fabio.lavagetto@unige.it; mario.marchese@unige.it;
   andrea.sciarrone@unige.it
RI BISIO, IGOR/L-9799-2015; MARCHESE, MARIO/K-7153-2015
OI BISIO, IGOR/0000-0003-3198-7306; LAVAGETTO, FABIO/0000-0003-3692-4021;
   MARCHESE, MARIO/0000-0002-9626-3483; Sciarrone,
   Andrea/0000-0001-7023-2710
CR Alizadet-Shabdiz F., 2010, U.S. Patent, Patent No. 7856234
   [Anonymous], P INT S ONGPS GNSS
   [Anonymous], 2011, MOBILE COMMERCE  JAN
   Beom-Ju Shin, 2010, 2010 International Conference on Information and Communication Technology Convergence (ICTC), P319, DOI 10.1109/ICTC.2010.5674691
   Chon Y, 2012, IEEE T SYST MAN CY C, V42, P518, DOI 10.1109/TSMCC.2011.2131129
   Chon Y, 2011, IEEE PERVAS COMPUT, V10, P58, DOI 10.1109/MPRV.2011.13
   Grance T., 2011, US DEP COMMERCE SPEC, P145
   Holtzman J., 2002, P INT C UN PERS COMM, P827
   Hui Tian, 2010, 2010 IEEE/ION Position, Location and Navigation Symposium - PLANS 2010, P357, DOI 10.1109/PLANS.2010.5507303
   Johnson MJ, 2005, CONSUM COMM NETWORK, P533
   Junger Ernst, 2008, On Pain, P1
   Kaemarungsi K, 2004, PROCEEDINGS OF MOBIQUITOUS 2004, P14
   Kelly D., 2009, SIGN SYST C ISSC 200, P1
   LaMarca A, 2005, LECT NOTES COMPUT SC, V3468, P116
   Lee M., 2011, P IEEE INT C PERV CO, P648
   Miao H, 2007, IEEE T VEH TECHNOL, V56, P2568, DOI 10.1109/TVT.2007.899948
   Rabinowitz M, 2005, IEEE T BROADCAST, V51, P51, DOI 10.1109/TBC.2004.837876
   Skyhook, 2011, WORLD LEAD LOC INF C
   Tamai K, 2011, FUJITSU SCI TECH J, V47, P426
   Valenzuela RA, 1997, IEEE T VEH TECHNOL, V46, P203, DOI 10.1109/25.554753
   Vu L, 2011, INT CONF PERVAS COMP, P54, DOI 10.1109/PERCOM.2011.5767595
   Zheng Y., LOCATION BASED SERVI
NR 22
TC 49
Z9 49
U1 0
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 858
EP 869
DI 10.1109/TMM.2013.2239631
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500014
DA 2024-07-18
ER

PT J
AU Yue, HJ
   Sun, XY
   Yang, JY
   Wu, F
AF Yue, Huanjing
   Sun, Xiaoyan
   Yang, Jingyu
   Wu, Feng
TI Cloud-Based Image Coding for Mobile Devices-Toward Thousands to One
   Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image compression; local descriptor; mobile devices; SIFT
   (scale-invariant feature transform); the cloud
ID RETRIEVAL; FEATURES; SIFT
AB Current image coding schemes make it hard to utilize external images for compression even if highly correlated images can be found in the cloud. To solve this problem, we propose a method of cloud-based image coding that is different from current image coding even on the ground. It no longer compresses images pixel by pixel and instead tries to describe images and reconstruct them from a large-scale image database via the descriptions. First, we describe an input image based on its down-sampled version and local feature descriptors. The descriptors are used to retrieve highly correlated images in the cloud and identify corresponding patches. The down-sampled image serves as a target to stitch retrieved image patches together. Second, the down-sampled image is compressed using current image coding. The feature vectors of local descriptors are predicted by the corresponding vectors extracted in the decoded down-sampled image. The predicted residual vectors are compressed by transform, quantization, and entropy coding. The experimental results show that the visual quality of reconstructed images is significantly better than that of intra-frame coding in HEVC and JPEG at thousands to one compression.
C1 [Yue, Huanjing; Yang, Jingyu] Tianjin Univ, Tianjin 300072, Peoples R China.
   [Sun, Xiaoyan; Wu, Feng] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Tianjin University; Microsoft; Microsoft Research Asia
RP Yang, JY (corresponding author), Tianjin Univ, Tianjin 300072, Peoples R China.
EM dayueer@tju.edu.cn; xysun@microsoft.com; yjy@tju.edu.cn;
   fengwu@microsoft.com
RI YANG, JINGYU (Gracy)/AAD-3341-2021; Wu, Feng/KCY-3017-2024
CR [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2012, P 20 ACM MULTIMEDIA
   [Anonymous], 2002, Methodology for the subjective assessment of the quality of television pictures
   [Anonymous], P EUR C COMP VIS
   [Anonymous], P 20 ACM INT C MULT
   Armbrust M, 2010, COMMUN ACM, V53, P50, DOI 10.1145/1721654.1721672
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chandrasekhar Vijay, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2504, DOI 10.1109/CVPRW.2009.5206733
   Chandrasekhar Vijay, 2009, Proceedings of the SPIE - The International Society for Optical Engineering, V7257, DOI 10.1117/12.805982
   Chandrasekhar V, 2012, INT J COMPUT VISION, V96, P384, DOI 10.1007/s11263-011-0453-z
   Chao JS, 2011, IEEE IMAGE PROC, P301, DOI 10.1109/ICIP.2011.6116299
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Daneshi Maryam., 2011, Image reconstruction based on local feature descriptors
   Edelman S., COMPLEX CELLS OBJECT
   Eitz M, 2011, IEEE COMPUT GRAPH, V31, P56, DOI 10.1109/MCG.2011.67
   Eitz Mathias., 2009, proceedings of SIGGRAPH, P1
   Farbman Z, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024209
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Francini G., 2011, N12367 ISOIEC JTC1SC
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   Hua G, 2007, IEEE I CONF COMP VIS, P229
   Jegou H., 2008, INRIA HOLIDAY DATASE
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Johnson MK, 2011, IEEE T VIS COMPUT GR, V17, P1273, DOI 10.1109/TVCG.2010.233
   Ke Y, 2004, PROC CVPR IEEE, P506
   Ke Yan., 2004, MULTIMEDIA 04 P 12 A, P869, DOI DOI 10.1145/1027527.1027729
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Makar M, 2009, INT CONF ACOUST SPEE, P821, DOI 10.1109/ICASSP.2009.4959710
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Shao H., 2003, 206 ETH ZUR
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   Sullivan G. J., 2010, P SPIE, V7798, P7798
   Szeliski R., 2012, FDN TRENDS COMPUT GR, V2, P1
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Torr PHS, 2003, IEEE T PATTERN ANAL, V25, P354, DOI 10.1109/TPAMI.2003.1182098
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang Changhu., 2010, proceedings of the International Conference on World Wide Web, P1309
   Weinzaepfel P, 2011, PROC CVPR IEEE, P337, DOI 10.1109/CVPR.2011.5995616
   Whyte O., 2009, P BRIT MACH VIS C LO, P1
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Yeo C, 2008, IEEE IMAGE PROC, P217, DOI 10.1109/ICIP.2008.4711730
   Zheng Q.-F., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, P77
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
NR 49
TC 84
Z9 88
U1 2
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 845
EP 857
DI 10.1109/TMM.2013.2239629
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500013
DA 2024-07-18
ER

PT J
AU Ahn, JK
   Koh, YJ
   Kim, CS
AF Ahn, Jae-Kyun
   Koh, Yeong Jun
   Kim, Chang-Su
TI Efficient Fine-Granular Scalable Coding of 3D Mesh Sequences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D mesh coding; entropy coding; fine-granular scalability; mesh sequence
   compression; predictive coding; spatial layer decomposition.
ID COMPRESSION
AB An efficient fine-granular scalable coding algorithm of 3-D mesh sequences for low-latency streaming applications is proposed in this work. First, we decompose a mesh sequence into spatial and temporal layers to support scalable decoding. To support the finest-granular spatial scalability, we decimate only a single vertex at each layer to obtain the next layer. Then, we predict the coordinates of decimated vertices spatially and temporally based on a hierarchical prediction structure. Last, we quantize and transmit the spatio-temporal prediction residuals using an arithmetic coder. We propose an efficient context model for the arithmetic coding. Experiment results show that the proposed algorithm provides significantly better compression performance than the conventional algorithms, while supporting finer-granular spatial scalability.
C1 [Ahn, Jae-Kyun; Koh, Yeong Jun; Kim, Chang-Su] Korea Univ, Sch Elect Engn, Seoul, South Korea.
C3 Korea University
RP Ahn, JK (corresponding author), Korea Univ, Sch Elect Engn, Seoul, South Korea.
EM demian@korea.ac.kr; koyongjun@korea.ac.kr; changsukim@korea.ac.kr
OI Kim, Chang-Su/0000-0002-4276-1831; Koh, Yeong Jun/0000-0003-1805-2960
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF); Ministry of Education, Science and Technology (MEST)
   [2012-0000916]; Global Frontier R&D Program on Human-centered
   Interaction for Coexistence; NRF grant of the Korean Government (MEST)
   [NRF-M1AXA003-2011-0031648]
FX This work was supported in part by the Basic Science Research Program
   through the National Research Foundation of Korea (NRF) funded by the
   Ministry of Education, Science and Technology (MEST) under Grant
   2012-0000916 and the Global Frontier R&D Program on Human-centered
   Interaction for Coexistence funded by the NRF grant of the Korean
   Government (MEST) (NRF-M1AXA003-2011-0031648). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Feng Wu.
CR Ahn JK, 2010, IEEE IMAGE PROC, P3417, DOI 10.1109/ICIP.2010.5653130
   Ahn JH, 2001, ELECTRON LETT, V37, P1445, DOI 10.1049/el:20010993
   Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   Amjoun R, 2007, JOURNAL WSCG, V15, P99
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   Bici MO, 2011, J VIS COMMUN IMAGE R, V22, P577, DOI 10.1016/j.jvcir.2011.07.006
   Bici MO, 2010, IEEE IMAGE PROC, P3413, DOI 10.1109/ICIP.2010.5652143
   Boulfani-Cuisinaud Y, 2007, IEEE IMAGE PROC, P217
   Cho JW, 2006, IEEE IMAGE PROC, P529, DOI 10.1109/ICIP.2006.312393
   Guskov I., 2004, Proc. 2004 ACM SIG- GRAPH/Eurographics Symp. Comput. Animation (SCA '04), P183
   Heu JH, 2009, J VIS COMMUN IMAGE R, V20, P439, DOI 10.1016/j.jvcir.2009.05.003
   Ibarria L., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P126
   Karni Z, 2004, COMPUT GRAPH-UK, V28, P25, DOI 10.1016/j.cag.2003.10.002
   Mamou K, 2008, IEEE IMAGE PROC, P2676, DOI 10.1109/ICIP.2008.4712345
   Mamou K, 2006, COMPUT ANIMAT VIRT W, V17, P337, DOI 10.1002/cav.137
   Mamou K, 2009, COMPUT ANIMAT VIRT W, V20, P343, DOI 10.1002/cav.319
   Moffat A., 1995, Proceedings. DCC '95 Data Compression Conference (Cat. No.95TH8037), P202, DOI 10.1109/DCC.1995.515510
   Payan F., 2005, P IEEE ACIDCA ICMI
   Payan F, 2007, COMPUT GRAPH-UK, V31, P77, DOI 10.1016/j.cag.2006.09.009
   Sattler M., 2005, P 2005 ACM SIGGRAPH, P209
   Schwarz H, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1929, DOI 10.1109/ICME.2006.262934
   STEFANOSKI N, 2007, 3DTV C 2007, P1, DOI DOI 10.1109/3DTV.2007.4379461
   Stefanoski N, 2006, IEEE IMAGE PROC, P2973, DOI 10.1109/ICIP.2006.312961
   Stefanoski N, 2010, COMPUT GRAPH FORUM, V29, P101, DOI 10.1111/j.1467-8659.2009.01547.x
   Stefanoski N, 2008, IEEE IMAGE PROC, P2696, DOI 10.1109/ICIP.2008.4712350
   Vása L, 2009, COMPUT GRAPH FORUM, V28, P1529, DOI 10.1111/j.1467-8659.2008.01304.x
   Vasa L, 2007, 3DTV CONF, P49, DOI 10.1109/3DTV.2007.4379408
   Vása L, 2010, COMPUT GRAPH FORUM, V29, P1921, DOI 10.1111/j.1467-8659.2010.01659.x
   Vása L, 2009, COMPUT ANIMAT VIRT W, V20, P447, DOI 10.1002/cav.227
NR 29
TC 17
Z9 20
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 485
EP 497
DI 10.1109/TMM.2012.2235417
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900002
DA 2024-07-18
ER

PT J
AU Gong, BQ
   Liu, JZ
   Wang, XG
   Tang, XO
AF Gong, Boqing
   Liu, Jianzhuang
   Wang, Xiaogang
   Tang, Xiaoou
TI Learning Semantic Signatures for 3D Object Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D object retrieval; semantic signature; attribute; reference set;
   user-friendly interface
AB In this paper, we propose two kinds of semantic signatures for 3D object retrieval (3DOR). Humans are capable of describing an object using attribute terms like "symmetric" and "flyable", or using its similarities to some known object classes. We convert such qualitative descriptions into attribute signature (AS) and reference set signature (RSS), respectively, and use them for 3DOR. We also show that AS and RSS can be understood as two different quantization methods of the same semantic space of human descriptions of objects. The advantages of the semantic signatures are threefold. First, they are much more compact than low-level shape features yet working with comparable retrieval accuracy. Therefore, the proposed semantic signatures require less storage space and computation cost in retrieval. Second, the high-level signatures are a good complement to low-level shape features. As a result, by incorporating the signatures we can improve the performance of state-of-the-art 3DOR methods by a large margin. To the best of our knowledge, we obtain the best results on two popular benchmarks. Third, the AS enables us to build a user-friendly interface, with which the user can trigger a search by simply clicking attribute bars instead of finding a 3D object as the query. This interface is of great significance in 3DOR considering the fact that while searching, the user usually does not have a 3D query at hand that is similar to his/her targeted objects in the database.
C1 [Gong, Boqing] Univ So Calif, Dept Comp Sci, Los Angeles, CA 90095 USA.
   [Liu, Jianzhuang] Huawei Technol Co Ltd, Media Lab, Shenzhen 518129, Peoples R China.
   [Liu, Jianzhuang; Tang, Xiaoou] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
   [Wang, Xiaogang] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
   [Tang, Xiaoou] Chinese Univ Hong Kong, Fac Engn, Hong Kong, Hong Kong, Peoples R China.
   [Tang, Xiaoou] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Comp Vis & Pattern Recognit, Beijing 100864, Peoples R China.
C3 University of Southern California; Huawei Technologies; Chinese
   University of Hong Kong; Chinese University of Hong Kong; Chinese
   University of Hong Kong; Chinese Academy of Sciences; Shenzhen Institute
   of Advanced Technology, CAS
RP Gong, BQ (corresponding author), Univ So Calif, Dept Comp Sci, Los Angeles, CA 90095 USA.
EM boqinggo@usc.edu; liu.jianzhuang@huawei.com; xgwang@ee.cuhk.edu.hk;
   xtang@ie.cuhk.edu
RI Wang, Xiaogang/L-4369-2014
OI Gong, Boqing/0000-0003-3915-5977
FU Natural Science Foundation of China [61070148]; Science, Industry,
   Trade, Information Technology Commission of Shenzhen Municipality, China
   [JC201005270378A]; Introduced Innovative R&D Team of Guangdong Province
   "Robot and Intelligent Information Technology"
FX Manuscript received July 14, 2011; revised January 04, 2012; accepted
   July 15, 2012. Date of publication November 30, 2012; date of current
   version January 15, 2013. This work was supported in part by the Natural
   Science Foundation of China, under Grant 61070148, and in part by the
   Science, Industry, Trade, Information Technology Commission of Shenzhen
   Municipality, China, under Grant JC201005270378A. It was also supported
   through Introduced Innovative R&D Team of Guangdong Province "Robot and
   Intelligent Information Technology". The associate editor coordinating
   the review of this manuscript and approving it for publication was Nicu
   Sebe.
CR Agathos A, 2010, VISUAL COMPUT, V26, P1301, DOI 10.1007/s00371-010-0523-1
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], P EUR C COMP VIS
   [Anonymous], 2010, P EUR C COMP VIS
   [Anonymous], P INT C COMP VIS
   [Anonymous], 2010, P IEEE C COMP VIS PA
   [Anonymous], 2004, P SHAP MOD INT
   [Anonymous], THESIS U LEIPZIG LEI
   [Anonymous], P IEEE C COMP VIS PA
   Cao L, 2006, P ACM INT C MULT
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Endres A, 2010, P IEEE C COMP VIS PA
   Farhadi Ali, 2009, CVPR
   Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169
   Gong B, 2009, P ACM INT C MULT
   Kazhdan M, 2004, P EUR S GEOM PROC
   Kazhdan M., 2003, P EUR S GEOM PROC
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Leifman G, 2005, VISUAL COMPUT, V21, P865, DOI 10.1007/s00371-005-0341-z
   Lian ZH, 2010, INT J COMPUT VISION, V89, P130, DOI 10.1007/s11263-009-0295-0
   Liu Y, 2006, P IEEE C COMP VIS PA
   Mademlis A, 2008, IEEE T MULTIMEDIA, V10, P819, DOI 10.1109/TMM.2008.922790
   Ohbuchi R, 2008, P ACM MULT INF RETR
   Ohbuchi R, 2005, INT J COMPUT APPL T, V23, P70, DOI 10.1504/IJCAT.2005.006466
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Ovsjanikov M, 2009, P IEEE C COMP VIS PA
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Tang XO, 2012, IEEE T PATTERN ANAL, V34, P1342, DOI 10.1109/TPAMI.2011.242
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tung T., 2005, International Journal of Shape Modeling, V11, P91, DOI 10.1142/S0218654305000748
   Veltkamp R, 2007, SHREE2007 3D SHAPE R
   Wang X, 2011, P IEEE C COMP VIS PA
   Wang YM, 2010, IEEE T PATTERN ANAL, V32, P1858, DOI 10.1109/TPAMI.2009.200
   Wu TF, 2004, J MACH LEARN RES, V5, P975
   Xu CJ, 2009, IEEE T PATTERN ANAL, V31, P180, DOI 10.1109/TPAMI.2008.199
   Yoon S, 2010, P ACM INT C MULT
   Zhang J, 2005, P IEEE C COMP VIS PA
   Zhou D, 2003, P NEUR INF PROC SYST
NR 41
TC 28
Z9 29
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 369
EP 377
DI 10.1109/TMM.2012.2231059
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500012
DA 2024-07-18
ER

PT J
AU Toldo, M
   Magli, E
AF Toldo, Marco
   Magli, Enrico
TI Low-Delay Peer-To-Peer Media Streaming Based on Network Coding Over
   Randomized Multicast Trees
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Network coding; peer-to-peer streaming; video streaming
AB We introduce randomized multicast trees (RMT), an overlay topology designed for low-delay media streaming using network coding. RMTs improve on tree-based overlays in terms of start-up delay. We develop a push-based streaming system that leverages network coding to efficiently distribute the information in the overlay without using buffer maps, followed by a short pull stage to recover from packet losses, and appropriate management procedures to handle ungraceful peers departures. We report performance results of the proposed system, and compare it with an optimized pull system, and with an existing peer-to-peer system employing network coding, showing a significant performance improvement in terms of delay and resiliency to peers' dynamics and packet losses.
C1 [Toldo, Marco; Magli, Enrico] Politecn Torino, Dipartimento Elettron & Telecomunicaz, I-10129 Turin, Italy.
C3 Polytechnic University of Turin
RP Toldo, M (corresponding author), Politecn Torino, Dipartimento Elettron & Telecomunicaz, I-10129 Turin, Italy.
EM enrico.magli@polito.it
FU PRIN project "ARACNE"; Italian Ministry of Education, University and
   Research; COAST [ICT-248036]; European Union
FX This work was supported in part by PRIN project "ARACNE" funded by the
   Italian Ministry of Education, University and Research, and by "COAST"
   project (ICT-248036) funded by the European Union under the Seventh
   Framework Programme. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Gene Cheung.
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   [Anonymous], P ACM S PRINC DISTR
   [Anonymous], P 10 IEEE GLOB INT S
   Castro M, 2002, IEEE J SEL AREA COMM, V20, P1489, DOI 10.1109/JSAC.2002.803069
   Tapia R. A., COMPUTATIONAL SCI TO
   Thomos N, 2010, IEEE T CIRC SYST VID, V20, P1834, DOI 10.1109/TCSVT.2010.2087830
   Toldo M., 2010, P IEEE MMSP
   Wang M, 2007, IEEE J SEL AREA COMM, V25, P1655, DOI 10.1109/JSAC.2007.071205
   Xie S, 2007, IEEE T MULTIMEDIA, V9, P1661, DOI 10.1109/TMM.2007.907469
NR 9
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 941
EP 945
DI 10.1109/TMM.2012.2188279
PN 2
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700024
DA 2024-07-18
ER

PT J
AU Ly, C
   Hsu, CH
   Hefeeda, M
AF Ly, Cong
   Hsu, Cheng-Hsin
   Hefeeda, Mohamed
TI IRS: A Detour Routing System to Improve Quality of Online Games
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaming experience; latency reduction; performance optimization;
   quality-of-service (QoS)
ID LATENCY
AB Long network latency negatively impacts the performance of online games, and thus mechanisms are needed to mitigate its effects in order to provide a high-quality gaming experience. In this paper, we propose an indirect relay system (IRS) to forward game-state updates over detour paths in order to reduce the round-trip time (RTT) among players. We first collect extensive traces for RTTs among actual players in online games. We then analyze these traces to quantify the potential performance gain of the detour routing. Our analysis reveals that substantial reduction in the RTTs is possible. For example, our results indicate that more than 40% of players can observe at least 100 ms of RTT reduction by routing game-state updates through 1-hop detour paths. Because of the reduction in RTTs, players can join more gaming sessions that were not available to them due to long RTTs of the direct paths. Most importantly, we design and implement a complete IRS system for online games. To the best of our knowledge, this is the first system that directly reduces RTTs among players in online games, while previous works in the literature mitigate the long RTT issue by either hiding it from players or preventing players with high RTTs from being in the same game session. We implement the proposed IRS system and deploy it on 500 PlanetLab nodes. The results from real experiments show that the IRS system improves the online gaming quality from several aspects, while incurring negligible network and processing overheads. In particular, we observe that, with the proposed IRS system, more than 80% of game sessions achieve 100 ms or higher RTT reduction.
C1 [Ly, Cong; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
   [Hsu, Cheng-Hsin] Deutsch Telekom R&D Lab USA, Los Altos, CA 94022 USA.
C3 Simon Fraser University; Deutsche Telekom AG
RP Ly, C (corresponding author), Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   British Columbia Innovation Council
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada and the British Columbia Innovation
   Council. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Gene Cheung.
CR Agarwal S, 2009, SIGCOMM 2009, P315
   AGGARWAL S, 2004, P 3 ACM SIGCOMM WORK, P161
   ANDERSEN D, 2001, SIGOPS OPER SYST REV, V35, P131
   [Anonymous], P NOSSDAV 04 CORK IR
   Armitage G, 2003, ICON 2003: 11TH IEEE INTERNATIONAL CONFERENCE ON NETWORKS, P137
   Armitage G., 2006, NETWORKING ONLINE GA
   Armitage G., 2008, 18th International workshop on Network and Operating Systems Support for Digital Audio and Video, P3
   BAUGHMAN N, 2001, P IEEE INFOCOM ANCH, P22
   Beigbeder Tom., 2004, NETGAMES 04, P144
   BERNIERR Y, 2001, P GAM DEV C SAN JOS
   Cecin FR, 2006, IEEE ACM DIS SIM, P43
   CHAMBERS C, 2003, P 11 ACM INT C MULT, P227
   Chen KT, 2008, I C DEPEND SYS NETWO, P410, DOI 10.1109/DSN.2008.4630111
   Claypool M, 2005, COMPUT NETW, V49, P52, DOI 10.1016/j.comnet.2005.04.008
   CLAYPOOL M, 2008, P SPIE ACM MULT COMP, P12
   Claypool M, 2006, COMMUN ACM, V49, P40, DOI 10.1145/1167838.1167860
   DABEK F, 2004, P 2004 C APPL TECHN, P15
   Feng WC, 2005, IEEE ACM T NETWORK, V13, P488, DOI 10.1109/TNET.2005.850221
   Fritsch Tobias., 2005, NETGAMES 05, P1
   Gummadi KP, 2002, IMW 2002: PROCEEDINGS OF THE SECOND INTERNET MEASUREMENT WORKSHOP, P5, DOI 10.1145/637201.637203
   HENDERSON T, 2002, P ACM SIGCOMM WORKSH, P144
   HENDERSON T, 2003, THESIS U LONDON LOND
   Ledlie Jonathan., 2007, NSDI '07: Proceedings of the 4th USENIX Symposium on Networked Systems Design Implementation, P299
   Lee YK, 2008, LECT NOTES COMPUT SC, V4979, P41
   LIN K, 1995, P AIAA FLIGHT SIM TE, P83
   LUMEZANU C, 2007, P ACM WORKSH HOT TOP, P1
   LUMEZANU C, 2009, P USENIX S NETW SYST, P469
   Lumezanu C, 2009, LECT NOTES COMPUT SC, V5448, P45, DOI 10.1007/978-3-642-00975-4_5
   LY C, 2010, THESIS S FRASER U SU
   Ly C, 2010, P 18 ACM INT C MULT, P55
   MONCH C, 2006, P ACM SIGCOMM WORKSH, P1
   PALANT W, 2006, P 16 ACM INT WORKSH, P20
   Pantel Lothar., 2002, NOSSDAV 02 P 12 INT, P23
   Ren Shansi., 2006, ICDCS 06, P70
   Savage S, 1999, IEEE MICRO, V19, P50, DOI 10.1109/40.748796
   Subramanian L, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE FIRST SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'04), P71
   Vogel J., 2001, P 9 ACM INT C MULTIM, P221
   Wang GH, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P175
   Yan J., 2005, P 4 ACM SIGCOMM WORK, P1, DOI [DOI 10.1145/1103599.1103606, 10.1145/1103599.1103606]
   Zander S., 2005, P 2005 ACM SIGCHI IN, P117, DOI DOI 10.1145/1178477.1178493
NR 40
TC 6
Z9 8
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 733
EP 747
DI 10.1109/TMM.2011.2114645
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300013
DA 2024-07-18
ER

PT J
AU Calix, RA
   Mallepudi, SA
   Chen, B
   Knapp, GM
AF Calix, Ricardo A.
   Mallepudi, Sri Abhishikth
   Chen, Bin
   Knapp, Gerald M.
TI Emotion Recognition in Text for 3-D Facial Expression Rendering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Machine learning; natural language processing; semantic analysis;
   text-to-scene processing
AB Emotions are a key semantic component of human communication. This study focuses on automatic emotion detection in descriptive sentences and how this can be used to tune facial expression parameters for 3-D character generation. A comparison of manual and automatic word feature selection approaches is performed to determine the influence of word features on classification accuracy using support vector machines (SVM). The automatic emotion feature selection algorithm presented here builds on the framework used by mutual information for feature selection. Results of the study indicate that the set of automatically selected features was as good as the set of manually selected features. The proposed automatic feature selection algorithm implemented in this study helped to detect new words from the training corpus which were relevant to the classification task but were not considered by the researchers. An example of potential outcomes from facial expression tuning is also presented. The analysis includes initial results for dealing with the class imbalance challenge present in the data.
C1 [Calix, Ricardo A.; Mallepudi, Sri Abhishikth; Chen, Bin; Knapp, Gerald M.] Louisiana State Univ, Dept Ind Engn, Baton Rouge, LA 70803 USA.
C3 Louisiana State University System; Louisiana State University
RP Calix, RA (corresponding author), Louisiana State Univ, Dept Ind Engn, Baton Rouge, LA 70803 USA.
EM rcalix1@lsu.edu; smalle3@lsu.edu; bchen5@lsu.edu; gknapp@lsu.edu
RI Chen, Bin/W-4943-2017
CR Alm C., 2008, THESIS U ILLINOIS UR
   ALM CO, AFFECT DATA
   [Anonymous], 1989, Cognition and Emotion, DOI [10.1080/02699938908408075, DOI 10.1080/02699938908408075]
   [Anonymous], 1997, 14 INT C MACH LEARN
   *AUT MAY, 2010, MAY SOFTW
   Baggia P., 2009, Elements of an Emotional ML
   Bethard S, 2007, ICSC 2007: INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING, PROCEEDINGS, P11, DOI 10.1109/ICSC.2007.77
   Bird S., 2009, NATURAL LANGUAGE PRO
   *BLEND FDN, 2009, BLEND SOFTW
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   CAMRAS L, 1980, AM J PSYCHOL, V93, P751, DOI 10.2307/1422394
   Cassell J, 2001, AI MAG, V22, P67
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Clay SR, 1996, IEEE COMPUT GRAPH, V16, P31, DOI 10.1109/38.486678
   COLLINS AM, 1975, PSYCHOL REV, V82, P407, DOI 10.1037/0033-295X.82.6.407
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   COYNE B, 2001, P C KNOWL CAPT, P147
   Descartes Rene., 1989, PASSIONS SOUL
   EKMAN P, 1998, HUMAN EMOTIONS READE, P63
   Forsyth D.A., 2009, Object Categorization: Computer and Human Vision Perspectives, P167
   Gratch J, 2002, IEEE INTELL SYST, V17, P54, DOI 10.1109/MIS.2002.1024753
   Griffiths TL, 2007, PSYCHOL REV, V114, P211, DOI 10.1037/0033-295X.114.2.211
   Jenkins Jennifer M, 1998, Human emotions: A reader
   JOHANSSON R, 2004, P WORKSH TEXT MEAN I
   Jurafsky D., 2021, SPEECH LANGUAGE PROC
   LISCOMBE J, 2006, P 2006 C N AM CHAPT, P231
   Manning C.D., 1999, FDN STAT NATURAL LAN
   MASSARO D, 2001, EMBODIED CONVERSATIO
   Mathieu YY, 2004, ITCC 2004: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, VOL 2, PROCEEDINGS, P314, DOI 10.1109/ITCC.2004.1286653
   Minsky Marvin, 2007, EMOTION MACHINE COMM
   Moilanen K., 2007, P REC ADV NAT LANG P, P378
   Neviarouskaya A., 2009, Proceedings IADIS international conference on applied computing, VAC, P27
   NGUYEN H, 2008, P IEEE 2008 INT C RE
   Niu HN, 2003, INT CONF ACOUST SPEE, P125
   ORTONY A, 1987, COGNITIVE SCI, V11, P341, DOI 10.1207/s15516709cog1103_4
   Ortony A., 1988, COGNITIVE STRUCTURE
   Osherenko A, 2007, LECT NOTES COMPUT SC, V4738, P230
   Pang B., 2008, INFORM RETRIEVAL, V2, P1, DOI [10.1561/1500000011, DOI 10.1561/1500000011, https://doi.org/10.1561/1500000011]
   Pelachaud C, 2009, PHILOS T R SOC B, V364, P3539, DOI 10.1098/rstb.2009.0186
   Porter M. F., 1980, An algorithm for suffix stripping
   ROBERTS TT, 2008, BEHAV BRAIN SCI, V31, P689
   TOKUHISA R, 2008, P 22 INT C COMP LING, P881
   VAITUTTI R, 2004, P 4 INT C LANG RES E
   VALITUTTI A, 2004, PSYCHNOLOGY J, V2, P61
   VANGUMSTER J, 2009, BLENDER DUMMIES
   Vickrey D., 2005, Proc. of HLT-EMNLP, P771
   Wiebe J, 2005, LANG RESOUR EVAL, V39, P165, DOI 10.1007/s10579-005-7880-9
NR 47
TC 30
Z9 33
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2010
VL 12
IS 6
BP 544
EP 551
DI 10.1109/TMM.2010.2052026
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 668TB
UT WOS:000283291900008
DA 2024-07-18
ER

PT J
AU Broilo, M
   De Natale, FGB
AF Broilo, Mattia
   De Natale, Francesco G. B.
TI A Stochastic Approach to Image Retrieval Using Relevance Feedback and
   Particle Swarm Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-based image retrieval; particle swarm optimizer (PSO); relevance
   feedback (RF)
AB Understanding the subjective meaning of a visual query, by converting it into numerical parameters that can be extracted and compared by a computer, is the paramount challenge in the field of intelligent image retrieval, also referred to as the "semantic gap" problem. In this paper, an innovative approach is proposed that combines a relevance feedback (RF) approach with an evolutionary stochastic algorithm, called particle swarm optimizer (PSO), as a way to grasp user's semantics through optimized iterative learning. The retrieval uses human interaction to achieve a twofold goal: 1) to guide the swarm particles in the exploration of the solution space towards the cluster of relevant images; 2) to dynamically modify the feature space by appropriately weighting the descriptive features according to the users' perception of relevance. Extensive simulations showed that the proposed technique outperforms traditional deterministic RF approaches of the same class, thanks to its stochastic nature, which allows a better exploration of complex, nonlinear, and highly-dimensional solution spaces.
C1 [Broilo, Mattia; De Natale, Francesco G. B.] Univ Trent, Dept Informat Engn & Comp Sci DISI, I-38050 Trento, Italy.
C3 University of Trento
RP Broilo, M (corresponding author), Univ Trent, Dept Informat Engn & Comp Sci DISI, I-38050 Trento, Italy.
EM broilo@disi.unitn.it; denatale@ing.unitn.it
FU EU Commission [231126]
FX This work was supported in part by the EU Commission under the framework
   of the LivingKnowledge project (EU FET Grant no. 231126). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Zhu Liu.
CR Aksoy S, 2001, PATTERN RECOGN LETT, V22, P563, DOI 10.1016/S0167-8655(00)00112-4
   [Anonymous], ACM INT C IM VID RET
   Bordogna G, 1996, FUZZY SET SYST, V82, P201, DOI 10.1016/0165-0114(95)00256-1
   BRATTON D, 2007, P IEEE SWARM INT S
   Broilo M., 2008, P INT WORKSH MULT SI
   CHANDRAMOULI K, 2006, P 7 INT WORKSH AN MU, P313
   CHANDRAMOULI K, 2006, P VIE 08
   CHANDRAMOULI K, 2008, SER ADV SEMANTIC MED
   Chandramouli K, 2007, SECOND INTERNATIONAL WORKSHOP ON SEMANTIC MEDIA ADAPTATION AND PERSONALIZATION, PROCEEDINGS, P225, DOI 10.1109/SMAP.2007.18
   Clerc M, 2002, IEEE T EVOLUT COMPUT, V6, P58, DOI 10.1109/4235.985692
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Deselaers T, 2008, INFORM RETRIEVAL, V11, P77, DOI 10.1007/s10791-007-9039-3
   Djordjevic D, 2007, IEEE T CIRC SYST VID, V17, P313, DOI 10.1109/TCSVT.2007.890634
   Doulamis N, 2006, SIGNAL PROCESS-IMAGE, V21, P334, DOI 10.1016/j.image.2005.11.006
   EBERHART R, 2001, P C EV COMP, V1
   Giacinto G, 2001, LECT NOTES ARTIF INT, V2123, P337
   Gies D., 2003, IEEE Antennas and Propagation Society International Symposium. Digest. Held in conjunction with: USNC/CNC/URSI North American Radio Sci. Meeting (Cat. No.03CH37450), P177
   Griffin G., 2007, CALTECH 256 OBJECT C
   Grigorova A, 2007, IEEE T MULTIMEDIA, V9, P1183, DOI 10.1109/TMM.2007.902828
   Hinchey MG, 2007, COMPUTER, V40, P111, DOI 10.1109/MC.2007.144
   Hoi SCH, 2006, IEEE T KNOWL DATA EN, V18, P509, DOI 10.1109/TKDE.2006.1599389
   Huang TS, 2008, P IEEE, V96, P648, DOI 10.1109/JPROC.2008.916364
   Kennedy J., 1995, 1995 IEEE International Conference on Neural Networks Proceedings (Cat. No.95CH35828), P1942, DOI 10.1109/ICNN.1995.488968
   KENNEDY J, 1995, P IEEE C NEUR NETW 4
   Kennedy J., 2001, Swarm Intelligence
   KERFI ML, 2004, P IEEE INT C IM PROC, V1, P689
   Koskela M, 2004, LECT NOTES COMPUT SC, V3115, P508
   Liu HB, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P3157, DOI 10.1109/ICMLC.2004.1378577
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Okayama M, 2008, LECT NOTES COMPUT SC, V4985, P608
   Parsopoulos K. E., 2002, Natural Computing, V1, P235, DOI 10.1023/A:1016568309421
   ROBINSON JR, 2004, IEEE T ANTENN PROPAG, V52, P771
   Rocchio J. J., 1971, SMART RETRIEVAL SYST, P313
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   SHI YH, 1998, P 7 ANN C EV PROGR M
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   TIAN Q, 2000, P IEEE INT C MULT EX
   Trelea IC, 2003, INFORM PROCESS LETT, V85, P317, DOI 10.1016/S0020-0190(02)00447-7
   Uysal M, 2003, LECT NOTES COMPUT SC, V2728, P141
   Van Rijsbergen C. J., 1979, Information Retrieval, V2nd
   Wang LW, 2005, IEEE T PATTERN ANAL, V27, P1334, DOI 10.1109/TPAMI.2005.165
   Wilson E.O., 1975, P1
   Wu YM, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P581
   YAP KH, 2005, P IEEE INT C MULT EX
   YUAN P, 2004, P IEEE INT C NETW SE, V1, P367
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
NR 48
TC 44
Z9 50
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2010
VL 12
IS 4
BP 267
EP 277
DI 10.1109/TMM.2010.2046269
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 596ER
UT WOS:000277668100004
DA 2024-07-18
ER

PT J
AU Dai, R
   Akyildiz, IF
AF Dai, Rui
   Akyildiz, Ian F.
TI A Spatial Correlation Model for Visual Information in Wireless
   Multimedia Sensor Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Camera selection; spatial correlation; visual information; wireless
   multimedia sensor networks
AB Wireless multimedia sensor networks (WMSNs) are interconnected devices that allow retrieving video and audio streams, still images, and scalar data from the environment. In a densely deployed WMSN, there exists correlation among the visual information observed by cameras with overlapped field of views. This paper proposes a novel spatial correlation model for visual information in WMSNs. By studying the sensing model and deployments of cameras, a spatial correlation function is derived to describe the correlation characteristics of visual information observed by cameras with overlapped field of views. The joint effect of multiple correlated cameras is also studied. An entropy-based analytical framework is developed to measure the amount of visual information provided by multiple cameras in the network. Furthermore, according to the proposed correlation function and entropy-based framework, a correlation-based camera selection algorithm is designed. Experimental results show that the proposed spatial correlation function can model the correlation characteristics of visual information in WMSNs through low computation and communication costs. Further simulations show that, given a distortion bound at the sink, the correlation-based camera selection algorithm requires fewer cameras to report to the sink than the random selection algorithm.
C1 [Dai, Rui; Akyildiz, Ian F.] Georgia Inst Technol, Sch Elect & Comp Engn, Broadband Wireless Networking Lab, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Dai, R (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Broadband Wireless Networking Lab, Atlanta, GA 30332 USA.
EM aprildai@ece.gatech.edu; ian@ece.gatech.edu
RI Akyildiz, Ian/ABD-5310-2021; Akyildiz, Ian F/G-7136-2011
OI Dai, Rui/0000-0001-6620-7862
FU U.S. National Science Foundation (NSF) [ECCS-0701559]
FX This work was supported by the U.S. National Science Foundation (NSF)
   under Grant No. ECCS-0701559.
CR Akyildiz IF, 2007, COMPUT NETW, V51, P921, DOI 10.1016/j.comnet.2006.10.002
   Bartsch A., 2006, Proceedings of GlobWetland: Looking at Wetlands from Space, 19-20 October, Frascati, Italy, P1
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Cucchiara R., 2005, P 3 ACM INT WORKSHOP, P3
   Devarajan D, 2008, P IEEE, V96, P1625, DOI 10.1109/JPROC.2008.928759
   Girod B, 2005, P IEEE, V93, P71, DOI 10.1109/JPROC.2004.839619
   Gonzalez RC., 2011, DIGITAL IMAGE PROCES
   Heinzelman W.B, 2000, Ph.D. thesis, Ph.D. dissertation
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Lu Q, 2008, COMPUT NETW, V52, P2594, DOI 10.1016/j.comnet.2008.05.006
   Ma H, 2005, 2005 INTERNATIONAL CONFERENCE ON WIRELESS NETWORKS, COMMUNICATIONS AND MOBILE COMPUTING, VOLS 1 AND 2, P987
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Pluim JPW, 2003, IEEE T MED IMAGING, V22, P986, DOI 10.1109/TMI.2003.815867
   Ponce J, 2002, COMPUTER VISION MODE, DOI 10.5555/580035
   Puri R, 2007, IEEE T IMAGE PROCESS, V16, P2436, DOI 10.1109/TIP.2007.904949
   Vuran MC, 2004, COMPUT NETW, V45, P245, DOI 10.1016/j.comnet.2004.03.007
   Wagner R, 2003, IEEE IMAGE PROC, P597
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu M, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/70481
   Xiong ZX, 2004, IEEE SIGNAL PROC MAG, V21, P80, DOI 10.1109/MSP.2004.1328091
NR 21
TC 97
Z9 109
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1148
EP 1159
DI 10.1109/TMM.2009.2026100
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700011
DA 2024-07-18
ER

PT J
AU Tian, XH
   Cheng, Y
   Liu, B
AF Tian, Xiaohua
   Cheng, Yu
   Liu, Bin
TI Design of a Scalable Multicast Scheme With an Application-Network
   Cross-Layer Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Application oriented networking; bloom filter; cross-layer design;
   multicast; scalability
ID SERVICES
AB This paper develops an efficient and scalable multicast scheme for high-quality multimedia distribution. The traditional IP multicast, a pure network-layer solution, is bandwidth efficient in data delivery but not scalable in managing the multicast tree. The more recent overlay multicast establishes the data-dissemination structure at the application layer; however, it induces redundant traffic at the network layer. We propose an application-oriented multicast (AOM) protocol, which exploits the application-network cross-layer design. With AOM, each packet carries explicit destinations information, instead of an implicit group address, to facilitate the multicast data delivery; each router leverages the unicast IP routing table to determine necessary multicast copies and next-hop interfaces. In our design, all the multicast membership and addressing information traversing the network is encoded with bloom filters for low storage and bandwidth overhead. We theoretically prove that the AOM service model is loop-free and incurs no redundant traffic. The false positive performance of the bloom filter implementation is also analyzed. Moreover, we show that the AOM protocol is a generic design, applicable for both intra-domain and inter-domain scenarios with either symmetric or asymmetric routing.
C1 [Tian, Xiaohua; Cheng, Yu] IIT, Dept Elect & Comp Engn, Chicago, IL 60616 USA.
   [Liu, Bin] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
C3 Illinois Institute of Technology; Tsinghua University
RP Tian, XH (corresponding author), IIT, Dept Elect & Comp Engn, Chicago, IL 60616 USA.
EM xtian3@iit.edu; cheng@iit.edu; liub@tsinghua.edu.cn
RI xu, mingyu/KMX-9517-2024
OI Cheng, Yu/0000-0002-4837-3370
CR Adams A., 1998, 3973 IETF RFC
   Almeroth KC, 2000, IEEE NETWORK, V14, P10, DOI 10.1109/65.819167
   [Anonymous], 1584 IETF RFC
   [Anonymous], NETWORK SIMULATOR NS
   [Anonymous], CISC APPL OR NETW
   Ballardie A, 1997, 2201 IETF RFC
   Bhattacharyya S., 2003, RFC 3569
   Broder Andrei, 2003, Internet Math, V1, DOI DOI 10.1080/15427951.2004.10129096
   CAIN B, 2002, 3376 IETF RFC
   Cetinkaya C, 2001, IEEE T MULTIMEDIA, V3, P69, DOI 10.1109/6046.909595
   Chalmers RC, 2003, IEEE ACM T NETWORK, V11, P153, DOI 10.1109/TNET.2002.804835
   Chu Y., 2001, P ACM SIGCOMM, P55
   Clark D., 2003, New Arch: Future Generation Internet Architecture
   Costa LHMK, 2006, IEEE ACM T NETWORK, V14, P543, DOI 10.1109/TNET.2006.876157
   Deering S, 1996, IEEE ACM T NETWORK, V4, P153, DOI 10.1109/90.490743
   Deering S., 1998, 2460 IETF RFC
   DEERING SE, 1990, ACM T COMPUT SYST, V8, P85, DOI 10.1145/78952.78953
   DOVERSPIKE R, 2007, P IEEE INFOCOM MAY 6
   Egger S, 2004, IEEE COMMUN MAG, V42, P121, DOI 10.1109/MCOM.2004.1262171
   ESTRIN D, 1998, 2362 IETF RFC
   Fahmy S, 2007, IEEE ACM T NETWORK, V15, P373, DOI 10.1109/TNET.2007.892847
   KUMAR S, 1998, P ACM SIGCOMM OCT, V28
   Pasley J, 2005, IEEE INTERNET COMPUT, V9, P60, DOI 10.1109/MIC.2005.56
   Paxson V, 1997, IEEE ACM T NETWORK, V5, P601, DOI 10.1109/90.649563
   RATNASAMY S, 2006, P ACM SIGCOMM SEP
   Stoica I, 2004, IEEE ACM T NETWORK, V12, P205, DOI 10.1109/TNET.2004.826279
   Stoica I., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1644, DOI 10.1109/INFCOM.2000.832563
   Tennenhouse DL, 1997, IEEE COMMUN MAG, V35, P80, DOI 10.1109/35.568214
   Tennenhouse DL, 2002, DARPA ACTIVE NETWORKS CONFERENCE AND EXPOSITION, PROCEEDINGS, P2, DOI 10.1109/DANCE.2002.1003480
   TIAN X, 2008, P IEEE ICC MAY 19 23, P5646
   WAITZMAN D, 1988, 1075 IETF RFC
NR 31
TC 13
Z9 15
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1160
EP 1169
DI 10.1109/TMM.2009.2026104
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700012
OA Green Published
DA 2024-07-18
ER

PT J
AU Sun, HM
   Leu, MC
AF Sun, Hung-Min
   Leu, Muh-Chyi
TI An Efficient Authentication Scheme for Access Control in Mobile Pay-TV
   Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Authentication; bilinear pairing; conditional access system (CAS);
   hand-off; mobile pay-TV
ID ELLIPTIC CURVE CRYPTOGRAPHY; KEY DISTRIBUTION; BROADCAST; PROTOCOL
AB In a mobile pay-TV system, a large number of messages are exchanged for mutual authentication purposes. In traditional authentication schemes, with one-to-one delivery, one authentication message per request is delivered from a head end system to subscribers. This results in the delivery of a large quantity of messages and therefore is inefficient and costly. Moreover, since most traditional schemes use an RSA-based signature for identity validation and nonrepudiation of communication, they suffer from high communication costs. Due to its wireless nature, mobile pay-TV is vulnerable to attacks during hand-off. As traditional schemes do not support hand-off authentication, they are insecure during hand-off. With these shortcomings, they are not suitable for mobile pay-TV. In this paper, we propose an innovative authentication scheme, in which, by providing one-to-many facility, only one authentication message for multiple requests is broadcasted from the head end system to subscribers. By employing bilinear property of pairing and elliptic curve cryptography, our scheme provides one-to-many facility in the case of multiple requests for the same service in a short period of time. This new scheme achieves better broadcast efficiency and performance on communication costs than traditional ones. Additionally, this scheme provides a hand-off authentication mechanism to protect the access of services while preventing attacks during hand-off; therefore, the scheme is more secure to support access control. Moreover, to provide anonymous authentication for protecting identity privacy, the scheme adopts an identity-based scheme while traditional schemes do not apply. The scheme inherits advantages of the identity-based scheme that a public key does not need to be certificated, the certification authority mechanism will not be needed and the key exchange overhead can be reduced. With these advantages of our scheme, it is well suited for mobile pay-TV system.
C1 [Sun, Hung-Min] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
   [Leu, Muh-Chyi] Ind Technol Res Inst, Hsinchu 31040, Taiwan.
C3 National Tsing Hua University; Industrial Technology Research Institute
   - Taiwan
RP Sun, HM (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
EM hmsun@cs.nthu.edu.tw; mcleu@itri.org.tw
FU Industrial Technology Research Institute (ITRI), Taiwan, R. O. C
   [6352B43000, 6101QV1311]; National Science Council, Taiwan [NSC
   97-2221-E-007-055-MY3, NSC 97-2745-P-001-001]
FX This work was supported in part by the Industrial Technology Research
   Institute (ITRI), Taiwan, R. O. C., under projects 6352B43000 and
   6101QV1311 and in part by the National Science Council, Taiwan, under
   Contracts NSC 97-2221-E-007-055-MY3 and NSC 97-2745-P-001-001.
CR Allamandri F, 2007, IEEE T BROADCAST, V53, P200, DOI 10.1109/TBC.2007.891706
   Almeroth KC, 1999, IEEE T KNOWL DATA EN, V11, P658, DOI 10.1109/69.790835
   [Anonymous], IP DAT DVB H SERV PU
   [Anonymous], P ISDA
   [Anonymous], P 15 IEEE INT C NETW
   [Anonymous], 2006144 CRYPT EPRINT
   Boneh D, 2003, SIAM J COMPUT, V32, P586, DOI 10.1137/S0097539701398521
   Caelli WJ, 1999, COMPUT SECUR, V18, P47, DOI 10.1016/S0167-4048(99)80008-X
   COUTROT F, 1989, IEEE T CONSUM ELECTR, V35, P464, DOI 10.1109/30.44305
   *DVB, 2003, DVBH185R3
   *EBU, EBU TECHN REV FUNCT
   *ETSI TS, 2004, 103197V141 ETSI TS
   Faria G, 2006, P IEEE, V94, P194, DOI 10.1109/JPROC.2005.861011
   Gallery E., 2005, Proceedings. DFMA 05. First International Conference on Distributed Frameworks for Multimedia Applications, P190
   Huang YL, 2004, IEEE T MULTIMEDIA, V6, P760, DOI 10.1109/TMM.2004.834861
   *ITU R, 1992, 810 ITUR
   Joux A, 2000, LECT NOTES COMPUT SC, V1838, P385
   Lauter K, 2004, IEEE WIREL COMMUN, V11, P62, DOI 10.1109/MWC.2004.1269719
   Lee O, 2000, CURR PERSPECT SOC TH, V20, P27
   Liu HC, 2007, J AM SOC MASS SPECTR, V18, P2007, DOI 10.1016/j.jasms.2007.08.015
   MACQ BM, 1995, P IEEE, V83, P944, DOI 10.1109/5.387094
   Menezes A.J., 1993, ELLIPTIC CURVE PUBLI
   MENEZES AJ, 1993, IEEE T INFORM THEORY, V39, P1639, DOI 10.1109/18.259647
   Ollikainen V, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P629, DOI 10.1109/ICME.2006.262487
   SEROUSSI G, 1999, P INF THEOR NETW WOR, P41
   Shamir A., 1985, Advances in Cryptology, V84 4, P47, DOI 10.1007/3-540-39568-7_5
   Smart NP, 2002, ELECTRON LETT, V38, P630, DOI 10.1049/el:20020387
   Song R, 2003, IEEE T CONSUM ELECTR, V49, P408, DOI 10.1109/TCE.2003.1209533
   Stallings W., 2003, CRYPTOGRAPHY NETWORK
   Stinson D. R., 2006, Cryptography Theory and Practice, V3rd
   Sun HM, 2008, IEEE T MULTIMEDIA, V10, P1109, DOI 10.1109/TMM.2008.2001381
   Tseng YC, 2002, IEEE T COMMUN, V50, P1348, DOI 10.1109/TCOMM.2002.801466
   Wang SY, 2008, IEEE T MULTIMEDIA, V10, P480, DOI 10.1109/TMM.2008.917417
NR 33
TC 30
Z9 33
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 947
EP 959
DI 10.1109/TMM.2009.2021790
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300013
DA 2024-07-18
ER

PT J
AU Li, T
   Rahardja, S
   Koh, SN
AF Li, Te
   Rahardja, Susanto
   Koh, Soo Ngee
TI Fixed Quality Layered Audio Based on Scalable Lossless Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio coding; scalable coding; transparent quality
ID COMPRESSION; NOISE
AB The paper addresses a bitstream scalable coder based on the MPEG-4 scalable lossless (SLS) coding system where, in contrast to SLS, the bitrate of the enhancement layer is not fixed but instead an attempt is made to create a quality-fixed enhancement layer. With a PCM audio input, the proposed structure is able to produce an audio version with near-transparent quality on top of the existing low-quality version. In particular, the proposed fixed quality enhancing process with checking procedures is able to provide the minimum amount of enhancement for the low-quality version to obtain a near-transparent quality that is almost indistinguishable from the CD quality. In addition, a bitrate estimation model is proposed. The model enables the direct estimation of the enhancing bitrate from two parameters extracted from the encoding process of the low-quality version. Evaluation results indicate that a better defined quality level is guaranteed compared to a fixed bitrate setting and that in the mean a lower (approximately 20%) bitrate is attained. It is also shown that the estimation model proposed is able to accurately predict the necessary enhancing bitrate and at the same time, reduce the complexity by around 17%.
C1 [Li, Te; Koh, Soo Ngee] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Li, Te; Rahardja, Susanto] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Li, T (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM lite@i2r.a-star.edu.sg; rsusanto@i2r.a-star.edu.sg; esnkoh@ntu.edu.sg
RI KOH, Soo Ngee/A-5081-2011
CR AGGARWAL A, 1999, P IEEE WORKSH SPEECH, P16
   [Anonymous], INTEL VTUNE PERFORMA
   Boland S, 1996, INT CONF ACOUST SPEE, P1041, DOI 10.1109/ICASSP.1996.543541
   CHEN FC, 2005, P 19 INT C ADV INF N, V2, P235
   DUNN C, 2001, P 111 AES CONV NEW Y
   Fletcher H, 1940, REV MOD PHYS, V12, P0047, DOI 10.1103/RevModPhys.12.47
   GEIGER R, 2001, P 111 ACS CONV NEW Y
   GEIGER R, 2006, P 120 AES CONV MAY
   GOLOMB SW, 1966, IEEE T INFORM THEORY, V12, P399, DOI 10.1109/TIT.1966.1053907
   GREENWOOD D, 1961, J ACOUST SOC AM, V33, P1344, DOI 10.1121/1.1908437
   Hans M, 2001, IEEE SIGNAL PROC MAG, V18, P21, DOI 10.1109/79.939834
   HELLMAN RP, 1972, PERCEPT PSYCHOPHYS, V11, P241, DOI 10.3758/BF03206257
   International Organization for Standardization, 1998, 144963 ISOIEC
   ISO/IEC, 2006, 144963 ISOIEC
   *ITU R BS, 1116 ITUR BS
   *ITU R BS, 1387 ITUR BS
   Johnston J. D., 1988, ICASSP 88: 1988 International Conference on Acoustics, Speech, and Signal Processing (Cat. No.88CH2561-9), P2524, DOI 10.1109/ICASSP.1988.197157
   JOHNSTON JD, 1988, IEEE J SEL AREA COMM, V6, P314, DOI 10.1109/49.608
   Li J., 2002, PROC 10 ACM MULTIMED, P592
   LI T, 2008, P 9 INT C SIGN PROC
   Li T, 2008, IEEE T AUDIO SPEECH, V16, P94, DOI 10.1109/TASL.2007.910777
   Li T, 2007, IEEE T AUDIO SPEECH, V15, P2236, DOI 10.1109/TASL.2007.905144
   Lu ZT, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P529, DOI 10.1109/MMSP.1998.739035
   NELSON M, 1991, DR DOBBS J       FEB
   PARK SH, 1997, P 103 AES CONV NEW Y
   Purat M, 1996, INT CONF ACOUST SPEE, P1021, DOI 10.1109/ICASSP.1996.543297
   Raad M, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P624
   SINHA DP, 1993, IEEE T SIGNAL PROCES, V41, P3463, DOI 10.1109/78.258086
   Strahl S, 2005, IEEE WORK APPL SIG, P219, DOI 10.1109/ASPAA.2005.1540209
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
   Yu R, 2003, INT CONF ACOUST SPEE, P277
   Yu R, 2005, INT CONF ACOUST SPEE, P169
   Yu R, 2006, IEEE T AUDIO SPEECH, V14, P1352, DOI 10.1109/TSA.2005.860841
   Zwicker E., 2013, Psychoacoustics: Facts and Models
NR 34
TC 4
Z9 4
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 422
EP 432
DI 10.1109/TMM.2009.2012917
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300008
DA 2024-07-18
ER

PT J
AU Bonfiglio, D
   Mellia, M
   Meo, M
   Rossi, D
AF Bonfiglio, Dario
   Mellia, Marco
   Meo, Michela
   Rossi, Dario
TI Detailed Analysis of Skype Traffic
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
AB Skype is beyond any doubt the VoIP application in the current Internet application spectrum. Its amazing success has drawn the attention of telecom operators and the research community, both interested in knowing its internal mechanisms, characterizing its traffic, understanding its users' behavior.
   In this paper, we investigate the characteristics of traffic streams generated by voice and video communications, and the signaling traffic generated by Skype. Our approach is twofold, as we make use of both active and passive measurement techniques to gather a deep understanding on the traffic Skype generates. From extensive testbed experiments, we devise a source model which takes into account: i) the service type, i.e., SkypeOut calls or calls between two Skype clients, ii) the selected source Codec, iii) the adopted transport layer protocol, and iv) network conditions. Leveraging on the use of an accurate Skype classification engine that we recently proposed, we study and characterize Skype traffic based on extensive passive measurements collected from our campus LAN.
C1 [Bonfiglio, Dario; Mellia, Marco; Meo, Michela] Politecn Turin, Dipartimento Elettron, Turin, Italy.
   [Rossi, Dario] TELECOM ParisTech INFRES, Paris, France.
C3 Polytechnic University of Turin; IMT - Institut Mines-Telecom; Institut
   Polytechnique de Paris; Telecom Paris
RP Bonfiglio, D (corresponding author), Politecn Turin, Dipartimento Elettron, Turin, Italy.
EM dorio.bonfiglio@gmail.com; marco.mellia@polito.it;
   michela.meo@polito.it; dario.rossi@enst.fr
RI Rossi, Dario/HSG-3271-2023; Meo, Michela/HGB-0742-2022; Mellia,
   Marco/G-7997-2018
OI Rossi, Dario/0000-0003-3936-8876; Meo, Michela/0000-0001-7403-6266;
   Mellia, Marco/0000-0003-1859-6693
FU Italian Ministry of University, Education and Research (MIUR); Vodafone
   Italia
FX Manuscript received May 20, 2009; revised September 26, 2008. First
   published December 16, 2009; current version published January 09, 2009.
   This work was Supported by the Italian Ministry of University, Education
   and Research (MIUR) under the PRIN RECIPE, and partly by a research
   contract with Vodafone Italia. The associate editor coordinating the
   reviwew of this manuscipt and approving it for publication was Prof.
   Jiangchun Liu.
CR [Anonymous], 5 INT WORKSH PEER TO
   Baset S., 2006, IEEE INFOCOM 06
   BIONDI P, 2006, BLACK HAT EUROPE 06
   BONFIGLIO D, 2006, ACM SIGCOMM 07
   BONFIGLIO D, 2008, IEEE INFOCOM 06
   Carson M, 2003, ACM SIGCOMM COMP COM, V33, P111, DOI 10.1145/956993.957007
   DECICCO L, 2008, ACM NOSSDAV 08
   Lindblom J, 2005, IEEE T SPEECH AUDI P, V13, P787, DOI 10.1109/TSA.2005.851913
   Suh K., 2006, IEEE INFOCOM 06
   SKYPE TESTBED TRACES
   2006, INT CARRIERS TRAFFIC
NR 11
TC 63
Z9 68
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 117
EP 127
DI 10.1109/TMM.2008.2008927
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700010
OA Green Published
DA 2024-07-18
ER

PT J
AU Park, HG
   van der Schaar, M
AF Park, Hyunggon
   van der Schaar, Mihaela
TI A Framework for Foresighted Resource Reciprocation in P2P Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bounded rationality; foresighted decision; Markov decision process;
   peer-to-peer (P2P) network; resource reciprocation game
ID PEER
AB We consider peer-to-peer (P2P) networks, where multiple peers are interested in sharing multimedia content. In such P2P networks, the shared resources are the peers' contributed content and their upload bandwidth. While sharing resources, autonomous and self-interested peers need to make decisions on the amount of their resource reciprocation (i.e., representing their actions) such that their individual utilities are maximized. We model the resource reciprocation among the peers as a stochastic game and show how the peers can determine optimal strategies for resource reciprocation using a Markov Decision Process (MDP) framework. Unlike existing resource reciprocation strategies, which focus on myopic decisions of peers, the optimal strategies determined based on MDP enable the peers to make foresighted decisions about resource reciprocation, such that they can explicitly consider both their immediate as well as future expected utilities. To successfully formulate the MDP framework, we propose a novel algorithm that identifies the state transition probabilities using representative resource reciprocation models of peers. These models express the peers' different attitudes toward resource reciprocation. We analytically investigate how the error between the true and estimated state transition probability impacts each peer's decisions for selecting its actions as well as the resulting utilities. Moreover, we also analytically study how bounded rationality (e.g., limited memory for reciprocation history and the limited number of state descriptions) can impact the interactions among the peers and the resulting resource reciprocation. Simulation results show that the proposed approach based on reciprocation models can effectively cope with a dynamically changing environment such as peers' joining or leaving P2P networks. Moreover, we show that the propose foresighted decisions lead to the best performance in terms of the cumulative expected utilities.
C1 [Park, Hyunggon; van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Park, HG (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM hgpark@ee.ucla.edu; mi-haela@ee.ucla.edu
FU NSF [CCF-0541867]; Microsoft Research
FX Manuscript received March 16, 2008, revised September 16, 2008. First
   published December 16, 2008; current version published January 09, 2009.
   This work was supported by NSF CAREER Award CCF-0541867, and grants from
   Microsoft Research. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Ling Guan.
CR Androutsellis-Theotokis S, 2004, ACM COMPUT SURV, V36, P335, DOI 10.1145/1041680.1041681
   [Anonymous], 2003, P 2 INT JOINT C AUT
   [Anonymous], 1976, Dynamic Programming and Stochastic Optimal Control
   Buragohain C, 2003, THIRD INTERNATIONAL CONFERENCE ON PEER-TO-PEER COMPUTING (P2P2003), PROCEEDINGS, P48, DOI 10.1109/PTP.2003.1231503
   COHEN B, 2003, P P2P EC WORKSH BERK
   Cui Y, 2004, IEEE J SEL AREA COMM, V22, P91, DOI 10.1109/JSAC.2003.818799
   de Veciana G., 2003, 41 ANN ALL C COMM CO
   Fudenberg D., 1991, GAME THEORY
   Gallager R. G., 1995, Discrete Stochastic Processes
   Gupta M., 2003, P 13 ACM INT WORKSHO, P144
   Haruvy E, 1999, ECON LETT, V63, P255, DOI 10.1016/S0165-1765(99)00028-2
   Jain K, 2007, DISTRIB COMPUT, V19, P301, DOI 10.1007/s00446-006-0014-9
   JIANG X, 2003, P 4 INT C MULT EXP J
   Lai K., 2003, P WORKSH EC PEER TO
   Legout A, 2007, PERF E R SI, V35, P301
   LIU J, 2007, P IEEE SPEC ISS REC
   Pai V., 2005, P 4 INT WORKSH PEER
   Shneidman J, 2003, LECT NOTES COMPUT SC, V2735, P139
   Simon HA, 1955, Q J ECON, V69, P99, DOI 10.2307/1884852
   van der Schaar M., 2007, MULTIMEDIA IP WIRELE
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Xiang Z, 2004, IEEE T MULTIMEDIA, V6, P343, DOI 10.1109/TMM.2003.822819
   ZHANG X, 2005, P INFOCOM 05
NR 23
TC 21
Z9 23
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 101
EP 116
DI 10.1109/TMM.2008.2008925
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700009
DA 2024-07-18
ER

PT J
AU Hsieh, JW
   Hsu, YT
   Liao, HYM
   Chen, CC
AF Hsieh, Jun-Wei
   Hsu, Yun-Tai
   Liao, Hong-Yuan Mark
   Chen, Chih-Chiang
TI Video-based human movement analysis and its application to surveillance
   systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE behavior analysis; event detection; string matching; triangulation
ID MOTION ANALYSIS; RECOGNITION; TRACKING
AB This paper presents a novel posture classification system that analyzes human movements directly from video sequences. In the system, each sequence of movements is converted into a posture sequence. To better characterize a posture in a sequence, we triangulate it into triangular meshes, from which we extract two features: the skeleton feature and the centroid context feature. The first feature is used as a coarse representation of the subject, while the second is used to derive a finer description We adopt a depth-first search (dfs) scheme to extract the skeletal features of a posture from the triangulation result. The proposed skeleton feature extraction scheme is more robust and efficient than conventional silhouette-based approaches. The skeletal features extracted in the first stage are used to extract the centroid context feature, which is a finer representation that can characterize the shape of a whole body or body parts. The two descriptors working together make human movement analysis a very efficient and accurate process because they generate a set of key postures from a movement sequence. The ordered key posture sequence is represented by a symbol string. Matching two arbitrary action sequences then becomes a symbol string matching problem. Our experiment results demonstrate that the proposed method is a robust, accurate, and powerful tool for human movement analysis.
C1 [Hsieh, Jun-Wei; Hsu, Yun-Tai; Chen, Chih-Chiang] Yuan Ze Univ, Dept Elect Engn, Chungli 320, Taiwan.
   [Liao, Hong-Yuan Mark] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
C3 Yuan Ze University; Academia Sinica - Taiwan
RP Hsieh, JW (corresponding author), Yuan Ze Univ, Dept Elect Engn, Chungli 320, Taiwan.
EM shieh@saturn.yzu.edu.tw; s937114@mail.yzu.edu.tw;
   liao@iis.sinica.edu.tw; s929106@mail.yzu.edu.tw
RI Liao, Hong-Yuan Mark/AAQ-5514-2021
FU National Science Council of Taiwan [NSC95-2221-E-155-071]; Ministry of
   Economic Affairs [94-EC-17-A-02-S1-032]
FX This work was supported in part by the National Science Council of
   Taiwan. R.O.C., under Grant NSC95-2221-E-155-071 and by the Ministry of
   Economic Affairs under Contract 94-EC-17-A-02-S1-032. The associate
   editor coordinating the reviews of this manuscript and approving it for
   publication was Prof. Kiyoharu Aizawa.
CR Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Ben-Arie J, 2002, IEEE T PATTERN ANAL, V24, P1091, DOI 10.1109/TPAMI.2002.1023805
   Bernays E.A., 1995, P47
   BORGEFORS G, 1986, COMPUT VISION GRAPH, V34, P344, DOI 10.1016/S0734-189X(86)80047-0
   Chang IC, 2000, IMAGE VISION COMPUT, V18, P1067, DOI 10.1016/S0262-8856(00)00046-9
   CHETVERIKOV D, 1999, P 23 WORKSH AUSTR PA, P175
   CHEW LP, 1989, ALGORITHMICA, V4, P97, DOI 10.1007/BF01553881
   Cucchiara R, 2005, IEEE T SYST MAN CY A, V35, P42, DOI 10.1109/TSMCA.2004.838501
   Fujiyoshi H, 2004, IEICE T INF SYST, VE87D, P113
   GUO PX, 1994, SEMIN VIROL, V5, P1, DOI 10.1006/smvy.1994.1001
   GUO Y, 1994, INT C PATT RECOG, P325, DOI 10.1109/ICPR.1994.576929
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   HOROWITZ E, FUNDAMENTALS DATA ST
   Ivanov YA, 2000, IEEE T PATTERN ANAL, V22, P852, DOI 10.1109/34.868686
   Ju SX, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P38, DOI 10.1109/AFGR.1996.557241
   KONTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489
   Maybank S, 2000, INT J COMPUT VISION, V37, P173, DOI 10.1023/A:1008151520284
   Mikic I, 2003, INT J COMPUT VISION, V53, P199, DOI 10.1023/A:1023012723347
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Park S, 2002, IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING (MOTION 2002), PROCEEDINGS, P105, DOI 10.1109/MOTION.2002.1182221
   PARK S, 2004, P 2004 C COMP VIS PA
   ROSALES R, 1999, P IEEE C COMP VIS PA, V2, P637
   Rui Yong, 1996, P 1 INT WORKSH IM DA
   SHEWCHUK J, 2002, THEORY APPL, V23, P21
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   UKKONEN E, 1983, P INT C FDN COMP THE, P487
   WEIK S, 2001, P INT WORKSH ROB VIS, P27
   Werghi N, 2005, PATTERN RECOGN LETT, V26, P663, DOI 10.1016/j.patrec.2004.09.018
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
NR 31
TC 89
Z9 93
U1 2
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 372
EP 384
DI 10.1109/TMM.2008.917403
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100007
DA 2024-07-18
ER

PT J
AU Tseng, YH
   Wu, EHK
   Chen, GH
AF Tseng, Yi-Hsien
   Wu, Eric Hsiao-Kuang
   Chen, Gen-Huey
TI An admission control scheme based on online measurement for VBR video
   streams over wireless home networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE admission control; effective bandwidth; Kalman filter; lognormal
   distribution; QoS
ID BANDWIDTH MANAGEMENT; ATM; ALGORITHM
AB This paper presents an online measurement-based admission control scheme on the basis that the aggregate VBR video traffic is lognormally distributed. The proposed scheme consists of two components: measurement process and admission decision. The measurement process applies a linear Kalman filter to estimate statistical parameters of aggregate VBR video traffic. The estimated statistical parameters are used to calculate the effective bandwidth for admission decision. Variable bit rate (VBR) video traffic with high data rate is expected to occupy a dominant proportion of bandwidth for future wireless broadband home networks. To guarantee quality-of-service (QoS) of such VBR video streams, while achieving a high level of channel utilization, an efficient admission control scheme is urgently required, especially for emerging wireless multimedia indoor services, such as HDTV, online video game, etc. The proposed scheme is computationally efficient and accurate without much prior traffic information. Simulation results verify its effectiveness and show that it performs well for both a small number of connections and a large number of connections.
C1 [Tseng, Yi-Hsien; Chen, Gen-Huey] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10764, Taiwan.
   [Wu, Eric Hsiao-Kuang] Natl Cent Univ, Dept Comp Sci & Informat Engn, Chungli 32054, Taiwan.
C3 National Taiwan University; National Central University
RP Tseng, YH (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10764, Taiwan.
EM d91007@csie.ntu.edu.tw; hsiao@csie.ncu.edu.tw; ghchen@csie.ntu.edu.tw
FU Institute of Information Industry; National Science Council
FX This work was supported by the Institute of Information Industry under
   the "Advanced Mobile Context Aware Application & Service Development"
   project and by the National Science Council under the "Robokid" project.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Wanjiun Liao.
CR ANICK D, 1982, AT&T TECH J, V61, P1871, DOI 10.1002/j.1538-7305.1982.tb03089.x
   [Anonymous], 1960, IRE Trans. Commun. Syst., DOI DOI 10.1109/TCOM.1960.1097606
   [Anonymous], ADAPTIVE FILTER THEO, DOI DOI 10.1109/ISCAS.2017.8050871
   [Anonymous], **NON-TRADITIONAL**
   [Anonymous], 1984, Introduction to Mathematical Statistics
   Casetti C, 2000, COMPUT COMMUN, V23, P1363, DOI 10.1016/S0140-3664(00)00182-1
   Dalgic I, 1997, IEEE J SEL AREA COMM, V15, P1115, DOI 10.1109/49.611163
   Dziong Z, 1997, IEEE ACM T NETWORK, V5, P134, DOI 10.1109/90.554728
   FOERSTER J, 2001, J INTEL TECHNOL
   GIBBENS RJ, 1995, IEEE J SEL AREA COMM, V13, P1101, DOI 10.1109/49.400665
   GUERIN R, 1991, IEEE J SEL AREA COMM, V9, P968, DOI 10.1109/49.103545
   IBRAHIM AAM, 1998, P ATM 98, P300
   *IEEE, 2003, 802153 IEEE
   Jamin S, 1997, IEEE ACM T NETWORK, V5, P56, DOI 10.1109/90.554722
   JAMIN S, 1997, P C COMP COMM IEEE I, V3, P973
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kang SH, 2005, IEEE T MULTIMEDIA, V7, P1139, DOI 10.1109/TMM.2005.858401
   Kelly F.P., 1996, Stochastic Networks: Theory and Applications, P141
   Koutsakis P, 2004, IEEE T VEH TECHNOL, V53, P1525, DOI 10.1109/TVT.2004.833639
   KRUNZ M, 1995, IEEE INFOCOM SER, P455, DOI 10.1109/INFCOM.1995.515909
   LEGALL D, 1991, COMMUN ACM, V34, P46, DOI 10.1145/103085.103090
   Liu NX, 2004, GLOB TELECOMM CONF, P2108
   Nagarajan K, 2000, CONF REC ASILOMAR C, P1245, DOI 10.1109/ACSSC.2000.910762
   RANGO FD, 2005, P IEEE GLOBELCOM 200, P3231
   REISSLEIN M, 2003, TRAFFIC QUALITY CHAR
   SAITO H, 1991, IEEE J SEL AREA COMM, V9, P982, DOI 10.1109/49.103546
   SAITO H, 1992, IEEE T COMMUN, V40, P1512, DOI 10.1109/26.163572
   Shiomoto Kohei, 1999, IEEE Communications Surveys & Tutorials, V2, P2, DOI 10.1109/COMST.1999.5340510
   SHIOMOTO K, 1995, IEICE T COMMUN, VE78B, P458
   Shiomoto K, 1998, IEEE ACM T NETWORK, V6, P625, DOI 10.1109/90.731198
   Stroh S, 2003, IEEE SPECTRUM, V40, P23, DOI 10.1109/MSPEC.2003.1228004
   TEDIJANTO TE, 1993, P IEEE INFOCOM 93, P358
   Tse D, 1997, IEEE INFOCOM SER, P981
NR 33
TC 10
Z9 10
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 470
EP 479
DI 10.1109/TMM.2008.917423
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Vinciarelli, A
AF Vinciarelli, Alessandro
TI Speakers role recognition in multiparty audio recordings using social
   network analysis and duration distribution modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio indexing; social network analysis; sociometry; speaker clustering;
   speaker segmentation; stochastic processes
ID VIDEO
AB This paper presents two approaches for speaker role recognition in multiparty audio recordings. The experiments are performed over a corpus of 96 radio bulletins corresponding to roughly 19 h of material. Each recording involves, on average, 11 speakers playing one among six roles belonging to a predefined set. Both proposed approaches start by segmenting automatically the recordings into single speaker segments, but perform role recognition using different techniques. The first approach is based on Social Network Analysis, the second relies on the intervention duration distribution across different speakers. The two approaches are used separately and combined and the results show that around 85% of the recording time can be labeled correctly in terms of role.
C1 IDIAP, Res Inst, CH-1920 Martigny, Switzerland.
   Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Vinciarelli, A (corresponding author), IDIAP, Res Inst, CH-1920 Martigny, Switzerland.
EM vincia@idiap.ch
RI Vinciarelli, Alessandro/HZI-8274-2023; Vinciarelli,
   Alessandro/C-1651-2012
OI Vinciarelli, Alessandro/0000-0002-9048-0524
CR AJMERA J, 2003, P IEEE WORKSH AUT SP
   AJMERA J, 2004, THESIS ECOLE POLYTEC
   [Anonymous], 1997, Statistical Methods for Speech Recognition
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   Baeza-Yates Ricardo., MODERN INFORM RETRIE
   BARZILAY R, 2000, P AM ASS ART INT S
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Ching Hau Chan, 2005, 13th Annual ACM International Conference on Multimedia, P427, DOI 10.1145/1101149.1101243
   Coldefy F., 2004, Proceedings of the 12th annual ACM international conference on Multimedia, P268
   Fay N, 2000, PSYCHOL SCI, V11, P481, DOI 10.1111/1467-9280.00292
   GATICAPEREZ D, 2005, P 7 ACM SIGMM INT WO, P242
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P1114, DOI 10.1109/TMM.2005.858397
   JAVED D, 2001, P INT C COMP VIS
   Koumpis K, 2005, IEEE SIGNAL PROC MAG, V22, P61, DOI 10.1109/MSP.2005.1511824
   LEE CM, 2005, IEEE T MULTIMEDIA, V7, P293
   LEE SL, 2005, IEEE SIGNAL PROCESSI, V22, P42
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   McCowan I, 2005, IEEE T PATTERN ANAL, V27, P305, DOI 10.1109/TPAMI.2005.49
   Papoulis A., 1991, Probability, Random Variables and Stochastic Processes
   Pevzner L, 2002, COMPUT LINGUIST, V28, P19, DOI 10.1162/089120102317341756
   SCOTT J., 2017, Social Network Analysis, V4th
   Snoek CGM, 2005, IEEE T MULTIMEDIA, V7, P638, DOI 10.1109/TMM.2005.850966
   Snoek CGM, 2005, MULTIMED TOOLS APPL, V25, P5, DOI 10.1023/B:MTAP.0000046380.27575.a5
   Wasserman S., 1994, Social network analysis: Methods and applications'
   Wrigley SN, 2005, IEEE T SPEECH AUDI P, V13, P84, DOI 10.1109/TSA.2004.838531
   Xuan Huang, 2021, 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), P867, DOI 10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00144
   ZHANG D, IN PRESS IEEE T MULT
   Zhang T, 2001, IEEE T SPEECH AUDI P, V9, P441, DOI 10.1109/89.917689
NR 29
TC 39
Z9 44
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1215
EP 1226
DI 10.1109/TMM.2007.902882
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000012
DA 2024-07-18
ER

PT J
AU Wang, HL
   Kwong, S
   Kok, CW
AF Wang, Hanli
   Kwong, Sam
   Kok, Chi-Wah
TI An efficient mode decision algorithm for H.264/AVC encoding optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE early termination; H.264; integer DCT; mode decision; motion estimation
AB The H.264 video coding standard significantly outperforms previous standards in terms of coding efficiency. However, this comes as a cost of extremely high computational complexity due to mode decision where variable block size Motion Estimation (ME) is employed. In this paper, we propose an efficient algorithm to jointly optimize mode decision and ME. A theoretical analysis is performed to study the sufficient condition to detect all-zero blocks in H.264, and thus adaptive thresholds are derived to early terminate mode decision and ME. Besides the aforementioned early termination technique, the proposed algorithm also introduces temporal-spatial checking, thresholds based prediction and monotonic error surface based prediction methods to skip checking unnecessary modes. Experimental results demonstrate that the proposed algorithm can significantly reduce the computational complexity of H.264 encoding while maintaining almost the same Rate Distortion (RD) performance as the original encoder.
C1 City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Wang, HL (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM wangh1@cs.cityu.edu.hk; cssamk@cityu.edu.hk; eekok@ieee.org
RI Kok, C.W./AAM-6407-2020; Kwong, Sam/C-9319-2012; Wang,
   Hanli/G-5111-2014; Wang, Hanli/K-5717-2019
OI Kwong, Sam/0000-0001-7484-7261; Wang, Hanli/0000-0002-9999-4871; Wang,
   Hanli/0000-0002-9999-4871
CR [Anonymous], 2005, 1449610 ISO IEC
   Jing X, 2004, ELECTRON LETT, V40, P1050, DOI 10.1049/el:20045243
   Malvar HS, 2003, IEEE T CIRC SYST VID, V13, P598, DOI 10.1109/TCSVT.2003.814964
   Moon YH, 2005, IEEE T CIRC SYST VID, V15, P1053, DOI 10.1109/TCSVT.2005.852411
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Pan F, 2005, IEEE T CIRC SYST VID, V15, P813, DOI 10.1109/TCSVT.2005.848356
   Wu D, 2005, IEEE T CIRC SYST VID, V15, P953, DOI 10.1109/TCSVT.2005.848304
   Yang CL, 2004, IEEE IMAGE PROC, P461
   Yang LB, 2005, IEEE T CIRC SYST VID, V15, P784, DOI 10.1109/TCSVT.2005.848306
   Yin P, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P853
NR 10
TC 71
Z9 87
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 882
EP 888
DI 10.1109/TMM.2007.893345
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200017
DA 2024-07-18
ER

PT J
AU Qu, W
   Schonfeld, D
   Mohamed, M
AF Qu, Wei
   Schonfeld, Dan
   Mohamed, Magdi
TI Real-time distributed multi-object tracking using multiple interactive
   trackers and a magnetic-inertia potential model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayesian tracking; data association; multiple object tracking; object
   occlusion; particle filter
ID TARGET TRACKING
AB This paper presents a method which avoids the common practice of using a complex joint state-space representation and performing tedious joint data association for multiple object tracking applications. Instead, we propose a distributed Bayesian formulation using multiple interactive trackers that requires much lower complexity for real-time tracking applications. When the objects' observations do not interact with each other, our approach performs as multiple independent trackers. However, when the objects' observations exhibit interaction, defined as close proximity or partial and complete occlusion, we extend the conventional Bayesian tracking framework by modeling such interaction in terms of potential functions. The proposed "magnetic-inertia" model represents the cumulative effect of virtual physical forces that objects undergo while interacting with each other. It implicitly handles the "error merge" and "object labeling" problems and thus solves the difficult object occlusion and data association problems in an innovative way. Our preliminary simulations have demonstrated that the proposed approach is far superior to other methods in both robustness and speed.
C1 Motorola Labs, Network Res COE, Schaumburg, IL 60196 USA.
   Univ Illinois, Dept Elect & Comp Engn, Chicago, IL 60607 USA.
   Motorola Labs, Phys Realizat Res COE, Schaumburg, IL 60196 USA.
C3 Legend Holdings; Lenovo; University of Illinois System; University of
   Illinois Chicago; University of Illinois Chicago Hospital; Legend
   Holdings; Lenovo
RP Qu, W (corresponding author), Motorola Labs, Network Res COE, Schaumburg, IL 60196 USA.
EM wei.qu@motorola.com; ds@ece.uic.edu; Magdi.Mohamed@motorola.com
CR Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   BARSHALOM Y, 1998, TRACKING DATA ASS
   BLACK J, 2003, JOINT IEEE INT WORKS
   Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038
   Doucet A., 1998, SEQUENTIAL MONTE CAR
   Gordon N, 1997, IEEE T AERO ELEC SYS, V33, P353, DOI 10.1109/7.570826
   Grant I.S., 2001, The elements of physics
   Hayt W.H., 2001, ENG ELECTROMAGNETICS, V6th
   Hue C, 2002, IEEE T SIGNAL PROCES, V50, P309, DOI 10.1109/78.978386
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   ISARD M, 2001, P INT C COMP VIS ICC, V2, P341
   KHAN Z, 2004, P 8 EUR C COMP VIS E
   MacCormick J, 2000, INT J COMPUT VISION, V39, P57, DOI 10.1023/A:1008122218374
   Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227
   NUMMIARO K, 2002, S PATT REC DAGM, P355
   Silverman, 2018, DENSITY ESTIMATION S, DOI 10.1201/9781315140919
   TUZEL O, 2005, P IEEE INT WORKSH MA
   Whittaker J, 1990, GRAPHICAL MODELS APP
   YU T, 2004, P IEEE C COMP VIS PA
NR 19
TC 46
Z9 58
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 511
EP 519
DI 10.1109/TMM.2006.886266
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100007
DA 2024-07-18
ER

PT J
AU Yousefi'zadeh, H
   Jafarkhani, H
   Habibi, A
AF Yousefi'zadeh, Homayoun
   Jafarkhani, Hamid
   Habibi, Amir
TI Layered media multicast control (LMMC): Real-time error control
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE A priori estimate of loss; error control; layered media; replicated
   media; statistical guarantee of QoS
AB We study the problem of real-time error control in layered and replicated media systems. We formulate an optimization problem aimed at minimizing a cost metric defined over the wasted bandwidth of redundancy in such systems. We also provide an analytical solution to the problem in the context of Layered Media Multicast Control (LMMC) protocol. In doing so, we present closed-form expressions describing the temporally correlated loss pattern of communication networks. Utilizing our closed form expressions, we rely on an a priori estimate of loss along with a hybrid proactive FEC-ARQ scheme to statistically guarantee the QoS for the receivers of a media system. We show the effectiveness of our protocol by means of simulating realistic error control scenarios.
C1 Univ Calif Irvine, Dept Elect Engn & Comp Sci, Irvine, CA 92697 USA.
C3 University of California System; University of California Irvine
RP Habibi, A (corresponding author), Univ Calif Irvine, Dept Elect Engn & Comp Sci, Irvine, CA 92697 USA.
EM hy-ousefi@uci.edu; hamidj@uci.edu; ahabibi1@uci.edu
OI Jafarkhani, Hamid/0000-0001-6838-8038
CR Bolot JC, 1999, IEEE INFOCOM SER, P1453, DOI 10.1109/INFCOM.1999.752166
   BOLOT JC, 1996, P INT C IM PROC SEPT, V1, P25
   Carle G, 2000, LECT NOTES COMPUT SC, V1922, P245
   Cheung SY, 1996, IEEE INFOCOM SER, P553, DOI 10.1109/INFCOM.1996.493348
   Chou PA, 2001, IEEE T MULTIMEDIA, V3, P108, DOI 10.1109/6046.909598
   DANZIG PB, 1994, IEEE T SOFTWARE ENG, V20, P1, DOI 10.1109/32.263751
   DEERING SE, 1990, ACM T COMPUT SYST, V8, P85, DOI 10.1145/78952.78953
   FLOYD S, 1995, P ACM SIGCOMM OCT
   Gersho A., 2003, Vector Quantization and Signal Compression
   GILBERT EN, 1960, BELL SYST TECH J SEP
   JAFARKHANI H, 1996, P IEEE VTC APR, V1, P492
   KERMODE RG, 1998, P ACM SIGCOMM SEP
   Lee HC, 2003, SIGNAL PROCESS-IMAGE, V18, P1, DOI 10.1016/S0923-5965(02)00089-9
   LEVINE B, 1996, P ACM MULT NOV
   Li X, 1999, IEEE NETWORK, V13, P46, DOI 10.1109/65.768488
   LI X, 1997, P ACM NOSSDAV MAY
   Lin JC, 1996, IEEE INFOCOM SER, P1414, DOI 10.1109/INFCOM.1996.493090
   Majumdar A, 2002, IEEE T CIRC SYST VID, V12, P524, DOI 10.1109/TCSVT.2002.800315
   Maxemchuk NF, 1997, 1997 INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS - PROCEEDINGS, P259, DOI 10.1109/ICNP.1997.643726
   MCCANNE S, 1996, P ACM SIGCOMM SEP
   NAMBURI P, 2004, P IASTED C COMM INT
   Nonnenmacher J, 1998, IEEE INFOCOM SER, P972, DOI 10.1109/INFCOM.1998.662906
   NONNENMACHER J, 1997, P ACM SIGCOMM SEP
   RHEE I, 2000, P IEEE INFOCOM 00, V2, P805
   RUBENSTEIN D, 1998, P ACM NOSSDAV
   Towsley D, 1997, IEEE J SEL AREA COMM, V15, P398, DOI 10.1109/49.564137
   Tzeng HY, 1997, IEEE J SEL AREA COMM, V15, P545, DOI 10.1109/49.564148
   Wang C. Y., 2001, Int. J. Struct. Stab. Dyn., V1, P163
   WANG HA, 1998, P ACM SIGCOMM SEP
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   XU XR, 1997, P ACM NOSSDAV MAY
   YAJNIK M, 1999, P IEEE INFOCOM
   YOUAWFIZADEH H, 2004, P IEEE DCC, V1, P345
   Yousefi'zadeh H, 2005, IEEE ACM T NETWORK, V13, P540, DOI 10.1109/TNET.2005.850227
   Yousefi'zadeh H, 2004, IEEE T IMAGE PROCESS, V13, P873, DOI 10.1109/TIP.2004.827234
   YOUSEFIZADEH H, 2003, P IEEE GLOBECOM
   YOUSEFIZADEH H, 2004, P IEEE IEE HIGH SPEE
NR 37
TC 2
Z9 4
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1219
EP 1227
DI 10.1109/TMM.2006.884612
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700011
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Kim, SE
   Han, JK
   Kim, JG
AF Kim, Sung-Eun
   Han, Jong-Ki
   Kim, Jae-Gon
TI An efficient scheme for motion estimation using multireference frames in
   H.264/AVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE h.264; motion estimation
AB The multiple reference frame motion compensation (MRMC) supported by H.264 makes use of the redundancy which is between multiple frames to enhance the coding efficiency over a scheme using the single reference frame motion compensation (SRMC) in which motion vectors are searched over a single reference frame. And, the technique using multiple reference frames can combat the channel errors efficiently. However, searching the motion vectors in multiple frames may require a huge computing time. This paper proposes a novel motion estimation procedure, which has a lower search complexity without sacrificing image quality. To reduce the complexity of motion estimation procedure, we use a temporary motion vector generated with little computation. The temporary motion vector is calculated from the motion vector map composed of motion vectors between successive frames, and used to predict the optimal motion vector for a reference frame. The proposed scheme requires the lower complexity than conventional schemes by using the temporary motion vector and refinement process over a narrow search range around the temporary predictive motion vector. Since the temporary predictive motion vector effectively chases the optimal motion vector for each reference frame, the encoded image quality by proposed scheme is very similar to that of full search algorithm.
   The proposed motion estimation process consists of three phases: 1) making a vector map between two consecutive frames, where the vector map is constructed by copying motion vectors which have been estimated in first reference frame, 2) composing a temporary motion vector with element vectors which are in the vector map, and 3) finally, the temporary predictive motion vector is refined over a narrow search range.
   We show experimental results which demonstrate the effectiveness of the proposed method. To compare the proposed motion estimation algorithm with the conventional schemes, we check the CPU times consumed by ME module in H.264 encoder using the proposed scheme. In the results, CPU time consumed by the proposed scheme has been reduced significantly without additional distortion of the encoded video quality.
C1 Sejong Univ, Dept Informat & Commun Engn, Seoul, South Korea.
   Elect & Telecommun Res Inst, Broadcasting Media Res Grp, Taejon 305606, South Korea.
C3 Sejong University; Electronics & Telecommunications Research Institute -
   Korea (ETRI)
RP Kim, SE (corresponding author), Sejong Univ, Dept Informat & Commun Engn, Seoul, South Korea.
EM kimse@tera-mail.com; hjk@sejong.ac.kr; jgkim@etri.re.kr
OI Han, Jong-Ki/0000-0002-5036-7199
CR ALMUALLA ME, 2000, P IEEE INT S ISCAS 0, V4, P733
   Chang A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P413
   Flierl M, 2003, IEEE T CIRC SYST VID, V13, P587, DOI 10.1109/TCSVT.2003.814963
   List P, 2003, IEEE T CIRC SYST VID, V13, P614, DOI 10.1109/TCSVT.2003.815175
   Luthra A, 2003, IEEE T CIRC SYST VID, V13, P557, DOI 10.1109/TCSVT.2003.815169
   Malvar HS, 2003, IEEE T CIRC SYST VID, V13, P598, DOI 10.1109/TCSVT.2003.814964
   Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173
   RICHARDSON IEG, 2003, H264 MPEG4
   Stockhammer T, 2003, IEEE T CIRC SYST VID, V13, P657, DOI 10.1109/TCSVT.2003.815167
   Ting CW, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS & SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P1258
   Wenger S, 2003, IEEE T CIRC SYST VID, V13, P645, DOI 10.1109/TCSVT.2003.814966
   Wiegand T, 1999, IEEE T CIRC SYST VID, V9, P70, DOI 10.1109/76.744276
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wien M, 2003, IEEE T CIRC SYST VID, V13, P604, DOI 10.1109/TCSVT.2003.815380
NR 14
TC 32
Z9 41
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 457
EP 466
DI 10.1109/TMM.2006.870740
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000003
DA 2024-07-18
ER

PT J
AU Zhang, D
   Gatica-Perez, D
   Bengio, S
   McCowan, I
AF Zhang, Dong
   Gatica-Perez, Daniel
   Bengio, Sainy
   McCowan, Lain
TI Modeling individual and group actions in meetings with layered HMMs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE human interaction recognition; multimodal processing and multimedia
   applications; statistical models
ID SPEAKING TURNS
AB We address the problem of recognizing sequences of human interaction patterns in meetings, with the goal of structuring them in semantic terms. The investigated patterns are inherently group-based (defined by the individual activities of meeting participants, and their interplay), and multimodal (as captured by cameras and microphones). By defining a proper set of individual actions, group actions can be modeled as a two-layer process, one that models basic individual activities from low-level audio-visual (AV) features, and another one that models the interactions. We propose a two-layer hidden Markov model (HMM) framework that implements such concept in a principled manner, and that has advantages over previous works. First, by decomposing the problem hierarchically, learning is performed on low-dimensional observation spaces, which results in simpler models. Second, our framework is easier to interpret, as both individual and group actions have a clear meaning, and thus easier to improve. Third, different HMMs can be used in each layer, to better reflect the nature of each subproblem. Our framework is general and extensible, and we illustrate it with a set of eight group actions, using a public 5-hour meeting corpus. Experiments and comparison with a single-layer HMM baseline system show its validity.
C1 IDIAP, Res Inst, CH-1920 Martigny, Switzerland.
RP Zhang, D (corresponding author), IDIAP, Res Inst, CH-1920 Martigny, Switzerland.
EM zhang@idiap.ch; gatica@idiap.ch; bengio@idiap.ch; iain.mccowan@csiro.au
CR Basu S., 2001, P IEEE CVPR WORKSH C
   BENGIO S, 2003, P NIPS, V15
   Chiu Patrick, 2001, P 10 INT C WORLD WID, P140
   Cutler R., 2002, P ACM MULTIMEDIA
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   DiBiase JH, 2001, DIGITAL SIGNAL PROC, P157
   DIELMANN A, 2004, P IEEE ICASSP
   DUNCAN S, 1972, J PERS SOC PSYCHOL, V23, P283, DOI 10.1037/h0033031
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Fay N, 2000, PSYCHOL SCI, V11, P481, DOI 10.1111/1467-9280.00292
   Galata A., 1999, BRIT MACH VIS C
   Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716
   Gibbon D., 1997, Handbook of standards and resources for spoken language system
   HILLARD D, 2003, P HLT NAACL C EDM AB
   HONGENG S, 2001, P IEEE ICCV      JUL
   KRAUSS RM, 1977, J PERS SOC PSYCHOL, V35, P523, DOI 10.1037/0022-3514.35.7.523
   Kubala F, 1999, ACM COMPUT SURV, V31, pU49
   LATHOUD G, 2003, P ICASSP
   MARKEL JD, 1972, IEEE T ACOUST SPEECH, VAU20, P367, DOI 10.1109/TAU.1972.1162410
   Massaro DW, 1998, AM SCI, V86, P236, DOI 10.1511/1998.25.861
   McCowan I, 2005, IEEE T PATTERN ANAL, V27, P305, DOI 10.1109/TPAMI.2005.49
   MCCOWAN I, 2003, P IEEE ICASSP    APR
   McGrath J.E., 1984, GROUPS INTERACTION P
   MIRGHAFORI N, 1998, P ICASSP
   MOORE D, 2002, IDIAPCOM07
   MORGAN N, 2001, P HLT C SAN DIEG CA
   Morgan N., 1998, P ICASSP
   Murphy Kevin Patrick, 2002, Dynamic bayesian networks: representation, inference and learning
   Novick D., 1996, P INT C SPOK LANG PR
   OLIVER N, 2002, P ICMI OCT
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   PADILHA E, 2003, P 1 INT NORD S MULT
   PARKER KCH, 1988, J PERS SOC PSYCHOL, V54, P965, DOI 10.1037/0022-3514.54.6.965
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   STARNER T, 1995, P INT WORKSH AFGR ZU
   STEIFELHAGEN R, 2002, IEEE C MULT INT PITT
   VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010
   WAIBEL A, 1999, P IEEE ICASSP    MAY
   WREDE B, 2003, P ASRU           DEC
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   XIE L, 2003, P ICME           JUL
   YANG J, 1998, P ACCV
   ZHANG D, 2004, P IEEE CVPR WORKSH E
NR 43
TC 104
Z9 124
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 509
EP 520
DI 10.1109/TMM.2006.870735
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, SQ
   Shen, B
   Wee, S
   Zhang, XD
AF Chen, SQ
   Shen, B
   Wee, S
   Zhang, XD
TI Segment-based streaming media proxy: Modeling and optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content distribution; proxy caching; streaming media; system design
ID VIDEO
AB Researchers often use segment-based proxy caching strategies to deliver streaming media by partially caching media objects. The existing strategies mainly consider increasing the byte hit ratio and/or reducing the client perceived startup latency (denoted by the metric delayed startup ratio). However, these efforts do not guarantee continuous media delivery because the to-be-viewed object segments may not be cached in the proxy when they are demanded. The potential consequence is playback jitter at the client side due to proxy delay in fetching the uncached segments, which we call proxy jitter. Thus, for the best interests of clients, a correct model for streaming proxy system design should aim to minimize proxy jitter subject to reducing the delayed startup ratio and increasing the byte hit ratio. However, we have observed two major pairs of conflicting interests inherent in this model: (1) one between improving the byte hit ratio and reducing proxy jitter, and (2) the other between improving the byte hit ratio and reducing the delayed startup ratio. In this study, we first propose and analyze prefetching methods for in-time prefetching of uncached segments, which provides insights into the first pair of conflicting interests. Second, to address the second pair of the conflicting interests, we build a general model to analyze the performance tradeoff between the second pair of conflicting performance objectives. Finally, considering our main objective of minimizing proxy jitter and optimizing the two tradeoffs, we propose a new streaming proxy system called Hyper Proxy. Synthetic and real workloads are used to evaluate our system. The performance results show that Hyper Proxy generates minimum proxy jitter with a low delayed startup ratio and a small decrease of byte hit ratio compared with existing schemes.
C1 George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
   Hewlett Packard Labs, Mobile & Media Syst Lab, Palo Alto, CA 94304 USA.
   Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
C3 George Mason University; Hewlett-Packard; University System of Ohio;
   Ohio State University
RP George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
EM sqchen@cs.gmu.edu; boshen@hpl.hp.com; swee@hpl.hp.com;
   zhang@cse.ohio-state.edu
CR BOMMAIAH E, 2000, P IEEE REAL TIM TECH, P121
   Breslau L, 1999, IEEE INFOCOM SER, P126, DOI 10.1109/INFCOM.1999.749260
   Chae YS, 2002, IEEE J SEL AREA COMM, V20, P1328, DOI 10.1109/JSAC.2002.802062
   CHEN S, 2004, P ACM SPIE C MULT CO
   CHEN S, 2003, P ACM NOSSDAV MONT C
   Chen SQ, 2004, IEEE INFOCOM SER, P1512
   CHERKASOVA L, 2002, P ACM NOSSDAV MIAM
   CHIU MY, 1997, P 13 INT C DAT ENG B
   Gruber S, 2000, COMPUT NETW, V33, P657, DOI 10.1016/S1389-1286(00)00058-X
   JUNG J, 2000, P WWW ASMT NETH MAY
   Kangasharju J, 2002, IEEE T COMPUT, V51, P622, DOI 10.1109/TC.2002.1009148
   Khan J. I., 2001, P 3 USENIX S INT TEC
   MA W, 2000, P INT C MULT EXP NEW
   Miao ZR, 2002, IEEE J SEL AREA COMM, V20, P1315, DOI 10.1109/JSAC.2002.802061
   REISSLEIN M, 2000, P 1 INT WORKSH INT M
   Rejaie R., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P980, DOI 10.1109/INFCOM.2000.832273
   REJAIE R, 1999, P ACM SIGCOMM CAMBR
   REJAIE R, 1999, P INT WEB CACH WORKS
   ROY S, 2004, P 9 INT WORKSH WEB C
   SEN S, 1999, P IEEE INFOCOM NEW Y
   TEWARI R, 1998, P ACM SPIE C MULT CO
   WANG B, 2002, P IEEE INFOCOM NEW Y
   WU K, 2001, P WWW HONG KONG MAY
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
   [No title captured]
NR 25
TC 26
Z9 38
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 243
EP 256
DI 10.1109/TMM.2005.864281
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kanumuri, S
   Cosman, PC
   Reibman, AR
   Vaishampayan, VA
AF Kanumuri, S
   Cosman, PC
   Reibman, AR
   Vaishampayan, VA
TI Modeling packet-loss visibility in MPEG-2 video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE packet-loss visibility; perceptual quality metrics; subjective testing;
   video quality
ID QUALITY; DETECTABILITY; QUANTIZATION; ARTIFACTS
AB We consider the problem of predicting packet loss visibility in MPEG-2 video. We use two modeling approaches: CART and GLM. The former classifies each packet loss as visible or not; the latter predicts the probability that a packet loss is visible. For each modeling approach, we develop three methods, which differ in the amount of information available to them. A reduced reference method has access to limited information based on the video at the encoder's side and has access to the video at the decoder's side. A no-reference pixel-based method has access to the video at the decoder's side but lacks access to information at the encoder's side. A no-reference bitstream-based method does not have access to the decoded video either; it has access only to the compressed video bit-stream, potentially affected by packet losses. We design our models using the results of a subjective test based on 1080 packet losses in 72 minutes of video.
C1 Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
   AT&T Labs Res, Florham Pk, NJ 07932 USA.
C3 University of California System; University of California San Diego;
   AT&T
RP Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM skanumur@code.ucsd.edu; pcosman@code.ucsd.edu; amy@research.att.com;
   vinay@research.att.com
OI Vaishampayan, Vinay/0000-0001-7781-0990; Reibman,
   Amy/0000-0003-1859-1091; Cosman, Pamela/0000-0002-4012-0176
CR Breiman L, 1998, BIOMETRICS, DOI [10.1201/9781315139470, DOI 10.2307/2530946]
   Brunnström K, 2002, P SOC PHOTO-OPT INS, V4662, P149, DOI 10.1117/12.469511
   CERMAK GW, 2003, 132003026 TIA VER LA
   Chandler DM, 2003, J OPT SOC AM A, V20, P1164, DOI 10.1364/JOSAA.20.001164
   CHEN B, 2003, AT T TECH MEM    FEB
   Conway AE, 2002, IEEE IC COMP COM NET, P116, DOI 10.1109/ICCCN.2002.1043055
   Gastaldo P, 2002, IEEE T NEURAL NETWOR, V13, P939, DOI 10.1109/TNN.2002.1021894
   Hughes CJ, 1993, IEEE T IMAGE PROCESS, V2, P212, DOI 10.1109/83.217224
   KANUMURI S, 2004, INT PACK VID WORKSH
   Lu JH, 1999, P SOC PHOTO-OPT INS, V3845, P290, DOI 10.1117/12.371212
   MASRY M, 2004, SIGNAL PROCESS IMAGE, V2
   MCCULLAGH P, GENERALIZED LINEAR M
   Mohamed S, 2002, IEEE T CIRC SYST VID, V12, P1071, DOI 10.1109/TCSVT.2002.806808
   Moore MS, 2000, P SOC PHOTO-OPT INS, V3959, P99, DOI 10.1117/12.387146
   MOORE MS, 2002, P IEEE ICIP JUN, V3, P45
   Ramos MG, 2001, J OPT SOC AM A, V18, P2385, DOI 10.1364/JOSAA.18.002385
   Reibman AR, 2004, IEEE T MULTIMEDIA, V6, P327, DOI 10.1109/TMM.2003.822785
   REIBMAN AR, 2004, P IEEE ICIP OCT
   Verscheure O, 1999, REAL-TIME IMAGING, V5, P305, DOI 10.1006/rtim.1999.0175
   Verscheure O, 1998, GLOBECOM 98: IEEE GLOBECOM 1998 - CONFERENCE RECORD, VOLS 1-6, P71, DOI 10.1109/GLOCOM.1998.775702
   WATSON A, 1998, ACM MULTIMEDIA, P55
   Winkler S, 2003, PROC SPIE, V5007, P104, DOI 10.1117/12.477766
   WOLF S, 1998, IN SERVICE PERFORMAN
   Yu ZH, 2002, P IEEE, V90, P154, DOI 10.1109/5.982412
NR 24
TC 103
Z9 117
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 341
EP 355
DI 10.1109/TMM.2005.864343
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300014
OA Green Published
DA 2024-07-18
ER

PT J
AU Daras, P
   Zarpalas, D
   Tzovaras, D
   Strintzis, MG
AF Daras, P
   Zarpalas, D
   Tzovaras, D
   Strintzis, MG
TI Efficient 3-D model search and retrieval using Generalized 3-D Radon
   Transforms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generalized Radon Transforms; 3-D content-based search
AB Measuring the similarity between three-dimensional (3-D) objects is a challenging problem, with applications in computer vision, molecular biology, computer graphics, and many other areas. This paper describes a novel method for 3-D model content-based search based on the 3-D Generalized Radon Transform and a querying by-3-D-model approach. A set of descriptor vectors is extracted using the Radial Integration Transform (RIT) and the Spherical Integration Transform (SIT), which represent significant shape characteristics. After the proper alignment of the models, descriptor vectors are produced which are invariant in terms of translation, scaling and rotation. Experiments were performed using three different databases and comparing the proposed method with those most commonly cited in the literature. Experimental results show that the proposed method is adequately satisfactory in terms of both precision versus recall and time needed for retrieval, and that it can be used for 3-D model search and retrieval in a highly efficient manner.
C1 Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Informat Proc Lab, Thessaloniki 54006, Greece.
   Informat & Telemat Inst, GR-57001 Thessaloniki, Greece.
C3 Aristotle University of Thessaloniki
RP Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Informat Proc Lab, Thessaloniki 54006, Greece.
EM daras@iti.gr; zarpalas@iti.gr; tzovaras@iti.gr; strintzi@eng.auth.gr
RI Daras, Petros/F-5284-2012; Tzovaras, Dimitrios/ABB-9576-2021
OI Daras, Petros/0000-0003-3814-6710; Tzovaras,
   Dimitrios/0000-0001-6915-6722
CR ANKERST M., 1999, P 6 INT S SPAT DAT S
   [Anonymous], SAE T J AEROSPACE
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 1996, RADON TRANSFORM THEO
   Berchtold S., 1997, PROC SIGMOD 97, P564
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BLAIS G, 1995, IEEE T PATTERN ANAL, V17, P820, DOI 10.1109/34.400574
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chua CS, 1996, INT J COMPUT VISION, V17, P77, DOI 10.1007/BF00127819
   Cicirello V, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P176, DOI 10.1109/SMA.2001.923388
   Cyr CM, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P254, DOI 10.1109/ICCV.2001.937526
   ELAD M, 1999, IEEE T KNOWL DATA EN, V11
   ELVINS T, 1997, ACM S US INT SOFTW T
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   GAIN J, 1999, SIGGRAPH TECHN SKETC, V241
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   HORN B, 1998, J OPTICAL SOC, P1127
   HUGLI H, 1995, ACCV, V3, P819
   HUGLI H, 1997, P INT C REC ADV 3 D
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kadyrov A, 2001, IEEE T PATTERN ANAL, V23, P811, DOI 10.1109/34.946986
   Kazhdan M, 2002, LECT NOTES COMPUT SC, V2351, P642
   KAZHDAN M, 2003, S GEOM PROC JUN
   Kolonias I, 2005, IEEE T MULTIMEDIA, V7, P114, DOI 10.1109/TMM.2004.840605
   LOFFLER J, 2000, P INT C INF VIS IV20
   Matusik Wojciech, 2000, ACM SIGGRAPH
   Noo F, 1997, IEEE T NUCL SCI, V44, P1309, DOI 10.1109/23.597006
   Novotni M, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P167, DOI 10.1109/SMA.2001.923387
   Ohbuchi R, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P265, DOI 10.1109/PCCGA.2002.1167870
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386
   PAQUET E, 1998, P IEEE C COMP VIS PA
   RUBNER Y, P IEEE INT C COMP VI
   SCHURMANS U, 2001, COMPUT APPL ARCHAEOL
   Shilane P., 2004, PRINCETON SHAPE BENC
   Shum HY, 1996, PROC CVPR IEEE, P526, DOI 10.1109/CVPR.1996.517122
   Simitopoulos D, 2003, IEEE T CIRC SYST VID, V13, P732, DOI 10.1109/TCSVT.2003.815947
   Suzuki MT, 2001, JOINT 9TH IFSA WORLD CONGRESS AND 20TH NAFIPS INTERNATIONAL CONFERENCE, PROCEEDINGS, VOLS. 1-5, P2271, DOI 10.1109/NAFIPS.2001.944425
   TANGELDER JWH, 2003, P INT C SHAP MOD APP
   Vranic D.V., 2001, P EURASIP C DIG SIGN
   Vranic DV, 2001, ICCIMA 2001: FOURTH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND MULTIMEDIA APPLICATIONS, PROCEEDINGS, P266, DOI 10.1109/ICCIMA.2001.970477
   Vranic DV, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P293, DOI 10.1109/MMSP.2001.962749
   Zhang C, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P935, DOI 10.1109/ICIP.2001.958278
   ZHANG D, 1999, P IEEE C COMP VIS PA
   [No title captured]
NR 45
TC 47
Z9 56
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 101
EP 114
DI 10.1109/TMM.2005.861287
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000010
DA 2024-07-18
ER

PT J
AU Hilt, V
   Mauve, M
   Vogel, J
   Effelsberg, W
AF Hilt, V
   Mauve, M
   Vogel, J
   Effelsberg, W
TI Recording and playing back interactive media streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE interactive media; media server; random access; recording
AB Recording systems and media servers for networked audio and video streams have become an important part of today's Internet. In contrast to this, only a few recording and playback solutions currently exist for the data streams of interactive media applications (e.g., shared whiteboards and distributed virtual environments). So far these solutions are application-specific: individual algorithms and implementations are required for each application that is to be recorded. In this paper, we are proposing generic algorithms for the recording and playback of interactive media streams. These algorithms are based on a common model for the class of interactive media. They enable full random access to recordings by initializing the replaying applications with the required state information (e.g., the current slide in a recorded presentation). We have implemented these algorithms in the Interactive Media on Demand (IMoD) system. In order to interpret the semantics of an interactive media stream, the system requires that the Real-Time Application-Level Protocol for Distributed Interactive Media (RTP/I) protocol is used for the framing of the transmitted data. Any application using RTP/I can be recorded directly using the system without any, modification. Interactive media streams not using RTP/I can be recorded using the generic recording algorithms. However, they require an adaptation of the system so that it is able to extract a minimal set of information from the application-level protocol of these streams. In addition to the generic recording algorithms, we present the architecture and major design considerations of the system and discuss the experiences gained from recording different interactive media applications.
C1 Univ Mannheim, D-68161 Mannheim, Germany.
C3 University of Mannheim
RP Hilt, V (corresponding author), Bell Labs, Lucent Technol, 101 Crawford Corner Rd, Holmdel, NJ 07733 USA.
EM volkerh@bell-labs.com; mauve@cs.uni-duesseldorf.de;
   juergen.vogel@eml-d.villa-bosch.de;
   effelsberg@informatik.uni-mannheim.de
OI Vogel, Juergen/0009-0006-8150-5888; Hilt, Volker/0000-0002-1826-8297
CR ALMEROTH K, 1998, P 7 INT WWW C BRISB
   GEYER W, 1998, P ED MEDIA 1998 FREI
   Greenhalgh C., 2000, Proceedings ACM Multimedia 2000, P67, DOI 10.1145/354384.354429
   HANDLY M, INTERNET DRAFT
   Hilt V, 2001, 21ST INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING SYSTEMS WORKSHOPS, PROCEEDINGS, P63, DOI 10.1109/CDCS.2001.918688
   Lambrinos L, 1998, IEEE IC COMP COM NET, P208, DOI 10.1109/ICCCN.1998.998778
   Makofske DB, 2001, SOFTWARE PRACT EXPER, V31, P781, DOI 10.1002/spe.389
   Mauve M, 2004, IEEE T MULTIMEDIA, V6, P47, DOI 10.1109/TMM.2003.819751
   Mauve M, 2001, IEEE T MULTIMEDIA, V3, P152, DOI 10.1109/6046.909602
   Mauve M, 2003, MULTIMED TOOLS APPL, V20, P283, DOI 10.1023/A:1024076306065
   MAUVE M, THESIS U MANNHEIM IN
   MINNEMAN S, 1995, P ACM C MULT SAN FRA, P523
   MOHAN C, 1992, ACM T DATABASE SYST, V17, P94, DOI 10.1145/128765.128770
   MULLER R, 2000, MULTIMEDIA SYST J, V8
   PARNES P, 1999, THESIS LULEAU TECHNO
   SCHUETT A, 1999, 3550 RFC
   Schulzrinne H., 2007, Internet Draft
   Shirmohammadi S, 2003, MULTIMED TOOLS APPL, V19, P135, DOI 10.1023/A:1022143111606
   Tung T.L., 1998, THESIS U CALIFORNIA
   Vogel J., 2001, P 9 ACM INT C MULTIM, P221
NR 20
TC 3
Z9 4
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 960
EP 971
DI 10.1109/TMM.2005.854400
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900017
DA 2024-07-18
ER

PT J
AU Choi, KH
   Hwang, JN
AF Choi, KH
   Hwang, JN
TI Automatic creation of a talking head from a video sequence
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE MPEG-4 facial object; probabilistic approach; speech-driven talking
   heads; talking heads; virtual face
AB In this paper, a real-time system to create a talking head from a video sequence without any user intervention is presented. In the proposed system, a probabilistic approach, to decide whether or not extracted facial features are appropriate for creating a three-dimensional (3-D) face model, is presented. Automatically extracted two-dimensional facial features from a video sequence are fed into the proposed probabilistic framework before a corresponding 3-D face model is built to avoid generating an unnatural or nonrealistic 3-D face model. To extract face shape, we also present a face shape extractor based on an ellipse model controlled by three anchor points, which is accurate and computationally cheap. To create a 3-D face model, a least-square approach is presented to find a coefficient vector that is necessary to adapt a generic 3-D model into the extracted facial features. Experimental results show that the proposed system can efficiently build a 3-D face model from a video sequence without any user intervention for various Internet applications including virtual conference and a virtual story teller that do not require much head movements or high-quality facial animation.
C1 Mokpo Natl Univ, Dept Elect Engn, Chungnam 534729, South Korea.
   Univ Washington, Dept Elect Engn, Informat Proc Lab, Seattle, WA 98195 USA.
C3 Mokpo National University; University of Washington; University of
   Washington Seattle
RP Mokpo Natl Univ, Dept Elect Engn, Chungnam 534729, South Korea.
EM khchoi@mokpo.ac.kr
CR Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Choi K, 2002, INT CONF ACOUST SPEE, P2121
   Choi KH, 2002, IEEE IMAGE PROC, P984
   Cosatto E, 2000, IEEE T MULTIMEDIA, V2, P152, DOI 10.1109/6046.865480
   DELVALLE ACA, 2001, P IEEE INT S CIRC SY, P325
   Farkas LG, 1994, Anthropometry of Head and Face, Vsecond
   Fua P, 1999, COMPUT VIS IMAGE UND, V75, P247, DOI 10.1006/cviu.1999.0778
   *ISO IEC, 1998, 144962 ISOIEC
   *ISO IEC, 1998, 144961 ISOIEC
   KIM C, 2000, P ACM INT MULT C
   Kuo CJ, 2002, IEEE T CIRC SYST VID, V12, P183, DOI 10.1109/76.993439
   LAVAGETTO F, 1995, IEEE T REHABIL ENG, V3, P1
   Lee WS, 1999, COMP ANIM CONF PROC, P186, DOI 10.1109/CA.1999.781211
   LIN IC, 1999, P 7 PAC C COMP GRAPH, P43
   LUETTIN J, 1995, 9544 U SHEFF
   LUETTIN J, 1995, 9544 U SHEFF EL SYST
   Moccozet L, 1997, COMP ANIM CONF PROC, P93, DOI 10.1109/CA.1997.601047
   Parke F., 1996, COMPUTER FACIAL ANIM
   Pighin F., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P143, DOI 10.1109/ICCV.1999.791210
   Rao RR, 1998, IEEE T IND ELECTRON, V45, P15, DOI 10.1109/41.661300
   REISFELD D, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL I, P117, DOI 10.1109/ICPR.1992.201521
   Tao H, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P166, DOI 10.1109/AFGR.1998.670943
   TIAN Y, 2000, P 4 AS C COMP VIS
   Wang RS, 1997, 1997 IEEE FIRST WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P233, DOI 10.1109/MMSP.1997.602641
   Yow KC, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P16, DOI 10.1109/AFGR.1996.557238
   Zobel M., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P2, DOI 10.1109/AFGR.2000.840604
NR 26
TC 9
Z9 11
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 628
EP 637
DI 10.1109/TMM.2005.850964
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000005
DA 2024-07-18
ER

PT J
AU Cheung, SCS
   Zakhor, A
AF Cheung, SCS
   Zakhor, A
TI Fast similarity search and clustering of video sequences on the
   world-wide-web
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE clustering; dimension reduction; similarity search; video signature; web
   search
AB We define similar video content as video sequences with almost identical content but possibly compressed at different qualities, reformatted to different sizes and frame-rates, undergone minor editing in either spatial or temporal domain, or summarized into keyframe sequences. Building a search engine to identify such similar content in the World-Wide Web requires: 1) robust video similarity measurements; 2) fast similarity search techniques on large databases; and 3) intuitive organization of search results. In a previous paper, we proposed a randomized technique called the video signature (ViSig) method for video similarity measurement. In this paper, we focus on the remaining two issues by proposing a feature extraction scheme for fast similarity search, and a clustering algorithm for identification of similar clusters. Similar to many other content-based methods, the ViSig method uses high-dimensional feature vectors to represent video. To warrant a fast response time for similarity searches on high dimensional vectors, we propose a novel nonlinear feature extraction scheme on arbitrary metric spaces that combines the triangle inequality with the classical Principal Component Analysis (PCA). We show experimentally that the proposed technique outperforms PCA, Fastmap, Triangle-Inequality Pruning, and Haar wavelet on signature data. To further improve retrieval performance, and provide better organization of similarity search results, we introduce a new graph-theoretical clustering algorithm on large databases of signatures. This algorithm treats all signatures as an abstract threshold graph, where the distance threshold is determined based on local data statistics. Similar clusters are then identified as highly connected regions in the graph. By measuring the retrieval performance against a ground-truth set, we show that our proposed algorithm outperforms simple thresholding, single-link and complete-link hierarchical clustering techniques.
C1 Univ Kentucky, Dept Elect & Comp Engn, Lexington, KY 40506 USA.
   Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
C3 University of Kentucky; University of California System; University of
   California Berkeley
RP Univ Kentucky, Dept Elect & Comp Engn, Lexington, KY 40506 USA.
EM sccheung@ieee.org; avz@eecs.berkeley.edu
RI Zakhor, Avideh/GYA-1602-2022
OI Zakhor, Avideh/0000-0003-4770-6353
CR Adjeroh DA, 1998, INTERNATIONAL WORKSHOP ON MULTI-MEDIA DATABASE MANAGEMENT SYSTEMS- PROCEEDINGS, P72, DOI 10.1109/MMDBMS.1998.709503
   Berman AP, 1999, COMPUT VIS IMAGE UND, V75, P175, DOI 10.1006/cviu.1999.0772
   Bezdek J., 1999, FUZZY MODELS ALGORIT
   BOURGAIN J, 1985, ISRAEL J MATH, V52, P46, DOI 10.1007/BF02776078
   Broder AZ, 1997, COMPUT NETWORKS ISDN, V29, P1157, DOI 10.1016/S0169-7552(97)00031-7
   Chang HS, 1999, IEEE T CIRC SYST VID, V9, P1269, DOI 10.1109/76.809161
   CHEUNG SC, 2002, THESIS U CALIFORNIA
   Cheung SCS, 2003, IEEE T CIRC SYST VID, V13, P59, DOI 10.1109/TCSVT.2002.808080
   Ciaccia P, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P426
   CIEPLINSKI L, 2000, WOKII ISOIEC JTC1SC2
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   Cox T.F., 2001, Multidimensional Scalin, V46, P1050
   Faloutsos C., 1996, SEARCHING MULTIMEDIA
   Faloutsos C., 1995, P 1995 ACM SIGMOD IN, P163
   Golub G.H., 1989, MATRIX COMPUTATIONS
   GREENSPAN H, 2001, P IEEE C COMP VIS PA
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   HRISTESCU G, 1999, 9950 DIMACS
   Indyk P., 1999, FINDING PIRATED VIDE
   IYENGAR G, 2000, P 1998 INT C IM PROC, V3, P81
   Kruskal J. B., 1956, Proceedings of the American Mathematical Society, V7, P48
   LIENHART R, 1998, P SOC PHOTO-OPT INS, V3312, P271
   LINIAL N, 1995, COMBINATORICA, V15, P215, DOI 10.1007/BF01200757
   Naphade MR, 2001, PROC SPIE, V4315, P188, DOI 10.1117/12.410927
   Samet H., 1989, DESIGN ANAL SPATIAL
   SILVERSTEIN C, 1998, 1998014 SRC COMP SYS
   Theodoridis S., 1999, Pattern recognition, P3
   Vasconcelos N, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P400, DOI 10.1109/ICCV.2001.937653
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   ZAHN CT, 1971, IEEE T COMPUT, VC 20, P68, DOI 10.1109/T-C.1971.223083
NR 30
TC 40
Z9 51
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 524
EP 537
DI 10.1109/TMM.2005.846906
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kolonias, I
   Tzovaras, D
   Malassiotis, S
   Strintzis, MG
AF Kolonias, I
   Tzovaras, D
   Malassiotis, S
   Strintzis, MG
TI Fast content-based search of VRML models based on shape descriptors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D objects; indexing and retrieval; shape descriptors; VRML
   content-based search
ID IMAGE; REGISTRATION
AB The present paper proposes a novel method for content-based search in a database of VRML three-dimensional (3-D) models. The proposed technique is based on a querying-by-3-D-model approach. A set of shape-based descriptors are extracted from the reference 3-D model and compared to the corresponding descriptors of the VRML models contained in the database. The descriptors used vary from simple geometric measurements such as the aspect ratio or a binary 3-D shape mask to more complex and sophisticated shape-based criteria such as the edge paths of each 3-D model. Similarity measures are then introduced for the specific descriptors and introduced into a 3-D model-matching algorithm. Experimental results are presented, evaluating the performance of the proposed method.
C1 Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Informat Proc Lab, GR-54006 Thessaloniki, Greece.
   Informat & Telemat Inst, Thessaloniki 57001, Greece.
C3 Aristotle University of Thessaloniki
RP Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Informat Proc Lab, GR-54006 Thessaloniki, Greece.
EM strintzi@eng.auth.gr
RI Tzovaras, Dimitrios/ABB-9576-2021
OI Tzovaras, Dimitrios/0000-0001-6915-6722
CR Ankerst M., 1999, P 6 INT S LARG SPAT
   [Anonymous], IEEE COMPUT
   [Anonymous], SAE T J AEROSPACE
   Aslandogan YA, 1999, IEEE T KNOWL DATA EN, V11, P56, DOI 10.1109/69.755615
   Berchtold S., 1997, PROC SIGMOD 97, P564
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BLAIS G, 1995, IEEE T PATTERN ANAL, V17, P820, DOI 10.1109/34.400574
   Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X
   CANG E, 1999, P SOC PHOTO-OPT INS, V3527, P58
   Carson C, 2002, IEEE T PATTERN ANAL, V24, P1026, DOI 10.1109/TPAMI.2002.1023800
   Chua CS, 1996, INT J COMPUT VISION, V17, P77, DOI 10.1007/BF00127819
   DELBIMBO A, 1993, VISUAL INFORMATION R
   ELAD M, 2000, HPL200020R1 HP LAB I
   ELVINS T, 1997, P ACM S US INT SOFTW
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   GAIN J, 1999, SIGGRAPH TECHNICAL S, V241
   Guéziec A, 1999, IEEE COMPUT GRAPH, V19, P68, DOI 10.1109/38.749125
   Huang T.S., 1996, P 33 ANN CLIN LIB AP
   HUGLI H, 1997, P INT C REC ADV 3 D
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   KamgarParsi B, 1997, IEEE T PATTERN ANAL, V19, P1090, DOI 10.1109/34.625109
   Kompatsiaris I., 2001, P IEEE INT C IM PROC
   Ma WY, 1999, MULTIMEDIA SYST, V7, P184, DOI 10.1007/s005300050121
   MORI G, 2001, P COMP VIS PATT REC
   Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386
   PAQUET E, 1998, P IEEE C COMP VIS PA
   Pedrycz W., 1998, An introduction to fuzzy sets: analysis and design
   Pentland A, 1996, INT J COMPUT VISION, V18, P233, DOI 10.1007/BF00123143
   SCHUMANS U, 2001, COMPUT APPL ARCHAEOL
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   Suzuki MT, 2001, JOINT 9TH IFSA WORLD CONGRESS AND 20TH NAFIPS INTERNATIONAL CONFERENCE, PROCEEDINGS, VOLS. 1-5, P2271, DOI 10.1109/NAFIPS.2001.944425
   Vranic DV, 2001, ICCIMA 2001: FOURTH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND MULTIMEDIA APPLICATIONS, PROCEEDINGS, P266, DOI 10.1109/ICCIMA.2001.970477
   ZHANG C, P ICIP 2001 THESS GR
   ZHANG D, 1999, P IEEE C COMP VIS PA
   [No title captured]
   2001, MESHNOSE
NR 36
TC 14
Z9 16
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 114
EP 126
DI 10.1109/TMM.2004.840605
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300012
DA 2024-07-18
ER

PT J
AU Ekin, A
   Tekalp, AM
   Mehrotra, R
AF Ekin, A
   Tekalp, AM
   Mehrotra, R
TI Integrated semantic-syntactic video modeling for search and browsing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE events; integrated video model; model-based query formation; object
   motion description; query resolution by graph matching; video objects
ID IMAGE RETRIEVAL; IMPLEMENTATION; SYSTEM
AB Video processing and computer vision communities usually employ shot-based or object-based structural video models and associate low-level (color, texture, shape, and motion) and semantic descriptions (textual annotations) with these structural (syntactic) elements. Database and information retrieval communities, on the other hand, employ entity-relation or object-oriented models to model the semantics of multimedia documents. This paper proposes a new generic integrated semantic-syntactic video model to include all of these elements within a single framework to enable structured video search and browsing combining textual and low-level descriptors. The proposed model includes semantic entities (video objects and events) and the relations between them. We introduce a new "actor" entity to enable grouping of object roles in specific events. This context-dependent classification of attributes of an object allows for more efficient browsing and retrieval. The model also allows for decomposition of events into elementary motion units and elementary reaction/interaction units in order to access mid-level semantics and low-level video features. The instantiations of the model are expressed as graphs. Users can formulate flexible queries that can be translated into such graphs. Alternatively, users can input query graphs by editing an abstract model (model template). Search and retrieval is accomplished by matching the query graph with those instantiated models in the database. Examples and experimental results are provided to demonstrate the effectiveness of the proposed integrated modeling and querying framework.
C1 Univ Rochester, Dept Elect & Comp Engn, Rochester, NY 14627 USA.
   Koc Univ, Coll Engn, Istanbul, Turkey.
   Eastman Kodak Co, Entertainment Imaging Div, Rochester, NY 14650 USA.
C3 University of Rochester; Koc University; Eastman Kodak
RP Philips Res, Eindhoven, Netherlands.
EM ahmet.ekin@philips.com; tekalp@ece.rochester.edu;
   rajiv.mehrotra@kodak.com
RI Tekalp, Murat/AAW-1060-2020
OI Tekalp, Ahmet Murat/0000-0003-1465-8121
CR Adali S, 1996, MULTIMEDIA SYST, V4, P172, DOI 10.1007/s005300050021
   ALLEN JF, 1983, COMMUN ACM, V26, P832, DOI 10.1145/182.358434
   ALSAFADI LAE, 2000, P 23 AUSTR COMP SCI, P2
   ANDRIES M, 1992, LECT NOTES COMPUT SC, V580, P21, DOI 10.1007/BFb0032421
   [Anonymous], DATABASE SYSTEMS, DOI DOI 10.1145/320434.320440
   Antani S, 2002, PATTERN RECOGN, V35, P945, DOI 10.1016/S0031-3203(01)00086-3
   Aslandogan YA, 1999, IEEE T KNOWL DATA EN, V11, P56, DOI 10.1109/69.755615
   BACH JR, 1996, P SPIE STOR RETR STI, V4, P76
   Chang SF, 1998, IEEE T CIRC SYST VID, V8, P602, DOI 10.1109/76.718507
   CHANG SK, 1984, IEEE T PATTERN ANAL, V6, P475, DOI 10.1109/TPAMI.1984.4767552
   CHEN W, 2000, P IEEE INT C MULT EX, V3, P1337
   Cheng J. K. C., 1999, Proceedings of the 14th World Congress. International Federation of Automatic Control, P199
   Colombo C, 1999, IEEE MULTIMEDIA, V6, P38, DOI 10.1109/93.790610
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596
   Dagtas S, 2000, IEEE T IMAGE PROCESS, V9, P88, DOI 10.1109/83.817601
   EGENHOFER MJ, 1991, INT J GEOGR INF SYST, V5, P161, DOI 10.1080/02693799108927841
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   EKIN A, 2001, P IEEE ICIP THESS GR
   EKIN A, 2002, P IS T SPIE C VIS CO
   FLICKNER M, 1995, COMPUTER, V28, P23, DOI 10.1109/2.410146
   Fu Y, 2002, IEEE T IMAGE PROCESS, V11, P135, DOI 10.1109/83.982821
   Garey M.R., 1979, COMPUTERS INTRACTABI
   GYSSENS M, 1994, IEEE T KNOWL DATA EN, V6, P572, DOI 10.1109/69.298174
   Hacid MS, 2000, IEEE T KNOWL DATA EN, V12, P729, DOI 10.1109/69.877505
   HJELSVOLD R, 1994, P 20 VLDB C SANT CHI
   Idris F, 1997, J VIS COMMUN IMAGE R, V8, P146, DOI 10.1006/jvci.1997.0355
   *ISO IEC COMM, 2001, ISOIECJTC1SC29WG11N4
   Koh JL, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P472, DOI 10.1109/MMCS.1999.778508
   LI JZ, 1996, P IEEE P INT WORKSH
   *MPEG 7, 2001, ISOIECJTC1SC29WG11MP
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   OOMOTO E, 1993, IEEE T KNOWL DATA EN, V5, P629, DOI 10.1109/69.234775
   PENTLAND A, 1994, P IS T SPIE C STOR R
   Porkaew K, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P747, DOI 10.1109/MMCS.1999.778578
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Santini S, 2000, IEEE MULTIMEDIA, V7, P26, DOI 10.1109/93.879766
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   SMITH JR, 1996, P ACM C MULT BOST MA
   SMITH JR, 2000, P IEEE INT C MULT EX, V2, P915
   Smoliar S. W., 1994, IEEE Multimedia, V1, P62, DOI 10.1109/93.311653
   Yoshitaka A, 1996, J VISUAL LANG COMPUT, V7, P423, DOI 10.1006/jvlc.1996.0022
   Yoshitaka A, 1999, IEEE T KNOWL DATA EN, V11, P81, DOI 10.1109/69.755617
   [No title captured]
NR 44
TC 26
Z9 33
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 839
EP 851
DI 10.1109/TMM.2004.837238
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200007
DA 2024-07-18
ER

PT J
AU Ferman, AM
   Tekalp, AM
AF Ferman, AM
   Tekalp, AM
TI Two-stage hierarchical video summary extraction to match low-level user
   browsing preferences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cluster validation; fuzzy clustering; MPEG-7; video databases; visual
   summarization
ID CLUSTER-VALIDITY; RETRIEVAL; COLOR
AB A compact summary of video that conveys visual content at various levels of detail enhances user interaction significantly. In this paper, we propose a two-stage framework to generate MPEG-7-compliant hierarchical key frame summaries of video sequences. At the first stage, which is carried out off-line at the time of content production, fuzzy clustering and data pruning methods are applied to given video segments to obtain a nonredundant set of key frames that comprise the finest level of the hierarchical summary. Here, the number of key frames allocated to each shot or segment is determined dynamically and without user supervision through the use of cluster validation techniques. A coarser summary is generated on-demand in the second stage by reducing the number of key frames to match the low-level browsing preferences of a user. The proposed method has been validated by experimental results on a collection of video programs.
C1 Amer Inc, Sharp Labs, Camas, WA 98607 USA.
   Koc Univ, Coll Engn, Istanbul, Turkey.
   Univ Rochester, Dept Elect & Comp Engn, Rochester, NY 14627 USA.
C3 Koc University; University of Rochester
RP Amer Inc, Sharp Labs, Camas, WA 98607 USA.
EM mferman@sharplabs.com; tekalp@ece.rochester.edu
RI Tekalp, Murat/AAW-1060-2020
OI Tekalp, Ahmet Murat/0000-0003-1465-8121
CR ADBELJAOUED Y, 2000, P 10 EUR SIGN PROC C, P151
   AIGRAIN P, 1996, MULTIMED TOOLS APPL, V3, P3
   [Anonymous], 15938 ISOIEC
   BALL GH, 1967, BEHAV SCI, V12, P153, DOI 10.1002/bs.3830120210
   BEDNAR JB, 1984, IEEE T ACOUST SPEECH, V32, P145, DOI 10.1109/TASSP.1984.1164279
   Bezdek J. C., 1981, Pattern recognition with fuzzy objective function algorithms
   Chang SF, 2001, IEEE T CIRC SYST VID, V11, P688, DOI 10.1109/76.927421
   CHEN JY, 1999, P SPIE MULTIMEDIA ST, V3846
   DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909
   Dunn J.C., 1974, J CYBERNETICS, V3, P95, DOI 10.1080/019697274085460590304.68093
   Ferman AM, 1998, J VIS COMMUN IMAGE R, V9, P336, DOI 10.1006/jvci.1998.0402
   Ferman AM, 2002, IEEE T IMAGE PROCESS, V11, P497, DOI 10.1109/TIP.2002.1006397
   Ferman AM, 2000, IEEE IMAGE PROC, P65, DOI 10.1109/ICIP.2000.900893
   Furht B., 1995, Video and Image Processing in Multimedia Systems
   GATH I, 1989, IEEE T PATTERN ANAL, V11, P773, DOI 10.1109/34.192473
   GRESLE P, 1997, P VISUAL 97 SAN DIEG
   Gunderson M., 1978, Proceedings of 7th Triennial World IFCA Cong., Helsinki, Filind, P1319, DOI [10.1016/s1474-6670(17)66090-7, DOI 10.1016/S1474-6670(17)66090-7]
   Hanjalic A, 1997, P SOC PHOTO-OPT INS, V3022, P427, DOI 10.1117/12.263432
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   HANJALIC A, 1997, IMAGE DATABASES MULT, P97
   Jain A K, 1988, ALGORITHMS CLUSTERIN
   KOMLODI A, 1998, P 3 ACM C DIG LIB, P118
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   *MPEG REQ GROUP, 1998, DESCR MPEG 7 CONT SE
   PAL NR, 1995, IEEE T FUZZY SYST, V3, P370, DOI 10.1109/91.413225
   RATAKONDA K, 1999, P SOC PHOTO-OPT INS, V3653, P1531
   RUI Y, 1999, MULTIMEDIA SYST J, V7
   Salembier P, 2001, IEEE T CIRC SYST VID, V11, P748, DOI 10.1109/76.927435
   Sun XD, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P190, DOI 10.1109/MMCS.1998.693638
   TENENBAUM AM, 1981, DATA STRUCTURES USIN
   VANBEEK P, 2000, MPEG 7 MULTIMEDI JUL
   WINDHAM MP, 1981, FUZZY SET SYST, V5, P177, DOI 10.1016/0165-0114(81)90015-4
   XIE XLL, 1991, IEEE T PATTERN ANAL, V13, P841, DOI 10.1109/34.85677
   YEUNG MM, 1996, RC20615 IBM
   Zhang HJ, 1997, PATTERN RECOGN, V30, P643, DOI 10.1016/S0031-3203(96)00109-4
   Zhong D, 1996, PROC SPIE, V2670, P239, DOI 10.1117/12.234800
NR 36
TC 66
Z9 72
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2003
VL 5
IS 2
BP 244
EP 256
DI 10.1109/TMM.2003.811617
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 695HB
UT WOS:000183824100009
DA 2024-07-18
ER

PT J
AU Bai, CY
   Chen, HP
   Kumar, S
   Leskovec, J
   Subrahmanian, VS
AF Bai, Chongyang
   Chen, Haipeng
   Kumar, Srijan
   Leskovec, Jure
   Subrahmanian, V. S.
TI M2P2: Multimodal Persuasion Prediction Using Adaptive Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Visualization; Predictive models; TV; Noise measurement;
   Training; Correlation; Multimodal learning; persuasion; adaptive fusion
AB Identifying persuasive speakers in an adversarial environment is a critical task. In a national election, politicians would like to have persuasive speakers campaign on their behalf. When a company faces adverse publicity, they would like to engage persuasive advocates for their position in the presence of adversaries who are critical of them. Debates represent a common platform for these forms of adversarial persuasion. This paper solves two problems: the Debate Outcome Prediction (DOP) problem predicts who wins a debate while the Intensity of Persuasion Prediction (IPP) problem predicts the change in the number of votes before and after a speaker speaks. Though DOP has been previously studied, we are the first to study IPP. Past studies on DOP fail to leverage two important aspects of multimodal data: 1) multiple modalities are often semantically aligned, and 2) different modalities may provide diverse information for prediction. Our M2P2 (Multimodal Persuasion Prediction) framework is the first to use multimodal (acoustic, visual, language) data to solve the IPP problem. To leverage the alignment of different modalities while maintaining the diversity of the cues they provide, M2P2 devises a novel adaptive fusion learning framework which fuses embeddings obtained from two modules - an alignment module that extracts shared information between modalities and a heterogeneity module that learns the weights of different modalities with guidance from three separately trained unimodal reference models. We test M2P2 on the popular IQ2US dataset designed for DOP. We also introduce a new dataset called QPS (from Qipashuo, a popular Chinese debate TV show) for IPP. M2P2 significantly outperforms 4 recent baselines on both datasets.
C1 [Bai, Chongyang] Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.
   [Chen, Haipeng] Harvard Univ, Dept Comp Sci, Boston, MA 02138 USA.
   [Kumar, Srijan] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
   [Leskovec, Jure] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Subrahmanian, V. S.] Northwestern Univ, Dept Comp Sci, Evanston, IL 60208 USA.
   [Subrahmanian, V. S.] Northwestern Univ, Roberta Buffett Inst Global Affairs, Evanston, IL 60208 USA.
C3 Dartmouth College; Harvard University; University System of Georgia;
   Georgia Institute of Technology; Stanford University; Northwestern
   University; Northwestern University
RP Subrahmanian, VS (corresponding author), Northwestern Univ, Dept Comp Sci, Evanston, IL 60208 USA.; Subrahmanian, VS (corresponding author), Northwestern Univ, Roberta Buffett Inst Global Affairs, Evanston, IL 60208 USA.
EM chongyang.bai.gr@dartmouth.edu; hpchen@seas.harvard.edu;
   srijan@gatech.edu; jure@cs.stanford.edu; vss@northwestern.edu
RI Subrahmanian, Venkatramanan/ABA-7399-2021; Bai, Chongyang/HRD-8818-2023
OI Leskovec, Jure/0000-0002-5411-923X; Chen, Haipeng/0000-0003-0572-8888;
   Subrahmanian, Venkatramanan/0000-0001-7191-0296
FU OAC [1835598, 1934578]; CCF [1918940]; IIS [2030477]; DARPA; ARO
   [W911NF.-16.-1.-0342]; Stanford Data Science Initiative
FX This work was supported in part by under Nos. OAC.-1835598 (CINES),
   OAC.-1934578 (HDR), CCF.-1918940 (Expeditions), IIS.-2030477 (RAPID),
   and IIS.-2027689 (RAPID), in part by DARPA under No. (MCS), in part by
   ARO under Nos. W911NF.-16.-1.-0342 (MURI), and W911NF.-16.-1.-0171
   (DURIP), and in part by Stanford Data Science Initiative, Wu Tsai
   Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, JPMorgan Chase,
   Docomo, Hi728 tachi, JD.com, KDDI, NVIDIA, Dell, Toshiba, United -Health
   Group, Adobe, Facebook, Microsoft, and IDEaS Insitute. J. L. is a Chan
   Zuckerberg Biohub investigator.
CR Aguilar G, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P991
   Andrew G., 2013, ICML, P1247
   [Anonymous], 2019, P 2019 C N AM CHAPTE
   Bai CY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4504
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Brilman M, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P149, DOI 10.1145/2733373.2806245
   Cer D, 2018, CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P169
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Drugman T, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P1984
   Dumpala SH, 2019, 32 C NEURAL INFORM P, P1
   Elmadany NE, 2019, IEEE T MULTIMEDIA, V21, P1317, DOI 10.1109/TMM.2018.2875510
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Ghosh S, 2016, INTERSPEECH, P3603, DOI 10.21437/Interspeech.2016-692
   Habernal I, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1589
   Hinton G. E., 2012, arXiv
   Huang X., 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, P73
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jo DU, 2020, AAAI CONF ARTIF INTE, V34, P11197
   Joo J, 2014, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2014.35
   Kim J., 2017, PROC 5 INT C LEARN R
   Kingma D. P., 2014, arXiv
   Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050
   Li JJ, 2019, IEEE T IMAGE PROCESS, V28, P6103, DOI 10.1109/TIP.2019.2924174
   Li JJ, 2019, IEEE T NEUR NET LEAR, V30, P1381, DOI 10.1109/TNNLS.2018.2868854
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Long X, 2018, AAAI CONF ARTIF INTE, P7202
   Mai SJ, 2020, IEEE T MULTIMEDIA, V22, P122, DOI 10.1109/TMM.2019.2925966
   Marín-Jiménez MJ, 2019, PROC CVPR IEEE, P3472, DOI 10.1109/CVPR.2019.00359
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   Panagakis Y, 2016, IEEE T PATTERN ANAL, V38, P1665, DOI 10.1109/TPAMI.2015.2497700
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Potash Peter., 2017, P 2017 C EMPIRICAL M
   Pu J, 2017, INT CONF ACOUST SPEE, P2901, DOI 10.1109/ICASSP.2017.7952687
   Santos P.B., 2016, Proceedings of the Workshop on Computational Modeling of People's Opinions, Personality, and Emotions in Social Media (PEOPLES), P163
   Santos P.H.O., 2018, PROC IEEE C EVOL COM, P1, DOI [DOI 10.1109/CEC.2018.8477938, DOI 10.1109/ULTSYM.2018.8579972]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song GL, 2021, IEEE T MULTIMEDIA, V23, P1882, DOI 10.1109/TMM.2020.3004963
   Song Y., 2018, P C N AM CHAPT ASS C, P175, DOI [DOI 10.18653/V1/N18-2028, 10.18653/v1/n18-2028]
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEUR IN, V30
   Verma S, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3627
   Wang Lu, 2017, Transactions of the Association for Computational Linguistics, V5, P219
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zhang J, 2016, ADV SOC SCI EDUC HUM, V74, P136
   Zhang XD, 2021, IEEE T MULTIMEDIA, V23, P611, DOI 10.1109/TMM.2020.2985526
NR 48
TC 0
Z9 0
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 942
EP 952
DI 10.1109/TMM.2021.3134168
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, HA
   Wang, JF
   Duan, MH
   Jin, Y
   Kan, Y
   Zhu, CA
AF Chen, Huaian
   Wang, Jianfeng
   Duan, Minghui
   Jin, Yi
   Kan, Yan
   Zhu, Changan
TI Video Denoising for Scenes With Challenging Motion: A Comprehensive
   Analysis and a New Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Comprehensive analysis; explicit motion estimation; motion level;
   progressive denoising; video denoising
ID IMAGE; ENHANCEMENT
AB Challenging motion, which tends to cause artifacts, is a key problem in the video denoising task. Recent video denoising methods have attempted to address this problem. However, they usually provide general performance evaluation on the overall dataset and cannot provide a comprehensive analysis for the influence of different motion levels. Thus, we questioned whether these methods can effectively deal with different scene motions. To this end, we synthesize a dataset containing videos with different motion levels and capture a new dataset that consists of videos involving large-scale motion. Then, we provide a comprehensive analysis on the elaborately collected datasets and find that, as the motion level increases, the performance of the denoising models based on implicit motion estimation (IME) declines sharply, while explicit motion estimation (EME) contributes to a more robust denoising quality. Therefore, in this work, we present an EME-embedded progressive denoising framework that fully considers the relationship between the noise removal and motion estimation. Specifically, we decouple video denoising into spatial denoising, EME-based frame reconstruction, and temporal refining processes. Spatial denoising improves the accuracy of EME process in the case of videos suffering from heavy noise, while the temporal refining process refines the denoised frame by utilizing temporal redundancy of the reconstructed motion-free frames. Extensive experiments demonstrate that the proposed method outperforms existing state-of-the-art methods, especially for videos containing large-scale motion.
C1 [Chen, Huaian; Duan, Minghui] Univ Sci & Technol China, Sch Engn Sci, Hefei 230022, Anhui, Peoples R China.
   [Wang, Jianfeng] Univ Sci & Technol China, Sch Data Sci, Hefei 230022, Anhui, Peoples R China.
   [Jin, Yi; Zhu, Changan] Univ Sci & Technol China, Sch Engn Sci, Sch Data Sci, Hefei 230022, Anhui, Peoples R China.
   [Kan, Yan] Univ Sci & Technol China, Innovat Lab WuHu State Owned Factory Machining, Hefei 230022, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS
RP Jin, Y (corresponding author), Univ Sci & Technol China, Sch Engn Sci, Sch Data Sci, Hefei 230022, Anhui, Peoples R China.
EM anchen@mail.ustc.edu.cn; wjfff@mail.ustc.edu.cn;
   dmhustc@mail.ustc.edu.cn; jinyi08@ustc.edu.cn; kanyan5720@gmail.com;
   changan@ustc.edu.cn
RI Zhu, ChangAn/KIL-0881-2024
OI Duan, Minghui/0000-0001-6878-5834; Wang, Jianfeng/0000-0003-0932-3060;
   Chen, Huaian/0000-0003-3999-2206
FU National Natural Science Foundation of China [61727809]; Special Fund
   for Key Program of Science and Technology of Anhui Province
   [201903c08020002]; National Key Research and Development Program of
   China [2019YFC0117800]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61727809, in part by the Special Fund
   for Key Program of Science and Technology of Anhui Province under Grant
   201903c08020002, and in part by the National Key Research and
   Development Program of China under Grant 2019YFC0117800. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Prof. Li Cheng. (Corresponding author: Yi Jin.)
CR Arias P, 2018, J MATH IMAGING VIS, V60, P70, DOI 10.1007/s10851-017-0742-4
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Buades A, 2016, IEEE T IMAGE PROCESS, V25, P2573, DOI 10.1109/TIP.2016.2551639
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Chang SN, 2022, IEEE T MULTIMEDIA, V24, P4067, DOI 10.1109/TMM.2021.3112814
   Chen C, 2019, IEEE I CONF COMP VIS, P3184, DOI 10.1109/ICCV.2019.00328
   Chen HA, 2022, IEEE T MULTIMEDIA, V24, P2164, DOI 10.1109/TMM.2021.3077140
   Chen HA, 2021, IEEE T IND INFORM, V17, P5369, DOI 10.1109/TII.2020.3024187
   Chen HA, 2022, IEEE T NEUR NET LEAR, V33, P4991, DOI 10.1109/TNNLS.2021.3066850
   Claus M, 2019, IEEE COMPUT SOC CONF, P1843, DOI 10.1109/CVPRW.2019.00235
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Davy A, 2021, J MATH IMAGING VIS, V63, P73, DOI 10.1007/s10851-020-00995-0
   Ding CX, 2018, IEEE T PATTERN ANAL, V40, P1002, DOI 10.1109/TPAMI.2017.2700390
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kalantari NK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073609
   Kingma D.P., 2014, ARXIV14126980
   Kroeger T, 2016, LECT NOTES COMPUT SC, V9908, P471, DOI 10.1007/978-3-319-46493-0_29
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Liu L, 2020, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR42600.2020.00652
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Maggioni M, 2021, PROC CVPR IEEE, P3465, DOI 10.1109/CVPR46437.2021.00347
   Maggioni M, 2013, IEEE T IMAGE PROCESS, V22, P119, DOI 10.1109/TIP.2012.2210725
   Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324
   Marinc T, 2019, IEEE IMAGE PROC, P2404, DOI [10.1109/ICIP.2019.8803335, 10.1109/icip.2019.8803335]
   Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi ZH, 2022, IEEE T MULTIMEDIA, V24, P426, DOI 10.1109/TMM.2021.3052419
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun L, 2021, INT J COMPUT VISION, V129, P2827, DOI 10.1007/s11263-021-01510-7
   Tan X, 2023, IEEE T MULTIMEDIA, V25, P750, DOI 10.1109/TMM.2021.3132165
   Tassano M, 2020, PROC CVPR IEEE, P1351, DOI 10.1109/CVPR42600.2020.00143
   Tassano M, 2019, IEEE IMAGE PROC, P1805, DOI [10.1109/ICIP.2019.8803136, 10.1109/icip.2019.8803136]
   Teed Zachary, 2020, EUR C COMP VIS, P402, DOI [DOI 10.1007/978-3-030-58536-524, DOI 10.1007/978-3-030-58536-5_24]
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Vaksman G, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2137, DOI 10.1109/ICCV48922.2021.00216
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wu SZ, 2018, LECT NOTES COMPUT SC, V11206, P120, DOI 10.1007/978-3-030-01216-8_8
   Xu XY, 2020, IEEE T IMAGE PROCESS, V29, P7153, DOI 10.1109/TIP.2020.2999209
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yue HJ, 2020, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR42600.2020.00237
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zheng YQ, 2020, PROC CVPR IEEE, P6748, DOI 10.1109/CVPR42600.2020.00678
NR 50
TC 3
Z9 3
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5704
EP 5719
DI 10.1109/TMM.2022.3198317
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500005
DA 2024-07-18
ER

PT J
AU Dong, X
   Zhang, GW
   Zhan, XL
   Ding, Y
   Wei, YC
   Lu, ML
   Liang, XD
AF Dong, Xiao
   Zhang, Gengwei
   Zhan, Xunlin
   Ding, Yi
   Wei, Yunchao
   Lu, Minlong
   Liang, Xiaodan
TI Caption-Aided Product Detection via Collaborative Pseudo-Label
   Harmonization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Product detection; pseudo-label; positive mining
AB Product detection, which aims to localize products of interest in the advertising images, helps advance many potential E-commerce applications like product retrieval and recommendation. However, labeling a massive number of fine-grained product categories and accurate product boxes is costly and especially not practical since products are ever-changing on E-commerce websites. In this work, we step forward to train a fine-grained product detector solely supervised by the advertising captions, which are naturally available but often severely flawed and noisy. To reformulate the weakly supervised detection research into a real-world setting, we introduce a large-scale benchmark, named CapProduct, where more than 80,000 product image-caption pairs are collected from E-commerce websites. The fine-grained nature of products and noisy captions in CapProduct make it intractable to excavate valid category labels to train a weakly supervised object detector. To tackle this challenge, we propose a Collaborative Pseudo-Label Harmonization (CoPLH) framework that harmonizes self-mined pseudo labels via modeling the global co-occurrence relationships of products. We construct a collaborative co-occurrence graph based on all training samples to improve the reliability of caption-predicted pseudo-labels as well as benefit the self-training procedure in a weakly supervised setting. Extensive experiments on the CapProduct dataset demonstrate the effectiveness and the superiority of the proposed CoPLH over the state-of-the-art baselines.
C1 [Dong, Xiao] Sun Yat Sen Univ, Zhuhai 528478, Peoples R China.
   [Dong, Xiao; Liang, Xiaodan] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Zhang, Gengwei] Univ Technol Sydney, Sydney 2007, Australia.
   [Zhan, Xunlin; Ding, Yi; Liang, Xiaodan] Sun Yat sen Univ, Shenzhen Campus, Shenzhen 518107, Peoples R China.
   [Wei, Yunchao] Beijing Jiaotong Univ, Beijing 100091, Peoples R China.
   [Lu, Minlong] Alibaba Grp, Hangzhou 311121, Peoples R China.
C3 Sun Yat Sen University; Peng Cheng Laboratory; University of Technology
   Sydney; Sun Yat Sen University; Beijing Jiaotong University; Alibaba
   Group
RP Liang, XD (corresponding author), Peng Cheng Lab, Shenzhen 518055, Peoples R China.; Liang, XD (corresponding author), Sun Yat sen Univ, Shenzhen Campus, Shenzhen 518107, Peoples R China.
EM dx.icandoit@gmail.com; zgwdavid@gmail.com; zhanxlin@mail2.sysu.edu.cn;
   dingy36@mail2.sysu.edu.cn; wychao1987@gmail.com; ymlml@zju.edu.cn;
   xdliang328@gmail.com
RI Lu, Minlong/KRO-9416-2024
OI Dong, Xiao/0000-0001-9519-612X; DING, Yi/0000-0002-6774-4536
FU National Natural Science Foundation of China [61976233]; Guangdong
   Province Basic and Applied Basic Research (Regional Joint Fund-Key)
   [2019B1515120039]; Guangdong Outstanding Youth Fund [2021B1515020061];
   Shenzhen Fundamental Research Program [RCYX20200714114642083,
   JCYJ20190807154211365]; CAAI-Huawei Mind Spore Open Fund; MindSpore;
   Guangdong Provincial Key Laboratory of Fire Science and Intelligent
   Emergency Technology, Guangzhou, China
FX This work was supported in partby the National Natural Science
   Foundation of China under Grant 61976233,in part by Guangdong Province
   Basic and Applied Basic Research (Regional Joint Fund-Key) under Grant
   2019B1515120039, in part by Guangdong Outstanding Youth Fund under Grant
   2021B1515020061, in part by Shenzhen Fundamental Research Program under
   Grants RCYX20200714114642083 and JCYJ20190807154211365, in part by
   CAAI-Huawei Mind Spore Open Fund, in part by MindSpore, which is a new
   deep learning computing framework, and in part by the Guangdong
   Provincial Key Laboratory of Fire Science and Intelligent Emergency
   Technology, Guangzhou, China.
CR Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Cevahir Ali, 2016, P 26 INT C COMP LING, P525
   Chen HS, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P2349, DOI 10.1145/3357384.3358170
   Cinbis RG, 2014, PROC CVPR IEEE, P2409, DOI 10.1109/CVPR.2014.309
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146
   Fang HS, 2019, IEEE I CONF COMP VIS, P682, DOI 10.1109/ICCV.2019.00077
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong C, 2022, IEEE T PATTERN ANAL, V44, P2841, DOI 10.1109/TPAMI.2020.3044997
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He RN, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P507, DOI 10.1145/2872427.2883037
   Jerbi A, 2020, Arxiv, DOI arXiv:2009.14558
   Jia C, 2021, PR MACH LEARN RES, V139
   Jie ZQ, 2017, PROC CVPR IEEE, P4294, DOI 10.1109/CVPR.2017.457
   Kantorov V, 2016, LECT NOTES COMPUT SC, V9909, P350, DOI 10.1007/978-3-319-46454-1_22
   Ke Yang, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P8371, DOI 10.1109/ICCV.2019.00846
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu JS, 2019, ADV NEUR IN, V32
   Maron O, 1998, ADV NEUR IN, V10, P570
   Mei T, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.10
   Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668
   Peng J., 2020, ARXIV200612634
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren S., 2015, ADV NEURAL INFORM PR, V28, P91
   Robinson J., 2021, PROC INT C LEARN REP, P1
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shen Y., 2020, Advances in Neural Information Processing Systems, V33
   Shi L, 2020, Arxiv, DOI arXiv:2007.13135
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Su Weijie, 2020, INT C LEARN REPR
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304
   Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326
   Tang P, 2017, PATTERN RECOGN, V71, P446, DOI 10.1016/j.patcog.2017.05.001
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wan F, 2018, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2018.00141
   Wang C, 2014, LECT NOTES COMPUT SC, V8694, P431, DOI 10.1007/978-3-319-10599-4_28
   Wei XS, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-022-3513-y
   Wei Y., 2018, P EUR C COMP VIS, P434
   Wu Y., 2019, DETECTRON2
   Xu H, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3413, DOI 10.1145/3308558.3313644
   Ye KR, 2019, IEEE I CONF COMP VIS, P9685, DOI 10.1109/ICCV.2019.00978
   Zeng ZY, 2019, IEEE I CONF COMP VIS, P8291, DOI 10.1109/ICCV.2019.00838
   Zhongzheng Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10595, DOI 10.1109/CVPR42600.2020.01061
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 47
TC 0
Z9 0
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1916
EP 1927
DI 10.1109/TMM.2022.3222653
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100026
DA 2024-07-18
ER

PT J
AU Fang, YC
   Cai, SR
   Cao, YT
   Li, ZC
   Zhang, ZX
AF Fang, Yuchun
   Cai, Sirui
   Cao, Yiting
   Li, Zhengchen
   Zhang, Zhaoxiang
TI Adversarial Learning Guided Task Relatedness Refinement for Multi-Task
   Deep Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Index Terms-Multi-task learning; deep learning; task relatedness
AB In machine learning, the relatedness across multiple tasks is usually complex and entangled. Due to dataset bias, the relatedness among tasks might be distorted and mislead the training of the models with solid learning ability, such as the multi-task neural networks. In this paper, we propose the idea of Relatedness Refinement Multi-Task Learning (RRMTDL) by introducing adversarial learning in the multi-task deep neural network to tackle the problem. The RRMTDL deep learning model restrains the misleading relatedness task by adversarial training and extracts information sharing across tasks with valuable relatedness. With RRMTDL, multi-task deep learning can enhance the task-specific representation for the major tasks by excluding the misleading relatedness. We design tests with various combinations of task-relatedness to validate the proposed model. Experimental results show that the RRMTDL model can effectively refine the task relatedness and prominently outperform other multi-task deep learning models in datasets with entangled task labels.
C1 [Fang, Yuchun; Cai, Sirui; Cao, Yiting; Li, Zhengchen] Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
   [Zhang, Zhaoxiang] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Zhang, Zhaoxiang] HKISI CAS, Ctr Artificial Intelligence & Robot, Beijing 100190, Peoples R China.
C3 Shanghai University; Chinese Academy of Sciences; Institute of
   Automation, CAS
RP Fang, YC (corresponding author), Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
EM ycfang@shu.edu.cn; caisirui@126.com; caoyiting12@foxmail.com;
   lizhengchen@shu.edu.cn; zhaoxiang.zhang@ia.ac.cn
FU National Natural Science Foundation of China [61976132, 61991411,
   U1811461]; Natural Science Foundation of Shanghai [19ZR1419200]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61976132, 61991411, and U1811461, and
   in part by the Natural Science Foundation of Shanghai under Grant
   19ZR1419200.
CR Adi Y, 2019, INT CONF ACOUST SPEE, P3742, DOI 10.1109/ICASSP.2019.8682468
   [Anonymous], 2012, AI Statistics
   [Anonymous], 2018, P 2018 C N AM CHAPT
   Cai S., 2019, PROC ACM MULTIMEDIA, P1
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Du L, 2014, AAAI CONF ARTIF INTE, P2746
   Fang YC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1668
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao Y, 2019, PROC CVPR IEEE, P3200, DOI 10.1109/CVPR.2019.00332
   Gong Pinghua, 2012, KDD, V2012, P895
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo P., 2020, ICML, P3854
   Han H, 2018, IEEE T PATTERN ANAL, V40, P2597, DOI 10.1109/TPAMI.2017.2738004
   Hand EM, 2017, AAAI CONF ARTIF INTE, P4068
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1636, DOI 10.1145/3123266.3123424
   Huang ZZ, 2021, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR46437.2021.00720
   Jayaraman D, 2014, PROC CVPR IEEE, P1629, DOI 10.1109/CVPR.2014.211
   Jordan M., 2006, Technical report
   Li JS, 2019, IEEE WINT CONF APPL, P932, DOI 10.1109/WACV.2019.00104
   Lu YX, 2017, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2017.126
   Maninis KK, 2019, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2019.00195
   Meng Z, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5969, DOI 10.1109/ICASSP.2018.8461932
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shen J., 2021, ADV NEURAL INFORM PR, V34, P21031
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Shinohara Y, 2016, INTERSPEECH, P2369, DOI 10.21437/Interspeech.2016-879
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   Tu GY, 2020, IEEE T MULTIMEDIA, V22, P148, DOI 10.1109/TMM.2019.2922129
   Wang JG, 2018, AAAI CONF ARTIF INTE, P451
   Xia XJ, 2020, IEEE T MULTIMEDIA, V22, P569, DOI 10.1109/TMM.2019.2933330
   Xiao L., 2018, P 27 INT C COMPUTATI, P2055
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang S, 2019, IEEE T MULTIMEDIA, V21, P3194, DOI 10.1109/TMM.2019.2919469
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhang ZJ, 2021, IEEE T MULTIMEDIA, V23, P3306, DOI 10.1109/TMM.2020.3023339
NR 38
TC 1
Z9 1
U1 13
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6946
EP 6957
DI 10.1109/TMM.2022.3216460
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000017
DA 2024-07-18
ER

PT J
AU Kaashki, NN
   Hu, PP
   Munteanu, A
AF Kaashki, Nastaran Nourbakhsh
   Hu, Pengpeng
   Munteanu, Adrian
TI Anet: A Deep Neural Network for Automatic 3D Anthropometric Measurement
   Extraction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Point cloud compression; Training; Deep
   learning; Loss measurement; Fitting; Data mining; Anthropometric
   measurement extraction; 3D scanning; deep neural networks; template
   fitting
ID HUMAN-BODY SHAPE
AB 3D Anthropometric measurement extraction is of paramount importance for several applications such as clothing design, online garment shopping, and medical diagnosis, to name a few. State-of-the-art 3D anthropometric measurement extraction methods estimate the measurements either through some landmarks found on the input scan or by fitting a template to the input scan using optimization-based techniques. Finding landmarks is very sensitive to noise and missing data. Template-based methods address this problem, but the employed optimization-based template fitting algorithms are computationally very complex and time-consuming. To address the limitations of existing methods, we propose a deep neural network architecture which fits a template to the input scan and outputs the reconstructed body as well as the corresponding measurements. Unlike existing template-based anthropocentric measurement extraction methods, the proposed approach does not need to transfer and refine the measurements from the template to the deformed template, thereby being faster and more accurate. A novel loss function, especially developed for 3D anthropometric measurement extraction is introduced. Additionally, two large datasets of complete and partial front-facing scans are proposed and used in training. This results in two models, dubbed Anet-complete and Anet-partial, which extract the body measurements from complete and partial front-facing scans, respectively. Experimental results on synthesized data as well as on real 3D scans captured by a photogrammetry-based scanner, an Azure Kinect sensor, and the very recent TrueDepth camera system demonstrate that the proposed approach systematically outperforms the state-of-the-art methods in terms of accuracy and robustness.
C1 [Kaashki, Nastaran Nourbakhsh; Hu, Pengpeng; Munteanu, Adrian] Vrije Univ Brussel, Dept Elect & Informat, B-1050 Brussels, Belgium.
C3 Vrije Universiteit Brussel
RP Hu, PP (corresponding author), Vrije Univ Brussel, Dept Elect & Informat, B-1050 Brussels, Belgium.
EM nknourba@etrovub.be; phu@etrovub.be; acmuntea@etrovub.be
RI Hu, Pengpeng/ADG-7735-2022; Munteanu, Adrian/HKO-9955-2023
OI Munteanu, Adrian/0000-0001-7290-0428; Nourbakhsh Kaashki,
   Nastaran/0000-0001-8317-4994; Hu, Pengpeng/0000-0002-2547-1517
FU FWO [G084117]
FX This work was supported in part by the Innoviris under Project eTailor
   and in part by FWO under Project G084117.
CR Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   [Anonymous], 85593 ISO
   [Anonymous], 2017, Agisoft PhotoScan
   Apeagyei Phoebe R., 2010, International Journal of Digital Content Technology and Its Applications, V4, P58, DOI DOI 10.4156/JDCTA.VOL4.ISSUE7.6
   Bartol K, 2021, IEEE ACCESS, V9, P67281, DOI 10.1109/ACCESS.2021.3076595
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Cheng ZQ, 2018, COMPUT GRAPH-UK, V71, P88, DOI 10.1016/j.cag.2017.11.008
   CMU, Carnegie-mellon mocap database
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng L., 2020, J PHYS CONF SER, V1576
   Gao L, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275028
   Groueix T, 2018, LECT NOTES COMPUT SC, V11206, P235, DOI 10.1007/978-3-030-01216-8_15
   Helten T, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P279, DOI 10.1109/3DV.2013.44
   Hirshberg DA, 2012, LECT NOTES COMPUT SC, V7577, P242, DOI 10.1007/978-3-642-33783-3_18
   Hu PP, 2021, IEEE T IND INFORM, V17, P3793, DOI 10.1109/TII.2020.3016591
   Jertec A, 2019, INT SYMP IMAGE SIG, P253, DOI [10.1109/ISPA.2019.8868844, 10.1109/ispa.2019.8868844]
   Kozbial M, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10041342
   Löffler-Wirth H, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0159887
   Nguyen LT, 2016, BRAIN-BROAD RES ARTI, V7, P5
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Loper M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661273
   Lunscher N, 2018, IEEE COMPUT SOC CONF, P1208, DOI 10.1109/CVPRW.2018.00157
   Markiewicz L, 2017, EXPERT SYST APPL, V85, P366, DOI 10.1016/j.eswa.2017.04.052
   Murtagh F., 1991, NEUROCOMPUTING, V2, P183
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Pheasant S., 2005, Bodyspace: Anthropometry, Ergonomics and the Design of Work
   Probst T, 2017, INT CONF 3D VISION, P486, DOI 10.1109/3DV.2017.00062
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Rong Y, 2021, IEEE INT CONF COMP V, P1749, DOI 10.1109/ICCVW54120.2021.00201
   Sekhavat YA, 2017, IEEE T MULTIMEDIA, V19, P1041, DOI 10.1109/TMM.2016.2639380
   Simonik M., 2020, HEGES VERION 1 4 2
   Tan XH, 2018, J VISUAL LANG COMPUT, V47, P9, DOI 10.1016/j.jvlc.2018.05.002
   treedys, TREEDYS 3 D SCANNER
   Tsoli A, 2014, IEEE WINT CONF APPL, P83, DOI 10.1109/WACV.2014.6836115
   Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492
   Vitali A, 2018, VIRTUAL PHYS PROTOTY, V13, P131, DOI 10.1080/17452759.2018.1474082
   Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087
   Wasenmuller O., 2015, P 6 INT C 3D BOD SCA, P27
   Xu Yang Gan, 2020, Journal of Physics: Conference Series, V1529, DOI 10.1088/1742-6596/1529/2/022067
   Xu ZY, 2021, IEEE T MULTIMEDIA, V23, P1542, DOI 10.1109/TMM.2020.3001540
   Yan S, 2020, MACH VISION APPL, V31, DOI 10.1007/s00138-019-01054-4
   Yang C, 2017, IEEE T MULTIMEDIA, V19, P1625, DOI 10.1109/TMM.2017.2672198
   Yang J, 2018, GRAPH MODELS, V98, P1, DOI 10.1016/j.gmod.2018.05.003
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zhao JY, 2019, ICRSA 2019: 2019 2ND INTERNATIONAL CONFERENCE ON ROBOT SYSTEMS AND APPLICATIONS, P9, DOI 10.1145/3378891.3378897
   Zhao TH, 2019, IEEE T MULTIMEDIA, V21, P114, DOI 10.1109/TMM.2018.2844087
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 49
TC 4
Z9 4
U1 3
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 831
EP 844
DI 10.1109/TMM.2021.3132487
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900012
OA Green Published
DA 2024-07-18
ER

PT J
AU Lan, X
   Hu, QH
   Cheng, J
AF Lan, Xing
   Hu, Qinghao
   Cheng, Jian
TI ATF: An Alternating Training Framework for Weakly Supervised Face
   Alignment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face alignment; multi-task learning; weakly supervised
ID NETWORK
AB In recent years, various face-landmark datasets have been published. Intuitively, it is significant to integrate multiple labeled datasets to achieve higher performance. Due to the different annotation schemes of datasets, it is hard to directly train models using them together. Although numerous efforts have been made in the joint use of datasets, there remain three shortages in previous methods, i.e., additional computation, limitation of the markups scheme, and limited support for the regression method. To solve the above issues, we proposed a novel Alternating Training Framework (ATF), which leverages the similarity and diversity across multiple datasets for a more robust detector. ATF mainly contains two sub-modules: Alternating Training with Decreasing Proportions (ATDP) and Mixed Branch Loss (L-MB). In particular, ATDP trains multiple datasets simultaneously via a weakly supervised way to take advantage of the diversity among them, and L-MB utilizes similar landmark pairs to constrain different branches of the corresponding datasets. Besides, we extend the framework to easily handle three situations: single target detector, joint detector, and novel detector. Extensive experiments demonstrate the effectiveness of our framework for both heatmap-based and direct coordinate regression. Moreover, we have achieved a joint detector that outperforms state-of-the-art methods on each benchmark.
C1 [Lan, Xing; Cheng, Jian] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Lan, Xing; Hu, Qinghao; Cheng, Jian] Chinese Acad Sci CASIA, Natl Lab Pattern Recognit NLPR, Inst Automat, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS
RP Cheng, J (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
EM lanxing2019@ia.ac.cn; huqinghao2014@ia.ac.cn; jcheng@nlpr.ia.ac.cn
RI LIU, HUI/JPX-8014-2023; WANG, HUI/JFA-9683-2023; liu,
   huan/JKI-3764-2023; luo, yuan/JLS-6416-2023; Yu, Yue/JWP-9103-2024;
   Wang, Minghao/JMD-0670-2023; wang, wenxin/JOZ-3291-2023; wang,
   yixuan/JGM-3893-2023; wang, hao/JKH-5890-2023; ,
   chengjian/KGL-5551-2024; Li, jiaqi/JOZ-6395-2023; Chen,
   Xin/JDN-2017-2023; WANG, Bin/JGM-2639-2023; wang, hang/JND-8481-2023;
   Liu, Yilin/JWP-9153-2024; yang, yue/KCK-7870-2024; zhang,
   zheng/KBQ-7815-2024; Wang, Hao/ABB-8923-2020; zhou, yang/JED-3951-2023;
   zhang, yueqi/JXM-4287-2024; Lan, Xing/GLR-1481-2022; liu,
   huan/JEO-4705-2023
OI , chengjian/0000-0003-1289-2758; Liu, Yilin/0000-0002-7581-3933; Wang,
   Hao/0000-0001-9109-6017; Lan, Xing/0000-0003-3624-5926; 
FU National Key Research and Development Program of China [2021ZD0201504];
   National Natural Science Foundation of China [62106267]; Jiangsu Key
   Research and Development Plan [BE2021012-2]; Jiangsu Leading Technology
   Basic Research Project [BK20192004]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2021ZD0201504, in part by the
   National Natural Science Foundation of China under Grant 62106267, in
   part by Jiangsu Key Research and Development Plan under Grant
   BE2021012-2,and in part by Jiangsu Leading Technology Basic Research
   Project under Grant BK20192004.
CR Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Caruana R., 1993, ICML, P41, DOI 10.1016/b978-1-55860-307-3.50012-5
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cristinacce D, 2008, PATTERN RECOGN, V41, P3054, DOI 10.1016/j.patcog.2008.01.024
   Dapogny A, 2019, IEEE I CONF COMP VIS, P6892, DOI 10.1109/ICCV.2019.00699
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
   Dong XY, 2019, IEEE I CONF COMP VIS, P783, DOI 10.1109/ICCV.2019.00087
   Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047
   Duong L, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P845
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238
   Feng ZH, 2017, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2017.392
   Guo XJ, 2019, Arxiv, DOI [arXiv:1902.10859, DOI 10.48550/ARXIV.1902.10859]
   Honari S, 2018, PROC CVPR IEEE, P1546, DOI 10.1109/CVPR.2018.00167
   Honari S, 2016, PROC CVPR IEEE, P5743, DOI 10.1109/CVPR.2016.619
   Khan MH, 2017, IEEE I CONF COMP VIS, P3811, DOI 10.1109/ICCV.2017.409
   Koestinger M., 2011, IEEE INT C COMP VIS, P2144, DOI DOI 10.1109/ICCVW.2011.6130513
   Kowalski M, 2017, IEEE COMPUT SOC CONF, P2034, DOI 10.1109/CVPRW.2017.254
   Kumar Abhinav, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8233, DOI 10.1109/CVPR42600.2020.00826
   Kumar A, 2018, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2018.00052
   Lan X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2140, DOI 10.1145/3394171.3414037
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liebel L, 2018, Arxiv, DOI [arXiv:1805.06334, DOI 10.48550/ARXIV.1805.06334]
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu YF, 2019, Arxiv, DOI arXiv:1909.02214
   Messer K., 1999, 2 INT C AUD VID BAS, V964, P965
   Qian SJ, 2019, IEEE I CONF COMP VIS, P10152, DOI 10.1109/ICCV.2019.01025
   Robinson JP, 2019, IEEE I CONF COMP VIS, P10102, DOI 10.1109/ICCV.2019.01020
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Smith BM, 2014, LECT NOTES COMPUT SC, V8694, P78, DOI 10.1007/978-3-319-10599-4_6
   Sun K, 2019, Arxiv, DOI arXiv:1904.04514
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Valle R, 2018, LECT NOTES COMPUT SC, V11218, P609, DOI 10.1007/978-3-030-01264-9_36
   Wu WY, 2017, IEEE COMPUT SOC CONF, P2096, DOI 10.1109/CVPRW.2017.261
   Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227
   Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yang J, 2017, IEEE COMPUT SOC CONF, P2025, DOI 10.1109/CVPRW.2017.253
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang J, 2015, IEEE I CONF COMP VIS, P3801, DOI 10.1109/ICCV.2015.433
   Zhang Y, 2022, IEEE T KNOWL DATA EN, V34, P5586, DOI 10.1109/TKDE.2021.3070203
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhu ML, 2019, PROC CVPR IEEE, P3481, DOI 10.1109/CVPR.2019.00360
   Zhu SZ, 2014, Arxiv, DOI arXiv:1409.0602
   Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zou X, 2019, IEEE I CONF COMP VIS, P141, DOI 10.1109/ICCV.2019.00023
NR 55
TC 2
Z9 2
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1798
EP 1809
DI 10.1109/TMM.2022.3164798
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100016
DA 2024-07-18
ER

PT J
AU Lentisco, CM
   Bellido, L
   Cárdenas, A
   Moyano, RF
   Fernández, D
AF Lentisco, Carlos M.
   Bellido, Luis
   Cardenas, Andres
   Flores Moyano, Ricardo
   Fernandez, David
TI Design of a 5G Multimedia Broadcast Application Function Supporting
   Adaptive Error Recovery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Digital multimedia broadcasting; 5G mobile communication; Streaming
   media; Computer architecture; Proposals; Long Term Evolution; Network
   architecture; Multimedia communication; Network function virtualization;
   Software-defined networking
ID VIDEO; NETWORKING
AB The demand for mobile multimedia streaming services has been steadily growing in recent years. Mobile multimedia broadcasting addresses the shortage of radio resources but introduces a network error recovery problem. Retransmitting multimedia segments that are not correctly broadcast can cause service disruptions and increased service latency, affecting the quality of experience perceived by end users. With the advent of networking paradigms based on virtualization technologies, mobile networks have been enabled with more flexibility and agility to deploy innovative services that improve the utilization of available network resources. This paper discusses how mobile multimedia broadcast services can be designed to prevent service degradation by using the computing capabilities provided by multiaccess edge computing (MEC) platforms in the context of a 5G network architecture. An experimental platform has been developed to evaluate the feasibility of a MEC application to provide adaptive error recovery for multimedia broadcast services. The results of the experiments carried out show that the proposal provides a flexible mechanism that can be deployed at the network edge to lower the impact of transmission errors on latency and service disruptions.
C1 [Lentisco, Carlos M.; Bellido, Luis; Cardenas, Andres; Fernandez, David] Univ Politecn Madrid, Dept Ingn Sistemas Telemat, ETSI Telecomun, Madrid 28040, Spain.
   [Flores Moyano, Ricardo] Univ San Francisco Quito, Dept Comp Sci Engn, Quito 170901, Ecuador.
C3 Universidad Politecnica de Madrid; Universidad San Francisco de Quito
RP Lentisco, CM (corresponding author), Univ Politecn Madrid, Dept Ingn Sistemas Telemat, ETSI Telecomun, Madrid 28040, Spain.
EM c.lentisco@upm.es; luis.bellido@upm.es; andres.cardenasc@alumnos.upm.es;
   rflores@usfq.edu.ec; david.fernandez@upm.es
RI Bellido Triana, Luis/GZH-2187-2022; FERNANDEZ CAMBRONERO,
   DAVID/L-3848-2014
OI BELLIDO TRIANA, LUIS/0000-0001-9591-0928; FERNANDEZ CAMBRONERO,
   DAVID/0000-0002-2172-9162; Cardenas, Andres/0000-0002-8725-7786
FU Spanish Ministry of Economy and Competitiveness; Spanish Ministry of
   Science and Innovation in the context of ECTICS Project
   [PID2019-105257RB-C21, RED2018-102585-T]
FX This work was supported in part by the Spanish Ministry of Economy and
   Competitiveness and in part by the Spanish Ministry of Science and
   Innovation in the context of ECTICS Project under Grants
   PID2019-105257RB-C21 and Go2Edge under Grant RED2018-102585-T.
CR 3rd Generation Partnership Project, 2020, 23502V1640 3GPP TS
   3rd Generation Partnership Project, 2020, 23501V1640 3GPP TS 5
   3rd Generation Partnership Project, 2019, 23246V1610 3GPP TS M
   [Anonymous], 2020, Cisco Annual Internet Report (2018-2023) White Paper
   [Anonymous], 2017, P NAACL HLT
   [Anonymous], 2019, 23214V160 3GPP TS
   [Anonymous], 2019, white paper
   [Anonymous], 2017, 230095 ISOIEC
   [Anonymous], 2020, 3GPP STAT LTE BAS 5G
   Anttonen A, 2014, IEEE T MULTIMEDIA, V16, P1176, DOI 10.1109/TMM.2014.2306656
   Bobarshad H, 2012, IEEE T MULTIMEDIA, V14, P401, DOI 10.1109/TMM.2011.2173477
   Chantre HD, 2018, IEEE COMMUN MAG, V56, P218, DOI 10.1109/MCOM.2018.1700648
   de la Fuente A, 2016, 2016 EUROPEAN CONFERENCE ON NETWORKS AND COMMUNICATIONS (EUCNC), P57, DOI 10.1109/EuCNC.2016.7561004
   Dutta S, 2016, IEEE GLOB COMM CONF
   Fernández D, 2016, INT J ENG EDUC, V32, P2569
   ISO/IEC, 2014, Rep. 23009-1. ISO/IEC JTCI/SC29/WG11
   Kekki Sami, 2020, CISC VIS NETW IND GL
   Lentisco CM, 2017, IEEE ACCESS, V5, P20684, DOI 10.1109/ACCESS.2017.2755438
   Lentisco CM, 2017, MOB INF SYST, V2017, DOI 10.1155/2017/1847538
   Lentisco CM, 2017, IEEE T MULTIMEDIA, V19, P173, DOI 10.1109/TMM.2016.2620605
   Lentisco CM, 2014, IEEE CONF WIREL MOB, P691, DOI 10.1109/WiMOB.2014.6962246
   Li H, 2021, IEEE T CLOUD COMPUT, V9, P834, DOI 10.1109/TCC.2018.2871118
   Lohmar T., 2011, IEEE INT S WORLD WIR, P1, DOI DOI 10.1109/WOWMOM.2011.5986186
   Mehrabi A, 2019, IEEE T MOBILE COMPUT, V18, P787, DOI 10.1109/TMC.2018.2850026
   Mobile Edge Computing (MEC), 2018, 017V111 ETSI GR MEC
   Multi-access Edge Computing (MEC), 2020, 031V211 ETSI GR MEC
   Multimedia Broadcast/Multicast Service (MBMS), 2020, 26346V1641 3GPP TS M
   Network Function Virtualisation (NFV), 2014, 001V111 ETSI GS NFV
   Network Functions Virtualisation (NFV), 2014, 002V121 ETSI GS NFV
   Nunes BAA, 2014, IEEE COMMUN SURV TUT, V16, P1617, DOI 10.1109/SURV.2014.012214.00180
   Peterson L, 2016, IEEE COMMUN MAG, V54, P96, DOI 10.1109/MCOM.2016.7588276
   Säily M, 2020, IEEE T BROADCAST, V66, P404, DOI 10.1109/TBC.2020.2985906
   Satyanarayanan M, 2017, COMPUTER, V50, P30, DOI 10.1109/MC.2017.9
   Taleb T, 2020, IEEE T MOBILE COMPUT, V19, P2010, DOI 10.1109/TMC.2019.2921712
   Tan E, 2012, IEEE T MULTIMEDIA, V14, P910, DOI 10.1109/TMM.2011.2180706
   Do TX, 2018, WIREL NETW, V24, P1715, DOI 10.1007/s11276-016-1433-6
   Tran T, 2020, IEEE T BROADCAST, V66, P428, DOI 10.1109/TBC.2020.2991548
   Walsh R., 2012, 6726 IETF RFC
   Wu J, 2015, IEEE NETWORK, V29, P35, DOI 10.1109/MNET.2015.7018201
NR 39
TC 0
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 378
EP 388
DI 10.1109/TMM.2021.3126458
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800004
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Li, WH
   Liu, H
   Ding, RW
   Liu, MY
   Wang, PC
   Yang, WM
AF Li, Wenhao
   Liu, Hong
   Ding, Runwei
   Liu, Mengyuan
   Wang, Pichao
   Yang, Wenming
TI Exploiting Temporal Contexts With Strided Transformer for 3D Human Pose
   Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Three-dimensional displays; Pose estimation; Task
   analysis; Videos; Solid modeling; Computer architecture; 3D human pose
   estimation; transformer; strided convolution
ID ACTION RECOGNITION
AB Despite the great progress in 3D human pose estimation from videos, it is still an open problem to take full advantage of a redundant 2D pose sequence to learn representative representations for generating one 3D pose. To this end, we propose an improved Transformer-based architecture, called Strided Transformer, which simply and effectively lifts a long sequence of 2D joint locations to a single 3D pose. Specifically, a Vanilla Transformer Encoder (VTE) is adopted to model long-range dependencies of 2D pose sequences. To reduce the redundancy of the sequence, fully-connected layers in the feed-forward network of VTE are replaced with strided convolutions to progressively shrink the sequence length and aggregate information from local contexts. The modified VTE is termed as Strided Transformer Encoder (STE), which is built upon the outputs of VTE. STE not only effectively aggregates long-range information to a single-vector representation in a hierarchical global and local fashion, but also significantly reduces the computation cost. Furthermore, a full-to-single supervision scheme is designed at both full sequence and single target frame scales applied to the outputs of VTE and STE, respectively. This scheme imposes extra temporal smoothness constraints in conjunction with the single target frame supervision and hence helps produce smoother and more accurate 3D poses. The proposed Strided Transformer is evaluated on two challenging benchmark datasets, Human3.6 M and HumanEva-I, and achieves state-of-the-art results with fewer parameters. Code and models are available at https://github.com/Vegetebird/StridedTransformer-Pose3D.
C1 [Li, Wenhao; Liu, Hong; Ding, Runwei] Peking Univ, Shenzhen Grad Sch, Key Lab Machine Percept, Beijing, Peoples R China.
   [Liu, Mengyuan] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China.
   [Wang, Pichao] Alibaba Grp, Bellevue, WA 98004 USA.
   [Yang, Wenming] Tsinghua Univ, Grad Sch Shenzhen, Dept Elect Engn, Shenzhen Engn Lab IS & DRM, Shenzhen, Peoples R China.
C3 Peking University; Sun Yat Sen University; Alibaba Group; Tsinghua
   Shenzhen International Graduate School; Tsinghua University
RP Liu, H (corresponding author), Peking Univ, Shenzhen Grad Sch, Key Lab Machine Percept, Beijing, Peoples R China.
EM wenhaoli@pku.edu.cn; hongliu@pku.edu.cn; dingrunwei@pku.edu.cn;
   nkliuyifang@gmail.com; pichao.wang@alibaba-inc.com; yangelwm@163.com
RI 刘, 梦媛/KIH-9841-2024
OI Liu, Hong/0000-0002-7498-6541; Wang, Pichao/0000-0002-1430-0237; Yang,
   Wenming/0000-0002-2506-1286
FU National Key R&D Program of China [2020AAA0108904]; Basic and Applied
   Basic Research Foundation of Guangdong [2020A1515110370]; Science and
   Technology Plan of Shenzhen [JCYJ20190808182209321,
   JCYJ20200109140410340]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2020AAA0108904, in part by the Basic and Applied Basic
   Research Foundation of Guangdong under Grant 2020A1515110370,and in part
   by the Science and Technology Plan of Shenzhen under Grants
   JCYJ20190808182209321 and JCYJ20200109140410340.
CR Ailing Zeng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P507, DOI 10.1007/978-3-030-58568-6_30
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen C, 2017, IEEE ACCESS, V5, P22590, DOI 10.1109/ACCESS.2017.2759058
   Chen TL, 2022, IEEE T CIRC SYST VID, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Chen XP, 2019, PROC CVPR IEEE, P10887, DOI 10.1109/CVPR.2019.01115
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chen Z, 2021, AAAI CONF ARTIF INTE, V35, P1113, DOI 10.1145/3474085.3475574
   Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081
   Dabral R, 2018, LECT NOTES COMPUT SC, V11213, P679, DOI 10.1007/978-3-030-01240-3_41
   Dai Z., 2020, ADV NEUR IN, V33, P4271
   Dosovitskiy A, 2020, PROC INT C LEARN REP
   Fang HS, 2018, AAAI CONF ARTIF INTE, P6821
   Garcia-Salguero M, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19224943
   Geva M, 2021, Arxiv, DOI arXiv:2012.14913
   Gong KH, 2021, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR46437.2021.00847
   Gui LY, 2018, IEEE INT C INT ROBOT, P562, DOI 10.1109/IROS.2018.8594452
   Han K, 2022, Arxiv, DOI arXiv:2012.12556
   Han L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1469, DOI 10.1145/3394171.3413927
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He S., 2021, P IEEECVF INT C COMP, P15013, DOI DOI 10.1109/ICCV48922.2021.01474
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Hu PP, 2022, IEEE T MULTIMEDIA, V24, P2139, DOI 10.1109/TMM.2021.3076340
   Hua G., 2021, ARXIV
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jiang Z.-H., 2020, Advances in Neural Information Processing Systems, V33, P12837
   Jingbo Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P764, DOI 10.1007/978-3-030-58601-0_45
   Kadkhodamohammadi A, 2020, MACH VISION APPL, V32, DOI 10.1007/s00138-020-01120-2
   Kenkun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P318, DOI 10.1007/978-3-030-58607-2_19
   Kocabas M, 2019, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2019.00117
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Li C, 2022, IEEE T NEUR NET LEAR, V33, P4800, DOI 10.1109/TNNLS.2021.3061115
   Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23
   Li XY, 2022, IEEE T COGN DEV SYST, V14, P246, DOI 10.1109/TCDS.2020.3048883
   Li XY, 2021, NEURAL COMPUT APPL, V33, P8031, DOI 10.1007/s00521-020-05545-8
   Lin J., 2019, arXiv
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P494, DOI 10.1109/TPAMI.2019.2894422
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Liu MY, 2018, IEEE T MULTIMEDIA, V20, P1932, DOI 10.1109/TMM.2017.2786868
   Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763
   Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Pullen K, 2002, ACM T GRAPHIC, V21, P501
   Radwan I, 2013, IEEE I CONF COMP VIS, P1888, DOI 10.1109/ICCV.2013.237
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Song YF, 2022, Arxiv, DOI arXiv:2106.15125
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Tay Y, 2022, Arxiv, DOI arXiv:2009.06732
   Tome D, 2018, INT CONF 3D VISION, P474, DOI 10.1109/3DV.2018.00061
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Wei P, 2019, IEEE T MULTIMEDIA, V21, P2195, DOI 10.1109/TMM.2019.2897902
   Wu Z., 2020, PROC INT C LEARN REP
   Xu JW, 2020, PROC CVPR IEEE, P896, DOI 10.1109/CVPR42600.2020.00098
   Xu TH, 2021, PROC CVPR IEEE, P16100, DOI 10.1109/CVPR46437.2021.01584
   Yang D., 2021, Unik: A unified framework for real-world skeleton-based action recognition
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang BC, 2017, IEEE T IMAGE PROCESS, V26, P4648, DOI 10.1109/TIP.2017.2718189
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zhao TH, 2019, IEEE T MULTIMEDIA, V21, P114, DOI 10.1109/TMM.2018.2844087
   Zhu X., 2020, PROC INT C LEARN REP
NR 65
TC 47
Z9 47
U1 27
U2 52
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1282
EP 1293
DI 10.1109/TMM.2022.3141231
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100020
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Li, YY
   Zhai, W
   Cao, Y
   Zha, ZJ
AF Li, Yangyang
   Zhai, Wei
   Cao, Yang
   Zha, Zheng-Jun
TI Location-Free Camouflage Generation Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Search problems; Object recognition; Iterative methods; Task analysis;
   Fuses; Visual effects; Vegetation mapping; Camouflage generation; deep
   convolution neural network; application
ID STYLE TRANSFER; IMAGE; TEXTURE; REGULARIZATION
AB Camouflage is a common visual phenomenon, which refers to hiding the foreground objects into the background images, making them briefly invisible to the human eye. Previous work has typically been implemented by an iterative optimization process. However, these methods struggle in 1) efficiently generating camouflage images using foreground and background with flexible structure; 2) camouflaging foreground objects to regions with multiple appearances (e.g. the junction of the vegetation and the mountains), which limit their practical application. To address these problems, this paper proposes a novel Location-free Camouflage Generation Network (LCG-Net) that fuse high-level features of foreground and background image, and generate result by one inference. Specifically, a Position-aligned Structure Fusion (PSF) module is devised to guide structure feature fusion based on the point-to-point structure similarity of foreground and background, and introduce local appearance features point-by-point. To retain the necessary identifiable features, a new immerse loss is adopted under our pipeline, while a background patch appearance loss is utilized to ensure that the hidden objects look continuous and natural at regions with multiple appearances. Experiments show that our method has results as satisfactory as state-of-the-art in the single-appearance regions and are less likely to be completely invisible, but far exceed the quality of the state-of-the-art in the multi-appearance regions. Moreover, our method is hundreds of times faster than previous methods. Benefitting from the unique advantages of our method, we provide some downstream applications for camouflage generation, which show its potential. The related code and dataset will be released at https://github.com/Tale17/LCG-Net.
C1 [Li, Yangyang; Zhai, Wei; Cao, Yang; Zha, Zheng-Jun] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Cao, Yang] Inst Artificial Intelligence, Hefei Comprehens Natl Sci Ctr, Hefei, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Cao, Y (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
EM lyy1030@mail.ustc.edu.cn; wzhai056@mail.ustc.edu.cn;
   forrest@ustc.edu.cn; zhazj@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020
FU National Key Ramp;D Program of China
FX No Statement Available
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   Ballester P, 2016, AAAI CONF ARTIF INTE, P1124
   Bethge M, 2019, Arxiv, DOI arXiv:1904.00760
   Brown TB, 2018, Arxiv, DOI arXiv:1712.09665
   Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126
   Chu HK, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778788
   Deng YY, 2021, AAAI CONF ARTIF INTE, V35, P1210
   Duan RJ, 2020, PROC CVPR IEEE, P997, DOI 10.1109/CVPR42600.2020.00108
   Dumoulin V, 2017, Arxiv, DOI [arXiv:1610.07629, DOI 10.48550/ARXIV.1610.07629]
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Frigo O, 2016, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2016.66
   Gatys L., 2015, NIPS
   Gatys LA, 2017, CURR OPIN NEUROBIOL, V46, P178, DOI 10.1016/j.conb.2017.08.019
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Geirhos R, 2019, Arxiv, DOI arXiv:1811.12231
   Gupta A, 2017, IEEE I CONF COMP VIS, P4087, DOI 10.1109/ICCV.2017.438
   Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Huang HZ, 2017, PROC CVPR IEEE, P7044, DOI 10.1109/CVPR.2017.745
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Jing YC, 2020, AAAI CONF ARTIF INTE, V34, P4369
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kazhdan M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360620
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Kingma Diederik P., 2015, P 3 INT C LEARN
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kubilius J, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004896
   Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11
   Levin A, 2004, LECT NOTES COMPUT SC, V2034, P377
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li J, 2019, IEEE T MULTIMEDIA, V21, P1437, DOI 10.1109/TMM.2018.2880604
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YJ, 2017, ADV NEUR IN, V30
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6629, DOI 10.1109/ICCV48922.2021.00658
   Lockerman YD, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925964
   Luan FJ, 2018, COMPUT GRAPH FORUM, V37, P95, DOI 10.1111/cgf.13478
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Reynolds C, 2011, ARTIF LIFE, V17, P123, DOI 10.1162/artl_a_00023
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tankus A, 2001, COMPUT VIS IMAGE UND, V82, P208, DOI 10.1006/cviu.2001.0912
   TREISMAN A, 1988, Q J EXP PSYCHOL-A, V40, P201, DOI 10.1080/02724988843000104
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Uyttendaele M, 2001, PROC CVPR IEEE, P509
   Virtusio JJ, 2021, IEEE T MULTIMEDIA, V23, P2245, DOI 10.1109/TMM.2021.3087026
   Wang X, 2017, PROC CVPR IEEE, P7178, DOI 10.1109/CVPR.2017.759
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
   Xiao B, 2020, IEEE T MULTIMEDIA, V22, P285, DOI 10.1109/TMM.2019.2928516
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhang LZ, 2020, IEEE WINT CONF APPL, P231, DOI [10.1109/wacv45572.2020.9093632, 10.1109/WACV45572.2020.9093632]
   Zhang Q, 2020, AAAI CONF ARTIF INTE, V34, P12845
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 67
TC 1
Z9 1
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5234
EP 5247
DI 10.1109/TMM.2022.3189250
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD7R0
UT WOS:001116593700001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lin, DY
   Li, YQ
   Cheng, Y
   Prasad, S
   Guo, AY
   Cao, YP
AF Lin, Dongyun
   Li, Yiqun
   Cheng, Yi
   Prasad, Shitala
   Guo, Aiyuan
   Cao, Yanpeng
TI Multi-Range View Aggregation Network With Vision Transformer Feature
   Fusion for 3D Object Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Feature extraction; Transformers;
   Convolutional neural networks; Visualization; Fuses; Deep learning; 3D
   object retrieval; multi-range view aggregation; multi-head
   self-attention; feature fusion
ID SIMILARITY; DIFFUSION
AB View-based methods have achieved state-of-the-art performance in 3D object retrieval. However, view-based methods still encounter two major challenges. The first is how to leverage the inter-view correlation to enhance view-level visual features. The second is how to effectively fuse view-level features into a discriminative global descriptor. Towards these two challenges, we propose a multi-range view aggregation network (MRVA-Net) with a vision transformer based feature fusion scheme for 3D object retrieval. Unlike the existing methods which only consider aggregating neighboring or adjacent views which could bring in redundant information, we propose a multi-range view aggregation module to enhance individual view representations through view aggregation beyond only neighboring views but also incorporate the views at different ranges. Furthermore, to generate the global descriptor from view-level features, we propose to employ the multi-head self-attention mechanism introduced by vision transformer to fuse the view-level features. Extensive experiments conducted on three public datasets including ModelNet40, ShapeNet Core55 and MCB-A demonstrate the superiority of the proposed network over the state-of-the-art methods in 3D object retrieval.
C1 [Lin, Dongyun; Li, Yiqun; Cheng, Yi; Prasad, Shitala; Guo, Aiyuan] ASTAR, Inst Infocomm Res I2R, Singapore 138632, Singapore.
   [Cao, Yanpeng] Zhejiang Univ, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); Zhejiang University
RP Lin, DY (corresponding author), ASTAR, Inst Infocomm Res I2R, Singapore 138632, Singapore.
EM lind@i2r.a-star.edu.sg; yqli@i2r.a-star.edu.sg;
   cheng_yi@i2r.a-star.edu.sg; shitala_prasad@i2r.a-star.edu.sg;
   aguo@i2r.a-star.edu.sg; caoyp@zju.edu.cn
OI LIN, DONGYUN/0000-0002-4595-4049
FU Agency for Science, Technology and Research
FX No Statement Available
CR Bai S, 2017, IEEE I CONF COMP VIS, P774, DOI 10.1109/ICCV.2017.90
   Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Bai X, 2015, INFORM SCIENCES, V325, P342, DOI 10.1016/j.ins.2015.07.022
   Brock A, 2016, Arxiv, DOI arXiv:1608.04236
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen S., 2021, P BRIT MACH VIS C
   Cheraghian A, 2019, IEEE WINT CONF APPL, P1194, DOI 10.1109/WACV.2019.00132
   Daras P, 2009, INT WORK CONTENT MUL, P115, DOI 10.1109/CBMI.2009.15
   Dominguez M, 2018, IEEE WINT CONF APPL, P1972, DOI 10.1109/WACV.2018.00218
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Furuya T., 2016, P BRIT MACH VIS C, V7
   Furuya T, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P171, DOI 10.1145/2671188.2749380
   Han ZZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P758
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XW, 2019, IEEE I CONF COMP VIS, P7514, DOI 10.1109/ICCV.2019.00761
   He XW, 2020, IEEE T IMAGE PROCESS, V29, P7917, DOI 10.1109/TIP.2020.3008970
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Jiang JW, 2019, AAAI CONF ARTIF INTE, P8513
   Kanezaki A, 2021, IEEE T PATTERN ANAL, V43, P269, DOI 10.1109/TPAMI.2019.2922640
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Leng B, 2019, IEEE T VIS COMPUT GR, V25, P2896, DOI 10.1109/TVCG.2018.2865317
   Li B, 2013, MULTIMED TOOLS APPL, V62, P821, DOI 10.1007/s11042-011-0873-3
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li YZ, 2018, ADV NEUR IN, V31
   Li ZQ, 2019, AAAI CONF ARTIF INTE, P8682
   Lian ZH, 2013, MACH VISION APPL, V24, P1685, DOI 10.1007/s00138-013-0501-5
   Lin DY, 2022, KNOWL-BASED SYST, V247, DOI 10.1016/j.knosys.2022.108754
   Liu AA, 2021, INFORM SCIENCES, V547, P984, DOI 10.1016/j.ins.2020.09.057
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1021, DOI 10.1109/TMM.2020.2991532
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1962, DOI 10.1109/TMM.2020.3006371
   Nie WZ, 2021, IEEE T IMAGE PROCESS, V30, P4371, DOI 10.1109/TIP.2021.3071687
   Qi CR, 2017, ADV NEUR IN, V30
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Shi Ziqi, 2022, CSSE 2022: 2022 5th International Conference on Computer Science and Software Engineering (CSSE 2022), P474, DOI 10.1145/3569966.3570092
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su Jong-Chyi, 2018, P EUR C COMP VIS ECC, P1
   Wang D, 2019, IEEE T MULTIMEDIA, V21, P2071, DOI 10.1109/TMM.2019.2892004
   Wang D, 2017, NEUROCOMPUTING, V252, P58, DOI 10.1016/j.neucom.2016.06.095
   Wang WJ, 2022, NEURAL COMPUT APPL, V34, P3201, DOI 10.1007/s00521-021-06588-1
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu C, 2019, IEEE I CONF COMP VIS, P3731, DOI 10.1109/ICCV.2019.00383
   Xu L, 2019, PROC CVPR IEEE, P3328, DOI 10.1109/CVPR.2019.00345
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yang ZT, 2021, PROC CVPR IEEE, P1863, DOI 10.1109/CVPR46437.2021.00190
   You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
   Zhu F, 2022, IMAGE VISION COMPUT, V121, DOI 10.1016/j.imavis.2022.104405
NR 59
TC 2
Z9 2
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9108
EP 9119
DI 10.1109/TMM.2023.3246229
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200023
DA 2024-07-18
ER

PT J
AU Liu, JW
   Wang, Q
   Fan, HJ
   Li, WT
   Qu, LQ
   Tang, YD
AF Liu, Jiawei
   Wang, Qiang
   Fan, Huijie
   Li, Wentao
   Qu, Liangqiong
   Tang, Yandong
TI A Decoupled Multi-Task Network for Shadow Removal
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Decomposition; feature decoupling; illumination compensation;
   multi-task; shadow removal
ID MOVING CAST SHADOWS; ILLUMINATION; MODEL
AB Shadow removal, which aims to restore the illumination in shadow regions, is challenging due to the diversity of shadows in terms of location, intensity, shape, and size. Different from most multi-task methods, which design elaborate multi-branch or multi-stage structures for better shadow removal, we introduce feature decomposition to learn better feature representations. Specifically, we propose a single-stage and decoupled multi-task network (DMTN) to explicitly learn the decomposed features for shadow removal, shadow matte estimation, and shadow image reconstruction. First, we propose several coarse-to-fine semi-convolution (SMC) modules to capture features sufficient for joint learning of these three tasks. Second, we design a theoretically supported feature decoupling layer to explicitly decouple the learned features into shadow image features and shadow matte features via weight reassignment. Last, these features are converted to a target shadow-free image, affiliated shadow matte, and shadow image, supervised by multi-task joint loss functions. With multi-task collaboration, DMTN effectively recovers the illumination in shadow areas while ensuring the fidelity of non-shadow areas. Experimental results show that DMTN competes favorably with state-of-the-art multi-branch/multi-stage shadow removal methods, while maintaining the simplicity of single-stage methods.
C1 [Liu, Jiawei; Wang, Qiang; Fan, Huijie; Li, Wentao; Tang, Yandong] Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang 110016, Peoples R China.
   [Liu, Jiawei; Fan, Huijie; Li, Wentao; Tang, Yandong] Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang 110169, Peoples R China.
   [Liu, Jiawei] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Wang, Qiang] Shenyang Univ, Key Lab Mfg Ind Integrated, Shenyang 110044, Peoples R China.
   [Qu, Liangqiong] Univ Hong Kong, Dept Stat & Actuarial Sci, Hong Kong 999077, Peoples R China.
   [Qu, Liangqiong] Univ Hong Kong, Inst Data Sci, Hong Kong 999077, Peoples R China.
C3 Chinese Academy of Sciences; Shenyang Institute of Automation, CAS;
   Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Shenyang University; University of
   Hong Kong; University of Hong Kong
RP Fan, HJ (corresponding author), Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang 110016, Peoples R China.
EM liujiawei@sia.cn; wangqiang@sia.cn; fanhuijie@sia.cn; liwentao@sia.cn;
   liangqqu@hku.hk; ytang@sia.cn
OI Qu, Liangqiong/0000-0001-8235-7852; Wang, Qiang/0000-0002-2018-1764;
   Liu, Jiawei/0000-0002-7516-5008; Tang, Yandong/0000-0003-3805-7654
FU National Natural Science Foundation of China
FX No Statement Available
CR Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Arbel E, 2011, IEEE T PATTERN ANAL, V33, P1202, DOI 10.1109/TPAMI.2010.157
   Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273
   Chen ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4723, DOI 10.1109/ICCV48922.2021.00470
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Chuang YY, 2003, ACM T GRAPHIC, V22, P494, DOI 10.1145/882262.882298
   Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909
   Cun XD, 2020, AAAI CONF ARTIF INTE, V34, P10680
   Das P, 2022, PROC CVPR IEEE, P19758, DOI 10.1109/CVPR52688.2022.01917
   Ding B, 2019, IEEE I CONF COMP VIS, P10212, DOI 10.1109/ICCV.2019.01031
   Dinh J., 2017, P INT C LEARN REPR
   Duan HY, 2023, IEEE T MULTIMEDIA, V25, P4267, DOI 10.1109/TMM.2022.3172882
   Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18
   Finlayson GD, 2009, INT J COMPUT VISION, V85, P35, DOI 10.1007/s11263-009-0243-z
   Fu L, 2021, PROC CVPR IEEE, P10566, DOI 10.1109/CVPR46437.2021.01043
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gong D., 2014, BRIT MACH VIS C, P1
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gryka M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2732407
   Guo RQ, 2013, IEEE T PATTERN ANAL, V35, P2956, DOI 10.1109/TPAMI.2012.214
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hieu Le, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P264, DOI 10.1007/978-3-030-58621-8_16
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XW, 2019, IEEE I CONF COMP VIS, P2472, DOI 10.1109/ICCV.2019.00256
   Hu XW, 2020, IEEE T PATTERN ANAL, V42, P2795, DOI 10.1109/TPAMI.2019.2919616
   Inoue N, 2021, IEEE T CIRC SYST VID, V31, P4187, DOI 10.1109/TCSVT.2020.3047977
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jin YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5007, DOI 10.1109/ICCV48922.2021.00498
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Jung CR, 2009, IEEE T MULTIMEDIA, V11, P571, DOI 10.1109/TMM.2009.2012924
   Khan SH, 2016, IEEE T PATTERN ANAL, V38, P431, DOI 10.1109/TPAMI.2015.2462355
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Le H, 2022, IEEE T PATTERN ANAL, V44, P9088, DOI 10.1109/TPAMI.2021.3124934
   Le H, 2019, IEEE I CONF COMP VIS, P8577, DOI 10.1109/ICCV.2019.00867
   Lee Y, 2020, P IEEE CVF C COMP VI
   Liu YF, 2020, PROC CVPR IEEE, P3245, DOI 10.1109/CVPR42600.2020.00331
   Liu ZH, 2021, PROC CVPR IEEE, P4925, DOI 10.1109/CVPR46437.2021.00489
   Liu ZH, 2021, IEEE T IMAGE PROCESS, V30, P1853, DOI 10.1109/TIP.2020.3048677
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Miyato T, 2018, INT C LEARN REPR
   Nadimi S, 2004, IEEE T PATTERN ANAL, V26, P1079, DOI 10.1109/TPAMI.2004.51
   Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Qu LQ, 2015, OPT EXPRESS, V23, P2220, DOI 10.1364/OE.23.002220
   Sanin Andres, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P141, DOI 10.1109/ICPR.2010.43
   Shin J, 2022, IEEE T MULTIMEDIA, V24, P245, DOI 10.1109/TMM.2021.3050053
   Shor Y, 2008, COMPUT GRAPH FORUM, V27, P577, DOI 10.1111/j.1467-8659.2008.01155.x
   Shu XB, 2023, IEEE T PATTERN ANAL, V45, P7559, DOI 10.1109/TPAMI.2022.3222871
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stauder J, 1999, IEEE T MULTIMEDIA, V1, P65, DOI 10.1109/6046.748172
   Tian JD, 2011, PROC CVPR IEEE, P985, DOI 10.1109/CVPR.2011.5995622
   Tian JD, 2009, IEEE T IMAGE PROCESS, V18, P2355, DOI 10.1109/TIP.2009.2026682
   Tu Z., 2022, P IEEE CVF C COMP VI, P5769
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wu TP, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243982
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P3852, DOI 10.1109/TIP.2022.3175605
   Yang QX, 2012, IEEE T IMAGE PROCESS, V21, P4361, DOI 10.1109/TIP.2012.2208976
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang L, 2020, AAAI CONF ARTIF INTE, V34, P12829
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P4623, DOI 10.1109/TIP.2015.2465159
   Zhang W, 2007, IEEE T MULTIMEDIA, V9, P1202, DOI 10.1109/TMM.2007.902842
   Zhang WM, 2019, IEEE T PATTERN ANAL, V41, P611, DOI 10.1109/TPAMI.2018.2803179
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
   Zhengxia Zou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12803, DOI 10.1109/CVPR42600.2020.01282
   Zhong YQ, 2022, PROC CVPR IEEE, P15324, DOI 10.1109/CVPR52688.2022.01491
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu YR, 2022, AAAI CONF ARTIF INTE, P3635
   Zhu YR, 2022, PROC CVPR IEEE, P5617, DOI 10.1109/CVPR52688.2022.00554
NR 77
TC 1
Z9 1
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9449
EP 9463
DI 10.1109/TMM.2023.3252271
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200038
DA 2024-07-18
ER

PT J
AU Liu, Q
   Yuan, D
   Fan, NA
   Gao, P
   Li, X
   He, ZY
AF Liu, Qiao
   Yuan, Di
   Fan, Nana
   Gao, Peng
   Li, Xin
   He, Zhenyu
TI Learning Dual-Level Deep Representation for Thermal Infrared Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object tracking; Biological system modeling; Task analysis; Correlation;
   Multitasking; Adaptation models; Feature extraction; Thermal infrared
   object tracking; Deep representation learning; Fine-grained correlation
   feature; Thermal infrared dataset
ID OBJECT TRACKING; NETWORKS
AB The feature models used by existing Thermal InfraRed (TIR) tracking methods are usually learned from RGB images due to the lack of a large-scale TIR image training dataset. However, these feature models are less effective in representing TIR objects and they are difficult to effectively distinguish distractors because they do not contain fine-grained discriminative information. To this end, we propose a dual-level feature model containing the TIR-specific discriminative feature and fine-grained correlation feature for robust TIR object tracking. Specifically, to distinguish inter-class TIR objects, we first design an auxiliary multi-classification network to learn the TIR-specific discriminative feature. Then, to recognize intra-class TIR objects, we propose a fine-grained aware module to learn the fine-grained correlation feature. These two kinds of features complement each other and represent TIR objects in the levels of inter-class and intra-class respectively. These two feature models are constructed using a multi-task matching framework and are jointly optimized on the TIR object tracking task. In addition, we develop a large-scale TIR image dataset to train the network for learning TIR-specific feature patterns. To the best of our knowledge, this is the largest TIR tracking training dataset with the richest object class and scenario. To verify the effectiveness of the proposed dual-level feature model, we propose an offline TIR tracker (MMNet) and an online TIR tracker (ECO-MM) based on the feature model and evaluate them on three TIR tracking benchmarks. Extensive experimental results on these benchmarks demonstrate that the proposed algorithms perform favorably against the state-of-the-art methods.
C1 [Liu, Qiao] Chongqing Normal Univ, Natl Ctr Appl Math Chongqing, Chongqing 401331, Peoples R China.
   [Liu, Qiao; Fan, Nana; Li, Xin; He, Zhenyu] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Yuan, Di] Xidian Univ, Guangzhou Inst Technol, Guangzhou 510555, Peoples R China.
   [Gao, Peng] Qufu Normal Univ, Sch Cyber Sci & Engn, Qufu 273165, Shandong, Peoples R China.
C3 Chongqing Normal University; Harbin Institute of Technology; Xidian
   University; Qufu Normal University
RP Li, X; He, ZY (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM liuqiao.hit@gmail.com; dyuanhit@gmail.com; nanafanhit@gmail.com;
   pgao@qfnu.edu.cn; xinlihitsz@gmail.com; zhenyuhe@hit.edu.cn
RI Gao, Peng/IXD-5790-2023; Yuan, Di/Q-6521-2019
OI Gao, Peng/0000-0003-2230-3937; Yuan, Di/0000-0001-9403-1112; Li,
   Xin/0000-0002-1670-1368; Liu, Qiao/0000-0003-0885-7976
FU National Natural Science Foundation of China [62172126, 62002241];
   Shenzhen Research Council [JCYJ20210324120202006]; Special Research
   Project on COVID-19 Prevention and Control of Guangdong Province
   [2020KZDZDX1227]; Foundation Project of Chongqing Normal University
   [202109000441]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172126 and 62002241, inpart by the
   Shenzhen Research Council under Grant JCYJ20210324120202006,in part by
   the Special Research Project on COVID-19 Prevention and Control
   ofGuangdong Province under Grant 2020KZDZDX1227, and in part by the
   Foun-dation Project of Chongqing Normal University under Grant
   202109000441.The Associate Editor coordinating the review of this
   manuscript and approv-ing it for publication was Prof. Houqiang Li
CR [Anonymous], 2015, PROC 12 IEEE INT C A
   [Anonymous], 2017, P IEEE INT C COMP VI
   [Anonymous], 2015, P 23 ACM INT C MULT
   [Anonymous], 2016, P EUR C COMP VIS WOR
   Berg A, 2016, IEEE COMPUT SOC CONF, P1248, DOI 10.1109/CVPRW.2016.158
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhattarai B, 2016, PROC CVPR IEEE, P4226, DOI 10.1109/CVPR.2016.458
   Chen WH, 2017, AAAI CONF ARTIF INTE, P3988
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Dai KN, 2020, PROC CVPR IEEE, P6297, DOI 10.1109/CVPR42600.2020.00633
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M., 2017, P IEEE C COMP VIS PA, P6638
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Davis JW, 2007, COMPUT VIS IMAGE UND, V106, P162, DOI 10.1016/j.cviu.2006.06.010
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Felsberg M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P639, DOI 10.1109/ICCVW.2015.86
   Gade R, 2014, MACH VISION APPL, V25, P245, DOI 10.1007/s00138-013-0570-5
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Gao P, 2018, INT C PATT RECOG, P2380, DOI 10.1109/ICPR.2018.8545716
   Gao SJ, 2016, 2016 RESEARCH IN ADAPTIVE AND CONVERGENT SYSTEMS, P40, DOI 10.1145/2987386.2987392
   Ge SM, 2021, IEEE T NEUR NET LEAR, V32, P1276, DOI 10.1109/TNNLS.2020.2984256
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Gundogdu Erhan, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301290
   Gundogdu E, 2016, IEEE COMPUT SOC CONF, P290, DOI 10.1109/CVPRW.2016.43
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   He YJ, 2015, INFRARED PHYS TECHN, V73, P103, DOI 10.1016/j.infrared.2015.09.010
   Huang LH, 2020, AAAI CONF ARTIF INTE, V34, P11037
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li CL, 2019, IEEE INT CONF COMP V, P2262, DOI 10.1109/ICCVW.2019.00279
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li X, 2019, KNOWL-BASED SYST, V166, P71, DOI 10.1016/j.knosys.2018.12.011
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P2114, DOI 10.1109/TMM.2020.3008028
   Liu Q, 2020, AAAI CONF ARTIF INTE, V34, P11604
   Liu Q, 2020, IEEE T MULTIMEDIA, V22, P666, DOI 10.1109/TMM.2019.2932615
   Liu Q, 2017, KNOWL-BASED SYST, V134, P189, DOI 10.1016/j.knosys.2017.07.032
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, Arxiv, DOI arXiv:1608.07242
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Portmann J, 2014, IEEE INT CONF ROBOT, P1794, DOI 10.1109/ICRA.2014.6907094
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen C, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1942, DOI 10.1145/3123266.3123452
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Tang M, 2015, IEEE I CONF COMP VIS, P3038, DOI 10.1109/ICCV.2015.348
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang CQ, 2020, PROC CVPR IEEE, P7062, DOI 10.1109/CVPR42600.2020.00709
   Wang Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P985
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Z, 2014, IEEE COMPUT SOC CONF, P201, DOI 10.1109/CVPRW.2014.39
   Yan B, 2019, IEEE I CONF COMP VIS, P2385, DOI 10.1109/ICCV.2019.00247
   Yu XG, 2017, PATTERN RECOGN LETT, V100, P152, DOI 10.1016/j.patrec.2017.10.026
   Yun S, 2018, IEEE T NEUR NET LEAR, V29, P2239, DOI 10.1109/TNNLS.2018.2801826
   Yunhua Zhang, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11213), P355, DOI 10.1007/978-3-030-01240-3_22
   Zhang LC, 2019, IEEE T IMAGE PROCESS, V28, P1837, DOI 10.1109/TIP.2018.2879249
   Zhang PY, 2021, INT J COMPUT VISION, V129, P2714, DOI 10.1007/s11263-021-01495-3
   Zhang PY, 2021, IEEE T IMAGE PROCESS, V30, P3335, DOI 10.1109/TIP.2021.3060862
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu YB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P465, DOI 10.1145/3343031.3350928
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
NR 75
TC 66
Z9 67
U1 11
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1269
EP 1281
DI 10.1109/TMM.2022.3140929
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100019
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Liu, YH
   He, JW
   Chen, XY
   Zhang, ZW
   Zhao, HY
   Dong, C
   Qiao, Y
AF Liu, Yihao
   He, Jingwen
   Chen, Xiangyu
   Zhang, Zhengwen
   Zhao, Hengyuan
   Dong, Chao
   Qiao, Yu
TI Very Lightweight Photo Retouching Network With Conditional Sequential
   Modulation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Modulation; Image enhancement; Task analysis;
   Visualization; Training; Reinforcement learning; Feature modulation;
   image enhancement; multi-layer perceptron; neural networks; photo
   retouching
AB Photo retouching aims at improving the aesthetic visual quality of images that suffer from photographic defects, especially for poor contrast, over/under exposure, and inharmonious saturation. In practice, photo retouching can be accomplished by a series of image processing operations. As most commonly-used retouching operations are pixel-independent, i.e., the manipulation on one pixel is uncorrelated with its neighboring pixels, we can take advantage of this property and design a specialized algorithm for efficient global photo retouching. We analyze these global operations and find that they can be mathematically formulated by a Multi-Layer Perceptron (MLP). Based on this observation, we propose an extremely lightweight framework - Conditional Sequential Retouching Network (CSRNet). Benefiting from the utilization of $1\times 1$ convolution, CSRNet only contains less than 37 K trainable parameters, which are orders of magnitude smaller than existing learning-based methods. Experiments show that our method achieves state-of-the-art performance on the benchmark MIT-Adobe FiveK dataset quantitively and qualitatively. In addition to achieve global photo retouching, the proposed framework can be easily extended to learn local enhancement effects. The extended model, namely CSRNet-L, also achieves competitive results in various local enhancement tasks.
C1 [Liu, Yihao; He, Jingwen; Chen, Xiangyu; Zhang, Zhengwen; Zhao, Hengyuan; Dong, Chao; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Liu, Yihao] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS
RP Dong, C (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM liuyihao14@mails.ucas.ac.cn; jw.he@siat.ac.cn; chxy95@gmail.com;
   zhengwen.zhang02@gmail.com; hy.zhao1@siat.ac.cn; chao.dong@siat.ac.cn;
   yu.qiao@siat.ac.cn
RI Chen, Xiangyu/AEY-9113-2022; Liu, Yihao/KIC-0534-2024
OI Chen, Xiangyu/0000-0003-2156-4959; Liu, Yihao/0000-0001-9874-0602
FU National Natural Science Foundation of China [61906184]; Joint Lab of
   CAS-HK; Shanghai Committee of Science and Technology, China
   [20DZ1100800, 21DZ1100100]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61906184, in part by the Joint Lab of
   CAS-HK, and in part by the Shanghai Committee of Science and Technology,
   China under Grants 20DZ1100800 and 21DZ1100100.
CR Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629645
   Bianco S, 2020, IEEE T IMAGE PROCESS, V29, P6223, DOI 10.1109/TIP.2020.2989584
   Bianco S, 2019, LECT NOTES COMPUT SC, V11418, P209, DOI 10.1007/978-3-030-13940-7_16
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Chai Y, 2020, IEEE WINT CONF APPL, P981, DOI 10.1109/WACV45572.2020.9093321
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Chen JW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982423
   Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng YB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P870, DOI 10.1145/3240508.3240531
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Han-Ul Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P339, DOI 10.1007/978-3-030-58595-2_21
   Han-Ul Kim, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P374, DOI 10.1007/978-3-030-58577-8_23
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Ignatov A, 2018, IEEE COMPUT SOC CONF, P804, DOI 10.1109/CVPRW.2018.00112
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jingwen He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P679, DOI 10.1007/978-3-030-58601-0_40
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kosugi S, 2020, AAAI CONF ARTIF INTE, V34, P11296
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee JY, 2016, PROC CVPR IEEE, P2470, DOI 10.1109/CVPR.2016.271
   Lin M, 2014, Arxiv, DOI [arXiv:1312.4400, DOI 10.48550/ARXIV.1312.4400]
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ni ZK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1697, DOI 10.1145/3394171.3413839
   Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621
   Salimans T, 2016, ADV NEUR IN, V29
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang X., 2018, PROC EUR C COMPUT VI, P1
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Yan JZ, 2014, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2014.382
   Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296
   Ying ZQ, 2017, IEEE INT CONF COMP V, P3015, DOI 10.1109/ICCVW.2017.356
   Yu RS, 2018, ADV NEUR IN, V31
   Yuanming Hu, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3181974
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P2058, DOI 10.1109/TPAMI.2020.3026740
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang Q, 2021, IEEE T MULTIMEDIA, V23, P189, DOI 10.1109/TMM.2020.2982045
   Zhang Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P582, DOI 10.1145/3240508.3240595
   Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 52
TC 2
Z9 2
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4638
EP 4652
DI 10.1109/TMM.2022.3179904
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Luo, KY
   Zhang, C
   Li, HX
   Jia, XY
   Chen, CL
AF Luo, Kaiyi
   Zhang, Chao
   Li, Huaxiong
   Jia, Xiuyi
   Chen, Chunlin
TI Adaptive Marginalized Semantic Hashing for Unpaired Cross-Modal
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; unpaired hashing; adaptive margins
ID CODES
AB In recent years, Cross-Modal Hashing (CMH) has attracted much attention due to its fast query speed and efficient storage. Previous studies have achieved promising results for Cross-Modal Retrieval (CMR) by discovering discriminative hash codes and modality-specific hash functions. Nonetheless, most existing CMR works are subjected to some restrictions: 1) It is assumed that data of different modalities are fully paired, which is impractical in real applications due to sample missing and false data alignment, and 2) binary regression targets including the label matrix and binary codes are too rigid to effectively learn semantic-preserving hash codes and hash functions. To address these problems, this paper proposes an Adaptive Marginalized Semantic Hashing (AMSH) method which not only enhances the discrimination of latent representations and hash codes by adaptive margins, but can also be used for both paired and unpaired CMR. As a two-step method, in the first step, AMSH generates semantic-aware modality-specific latent representations with adaptively marginalized labels, thereby enlarging the distances between different classes, and exploiting the labels to preserve the inter-modal and intra-modal semantic similarities into latent representations and hash codes. In the second step, adaptive margin matrices are embedded into the hash codes, and enlarge the gaps between positive and negative bits, which improves the discrimination and robustness of hash functions. On this basis, AMSH generates similarity-preserving hash codes and robust hash functions without the strict one-to-one data correspondence requirement. Experiments are conducted on several benchmark datasets to demonstrate the superiority and flexibility of AMSH over some state-of-the-art CMR methods.
C1 [Luo, Kaiyi; Zhang, Chao; Li, Huaxiong; Chen, Chunlin] Nanjing Univ, Dept Control Sci & Intelligence Engn, Nanjing 210093, Peoples R China.
   [Jia, Xiuyi] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210014, Peoples R China.
C3 Nanjing University; Nanjing University of Science & Technology
RP Li, HX (corresponding author), Nanjing Univ, Dept Control Sci & Intelligence Engn, Nanjing 210093, Peoples R China.
EM ky_luo@smail.nju.edu.cn; chzhang@smail.nju.edu.cn;
   huaxiongli@nju.edu.cn; jiaxy@njust.edu.cn; clchen@nju.edu.cn
RI Li, Huaxiong/AAR-8881-2020
OI Li, Huaxiong/0000-0003-0395-1525; Zhang, Chao/0000-0002-5929-0853; Chen,
   Chunlin/0000-0003-3929-4707
FU National Natural Science Foundation of China
FX No Statement Available
CR Chen Y, 2022, IEEE T KNOWL DATA EN, V34, P1177, DOI 10.1109/TKDE.2020.2995195
   Chen ZD, 2020, IEEE T CIRC SYST VID, V30, P2262, DOI 10.1109/TCSVT.2019.2911359
   Chen ZD, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1694, DOI 10.1145/3343031.3350862
   Cheng MM, 2020, ACM T INFORM SYST, V38, DOI 10.1145/3389547
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gao J, 2020, NEUROCOMPUTING, V418, P178, DOI 10.1016/j.neucom.2020.08.029
   Guo J, 2020, IEEE T IMAGE PROCESS, V29, P1344, DOI 10.1109/TIP.2019.2941858
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P3877, DOI 10.1109/TPAMI.2022.3177356
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Li HX, 2023, IEEE T KNOWL DATA EN, V35, P1185, DOI 10.1109/TKDE.2021.3102119
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Liu Hong, 2016, IJCAI, P1767, DOI DOI 10.1109/TIP.2016.2564638
   Liu L, 2017, IEEE T IMAGE PROCESS, V26, P107, DOI 10.1109/TIP.2016.2619262
   Liu S, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1379, DOI 10.1145/3397271.3401086
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu X, 2022, IEEE T NEUR NET LEAR, V33, P6306, DOI 10.1109/TNNLS.2021.3076684
   Liu X, 2021, IEEE T PATTERN ANAL, V43, P964, DOI 10.1109/TPAMI.2019.2940446
   Long MS, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P579, DOI 10.1145/2911451.2911493
   Luo X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2518
   Mandal D, 2019, IEEE T IMAGE PROCESS, V28, P102, DOI 10.1109/TIP.2018.2863040
   Nie XS, 2021, IEEE T CIRC SYST VID, V31, P3669, DOI 10.1109/TCSVT.2020.3042972
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Peng YX, 2016, IEEE T CIRC SYST VID, V26, P583, DOI 10.1109/TCSVT.2015.2400779
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rudin W., 1976, PRINCIPLES MATH ANAL
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441
   Shi YF, 2022, IEEE T CIRC SYST VID, V32, P7255, DOI 10.1109/TCSVT.2022.3172716
   Wang D, 2018, IEEE T CIRC SYST VID, V28, P2703, DOI 10.1109/TCSVT.2017.2723302
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang YX, 2022, IEEE T CYBERNETICS, V52, P10064, DOI 10.1109/TCYB.2021.3059886
   Wang YX, 2021, IEEE T KNOWL DATA EN, V33, P3507, DOI 10.1109/TKDE.2020.2974825
   Xiang SM, 2012, IEEE T NEUR NET LEAR, V23, P1738, DOI 10.1109/TNNLS.2012.2212721
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yang EK, 2017, AAAI CONF ARTIF INTE, P1618
   Yu GX, 2022, IEEE T NEUR NET LEAR, V33, P304, DOI 10.1109/TNNLS.2020.3027729
   Zeng ZX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5427, DOI 10.1145/3474085.3475670
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang C, 2023, IEEE T KNOWL DATA EN, V35, P6475, DOI 10.1109/TKDE.2022.3172216
   Zhang C, 2021, IEEE T CIRC SYST VID, V31, P2646, DOI 10.1109/TCSVT.2020.3032964
   Zhang DL, 2023, IEEE T KNOWL DATA EN, V35, P1365, DOI 10.1109/TKDE.2021.3099125
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhang X, 2018, LECT NOTES COMPUT SC, V11219, P614, DOI 10.1007/978-3-030-01267-0_36
   Zheng CQ, 2021, IEEE T MULTIMEDIA, V23, P4079, DOI 10.1109/TMM.2020.3037456
   Zhu L, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3365841
NR 54
TC 11
Z9 12
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9082
EP 9095
DI 10.1109/TMM.2023.3245400
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Luo, ZX
   Wang, ZL
   Hu, M
   Zhou, YP
   Wu, D
AF Luo, Zhenxiao
   Wang, Zelong
   Hu, Miao
   Zhou, Yipeng
   Wu, Di
TI LiveSR: Enabling Universal HD Live Video Streaming With Crowdsourced
   Online Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HD video; imitation learning; live streaming; super-resolution
ID QUALITY ASSESSMENT
AB The high-definition (HD) live video streaming has gained significant popularity due to the rapid growth of 4 G/5 G and social media. However, for devices with constrained bandwidth, they still have no sufficient bandwidth to support HD live video streaming. In this paper, we propose a neural-enhanced HD live video streaming framework called LiveSR to provide universal HD live video streaming for both bandwidth-constrained and bandwidth-rich devices. For bandwidth-constrained devices, LiveSR delivers low-quality video streams and then boosts video quality at the device side with super-resolution (SR) techniques. The difficulty lies in how to train the SR model with low cost and conduct quality enhancement in real time. To address these challenges, we design a crowdsourced online training method by exploiting computation resources and HD video data on bandwidth-rich devices in the same video channel. We also propose an imitation learning-based decision making algorithm to make downloading decisions for video chunks and SR models under limited bandwidth. We implement and evaluate our proposed LiveSR framework using real network traces, and the experiment results show that LiveSR outperforms all the other baseline approaches, with 65.5% improvement in terms of the average QoE and 5.7% in terms of video quality (i.e., PSNR), and the achieved frame rate can be as high as 30 frames per second.
C1 [Luo, Zhenxiao; Wang, Zelong; Hu, Miao; Wu, Di] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Luo, Zhenxiao; Wang, Zelong; Hu, Miao; Wu, Di] Sun Yat Sen Univ, Guangdong Key Lab Big Data Anal & Proc, Guangzhou 510006, Peoples R China.
   [Zhou, Yipeng] Macquarie Univ, Sch Comp, Sydney, NSW 2109, Australia.
   [Zhou, Yipeng] Macquarie Univ, Fac Sci & Engn, Sydney, NSW 2109, Australia.
C3 Sun Yat Sen University; Sun Yat Sen University; Macquarie University;
   Macquarie University
RP Wu, D (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM luozhx6@mail2.sysu.edu.cn; wangzl7@mail2.sysu.edu.cn;
   humiao@outlook.com; yipeng.zhou@mq.edu.au; wudi27@mail.sysu.edu.cn
RI Luo, Zhenxiao/KVB-6380-2024; Wu, Di/HNP-3772-2023
OI Zhou, Yipeng/0000-0003-1533-0865
FU National Natural Science Foundation of China [U1911201, U2001209,
   62072486]; Science and Technology Planning Project of Guangdong Province
   [2021A0505110008]; Science and Technology Program of Guangzhou
   [202007040006, 202103010004]; Australia Research Council [DE180100950]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1911201, U2001209, and 62072486, in
   part by the Science and Technology Planning Project of Guangdong
   Province under Grant 2021A0505110008, in part by the Science and
   Technology Program of Guangzhou under Grants 202007040006 and
   202103010004, and in part by the Australia Research Council under Grant
   DE180100950.& nbsp;
CR Abay A, 2020, Arxiv, DOI [arXiv:2012.02447, 10.48550/arXiv.2012.02447]
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Aiolibs, 2021, AIOHTTP
   Akamai, 2021, AK SEC CLOUD DEL PER
   Amazon, 2021, Amazon Web Services cloud regions
   [Anonymous], 2014, NOSSDAV 14
   AsyncIO, 2021, ASYNCIO
   Baig Ghufran, 2019, P 25 ANN INT C MOB C, P1
   Bilibili, 2021, BILIBILI
   Chen JW, 2020, NOSSDAV '20: PROCEEDINGS OF THE 2020 WORKSHOP ON NETWORK AND OPERATING SYSTEM SUPPORT FOR DIGITAL AUDIO AND VIDEO, P1, DOI 10.1145/3386290.3396929
   Cisco Visual Networking Index, 2020, Cisco visual networking index (vni) global mobile data traffic forecast update, 2017-2022
   Cui LZ, 2021, IEEE T MULTIMEDIA, V23, P651, DOI 10.1109/TMM.2020.2985631
   DASH-IF, 2021, DASH IND FOR
   Do TT, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1467, DOI 10.1109/ICC.2004.1312755
   Douyu, 2021, US
   Facebook, 2021, FAC LIV
   FFmpeg, FFMPEG
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huang TC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1208, DOI 10.1145/3240508.3240545
   ITU, 2016, VID COD H265 STAND
   Jakobsson M, 2003, LECT NOTES COMPUT SC, V2742, P15
   Johnson DH., 2006, SCHOLARPEDIA, V1, P2088, DOI [DOI 10.4249/SCHOLARPEDIA.2088, 10.4249/scholarpedia.2088]
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2020, SIGCOMM '20: PROCEEDINGS OF THE 2020 ANNUAL CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION ON THE APPLICATIONS, TECHNOLOGIES, ARCHITECTURES, AND PROTOCOLS FOR COMPUTER COMMUNICATION, P107, DOI 10.1145/3387514.3405856
   Kingma D. P., 2014, arXiv
   Le Feuvre J, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P249, DOI 10.1145/3339825.3394929
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Limelight, 2019, STAT ONL VID 2019
   Liu W, 2003, GLOB TELECOMM CONF, P3802, DOI 10.1109/GLOCOM.2003.1258943
   Mao H., 2017, PENSIEVE OFFICIAL GI
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   McMahan HB, 2017, PR MACH LEARN RES, V54, P1273
   Microsoft Azure, 2021, MICR AZ CLOUD COMP S
   Mnih V, 2016, PR MACH LEARN RES, V48
   Nasrabadi AT, 2018, PROCEEDINGS OF THE 28TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'18), P31, DOI 10.1145/3210445.3210454
   Oh J, 2018, PR MACH LEARN RES, P3878
   OpenCV, 2021, OPENCV PYTH
   PyTorch, 2021, PyTorch
   Ravi Netravali, 2015, 2015 USENIX ANN TECH, P417
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Sadat MN, 2020, IEEE T MULTIMEDIA, V22, P2321, DOI 10.1109/TMM.2019.2957995
   Salva-Garcia P, 2018, COMPUT COMMUN, V118, P171, DOI 10.1016/j.comcom.2017.11.007
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Twitch TV, 2021, TWITCH TV
   Twitchtracker, 2021, TWITCHTRACKER
   U. S. FCC, 2016, RAW DAT MEAS BROADB
   Vega MT, 2017, IEEE SIGNAL PROC LET, V24, P736, DOI 10.1109/LSP.2017.2691160
   Wamser Florian, 2017, 2017 IFIP/IEEE Symposium on Integrated Network and Service Management (IM), P893, DOI 10.23919/INM.2017.7987400
   Wang F, 2008, IEEE INFOCOM SER, P36
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wikipedia, 2021, WEBSOCKET
   Wu JY, 2016, IEEE T PARALL DISTR, V27, P710, DOI 10.1109/TPDS.2015.2416736
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yeo H, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P645
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   YouTube, 2021, YOUTUBE REC UPL ENC
   YouTube, 2021, YOUT LIV
   Zhang YJ, 2020, IEEE INFOCOM SER, P1957, DOI [10.1109/infocom41043.2020.9155384, 10.1109/INFOCOM41043.2020.9155384]
   Zheng ZB, 2017, IEEE INT CONGR BIG, P557, DOI 10.1109/BigDataCongress.2017.85
NR 60
TC 2
Z9 3
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2788
EP 2798
DI 10.1109/TMM.2022.3151259
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600026
DA 2024-07-18
ER

PT J
AU Ni, SJ
   Shao, F
   Chai, XL
   Chen, HW
   Ho, YS
AF Ni, Shijia
   Shao, Feng
   Chai, Xiongli
   Chen, Hangwei
   Ho, Yo-Sung
TI Composition-Guided Neural Network for Image Cropping Aesthetic
   Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Composition pattern; composition-guided image cropping; feature
   aggregation; image cropping aesthetic assessment; photographic
   composition
AB How to explore the interaction between image aesthetic rules and crops is the key to finding views with good composition. Besides, it is subjective to evaluate candidate crops, which mainly depends on aesthetic knowledge, but it is not an easy task for people without extensive photography experience. However, existing methods mostly find good views by extracting general aesthetic features of crops without fully exploring the aesthetic rules. Motivated by this, we innovatively propose a composition-guided image cropping aesthetic assessment network (CGICAANet) for efficiently finding good crops and optimizing the cropping operation. Specifically, we adopt a direct and comprehensive composition pattern module, which adaptively mines suitable compositions for the images and emphasizes the dominant position of visual elements to contribute to optimizing the best crops in an interpretable way. Moreover, we designed a multi-task loss function to train the model. Particularly, to explore the commonality between predicted crops and labels, the complete intersection-over-union loss is adopted thoroughly considering the overlap area, central point distance and the consistency of aspect ratios for crops concurrently. Therefore, the predicted best crop can preserve the visual elements and have better composition. Experimental results with lightweight MobileNetV2 and ShuffleNetV2 as backbone networks demonstrate that our method can obtain comparable or better performance in terms of efficiency and accuracy.
C1 [Ni, Shijia; Shao, Feng; Chai, Xiongli; Chen, Hangwei] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol, Sch Informat & Commun, Gwangju 500712, South Korea.
C3 Ningbo University; Gwangju Institute of Science & Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM nishijiahhh@163.com; shaofeng@nbu.edu.cn; 747866472@qq.com;
   1010075746@qq.com; hoyo@gist.ac.kr
OI Chen, Hangwei/0000-0002-3756-2029; HO, YO-SUNG/0000-0002-7220-1034;
   Chai, Xiongli/0000-0002-4245-5391
FU Natural Science Foundation of China [62071261]; Zhejiang Natural Science
   Foundation of China [R18F010008]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 62071261, and in part by the Zhejiang Natural Science
   Foundation of China under Grant R18F010008.
CR [Anonymous], 2010, ACM MULTIMEDIA
   Bhattacharya S., 2010, P 18 ACM INT C MULTI, P271
   Bhattacharya S, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037678
   Chen JS, 2016, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2016.61
   Chen YL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P37, DOI 10.1145/3123266.3123274
   Chen YL, 2017, IEEE WINT CONF APPL, P226, DOI 10.1109/WACV.2017.32
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Fang C, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1105, DOI 10.1145/2647868.2654979
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Ghosal K, 2019, IEEE INT CONF COMP V, P4550, DOI 10.1109/ICCVW.2019.00556
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo GJ, 2018, IEEE T MULTIMEDIA, V20, P2073, DOI 10.1109/TMM.2018.2794262
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong CY, 2021, PROC CVPR IEEE, P7053, DOI 10.1109/CVPR46437.2021.00698
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   Kao YY, 2017, INT CONF ACOUST SPEE, P1982, DOI 10.1109/ICASSP.2017.7952503
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li DB, 2020, PROC CVPR IEEE, P4212, DOI 10.1109/CVPR42600.2020.00427
   Li DB, 2018, PROC CVPR IEEE, P8193, DOI 10.1109/CVPR.2018.00855
   Lian TP, 2021, IEEE IMAGE PROC, P2573, DOI 10.1109/ICIP42928.2021.9506666
   Liu D, 2020, IEEE WINT CONF APPL, P3558, DOI [10.1109/WACV45572.2020.9093412, 10.1109/wacv45572.2020.9093412]
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Lu P, 2019, Arxiv, DOI arXiv:1907.01432
   Lu P, 2021, IEEE T MULTIMEDIA, V23, P3618, DOI 10.1109/TMM.2020.3029882
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Obrador P, 2010, IEEE IMAGE PROC, P3185, DOI 10.1109/ICIP.2010.5654231
   Pan ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4198, DOI 10.1109/ICCV48922.2021.00418
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stentiford F., 2007, P WORKSH COMP ATT AP
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tu Y, 2020, AAAI CONF ARTIF INTE, V34, P12104
   Wang LJ, 2019, IEEE COMPUT SOC CONF, P1833, DOI 10.1109/CVPRW.2019.00234
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2017, IEEE I CONF COMP VIS, P2205, DOI 10.1109/ICCV.2017.240
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570
   Xu YF, 2021, EXPERT SYST APPL, V171, DOI 10.1016/j.eswa.2021.114596
   Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P1304, DOI 10.1109/TPAMI.2020.3024207
   Zhang B., 2021, P BRIT MACH VIS C, P1
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
   Zhang MJ, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P438
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
NR 59
TC 1
Z9 1
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6836
EP 6851
DI 10.1109/TMM.2022.3215003
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000010
DA 2024-07-18
ER

PT J
AU Song, TS
   Li, LD
   Wu, JJ
   Yang, YZ
   Li, YQ
   Guo, YD
   Shi, GM
AF Song, Tianshu
   Li, Leida
   Wu, Jinjian
   Yang, Yuzhe
   Li, Yaqian
   Guo, Yandong
   Shi, Guangming
TI Knowledge-Guided Blind Image Quality Assessment With Few Training
   Samples
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Distortion; Training; Measurement; Image quality;
   Predictive models; Knowledge representation; Image quality assessment;
   knowledge embedding; human visual system; natural scene statistics;
   generalization
AB Blind image quality assessment (BIQA) for in-the-wild images has achieved great progress by training advanced deep neural networks. However, the current BIQA models are suffering the generalization challenge, meaning that a well-trained BIQA model is still very limited in evaluating images with different distributions. Deep BIQA models are data-intensive, but the annotation of image quality labels is extremely expensive. To design a generalizable BIQA model with few training samples is highly desired. Motivated by the above fact, this paper presents a knowledge-guided BIQA (KG-IQA) framework by integrating domain knowledge from the human visual system (HVS) and natural scene statistics (NSS). Specifically, the quality-aware HVS and NSS features are first extracted as prior knowledge. Then, we embed the two types of knowledge into the conventional deep neural network by learning to predict the HVS and NSS features, producing the knowledge-enhanced quality features, based on which the final image quality score is obtained. We conduct extensive experiments and comparisons on five authentically distorted IQA datasets. The experimental results demonstrate that the introduction of knowledge greatly reduces the requirement on the amount of training images, and the proposed KG-IQA model achieves superior performance in terms of both prediction accuracy and generalization ability.
C1 [Song, Tianshu] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
   [Li, Leida; Wu, Jinjian; Shi, Guangming] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Yang, Yuzhe; Li, Yaqian; Guo, Yandong] OPPO Res Inst, Shanghai 200032, Peoples R China.
C3 China University of Mining & Technology; Xidian University
RP Li, LD (corresponding author), Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
EM tianshusong@cumt.edu.cn; reader1104@hotmail.com;
   jinjian.wu@mail.xidian.edu.cn; ippllewis@gmail.com; liyaqian@oppo.com;
   yandong.guo@live.com; gmshi@xidian.edu.cn
OI Yang, Yuzhe/0000-0001-9098-2105
FU National Natural Science Foundation of China
FX No Statement Available
CR Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Bull DR, 2014, COMMUNICATING PICTURES: A COURSE IN IMAGE AND VIDEO CODING, P17
   Caruana R, 1997, ADV NEUR IN, V9, P389
   Chaudhary C, 2020, IEEE T MULTIMEDIA, V22, P897, DOI 10.1109/TMM.2019.2937181
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen WL, 2021, IEEE T MULTIMEDIA, V23, P1008, DOI 10.1109/TMM.2020.2991546
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Jiang QP, 2022, IEEE T IMAGE PROCESS, V31, P3697, DOI 10.1109/TIP.2022.3174398
   Jiang QP, 2022, IEEE T IMAGE PROCESS, V31, P2279, DOI 10.1109/TIP.2022.3154588
   Jiang QP, 2022, IEEE T BROADCAST, V68, P43, DOI 10.1109/TBC.2021.3113280
   Jiang QP, 2019, IEEE T CIRC SYST VID, V29, P323, DOI 10.1109/TCSVT.2017.2783938
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Li LD, 2021, IEEE T MULTIMEDIA, V23, P320, DOI 10.1109/TMM.2020.2980185
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu JZ, 2023, IEEE T MULTIMEDIA, V25, P5358, DOI 10.1109/TMM.2022.3190700
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma R, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5248, DOI 10.1145/3474085.3475642
   Madhusudana PC, 2022, IEEE T IMAGE PROCESS, V31, P4149, DOI 10.1109/TIP.2022.3181496
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Peng ZY, 2022, IEEE T CIRC SYST VID, V32, P3422, DOI 10.1109/TCSVT.2021.3112933
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Tao CX, 2022, Arxiv, DOI arXiv:2206.01204
   Taubman DS, 2002, P IEEE, V90, P1336, DOI 10.1109/JPROC.2002.800725
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Varga D, 2018, IEEE INT CON MULTI
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Wang ZJ, 2011, IEEE SIGNAL PROC MAG, V28, P2, DOI 10.1109/MSP.2011.940297
   Wang Z, 2011, IEEE SIGNAL PROC MAG, V28, P137, DOI 10.1109/MSP.2011.942295
   Wei C, 2022, PROC CVPR IEEE, P14648, DOI 10.1109/CVPR52688.2022.01426
   Wu JJ, 2012, J VIS COMMUN IMAGE R, V23, P845, DOI 10.1016/j.jvcir.2012.04.010
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhang WX, 2023, IEEE T PATTERN ANAL, V45, P2864, DOI 10.1109/TPAMI.2022.3178874
   Zhang WX, 2023, Arxiv, DOI arXiv:2107.13429
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zheng LR, 2019, IEEE T MULTIMEDIA, V21, P2057, DOI 10.1109/TMM.2019.2894939
NR 57
TC 5
Z9 5
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8145
EP 8156
DI 10.1109/TMM.2022.3233244
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000066
DA 2024-07-18
ER

PT J
AU Song, YJ
   Liu, Z
   Li, GY
   Zeng, D
   Zhang, TH
   Xu, LH
   Wang, JJ
AF Song, Yingjie
   Liu, Zhi
   Li, Gongyang
   Zeng, Dan
   Zhang, Tianhong
   Xu, Lihua
   Wang, Jijun
TI RINet: Relative Importance-Aware Network for Fixation Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fixation prediction; relative importance; self-attention mechanism;
   complexity-relevant focal loss
ID ENCODER-DECODER NETWORK; VISUAL-ATTENTION; SALIENCY; QUALITY; MODEL
AB Fixation prediction aims to simulate human visual selection mechanism and estimate the visual saliency degree of regions in a scene. In semantically rich scenes, there are generally multiple salient regions. This condition requires a fixation prediction model to understand the relative importance relationship of multiple salient regions, that is, to identify which region is more important. In practice, existing fixation prediction models implicitly explore the relative importance relationship in the end-to-end training process while they do not work well. In this article, we propose a novel Relative Importance-aware Network (RINet) to explicitly explore the modeling of relative importance in fixation prediction. RINet perceives multi-scale local and global relative importance through the Hierarchical Relative Importance Enhancement (HRIE) module. Within a single scale subspace, on the one hand, HRIE module regards the similarity matrix as the local relative importance map to weight the input feature. On the other hand, HRIE module integrates a set of local relative importance maps into one map, defined as the global relative importance map, to grasp global relative importance. Moreover, we propose a Complexity-Relevant Focal (CRF) loss for network training. As such, we can progressively emphasize learning difficult samples for better handling the complicated scenarios, further improving the performance. The ablation studies confirm the contributions of key components of our RINet, and extensive experiments on five datasets demonstrate our RINet is superior to 28 relevant state-of-the-art models.
C1 [Song, Yingjie; Liu, Zhi; Li, Gongyang; Zeng, Dan] Shanghai Univ, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shanghai Inst Adv Commun & Data Sci, Key Labora tory Specialty Fiber Opt & Opt Access N, Shanghai, Peoples R China.
   [Song, Yingjie; Liu, Zhi; Li, Gongyang; Zeng, Dan] Shanghai Univ, Sch Commun & Informat Engn, Joint Int Res Lab Specialty Fiber Opt & Adv Commu, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai 200444, Peoples R China.
   [Li, Gongyang] Shanghai Jiao Tong Univ, Sch Med, Shanghai Mental Hlth Ctr, Shanghai Key Lab Psychot Disorders, Shanghai 200030, Peoples R China.
C3 Shanghai University; Shanghai University; Shanghai Jiao Tong University
RP Liu, Z (corresponding author), Shanghai Univ, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shanghai Inst Adv Commun & Data Sci, Key Labora tory Specialty Fiber Opt & Opt Access N, Shanghai, Peoples R China.; Liu, Z (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Joint Int Res Lab Specialty Fiber Opt & Adv Commu, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai 200444, Peoples R China.
EM jie0222@shu.edu.cn; liuzhisjtu@163.com; ligongyang@shu.edu.cn;
   dzeng@shu.edu.cn; zhang_tianhong@126.com; dr_xulihua@163.com;
   dr_wangjijun@126.com
RI Li, Gongyang/IXD-9078-2023; Xu, Lihua/P-8054-2019; LIU, Zhi/D-4518-2012
OI LIU, Zhi/0000-0002-8428-1131; , Gongyang Li/0000-0001-7324-1196
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2008, P INT C NEUR INF PRO
   [Anonymous], 2005, Advances in neural information processing systems, DOI DOI 10.5555/2976248.2976268
   Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Bian P, 2009, LECT NOTES COMPUT SC, V5506, P251, DOI 10.1007/978-3-642-02490-0_31
   Borji A, 2021, IEEE T PATTERN ANAL, V43, P679, DOI 10.1109/TPAMI.2019.2935715
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Che ZH, 2020, IEEE T IMAGE PROCESS, V29, P2287, DOI 10.1109/TIP.2019.2945857
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   Ding GQ, 2022, IMAGE VISION COMPUT, V120, DOI 10.1016/j.imavis.2022.104395
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785
   Fang CW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1584, DOI 10.1145/3503161.3547804
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao D., 2005, P ADV NEUR INF PROC, P481
   Gao DS, 2009, IEEE T PATTERN ANAL, V31, P989, DOI 10.1109/TPAMI.2009.27
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Guo M.-H., 2021, arXiv
   Han JW, 2018, PROC CVPR IEEE, P9080, DOI 10.1109/CVPR.2018.00946
   Han JW, 2018, IEEE T CIRC SYST VID, V28, P2473, DOI 10.1109/TCSVT.2017.2706264
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hastie T., 2009, The Elements of Statistical Learning
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Hu FY, 2021, INT C PATT RECOG, P9054, DOI 10.1109/ICPR48806.2021.9413057
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang NAC, 2022, IEEE T MULTIMEDIA, V24, P1651, DOI 10.1109/TMM.2021.3069297
   Huang PL, 2022, IEEE-CAA J AUTOMATIC, V9, P339, DOI 10.1109/JAS.2021.1004210
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2009, Advances in neural information processing systems, V49, P1295, DOI [10.1016/j.visres.2008.09.007, DOI 10.1016/J.VISRES.2008.09.007]
   Jia S, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103887
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Judd T., 2012, A benchmark of computational models of saliency to predict human fixations
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kienzle W, 2009, J VISION, V9, DOI 10.1167/9.5.7
   Kim H, 2015, IEEE T MULTIMEDIA, V17, P2198, DOI 10.1109/TMM.2015.2493367
   Kingma D. P., 2014, arXiv
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kroner A, 2020, NEURAL NETWORKS, V129, P261, DOI 10.1016/j.neunet.2020.05.004
   Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620
   Kümmerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513
   Kummerer M, 2015, Arxiv, DOI [arXiv:1411.1045, 10.48550/arXiv.1411.1045]
   Le Meur O, 2006, IEEE T PATTERN ANAL, V28, P802, DOI 10.1109/TPAMI.2006.86
   Li B, 2022, Arxiv, DOI arXiv:2201.02593
   Li GY, 2023, IEEE T CYBERNETICS, V53, P526, DOI 10.1109/TCYB.2022.3162945
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P1461, DOI 10.1109/TIP.2020.3044440
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li GY, 2019, NEUROCOMPUTING, V368, P180, DOI 10.1016/j.neucom.2019.08.051
   Li HC, 2018, Arxiv, DOI [arXiv:1805.10180, DOI 10.48550/ARXIV.1805.10180]
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002, DOI DOI 10.48550/ARXIV.2006.04388
   Li X, 2021, PROC CVPR IEEE, P11627, DOI 10.1109/CVPR46437.2021.01146
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Linardos A., 2021, P IEEECVF INT C COMP, P12919
   Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047
   Liu N, 2015, PROC CVPR IEEE, P362, DOI 10.1109/CVPR.2015.7298633
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lou JX, 2022, NEUROCOMPUTING, V494, P455, DOI 10.1016/j.neucom.2022.04.080
   Murray N, 2011, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2011.5995506
   Pan JT, 2018, Arxiv, DOI arXiv:1701.01081
   Park J, 2018, Arxiv, DOI [arXiv:1807.06514, 10.48550/arXiv.1807.06514, DOI 10.48550/ARXIV.1807.06514]
   Paszke A, 2019, ADV NEUR IN, V32
   Qi F, 2019, IEEE ACCESS, V7, P60428, DOI 10.1109/ACCESS.2019.2915630
   Qing CM, 2022, DIGIT SIGNAL PROCESS, V120, DOI 10.1016/j.dsp.2021.103289
   Reddy N, 2020, IEEE INT C INT ROBOT, P10241, DOI 10.1109/IROS45743.2020.9341574
   Ridnik T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P82, DOI 10.1109/ICCV48922.2021.00015
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Souza LS, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107028
   Tliba M, 2022, IEEE ACCESS, V10, P20701, DOI 10.1109/ACCESS.2022.3152189
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang ZQ, 2023, IEEE T MULTIMEDIA, V25, P1161, DOI 10.1109/TMM.2021.3139743
   Wang ZQ, 2021, IMAGE VISION COMPUT, V109, DOI 10.1016/j.imavis.2021.104149
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yang Y, 2022, LECT NOTES COMPUT SC, V12962, P441, DOI 10.1007/978-3-031-08999-2_38
   Zhang DW, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107562
   Zhang DW, 2020, IEEE T IMAGE PROCESS, V29, P9032, DOI 10.1109/TIP.2020.3023609
   Zhang DW, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3158674
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang Q, 2020, IEEE T IMAGE PROCESS, V29, P3321, DOI 10.1109/TIP.2019.2959253
NR 97
TC 2
Z9 2
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9263
EP 9277
DI 10.1109/TMM.2023.3249481
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300013
DA 2024-07-18
ER

PT J
AU Wang, QT
   Fu, B
   Li, M
   He, JJ
   Peng, X
   Qiao, Y
AF Wang, Qitong
   Fu, Bin
   Li, Ming
   He, Junjun
   Peng, Xi
   Qiao, Yu
TI Region-Aware Arbitrary-Shaped Text Detection With Progressive Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scene text detection; scene understanding; deep learning
AB Segmentation-based text detectors are flexible to capture arbitrary-shaped text regions. Due to large geometry variance, it is necessary to construct effective and robust representations to identify text regions with various shapes and scales. In this paper, we focus on designing effective multi-scale contextual features for locating text instances. Specially, we develop a Region Context Module (RCM) to summarize the semantic response and adaptively extract text-region-aware information in a limited local area. To construct complementary multi-scale contextual representations, multiple RCM branches with different scales are employed and integrated via Progressive Fusion Module (PFM). Our proposed RCM and PFM serve as the plug-and-play modules which can be incorporated into existing scene text detection platforms to further boost detection performance. Extensive experiments show that our methods achieve state-of-the-art performances on Total-Text, SCUT-CTW1500 and MSRA-TD500 datasets. The code with models will become publicly available at https://github.com/wqtwjt1996/RP-Text.
C1 [Wang, Qitong] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Wang, Qitong; Peng, Xi] Univ Delaware, Dept Comp & Informat Sci, Newark, DE 19716 USA.
   [Fu, Bin; Li, Ming; He, Junjun; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Hong Kong Macao Joint Lab Human Machine, Shenzhen 518055, Peoples R China.
   [Qiao, Yu] Shanghai AI Lab, Shanghai 200031, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; University of Delaware; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS; Shanghai Artificial Intelligence
   Laboratory
RP Qiao, Y (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Hong Kong Macao Joint Lab Human Machine, Shenzhen 518055, Peoples R China.; Qiao, Y (corresponding author), Shanghai AI Lab, Shanghai 200031, Peoples R China.
EM wqtwjt@udel.edu; bin.fu@siat.ac.cn; ming.li3@siat.ac.cn;
   hejunjun@sjtu.edu.cn; xipeng@udel.edu; yu.qiao@siat.ac.cn
RI Wang, Qitong/JWH-3687-2024
OI Fu, Bin/0000-0002-1526-8654; He, Junjun/0000-0002-1813-1784
FU Joint Laboratory of CAS-HK; Shenzhen Research Program
   [JSGG20191129141212311, RCJC20200714114557087]; Shanghai Committee of
   Science and Technology [21DZ1100100]
FX This work was supported in part by the Joint Laboratory of CAS-HK, in
   part by the Shenzhen Research Program under Grants JSGG20191129141212311
   and RCJC20200714114557087, in part by the Shanghai Committee of Science
   and Technology under Grant 21DZ1100100.
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Ch'ng CK, 2020, INT J DOC ANAL RECOG, V23, P31, DOI 10.1007/s10032-019-00334-z
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chung Junyoung, 2014, ARXIV14123555
   Dauphin YN, 2017, PR MACH LEARN RES, V70
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Fu J, 2019, IEEE I CONF COMP VIS, P6747, DOI 10.1109/ICCV.2019.00685
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Li H., 2018, P BRIT MACH VIS C
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W., 2015, ARXIV150604579
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Liu YL, 2017, PROC CVPR IEEE, P3454, DOI 10.1109/CVPR.2017.368
   Liu ZC, 2019, PROC CVPR IEEE, P7261, DOI 10.1109/CVPR.2019.00744
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Wang Hao, 2019, arXiv
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661
   Wolf C, 2003, PATTERN ANAL APPL, V6, P309, DOI 10.1007/s10044-003-0197-7
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang Shi-Xue, 2020, IEEE C COMPUTER VISI, P9699
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
NR 48
TC 0
Z9 0
U1 7
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4718
EP 4729
DI 10.1109/TMM.2022.3181448
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300009
DA 2024-07-18
ER

PT J
AU Wang, W
   Gao, JY
   Yang, XS
   Xu, CS
AF Wang, Wei
   Gao, Junyu
   Yang, Xiaoshan
   Xu, Changsheng
TI Many Hands Make Light Work: Transferring Knowledge From Auxiliary Tasks
   for Video-Text Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Auxiliary tasks; graph neural networks; knowledge transfer; video-text
   retrieval
ID IMAGE; NETWORK
AB The problem of video-text retrieval, which searches videos via natural language descriptions or vice versa, has attracted growing attention due to the explosive scale of videos produced every day. The dominant approaches for this problem follow the pipeline that firstly learns compact feature representations of videos and texts, and then jointly embeds them into a common feature space where matched video-text pairs are close and unmatched pairs are far away. However, most of them neither consider the structural similarities among cross-modal samples in a global view, nor leverage useful information from other relevant retrieval processes. We argue that both information has great potential for video-text retrieval. In this paper, we treat the relevant retrieval processes as auxiliary tasks and we extract useful knowledge from them by exploiting structural similarities via Graph Neural Networks (GNNs). We then progressively transfer the knowledge from auxiliary tasks in a general-to-specific manner to assist the main task of the current retrieval process. Specifically, for the retrieval of the given query, we first construct a sequence of query-graphs whose central queries are chosen from distant to close to the given query. Then we conduct knowledge-guided message passing in each query-graph to exploit regional structural similarities and gather knowledge of different levels from the updated query-graphs with a knowledge-based attention mechanism. Finally, we transfer the extracted useful knowledge from general to specific to assist the current retrieval process. Extensive experimental results show that our model outperforms the state-of-the-arts on four benchmarks.
C1 [Wang, Wei; Gao, Junyu; Yang, Xiaoshan; Xu, Changsheng] Chinese Acad Sci, Inst Automation, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Wang, Wei; Gao, Junyu; Yang, Xiaoshan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Wang, Wei; Gao, Junyu; Yang, Xiaoshan; Xu, Changsheng] PengCheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automation, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.; Xu, CS (corresponding author), PengCheng Lab, Shenzhen 518066, Peoples R China.
EM wangwei2018@ia.ac.cn; gaojunyu2015@ia.ac.cn;
   xiaoshan.yang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI Gao, Junyu/HDO-5516-2022; xu, cj/HJZ-3488-2023
OI Gao, Junyu/0000-0002-8105-5497; xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Plan of China [2020AAA0106200];
   National Natural Science Foundation of China [62036012, 61721004,
   62102415, 62072286, 61720106006, 61832002, 62072455, 62002355, U1836220,
   U1705262]; Key Research Program of Frontier Sciences of CAS
   [QYZDJSSWJSC039]; Beijing Natural Science Foundation [L201001]; Open
   Research Projects of Zhejiang Lab [2022RC0AB02]; CCF-Hikvision Open Fund
FX This work was supported in part by the National Key Research and
   Development Plan of China under Grant 2020AAA0106200, in part by the
   National Natural Science Foundation of China under Grants 62036012,
   61721004, 62102415, 62072286, 61720106006, 61832002, 62072455, 62002355,
   U1836220, and U1705262, in part by the Key Research Program of Frontier
   Sciences of CAS under Grant QYZDJSSWJSC039, in part by the Beijing
   Natural Science Foundation under Grant L201001, in part by the Open
   Research Projects of Zhejiang Lab under Grant 2022RC0AB02, and in part
   by CCF-Hikvision Open Fund.
CR [Anonymous], 2014, PROC 2 INT C LEARN R
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Bai JL, 2019, IEEE T MULTIMEDIA, V21, P3178, DOI 10.1109/TMM.2019.2920601
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Beck D, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P273
   Chang C, 2019, PROC CVPR IEEE, P9415, DOI 10.1109/CVPR.2019.00965
   Chaudhuri U, 2019, COMPUT VIS IMAGE UND, V184, P22, DOI 10.1016/j.cviu.2019.04.004
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1749, DOI 10.1145/3343031.3351055
   Chun S, 2021, PROC CVPR IEEE, P8411, DOI 10.1109/CVPR46437.2021.00831
   Croitoru I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11563, DOI 10.1109/ICCV48922.2021.01138
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong JF, 2022, IEEE T PATTERN ANAL, V44, P4065, DOI 10.1109/TPAMI.2021.3059295
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Faghri F., 2018, PROC BRIT MACH VIS C
   Frome A., 2013, P ADV NEUR INF PROC
   Gabeur Valentin, 2020, P EUR C COMP VIS, V12349, P214, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Gao JY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P690, DOI 10.1145/3240508.3240566
   Garcia Victor., 2017, 6 INT C LEARN REPR
   Gong LY, 2019, PROC CVPR IEEE, P9203, DOI 10.1109/CVPR.2019.00943
   Gori M, 2005, IEEE IJCNN, P729
   Hu LM, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4821
   Kamra N, 2018, Arxiv, DOI arXiv:1710.10368
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010
   Kingma D. P., 2014, arXiv
   Kipf T, 2018, PR MACH LEARN RES, V80
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Lesort T, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8852042
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li XR, 2021, IEEE T MULTIMEDIA, V23, P4351, DOI 10.1109/TMM.2020.3042067
   Li XR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1786, DOI 10.1145/3343031.3350906
   Li Y., 2020, P IEEE CVF C COMP VI, P12786
   Li Y., 2016, P 4 INT C LEARNING R
   Li YC, 2016, PROC CVPR IEEE, P4641, DOI 10.1109/CVPR.2016.502
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11895, DOI 10.1109/ICCV48922.2021.01170
   Liu Y., 2019, PROC BRIT MACH VIS C
   Long M., 2015, arXiv, DOI DOI 10.48550/ARXIV.1506.02117
   Lu YX, 2017, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2017.126
   Luo JY, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P2309, DOI 10.1145/3357384.3358104
   Luo YD, 2020, AAAI CONF ARTIF INTE, V34, P5021
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Magliani F, 2019, LECT NOTES COMPUT SC, V11752, P537, DOI 10.1007/978-3-030-30645-8_49
   Miech A, 2020, Arxiv, DOI arXiv:1804.02516
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Mithun NC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1856, DOI 10.1145/3240508.3240712
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Otani Mayu, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P651, DOI 10.1007/978-3-319-46604-0_46
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Patrick M., 2020, PROC INT C LEARN REP
   Radford A., 2019, LANGUAGE MODELS ARE
   Raffel C, 2020, J MACH LEARN RES, V21
   Rei M, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P2121, DOI 10.18653/v1/P17-1194
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Seff A., 2017, arXiv
   Shao D, 2018, LECT NOTES COMPUT SC, V11213, P202, DOI 10.1007/978-3-030-01240-3_13
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10635, DOI 10.1109/CVPR42600.2020.01065
   Socher R, 2010, PROC CVPR IEEE, P966, DOI 10.1109/CVPR.2010.5540112
   Sogaard A, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2016), VOL 2, P231
   Sohn K, 2016, ADV NEUR IN, V29
   Song JK, 2020, INT J COMPUT VISION, V128, P2243, DOI 10.1007/s11263-020-01305-2
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Torabi A, 2016, Arxiv, DOI arXiv:1609.08124
   Vaswani A, 2017, ADV NEUR IN, V30
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang W, 2021, IEEE T MULTIMEDIA, V23, P2386, DOI 10.1109/TMM.2020.3011288
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang XH, 2021, PROC CVPR IEEE, P5075, DOI 10.1109/CVPR46437.2021.00504
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang X, 2019, IEEE I CONF COMP VIS, P4580, DOI 10.1109/ICCV.2019.00468
   Wang XS, 2019, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR.2019.00535
   Wray M, 2019, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2019.00054
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Xu RQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P982
   Yang X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1339, DOI 10.1145/3397271.3401151
   Yu Jianfei, 2016, P C EMP METH NAT LAN
   Yu J, 2018, LECT NOTES COMPUT SC, V11164, P223, DOI 10.1007/978-3-030-00776-8_21
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Zenke F, 2017, PR MACH LEARN RES, V70
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang J, 2020, IEEE T CYBERNETICS, V50, P489, DOI 10.1109/TCYB.2018.2868826
   Zhang J, 2018, AAAI CONF ARTIF INTE, P539
   Zhang X, 2018, LECT NOTES COMPUT SC, V11219, P614, DOI 10.1007/978-3-030-01267-0_36
   Zhao MM, 2019, PROC CVPR IEEE, P12728, DOI 10.1109/CVPR.2019.01302
NR 94
TC 7
Z9 7
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2661
EP 2674
DI 10.1109/TMM.2022.3149716
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600016
DA 2024-07-18
ER

PT J
AU Xiang, JJ
   Jiang, GY
   Yu, M
   Jiang, ZD
   Ho, YS
AF Xiang, Jianjun
   Jiang, Gangyi
   Yu, Mei
   Jiang, Zhidi
   Ho, Yo-Sung
TI No-Reference Light Field Image Quality Assessment Using Four-Dimensional
   Sparse Transform
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Image coding; Frequency-domain analysis; Tensors;
   Principal component analysis; Periodic structures; Information filters;
   Light field image quality assessment; no-reference; 4D discrete cosine
   transform; sub-aperture gradient image array; spatial-angular quality
AB Light field imaging can simultaneously capture the intensity and direction information of light rays in the real world. Light field image (LFI) with four-dimensional (4D) data suffers from quality degradation in the process of compression, reconstruction and processing. How to evaluate the visual quality of LFI is thought-provoking. This paper proposes a no-reference LFI quality assessment metric based on high-dimensional sparse transform. Firstly, LFI's sub-aperture gradient image array (SAGIA), which is still a 4D signal, is generated by high-pass filtering between adjacent SAIs. Then, SAGIA is transformed with 4D discrete cosine transform (4D-DCT). 4D-DCT coefficients of SAGIA can characterize the angular and spatial information of LFI. And the logarithmic amplitudes of the coefficients at the same position of SAGIA?s transformed 4D blocks are averaged as the coefficient energy. Subsequently, the 4D-DCT coefficients of SAGIA are divided into the spatial-angular frequency bands and spatial-angular orientation bands, and the corresponding energy features are extracted by converging the coefficient energy of the same band. In addition, the coefficients' amplitudes at the same position of blocks are fitted by the Weibull distribution. Then, the fitted parameters of each position are concatenated, and cropped with principal component analysis to obtain the compact features. Finally, the extracted features are pooled to predict the visual quality of the distorted LFIs. The experimental results demonstrate that the proposed method is more consistent with the subjective evaluation on three LFI databases, compared with the state-of-the-art image quality assessment methods and LFI quality assessment methods.
C1 [Xiang, Jianjun; Jiang, Gangyi; Yu, Mei; Jiang, Zhidi] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol, Sch Elect Engn & Comp Sci, Gwangju 61005, South Korea.
C3 Ningbo University; Gwangju Institute of Science & Technology (GIST)
RP Jiang, GY; Yu, M (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
EM 1811082122@nbu.edu.cn; jianggangyi@nbu.edu.cn; yumei2@126.com;
   jiangzhidi@nbu.edu.cn; hoyo@gist.ac.kr
RI jiang, gang/KII-8233-2024
OI HO, YO-SUNG/0000-0002-7220-1034
FU National Natural Science Foundation of China [62071266, U1301257,
   61671258, 61931022]; K.C. Wong Magna Fund of Ningbo University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62071266, 61871247, U1301257, 61671258
   and 61931022, and in part sponsored by the K.C. Wong Magna Fund of
   Ningbo University.
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   Adhikarla VK, 2017, PROC CVPR IEEE, P3720, DOI 10.1109/CVPR.2017.396
   Alves G, 2018, IEEE IMAGE PROC, P1148, DOI 10.1109/ICIP.2018.8451583
   [Anonymous], 2019, 2179422019 ISOIEC CD
   Brites C, 2021, IEEE T CIRC SYST VID, V31, P339, DOI 10.1109/TCSVT.2020.2976784
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   de Carvalho MB, 2018, IEEE IMAGE PROC, P435, DOI 10.1109/ICIP.2018.8451684
   Alves GD, 2020, IEEE ACCESS, V8, P170807, DOI 10.1109/ACCESS.2020.3024844
   Ghassab VK, 2020, IEEE T MULTIMEDIA, V22, P1447, DOI 10.1109/TMM.2019.2946094
   Gu K, 2020, IEEE T BROADCAST, V66, P127, DOI 10.1109/TBC.2019.2906768
   Gunlu G., 2008, PROC INT C SIGNAL PR, P1
   Hahne C, 2018, IEEE T IND ELECTRON, V65, P9757, DOI 10.1109/TIE.2018.2818644
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Huang ZJ, 2019, INT SYM NETWO COMP, DOI 10.1109/isncc.2019.8909170
   Jung H, 2020, IEEE T MULTIMEDIA, V22, P980, DOI 10.1109/TMM.2019.2934819
   Le Pendu M, 2018, IEEE T IMAGE PROCESS, V27, P1981, DOI 10.1109/TIP.2018.2791864
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P3329, DOI 10.1109/TIP.2016.2568752
   Liu DY, 2020, IEEE T MULTIMEDIA, V22, P846, DOI 10.1109/TMM.2019.2934426
   Liu LX, 2017, SIGNAL PROCESS-IMAGE, V58, P287, DOI 10.1016/j.image.2017.08.011
   Meng CL, 2020, IEEE SIGNAL PROC LET, V27, P525, DOI 10.1109/LSP.2020.2982060
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Miyagi Y, 2015, IEEE IMAGE PROC, P502, DOI 10.1109/ICIP.2015.7350849
   Narwaria M, 2012, IEEE T SYST MAN CY B, V42, P347, DOI 10.1109/TSMCB.2011.2163391
   Paudyal P, 2019, IEEE T BROADCAST, V65, P152, DOI 10.1109/TBC.2019.2892092
   Paudyal P, 2017, IEEE T BROADCAST, V63, P507, DOI 10.1109/TBC.2017.2704430
   Rerabek M., 2016, P 8 INT C QUAL MULT
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sawant S, 2011, IEEE T BROADCAST, V57, P765, DOI 10.1109/TBC.2011.2165237
   Scholte HS, 2009, J VISION, V9, DOI 10.1167/9.4.29
   Shan L, 2019, IEEE ACCESS, V7, P127217, DOI 10.1109/ACCESS.2019.2940093
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shi LK, 2020, IEEE T CIRC SYST VID, V30, P4114, DOI 10.1109/TCSVT.2019.2955011
   Shi LK, 2019, IEEE IMAGE PROC, P3781, DOI [10.1109/icip.2019.8803559, 10.1109/ICIP.2019.8803559]
   Shi LK, 2018, IEEE IMAGE PROC, P41, DOI 10.1109/ICIP.2018.8451077
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Tian Y, 2020, IEEE T IMAGE PROCESS, V29, P7945, DOI 10.1109/TIP.2020.3008856
   Tian Y, 2018, J VIS COMMUN IMAGE R, V57, P212, DOI 10.1016/j.jvcir.2018.11.005
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Wang GC, 2020, IEEE T IMAGE PROCESS, V29, P1802, DOI 10.1109/TIP.2019.2945675
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wanner S., 2013, INT S VIS MOD VIS, P225
   Wu GC, 2019, IEEE T IMAGE PROCESS, V28, P3261, DOI 10.1109/TIP.2019.2895463
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Xiang J., 2020, P IEEE INT C MULT EX, P1
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zheng LR, 2019, IEEE T MULTIMEDIA, V21, P2057, DOI 10.1109/TMM.2019.2894939
   Zhou W, 2020, IEEE T IMAGE PROCESS, V29, P4070, DOI 10.1109/TIP.2020.2969777
   Zhu KF, 2015, IEEE T CIRC SYST VID, V25, P533, DOI 10.1109/TCSVT.2014.2363737
NR 52
TC 10
Z9 11
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 457
EP 472
DI 10.1109/TMM.2021.3127398
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800010
DA 2024-07-18
ER

PT J
AU Xiao, J
   Xu, KM
   Hu, MS
   Liao, L
   Wang, Z
   Lin, CW
   Wang, M
   Satoh, S
AF Xiao, Jing
   Xu, Kangmin
   Hu, Mengshun
   Liao, Liang
   Wang, Zheng
   Lin, Chia-Wen
   Wang, Mi
   Satoh, Shin'ichi
TI Progressive Motion Boosting for Video Frame Interpolation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Frame interpolation; motion estimation; multi-scale framework;
   progressive boosting
AB Video frame interpolation has made great progress in estimating advanced optical flow and synthesizing in-between frames sequentially. However, frame interpolation involving various resolutions and motions remains challenging due to limited or fixed pre-trained networks. Inspired by the success of the coarse-to-fine scheme for video frame interpolation, i.e., gradually interpolating frames of different resolutions, we propose a progressive boosting network (ProBoost-Net) based on a multi-scale framework to achieve flexible recurrent scales and then gradually optimize optical flow estimation and frame interpolation. Specifically, we designed a dense motion boosting (DMB) module to transfer features close to real motion to the decoded features from the later scales, which provides complementary information to refine the motion further. Furthermore, to ensure the accuracy of the estimated motion features at each scale, we propose a motion adaptive fusion (MAF) module that adaptively deals with motions with different receptive fields according to the motion conditions. Thanks to the framework's flexible recurrent scales, we can customize the number of scales and make trade-offs between computation and quality depending on the application scenario. Extensive experiments with various datasets demonstrated the superiority of our proposed method over state-of-the-art approaches in various scenarios.
C1 [Xiao, Jing; Xu, Kangmin; Hu, Mengshun; Wang, Zheng] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Liao, Liang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 30013, Taiwan.
   [Wang, Mi] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & Re, Wuhan 430072, Peoples R China.
   [Satoh, Shin'ichi] Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.
C3 Wuhan University; Nanyang Technological University; National Tsing Hua
   University; National Tsing Hua University; Wuhan University; Research
   Organization of Information & Systems (ROIS); National Institute of
   Informatics (NII) - Japan
RP Liao, L (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM jing@whu.edu.cn; xukangmin@whu.edu.cn; shunmh@whu.edu.cn;
   liang.liao@ntu.edu.sg; wangzwhu@whu.edu.cn; cwlin@ee.nthu.edu.tw;
   wangmi@whu.edu.cn; satoh@nii.ac.jp
RI Lin, Chia-Wen/ABH-6075-2020
OI Liao, Liang/0000-0002-2238-2420; Satoh, Shin'ichi/0000-0001-6995-6447; ,
   Jing/0000-0001-9794-0119
FU National Natural Science Foundation of China; Grants-in-Aid for
   Scientific Research [22H05015] Funding Source: KAKEN
FX No Statement Available
CR Ahn HE, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11050619
   [Anonymous], 2021, Xiph.org video test media
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   CHARBONNIER P, 1994, IEEE IMAGE PROC, P168
   Cheng XH, 2022, IEEE T PATTERN ANAL, V44, P7029, DOI 10.1109/TPAMI.2021.3100714
   Cheng XH, 2020, AAAI CONF ARTIF INTE, V34, P10607
   Dai SY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1039
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Galteri L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1065, DOI 10.1145/3343031.3350592
   Galvane Q, 2015, AAAI CONF ARTIF INTE, P753
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Gupta A, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P256, DOI 10.1145/3394171.3413686
   Haoxian Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P474, DOI 10.1007/978-3-030-58595-2_29
   Haris M, 2021, IEEE T PATTERN ANAL, V43, P4323, DOI 10.1109/TPAMI.2020.3002836
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hong HJ, 2018, IEEE T MULTIMEDIA, V20, P345, DOI 10.1109/TMM.2017.2744183
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu MS, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2145, DOI 10.1145/3503161.3547875
   Hu MS, 2022, PROC CVPR IEEE, P3564, DOI 10.1109/CVPR52688.2022.00356
   Hu MS, 2022, IEEE T CIRC SYST VID, V32, P3390, DOI 10.1109/TCSVT.2021.3110796
   Hu MS, 2020, INT CONF ACOUST SPEE, P4347, DOI [10.1109/ICASSP40776.2020.9053223, 10.1109/icassp40776.2020.9053223]
   Huang ZW, 2022, LECT NOTES COMPUT SC, V13674, P624, DOI 10.1007/978-3-031-19781-9_36
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Hur J, 2019, PROC CVPR IEEE, P5747, DOI 10.1109/CVPR.2019.00590
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Kamgar-Parsi B, 1999, IEEE T IMAGE PROCESS, V8, P1467, DOI 10.1109/83.791975
   Kong LT, 2022, PROC CVPR IEEE, P1968, DOI 10.1109/CVPR52688.2022.00201
   Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liang Liao, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P837, DOI 10.1145/3503161.3547849
   Liao L, 2022, IEEE T IMAGE PROCESS, V31, P3525, DOI 10.1109/TIP.2022.3172208
   Liu T, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3292, DOI 10.1145/3394171.3413899
   Liu YL, 2019, AAAI CONF ARTIF INTE, P8794
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Long GC, 2016, LECT NOTES COMPUT SC, V9910, P434, DOI 10.1007/978-3-319-46466-4_26
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Meyer S, 2018, PROC CVPR IEEE, P498, DOI 10.1109/CVPR.2018.00059
   Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747
   Niklaus S, 2021, IEEE WINT CONF APPL, P1098, DOI 10.1109/WACV48630.2021.00114
   Niklaus S, 2020, PROC CVPR IEEE, P5436, DOI 10.1109/CVPR42600.2020.00548
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Oh SW, 2019, IEEE I CONF COMP VIS, P4402, DOI 10.1109/ICCV.2019.00450
   Park J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14519, DOI 10.1109/ICCV48922.2021.01427
   Peleg T, 2019, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2019.00250
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Serrano A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073668
   Shen W, 2020, PROC CVPR IEEE, P5113, DOI 10.1109/CVPR42600.2020.00516
   Shi XJ, 2015, ADV NEUR IN, V28
   Shurui Gui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14001, DOI 10.1109/CVPR42600.2020.01402
   Sim Hyeonjun, 2021, P IEEECVF INT C COMP, P14489
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tak-Wai Hui, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P169, DOI 10.1007/978-3-030-58565-5_11
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   van Amersfoort J, 2019, Arxiv, DOI arXiv:1711.06045
   Waltl M., 2012, P 20 ACM INT C MULT, P1469
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang X, 2018, LECT NOTES COMPUT SC, V11165, P35, DOI 10.1007/978-3-030-00767-6_4
   Wu HN, 2023, Arxiv, DOI arXiv:2211.04894
   Wu HN, 2022, LECT NOTES COMPUT SC, V13666, P538, DOI 10.1007/978-3-031-20068-7_31
   Wu HN, 2022, Arxiv, DOI arXiv:2206.09853
   Wu Y, 2022, PROC CVPR IEEE, P17793, DOI 10.1109/CVPR52688.2022.01729
   Xinghao Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P614, DOI 10.1007/978-3-030-58539-6_37
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang GS, 2019, ADV NEUR IN, V32
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Zhang HX, 2019, IEEE ACCESS, V7, P130610, DOI 10.1109/ACCESS.2019.2940510
   Zhu Y, 2017, IEEE IMAGE PROC, P790, DOI 10.1109/ICIP.2017.8296389
NR 74
TC 2
Z9 2
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8076
EP 8090
DI 10.1109/TMM.2022.3233310
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000071
DA 2024-07-18
ER

PT J
AU Zhang, Q
   Wei, JC
   Wang, SS
   Ma, SW
   Gao, W
AF Zhang, Qi
   Wei, Jianchao
   Wang, Shanshe
   Ma, Siwei
   Gao, Wen
TI RealVR: Efficient, Economical, and Quality-of- Experience-Driven VR
   Video System Based on MPEG OMAF
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 360-degree video; mobile computing; video streaming; VR video
ID 360-DEGREES; PERCEPTION; IMAGE
AB Recent years have witnessed the explosion of virtual reality (VR) videos and applications. This new form of media grants us the precious freedom we never had before to look at any directions of the video content. With such privilege, our desires for higher video resolution and frame-rate, better visual quality, and smoother watching experience have remarkably risen. However, VR videos occupy an astronomical amount of data, which poses unprecedented challenges to computation efficiency, deployment cost, and user experience of the system. In this paper, we propose RealVR, an end-to-end tile-based VR video system that faces and tackles such challenges. With all essential procedures from the initial capturing to the ultimate rendering included, the system is designed, implemented, and configured specifically to achieve the best balance among efficiency, economy, and quality-of-experience (QoE). It leverages the promising international VR standard, MPEG OMAF, to process and deliver 8K 60 fps VR video, but is also improved for much better performance and user experience than the pristine standard, especially for reducing the motion-to-high-quality (MtHQ) latency. It does not rely on additional expensive edge servers to offer an immersive user experience, which makes it more economical than many other works. Through extensive experiments, it is proven that RealVR can significantly improve the MtHQ experience and save bandwidth consumption without compromising on encoding efficiency or application cost, maximizing the user's freedom.
C1 [Zhang, Qi; Wang, Shanshe; Ma, Siwei; Gao, Wen] Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Zhang, Qi; Wei, Jianchao] Company Ltd, Beijing Boya Realscene Technol, Beijing 100083, Peoples R China.
   [Gao, Wen] Peng Cheng Lab, Shenzhen 518066, Guangdong, Peoples R China.
C3 Peking University; Peng Cheng Laboratory
RP Wang, SS (corresponding author), Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing 100871, Peoples R China.
EM qizhang@stu.pku.edu.cn; jcwei@realscene.cn; sswang@pku.edu.cn;
   swma@pku.edu.cn; wgao@pku.edu.cn
RI jianchao, wei/O-9237-2018
OI Zhang, Qi/0000-0002-1189-8755
FU National Key Research and Development Project of China [2021YFF0900501];
   National Natural Science Foundation of China [U21B2012, 62072008];
   High-performance Computing Platform of Peking University
FX This work was supported in part by the National Key Research and
   Development Project of China under Grant 2021YFF0900501, in part by the
   National Natural Science Foundation of China under Grants U21B2012 and
   62072008, and in part by the High-performance Computing Platform of
   Peking University.
CR AirPano, 2021, Around World. Best. 8K 360 video-YouTube
   AirPano, 2021, 360 video, lake baikal, magical ice, Russia. 12K aerial video-YouTube
   AirPano. China, 2021, Colourful mountains of the zhangye danxia geopark, 12K aerial 360 video-YouTube
   Alface PR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1105, DOI 10.1145/3123266.3123307
   Alface PR, 2012, BELL LABS TECH J, V16, P135, DOI 10.1002/bltj.20538
   Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   [Anonymous], 2019, ISO/IEC Standard 23090-2:2019
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chen S., 2018, PROC IEEE INT C MULT, P1
   Chiariotti F, 2021, COMPUT COMMUN, V177, P133, DOI 10.1016/j.comcom.2021.06.029
   de la Fuente YS, 2019, IEEE J EM SEL TOP C, V9, P18, DOI 10.1109/JETCAS.2019.2899516
   Deshpande S., 2020, SC29 WG3 N00072
   Evgeny K., 2017, End-to-end optimizations for dynamic streaming
   Evgeny K, 2016, Next-generation video encoding techniques for 360 video and VR
   Fan CL, 2021, IEEE T CIRC SYST VID, V31, P1632, DOI 10.1109/TCSVT.2020.3007288
   Fan CL, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329119
   FFmpeg, 2021, ABOUT US
   Graf M, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P261, DOI 10.1145/3083187.3084016
   Hannuksela MM, 2021, P IEEE, V109, P1590, DOI 10.1109/JPROC.2021.3063544
   Hannuksela MM, 2019, IEEE DATA COMPR CONF, P418, DOI 10.1109/DCC.2019.00050
   He J, 2018, MOBISYS'18: PROCEEDINGS OF THE 16TH ACM INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P482, DOI 10.1145/3210240.3210323
   Hosseini M, 2016, IEEE INT SYM MULTIM, P107, DOI [10.1109/ISM.2016.45, 10.1109/ISM.2016.0028]
   Hu J, 2021, arXiv
   Insta360, 2021, Insta360 pro 8K sample outdoor
   Kim HG, 2020, IEEE T CIRC SYST VID, V30, P917, DOI 10.1109/TCSVT.2019.2898732
   Li C, 2019, PROC CVPR IEEE, P10169, DOI 10.1109/CVPR.2019.01042
   Li C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P932, DOI 10.1145/3240508.3240581
   Li J, 2023, IEEE T MULTIMEDIA, V25, P2939, DOI 10.1109/TMM.2022.3153208
   [黎吉国 Li Jiguo], 2018, [中国科学. 信息科学, Scientia Sinica Informationis], V48, P261
   Liu XN, 2021, IEEE T WIREL COMMUN, V20, P6356, DOI 10.1109/TWC.2021.3073623
   Long KX, 2021, IEEE T MULTIMEDIA, V23, P3670, DOI 10.1109/TMM.2020.3029880
   Lopes F, 2018, PROC SPIE, V10752, DOI 10.1117/12.2321679
   Nasrabadi AT, 2017, P IEEE VIRT REAL ANN, P347, DOI 10.1109/VR.2017.7892319
   Nguyen V. D., 2017, PROC TRON S, P1
   Nokiatech, 2021, Nokia OMAF Implementation
   Open Visual Cloud, 2021, Immersive-video-Sample
   Open Visual Cloud, 2021, SVT-HEVC
   Overbeck RS, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 TALKS, DOI 10.1145/3214745.3214811
   Ozcinar C, 2019, IEEE J EM SEL TOP C, V9, P217, DOI 10.1109/JETCAS.2019.2895096
   Ozcinar C, 2017, IEEE IMAGE PROC, P2174, DOI 10.1109/ICIP.2017.8296667
   Ozcinar C, 2017, IEEE INT SYM MULTIM, P45, DOI 10.1109/ISM.2017.17
   Petrangeli S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P306, DOI 10.1145/3123266.3123453
   Podborski D, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P324, DOI 10.1145/3304109.3323835
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Qian F, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P99, DOI 10.1145/3241539.3241565
   Requirements for Immersive Media Access and Delivery, 2020, ISO/IEC JTC1, SC29 WG3 N19245
   Shi Shu., 2019, Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services, P130
   Skupin R, 2017, IEEE IMAGE PROC, P4592, DOI 10.1109/ICIP.2017.8297155
   Skupin R, 2016, IEEE INT SYM MULTIM, P399, DOI [10.1109/ISM.2016.0089, 10.1109/ISM.2016.137]
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun LY, 2019, IEEE J EM SEL TOP C, V9, P43, DOI 10.1109/JETCAS.2019.2898877
   Sun YL, 2017, IEEE SIGNAL PROC LET, V24, P1408, DOI 10.1109/LSP.2017.2720693
   Tiledmedia, 2021, How ClearVR drives and leverages standards
   Xiao MB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P708, DOI 10.1145/3123266.3123339
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Xu M, 2020, IEEE J-STSP, V14, P5, DOI 10.1109/JSTSP.2020.2966864
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Yaqoob A, 2020, IEEE COMMUN SURV TUT, V22, P2801, DOI 10.1109/COMST.2020.3006999
   Zare A., 2016, P 24 ACM INT C MULT, P601, DOI DOI 10.1145/2964284.2967292
   Zhang HZ, 2020, IEEE T MULTIMEDIA, V22, P3210, DOI 10.1109/TMM.2020.2973828
   Zhang WX, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2232, DOI 10.1145/3442381.3449829
   Zhang XY, 2020, IEEE T CIRC SYST VID, V30, P217, DOI 10.1109/TCSVT.2018.2886805
   Zheng CY, 2022, IEEE T CIRC SYST VID, V32, P2396, DOI 10.1109/TCSVT.2021.3094909
   Zhou C, 2018, IEEE INFOCOM SER, P962, DOI 10.1109/INFOCOM.2018.8486282
   Zhou C, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P27, DOI 10.1145/3083187.3083190
   Zhou YF, 2018, INT CONF SIGN PROCES, P54, DOI 10.1109/ICSP.2018.8652269
NR 66
TC 3
Z9 3
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5386
EP 5399
DI 10.1109/TMM.2022.3190697
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300055
DA 2024-07-18
ER

PT J
AU Zhu, PP
   Wang, X
   Luo, Y
   Sun, ZL
   Zheng, WS
   Wang, YW
   Chen, CW
AF Zhu, Peipei
   Wang, Xiao
   Luo, Yong
   Sun, Zhenglong
   Zheng, Wei-Shi
   Wang, Yaowei
   Chen, Changwen
TI Unpaired Image Captioning by Image-Level Weakly-Supervised Visual
   Concept Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Image recognition; Task analysis; Object detection;
   Training; Annotations; Data models; Graph neural network; unpaired image
   captioning; weakly-supervised instance segmentation
AB The goal of unpaired image captioning (UIC) is to describe images without using image-caption pairs in the training phase. Although challenging, we expect the task can be accomplished by leveraging images aligned with visual concepts. Most existing studies use off-the-shelf algorithms to obtain the visual concepts because the Bounding Box (BBox) labels or relationship-triplet labels used for training are expensive to acquire. To avoid exhaustive annotations, we propose a novel approach to achieve cost-effective UIC. Specifically, we adopt image-level labels to optimize the UIC model in a weakly-supervised manner. For each image, we assume that only the image-level labels are available without specific locations and numbers. The image-level labels are utilized to train a weakly-supervised object recognition model to extract object information (e.g., instance), and the extracted instances are adopted to infer the relationships among different objects using an enhanced graph neural network (GNN). The proposed approach achieves comparable or even better performance compared with previous methods without expensive annotations. Furthermore, we design an unrecognized object (UnO) loss to improve the alignment of the inferred object and relationship information with the images. It can effectively alleviate the issue encountered by existing UIC models when generating sentences with nonexistent objects. To the best of our knowledge, this is the first attempt to address the problem of Weakly-Supervised visual concept recognition for UIC (WS-UIC) based only on image-level labels. Extensive experiments demonstrate that the proposed method achieves inspiring results on the COCO dataset while significantly reducing the labeling cost.
C1 [Zhu, Peipei] Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China.
   [Zhu, Peipei; Wang, Xiao; Zheng, Wei-Shi; Wang, Yaowei] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
   [Luo, Yong] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Sun, Zhenglong] Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China.
   [Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.
   [Chen, Changwen] Hong Kong Polytech Univ, Dept Comp, Hong Kong 999077, Peoples R China.
C3 The Chinese University of Hong Kong, Shenzhen; Peng Cheng Laboratory;
   Wuhan University; The Chinese University of Hong Kong, Shenzhen; Sun Yat
   Sen University; Hong Kong Polytechnic University
RP Wang, YW (corresponding author), Peng Cheng Lab, Shenzhen 518066, Peoples R China.
EM peipeizhu@link.cuhk.edu.cn; wangxiaocvpr@foxmail.com; yluo180@gmail.com;
   sunzhenglong@cuhk.edu.cn; wszheng@ieee.org; wangyw@pcl.ac.cn;
   changwen.chen@polyu.edu.hk
RI SUN, ZHENGLONG/A-9128-2012
OI , Peipei/0000-0001-7678-0005; Chen, Chang Wen/0000-0002-6720-234X; Sun,
   Zhenglong/0000-0002-8135-1659
FU Key-Area Research and Development Program of Guangdong Province
   [2019B010155002]; Peng Cheng Laboratory Research [PCL2021A07]; National
   Natural Science Foundation of China [62102205, 62276195]
FX This work was supported in part by the Key-Area Research and Development
   Program of Guangdong Province under Grant 2019B010155002, in part by
   Peng Cheng Laboratory Research under Grant PCL2021A07, and in part by
   the National Natural Science Foundation of China under Grants 62102205
   and 62276195.
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Baldassarre Federico, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P612, DOI 10.1007/978-3-030-58604-1_37
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Cao S, 2020, NEUROCOMPUTING, V417, P419, DOI 10.1016/j.neucom.2020.08.019
   Chen WH, 2017, Arxiv, DOI arXiv:1611.05321
   Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425
   Gao C, 2021, PROC CVPR IEEE, P3063, DOI 10.1109/CVPR46437.2021.00308
   Gu JX, 2018, LECT NOTES COMPUT SC, V11205, P519, DOI 10.1007/978-3-030-01246-5_31
   Gu JX, 2019, IEEE I CONF COMP VIS, P10322, DOI 10.1109/ICCV.2019.01042
   Guo D, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P920
   Guo LT, 2020, IEEE T MULTIMEDIA, V22, P2149, DOI 10.1109/TMM.2019.2951226
   Guo LT, 2019, PROC CVPR IEEE, P4199, DOI 10.1109/CVPR.2019.00433
   Gurari D, 2018, PROC CVPR IEEE, P3608, DOI 10.1109/CVPR.2018.00380
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks LA, 2016, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2016.8
   Hossain MZ, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3295748
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Ipeirotis P. G., 2010, P ACM SIGKDD WORKSHO, P64, DOI 10.1145/1837885.1837906
   Jin W, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P2763
   Jingna Mao, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348279
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim DJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2012
   Kim J, 2018, LECT NOTES COMPUT SC, V11206, P577, DOI 10.1007/978-3-030-01216-8_35
   Komodakis N, 2017, P ICLR
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Laina I, 2019, IEEE I CONF COMP VIS, P7413, DOI 10.1109/ICCV.2019.00751
   Lample Guillaume, 2018, P 6 INT C LEARN REPR
   Lebret R, 2015, PR MACH LEARN RES, V37, P2085
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Lin C.-Y., 2004, P 4 NTCIR WORKSH, P1
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ling H, 2017, ADV NEUR IN, V30
   Liu FL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4153, DOI 10.1145/3394171.3414004
   Liu FL, 2019, IEEE DATA MINING, P439, DOI 10.1109/ICDM.2019.00054
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo JY, 2022, PROC CVPR IEEE, P16433, DOI 10.1109/CVPR52688.2022.01596
   Mao JH, 2015, IEEE I CONF COMP VIS, P2533, DOI 10.1109/ICCV.2015.291
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Song YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P784, DOI 10.1145/3343031.3350996
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2017, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2017.130
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang X, 2018, LECT NOTES COMPUT SC, V11220, P38, DOI 10.1007/978-3-030-01270-0_3
   Wu J, 2021, IEEE T MULTIMEDIA, V23, P2413, DOI 10.1109/TMM.2020.3011317
   Wu LX, 2020, IEEE T MULTIMEDIA, V22, P808, DOI 10.1109/TMM.2019.2931815
   Wu SM, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P1180, DOI 10.1145/2998181.2998364
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang R, 2018, P 27 INT C COMP LING
   Yang X, 2023, IEEE T PATTERN ANAL, V45, P12996, DOI 10.1109/TPAMI.2021.3121705
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2017, PROC CVPR IEEE, P5263, DOI 10.1109/CVPR.2017.559
   Ye KQ, 2003, ANN STAT, V31, P984, DOI 10.1214/aos/1056562470
   Zhang J, 2021, IEEE T MULTIMEDIA, V23, P92, DOI 10.1109/TMM.2020.2976552
   Zhang XL, 2022, IEEE T CIRC SYST VID, V32, P704, DOI 10.1109/TCSVT.2021.3063377
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 71
TC 2
Z9 2
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6702
EP 6716
DI 10.1109/TMM.2022.3214090
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, J
   Li, XJ
   Luo, LB
   Ma, JY
AF Chen, Jun
   Li, Xuejiao
   Luo, Linbo
   Ma, Jiayi
TI Multi-Focus Image Fusion Based on Multi-Scale Gradients and Image
   Matting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Transforms; Image segmentation; Image edge detection;
   Correlation; Data mining; Training; Decision map; focus measurement;
   image matting; multi-focus image fusion
ID GENERATIVE ADVERSARIAL NETWORK; QUALITY ASSESSMENT; ALGORITHM;
   PERFORMANCE
AB Multi-focus image fusion technology is to extract different focused regions of the same scene among partially focused images and merge them together to generate a composite image where all objects are clear. Two crucial points to multi-focus image fusion are the effective focus measurement method to evaluate the sharpness of the source images and the accurate segmentation method to extract the focused regions. In conventional multi-focus image fusion methods, the decision map obtained according to the focus measurement is sensitive to mis-registration, or produces an uneven boundary lines. In this paper, the maximum value in the top-hat transform and the bottom-hat transform is used as the gradient measurement value, and the complementary features between multiple scales are used to achieve accurate focus measurement for initial segmentation. In order to obtain a better fusion decision map, a robust image matting algorithm is used to refine the trimap generated by the initial segmentation. Then, make full use of the strong correlation between the source images to optimize the edge regions of the decision map to improve the image fusion quality. Finally, a fusion image is constructed based on the fusion decision map and the source images. We perform qualitative and quantitative experiments on publicly available databases to verify the effectiveness of the method. The results show that compared with several state-of-the-art algorithms, the proposed fusion method can obtain accurate decision maps and achieve better performance in visual perception and quantitative analysis.
C1 [Chen, Jun; Li, Xuejiao] China Univ Geosci, Sch Automat, Hubei Key Lab Adv Control & Intelligent Automat C, Wuhan 430074, Peoples R China.
   [Luo, Linbo] China Univ Geosci, Sch Mech Engn & Elect Informat, Wuhan 430074, Peoples R China.
   [Ma, Jiayi] Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
C3 China University of Geosciences; China University of Geosciences; Wuhan
   University
RP Ma, JY (corresponding author), Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
EM chenjun71983@163.com; lixuejiao9537@163.com; luolb@ige-live.com;
   jyma2010@gmail.com
RI Ma, Jiayi/Y-2470-2019
OI Ma, Jiayi/0000-0003-3264-3265; Chen, Jun/0000-0001-9005-6849
FU National Natural Science Foundation of China [62073304, 61773295,
   41977242, 61973283]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62073304, 61773295, 41977242, and 61973283.
CR Aslantas V, 2010, EXPERT SYST APPL, V37, P8861, DOI 10.1016/j.eswa.2010.06.011
   Bai XZ, 2015, INFORM FUSION, V22, P105, DOI 10.1016/j.inffus.2014.05.003
   Bai XZ, 2011, OPT EXPRESS, V19, P8444, DOI 10.1364/OE.19.008444
   Belgiu M, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11070818
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Chen Y, 2009, IMAGE VISION COMPUT, V27, P1421, DOI 10.1016/j.imavis.2007.12.002
   De I, 2013, INFORM FUSION, V14, P136, DOI 10.1016/j.inffus.2012.01.007
   Gao GR, 2013, IET IMAGE PROCESS, V7, P633, DOI 10.1049/iet-ipr.2012.0558
   Goshtasby AA, 2007, INFORM FUSION, V8, P114, DOI 10.1016/j.inffus.2006.04.001
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Jiang Y, 2014, INFORM FUSION, V18, P107, DOI 10.1016/j.inffus.2013.06.001
   Kong J, 2008, INT J COMPUT SCI NET, V8, P220
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li S, 2013, INFORM FUSION, V14, P147, DOI 10.1016/j.inffus.2011.07.001
   Li ST, 2012, IEEE T CONSUM ELECTR, V58, P626, DOI 10.1109/TCE.2012.6227469
   Liang JL, 2012, IEEE T IMAGE PROCESS, V21, P2898, DOI 10.1109/TIP.2012.2183140
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Liu Y, 2015, INFORM FUSION, V23, P139, DOI 10.1016/j.inffus.2014.05.004
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Ma JY, 2020, INFORM FUSION, V62, P110, DOI 10.1016/j.inffus.2020.04.006
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JL, 2019, NEUROCOMPUTING, V335, P9, DOI 10.1016/j.neucom.2019.01.048
   Ma JL, 2017, CHIN CONTR CONF, P5464, DOI 10.23919/ChiCC.2017.8028223
   Maqsood S., 2020, ELECTRON, V9
   Miao QG, 2011, CHIN OPT LETT, V9, DOI 10.3788/COL201109.041001
   Mitianoudis N, 2007, INFORM FUSION, V8, P131, DOI 10.1016/j.inffus.2005.09.001
   Nejati M, 2015, INFORM FUSION, V25, P72, DOI 10.1016/j.inffus.2014.10.004
   Nencini F, 2007, INFORM FUSION, V8, P143, DOI 10.1016/j.inffus.2006.02.001
   Petrovic VS, 2004, IEEE T IMAGE PROCESS, V13, P228, DOI 10.1109/tip.2004.823821
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Saha A, 2013, DIGIT SIGNAL PROCESS, V23, P1121, DOI 10.1016/j.dsp.2013.03.001
   Shreyamsha Kumar BK, 2015, SIGNAL IMAGE VIDEO P, V9, P1193, DOI 10.1007/s11760-013-0556-9
   Tang H, 2018, INFORM SCIENCES, V433, P125, DOI 10.1016/j.ins.2017.12.043
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Wan T, 2013, PATTERN RECOGN LETT, V34, P1001, DOI 10.1016/j.patrec.2013.03.003
   Wan T, 2009, IEEE T MULTIMEDIA, V11, P624, DOI 10.1109/TMM.2009.2017640
   Wang DM, 1997, PATTERN RECOGN, V30, P2043, DOI 10.1016/S0031-3203(97)00015-0
   Wang J, 2007, PROC CVPR IEEE, P281
   Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao B, 2020, IEEE T MULTIMEDIA, V22, P285, DOI 10.1109/TMM.2019.2928516
   Xiao B, 2019, IEEE T CIRC SYST VID, V29, P2796, DOI 10.1109/TCSVT.2018.2869841
   Xu H, 2020, AAAI CONF ARTIF INTE, V34, P12484
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yang B, 2010, IEEE T INSTRUM MEAS, V59, P884, DOI 10.1109/TIM.2009.2026612
   Yang C, 2008, INFORM FUSION, V9, P156, DOI 10.1016/j.inffus.2006.09.001
   Yang SY, 2010, INFORM FUSION, V11, P78, DOI 10.1016/j.inffus.2009.05.001
   Zhang H., 2020, P C ART INT, P797
   Zhang H, 2021, INFORM FUSION, V66, P40, DOI 10.1016/j.inffus.2020.08.022
   Zhang Q, 2018, INFORM FUSION, V40, P57, DOI 10.1016/j.inffus.2017.05.006
   Zhang Y, 2017, INFORM FUSION, V35, P81, DOI 10.1016/j.inffus.2016.09.006
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
NR 58
TC 40
Z9 40
U1 17
U2 90
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 655
EP 667
DI 10.1109/TMM.2021.3057493
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100011
DA 2024-07-18
ER

PT J
AU Gong, X
   Yao, Z
   Li, X
   Fan, YQ
   Luo, B
   Fan, JF
   Lao, BJ
AF Gong, Xun
   Yao, Zu
   Li, Xin
   Fan, Yueqiao
   Luo, Bin
   Fan, Jianfeng
   Lao, Boji
TI LAG-Net: Multi-Granularity Network for Person Re-Identification via
   Local Attention System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Person re-identification; local attention; multi-granularity feature;
   deep learning
AB Person re-identification (Re-ID) is a challenging research topic which aims to retrieve the pedestrian images of the same person that captured by non-overlapping cameras. Existing methods either assume the body parts of the same person are well-aligned, or use attention selection mechanisms to constrain the effective region of feature learning. But these methods concentrate only on coarse feature representation and cannot model complex real scenes effectively. We propose a novel Local Attention Guided Network (LAG-Net) to not only exploit the most salient area among different people, but also extract important local detail through a Local Attention System (LAS). LAS is an attention selection unit that could extract approximate semantic local features of human body parts without extra supervision. To learn discriminative attention feature representation, we explore an attention feature regularization scheme to enhance the relevance of body part features that belong to same personal identity. Considering the effectiveness of feature augmentation in the Re-ID task and the defect of the existing methods, we propose a Batch Attention DropBlock (BA-DropBlock) to further improve DropBlock by combining the attention selection mechanism. Results on mainstream datasets demonstrate the superiority of our model over the state-of-the-art. Especially, our approach exceeds the current best method by a large margin of 4.6% on the most challenging dataset CUHK03.
C1 [Gong, Xun; Yao, Zu; Luo, Bin; Lao, Boji] Southwest Jiaotong Univ, Sch Comp & Artificial Intelligence, Chengdu 610031, Peoples R China.
   [Li, Xin] Louisiana State Univ, Sch Elect Engn & Comp Sci, Baton Rouge, LA 70808 USA.
   [Fan, Yueqiao; Fan, Jianfeng] Southwest Jiaotong Univ, Grad Sch Tangshan, Tangshan 063000, Peoples R China.
C3 Southwest Jiaotong University; Louisiana State University System;
   Louisiana State University; Southwest Jiaotong University
RP Gong, X (corresponding author), Southwest Jiaotong Univ, Sch Comp & Artificial Intelligence, Chengdu 610031, Peoples R China.
EM gongxun@foxmail.com; 976258321@qq.com; xinli@cct.lsu.edu;
   fanyueqiao@my.swjtu.edu.cn; ansvic@my.swjtu.edu.cn;
   fanfront@my.swjtu.edu.cn; lao@my.swjtu.edu.cn
RI Li, Xin/HGE-2316-2022
OI Li, Xin/0000-0002-0144-9489; Gong, Xun/0000-0002-1494-0955
FU National Natural Science Foundation of China [61876158, 61772435];
   Sichuan Science and Technology Program [2019YFS0432]; National Science
   Foundation of USA [OIA-1946231]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61876158 and 61772435 and in part by
   the Sichuan Science and Technology Program under Grant 2019YFS0432. This
   work of Xin Li was supported by the National Science Foundation of USA
   (OIA-1946231).
CR Barbosa IB, 2018, COMPUT VIS IMAGE UND, V167, P50, DOI 10.1016/j.cviu.2017.12.002
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen H., 2018, P IEEE 4 INT C MULT, P1, DOI DOI 10.1109/BIGMM.2018.8499067
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ghiasi G, 2018, ADV NEUR IN, V31
   Guo T., 2018, 2018 IEEE WIRELESS P, P1, DOI DOI 10.1109/PESGM.2017.8274435
   Han K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2040, DOI 10.1145/3240508.3240550
   Hermans Alexander, 2017, ARXIV170307737
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Lejbolle AR, 2020, IEEE T INF FOREN SEC, V15, P1216, DOI 10.1109/TIFS.2019.2938870
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Liu JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P737, DOI 10.1145/3240508.3240585
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Luo H, 2019, PATTERN RECOGN, V94, P53, DOI 10.1016/j.patcog.2019.05.028
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Qian XL, 2020, IEEE T PATTERN ANAL, V42, P371, DOI 10.1109/TPAMI.2019.2928294
   Sarfraz M. S., 2019, P BRIT MACH VIS C
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shen C, 2019, IEEE T CIRC SYST VID, V29, P3016, DOI 10.1109/TCSVT.2018.2872503
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun LC, 2019, IEEE IMAGE PROC, P2254, DOI [10.1109/icip.2019.8803292, 10.1109/ICIP.2019.8803292]
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu Xiaoxia, 2020, ARXIV201203107
   Xia B, 2019, IEEE I CONF COMP VIS, P3759, DOI 10.1109/ICCV.2019.00386
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie YX, 2020, IEEE SENS J, V20, P359, DOI 10.1109/JSEN.2019.2942106
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yang JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1074, DOI 10.1145/3240508.3240645
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
NR 47
TC 28
Z9 28
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 217
EP 229
DI 10.1109/TMM.2021.3050082
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300017
DA 2024-07-18
ER

PT J
AU Lee, S
   Yang, H
   Choi, H
   Seong, W
AF Lee, Seungjun
   Yang, Haesang
   Choi, Hwiyong
   Seong, Woojae
TI Zero-Shot Single-Microphone Sound Classification and Localization in a
   Building Via the Synthesis of Unseen Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Location awareness; Microphones; Buildings; Feature extraction;
   Training; Reverberation; Data models; Generative adversarial network;
   sound classification; sound source localization; zero-shot learning
ID EVENT LOCALIZATION; NEURAL-NETWORKS; NOISE
AB In this paper, we propose a learning-based approach to identify the type and position of sounds using a single microphone in a real-world building. We attempt to treat this problem as a joint classification problem in which we predict the exact positions of sounds while classifying the types that are assumed to be from pre-defined types of sounds. The most problematic issue is that while the types are readily classified under supervised learning frameworks with one-hot encoded labels, it is difficult to predict the exact positions of the sound from unseen positions during training. To address this potential discrepancy, we formulate the position identification problem as a zero-shot learning problem inspired by the human ability to perceive new concepts from previously learned concepts. We extract feature representations from audio data and vectorize the type and position of the sound source as 'type/position-aware attributes,' instead of labeling each class with a simple one-hot vector. We then train a promising generative model to bridge the extracted features and the attributes by learning the class-invariant structure to transfer the knowledge from seen to unseen classes through their attributes; generative adversarial networks are conditioned on the class-embeddings. Our proposed methods are evaluated on an indoor noise dataset, SNU-B36-EX, a real-world dataset collected inside a building.
C1 [Lee, Seungjun; Yang, Haesang; Choi, Hwiyong] Seoul Natl Univ, Dept Naval Architecture & Ocean Engn, Seoul 08826, South Korea.
   [Seong, Woojae] Seoul Natl Univ, Dept Naval Architecture & Ocean Enn, Seoul 08826, South Korea.
   [Seong, Woojae] Seoul Natl Univ, Res Inst Marine Syst Engn, Seoul 08826, South Korea.
C3 Seoul National University (SNU); Seoul National University (SNU); Seoul
   National University (SNU)
RP Seong, W (corresponding author), Seoul Natl Univ, Dept Naval Architecture & Ocean Enn, Seoul 08826, South Korea.; Seong, W (corresponding author), Seoul Natl Univ, Res Inst Marine Syst Engn, Seoul 08826, South Korea.
EM tl7qns7ch@snu.ac.kr; coupon3@snu.ac.kr; its_me_chy@snu.ac.kr;
   wseong@snu.ac.kr
OI Lee, Seungjun/0009-0001-4314-0260; Yang, Haesang/0000-0001-7101-5195
CR Adavanne S, 2019, IEEE J-STSP, V13, P34, DOI 10.1109/JSTSP.2018.2885636
   Adavanne S, 2017, INT CONF ACOUST SPEE, P771, DOI 10.1109/ICASSP.2017.7952260
   Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Alameda-Pineda X, 2014, IEEE-ACM T AUDIO SPE, V22, P1082, DOI 10.1109/TASLP.2014.2317989
   An I, 2019, IEEE INT CONF ROBOT, P4061, DOI [10.1109/ICRA.2019.8794093, 10.1109/icra.2019.8794093]
   [Anonymous], 2015, P IEEE INT JOINT C N, DOI DOI 10.1109/IJCNN.2015.7280624
   [Anonymous], 2016, PROF DCASE
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Atmoko H, 2008, MEAS SCI TECHNOL, V19, DOI 10.1088/0957-0233/19/2/024003
   Barchiesi D, 2015, IEEE SIGNAL PROC MAG, V32, P16, DOI 10.1109/MSP.2014.2326181
   Bertin-Mahieux T., 2011, ISMIR, P591
   Bisot V, 2016, INT CONF ACOUST SPEE, P6445, DOI 10.1109/ICASSP.2016.7472918
   Butko T, 2011, EUR SIGNAL PR CONF, P1317
   Çakir E, 2017, IEEE-ACM T AUDIO SPE, V25, P1291, DOI 10.1109/TASLP.2017.2690575
   Chakrabarty S, 2019, IEEE J-STSP, V13, P8, DOI 10.1109/JSTSP.2019.2901664
   Choi H, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9183735
   Clevert D., 2016, ARXIV151107289
   Dargie W, 2009, IEEE T SYST MAN CY A, V39, P715, DOI 10.1109/TSMCA.2009.2015676
   Defferrard M., 2016, P ISMIR OCT
   Deng F, 2017, IEEE T IND ELECTRON, V64, P4894, DOI 10.1109/TIE.2017.2652394
   Eronen AJ, 2006, IEEE T AUDIO SPEECH, V14, P321, DOI 10.1109/TSA.2005.854103
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Floor Noise Management Centre, 2018, MONTHL REP
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fu YW, 2018, IEEE SIGNAL PROC MAG, V35, P112, DOI 10.1109/MSP.2017.2763441
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Gemmeke JortF., 2013, 2013 IEEE workshop on applications of signal processing to audio and acoustics, P1, DOI DOI 10.1109/WASPAA.2013.6701847
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gulrajani Ishaan, 2017, P ADV NEUR INF PROC, P5767
   Gustafsson T, 2003, IEEE T SPEECH AUDI P, V11, P791, DOI 10.1109/TSA.2003.818027
   He WP, 2019, INT CONF ACOUST SPEE, P770, DOI 10.1109/ICASSP.2019.8682655
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hirvonen T., 2015, AUDIO ENG SOC CONVEN, V2
   Ioffe S., PROC INT C MACH LEAR
   Johnson M., 2017, T ASSOC COMPUT LING, V5, P339, DOI [10.1162/tacla00065, 10.1162/tacl_a_00065, DOI 10.1162/TACL_A_00065]
   Khunarsal P, 2013, INFORM SCIENCES, V243, P57, DOI 10.1016/j.ins.2013.04.014
   Kingma D. P., 2014, arXiv
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Komatsu T, 2021, EUR SIGNAL PR CONF, P41, DOI 10.23919/Eusipco47968.2020.9287372
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Larochelle H., 2008, AAAI, V1, P3, DOI DOI 10.5555/1620163.1620172
   Lopatka K, 2016, MULTIMED TOOLS APPL, V75, P10407, DOI 10.1007/s11042-015-3105-4
   Mesaros A, 2015, INT CONF ACOUST SPEE, P151, DOI 10.1109/ICASSP.2015.7177950
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Nguyen TNT, 2020, IEEE-ACM T AUDIO SPE, V28, P2626, DOI 10.1109/TASLP.2020.3019646
   Noh Kyoungjin, 2019, 3 STAGE APPROACH SOU
   Norouzi M, 2013, ARXIV13125650
   Pak J, 2019, IEEE-ACM T AUDIO SPE, V27, P1335, DOI 10.1109/TASLP.2019.2919378
   Parascandolo G, 2016, INT CONF ACOUST SPEE, P6440, DOI 10.1109/ICASSP.2016.7472917
   Parhizkar Reza, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1434, DOI 10.1109/ICASSP.2014.6853834
   Park J., 2019, P 20 INT SOC MUS INF
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Perotin L, 2019, IEEE J-STSP, V13, P22, DOI 10.1109/JSTSP.2019.2900164
   Phan H., 2020, ARXIV200905527
   Piczak K. J., 2015, IEEE INT WORKS MACH, P1
   Piczak KJ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1015, DOI 10.1145/2733373.2806390
   Politis A, 2021, IEEE-ACM T AUDIO SPE, V29, P684, DOI 10.1109/TASLP.2020.3047233
   Qin J, 2017, PROC CVPR IEEE, P1042, DOI 10.1109/CVPR.2017.117
   Rakotomamonjy A, 2015, IEEE-ACM T AUDIO SPE, V23, P142, DOI 10.1109/TASLP.2014.2375575
   Ribeiro F, 2010, IEEE T AUDIO SPEECH, V18, P1781, DOI 10.1109/TASL.2010.2052250
   Romera-Paredes Bernardino, 2015, ICML
   Ronchini F., 2020, P DCASE2020 TOK JP N
   Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
   Sang J, 2018, EUR SIGNAL PR CONF, P2444, DOI 10.23919/EUSIPCO.2018.8553247
   Saxena Ashutosh, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P1737, DOI 10.1109/ROBOT.2009.5152861
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Sheng XH, 2005, IEEE T SIGNAL PROCES, V53, P44, DOI 10.1109/TSP.2004.838930
   Socher R., 2013, ADV NEURAL INFORM PR, V26, P935, DOI DOI 10.1007/978-3-319-46478-7
   Takashima R, 2010, INT CONF ACOUST SPEE, P2830, DOI 10.1109/ICASSP.2010.5496188
   Takeda R, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3514, DOI 10.1109/ICASSP.2018.8461723
   Takeda R, 2016, INT CONF ACOUST SPEE, P405, DOI 10.1109/ICASSP.2016.7471706
   Talmon R, 2011, 2011 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS (WASPAA), P245, DOI 10.1109/ASPAA.2011.6082267
   Nguyen TNT, 2020, INT CONF ACOUST SPEE, P71, DOI [10.1109/ICASSP40776.2020.9053045, 10.1109/icassp40776.2020.9053045]
   Valenzise G, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P21, DOI 10.1109/AVSS.2007.4425280
   Varzandeh R, 2020, INT CONF ACOUST SPEE, P566, DOI [10.1109/icassp40776.2020.9054754, 10.1109/ICASSP40776.2020.9054754]
   Vergin R., 1995, 1995 Canadian Conference on Electrical and Computer Engineering (Cat. No.95TH8103), P1062, DOI 10.1109/CCECE.1995.526613
   Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450
   Wang JC, 2006, IEEE IJCNN, P1731
   Xian Y., 2019, PROC IEEE C COMPUT V, p10 275
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xie H, 2019, IEEE WORK APPL SIG, P264, DOI [10.1109/WASPAA.2019.8937283, 10.1109/waspaa.2019.8937283]
   Xu X, 2017, INT J COMPUT VISION, V123, P309, DOI 10.1007/s11263-016-0983-5
   Zhang HM, 2015, INT CONF ACOUST SPEE, P559, DOI 10.1109/ICASSP.2015.7178031
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
NR 87
TC 1
Z9 1
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2339
EP 2351
DI 10.1109/TMM.2021.3079705
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600009
DA 2024-07-18
ER

PT J
AU Ling, CW
   Zhang, XG
   Chen, H
AF Ling, Chuanwu
   Zhang, Xiaogang
   Chen, Hua
TI Unsupervised Monocular Depth Estimation Using Attention and Multi-Warp
   Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Estimation; Image reconstruction; Convolution; Task analysis; Training;
   Videos; Unsupervised learning; Monocular depth estimation; unsupervised;
   attention; multi-Warp reconstruction
AB Monocular depth estimation has become one of the most studied topics in computer vision. Most approaches treat depth prediction as a fully supervised regression problem requiring vast amounts of corresponding ground-truth depth and image pairs for training. Unsupervised monocular depth estimation has emerged as a promising alternative that eliminates dataset limitations. This paper proposes an end-to-end unsupervised deep learning framework integrating attention blocks and multi-warp loss for monocular depth estimation. In this framework, to explore more general contextual information among the feature volumes, an attention block that sequentially refines the feature maps along the channel and spatial dimensions is inserted after the first and last stages of the network encoder. Additionally, to further utilize the errors in the original disparity estimation from the network, a novel multi-warp reconstruction strategy is designed for the loss function. The experimental results evaluated on the KITTI, CityScapes and Make3D datasets demonstrate the state-of-the-art performance and satisfactory generalization ability of our proposed method.
C1 [Ling, Chuanwu; Zhang, Xiaogang] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.
   [Chen, Hua] Hunan Univ, Coll Comp Sci & Elec tron Engn, Changsha 410082, Peoples R China.
C3 Hunan University; Hunan University
RP Zhang, XG (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.
EM lingchuanwu@hnu.edu.cn; zhangxg@hnu.edu.cn; chua@hnu.edu.cn
FU National Key Research and Development Program of China [2018YFB1305900];
   Science, and Technology Innovation Program of Hunan Province
   [2020GK2020]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFB1305900, and in part by
   the Science, and Technology Innovation Program of Hunan Province under
   Grant 2020GK2020.
CR Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Clevert D., 2016, ARXIV151107289
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dubská M, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.90
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Ghiasi G, 2018, ADV NEUR IN, V31
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiao JB, 2018, LECT NOTES COMPUT SC, V11219, P55, DOI 10.1007/978-3-030-01267-0_4
   Johnston A, 2020, PROC CVPR IEEE, P4755, DOI 10.1109/CVPR42600.2020.00481
   Kingma D. P., 2014, arXiv
   Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lam Huynh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P581, DOI 10.1007/978-3-030-58574-7_35
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JH, 2019, PROC CVPR IEEE, P9721, DOI 10.1109/CVPR.2019.00996
   Li J, 2017, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2017.365
   Li RB, 2019, LECT NOTES COMPUT SC, V11364, P663, DOI 10.1007/978-3-030-20870-7_41
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Loukas A., 2020, IBER CONF INF SYST, DOI DOI 10.23919/cisti49556.2020.9141108
   Luo Y, 2018, PROC CVPR IEEE, P155, DOI 10.1109/CVPR.2018.00024
   Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Park Jongchan, 2018, arXiv preprint arXiv:1807.06514
   Pilzer A, 2019, PROC CVPR IEEE, P9760, DOI 10.1109/CVPR.2019.01000
   Pilzer A, 2018, INT CONF 3D VISION, P587, DOI 10.1109/3DV.2018.00073
   Puscas MM, 2019, INT CONF 3D VISION, P18, DOI 10.1109/3DV.2019.00012
   Ramachandran P, 2019, ADV NEUR IN, V32
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saxena A., 2006, NIPS, P1161, DOI DOI 10.1109/TPAMI.2015.2505283A
   Saxena A, 2008, INT J COMPUT VISION, V76, P53, DOI 10.1007/s11263-007-0071-y
   Song CX, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185389
   Song WF, 2020, IEEE T MULTIMEDIA, V22, P1220, DOI 10.1109/TMM.2019.2941776
   Stoyanov D, 2010, LECT NOTES COMPUT SC, V6361, P275
   Tao MW, 2013, IEEE I CONF COMP VIS, P673, DOI 10.1109/ICCV.2013.89
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   Tosi F, 2019, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR.2019.01003
   Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wong A, 2019, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR.2019.00579
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu D, 2019, IEEE T PATTERN ANAL, V41, P1426, DOI 10.1109/TPAMI.2018.2839602
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Yang ZH, 2018, AAAI CONF ARTIF INTE, P7493
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Yuan Y, 2018, OCNET OBJECT CONTEXT
   Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043
   Zhang ML, 2020, NEUROCOMPUTING, V404, P1, DOI 10.1016/j.neucom.2020.05.015
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhao YH, 2020, PROC CVPR IEEE, P3327, DOI 10.1109/CVPR42600.2020.00339
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 65
TC 18
Z9 18
U1 8
U2 55
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2938
EP 2949
DI 10.1109/TMM.2021.3091308
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000020
DA 2024-07-18
ER

PT J
AU Qian, XY
   Brutti, A
   Lanz, O
   Omologo, M
   Cavallaro, A
AF Qian, Xinyuan
   Brutti, Alessio
   Lanz, Oswald
   Omologo, Maurizio
   Cavallaro, Andrea
TI Audio-Visual Tracking of Concurrent Speakers
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Three-dimensional displays; Acoustics; Faces; Cameras;
   Visualization; Image color analysis; 3D multiple target tracking;
   audio-visual fusion; concurrent speakers; particle filter
ID MULTISPEAKER TRACKING; VISUAL TRACKING; PERFORMANCE EVALUATION;
   MULTITARGET TRACKING; INFORMATION FUSION; SOURCE SEPARATION; MEAN-SHIFT;
   LOCALIZATION; AUDIO
AB Audio-visual tracking of an unknown number of concurrent speakers in 3D is a challenging task, especially when sound and video are collected with a compact sensing platform. In this paper, we propose a tracker that builds on generative and discriminative audio-visual likelihood models formulated in a particle filtering framework. We localize multiple concurrent speakers with a de-emphasized acoustic map assisted by the image detection-derived 3D video observations. The 3D multi-modal observations are either assigned to existing tracks for discriminative likelihood computation or used to initialize new tracks. The generative likelihoods rely on color distribution of the target and the de-emphasized acoustic map value. Experiments on AV16.3 and CAV3D datasets show that the proposed tracker outperforms the uni-modal trackers and the state-of-the-art approaches both in 3D and on the image plane.
C1 [Qian, Xinyuan; Cavallaro, Andrea] Queen Mary Univ London QMUL London, Ctr Intelligent Sensing CIS, London E1 4NS, England.
   [Brutti, Alessio; Lanz, Oswald] Fdn Bruno Kessler FBK, I-38123 Trento, Italy.
   [Omologo, Maurizio] FBK, Trento, Italy.
C3 Fondazione Bruno Kessler; Fondazione Bruno Kessler
RP Qian, XY (corresponding author), Queen Mary Univ London QMUL London, Ctr Intelligent Sensing CIS, London E1 4NS, England.
EM eleqian@nus.edu.sg; brutti@fbk.eu; lanz@fbk.eu;
   maurizio.omologo@gmail.com; a.cavallaro@qmul.ac.uk
RI qian, xinyuan/ADN-5425-2022; Lanz, Oswald/AAW-7865-2021
OI Lanz, Oswald/0000-0003-4793-4276; Brutti, Alessio/0000-0003-4146-3071;
   Omologo, Maurizio/0000-0003-0879-0548; Qian, Xinyuan/0000-0002-9511-6713
CR Alameda-Pineda X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1059, DOI 10.1145/3343031.3350590
   [Anonymous], 1998, SPOKEN DIALOGUE COMP
   Bachu R., 2008, AM SOC ENG ED ASEE Z, P1
   Ban YT, 2021, IEEE T PATTERN ANAL, V43, P1761, DOI 10.1109/TPAMI.2019.2953020
   Ban YT, 2017, IEEE INT CONF COMP V, P446, DOI 10.1109/ICCVW.2017.60
   Bar-Shalom Y., 2011, Tracking and Data Fusion: A Handbook of Algorithms
   Barnard M, 2014, IEEE T MULTIMEDIA, V16, P864, DOI 10.1109/TMM.2014.2301977
   Birchfield ST, 2005, PROC CVPR IEEE, P1158
   Breitenstein MD, 2009, IEEE I CONF COMP VIS, P1515, DOI 10.1109/ICCV.2009.5459278
   Brunelli R., 2006, Multimodal Technologies for Perception of Humans. First International Evaluation Workshop on Classification of Events, Activities and Relationships, CLEAR 2006. Revised Selected Papers (Lecture Notes in Computer Science Vol.4122), P55
   Brutti A, 2010, EUR SIGNAL PR CONF, P974
   Brutti A, 2010, EURASIP J AUDIO SPEE, DOI 10.1155/2010/147495
   Chen XZ, 2018, IEEE T PATTERN ANAL, V40, P1259, DOI 10.1109/TPAMI.2017.2706685
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   D'Arca E., 2012, Data Fusion Target Tracking Conference (DF TT 2012): Algorithms Applications, 9th IET, P1
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Dmochowski J, 2007, IEEE T AUDIO SPEECH, V15, P1327, DOI 10.1109/TASL.2006.889795
   Gebru ID, 2018, IEEE T PATTERN ANAL, V40, P1086, DOI 10.1109/TPAMI.2017.2648793
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   Kilic V, 2016, IEEE T MULTIMEDIA, V18, P2417, DOI 10.1109/TMM.2016.2599150
   Kiliç V, 2015, IEEE T MULTIMEDIA, V17, P186, DOI 10.1109/TMM.2014.2377515
   Kirchmaier U, 2011, INFORM FUSION, V12, P275, DOI 10.1016/j.inffus.2010.06.005
   KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830
   Lathoud G, 2005, LECT NOTES COMPUT SC, V3361, P182
   Lei Y, 2008, IEEE T SYST MAN CY B, V38, P1578, DOI 10.1109/TSMCB.2008.928226
   Li XF, 2019, IEEE J-STSP, V13, P88, DOI 10.1109/JSTSP.2019.2903472
   Lin SF, 2019, IEEE-ACM T AUDIO SPE, V27, P827, DOI 10.1109/TASLP.2019.2898818
   Lin SF, 2018, IEEE-ACM T AUDIO SPE, V26, P2098, DOI 10.1109/TASLP.2018.2854871
   Liu QJ, 2018, IEEE T MULTIMEDIA, V20, P1767, DOI 10.1109/TMM.2017.2777671
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Llinas J, 2017, HDB MULTISENSOR DATA
   Loizou P. C, 2013, SPEECH ENHANCEMENT T
   Luo WJ, 2018, PROC CVPR IEEE, P3569, DOI 10.1109/CVPR.2018.00376
   Maggio E, 2005, INT CONF ACOUST SPEE, P221
   Markovic I, 2010, ROBOT AUTON SYST, V58, P1185, DOI 10.1016/j.robot.2010.08.001
   Minotto VP, 2015, IEEE T MULTIMEDIA, V17, P1694, DOI 10.1109/TMM.2015.2463722
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Naqvi SM, 2012, IET SIGNAL PROCESS, V6, P466, DOI 10.1049/iet-spr.2011.0124
   Naqvi SM, 2010, IEEE J-STSP, V4, P895, DOI 10.1109/JSTSP.2010.2057198
   Nickel K., 2005, Proceedings of the ACM International Conference on Multimodal Interfaces, P61
   Omologo M, 1997, IEEE T SPEECH AUDI P, V5, P288, DOI 10.1109/89.568735
   Panta K, 2004, P SOC PHOTO-OPT INS, V5429, P284, DOI 10.1117/12.543357
   Pavlidi D, 2013, IEEE T AUDIO SPEECH, V21, P2193, DOI 10.1109/TASL.2013.2272524
   Qian XY, 2019, IEEE T MULTIMEDIA, V21, P2576, DOI 10.1109/TMM.2019.2902489
   Qian XY, 2017, INT CONF ACOUST SPEE, P2896, DOI 10.1109/ICASSP.2017.7952686
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Ristic B, 2011, IEEE T SIGNAL PROCES, V59, P3452, DOI 10.1109/TSP.2011.2140111
   Schuhmacher D, 2008, IEEE T SIGNAL PROCES, V56, P3447, DOI 10.1109/TSP.2008.920469
   Shivappa ST, 2010, P IEEE, V98, P1692, DOI 10.1109/JPROC.2010.2057231
   Smaragdis P, 2007, IEEE T AUDIO SPEECH, V15, P358, DOI 10.1109/TASL.2006.876758
   Stiefelhagen R., 2007, MULTIMODAL TECHNOLOG
   Sun C, 2017, IEEE T CIRC SYST VID, V27, P2567, DOI 10.1109/TCSVT.2016.2595265
   Taj M, 2009, INT CONF ACOUST SPEE, P3517, DOI 10.1109/ICASSP.2009.4960384
   Talantzis F, 2008, IEEE T SYST MAN CY B, V38, P799, DOI 10.1109/TSMCB.2008.922063
   Vermaak J, 2005, IEEE T AERO ELEC SYS, V41, P309, DOI 10.1109/TAES.2005.1413764
   Wang N., 2013, Advances in Neural Information Processing Systems, P809
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Xie Y, 2014, IEEE T CYBERNETICS, V44, P539, DOI 10.1109/TCYB.2013.2259230
   Yang HX, 2011, NEUROCOMPUTING, V74, P3823, DOI 10.1016/j.neucom.2011.07.024
   Yu Q, 2008, LECT NOTES COMPUT SC, V5303, P678
   Zhou HY, 2008, IEEE J-STSP, V2, P503, DOI 10.1109/JSTSP.2008.2001429
   Zotkin DN, 2002, EURASIP J APPL SIG P, V2002, P1154, DOI 10.1155/S1110865702206058
NR 64
TC 10
Z9 10
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 942
EP 954
DI 10.1109/TMM.2021.3061800
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100033
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xi, M
   Zhou, WG
   Wang, N
   Li, HQ
AF Xi, Mao
   Zhou, Wengang
   Wang, Ning
   Li, Houqiang
TI Learning Temporal-Correlated and Channel- Decorrelated Siamese Networks
   for Visual Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Target tracking; Visualization; Decorrelation; Training;
   Feature extraction; Benchmark testing; Siamese tracking; temporal
   correlation; channel decorrelation; template update
ID CORRELATION FILTER
AB Recently, Siamese network based trackers have attracted growing popularity in visual tracking, which tackle the tracking by template matching between the initial template and successive search regions. The initial template patch is generally encoded into a convolutional feature for matching. However, the limited representational capability of the template feature limits the tracking accuracy. Besides, this fixed representation also fails to adapt to the target appearance changes. To alleviate these issues, we improve the Siamese trackers by introducing temporal correlation and channel decorrelation mechanisms. On the one hand, we consider the channel-wise correlations between the initial and historical template features to adaptively aggregate informative channel-wise representations for template update. On the other hand, we propose a decorrelation regularization to weaken the channel-wise correlations of individual template features. By end-to-end training, we learn a more complete and adaptive template for accurate object tracking. We demonstrate the generality of our approach by applying it to two prevalent Siamese trackers, i.e., SiamFC and SiamRPN. Extensive experiments on seven benchmark datasets verify the effectiveness of our method.
C1 [Xi, Mao; Zhou, Wengang; Wang, Ning; Li, Houqiang] Univ Sci & Technol China, CAS, Key Lab Technol Geospatial Informat Proc & Appli, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, CAS, Key Lab Technol Geospatial Informat Proc & Appli, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM ximao@mail.ustc.edu.cn; zhwg@ustc.edu.cn; wn6149@mail.ustc.edu.cn;
   lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013; Wang, Zhiquan/HHS-6768-2022
FU National Key R&D Program of China [2017YFB1002202]; National Natural
   Science Foundation of China [61822208, 62021001]; Youth Innovation
   Promotion Association CAS [2018497]; GPU cluster built by MCC Laboratory
   of Information Science and Technology Institution, USTC;  [WK3490000005]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFB1002202, in part by the National Natural Science
   Foundation of China under Grants 61822208 and 62021001, in part by the
   Youth Innovation Promotion Association CAS under Grant 2018497, in part
   by the Central Universities under Grant WK3490000005, and in part by the
   GPU cluster built by MCC Laboratory of Information Science and
   Technology Institution, USTC.
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen HZ, 2020, INT CONF ACOUST SPEE, P2153, DOI [10.1109/icassp40776.2020.9054757, 10.1109/ICASSP40776.2020.9054757]
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Choi J, 2019, IEEE I CONF COMP VIS, P911, DOI 10.1109/ICCV.2019.00100
   Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gordon D, 2018, IEEE ROBOT AUTOM LET, V3, P788, DOI 10.1109/LRA.2018.2792152
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Han ZJ, 2020, IEEE T CIRC SYST VID, V30, P155, DOI 10.1109/TCSVT.2018.2888492
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huang B, 2020, IEEE T MULTIMEDIA, V22, P2820, DOI 10.1109/TMM.2020.2965482
   Huang JL, 2019, AAAI CONF ARTIF INTE, P8457
   Huang LH, 2019, IEEE I CONF COMP VIS, P3998, DOI 10.1109/ICCV.2019.00410
   Jung I., 2018, P ECCV, P83
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu XK, 2018, LECT NOTES COMPUT SC, V11218, P369, DOI 10.1007/978-3-030-01264-9_22
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Tian SJ, 2021, IEEE T MULTIMEDIA, V23, P120, DOI 10.1109/TMM.2020.2978636
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vaswani A, 2017, ADV NEUR IN, V30
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang GT, 2020, PROC CVPR IEEE, P6287, DOI 10.1109/CVPR42600.2020.00632
   Wang GT, 2019, PROC CVPR IEEE, P3638, DOI 10.1109/CVPR.2019.00376
   Wang N, 2019, IEEE T CIRC SYST VID, V29, P730, DOI 10.1109/TCSVT.2018.2816570
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201
   Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Yunhua Zhang, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11213), P355, DOI 10.1007/978-3-030-01240-3_22
   Zha YF, 2020, IEEE T MULTIMEDIA, V22, P96, DOI 10.1109/TMM.2019.2922125
   Zhang LC, 2019, IEEE I CONF COMP VIS, P4009, DOI 10.1109/ICCV.2019.00411
   Zhang MD, 2018, LECT NOTES COMPUT SC, V11207, P484, DOI 10.1007/978-3-030-01219-9_29
   Zhang SQ, 2021, IEEE T MULTIMEDIA, V23, P859, DOI 10.1109/TMM.2020.2990089
   Zhang SL, 2020, IEEE T CYBERNETICS, V50, P270, DOI 10.1109/TCYB.2018.2868782
   Zhang TZ, 2018, IEEE T IMAGE PROCESS, V27, P2676, DOI 10.1109/TIP.2017.2781304
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 69
TC 10
Z9 10
U1 4
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2791
EP 2803
DI 10.1109/TMM.2021.3087340
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000008
DA 2024-07-18
ER

PT J
AU Zhai, YH
   Wang, L
   Tang, W
   Zhang, QL
   Zheng, NN
   Hua, G
AF Zhai, Yuanhao
   Wang, Le
   Tang, Wei
   Zhang, Qilin
   Zheng, Nanning
   Hua, Gang
TI Action Coherence Network for Weakly-Supervised Temporal Action
   Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Location awareness; Coherence; Feature extraction; Training;
   Optical losses; Optical fiber networks; Temporal action localization;
   weakly-supervised learning
ID ACTION RECOGNITION; EVENT DETECTION; MODEL
AB Weakly-supervised Temporal Action Localization (W-TAL) aims at simultaneously classifying and locating all action instances with only video-level supervision. However, current W-TAL methods have two limitations. First, they ignore the difference in video representations between an action instance and its surrounding background when generating and scoring action proposals. Second, the unique characteristics of the RGB frames and optical flow are largely ignored when fusing these two modalities. To address these problems, an Action Coherence Network (ACN) is proposed in this paper. Its core is a new coherence loss which exploits both classification predictions and video content representations to supervise action boundary regression and thus leads to more accurate action localization results. Besides, the proposed ACN explicitly takes into account the specific characteristics of RGB frames and optical flow by training two separate sub-networks, each of which is able to generate modality-specific action proposals independently. Finally, to take advantage of the complementary action proposals generated by two streams, a novel fusion module is introduced to reconcile them and obtain the final action localization results. Experiments on the THUMOS14 and ActivityNet datasets show that our ACN outperforms the state-of-the-art W-TAL methods, and is even comparable to some recent fully-supervised methods. Particularly, ACN achieves a mean average precision of 26.4% on the THUMOS14 dataset under the IoU threshold 0.5.
C1 [Zhai, Yuanhao; Wang, Le; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
   [Tang, Wei] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
   [Zhang, Qilin] ABB Corp Res Ctr, Raleigh, NC 27606 USA.
   [Hua, Gang] Wormpex AI Res, Bellevue, WA 98004 USA.
C3 Xi'an Jiaotong University; University of Illinois System; University of
   Illinois Chicago; University of Illinois Chicago Hospital; ABB
RP Wang, L (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
EM yuanhaozhai@gmail.com; lewang@mail.xjtu.edu.cn; tangw@uic.edu;
   samqzhang@gmail.com; nnzheng@mail.xjtu.edu.cn; ganghua@gmail.com
RI Zhang, Qilin/HOF-0365-2023; Zhai, Yuanhao/ABG-7095-2020; Zhang,
   Qilin/B-5171-2018
OI Zhai, Yuanhao/0000-0002-3277-3329; Zhang, Qilin/0000-0002-7917-9749;
   Wang, Le/0000-0001-6636-6396
FU National Key R and D Program of China [2018AAA0101400]; NSFC [62088102,
   61773312, 61976171]; Young Elite Scientists Sponsorship Program by CAST
   [2018QNRC001]; Natural Science Foundation of Shaanxi [2020JQ-069]
FX This work was supported in part by National Key R and D Program of China
   under Grant 2018AAA0101400, in part by NSFC under Grants 62088102,
   61773312, and 61976171, in part by Young Elite Scientists Sponsorship
   Program by CAST under Grant 2018QNRC001, and in part by the Natural
   Science Foundation of Shaanxi under Grant 2020JQ-069.
CR Alwassel H, 2018, LECT NOTES COMPUT SC, V11213, P253, DOI 10.1007/978-3-030-01240-3_16
   [Anonymous], 2018, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2017.2712608
   Black M.J, 2018, GCPR
   Buch S., 2019, P BRIT MACH VIS C
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao Jiyang, 2017, BMVC
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Heilbron FC, 2017, PROC CVPR IEEE, P3175, DOI 10.1109/CVPR.2017.338
   Henderson P, 2017, LECT NOTES COMPUT SC, V10115, P198, DOI 10.1007/978-3-319-54193-8_13
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jiang Y.-G., 2014, THUMOS CHALLENGE ACT, V1, P2
   Karaman S, 2014, ECCV THUMOS Workshop, V1, P5
   Korbar B, 2019, IEEE I CONF COMP VIS, P6241, DOI 10.1109/ICCV.2019.00633
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu Z., 2021, P AAAI C ART INT
   Liu ZY, 2019, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2019.00400
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Luo CX, 2019, IEEE I CONF COMP VIS, P5511, DOI 10.1109/ICCV.2019.00561
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Ma Z, 2013, IEEE T MULTIMEDIA, V15, P1628, DOI 10.1109/TMM.2013.2264928
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Paszke A, 2019, ADV NEUR IN, V32
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Piergiovanni AJ, 2019, PROC CVPR IEEE, P9937, DOI 10.1109/CVPR.2019.01018
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Richard A, 2016, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2016.341
   Satkin S, 2010, LECT NOTES COMPUT SC, V6311, P536, DOI 10.1007/978-3-642-15549-9_39
   Schindler K, 2008, PROC CVPR IEEE, P3025
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2019, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2019.00136
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, P ECCV
   Wang L, 2019, IEEE I CONF COMP VIS, P8697, DOI 10.1109/ICCV.2019.00879
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Xu YL, 2019, AAAI CONF ARTIF INTE, P9070
   Yang K, 2018, AAAI CONF ARTIF INTE, P7477
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337
   Yuan ZH, 2017, PROC CVPR IEEE, P3215, DOI 10.1109/CVPR.2017.342
   Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhai YH, 2019, IEEE IMAGE PROC, P3696, DOI [10.1109/ICIP.2019.8803447, 10.1109/icip.2019.8803447]
   Zhang H, 2019, IEEE T MULTIMEDIA, V21, P1450, DOI 10.1109/TMM.2018.2884478
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhong JX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P35, DOI 10.1145/3240508.3240511
NR 76
TC 13
Z9 13
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1857
EP 1870
DI 10.1109/TMM.2021.3073235
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200007
DA 2024-07-18
ER

PT J
AU Fang, YM
   Yan, JB
   Du, RG
   Zuo, YF
   Wen, WY
   Zeng, Y
   Li, LD
AF Fang, Yuming
   Yan, Jiebin
   Du, Rengang
   Zuo, Yifan
   Wen, Wenying
   Zeng, Yan
   Li, Leida
TI Blind Quality Assessment for Tone-Mapped Images by Analysis of Gradient
   and Chromatic Statistics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Degradation; Distortion; Feature extraction; Quality
   assessment; Histograms; Dynamic range; High dynamic range; tone-mapped
   images; blind quality assessment; gradient correlation; local binary
   pattern
AB A tone-mapped image (TMI) obtained from the corresponding high dynamic range (HDR) image induces artifacts and distortion, which might result in the loss of structure information and impaired color. By analyzing the visual characteristics of TMIs, this work proposes a robust blind visual quality evaluation method for TMIs by using gradient and chromatic statistics (VQGC). First, motivated by the perceptual mechanism that the human visual system (HVS) is sensitive to image structure variation, we employ the gradient features to measure structure degradation in TMIs. To predict structure distortion accurately, we compute the gradient magnitude and orientation to measure image structure variation, and the relative gradient magnitude and orientation are also computed to capture microstructure change. Second, the color invariance descriptors are utilized to capture the visual degradation of colorfulness by local binary pattern (LBP) on four chromatic feature maps. Finally, the gradient and chromatic features are combined together as the final quality-aware feature vector, which is applied to assess the perceptual quality of TMIs by support vector regression (SVR). Comparison experiments show that the performance of the proposed method is better than other existing blind quality assessment methods on public databases.
C1 [Fang, Yuming; Yan, Jiebin; Du, Rengang; Zuo, Yifan; Wen, Wenying; Zeng, Yan] Jiangxi Univ Finance & Econ, Sch Informat Management, Nanchang 330013, Jiangxi, Peoples R China.
   [Li, Leida] Xidian Univ, Sch Artif Intelligence, Xian 710071, Peoples R China.
C3 Jiangxi University of Finance & Economics; Xidian University
RP Li, LD (corresponding author), Xidian Univ, Sch Artif Intelligence, Xian 710071, Peoples R China.
EM fa0001ng@e.ntu.edu.sg; jiebinyan@foxmail.com; 149810742@qq.com;
   Yifan.Zuo-1@student.uts.edu.au; wenyingwen@sina.cn; 614867593@qq.com;
   ldli@xidian.edu.cn
RI Li, Li/AEM-3636-2022; Zuo, Yifan/JVZ-3041-2024; li, li/HII-4157-2022
OI Zuo, Yifan/0000-0003-4980-7211; 
FU National Natural Science Foundation of China [61822109, 61901197,
   61961022]; Fok Ying Tung Education Foundation [161061]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61822109, 61901197, and 61961022, and
   in part by the Fok Ying Tung Education Foundation under Grant 161061.
CR [Anonymous], 2000, EESSI CERTIFICATE PA
   Cadík M, 2008, COMPUT GRAPH-UK, V32, P330, DOI 10.1016/j.cag.2008.04.003
   Chalmers A, 2017, SIGNAL PROCESS-IMAGE, V54, P49, DOI 10.1016/j.image.2017.02.003
   CLARK M, 1989, PATTERN RECOGN, V22, P707, DOI 10.1016/0031-3203(89)90007-1
   Drago Frederic., 2003, Proceedings of ACM SIGGRAPH 2003 Sketches Applications, P1
   Fang YM, 2019, J VIS COMMUN IMAGE R, V60, P140, DOI 10.1016/j.jvcir.2018.12.035
   Fang YM, 2018, IEEE T IMAGE PROCESS, V27, P1600, DOI 10.1109/TIP.2017.2781307
   Fang YM, 2017, IEEE T IMAGE PROCESS, V26, P2016, DOI 10.1109/TIP.2017.2669840
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Hadizadeh H, 2018, IEEE T MULTIMEDIA, V20, P392, DOI 10.1109/TMM.2017.2740023
   Hoefflinger B, 2007, SPR SER ADV MICROELE, V26, P1, DOI 10.1007/978-3-540-44433-6
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Hughes HC, 1996, J COGNITIVE NEUROSCI, V8, P197, DOI 10.1162/jocn.1996.8.3.197
   Jiang GY, 2018, IEEE ACCESS, V6, P2231, DOI 10.1109/ACCESS.2017.2782320
   Khan IR, 2018, IEEE T IND ELECTRON, V65, P3469, DOI 10.1109/TIE.2017.2760247
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P4725, DOI 10.1109/TIP.2017.2713945
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   Lee D, 2016, IEEE T IMAGE PROCESS, V25, P3875, DOI 10.1109/TIP.2016.2579308
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Liu XM, 2019, IEEE T IMAGE PROCESS, V28, P1068, DOI 10.1109/TIP.2018.2872175
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Margulis D., 2005, Photoshop LAB Color: The Canyon Conundrum and Other Adventures in the Most Powerful Colorspace
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Montesinos P, 1998, INT C PATT RECOG, P838, DOI 10.1109/ICPR.1998.711280
   Nafchi HZ, 2015, IEEE SIGNAL PROC LET, V22, P1026, DOI 10.1109/LSP.2014.2381458
   Nasrinpour HR, 2015, IEEE IMAGE PROC, P4947, DOI 10.1109/ICIP.2015.7351748
   NOTHDURFT HC, 1985, VISION RES, V25, P1957, DOI 10.1016/0042-6989(85)90020-3
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Raman S., 2009, P 26 INT C MACHINE L, P1
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Schlkopf B., 2002, ADAPT COMPUT MACH LE
   Sheskin J.D., 2004, Handbook of Parametric and Nonparametric Statistical Procedures, VThird
   Song Y., 2016, APPL OPTICS, V55
   van de Weijer J, 2006, IEEE T PATTERN ANAL, V28, P150, DOI 10.1109/TPAMI.2006.3
   van de Weijer J, 2006, LECT NOTES COMPUT SC, V3952, P334
   Velmurugan K., 2011, Int. J. Comput. Appl, V24, P6, DOI 10.5120/2968-3968
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu JJ, 2015, IEEE T IMAGE PROCESS, V24, P4602, DOI 10.1109/TIP.2015.2460467
   Wu QB, 2018, IEEE T IMAGE PROCESS, V27, P2499, DOI 10.1109/TIP.2018.2799331
   Xie LJ, 2016, IEEE INT SYMP CIRC S, P2218, DOI 10.1109/ISCAS.2016.7539023
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yanping Lu, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457862
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Yue GH, 2019, IEEE T IND ELECTRON, V66, P3784, DOI 10.1109/TIE.2018.2851984
   Zhou Y, 2018, IEEE T MULTIMEDIA, V20, P3019, DOI 10.1109/TMM.2018.2829607
NR 52
TC 20
Z9 23
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 955
EP 966
DI 10.1109/TMM.2020.2991528
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300010
DA 2024-07-18
ER

PT J
AU Hu, YF
   Gao, JY
   Xu, CS
AF Hu, Yufan
   Gao, Junyu
   Xu, Changsheng
TI Learning Dual-Pooling Graph Neural Networks for Few-Shot Video
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Feature extraction; Training; Testing; Streaming media;
   Data models; Semantics; Few-shot learning; graph neural networks; video
   classification
ID ACTION RECOGNITION
AB We address the problem of few-shot video classification that learns classifiers for novel concepts from only a few examples. Most current methods ignore to explicitly consider the relations in both intra-video and inter-video domains, thus cannot take full advantage of the structural information in few-shot learning. In this paper, we propose to exploit the comprehensive intra-video and inter-video relations via Graph Neural Networks (GNNs). To improve the discriminative ability for accurately selecting the representative video content and refining video relations, a Dual-Pooling GNN (DPGNN) is constructed, which stacks customized graph pooling layers in a hierarchical fashion. Specifically, to select the most representative frames in a video, we build intra-video graphs and utilize a node pooling module to extract robust video-level features. We construct an inter-video graph by taking the video-level features as nodes. By designing an edge pooling module, the proposed method can adaptively eliminate the negative relations in the inter-video graph. Extensive experimental results show that our method consistently outperforms the state-of-the-art on two benchmarks.
C1 [Hu, Yufan] Hefei Univ Technol, Hefei 230009, Peoples R China.
   [Hu, Yufan; Gao, Junyu; Xu, Changsheng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Gao, Junyu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Gao, Junyu; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100864, Peoples R China.
C3 Hefei University of Technology; Peng Cheng Laboratory; Chinese Academy
   of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM huyufanqaixuan@gmail.com; gaojunyu2015@ia.ac.cn; csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; Gao, Junyu/HDO-5516-2022; xu, cj/HJZ-3488-2023
OI xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2018AAA0100604];
   National Natural Science Foundation of China [61720106006,
   6207245561721004, 61832002, 61702511, 61751211, 61532009, U1836220,
   U1705262, 61872424]; Key Research Program of Frontier Sciences of CAS
   [QYZDJSSWJSC039]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0100604, in part by the
   National Natural Science Foundation of China under Grants 61720106006,
   6207245561721004, 61832002, 61702511, 61751211, 61532009, U1836220,
   U1705262, and 61872424, and in part by the Key Research Program of
   Frontier Sciences of CAS under Grant QYZDJSSWJSC039.
CR [Anonymous], 2016, INT C LEARNING REPRE
   Cao Kaidi, 2019, ARXIV190611415
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen SH, 2015, IEEE T SIGNAL PROCES, V63, P6510, DOI 10.1109/TSP.2015.2469645
   Defferrard M, 2016, ADV NEUR IN, V29
   Devito Z., 2017, NEURIPS WORKSHOPS, P1
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan JP, 2004, IEEE T MULTIMEDIA, V6, P70, DOI 10.1109/TMM.2003.819583
   Finn C, 2017, PR MACH LEARN RES, V70
   Gao H., 2019, PR MACH LEARN RES, P2083
   Gao J., 2020, IEEE T PATTERN ANAL
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Gao JY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P690, DOI 10.1145/3240508.3240566
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Gao JY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P127, DOI 10.1145/3123266.3123433
   Gao JY, 2018, IEEE T IMAGE PROCESS, V27, P3074, DOI 10.1109/TIP.2018.2813166
   Gardner MW, 1998, ATMOS ENVIRON, V32, P2627, DOI 10.1016/S1352-2310(97)00447-0
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Gori M, 2005, IEEE IJCNN, P729
   Grant E., 2018, 6 INT C LEARN REPR
   Güder M, 2018, MULTIMEDIA SYST, V24, P55, DOI 10.1007/s00530-017-0535-z
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henaff M., 2015, ARXIV150605163
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, CORR ABS170506950
   Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010
   King DB, 2015, ACS SYM SER, V1214, P1
   Klaser A., 2008, BMVC 2008 19 BRIT MA, P1
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee J, 2019, PR MACH LEARN RES, V97
   Lee Y, 2018, PR MACH LEARN RES, V80
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Li Y., 2016, P 4 INT C LEARNING R
   Liu J, 2017, MULTIMEDIA SYST, V23, P95, DOI 10.1007/s00530-015-0455-8
   Mishra N., 2017, ICLR
   Morris C, 2019, AAAI CONF ARTIF INTE, P4602
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Nichol A., 2018, ARXIV180302999
   Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29
   Rusu A.A., 2018, P INT C LEARN REPR V
   Sandryhaila A, 2013, IEEE T SIGNAL PROCES, V61, P1644, DOI 10.1109/TSP.2013.2238935
   Santoro A, 2016, PR MACH LEARN RES, V48
   Satorras V.G., 2018, ICLR
   Schlichtkrull Michael, 2018, PROC EUR SEMANTIC WE, P593
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Simonyan K., 2014, 14091556 ARXIV
   Simonyan K, 2014, ADV NEUR IN, V27
   Snell J, 2017, ADV NEUR IN, V30
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Ullah H., 2020, MUL TIMEDIA SYSTEMS, P1
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang H., 2009, BMVC
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang JZ, 2018, IEEE T MULTIMEDIA, V20, P2578, DOI 10.1109/TMM.2018.2855081
   Wang L., 2016, P ECCV
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wu L, 2020, IEEE T IMAGE PROCESS, V29, P1233, DOI 10.1109/TIP.2019.2940684
   Yang XS, 2015, MULTIMEDIA SYST, V21, P133, DOI 10.1007/s00530-014-0376-y
   Ying Zhitao., 2018, Advances in Neural Information Processing Systems, P4800
   Zhang CJ, 2020, IEEE T CIRC SYST VID, V30, P2867, DOI 10.1109/TCSVT.2019.2920783
   Zhang RX, 2018, ADV NEUR IN, V31
   Zhu LC, 2018, LECT NOTES COMPUT SC, V11211, P782, DOI 10.1007/978-3-030-01234-2_46
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 73
TC 21
Z9 22
U1 3
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4285
EP 4296
DI 10.1109/TMM.2020.3039329
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900029
DA 2024-07-18
ER

PT J
AU Huang, ZQ
   Liu, SG
AF Huang, Ziqing
   Liu, Shiguang
TI Perceptual Image Hashing With Texture and Invariant Vector Distance for
   Copy Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Robustness; Discrete cosine transforms; Image color
   analysis; Databases; Law; Copy detection; perceptual hashing;
   robustness; discrimination; invariant vector distance
ID ROBUST; DESCRIPTORS; SECURE
AB Content-based image copy detection has become one of the important technologies in copyright protection, where two major processes, content-based feature extraction and matching are included. However, it is certainly true that enough storage space is required to establish feature database for matching, which greatly increases time and storage consumption, as well as lacks flexibility. Fortunately, perceptual image hashing is a good strategy to address these problems, in which content-based features are extracted and further encoded to hash codes. On the one hand, content-based features provide and ensure higher copy detection accuracy, while on the other hand, hash codes instead of feature database reduce storage space and improve time efficiency. Meanwhile, a better balance between robustness and discrimination is one of the most objectives of image hashing, which is conducive to its application in multimedia management and security. Consequently, we present an effective image hashing method for copy detection. Specifically, to obtain perceptual robustness against to copy attacks, we extract the global statistical characteristics in gray-level co-occurrence matrix (GLCM) to reveal texture changes. Then, to make up the discrimination limitation, we leverage the local dominant DCT coefficients from the first row/column in each sub-image to calculate vector distance. Finally, two kinds of complementary information (global feature via texture and local feature via vector distance) are simultaneously preserved to generate hash codes. Various experiments performed on benchmark database indicate that our proposed perceptual image hashing provides higher detection accuracy and better balance between robustness and discrimination than the state-of-the-art algorithms.
C1 [Huang, Ziqing; Liu, Shiguang] Tianjin Univ, Coll Intelligence & Comp, Sch Comp Sci & Technol, Tianjin 300350, Peoples R China.
   [Liu, Shiguang] Tianjin Univ, Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300350, Peoples R China.
C3 Tianjin University; Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Sch Comp Sci & Technol, Tianjin 300350, Peoples R China.
EM skyhuangzq@163.com; lsg@tju.edu.cn
FU Natural Science Foundation of China [61672375, 61170118]
FX This work was supported by the Natural Science Foundation of China under
   Grants 61672375 and 61170118. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Benoit Huet.
CR Ahmed F, 2010, SIGNAL PROCESS, V90, P1456, DOI 10.1016/j.sigpro.2009.05.024
   Amsaleg L, 2001, PATTERN ANAL APPL, V4, P108, DOI 10.1007/s100440170011
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Chen Y., 2017, COMPUT VISION IMAGE, V26, P1
   Davarzani R, 2016, MULTIMED TOOLS APPL, V75, P4639, DOI 10.1007/s11042-015-2496-6
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hsiao JH, 2007, IEEE T IMAGE PROCESS, V16, P2069, DOI 10.1109/TIP.2007.900099
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Kang LW, 2010, IEEE INT CON MULTI, P1248, DOI 10.1109/ICME.2010.5582615
   Ke Y, 2004, PROC CVPR IEEE, P506
   Khelifi F, 2010, IEEE T IMAGE PROCESS, V19, P981, DOI 10.1109/TIP.2009.2038637
   Kim C, 2003, SIGNAL PROCESS-IMAGE, V18, P169, DOI 10.1016/S0923-5965(02)00130-3
   Kozat SS, 2004, IEEE IMAGE PROC, P3443, DOI 10.1109/ICIP.2004.1421855
   Li YN, 2012, IEEE T IMAGE PROCESS, V21, P1963, DOI 10.1109/TIP.2011.2171698
   Li Z., 2009, P 1 ACM WORKSH LARG, P65, DOI DOI 10.1145/1631058.1631072
   Lin CY, 2001, IEEE T CIRC SYST VID, V11, P153, DOI 10.1109/76.905982
   Ling HF, 2013, SIGNAL PROCESS, V93, P2328, DOI 10.1016/j.sigpro.2012.08.011
   Ling HF, 2012, IEEE MULTIMEDIA, V19, P60, DOI 10.1109/MMUL.2011.75
   Ling HF, 2011, MULTIMED TOOLS APPL, V52, P551, DOI 10.1007/s11042-009-0439-9
   Lv XD, 2012, IEEE T INF FOREN SEC, V7, P1081, DOI 10.1109/TIFS.2012.2190594
   Lv XD, 2009, EURASIP J INF SECUR, DOI 10.1155/2009/859859
   Monga V, 2007, IEEE T INF FOREN SEC, V2, P376, DOI 10.1109/TIFS.2007.902670
   Monga V, 2006, IEEE T IMAGE PROCESS, V15, P3452, DOI 10.1109/TIP.2006.881948
   Nie XS, 2018, IEEE T INF FOREN SEC, V13, P1509, DOI 10.1109/TIFS.2018.2790953
   Podilchuk CI, 2001, IEEE SIGNAL PROC MAG, V18, P33, DOI 10.1109/79.939835
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Schneider M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P227, DOI 10.1109/ICIP.1996.560425
   Shijun Xiang, 2010, Proceedings 2010 Second International Conference on Multimedia Information Networking and Security (MINES 2010), P653, DOI 10.1109/MINES.2010.142
   Tang ZJ, 2017, SIGNAL PROCESS, V137, P240, DOI 10.1016/j.sigpro.2017.02.008
   Tang ZJ, 2016, COMPUT SECUR, V62, P133, DOI 10.1016/j.cose.2016.07.006
   Tang ZJ, 2016, AEU-INT J ELECTRON C, V70, P833, DOI 10.1016/j.aeue.2016.03.010
   Tang ZJ, 2015, DIGIT SIGNAL PROCESS, V43, P17, DOI 10.1016/j.dsp.2015.05.002
   Tang ZJ, 2014, OPTIK, V125, P5102, DOI 10.1016/j.ijleo.2014.05.015
   Tang ZJ, 2014, IEEE T KNOWL DATA EN, V26, P711, DOI 10.1109/TKDE.2013.45
   Trigeorgis G, 2014, PR MACH LEARN RES, V32, P1692
   Venkatesan R, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P664, DOI 10.1109/ICIP.2000.899541
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Woods R. E., 2007, DIG IMAGE PROCESSING, V3rd
   Xu ZH, 2011, MULTIMED TOOLS APPL, V52, P445, DOI 10.1007/s11042-009-0438-x
   Yan CP, 2016, IEEE T INF FOREN SEC, V11, P2664, DOI 10.1109/TIFS.2016.2594136
   Zhao K, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1089, DOI 10.1145/2647868.2654971
   Zhou ZL, 2017, IEEE T INF FOREN SEC, V12, P48, DOI 10.1109/TIFS.2016.2601065
   Zhou ZL, 2016, IEICE T INF SYST, VE99D, P1531, DOI 10.1587/transinf.2015EDP7341
   Zhu XF, 2017, IEEE T MULTIMEDIA, V19, P2033, DOI 10.1109/TMM.2017.2703636
   Zwiggelaar R., P INT C COMP VIS, P1
NR 47
TC 24
Z9 26
U1 2
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1516
EP 1529
DI 10.1109/TMM.2020.2999188
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300005
DA 2024-07-18
ER

PT J
AU Lin, XM
   Li, R
   Zheng, XW
   Peng, P
   Wu, YJ
   Huang, FY
   Ji, RR
AF Lin, Xianming
   Li, Run
   Zheng, Xiawu
   Peng, Pai
   Wu, Yongjian
   Huang, Feiyue
   Ji, Rongrong
TI Aggregating Global and Local Visual Representation for Vehicle
   Re-IDentification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Visualization; Inspection; Image color analysis; Lighting;
   Vehicles; Vehicle Re-ID; aggregation of multilevel features; MCMS-Siam
   network
ID LICENSE PLATE-RECOGNITION; TRACKING
AB Vehicle Re-Identification targets at searching for vehicle instances of the same identity with a given query. It has gained an increasing attention recently with a wide application prospects in video surveillance and intelligent transportation. The main challenge lies in how to distinguish the subtle differences between different vehicles, while capturing the slight similarity between the instance of the same vehicle in different viewpoints or illuminations. To this end, most existing methods focus on learning discriminative global representations, which leave the unique local details such as stickers, inspection labels and driver wearing being unexploited. In this paper, we present a novel coarse-to-fine scheme that aggregates global and local visual representations to boost the retrieval accuracy. Specifically, a multi-task learning framework combining an Attribute Learning branch and a Deep Ranking branch (termed ALDR) is first adopted to learn robust global features, which produces an initial ranking list. Then the local similarities between image patches in the initial ranking list and in the query is computed via a Multi-Channel and Multi-Scale Siamese network (termed MCMS-Siam). Finally, the retrieval result is returned after re-ranking the initial list according to such a combination of global and local similarities. Experimental results on the widely-used VehicleID dataset and VECH-WILD dataset demonstrate the merits of the proposed method over the state-of-the-art methods.
C1 [Lin, Xianming; Li, Run; Zheng, Xiawu; Ji, Rongrong] Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
   [Peng, Pai; Wu, Yongjian; Huang, Feiyue] Tencent Technol Shanghai Co Ltd, Youtu Lab, Shanghai 361005, Peoples R China.
C3 Xiamen University
RP Lin, XM (corresponding author), Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
EM linxm@xmu.edu.cn; lirunchh@gmail.com; zhengxiawu@stu.xmu.edu.cn;
   popeyepeng@tencent.com; littlekenwu@tencent.com; garyhuang@tencent.com;
   jirongrong@gmail.com
OI Lin, Xianming/0000-0003-4739-8936
FU National Natural Science Foundation of China [U1705262, 62072386,
   62072387, 62072389, 62002305, 61772443, 61802324, 61702136]
FX This work is supported by National Natural Science Foundation of China
   (Nos. U1705262, 62072386, 62072387, 62072389, 62002305, 61772443,
   61802324, and 61702136).
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Anagnostopoulos CNE, 2006, IEEE T INTELL TRANSP, V7, P377, DOI 10.1109/TITS.2006.880641
   Anagnostopoulos CNE, 2008, IEEE T INTELL TRANSP, V9, P377, DOI 10.1109/TITS.2008.922938
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Bai Y, 2017, IEEE INT CON MULTI, P1452, DOI 10.1109/ICME.2017.8019371
   Betke M, 2000, MACH VISION APPL, V12, P69, DOI 10.1007/s001380050126
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Chang SL, 2004, IEEE T INTELL TRANSP, V5, P42, DOI 10.1109/TITS.2004.825086
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Coifman B, 2002, TRANSPORT RES A-POL, V36, P899, DOI 10.1016/S0965-8564(01)00046-5
   Doulamis ND, 2010, MULTIMED TOOLS APPL, V50, P173, DOI 10.1007/s11042-009-0370-0
   Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirzer M, 2012, LECT NOTES COMPUT SC, V7577, P780, DOI 10.1007/978-3-642-33783-3_56
   Howard A. G., 2017, PREPRINT
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Khorramshahi P, 2019, IEEE I CONF COMP VIS, P6131, DOI 10.1109/ICCV.2019.00623
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin WYD, 2014, LECT NOTES COMPUT SC, V8692, P341, DOI 10.1007/978-3-319-10593-2_23
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu XC, 2016, IEEE INT CON MULTI
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun C, 1999, TRANSPORT RES C-EMER, V7, P167, DOI 10.1016/S0968-090X(99)00018-2
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Yan K, 2017, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2017.68
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Yuan YH, 2017, IEEE I CONF COMP VIS, P814, DOI 10.1109/ICCV.2017.94
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang SH, 2017, IEEE I CONF COMP VIS, P3687, DOI 10.1109/ICCV.2017.396
   Zhang XF, 2016, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR.2016.126
   Zheng XW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1226
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 47
TC 5
Z9 5
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3968
EP 3977
DI 10.1109/TMM.2020.3035279
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900004
DA 2024-07-18
ER

PT J
AU Liu, F
   Liu, J
   Fang, ZW
   Hong, RC
   Lu, HQ
AF Liu, Fei
   Liu, Jing
   Fang, Zhiwei
   Hong, Richang
   Lu, Hanqing
TI Visual Question Answering With Dense Inter- and Intra-Modality
   Interactions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Knowledge discovery; Connectors; Encoding; Task analysis;
   Image coding; Stacking; Visual question answering; attention; dense
   interactions
AB Learning effective interactions between multi-modal features is at the heart of visual question answering (VQA). A common defect of the existing VQA approaches is that they only consider a very limited amount of inter-modality interactions, which may be not enough to model latent complex image-question relations that are necessary for accurately answering questions. Besides, most methods neglect the modeling of the intra-modality interactions that is also important to VQA. In this work, we propose a novel DenIII framework for modeling dense inter-, and intra-modality interactions. It densely connects all pairwise layers of the network via the proposed Inter-, and Intra-modality Attention Connectors, capturing fine-grained interplay across all hierarchical levels. The Inter-modality Attention Connector efficiently connects the multi-modality features at any two layers with bidirectional attention, capturing the inter-modality interactions. While the Intra-modality Attention Connector connects the features of the same modality with unidirectional attention, and models the intra-modality interactions. Extensive ablation studies, and visualizations validate the effectiveness of our method, and DenIII achieves state-of-the-art or competitive performance on three publicly available datasets.
C1 [Liu, Fei; Liu, Jing; Fang, Zhiwei; Lu, Hanqing] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Liu, Fei; Liu, Jing; Fang, Zhiwei; Lu, Hanqing] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Hong, Richang] Hefei Univ Technol, Sch Comp & Informat, Hefei 230000, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Hefei University of Technology
RP Liu, J (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM liufei2017@ia.ac.cn; jliu@nlpr.ia.ac.cn; zhiwei.fang@nlpr.ia.ac.cn;
   hongrc.hfut@gmail.com; luhq@nlpr.ia.ac.cn
RI Liu, Jing/A-7644-2016; Liu, Feifei/AAP-9241-2021
OI Liu, Feifei/0000-0003-1966-8759; liu, jing/0000-0003-0903-9131
FU Beijing Natural Science Foundation [4192059, JQ20022]; National Natural
   Science Foundation of China [61922086, 61872366, 61872364]
FX This work was supported by Beijing Natural Science Foundation (4192059,
   JQ20022), and the National Natural Science Foundation of China
   underGrants 61922086, 61872366, and 61872364. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Guo-Jun Qi.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], Simple baseline for visual question answering
   [Anonymous], 2018, P INT C LEARN REPR
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bai YL, 2018, LECT NOTES COMPUT SC, V11216, P21, DOI 10.1007/978-3-030-01258-8_2
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Chang SY, 2014, IEEE DATA MINING, P60, DOI 10.1109/ICDM.2014.115
   Chang SY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P119, DOI 10.1145/2783258.2783296
   Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6
   Chen K., 2015, Abc-cnn: An attention based convolutional neural network for visual question answering
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Farajollahi M., 2018, 2018 IEEE Power Energy Society General Meeting (PESGM), P1, DOI 10.1109/PESGM.2018.8586273
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao H., 2015, INT C NEURAL INF PRO, P2296
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Gao P, 2018, LECT NOTES COMPUT SC, V11205, P485, DOI 10.1007/978-3-030-01246-5_29
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He TY, 2018, ADV NEUR IN, V31
   Huang FR, 2019, IEEE T IMAGE PROCESS, V28, P2008, DOI 10.1109/TIP.2018.2882225
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44
   Jiang Y., 2018, arXiv
   Jun SH, 2017, ICEC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ELECTRONIC COMMERCE, DOI 10.1145/3154943.3154947
   Kafle K, 2017, IEEE I CONF COMP VIS, P1983, DOI 10.1109/ICCV.2017.217
   Kim JH, 2016, ADV NEUR IN, V29
   Kim JH, 2018, ADV NEUR IN, V31
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li H., 2018, P BRIT MACH VIS C
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu F., 2018, 2018 IEEE 4 INT C, P1
   Liu F, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P869
   Liu F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1175, DOI 10.1145/3343031.3350993
   Liu F, 2019, IEEE IMAGE PROC, P3307, DOI [10.1109/icip.2019.8803670, 10.1109/ICIP.2019.8803670]
   Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d
   Lu JS, 2016, ADV NEUR IN, V29
   Lu P, 2018, AAAI CONF ARTIF INTE, P7218
   Luong M.-T., 2015, P 2015 C EMPIRICAL M, DOI DOI 10.18653/V1/D15-1166
   Ma C, 2018, PROC CVPR IEEE, P6975, DOI 10.1109/CVPR.2018.00729
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qi G.-J., 2012, Proc. of the 5th ACM Intl. Conf. on Web Search and Data Mining (WSDM), P553, DOI DOI 10.1145/2124295.2124363
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Qi GJ, 2013, PROC INT CONF DATA, P793, DOI 10.1109/ICDE.2013.6544875
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi Y, 2018, LECT NOTES COMPUT SC, V11208, P158, DOI 10.1007/978-3-030-01225-0_10
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Su Z, 2018, PROC CVPR IEEE, P7736, DOI 10.1109/CVPR.2018.00807
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Teney D, 2018, PROC CVPR IEEE, P4223, DOI 10.1109/CVPR.2018.00444
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JD, 2012, INFORM RETRIEVAL, V15, P278, DOI 10.1007/s10791-012-9193-0
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu CF, 2018, ADV NEUR IN, V31
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhang Y., 2018, 6 INT C LEARN REPR I
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
NR 71
TC 23
Z9 23
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3518
EP 3529
DI 10.1109/TMM.2020.3026892
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100007
DA 2024-07-18
ER

PT J
AU Ma, C
   Lu, JW
   Zhou, J
AF Ma, Cheng
   Lu, Jiwen
   Zhou, Jie
TI Rank-Consistency Deep Hashing for Scalable Multi-Label Image Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Semantics; Binary codes; Task analysis; Neural
   networks; Quantization (signal); Training; Hashing; multi-label; image
   retrieval; rank-consistency; deep neural network
ID REPRESENTATION; ALGORITHMS; SCALE; CODES
AB As hashing becomes an increasingly appealing technique for large-scale image retrieval, multi-label hashing is also attracting more attention for the ability to exploit multi-level semantic contents. In this paper, we propose a novel deep hashing method for scalable multi-label image search. Unlike existing approaches with conventional objectives such as contrast and triplet losses, we employ a rank list, rather than pairs or triplets, to provide sufficient global supervision information for all the samples. Specifically, a new rank-consistency objective is applied to align the similarity orders from two spaces, the original space and the hamming space. A powerful loss function is designed to penalize the samples whose semantic similarity and hamming distance are mismatched in two spaces. Besides, a multi-label softmax cross-entropy loss is presented to enhance the discriminative power with a concise formulation of the derivative function. In order to manipulate the neighborhood structure of the samples with different labels, we design a multi-label clustering loss to cluster the hashing vectors of the samples with the same labels by reducing the distances between the samples and their multiple corresponding class centers. The state-of-the-art experimental results achieved on three public multi-label datasets, MIRFLICKR-25K, IAPRTC12 and NUS-WIDE, demonstrate the effectiveness of the proposed method.
C1 [Ma, Cheng; Lu, Jiwen] Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China.
   [Ma, Cheng; Lu, Jiwen] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Zhou, Jie] Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Dept Automat, Beijing 100084, Peoples R China.
   [Zhou, Jie] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
C3 Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua
   Shenzhen International Graduate School; Tsinghua University
RP Lu, JW (corresponding author), Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China.; Lu, JW (corresponding author), Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
EM macheng17@mails.tsinghua.edu.cn; lujiwen@tsinghua.edu.cn;
   jzhou@tsinghua.edu.cn
RI Lu, Jiwen/C-5291-2009
OI Lu, Jiwen/0000-0002-6121-5529
FU National Key Research and Development Program of China [2017YFA0700802];
   National Natural Science Foundation of China [61822603, U1813218,
   U1713214, 61672306]; Beijing Academy of Artificial Intelligence (BAAI)
   [BAAI2020ZJ0202]; Institute for Guo Qiang, Tsinghua University; Shenzhen
   Fundamental Research Fund [JCYJ20170412170602564]; Tsinghua University
   Initiative Scientific Research Program
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFA0700802, in part by the
   National Natural Science Foundation of China under Grants 61822603,
   U1813218, U1713214, and 61672306, in part by Beijing Academy of
   Artificial Intelligence (BAAI) under Grant BAAI2020ZJ0202, in part by a
   grant from the Institute for Guo Qiang, Tsinghua University, in part by
   the Shenzhen Fundamental Research Fund (Subject Arrangement) under Grant
   JCYJ20170412170602564, and in part by Tsinghua University Initiative
   Scientific Research Program. The associate editor coordinating the
   review of this manuscript and approving it for publication was Marco
   Bertini. (Corresponding author: Jiwen Lu.)
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2014, DISCRETE GRAPH HASHI
   [Anonymous], 2016, IJCAI
   [Anonymous], 2009, NIPS
   [Anonymous], 2015, IEEE T IMAGE PROCESS, DOI DOI 10.1109/TIP.2015.2467315
   Burges C., 2005, ICML, P89
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Cao Z., 2007, P 24 INT C MACHINE L, P129, DOI DOI 10.1145/1273496.1273513
   Chen BL, 2018, IEEE INT WORKSH COMP, P1, DOI 10.1080/00207160.2018.1478415
   Chen ZX, 2018, PROC CVPR IEEE, P6838, DOI 10.1109/CVPR.2018.00715
   Chen ZX, 2018, LECT NOTES COMPUT SC, V11070, P620, DOI 10.1007/978-3-030-00928-1_70
   Chen ZX, 2018, IEEE T CIRC SYST VID, V28, P1421, DOI 10.1109/TCSVT.2017.2669095
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Friedman J, 2018, SLACPUB1549
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gu SM, 2013, INT CONF MACH LEARN, P108, DOI 10.1109/ICMLC.2013.6890453
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Guttman Antonin., 1984, P 1984 ACM SIGMOD C, P47
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Huang CQ, 2018, IEEE T IMAGE PROCESS, V27, P4490, DOI 10.1109/TIP.2018.2839522
   Jain P, 2008, PROC CVPR IEEE, P3879
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Ji TX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1005, DOI 10.1145/2647868.2655018
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li Q, 2017, ADV NEUR IN, V30
   Li WJ, 2016, IJCAI, P1711
   Lin GS, 2015, IEEE T PATTERN ANAL, V37, P2317, DOI 10.1109/TPAMI.2015.2404776
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liong VE, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Liong VE, 2018, PATTERN RECOGN, V79, P114, DOI 10.1016/j.patcog.2018.02.002
   Liong VE, 2017, IEEE IMAGE PROC, P3700, DOI 10.1109/ICIP.2017.8296973
   Liong VE, 2017, IEEE I CONF COMP VIS, P4097, DOI 10.1109/ICCV.2017.439
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu TY, 2011, LEARNING TO RANK FOR INFORMATION RETRIEVAL, P1, DOI 10.1007/978-3-642-14267-3
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu XL, 2015, IEEE I CONF COMP VIS, P1107, DOI 10.1109/ICCV.2015.132
   Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P2352, DOI 10.1109/TIP.2017.2678163
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Severyn A, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P373, DOI 10.1145/2766462.2767738
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Song DJ, 2015, IEEE I CONF COMP VIS, P1922, DOI 10.1109/ICCV.2015.223
   Nguyen VA, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P989, DOI 10.1145/2647868.2655003
   Wang Jianfeng., 2013, ACM Multimedia, P133
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang J, 2013, IEEE I CONF COMP VIS, P3032, DOI 10.1109/ICCV.2013.377
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang X., 2016, AS C COMP VIS ACCV
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu DY, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P155, DOI 10.1145/3078971.3078989
   Xia F., 2008, P 25 INT C MACH LEAR, P1192, DOI DOI 10.1145/1390156.1390306
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yuan X, 2018, LECT NOTES COMPUT SC, V11208, P141, DOI 10.1007/978-3-030-01225-0_9
   Yuan X, 2018, PATTERN RECOGN, V79, P147, DOI 10.1016/j.patcog.2018.02.003
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zheng WZ, 2019, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.2019.00016
   Zhuang BH, 2016, PROC CVPR IEEE, P5955, DOI 10.1109/CVPR.2016.641
NR 71
TC 7
Z9 7
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3943
EP 3956
DI 10.1109/TMM.2020.3034534
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tang, XW
   Huang, XL
   Hu, F
AF Tang, Xiao-Wei
   Huang, Xin-Lin
   Hu, Fei
TI QoE-Driven UAV-Enabled Pseudo-Analog Wireless Video Broadcast: A Joint
   Optimization of Power and Trajectory
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Quality of experience; Trajectory; Resource management;
   Wireless communication; Unmanned aerial vehicles; PSNR; Unmanned aerial
   vehicles; video broadcast; peak signal-to-noise ratio; and joint power
   and trajectory optimization
ID DESIGN; NETWORKS; COMMUNICATION; TRANSMISSION; ALLOCATION; QUALITY;
   ACCESS; RAN
AB The explosive demands for high quality mobile video services have caused heavy overload to the existing cellular networks. Although the small cell has been proposed to alleviate such a problem, the network operators may not be interested in deploying numerous base stations (BSs) due to expensive infrastructure construction and maintenance. The unmanned aerial vehicles (UAVs) can provide the low-cost and quick deployment, which can support high-quality line-of-sight communications and have become promising mobile BSs. In this paper, we propose a quality-of-experience (QoE)-driven UAV-enabled pseudo-analog wireless video broadcast scheme, which provides mobile video broadcast services for ground users (GUs). Due to limited energy available in UAV, the aim of the proposed scheme is to maximize the minimum peak signal-to-noise ratio (PSNR) of GUs' video reconstruction quality by jointly optimizing the transmission power allocation strategy and the UAV trajectory. Firstly, the reconstructed video quality at GUs is defined under the constraints of the UAV's total energy and motion mechanism, and the proposed scheme is formulated as a complex non-convex optimization problem. Then, the optimization problem is simplified to obtain a tractable suboptimal solution with the help of the block coordinate descent model and the successive convex approximation model. Finally, the experimental results are presented to show the effectiveness of the proposed scheme. Specifically, the proposed scheme can achieve over 1.6 dB PSNR gains in terms of GUs' minimum PSNR, compared with the state-of-the-art schemes, e.g., DVB, SoftCast, and SharpCast.
C1 [Tang, Xiao-Wei] Tongji Univ, Dept Control Sci & Engn, Shanghai 201804, Peoples R China.
   [Huang, Xin-Lin] Tongji Univ, Dept Informat & Commun Engn, Shanghai 201804, Peoples R China.
   [Hu, Fei] Univ Alabama, Dept Elect & Comp Engn, Tuscaloosa, AL 35487 USA.
C3 Tongji University; Tongji University; University of Alabama System;
   University of Alabama Tuscaloosa
RP Huang, XL (corresponding author), Tongji Univ, Dept Informat & Commun Engn, Shanghai 201804, Peoples R China.
EM xwtang@tongji.edu.cn; xlhuang@tongji.edu.cn; fei@eng.ua.edu
FU National Natural Science Foundation of China [U1733114, 61631017];
   Fundamental Research Funds for the Central Universities; Shanghai
   Rising-Star Program [19QA1409100]; Shanghai Science and Technology
   Innovation Action Plan [19DZ1201100]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1733114 and 61631017, in part by the
   Fundamental Research Funds for the Central Universities, in part by
   Shanghai Rising-Star Program under Grant 19QA1409100, and in part by
   Shanghai Science and Technology Innovation Action Plan under Grant
   19DZ1201100. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. M. Wang.
CR Alvear O, 2018, MOBILE NETW APPL, V23, P1693, DOI 10.1007/s11036-018-1065-4
   [Anonymous], 2016, P IEEE 12 INT C WIR
   [Anonymous], 2011, P 45 ANN C INF SCI S
   [Anonymous], [No title captured]
   Atta R, 2018, IEEE T CIRC SYST VID, V28, P2331, DOI 10.1109/TCSVT.2017.2702648
   Bor-Yaliniz I, 2016, IEEE COMMUN MAG, V54, P48, DOI 10.1109/MCOM.2016.1600178CM
   Bor-Yaliniz R. I., 2016, PROC IEEE INT C COMM, P1
   C. V. N. Index,, 2019, CISCO VISUAL NETWORK
   Cai SJ, 2016, IEEE J SEL AREA COMM, V34, P1103, DOI 10.1109/JSAC.2016.2520217
   Chang HB, 2015, IEEE GLOB COMM CONF, DOI 10.1109/GLOCOM.2015.7417022
   Cho Y, 2013, IEEE T CIRC SYST VID, V23, P1003, DOI 10.1109/TCSVT.2013.2248215
   Cicalò S, 2016, IEEE T CIRC SYST VID, V26, P2284, DOI 10.1109/TCSVT.2015.2504732
   Colonnese S, 2017, IEEE COMMUN LETT, V21, P386, DOI 10.1109/LCOMM.2016.2628378
   Dai WR, 2016, IEEE T IMAGE PROCESS, V25, P4580, DOI 10.1109/TIP.2016.2594490
   Elgabli A, 2018, IEEE ACM T NETWORK, V26, P1633, DOI 10.1109/TNET.2018.2844123
   Erdelj M, 2017, COMPUT NETW, V124, P72, DOI 10.1016/j.comnet.2017.05.021
   Fan XP, 2012, IEEE DATA COMPR CONF, P199, DOI 10.1109/DCC.2012.27
   Ferranti L, 2018, AD HOC NETW, V80, P130, DOI 10.1016/j.adhoc.2018.06.004
   Guo WS, 2014, 2014 9TH INTERNATIONAL SYMPOSIUM ON COMMUNICATION SYSTEMS, NETWORKS & DIGITAL SIGNAL PROCESSING (CSNDSP), P658, DOI 10.1109/CSNDSP.2014.6923909
   He C., 2019, SENSORS-BASEL, V19, P1
   He DL, 2015, IEEE T MULTIMEDIA, V17, P1658, DOI 10.1109/TMM.2015.2451956
   He ZF, 2016, IEEE T MULTIMEDIA, V18, P1401, DOI 10.1109/TMM.2016.2564104
   Hu ZW, 2018, IEEE T WIREL COMMUN, V17, P6093, DOI 10.1109/TWC.2018.2854598
   Hua M, 2019, IEEE WIREL COMMUN LE, V8, P769, DOI 10.1109/LWC.2019.2891727
   Huang XL, 2020, IEEE T MULTIMEDIA, V22, P201, DOI 10.1109/TMM.2019.2925960
   Huang XL, 2018, MOBILE NETW APPL, V23, P100, DOI 10.1007/s11036-017-0886-x
   Huang XL, 2017, IEEE T CIRC SYST VID, V27, P6, DOI 10.1109/TCSVT.2016.2555758
   Husemann R, 2018, IEEE MICRO, V38, P8, DOI 10.1109/MM.2018.2880163
   Kua J, 2017, IEEE COMMUN SURV TUT, V19, P1842, DOI 10.1109/COMST.2017.2685630
   Liotou E, 2015, IEEE COMMUN MAG, V53, P145, DOI 10.1109/MCOM.2015.7158278
   Liu SH, 2018, DISCRETE APPL MATH, V241, P48, DOI 10.1016/j.dam.2016.06.028
   Mozaffari M, 2015, IEEE GLOB COMM CONF, DOI 10.1109/GLOCOM.2015.7417609
   Razi P, 2018, IEEE ACCESS, V6, P12395, DOI 10.1109/ACCESS.2018.2804899
   Saha C, 2018, IEEE T COMMUN, V66, P2219, DOI 10.1109/TCOMM.2017.2782741
   Sharma V, 2017, J NETW COMPUT APPL, V85, P94, DOI 10.1016/j.jnca.2016.12.012
   Sun LF, 2018, IEEE T MULTIMEDIA, V20, P3414, DOI 10.1109/TMM.2018.2834861
   Tang XW, 2020, IEEE T VEH TECHNOL, V69, P9896, DOI 10.1109/TVT.2020.3003478
   Tang XW, 2020, MOBILE NETW APPL, V25, P2495, DOI 10.1007/s11036-020-01592-6
   Tang XW, 2018, MOBILE NETW APPL, V23, P318, DOI 10.1007/s11036-017-0949-z
   Wang M, 2018, IEEE T MULTIMEDIA, V20, P620, DOI 10.1109/TMM.2017.2748459
   Wu J, 2015, IEEE WIREL COMMUN, V22, P130, DOI 10.1109/MWC.2015.7143336
   Wu QQ, 2018, IEEE T COMMUN, V66, P6614, DOI 10.1109/TCOMM.2018.2865922
   Wu QQ, 2018, IEEE J SEL AREA COMM, V36, P1955, DOI 10.1109/JSAC.2018.2864421
   Wu QQ, 2018, IEEE T WIREL COMMUN, V17, P2109, DOI 10.1109/TWC.2017.2789293
   Yao RX, 2015, IEEE T MULTIMEDIA, V17, P434, DOI 10.1109/TMM.2015.2394385
   You CS, 2019, IEEE T WIREL COMMUN, V18, P3192, DOI 10.1109/TWC.2019.2911939
   Zeng Y, 2018, IEEE T WIREL COMMUN, V17, P2233, DOI 10.1109/TWC.2018.2790401
   Zeng Y, 2017, IEEE T WIREL COMMUN, V16, P3747, DOI 10.1109/TWC.2017.2688328
   Zeng Y, 2016, IEEE COMMUN MAG, V54, P36, DOI 10.1109/MCOM.2016.7470933
   Zhan C, 2020, IEEE T MULTIMEDIA, V22, P795, DOI 10.1109/TMM.2019.2931441
   Zhang SW, 2019, IEEE T COMMUN, V67, P2580, DOI 10.1109/TCOMM.2018.2880468
   Zhao MC, 2017, SIGNAL PROCESS-IMAGE, V57, P157, DOI 10.1016/j.image.2017.05.015
   Zhao S, 2016, AER ADV ENG RES, V83, P1
   Zhu LY, 2018, INT WIREL COMMUN, P30, DOI 10.1109/IWCMC.2018.8450454
NR 54
TC 15
Z9 16
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2398
EP 2412
DI 10.1109/TMM.2020.3011319
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tu, XG
   Zhao, J
   Xie, M
   Jiang, ZH
   Balamurugan, A
   Luo, Y
   Zhao, Y
   He, LX
   Ma, Z
   Feng, JS
AF Tu, Xiaoguang
   Zhao, Jian
   Xie, Mei
   Jiang, Zihang
   Balamurugan, Akshaya
   Luo, Yao
   Zhao, Yang
   He, Lingxiao
   Ma, Zheng
   Feng, Jiashi
TI 3D Face Reconstruction From A Single Image Assisted by 2D Face Images in
   the Wild
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face; Three-dimensional displays; Solid modeling; Two dimensional
   displays; Image reconstruction; Shape; Task analysis; 3D face
   reconstruction; face alignment; self-supervision
ID ALIGNMENT; MODEL; AUDIO; SHAPE
AB 3D face reconstruction from a single image is an important task in many multimedia applications. Recent works typically learn a CNN-based 3D face model that regresses coefficients of a 3D Morphable Model (3DMM) from 2D images to perform 3D face reconstruction. However, the shortage of training data with 3D annotations considerably limits performance of these methods. To alleviate this issue, we propose a novel 2D-Assisted Learning (2DAL) method that can effectively use "in the wild" 2D face images with noisy landmark information to substantially improve 3D face model learning. Specifically, taking the sparse 2D facial landmark heatmaps as additional information, 2DAL introduces four novel self-supervision schemes that view the 2D landmark and 3D landmark prediction as a self-mapping process, including the landmark self-prediction consistency for 2D and 3D faces respectively, cycle-consistency over the 2D landmark prediction and self-critic over the predicted 3DMM coefficients based on landmark prediction. Using these four self-supervision schemes, 2DAL significantly relieves the demands for the the conventional paired 2D-to-3D annotations and gives much higher-quality 3D face models without requiring any additional 3D annotations. Experiments on AFLW2000-3D, AFLW-LFPA and Florence benchmarks show that our method outperforms state-of-the-arts for both 3D face reconstruction and dense face alignment by a large margin.
C1 [Tu, Xiaoguang; Xie, Mei; Luo, Yao; Ma, Zheng] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 610054, Peoples R China.
   [Tu, Xiaoguang; Jiang, Zihang; Feng, Jiashi] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
   [Zhao, Jian] Inst North Elect Equipment, Beijing 100080, Peoples R China.
   [Balamurugan, Akshaya] Pensees Pte Ltd, Pensees Singapore Inst, Singapore 138633, Singapore.
   [Zhao, Yang] Natl Univ Def Technol, Changsha 410073, Peoples R China.
   [He, Lingxiao] JD AI Res, Beijing 100864, Peoples R China.
C3 University of Electronic Science & Technology of China; National
   University of Singapore; National University of Defense Technology -
   China
RP Xie, M; Ma, Z (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 610054, Peoples R China.
EM xguangtu@outlook.com; zhaojian90@u.nus.edu; mxie@uestc.edu.cn;
   jzh0103@mail.ustc.edu.cn; akshaya.balamurugan@pensees.ai;
   luoyao_alpha@outlook.com; zhaoyang10@nudt.edu.cn;
   lingxiao.he@nlpr.ia.ac.cn; zma@uestd.edu.cn; elefjia@nus.edu.sg
RI Feng, Jiashi/AGX-6209-2022; ma, zheng/IRY-8826-2023; Jiang,
   Zihang/HKN-5140-2023; zhao, jian/HTM-3920-2023; Jiang,
   Zihang/JEZ-6552-2023
OI Jiang, Zihang/0000-0002-8096-842X; Zhao, Yang/0000-0003-1994-6050; Feng,
   Jiashi/0000-0001-6843-0064; Zhao, Jian/0000-0002-3508-756X
FU National Key Research and Development Program of China [2018AAA0103203]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2018AAA0103203.
CR Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   [Anonymous], 2005, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2005.145
   [Anonymous], 2015, ARXIV150703409
   Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442
   Bagdanov Andrew D, 2011, P JOINT ACM WORKSH H
   Bansal Ankan, 2017, 2017 IEEE International Joint Conference on Biometrics (IJCB), P464, DOI 10.1109/BTAS.2017.8272731
   Bas A, 2017, IEEE INT CONF COMP V, P895, DOI 10.1109/ICCVW.2017.110
   Bhagavatula C, 2017, IEEE I CONF COMP VIS, P4000, DOI 10.1109/ICCV.2017.429
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao C, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925873
   Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cristinacce D., 2006, BRIT MACH VIS C, V1, P3
   Zavan FHD, 2016, LECT NOTES COMPUT SC, V9914, P581, DOI 10.1007/978-3-319-48881-3_40
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Dou P, 2017, PROC CVPR IEEE, P1503, DOI 10.1109/CVPR.2017.164
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Gou C, 2016, LECT NOTES COMPUT SC, V9914, P604, DOI 10.1007/978-3-319-48881-3_42
   Grewe CM, 2016, LECT NOTES COMPUT SC, V9914, P552, DOI 10.1007/978-3-319-48881-3_38
   Gu L., 2006, COMP VIS PATT REC 20, V1, P1305, DOI DOI 10.1109/CVPR.2006.11
   Güler RA, 2017, PROC CVPR IEEE, P2614, DOI 10.1109/CVPR.2017.280
   Guo H, 2019, ICIGP 2019: PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS PROCESSING / 2019 5TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY, P23, DOI 10.1145/3313950.3313964
   Hassner T, 2013, IEEE I CONF COMP VIS, P3607, DOI 10.1109/ICCV.2013.448
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Huber P, 2015, IEEE IMAGE PROC, P1195, DOI 10.1109/ICIP.2015.7350989
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jeni Laszlo A., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163142
   Jeni LA, 2016, LECT NOTES COMPUT SC, V9914, P511, DOI 10.1007/978-3-319-48881-3_35
   Jourabloo A, 2017, INT J COMPUT VISION, V124, P187, DOI 10.1007/s11263-017-1012-z
   Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454
   Jourabloo A, 2015, IEEE I CONF COMP VIS, P3694, DOI 10.1109/ICCV.2015.421
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   Koestinger M., 2011, IEEE INT C COMP VIS, P2144, DOI DOI 10.1109/ICCVW.2011.6130513
   Lee YJ, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-176
   Li ZF, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2807705
   Li ZF, 2014, IEEE T IMAGE PROCESS, V23, P2436, DOI 10.1109/TIP.2014.2315920
   Liu F, 2016, LECT NOTES COMPUT SC, V9909, P545, DOI 10.1007/978-3-319-46454-1_33
   Liu S, 2016, COMPUT ANIMAT VIRT W, V27, P301, DOI 10.1002/cav.1697
   Liu YJ, 2017, IEEE INT CONF COMP V, P1619, DOI 10.1109/ICCVW.2017.190
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767
   McDonagh J, 2016, LECT NOTES COMPUT SC, V9914, P569, DOI 10.1007/978-3-319-48881-3_39
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Paszke A., 2019, ADV NEURAL INFORM PR, P8026, DOI DOI 10.48550/ARXIV.1912.01703
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Peng X, 2016, LECT NOTES COMPUT SC, V9905, P38, DOI 10.1007/978-3-319-46448-0_3
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Richardson E, 2016, INT CONF 3D VISION, P460, DOI 10.1109/3DV.2016.56
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Sánta Z, 2016, LECT NOTES COMPUT SC, V9914, P521, DOI 10.1007/978-3-319-48881-3_36
   Saragih J, 2007, IEEE I CONF COMP VIS, P2173
   Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4
   Tang XO, 2009, IEEE T CIRC SYST VID, V19, P955, DOI 10.1109/TCSVT.2009.2022694
   Tang XO, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P345
   Tewari A, 2017, IEEE INT CONF COMP V, P1274, DOI 10.1109/ICCVW.2017.153
   Theobalt Chris- tian, 2016, P IEEE C COMPUTER VI, DOI DOI 10.1109/CVPR.2016.262
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79
   Wang YJ, 2012, IEEE T MULTIMEDIA, V14, P597, DOI 10.1109/TMM.2012.2189550
   Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882
   Yan YF, 2019, IEEE INT CONF MULTI, P96, DOI 10.1109/ICMEW.2019.0-104
   Yu R, 2017, IEEE I CONF COMP VIS, P4733, DOI 10.1109/ICCV.2017.506
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao J, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4397
   Zhao J, 2020, INT J COMPUT VISION, V128, P460, DOI 10.1007/s11263-019-01252-7
   Zhao J, 2017, ADV NEUR IN, V30
   Zhao J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1184
   Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235
   Zhao J, 2019, IEEE T PATTERN ANAL, V41, P2380, DOI 10.1109/TPAMI.2018.2858819
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679
NR 81
TC 56
Z9 60
U1 3
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1160
EP 1172
DI 10.1109/TMM.2020.2993962
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yan, F
   Zheng, YJ
   Cong, JY
   Liu, L
   Tao, DC
   Hou, SJ
AF Yan, Fang
   Zheng, Yuanjie
   Cong, Jinyu
   Liu, Liu
   Tao, Dacheng
   Hou, Sujuan
TI Solving Jigsaw Puzzles via Nonconvex Quadratic Programming With the
   Projected Power Method
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quadratic programming; Image reconstruction; Noise measurement; Shape;
   Image edge detection; Computational modeling; Square-piece jigsaw;
   unknown orientation; nonconvex quadratic programming; multiple images
ID RECONSTRUCTION; ALGORITHM; RECOVERY
AB Jigsaw puzzles consist of reconstructing a picture that has been divided into many interlocking pieces. This paper describes an automatic global method for solving the square-piece jigsaw puzzle problem in which neither the orientations nor the locations of the jigsaw pieces are known. This hard combinatorial sorting task is formulated as a nonconvex quadratic programming problem that is solved via the projected power method. Specifically, this work aims to specify the locations and orientations of puzzle pieces by maximizing a constrained quadratic function that resolves an optimized permutation matrix composed of the noisy pairwise affinities between jigsaw pieces. The experimental results obtained in the MIT, McGill and Pomeranz datasets indicate that our method outperforms state-of-the-art techniques.
C1 [Yan, Fang; Zheng, Yuanjie; Cong, Jinyu; Hou, Sujuan] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Peoples R China.
   [Zheng, Yuanjie] Univ Shandong, Key Lab Intelligent Comp & Informat Secur, Jinan 250014, Peoples R China.
   [Zheng, Yuanjie] Shandong Prov Key Lab Novel Distributed Comp Soft, Jinan 250014, Peoples R China.
   [Zheng, Yuanjie] Shandong Normal Univ, Inst Biomed Sci, Jinan 250014, Peoples R China.
   [Liu, Liu; Tao, Dacheng] Univ Sydney, URTECH Sydney Artificial Intelligence Ctr, Fac Engn, Darlington, NSW 2008, Australia.
   [Liu, Liu; Tao, Dacheng] Univ Sydney, Sch Comp Sci, Fac Engn, Darlington, NSW 2008, Australia.
C3 Shandong Normal University; Shandong University; Shandong Normal
   University; University of Sydney; University of Sydney
RP Hou, SJ (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Peoples R China.
EM yanfangsdnu@gmail.com; zhengyuanjie@gmail.com; congjinyu1991@gmail.com;
   liu.liu1@sydney.edu.au; dacheng.tao@sydney.edu.au; hsj1985@126.com
RI Tao, Dacheng/A-5449-2012
OI Tao, Dacheng/0000-0001-7225-5449; hou, sujuan/0000-0002-6547-6048;
   Zheng, Yuanjie/0000-0002-5786-2491
FU National Natural Science Foundation of China [81871508, 61773246,
   61702313]; Taishan Scholar Program of Shandong Province of China
   [TSHW201502038]; Major Program of Shandong Province Natural Science
   Foundation [ZR2019ZD04, ZR2018ZB0419]; Postdoctoral Science Foundation
   of China [2017M612338]; Shandong Science and Technology Plan Project
   [J17KB177]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 81871508, 61773246, and 61702313, in
   part by Taishan Scholar Program of Shandong Province of China underGrant
   TSHW201502038, in part by Major Program of Shandong Province Natural
   Science Foundation underGrants ZR2019ZD04 and ZR2018ZB0419, in part by
   Postdoctoral Science Foundation of China under Grant 2017M612338, and in
   part by Shandong Science and Technology Plan Project under Grant
   J17KB177. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Ramazan S. Aygun.
CR Alajlan Naif, 2009, American Journal of Applied Sciences, V6, P1941, DOI 10.3844/ajassp.2009.1941.1947
   Altman T., 1989, Applied Artificial Intelligence, V3, P453, DOI 10.1080/08839518908949937
   Andaló FA, 2017, IEEE T PATTERN ANAL, V39, P385, DOI 10.1109/TPAMI.2016.2547394
   [Anonymous], 2003, IEEE CVPR WORKSH, DOI DOI 10.1109/CVPRW.2003.10008
   Antonacopoulos Apostolos, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P296, DOI 10.1109/ICDAR.2009.271
   Brandao Susana, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P768, DOI 10.1007/978-3-319-46604-0_53
   Brown BJ, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360683
   Candès EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Chang Y., 2017, THESIS, V2545
   Chen YX, 2018, COMMUN PUR APPL MATH, V71, P1648, DOI 10.1002/cpa.21760
   Cho TS, 2010, PROC CVPR IEEE, P183, DOI 10.1109/CVPR.2010.5540212
   Cho TS, 2010, IEEE T PATTERN ANAL, V32, P1489, DOI 10.1109/TPAMI.2009.133
   Cho Taeg Sang, 2008, PROC IEEE C COMPUT V, P1
   Chuman T, 2017, INT CONF ACOUST SPEE, P2157, DOI 10.1109/ICASSP.2017.7952538
   Dan G., 2015, MATHEMATICS-BASEL
   Demaine ED, 2007, GRAPH COMBINATOR, V23, P195, DOI 10.1007/s00373-007-0713-4
   Duchi J., 2008, P 25 INT C MACH LEAR, P272
   Fogel F., 2013, ADV NEURAL INFORM PR, P1016
   FREEMAN H, 1964, IEEE T COMPUT, VEC13, P118, DOI 10.1109/PGEC.1964.263781
   Gallagher AC, 2012, PROC CVPR IEEE, P382, DOI 10.1109/CVPR.2012.6247699
   Ghasemzadeh H., 2017, JIGSAW CRYPTANALYSIS
   Goldberg D, 2004, COMP GEOM-THEOR APPL, V28, P165, DOI 10.1016/j.comgeo.2004.03.007
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   KOSIBA DA, 1994, INT C PATT RECOG, P616, DOI 10.1109/ICPR.1994.576377
   KUCZYNSKI J, 1992, SIAM J MATRIX ANAL A, V13, P1094, DOI 10.1137/0613066
   Li HS, 2014, IEEE T MULTIMEDIA, V16, P571, DOI 10.1109/TMM.2013.2291968
   Liu HR, 2011, IEEE T MULTIMEDIA, V13, P1154, DOI 10.1109/TMM.2011.2160845
   Marande W, 2007, SCIENCE, V318, P415, DOI 10.1126/science.1148033
   Nielsen TR, 2008, PATTERN RECOGN LETT, V29, P1924, DOI 10.1016/j.patrec.2008.05.027
   Olmos A, 2004, PERCEPTION, V33, P1463, DOI 10.1068/p5321
   Paikin G, 2015, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR.2015.7299116
   Paumard MM, 2020, IEEE T IMAGE PROCESS, V29, P3569, DOI 10.1109/TIP.2019.2963378
   Pomeranz D, 2011, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2011.5995331
   Sholomon D, 2013, PROC CVPR IEEE, P1767, DOI 10.1109/CVPR.2013.231
   Son K, 2019, IEEE T PATTERN ANAL, V41, P2222, DOI 10.1109/TPAMI.2018.2857776
   Son K, 2016, PROC CVPR IEEE, P1193, DOI 10.1109/CVPR.2016.134
   Son K, 2014, LECT NOTES COMPUT SC, V8694, P32, DOI 10.1007/978-3-319-10599-4_3
   Sun J, 2018, FOUND COMPUT MATH, V18, P1131, DOI 10.1007/s10208-017-9365-9
   Sun J, 2015, PR MACH LEARN RES, V37, P2351
   Wolfson H., 1988, Annals of Operations Research, V12, P51, DOI 10.1007/BF02186360
   Yang Xingwei, 2011, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, V2011, P2873, DOI 10.1109/CVPR.2011.5995535
   Yu R., 2016, P BRIT MACH VIS C, V139, P1
   Zagoris K, 2012, INT CONF FRONT HAND, P103, DOI 10.1109/ICFHR.2012.207
   Zhao Yu-Xiang, 2007, WSEAS INT C COMPUTER, P171
   Zhu LJ, 2008, IEEE T PATTERN ANAL, V30, P1, DOI 10.1109/TPAMI.2007.1163
NR 45
TC 0
Z9 0
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2310
EP 2320
DI 10.1109/TMM.2020.3009501
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800012
DA 2024-07-18
ER

PT J
AU Yang, F
   Yan, K
   Lu, SJ
   Jia, HZ
   Xie, D
   Yu, ZQ
   Guo, XW
   Huang, FY
   Gao, W
AF Yang, Fan
   Yan, Ke
   Lu, Shijian
   Jia, Huizhu
   Xie, Don
   Yu, Zongqiao
   Guo, Xiaowei
   Huang, Feiyue
   Gao, Wen
TI Part-aware Progressive Unsupervised Domain Adaptation for Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Cameras; Dictionaries; Adaptation models; Supervised
   learning; Measurement; Image color analysis; Unsupervised domain
   adaptation; Person re-identification; Part-aware; Feature alignment;
   Progressive adaptation
ID TRACKING; DESCRIPTOR; RETRIEVAL; NETWORK
AB Unsupervised domain adaptation (UDA) aims to mitigate the domain shift that occurs when transferring knowledge from a labeled source domain to an unlabeled target domain. While it has been studied for application in unsupervised person re-identification (ReID), the relations of feature distribution across the source and target domains remain underexplored, as they either ignore the local relations or omit the in-depth consideration of negative transfer when two domains do not share identical label spaces. In light of the above, this paper presents an innovative part-aware progressive adaptation network (PPAN) that exploits global and local relations for UDA-based ReID across domains. A multi-branch network is developed that explicitly learns discriminative feature representation from both whole-body images and body-part images under the supervision of a labeled source domain. Within each network branch, an independent UDA constraint is designed that aligns the global and local feature distributions from a labeled source domain with those of an unlabeled target domain. In addition, a novel progressive adaptation strategy (PAS) is designed that effectively alleviates the negative influence of outlier source identities. The proposed unsupervised ReID model is evaluated on five widely used datasets (Market-1501, DukeMTMC-reID, CUHK03, VIPeR and PRID), and experimental results demonstrate its superior robustness and effectiveness relative to state-of-the-art approaches.
C1 [Yang, Fan; Jia, Huizhu; Xie, Don; Gao, Wen] Peking Univ, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Yan, Ke; Yu, Zongqiao; Guo, Xiaowei; Huang, Feiyue] Tencent, Youtu Lab, Shanghai 201103, Peoples R China.
   [Lu, Shijian] Nanyang Technol Univ, Singapore 639798, Singapore.
C3 Peking University; Tencent; Nanyang Technological University
RP Jia, HZ (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
EM fyang.eecs@pku.edu.cn; kerwinyan@tencent.com; Shijian.Lu@ntu.edu.sg;
   hzjia@pku.edu.cn; donxie@pku.edu.cn; quentinyu@tencent.com;
   scorpioguo@tencent.com; garyhuang@tencent.com; wgao@pku.edu.cn
RI Yang, Fan/AAU-1689-2021; Lu, Shijian/AAU-4831-2021
OI Lu, Shijian/0000-0002-6766-2506; Yang, Fan/0000-0002-3431-2585
FU Beijing Major Science and Technology Project [Z191100010618003,
   2019BD004]; PKU-Baidu Fund; NVIDIA NVAIL program
FX This work was supported in part by the Beijing Major Science and
   Technology Project under Contract no. Z191100010618003 and Project
   2019BD004 supported by PKU-Baidu Fund and in part by grants from NVIDIA
   NVAIL program.
CR Bai S, 2017, AAAI CONF ARTIF INTE, P1281
   Bousmalis K, 2016, ADV NEUR IN, V29
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chang XB, 2019, AAAI CONF ARTIF INTE, P3288
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Ganin Y., 2015, ICML
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Guo YQ, 2017, IEEE INT C INTELL TR
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Karanam S, 2019, IEEE T PATTERN ANAL, V41, P523, DOI 10.1109/TPAMI.2018.2807450
   Kodirov E., 2015, BMVC, DOI DOI 10.5244/C.29.44
   Kodirov E, 2016, LECT NOTES COMPUT SC, V9905, P178, DOI 10.1007/978-3-319-46448-0_11
   Lee KH, 2015, IEEE T MULTIMEDIA, V17, P1429, DOI 10.1109/TMM.2015.2455418
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin Shan, 2018, 2018 IEEE 36 VLSI TE
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Liu Z, 2018, IEEE T MULTIMEDIA, V20, P1321, DOI 10.1109/TMM.2017.2767781
   Liu ZM, 2017, IEEE I CONF COMP VIS, P2448, DOI 10.1109/ICCV.2017.266
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lv JM, 2018, PROC CVPR IEEE, P7948, DOI 10.1109/CVPR.2018.00829
   Ma BP, 2014, IMAGE VISION COMPUT, V32, P379, DOI 10.1016/j.imavis.2014.04.002
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Peng PX, 2018, IEEE T PATTERN ANAL, V40, P1625, DOI 10.1109/TPAMI.2017.2723882
   Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Qin C, 2015, NEUROCOMPUTING, V168, P609, DOI 10.1016/j.neucom.2015.05.064
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Sunderrajan S, 2016, IEEE T MULTIMEDIA, V18, P51, DOI 10.1109/TMM.2015.2496139
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wang H., 2014, IEEE PES TD Conference and Exposition, P1, DOI DOI 10.5244/C.28.48
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1
   Yang F, 2019, PATTERN RECOGN, V86, P143, DOI 10.1016/j.patcog.2018.08.015
   Ye Jianbo, 2018, P INT C LEARN REPR
   Ye M, 2019, IEEE T IMAGE PROCESS, V28, P2976, DOI 10.1109/TIP.2019.2893066
   Ye M, 2017, IEEE I CONF COMP VIS, P5152, DOI 10.1109/ICCV.2017.550
   Yu HX, 2020, IEEE T PATTERN ANAL, V42, P956, DOI 10.1109/TPAMI.2018.2886878
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Zhang YY, 2019, AAAI CONF ARTIF INTE, P9243
   Zhang ZZ, 2019, IEEE T MULTIMEDIA, V21, P2878, DOI 10.1109/TMM.2019.2915036
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao R, 2017, IEEE T PATTERN ANAL, V39, P356, DOI 10.1109/TPAMI.2016.2544310
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 80
TC 49
Z9 50
U1 2
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1681
EP 1695
DI 10.1109/TMM.2020.3001522
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300016
DA 2024-07-18
ER

PT J
AU Yang, X
   Yuan, ZK
   Zhu, DF
   Chi, C
   Li, K
   Liao, CY
AF Yang, Xin
   Yuan, Zikang
   Zhu, Dongfu
   Chi, Cheng
   Li, Kun
   Liao, Chunyuan
TI Robust and Efficient RGB-D SLAM in Dynamic Environments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamics; Simultaneous localization and mapping; Cameras;
   Three-dimensional displays; Pose estimation; Robustness; Motion
   segmentation; Robotics and automation; robots; robot sensing systems;
   simultaneous localization and mapping
ID DEPTH PREDICTION; ALGORITHM; OPTIMIZATION
AB Simultaneous localization and mapping (SLAM) using an RGB-D camera is a key enabling technique for many augmented reality (AR) applications. However, most existing RGB-D SLAM methods could fail in dynamic scenarios due to non-trivial pose estimation errors arising from moving objects. In this study, we present an accurate and robust RGB-D SLAM system for dynamic scenarios which can run real-time on a single dual-core CPU. The core of our system is a robust and efficient dynamic keypoint exclusion method which consists of three steps: 1) grouping spatially and appearance related pixels of a keyframe into regions; 2) identifying dynamic regions by checking motion consistency of keypoints in every region; 3) excluding keypoints in the identified dynamic regions as well as the matching points in the 3D local map. The dynamic keypoint exclusion method can be easily integrated into any keypoint based RGB-D SLAM system for improving the accuracy and robustness in dynamic scenes with trivial time increase (16.6ms per frame). Experimental results on the TUM dataset demonstrates that our method which runs on an Intel i7-4900 CPU is even 2.3X faster than the state-of-the-art method DS-SLAM [1] which runs parallel on a P4000 GPU and a comparable CPU. In addition, our system outperforms the state-of-the-art methods [1]-[4] in terms of smaller absolute trajectory errors (ATE). We also apply our system to a real AR application and live experiments with a hand-held RGB-D camera demonstrate the robustness and generalizability of our method in practical scenarios.(1) (1) A demo video is provided on https://github.com/cc-qy/Dynamic-RGB-D-SLAM
C1 [Yang, Xin; Yuan, Zikang; Zhu, Dongfu; Chi, Cheng; Li, Kun] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Liao, Chunyuan] HiScene Informat Technol Co Ltd, Shanghai 201210, Peoples R China.
C3 Huazhong University of Science & Technology
RP Yang, X (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM xinyang2014@hust.edu.cn; zikangyuan@hust.edu.cn; dongfuzhu@hust.edu.cn;
   cheng-chi@hust.edu.cn; 1162506431@qq.com; liaocy@hiscene.com
RI Yuan, Zikang/IXN-0401-2023
FU National Natural Science Foundation of China [61872417]; Fundamental
   Research Funds for the Central Universities [2019kfyRCPY118,
   2020kfyXGYJ026]; Open Project ofWuhan National Laboratory for
   Optoelectronics [2018WNLOKF025]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61872417, in part by the Fundamental Research Funds
   for the Central Universities under Grants 2019kfyRCPY118, 2020kfyXGYJ026
   and in part by the Open Project ofWuhan National Laboratory for
   Optoelectronics under Grant 2018WNLOKF025. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. James She.
CR Azartash Haleh, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1280, DOI 10.1109/ICASSP.2014.6853803
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Barber CB, 1996, ACM T MATH SOFTWARE, V22, P469, DOI 10.1145/235815.235821
   Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Chen JY, 2021, IEEE T PATTERN ANAL, V43, P2598, DOI 10.1109/TPAMI.2020.2977021
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dai W., 2018, ARXIV181103217
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Furgale P, 2013, IEEE INT C INT ROBOT, P1280, DOI 10.1109/IROS.2013.6696514
   Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650
   Kerl C, 2013, IEEE INT CONF ROBOT, P3748, DOI 10.1109/ICRA.2013.6631104
   Kim DH, 2015, ADV INTELL SYST, V345, P11, DOI 10.1007/978-3-319-16841-8_2
   Li SL, 2017, IEEE ROBOT AUTOM LET, V2, P2263, DOI 10.1109/LRA.2017.2724759
   Liang YQ, 2020, INTEGR COMPUT-AID E, V27, P417, DOI 10.3233/ICA-200641
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Riazuelo L, 2017, 2017 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR)
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Shin J, 2020, IEEE T MULTIMEDIA, V22, P30, DOI 10.1109/TMM.2019.2922127
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012
   Tee G. J, ACM SIGNUM NEWSLETT, V7, P19
   Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2701, DOI 10.1109/TMM.2019.2912121
   Yu C, 2018, IEEE INT C INT ROBOT, P1168, DOI 10.1109/IROS.2018.8593691
   Yuan ZK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1410, DOI 10.1145/3343031.3351079
NR 30
TC 16
Z9 16
U1 4
U2 46
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4208
EP 4219
DI 10.1109/TMM.2020.3038323
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900023
DA 2024-07-18
ER

PT J
AU Zhang, DY
   Shao, J
   Liang, ZW
   Gao, LL
   Shen, HT
AF Zhang, Dongyang
   Shao, Jie
   Liang, Zhenwen
   Gao, Lianli
   Shen, Heng Tao
TI Large Factor Image Super-Resolution With Cascaded Convolutional Neural
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; Convolutional neural networks; Image reconstruction;
   Computer architecture; Computational efficiency; Cascaded architecture;
   convolutional neural networks; image super-resolution
ID INFORMATION
AB Recently, convolutional neural networks (CNNs) have attracted considerable attention in single image super-resolution (SISR) and have enabled great performance improvements. However, most of the existing methods super-resolve input images to the desired size with an interpolation operation during the beginning stage, which brings about heavy aliasing artifacts and high computational costs. Especially for large upsampling factors (e.g., 8x), it remains a challenge to restore high-quality results for deeply degraded images. To tackle this problem, we propose a cascaded super-resolution convolutional neural network (CSRCNN), which takes a single low-resolution (LR) image as an input and reconstructs high-resolution (HR) images in a progressive way. At each cascaded level, to help converge and improve the accuracy, a novel U-net based block with backprojection is first introduced, which exploits the mutual relation between HR and LR feature spaces. A refined block following the U-net block is also used to reconstruct the realistic texture details. In addition, we naturally utilize the strategy of curriculum learning, organizing the learning process from easy (small factors) to hard (large factors). Comprehensive experiments on benchmark datasets demonstrate that the proposed network achieves superior results compared with those of other state-of-the-art methods, particularly with the 8x upsampling factor.
C1 [Zhang, Dongyang; Shao, Jie; Liang, Zhenwen; Gao, Lianli; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Shao, Jie; Shen, Heng Tao] Sichuan Artificial Intelligence Res Inst, Yibin 644000, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Shao, J (corresponding author), Univ Elect Sci & Technol China, Ctr Future Media, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM dyzhang@std.uestc.edu.cn; shaojie@uestc.edu.cn;
   zhenwenliang@std.uestc.edu.cn; lianli.gao@uestc.edu.cn;
   shenhengtao@uestc.edu.cn
RI Liang, Zhenwen/GXN-2210-2022; Shen, Heng Tao/ABD-5331-2021
FU National Natural Science Foundation of China [61672133, 61832001,
   61632007]; Sichuan Science and Technology Program [2019YFG0535]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672133, 61832001, and 61632007 and in
   part by the Sichuan Science and Technology Program under Grant
   2019YFG0535.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   [Anonymous], 2015, PROCIEEE CONFCOMPUT
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Chen J, 2020, PATTERN RECOGN LETT, V132, P62, DOI 10.1016/j.patrec.2018.06.030
   Chen SJ, 2020, IEEE COMPUT SOC CONF, P1924, DOI 10.1109/CVPRW50498.2020.00242
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Ferstl D, 2015, IEEE I CONF COMP VIS, P513, DOI 10.1109/ICCV.2015.66
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han D, 2015, PROC CVPR IEEE, P5016, DOI 10.1109/CVPR.2015.7299136
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He CK, 2018, MULTIMED TOOLS APPL, V77, P29573, DOI 10.1007/s11042-017-5255-z
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2018, P INT C LEARN REPR I
   Khan AA, 2020, NEURAL PROCESS LETT, V52, P1945, DOI 10.1007/s11063-020-10200-3
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kong Hanyang, 2019, ARXIV190510777
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar N, 2016, IEEE T MULTIMEDIA, V18, P1504, DOI 10.1109/TMM.2016.2571625
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li L, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17051062
   Liang ZW, 2020, NEURAL COMPUT APPL, V32, P6533, DOI 10.1007/s00521-019-04086-z
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Mao XJ, 2016, ADV NEUR IN, V29
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Odena A., 2016, DISTILL, V1, pe3, DOI 10.23915/distill.00003.-URL
   Romano Y, 2017, IEEE T COMPUT IMAG, V3, P110, DOI 10.1109/TCI.2016.2629284
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shah S., 2018, Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi WZ, 2013, LECT NOTES COMPUT SC, V8151, P9, DOI 10.1007/978-3-642-40760-4_2
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sonderby C. K., 2017, ICLR
   Sun JY, 2018, IEEE ACCESS, V6, P33353, DOI 10.1109/ACCESS.2018.2848210
   Sun JY, 2019, MULTIMED TOOLS APPL, V78, P3633, DOI 10.1007/s11042-017-5244-2
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang YF, 2018, IEEE COMPUT SOC CONF, P977, DOI 10.1109/CVPRW.2018.00131
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Xiong ZW, 2013, IEEE T MULTIMEDIA, V15, P1458, DOI 10.1109/TMM.2013.2264654
   Xu J, 2019, IEEE T MULTIMEDIA, V21, P1108, DOI 10.1109/TMM.2018.2871948
   Xu X, 2020, IEEE T NEUR NET LEAR, V31, P5412, DOI 10.1109/TNNLS.2020.2967597
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang JC, 2013, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2013.141
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Zhang DY, 2020, NEUROCOMPUTING, V412, P187, DOI 10.1016/j.neucom.2020.05.069
   Zhang DY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3398685
   Zhang DY, 2017, LECT NOTES COMPUT SC, V10636, P217, DOI 10.1007/978-3-319-70090-8_23
   Zhang YQ, 2015, IEEE T IMAGE PROCESS, V24, P2797, DOI 10.1109/TIP.2015.2431435
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 69
TC 13
Z9 13
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2172
EP 2184
DI 10.1109/TMM.2020.3008041
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100026
DA 2024-07-18
ER

PT J
AU Zhang, JL
   Lin, LX
   Zhu, JK
   Li, Y
   Chen, YC
   Hu, Y
   Hoi, SCH
AF Zhang, Jialiang
   Lin, Lixiang
   Zhu, Jianke
   Li, Yang
   Chen, Yun-chen
   Hu, Yao
   Hoi, Steven C. H.
TI Attribute-Aware Pedestrian Detection in a Crowd
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Detectors; Semantics; Feature extraction; Proposals; Object detection;
   Task analysis; Training; Attribute-aware; non-maximum suppression (nms);
   pedestrian detection
AB Pedestrian detection is an initial step to perform outdoor scene analysis, which plays an essential role in many real-world applications. Although having enjoyed the merits of deep learning frameworks from the generic object detectors, pedestrian detection is still a very challenging task due to heavy occlusions, and highly crowded group. Generally, the conventional detectors are unable to differentiate individuals from each other effectively under such a dense environment. To tackle this critical problem, we propose an attribute-aware pedestrian detector to explicitly model people's semantic attributes in a high-level feature detection fashion. Besides the typical semantic features, center position, target's scale, and offset, we introduce a pedestrian-oriented attribute feature to encode the high-level semantic differences among the crowd. Moreover, a novel attribute-feature-based Non-Maximum Suppression (NMS) is proposed to distinguish the person from a highly overlapped group by adaptively rejecting the false-positive results in a very crowd settings. Furthermore, an enhanced ground truth target is designed to alleviate the difficulties caused by the attribute configuration, and to ease the class imbalance issue during training. Finally, we evaluate our proposed attribute-aware pedestrian detector on three benchmark datasets including CityPerson, CrowdHuman, and EuroCityPerson, and achieves the state-of-the-art results.
C1 [Zhang, Jialiang; Lin, Lixiang; Zhu, Jianke; Li, Yang] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
   [Zhu, Jianke] Alibaba Zhejiang Univ Joint Res Inst Frontier Tec, Hangzhou 310027, Peoples R China.
   [Chen, Yun-chen; Hoi, Steven C. H.] Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore.
   [Hu, Yao] Co Alibaba, Youku Cognit & Intelligent Lab, Beijing, Peoples R China.
C3 Zhejiang University; Singapore Management University
RP Zhu, JK (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
EM zjialiang@zju.edu.cn; lxlin@zju.edu.cn; jkzhu@zju.edu.cn;
   liyang89@zju.edu.cn; jeanchen@smu.edu.sg; yaoohu@alibaba-inc.com;
   chhoi@smu.edu.sg
RI HOI, Steven C. H./A-3736-2011; Hu, Yao/KEH-3649-2024
OI Zhang, Jialiang/0000-0001-5085-3771; Zhu, Jianke/0000-0003-1831-0106; ,
   li xiang/0000-0001-8319-2009; Li, Yang/0000-0001-9427-7665
FU National Natural Science Foundation of China [61831015]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61831015.
CR Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Braun M, 2019, IEEE T PATTERN ANAL, V41, P1844, DOI 10.1109/TPAMI.2019.2897684
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chenhan Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11860, DOI 10.1109/CVPR42600.2020.01188
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Enzweiler M, 2010, PROC CVPR IEEE, P990, DOI 10.1109/CVPR.2010.5540111
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hasan I., 2020, ABS200308799 CORR
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosang J, 2017, PROC CVPR IEEE, P6469, DOI 10.1109/CVPR.2017.685
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48
   Kingma D. P., 2014, arXiv
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Lee D, 2016, LECT NOTES COMPUT SC, V9910, P330, DOI 10.1007/978-3-319-46466-4_20
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu ST, 2019, PROC CVPR IEEE, P6452, DOI 10.1109/CVPR.2019.00662
   Liu W, 2018, LECT NOTES COMPUT SC, V11218, P643, DOI 10.1007/978-3-030-01264-9_38
   Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mao JY, 2017, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2017.639
   Mathias M, 2013, IEEE I CONF COMP VIS, P1505, DOI 10.1109/ICCV.2013.190
   Nam W., 2014, P 27 INT C NEURAL IN, V27
   Ouyang WL, 2012, PROC CVPR IEEE, P3258, DOI 10.1109/CVPR.2012.6248062
   Pang YW, 2019, IEEE I CONF COMP VIS, P4966, DOI 10.1109/ICCV.2019.00507
   Papageorgiou C, 2000, INT J COMPUT VISION, V38, P15, DOI 10.1023/A:1008162616689
   Ranjan V, 2018, LECT NOTES COMPUT SC, V11211, P278, DOI 10.1007/978-3-030-01234-2_17
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rujikietgumjorn S, 2013, PROC CVPR IEEE, P3690, DOI 10.1109/CVPR.2013.473
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shao S., 2018, CROWDHUMAN BENCHMARK
   Song T, 2018, LECT NOTES COMPUT SC, V11211, P554, DOI 10.1007/978-3-030-01234-2_33
   Tarvainen A, 2017, ADV NEUR IN, V30
   Tian YL, 2015, IEEE I CONF COMP VIS, P1904, DOI 10.1109/ICCV.2015.221
   Tian YL, 2015, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2015.7299143
   Tychsen-Smith L, 2018, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR.2018.00719
   Wang SG, 2018, IEEE T MULTIMEDIA, V20, P3148, DOI 10.1109/TMM.2018.2829602
   Wang XL, 2018, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2018.00811
   Xuangeng Chu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12211, DOI 10.1109/CVPR42600.2020.01223
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhang SS, 2015, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2015.7298784
   Zhang SF, 2020, IEEE T MULTIMEDIA, V22, P380, DOI 10.1109/TMM.2019.2929005
   Zhang SF, 2018, LECT NOTES COMPUT SC, V11207, P657, DOI 10.1007/978-3-030-01219-9_39
   Zhou CL, 2018, LECT NOTES COMPUT SC, V11205, P138, DOI 10.1007/978-3-030-01246-5_9
   Zhou CL, 2017, IEEE I CONF COMP VIS, P3506, DOI 10.1109/ICCV.2017.377
   Zhou X., 2019, ABS190407850 ARXIV
NR 56
TC 32
Z9 33
U1 3
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3085
EP 3097
DI 10.1109/TMM.2020.3020691
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000011
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, SZ
   Zhang, Q
   Yang, YF
   Wei, X
   Wang, P
   Jiao, BL
   Zhang, YN
AF Zhang, Shizhou
   Zhang, Qi
   Yang, Yifei
   Wei, Xing
   Wang, Peng
   Jiao, Bingliang
   Zhang, Yanning
TI Person Re-Identification in Aerial Imagery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Surveillance; Task analysis; Unmanned aerial vehicles; Cameras;
   Visualization; Object detection; Distance measurement; Person
   Re-identification; aerial imagery; UAV; video surveillance; subspace
   pooling
ID DEEP; NETWORK
AB Nowadays, with the rapid development of consumer Unmanned Aerial Vehicles (UAVs), visual surveillance by utilizing the UAV platform has been very attractive. Most of the research works for UAV captured visual data are mainly focused on the tasks of object detection and tracking. However, limited attention has been paid to the task of person Re-identification (ReID) which has been widely studied in ordinary surveillance cameras with fixed emplacements. In this paper, to facilitate the research of person ReID in aerial imagery, we collect a large scale airborne person ReID dataset named as Person ReID in Aerial Imagery (PRAI-1581), which consists of 39,461 images of 1581 person identities. The images of the dataset are shot by two DJI consumer UAVs flying at an altitude ranging from 20 to 60 meters above the ground, which covers most of the real UAV surveillance scenarios. In addition, we propose to utilize subspace pooling of convolution feature maps to represent the input person images. Our method can learn a discriminative and compact feature representation for ReID in aerial imagery and can be trained in an end-to-end fashion efficiently. We conduct extensive experiments on the proposed dataset and the experimental results demonstrate that re-identifying persons in aerial imagery is a challenging problem, where our method performs favorably against state of the arts.
C1 [Zhang, Shizhou; Zhang, Qi; Yang, Yifei; Wang, Peng; Jiao, Bingliang; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci & Engn, Natl Engn Lab Integrated Aerosp Ground Ocean Big, Xian 710072, Peoples R China.
   [Wei, Xing] Xi An Jiao Tong Univ, Fac Elect & Informat Engn, Xian 710049, Peoples R China.
C3 Northwestern Polytechnical University; Xi'an Jiaotong University
RP Wei, X (corresponding author), Xi An Jiao Tong Univ, Fac Elect & Informat Engn, Xian 710049, Peoples R China.
EM szzhang@nwpu.edu.cn; zhangqi1102@mail.nwpu.edu.cn;
   yfyang@mail.nwpu.edu.cn; xingxjtu@gmail.com; peng.wang@nwpu.edu.cn;
   bingliangjiao@mail.nwpu.edu.cn; ynzhang@nwpu.edu.cn
RI Yang, Yifei/HJA-0450-2022; 郑, 琦/KQA-9998-2024
OI Wei, Xing/0000-0002-5025-3941
FU Natural Science Basic Research Plan in Shaanxi Province of China Program
   [2019JQ158]; China Postdoctoral Science Foundation [2018M633577];
   Fundamental Research Funds for Central Universities of China
   [G2018KY0303]; National Science Foundation of China [61876152]
FX This work was supported in part by Natural Science Basic Research Plan
   in Shaanxi Province of China Program no. 2019JQ158, in part by China
   Postdoctoral Science Foundation funded project under Grant 2018M633577,
   in part by the Fundamental Research Funds for Central Universities of
   China under Grant G2018KY0303, and in part by the National Science
   Foundation of China under Grant 61876152. The associate editor
   coordinating the reviewof this manuscript and approving it for
   publication was Dr. Jingdong Wang.
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Alam MR, 2017, IEEE T MULTIMEDIA, V19, P317, DOI 10.1109/TMM.2016.2615524
   [Anonymous], 2018, P ECCV
   [Anonymous], 2016, ARXIV
   [Anonymous], 2017, CVPR
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chen D, 2018, INT CONF CLOUD COMPU, P507, DOI 10.1109/CCIS.2018.8691205
   Chen Y., 2019, P IEEE INT C COMP VI, P2590
   Cheng G, 2016, IEEE T GEOSCI REMOTE, V54, P7405, DOI 10.1109/TGRS.2016.2601622
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Ge Y, 2018, INT POWER ELECT ELEC, P1227, DOI 10.1109/SPEEDAM.2018.8445201
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   Jayasumana S, 2015, IEEE T PATTERN ANAL, V37, P2464, DOI 10.1109/TPAMI.2015.2414422
   Layne R, 2015, LECT NOTES COMPUT SC, V8927, P225, DOI 10.1007/978-3-319-16199-0_16
   Leibe B., 2017, ARXIV170307737CS
   Li M., 2018, PROC EUR C COMPUT, P737
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liu CL, 2018, PROCEEDINGS OF 2018 10TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING (ICMLC 2018), P172, DOI 10.1145/3195106.3195112
   Liu H, 2015, NEUROCOMPUTING, V151, P1283, DOI 10.1016/j.neucom.2014.11.002
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Singh A., 2018, IEEE COMPUT SOC CONF, P1629, DOI DOI 10.1109/CVPRW.2018.00214
   Sun XX, 2019, PROC CVPR IEEE, P608, DOI 10.1109/CVPR.2019.00070
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Tan LZ, 2016, NPJ COMPUT MATER, V2, DOI 10.1038/npjcompumats.2016.26
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei X, 2018, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2018.00200
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Xiang WM, 2019, IEEE IMAGE PROC, P1237, DOI [10.1109/icip.2019.8803735, 10.1109/ICIP.2019.8803735]
   Xiang Y, 2014, LECT NOTES COMPUT SC, V8694, P220, DOI 10.1007/978-3-319-10599-4_15
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Zaitseva N. V, 2017, Meditsina Truda i Promyshlennaya Ekologiya, P1
   Zhang SZ, 2018, NEUROCOMPUTING, V283, P120, DOI 10.1016/j.neucom.2017.12.042
   Zhang Xiangyu, 2017, CoRRabs/1711.08184
   Zhang YQ, 2018, PROC CVPR IEEE, P928, DOI 10.1109/CVPR.2018.00103
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
   Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhu P., 2018, EUROPEAN C COMPUTER
NR 52
TC 47
Z9 49
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 281
EP 291
DI 10.1109/TMM.2020.2977528
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600022
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhu, YC
   Zhai, GT
   Min, XK
   Zhou, JT
AF Zhu, Yucheng
   Zhai, Guangtao
   Min, Xiongkuo
   Zhou, Jiantao
TI The Prediction of Saliency Map for Head and Eye Movements in 360 Degree
   Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Head; Visualization; Feature extraction; Predictive models; Magnetic
   heads; Frequency-domain analysis; Uncertainty; VR; 360 degree; saliency;
   head-eye motion; scanpath; spherical harmonics; center and peripheral
   vision
ID GAZE PREDICTION; MODEL; FIXATION; COLOR
AB By recording the whole scene around the capturer, virtual reality (VR) techniques can provide viewers the sense of presence. To provide a satisfactory quality of experience, there should be at least 60 pixels per degree, so the resolution of panoramas should reach 21600 x 10800. The huge amount of data will put great demands on data processing and transmission. However, when exploring in the virtual environment, viewers only perceive the content in the current field of view (FOV). Therefore if we can predict the head and eye movements which are important behaviors of viewer, more processing resources can be allocated to the active FOV. But conventional saliency prediction methods are not fully adequate for panoramic images. In this paper, a new panorama-oriented model, to predict head and eye movements, is proposed. Due to the superiority of computation in the spherical domain, the spherical harmonics are employed to extract features at different frequency bands and orientations. Related low- and high-level features including the rare components in the frequency domain and color domain, the difference between center vision and peripheral vision, visual equilibrium, person and car detection, and equator bias are extracted to estimate the saliency. To predict head movements, visual mechanisms including visual uncertainty and equilibrium are incorporated, and the graphical model and functional representation for the switch of head orientation are established. Extensive experimental results on the publicly available database demonstrate the effectiveness of our methods.
C1 [Zhu, Yucheng; Zhai, Guangtao; Min, Xiongkuo] Shanghai Jiao Tong Univ, Inst Image Commu & Informat Proc, Shanghai 200240, Peoples R China.
   [Zhou, Jiantao] Univ Macau, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.
   [Zhou, Jiantao] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 Shanghai Jiao Tong University; University of Macau; University of Macau
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commu & Informat Proc, Shanghai 200240, Peoples R China.
EM zyc420@sjtu.edu.cn; zhaiguangtao@sjtu.edu.cn; minxiongkuo@sjtu.edu.cn;
   jtzhou@umac.mo
RI Zhai, Guangtao/X-5949-2019; Min, Xiongkuo/A-7097-2019
OI Zhai, Guangtao/0000-0001-8165-9322; Min, Xiongkuo/0000-0001-5693-0416;
   Zhu, Yucheng/0000-0002-3069-060X
FU National Natural Science Foundation of China [61831015, 61771305,
   61521062, 61527804]; China Postdoctoral Science Foundation [BX20180197,
   2019M651496]; Macau Science and Technology Development Fund
   [FDCT/022/2017/A1, FDCT/077/2018/A2]; Research Committee at the
   University of Macau [MYRG2016-00137-FST, MYRG2018-00029-FST]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61831015, 61771305, 61521062, and
   61527804, in part by the China Postdoctoral Science Foundation under
   Grants BX20180197 and 2019M651496, in part by the Macau Science and
   Technology Development Fund under Grants FDCT/022/2017/A1 and
   FDCT/077/2018/A2, and in part by the Research Committee at the
   University of Macau under Grants MYRG2016-00137-FST and
   MYRG2018-00029-FST.
CR [Anonymous], 2018, REV AMBIENT CONTAB
   [Anonymous], 2012, LECT NOTES COMPUT SC
   [Anonymous], 2019, SHENZHEN STAT YB
   [Anonymous], 2018, P 1 WORKSH RAD EXP, DOI DOI 10.1145/3203422.3203429
   [Anonymous], 2016, PROC SPIE, DOI DOI 10.1117/12.2213478
   [Anonymous], 2012, J NEUROSCI, DOI DOI 10.1523/JNEUROSCI.0696-12.2012
   [Anonymous], 2017, 2017 INT C EM
   Barten PGJ, 2004, P SOC PHOTO-OPT INS, V5294, P231, DOI 10.1117/12.537476
   Battisti F, 2018, SIGNAL PROCESS-IMAGE, V69, P53, DOI 10.1016/j.image.2018.03.008
   BRANDT T, 1973, EXP BRAIN RES, V16, P476, DOI 10.1007/BF00234474
   Carlson C., 2011, I MADE WINE GLASSES
   Carroll R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531349
   Chan JYH, 2017, IEEE T SIGNAL PROCES, V65, P5, DOI 10.1109/TSP.2016.2600506
   Chen ZZ, 2018, SIGNAL PROCESS, V146, P66, DOI 10.1016/j.sigpro.2018.01.004
   Cheng H, 2019, IEEE T MULTIMEDIA, V21, P678, DOI 10.1109/TMM.2018.2864613
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cohen T. S., 2018, P 6 INT C LEARN REPR
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139
   De Abreu A., 2017, INT WORK QUAL MULTIM, P1
   De Simone F., 2019, ELECT IMAG, V2019
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fremerey S, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P403, DOI 10.1145/3204949.3208134
   Guo YW, 2012, COMPUT GRAPH FORUM, V31, P2193, DOI 10.1111/j.1467-8659.2012.03212.x
   Gutiérrez J, 2018, SIGNAL PROCESS-IMAGE, V69, P35, DOI 10.1016/j.image.2018.05.003
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153
   Itti L., 2015, ARXIV151007748
   Jahanian A., 2013, PROC INT C INTELL US, P95
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Jost T, 2005, COMPUT VIS IMAGE UND, V100, P107, DOI 10.1016/j.cviu.2004.10.009
   Judd T., 2012, MITCSAILTR2012001
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kandemir B, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P26, DOI 10.1145/3126686.3126712
   Kennedy M., 2000, UNDERSTANDING MAP PR
   Kümmerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513
   Lai WS, 2018, IEEE T VIS COMPUT GR, V24, P2610, DOI 10.1109/TVCG.2017.2750671
   Lebreton P, 2018, SIGNAL PROCESS-IMAGE, V69, P69, DOI 10.1016/j.image.2018.03.006
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li BJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02116
   Ling J, 2018, SIGNAL PROCESS-IMAGE, V69, P60, DOI 10.1016/j.image.2018.03.007
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   McEwen J. D., 2015, ARXIV150906749
   McManus IC, 2011, I-PERCEPTION, V2, P615, DOI 10.1068/i0445aap
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Nakashima R, 2015, VISION RES, V117, P59, DOI 10.1016/j.visres.2015.10.001
   Ohlsson J, 2005, ACTA OPHTHALMOL SCAN, V83, P487, DOI 10.1111/j.1600-0420.2005.00516.x
   Oudeyer Pierre-Yves, 2007, Front Neurorobot, V1, P6, DOI 10.3389/neuro.12.006.2007
   Pan J., 2017, ARXIV170101081
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Rai Y, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P205, DOI 10.1145/3083187.3083218
   Renninger LW, 2007, J VISION, V7, DOI 10.1167/7.3.6
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Startsev M, 2018, SIGNAL PROCESS-IMAGE, V69, P43, DOI 10.1016/j.image.2018.03.013
   Su YC, 2017, ADV NEUR IN, V30
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   Wieczorek MA, 2005, GEOPHYS J INT, V162, P655, DOI [10.1111/j.1365-246X.2005.02687.x, 10.1111/j-1365-246X.2005.02687.x]
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Youvalari RG, 2016, IEEE INT SYM MULTIM, P525, DOI [10.1109/ISM.2016.74, 10.1109/ISM.2016.0115]
   Zelnik-Manor L, 2005, IEEE I CONF COMP VIS, P1292
   Zhang K, 2019, IEEE T CIRC SYST VID, V29, P3544, DOI 10.1109/TCSVT.2018.2883305
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
NR 68
TC 46
Z9 48
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2331
EP 2344
DI 10.1109/TMM.2019.2957986
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200009
DA 2024-07-18
ER

PT J
AU Wang, DY
   Sun, Y
   Zhu, C
   Li, WS
   Dufaux, F
AF Wang, Dayong
   Sun, Yu
   Zhu, Ce
   Li, Weisheng
   Dufaux, Frederic
TI Fast Depth and Inter Mode Prediction for Quality Scalable High
   Efficiency Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoding; Correlation; Prediction algorithms; Video coding; Complexity
   theory; Static VAr compensators; Scalability; SHVC; coding depth; coding
   mode; mode prediction; early termination
ID DECISION ALGORITHM; MOTION ESTIMATION; UNIT DEPTH; SIZE; CU
AB The scalable high efficiency video coding (SHVC) is an extension of high efficiency video coding (HEVC). It introduces multiple layers and inter-layer prediction, thus significantly increases the coding complexity on top of the already complicated HEVC encoder. In inter prediction for quality SHVC, in order to determine the best possible mode at each depth level, a coding tree unit can be recursively split into four depth levels, including merge mode, inter2Nx2N, inter2NxN, interNx2N, interNxN, inter2NxnU, inter2NxnD, internLx2N and internRxx2N, intra modes and inter-layer reference (ILR) mode. This can obtain the highest coding efficiency, but also result in very high coding complexity. Therefore, it is crucial to improve coding speed while maintaining coding efficiency. In this research, we have proposed a new depth level and inter mode prediction algorithm for quality SHVC. First, the depth level candidates are predicted based on inter-layer correlation, spatial correlation and its correlation degree. Second, for a given depth candidate, we divide mode prediction into square and non-square mode predictions respectively. Third, in the square mode prediction, ILR and merge modes are predicted according to depth correlation, and early terminated whether residual distribution follows a Gaussian distribution. Moreover, ILR mode, merge mode and inter2Nx2N are early terminated based on significant differences in Rate Distortion (RD) costs. Fourth, if the early termination condition cannot be satisfied, non-square modes are further predicted based on significant differences in expected values of residual coefficients. Finally, inter-layer and spatial correlations are combined with residual distribution to examine whether to early terminate depth selection. Experimental results have demonstrated that, on average, the proposed algorithm can achieve a time saving of 71.14%, with a bit rate increase of 1.27%.
C1 [Wang, Dayong] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Big Data Bio Intelligence, Chongqing 400065, Peoples R China.
   [Wang, Dayong; Zhu, Ce] Univ Elect Sci & Technol China, Sch Informat & Commun Gineering, Chengdu 611731, Peoples R China.
   [Sun, Yu] Univ Cent Arkansas, Dept Comp Sci, Conway, AR 72035 USA.
   [Li, Weisheng] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
   [Dufaux, Frederic] Univ Paris Sud, Lab Signaux & Syst, CNRS, CentraleSupelec, F-91192 Gif Sur Yvette, France.
C3 Chongqing University of Posts & Telecommunications; University of
   Electronic Science & Technology of China; University of Central
   Arkansas; Chongqing University of Posts & Telecommunications; Universite
   Paris Cite; Centre National de la Recherche Scientifique (CNRS);
   Universite Paris Saclay
RP Zhu, C (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Gineering, Chengdu 611731, Peoples R China.
EM wangdayong@cqupt.edu.cn; yusun@uca.edu; eczhu@uestc.edu.cn;
   liws@cqupt.edu.cn; frederic.dufaux@l2s.centralesupelec.fr
RI Zhu, Ce/AEN-1875-2022; Dufaux, Frederic/HJJ-1496-2023
OI Dufaux, Frederic/0000-0001-6388-4112
FU Natural Science Foundation of China [61401247, 61571102, U1713213,
   61972060]; Chongqing Science and Technology Commission Project
   [cstc2016jcyjA0543, cstc2017jcyjAX0142, cstc2018jcyjAX0525,
   cstc2018jcyjAX0225, cstc2018jszx-cyzdX0124]; Key Project of Sichuan
   Provincial Department of Science and Technology [2018JY0035]; Project of
   Sichuan Provincial Department of Science and Technology [2018RZ0072];
   Open Project of Hubei University of Arts and Science [XK2018013];
   International Science and Cooperation Project of Hubei Province
   [2019AHB059]; University of Central Arkansas
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61401247, 61571102, U1713213, and 61972060, in part
   by Chongqing Science and Technology Commission Project under Grants
   cstc2016jcyjA0543, cstc2017jcyjAX0142, cstc2018jcyjAX0525,
   cstc2018jcyjAX0225, and cstc2018jszx-cyzdX0124, in part by the Key
   Project of Sichuan Provincial Department of Science and Technology under
   Grant 2018JY0035, in part by Project of Sichuan Provincial Department of
   Science and Technology under Grant 2018RZ0072, in part by Open Project
   of Hubei University of Arts and Science under Grant XK2018013, in part
   by International Science and Cooperation Project of Hubei Province under
   Grant 2019AHB059, and in part by faculty sabbatical leave fund from
   University of Central Arkansas.
CR [Anonymous], 2014, JCTVCQ1009
   Bjontegaard G., 2001, P ITU T Q 6 SG16 VCE
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Chen MJ, 2018, IEEE T IND INFORM, V14, P4735, DOI 10.1109/TII.2018.2801852
   Cho S, 2013, IEEE T CIRC SYST VID, V23, P1555, DOI 10.1109/TCSVT.2013.2249017
   Corrêa G, 2012, IEEE T CIRC SYST VID, V22, P1899, DOI 10.1109/TCSVT.2012.2223411
   Hu N, 2014, IEEE T CIRC SYST VID, V24, P1310, DOI 10.1109/TCSVT.2014.2306035
   Jung SW, 2010, IEEE T CIRC SYST VID, V20, P201, DOI 10.1109/TCSVT.2009.2031387
   Lee Bumshik, 2008, 5th International Conference on Visual Information Engineering, VIE 2008, P753, DOI 10.1049/cp:20080412
   Lee B, 2011, IEEE SIGNAL PROC LET, V18, P571, DOI 10.1109/LSP.2011.2163935
   Li H, 2006, IEEE INT SYMP CIRC S, P3005
   Li H, 2006, IEEE T CIRC SYST VID, V16, P889, DOI 10.1109/TCSVT.2006.877404
   Lin HC, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P765, DOI 10.1109/ICME.2008.4607547
   Lu X, 2013, IEEE T CIRC SYST VID, V23, P846, DOI 10.1109/TCSVT.2012.2226525
   Pan ZQ, 2014, IEEE T BROADCAST, V60, P405, DOI 10.1109/TBC.2014.2321682
   Park CS, 2009, IEEE T CIRC SYST VID, V19, P1915, DOI 10.1109/TCSVT.2009.2031520
   Ren JF, 2008, IEEE T CONSUM ELECTR, V54, P877, DOI 10.1109/TCE.2008.4560174
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shen L., IEEE T MULTIMEDIA
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen LQ, 2010, IEEE SIGNAL PROC LET, V17, P887, DOI 10.1109/LSP.2010.2066966
   Tohidypour H. R., 2013, P 6 BALK C INF BCI T, P61
   Tohidypour H. R., 2013, JCTVCL0042 ITUT SG 1
   Tohidypour HR, 2017, IEEE T CIRC SYST VID, V27, P2204, DOI 10.1109/TCSVT.2016.2576738
   Tohidypour HR, 2016, IEEE T BROADCAST, V62, P664, DOI 10.1109/TBC.2016.2576600
   Tohidypour HR, 2016, IEEE T MULTIMEDIA, V18, P182, DOI 10.1109/TMM.2015.2510332
   Tohidypour HR, 2013, INT CONF ACOUST SPEE, P1744, DOI 10.1109/ICASSP.2013.6637951
   Vanne J, 2014, IEEE T CIRC SYST VID, V24, P1579, DOI 10.1109/TCSVT.2014.2308453
   Wang DY, 2019, IEEE T IMAGE PROCESS, V28, P2063, DOI 10.1109/TIP.2017.2740161
   Wang DY, 2014, J VIS COMMUN IMAGE R, V25, P1631, DOI 10.1016/j.jvcir.2014.07.006
   Wang HL, 2007, IEEE T MULTIMEDIA, V9, P728, DOI 10.1109/TMM.2007.893336
   Xiong B, 2008, IEEE T CIRC SYST VID, V18, P1441, DOI 10.1109/TCSVT.2008.2002824
   Yeh CH, 2018, J VIS COMMUN IMAGE R, V55, P342, DOI 10.1016/j.jvcir.2018.06.008
   Yeh CH, 2010, IEEE T CIRC SYST VID, V20, P563, DOI 10.1109/TCSVT.2010.2041825
   Yeh CH, 2019, J VIS COMMUN IMAGE R, V58, P462, DOI 10.1016/j.jvcir.2018.12.021
   Zhang Y, 2015, IEEE T IMAGE PROCESS, V24, P2225, DOI 10.1109/TIP.2015.2417498
   Zhao TS, 2012, IEEE T IMAGE PROCESS, V21, P2607, DOI 10.1109/TIP.2012.2186148
   Zhu C, 2004, IEEE T CIRC SYST VID, V14, P1210, DOI 10.1109/TCSVT.2004.833166
   Zhu C, 2009, IEEE T CIRC SYST VID, V19, P1183, DOI 10.1109/TCSVT.2009.2020264
NR 39
TC 15
Z9 15
U1 4
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 833
EP 845
DI 10.1109/TMM.2019.2937240
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xia, ZQ
   Hong, XP
   Gao, XY
   Feng, XY
   Zhao, GY
AF Xia, Zhaoqiang
   Hong, Xiaopeng
   Gao, Xingyu
   Feng, Xiaoyi
   Zhao, Guoying
TI Spatiotemporal Recurrent Convolutional Networks for Recognizing
   Spontaneous Micro-Expressions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Spatiotemporal phenomena; Feature extraction; Task analysis; Videos;
   Training; Strain; Deep learning; Micro-Expression Recognition;
   Spatiotemporal Modeling; Temporal Connectivity; Recurrent Convolutional
   Networks; Data Augmentation; Balanced Loss
ID RECOGNITION; DYNAMICS
AB Recently, the recognition task of spontaneous facial micro-expressions has attracted much attention with its various real-world applications. Plenty of handcrafted or learned features have been employed for a variety of classifiers and achieved promising performances for recognizing micro-expressions. However, the micro-expression recognition is still challenging due to the subtle spatiotemporal changes of micro-expressions. To exploit the merits of deep learning, we propose a novel deep recurrent convolutional networks based micro-expression recognition approach, capturing the spatiotemporal deformations of micro-expression sequence. Specifically, the proposed deep model is constituted of several recurrent convolutional layers for extracting visual features and a classificatory layer for recognition. It is optimized by an end-to-end manner and obviates manual feature design. To handle sequential data, we exploit two ways to extend the connectivity of convolutional networks across temporal domain, in which the spatiotemporal deformations are modeled in views of facial appearance and geometry separately. Besides, to overcome the shortcomings of limited and imbalanced training samples, two temporal data augmentation strategies as well as a balanced loss are jointly used for our deep network. By performing the experiments on three spontaneous micro-expression datasets, we verify the effectiveness of our proposed micro-expression recognition approach compared to the state-of-the-art methods.
C1 [Xia, Zhaoqiang; Feng, Xiaoyi] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China.
   [Hong, Xiaopeng] Xi An Jiao Tong Univ, Xian 710049, Peoples R China.
   [Gao, Xingyu] Chinese Acad Sci, Inst Microelect, Beijing 100029, Peoples R China.
   [Zhao, Guoying] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland.
C3 Northwestern Polytechnical University; Xi'an Jiaotong University;
   Chinese Academy of Sciences; Institute of Microelectronics, CAS;
   University of Oulu
RP Gao, XY (corresponding author), Chinese Acad Sci, Inst Microelect, Beijing 100029, Peoples R China.
EM xiazhaoqiang@gmail.com; hongxiaopeng@mail.xjtu.edu.cn;
   gxy9910@gmail.com; fengxiao@nwpu.edu.cn; guoying.zhao@oulu.fi
RI Xia, Zhaoqiang/AAC-4021-2019; HONG, Xiaopeng/V-6078-2019; Gao,
   Xingyu/AAL-3288-2021; Zhao, Guoying/ABE-7716-2020
OI Xia, Zhaoqiang/0000-0003-0630-3339; HONG, Xiaopeng/0000-0002-0611-0636;
   Gao, Xingyu/0000-0002-4660-8092; Zhao, Guoying/0000-0003-3694-206X
FU National Natural Science Foundation of China [61702419, 61702491];
   Natural Science Basic Research Plan in Shaanxi Province of China
   [2018JQ6090]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61702419 and Grant 61702491 and in part
   by the Natural Science Basic Research Plan in Shaanxi Province of China
   (2018JQ6090). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhu Liu.
CR Afzal S., 2009, Proceedings of the 3rd International Conference on Affective Computing and Intelligent Interaction, P1
   Ngo ACL, 2017, IEEE T AFFECT COMPUT, V8, P396, DOI 10.1109/TAFFC.2016.2523996
   [Anonymous], 2013, 2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG), DOI [DOI 10.1109/FG.2013.6553799, DOI 10.1109/FG.2013.6553799.IEEE]
   [Anonymous], 2017, 2017 INT C DIG IM
   [Anonymous], 2012, ACM Transactions on Graphics (TOG), DOI DOI 10.1145/2185520.2185561
   Chung J., 2014, NIPS 2014 WORKSH DEE, Vabs/1412.3555, P1, DOI DOI 10.48550/ARXIV.1412.3555
   Davison AK, 2018, IEEE T AFFECT COMPUT, V9, P116, DOI 10.1109/TAFFC.2016.2573832
   Davison C, 2014, TRANSLATION AS COLLABORATION: VIRGINIA WOOLF, KATHERINE MANSFIELD AND S. S. KOTELIANSKY, P111
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan XD, 2016, NEUROCOMPUTING, V217, P27, DOI 10.1016/j.neucom.2016.03.090
   Ekman P., 2007, MICROEXPRESSION TRAI
   Fangbing Qu, 2018, IEEE Transactions on Affective Computing, V9, P424, DOI 10.1109/TAFFC.2017.2654440
   Gao XY, 2017, ACM T INTEL SYST TEC, V8, DOI 10.1145/3065950
   Gao XY, 2014, AAAI CONF ARTIF INTE, P1206
   Gao XY, 2016, NEUROCOMPUTING, V173, P1927, DOI 10.1016/j.neucom.2015.09.064
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Huang XH, 2019, IEEE T AFFECT COMPUT, V10, P32, DOI 10.1109/TAFFC.2017.2713359
   Huang XH, 2016, NEUROCOMPUTING, V175, P564, DOI 10.1016/j.neucom.2015.10.096
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kim H, 2018, IEEE T MULTIMEDIA, V20, P2415, DOI 10.1109/TMM.2018.2806224
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Li X., 2017, IEEE Transactions on Affective Computing
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liong ST, 2018, SIGNAL PROCESS-IMAGE, V62, P82, DOI 10.1016/j.image.2017.11.006
   Liong ST, 2016, SIGNAL PROCESS-IMAGE, V47, P170, DOI 10.1016/j.image.2016.06.004
   Liu YJ, 2016, IEEE T AFFECT COMPUT, V7, P299, DOI 10.1109/TAFFC.2015.2485205
   Lu ZY, 2015, LECT NOTES COMPUT SC, V9009, P698, DOI 10.1007/978-3-319-16631-5_51
   Moilanen A, 2014, INT C PATT RECOG, P1722, DOI 10.1109/ICPR.2014.303
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Oh TH, 2018, LECT NOTES COMPUT SC, V11208, P663, DOI 10.1007/978-3-030-01225-0_39
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Pfister T, 2011, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2011.6126401
   Pinheiro Pedro., 2014, International conference on machine learning. PMLR, P82
   POLIKOVSKY S., 2009, 3 INT C IM CRIM DET, P1, DOI [10.1049/ic.2009.0244, DOI 10.1049/IC.2009.0244]
   Ruiz Ruiz Jorge Carlos, 2013, ISRN Biotechnol, V2013, P341974, DOI 10.5402/2013/341974
   Shen XB, 2012, J ZHEJIANG UNIV-SC B, V13, P221, DOI 10.1631/jzus.B1100063
   Shreve Matthew, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P51, DOI 10.1109/FG.2011.5771451
   SHREVE M., 2009, 2009 WORKSH APPL COM, P1, DOI [10.1109/WACV.2009.5403044, DOI 10.1109/WACV.2009.5403044]
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Sun YX, 2018, IEEE ACCESS, V6, P56977, DOI 10.1109/ACCESS.2018.2873019
   Takalkar M, 2018, MULTIMED TOOLS APPL, V77, P19301, DOI 10.1007/s11042-017-5317-2
   Urban G., 2017, ICLR
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang SJ, 2017, NEUROCOMPUTING, V230, P382, DOI 10.1016/j.neucom.2016.12.034
   Wang SJ, 2015, IEEE T IMAGE PROCESS, V24, P6034, DOI 10.1109/TIP.2015.2496314
   Wang SJ, 2015, LECT NOTES COMPUT SC, V8925, P325, DOI 10.1007/978-3-319-16178-5_23
   Wang SJ, 2014, INT C PATT RECOG, P4678, DOI 10.1109/ICPR.2014.800
   Wang YX, 2014, 2014 IEEE 13TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI-CC), P21, DOI 10.1109/ICCI-CC.2014.6921436
   Xia ZQ, 2016, COMPUT VIS IMAGE UND, V147, P87, DOI 10.1016/j.cviu.2015.12.006
   Xiao-Bin Huang, 2015, 2015 Asia-Pacific Microwave Conference (APMC). Proceedings, P1, DOI 10.1109/APMC.2015.7413025
   Xu F, 2017, IEEE T AFFECT COMPUT, V8, P254, DOI 10.1109/TAFFC.2016.2518162
   Xu L, 2016, PROCEEDINGS OF 2016 2ND INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND INTERNET OF THINGS (CCIOT), P1, DOI 10.1109/CCIOT.2016.7868293
   Xue L, 2014, ROCK MECH ROCK ENG, V47, P1183, DOI 10.1007/s00603-013-0479-3
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Yao L, 2014, PROCEEDING OF KNOWLEDGE MANAGEMENT INTERNATIONAL CONFERENCE (KMICE) 2014, VOLS 1 AND 2, P1
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhou J, 2016, IEEE INT CON DIS, P84, DOI 10.1109/ICDCSW.2016.14
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
   Zong Y, 2018, IEEE T MULTIMEDIA, V20, P3160, DOI 10.1109/TMM.2018.2820321
NR 67
TC 120
Z9 126
U1 6
U2 100
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 626
EP 640
DI 10.1109/TMM.2019.2931351
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700005
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Xu, ZY
   Cao, Y
   Wang, W
   Jiang, T
   Zhang, Q
AF Xu, Zeyu
   Cao, Yang
   Wang, Wei
   Jiang, Tao
   Zhang, Qian
TI Incentive Mechanism for Cooperative Scalable Video Coding (SVC)
   Multicast Based on Contract Theory
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Relays; Contracts; Streaming media; Static VAr compensators; Receivers;
   Quality of experience; Video coding; SVC multicast; relay; asymmetric
   information; contract theory; QoE
ID RESOURCE-ALLOCATION; RELAY-SELECTION; NETWORKS; CSI
AB In scalable video coding (SVC) multicast, videos are encoded into several layers that represent multiple quality levels. Mobile users with different wireless channel conditions can obtain different numbers of layers and have different quality of experience (QoE). To enhance the QoE of the users that suffer from the worse channel quality, it is beneficial to stimulate users' cooperation in relaying enhancement layers. However, potential relays may be unwilling to truthfully cooperate with receivers, which results in the asymmetric information problem in relay selecting. In this paper, we model the video relaying selection as a market with multiple receivers (principals) and relays (agents), and solve the problem according to the contract theory. The proposed solution is divided into following two steps: first, contract design and item preselection, and second, matching between each principal and agent. We propose a contract parameter determination method termed as the Matching-Aware strategy. Different from traditional strategies, the proposed Matching-Aware strategy makes the contract competitive in principal-agent matching without knowing the probability distribution of relays' types. The matching step is undertaken by the base station with the purpose of maximizing the social welfare. Numerical results corroborate that the contract-based video relaying scheme can tackle the asymmetric information problem. Besides, compared with other two baseline strategies, the proposed Matching-Aware strategy achieves higher QoE.
C1 [Xu, Zeyu; Cao, Yang; Wang, Wei; Jiang, Tao] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
   [Zhang, Qian] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
C3 Huazhong University of Science & Technology; Hong Kong University of
   Science & Technology
RP Cao, Y (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
EM xzyxzyxzy@hust.edu.cn; ycao@hust.edu.cn; weiwangw@hust.edu.cn;
   Tao.hang@ieee.org; qianzh@cse.ust.hk
RI Zhang, Qian/B-9058-2009; jiang, tao/GWC-7108-2022; Cao,
   Yang/HGD-6463-2022; zeyu, xu/GSD-2526-2022; Jiang, Tao/IWM-7503-2023;
   Wang, Wei/AAB-7341-2019
OI Zhang, Qian/0000-0001-9205-1881; Wang, Wei/0000-0002-2772-4856
FU National Natural Science Foundation of China [61729101, 61601193,
   61720106001]; National Natural Science Foundation of Hubei in China
   [2016CFA009]; Fundamental Research Funds for the Central Universities
   [2015ZDTD012]; RGC [CERG 16203215]; Guangdong Natural Science Foundation
   [2017A030312008]; Key Laboratory of Dynamic Cognitive System of
   Electromagnetic Spectrum Space (Nanjing University of Aeronautics and
   Astronautics), Ministry of Industry and Information Technology, Nanjing,
   China [KF20181911]; Young Elite Scientists Sponsorship Program by CAST
   [2018QNRC001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61729101, 61601193, 61720106001; in
   part the Major Program of National Natural Science Foundation of Hubei
   in China under Grant 2016CFA009; in part by the Fundamental Research
   Funds for the Central Universities under Grant 2015ZDTD012; in part by
   the RGC under Contract CERG 16203215; in part by the Guangdong Natural
   Science Foundation under Grant 2017A030312008; in part by the Key
   Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space
   (Nanjing University of Aeronautics and Astronautics), Ministry of
   Industry and Information Technology, Nanjing, 211106, China under Grant
   KF20181911; and in part by the Young Elite Scientists Sponsorship
   Program by CAST under Grant 2018QNRC001. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sanjeev Mehrotra.
CR Canzian L, 2013, IEEE T COMMUN, V61, P700, DOI 10.1109/TCOMM.2012.120512.110654
   Cao Y, 2016, IEEE WIREL COMMUN, V23, P52, DOI 10.1109/MWC.2016.7553026
   Cao Y, 2016, IEEE T MOBILE COMPUT, V15, P1528, DOI 10.1109/TMC.2015.2461214
   Cao Y, 2015, IEEE WIREL COMMUN, V22, P124, DOI 10.1109/MWC.2015.7143335
   Choi YI, 2012, IEEE COMMUN LETT, V16, P1436, DOI 10.1109/LCOMM.2012.070512.120753
   Duan LJ, 2014, IEEE T MOBILE COMPUT, V13, P174, DOI 10.1109/TMC.2012.231
   Feng DQ, 2016, IEEE T VEH TECHNOL, V65, P6051, DOI 10.1109/TVT.2015.2479258
   Gao L, 2011, IEEE J SEL AREA COMM, V29, P843, DOI 10.1109/JSAC.2011.110415
   Guan ZY, 2013, IEEE ACM T NETWORK, V21, P1173, DOI 10.1109/TNET.2013.2248020
   Hasan Z, 2013, IEEE T WIREL COMMUN, V12, P3824, DOI 10.1109/TWC.2013.051613.121171
   Huang HY, 2015, IET COMMUN, V9, P460, DOI 10.1049/iet-com.2014.0429
   Ibrahim AS, 2008, IEEE T WIREL COMMUN, V7, P2814, DOI 10.1109/TWC.2008.070176
   Jaafar W, 2016, IEEE T VEH TECHNOL, V65, P673, DOI 10.1109/TVT.2015.2402193
   Krikidis I, 2008, IEEE COMMUN LETT, V12, P235, DOI 10.1109/LCOMM.2008.071987
   Krishna V, 2010, AUCTION THEORY, 2ND EDITION, P1
   Kuo WH, 2015, IEEE T CIRC SYST VID, V25, P812, DOI 10.1109/TCSVT.2014.2363741
   Li C., 2017, P ITU KAL CHALL DAT, P1
   Little T, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P1, DOI 10.1109/GlobalSIP.2015.7416924
   Liu YS, 2014, EURASIP J WIREL COMM, DOI 10.1186/1687-1499-2014-211
   Long CC, 2018, IEEE T MULTIMEDIA, V20, P1126, DOI 10.1109/TMM.2017.2764330
   Makki B, 2010, IEEE COMMUN LETT, V14, P806, DOI 10.1109/LCOMM.2010.072610.100670
   Nazari B., 2012, 2012 1st IEEE International Conference on Communications in China (ICCC 2012), P798, DOI 10.1109/ICCChina.2012.6356994
   Nazari B, 2013, ASIA-PAC CONF COMMUN, P757, DOI 10.1109/APCC.2013.6766050
   Schmitt M., 2016, P 8 INT C QUAL MULT, P1, DOI DOI 10.1109/QOMEX.2016.7498961
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shao MK, 2011, IEEE T MULTIMEDIA, V13, P353, DOI 10.1109/TMM.2010.2095833
   Steiglitz K., 1998, Combinatorial Optimization: Algorithms and Complexity
   Vella JM, 2013, IEEE COMMUN SURV TUT, V15, P718, DOI 10.1109/SURV.2012.050412.00095
   Wang R, 2016, IEEE T WIREL COMMUN, V15, P7594, DOI 10.1109/TWC.2016.2604813
   Wang W, 2019, IEEE T WIREL COMMUN, V18, P796, DOI 10.1109/TWC.2018.2883443
   Wang YX, 2013, 2013 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE WORKSHOPS (WCNCW), P60, DOI 10.1109/WCNCW.2013.6533316
   Wu D, 2017, IEEE T MULTIMEDIA, V19, P2571, DOI 10.1109/TMM.2017.2700621
   Yang DJ, 2012, IEEE WIREL COMMUN, V19, P44, DOI 10.1109/MWC.2012.6189412
   Yu CY, 2012, PROCEEDINGS OF THE 2012 12TH INTERNATIONAL SYMPOSIUM ON PERVASIVE SYSTEMS, ALGORITHMS, AND NETWORKS (I-SPAN 2012), P162, DOI 10.1109/I-SPAN.2012.30
   Zhang SJ, 2018, IEEE T COGN COMMUN, V4, P1, DOI 10.1109/TCCN.2017.2779858
   Zhang YR, 2017, IEEE J SEL AREA COMM, V35, P643, DOI 10.1109/JSAC.2017.2672178
   Zhao HV, 2012, IEEE T MULTIMEDIA, V14, P717, DOI 10.1109/TMM.2012.2191394
   Zhao RM, 2018, IEEE T MULTIMEDIA, V20, P3069, DOI 10.1109/TMM.2018.2827783
   Zhou H, 2015, IEEE T WIREL COMMUN, V14, P3673, DOI 10.1109/TWC.2015.2409834
   Zhou H, 2012, IEEE GLOB COMM CONF, P5681, DOI 10.1109/GLOCOM.2012.6504026
   Zhu H, 2015, IEEE NETWORK, V29, P6, DOI 10.1109/MNET.2015.7340418
NR 41
TC 10
Z9 10
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 445
EP 458
DI 10.1109/TMM.2019.2929965
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300013
DA 2024-07-18
ER

PT J
AU An, S
   Liu, S
   Huang, ZB
   Che, GF
   Bao, Q
   Zhu, ZQ
   Chen, Y
   Weng, DZ
AF An, Shan
   Liu, Si
   Huang, Zhibiao
   Che, Guangfu
   Bao, Qian
   Zhu, Zhaoqi
   Chen, Yu
   Weng, Dennis Z.
TI RotateView: A Video Composition System for Interactive Product Display
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video object segmentation; optical flow; video composition
ID OBJECT SEGMENTATION; EXTRACTION
AB Product display in e-commerce commonly uses static images or videos. In this paper, we design a novel video composition system "RotateView" which displays real-world products videos on the mobile phone in an interactive and 3D-like way. The system estimates the rotation direction of products using optical flow, segments products using an improved version of online adaptive video object segmentation, and adjusts motion and color for composition. The rotation direction estimation methods are evaluated on a video dataset of 1500 rotated videos that are uploaded by sellers, and on our collected real-world video object segmentation (RVOS) dataset. On our RVOS dataset, experimental results show an improvement in segmentation over the state of the art. Moreover, the proposed algorithms for motion and color adjustment are evaluated using extensive user studies. The RVOS dataset can be downloaded via https://realvos.github.io/ for further studies and code will be made available.
C1 [An, Shan; Liu, Si] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Huang, Zhibiao; Che, Guangfu; Bao, Qian; Zhu, Zhaoqi; Chen, Yu; Weng, Dennis Z.] JD COM, Beijing 100108, Peoples R China.
C3 Beihang University
RP Liu, S (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
EM 384734954@qq.com; liusi@buaa.edu.cn; huangzhibiao3@jd.com;
   cheguangfu1@jd.com; baoqian@jd.com; zhuzhaoqi@jd.com; chenyu6@jd.com;
   wengzhi@jd.com
RI An, Shan/AAK-9016-2020
OI liu, si/0000-0002-9180-2935; An, Shan/0000-0001-7796-6952
FU Natural Science Foundation of China [U1536203, 61572493, 61876177]
FX This work was supported by the Natural Science Foundation of China under
   Grants U1536203, 61572493, and 61876177. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Chang-Su Kim.
CR Abdollahian G, 2010, IEEE T MULTIMEDIA, V12, P28, DOI 10.1109/TMM.2009.2036286
   [Anonymous], 2017, CVPR
   [Anonymous], EFFICIENT INFERENCE
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2017, 2017 IEEE C COMP VIS
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2017, IEEE CVPR
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P2225, DOI 10.1109/TMM.2015.2481711
   Chen Q, 2013, IEEE T MULTIMEDIA, V15, P521, DOI 10.1109/TMM.2012.2236306
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chen T, 2013, IEEE T IMAGE PROCESS, V22, P2532, DOI 10.1109/TIP.2013.2251642
   Cheng JC, 2017, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2017.81
   Chien SY, 2004, IEEE T MULTIMEDIA, V6, P732, DOI 10.1109/TMM.2004.834868
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Galasso Fabio., 2012, ACCV, P760
   Hu YL, 2016, PROC CVPR IEEE, P5704, DOI 10.1109/CVPR.2016.615
   Jain SD, 2014, LECT NOTES COMPUT SC, V8692, P656, DOI 10.1007/978-3-319-10593-2_43
   Kuehni RG, 2002, COLOR RES APPL, V27, P20, DOI 10.1002/col.10002
   Lei LH, 2015, IEEE INT CONF MOB CL, P6, DOI 10.1109/MobileCloud.2015.30
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SC, 2014, PROC CVPR IEEE, P4209, DOI 10.1109/CVPR.2014.536
   Lou ZY, 2014, IEEE T MULTIMEDIA, V16, P2110, DOI 10.1109/TMM.2014.2363936
   Luo B, 2017, IEEE T MULTIMEDIA, V19, P1482, DOI 10.1109/TMM.2017.2671447
   Ma TY, 2012, PROC CVPR IEEE, P670, DOI 10.1109/CVPR.2012.6247735
   Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Pitie F., 2007, P EUR C VIS MED PROD
   Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Redmon Joseph, 2013, Darknet: Open Source Neural Networks in C, DOI DOI 10.1109/CVPR.2016.91
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x
   Sunkavalli Kalyan, 2010, ACM T GRAPHIC, V29, P4
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423
   Tsai YH, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925942
   Tsai Yi-Hsuan, 2017, CVPR, P3789
   Voigtlaender Paul, 2017, ARXIV170609364
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Wu ZF, 2019, PATTERN RECOGN, V90, P119, DOI 10.1016/j.patcog.2019.01.006
   Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45
   Yang J, 2016, IEEE T CIRC SYST VID, V26, P1070, DOI 10.1109/TCSVT.2015.2433171
   Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
NR 53
TC 3
Z9 3
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3095
EP 3105
DI 10.1109/TMM.2019.2918720
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200010
DA 2024-07-18
ER

PT J
AU Purwanto, D
   Chen, YT
   Fang, WH
AF Purwanto, Didik
   Chen, Yie-Tarng
   Fang, Wen-Hsien
TI First-Person Action Recognition With Temporal Pooling and Hilbert-Huang
   Transform
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hilbert-Huang transform; temporal pooling; video descriptor; temporal
   aggregation; first-person videos; action recognition
ID EMPIRICAL MODE DECOMPOSITION; CLASSIFICATION; FEATURES; VISION; DENSE
AB This paper presents a convolutional neural network (CNN)-based approach for first-person action recognition with a combination of temporal pooling and the Hilbert-Huang transform (HHT). The new approach first adaptively performs temporal subaction localization, treats each channel of the extracted trajectory pooled CNN features as a time series, and summarizes the temporal dynamic information in each sub-action by temporal pooling. The temporal evolution across sub-actions is then modeled by rank pooling. Thereafter, to account for the highly dynamic scene changes in first-person videos, the HHT is employed to decompose the ranked pooling features into finite and often few data-dependent functions, called intrinsic mode functions (IMFs), through empirical mode decomposition. Hilbert spectral analysis is then applied to each IMF component, and four salient descriptors are scrutinized and aggregated into the final video descriptor. Such a framework cannot only precisely acquire both long- and short-term tendencies, but also address the cumbersome significant camera motion in first-person videos to render better accuracy. Furthermore, it works well for complex actions for limited training samples. Simulations show that the proposed approach outperforms the main state-of-the-art methods when applied to four publicly available first-person video datasets.
C1 [Purwanto, Didik; Chen, Yie-Tarng; Fang, Wen-Hsien] Natl Taiwan Univ Sci & Technol, Dept Elect & Comp Engn, Taipei 106, Taiwan.
C3 National Taiwan University of Science & Technology
RP Chen, YT (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Elect & Comp Engn, Taipei 106, Taiwan.
EM d10602806@mail.ntust.edu.tw; ytchen@mail.ntust.edu.tw;
   whf@mail.ntust.edu.tw
RI Purwanto, Didik/AAX-6416-2020
OI Purwanto, Didik/0000-0001-7240-052X; Fag, Wen-Hsien/0000-0001-6402-2688
FU Ministry of Science and Technology, Taiwan [MOST 107-2221-E-011124, MOST
   107-2221-E-011-078-MY2]
FX This work was supported by the Ministry of Science and Technology,
   Taiwan, under Contracts MOST 107-2221-E-011124 and MOST
   107-2221-E-011-078-MY2. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Xilin Chen.
CR Abebe G, 2017, IEEE INT CONF COMP V, P1339, DOI 10.1109/ICCVW.2017.159
   Abebe G, 2016, COMPUT VIS IMAGE UND, V149, P229, DOI 10.1016/j.cviu.2015.10.015
   [Anonymous], 2017, ARXIV170200338
   [Anonymous], 2016 IEEE Winter Conf. Appl. Comput. Vision, DOI [DOI 10.1109/WACV.2016.7477708, 10.1109/WACV.2016.7477708]
   Barros, 2018, IEEE INT JOINT C NEU, P1
   Bolaños M, 2014, LECT NOTES COMPUT SC, V8563, P1, DOI 10.1007/978-3-319-08849-5_1
   Bracewell R. N., 1986, FOURIER TRANSFORM IT
   Bregonzio M, 2009, PROC CVPR IEEE, P1948, DOI 10.1109/CVPRW.2009.5206779
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fa LL, 2018, LECT NOTES COMPUT SC, V10705, P153, DOI 10.1007/978-3-319-73600-6_14
   Fernando B, 2017, IEEE T PATTERN ANAL, V39, P773, DOI 10.1109/TPAMI.2016.2558148
   Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176
   Han YM, 2017, IEEE COMPUT SOC CONF, P1226, DOI 10.1109/CVPRW.2017.162
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Huang NE, 2014, INTRO HILBERT HUANG
   Hyvärinen A, 2010, NEUROIMAGE, V49, P257, DOI 10.1016/j.neuroimage.2009.08.028
   Ibrahim RK, 2008, IEEE ENG MED BIO, P3852, DOI 10.1109/IEMBS.2008.4650050
   Iwashita Y, 2014, INT C PATT RECOG, P4310, DOI 10.1109/ICPR.2014.739
   Javidani A, 2018, IRAN CONF ELECTR ENG, P1629, DOI 10.1109/ICEE.2018.8472580
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kahani R, 2016, IRAN CONF ELECTR ENG, P805, DOI 10.1109/IranianCEE.2016.7585630
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kitani K. M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3241, DOI 10.1109/CVPR.2011.5995406
   Krinidis S, 2013, IMAGE VISION COMPUT, V31, P533, DOI 10.1016/j.imavis.2013.04.005
   Kumano S, 2017, IEEE T MULTIMEDIA, V19, P107, DOI 10.1109/TMM.2016.2608002
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Li Q, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/2911996.2912001
   Li WX, 2013, IEEE I CONF COMP VIS, P2728, DOI 10.1109/ICCV.2013.339
   Liu MY, 2018, IEEE T MULTIMEDIA, V20, P1932, DOI 10.1109/TMM.2017.2786868
   Matikainen P, 2010, LECT NOTES COMPUT SC, V6311, P508, DOI 10.1007/978-3-642-15549-9_37
   Monteiro J, 2017, IEEE IJCNN, P2267, DOI 10.1109/IJCNN.2017.7966130
   Moreira TP, 2017, INT CONF ACOUST SPEE, P2627, DOI 10.1109/ICASSP.2017.7952632
   Narayan S, 2014, IEEE COMPUT SOC CONF, P526, DOI 10.1109/CVPRW.2014.82
   Nguyen K, 2018, IEEE ACCESS, V6, P18848, DOI 10.1109/ACCESS.2017.2784352
   Ni BB, 2015, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2015.7298993
   Ni BB, 2015, INT J COMPUT VISION, V111, P229, DOI 10.1007/s11263-014-0742-4
   Oweis RJ, 2011, BIOMED ENG ONLINE, V10, DOI 10.1186/1475-925X-10-38
   Özkan F, 2017, EUR SIGNAL PR CONF, P1050, DOI 10.23919/EUSIPCO.2017.8081368
   Paithane A. N., 2016, INT J COMP APPL, V9, P6
   Piergiovanni AJ, 2018, PROC CVPR IEEE, P5304, DOI 10.1109/CVPR.2018.00556
   Purwanto D, 2017, IEEE INT CON MULTI, P895, DOI 10.1109/ICME.2017.8019520
   Riaz F, 2016, IEEE T NEUR SYS REH, V24, P28, DOI 10.1109/TNSRE.2015.2441835
   Rogez G, 2015, PROC CVPR IEEE, P4325, DOI 10.1109/CVPR.2015.7299061
   Ryoo MS, 2015, PROC CVPR IEEE, P896, DOI 10.1109/CVPR.2015.7298691
   Ryoo MS, 2013, PROC CVPR IEEE, P2730, DOI 10.1109/CVPR.2013.352
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shu XB, 2017, IEEE COMPUT SOC CONF, P2176, DOI 10.1109/CVPRW.2017.270
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Song SB, 2015, LECT NOTES COMPUT SC, V9010, P445, DOI 10.1007/978-3-319-16634-6_33
   Sudhakaran S, 2017, IEEE INT CONF COMP V, P2339, DOI 10.1109/ICCVW.2017.276
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Tjandrasa H, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P1498, DOI 10.1109/CISP-BMEI.2016.7852954
   Tolwinski S., 2007, The hilbert transform and empirical mode decomposition as tools for data analysis
   Tran Du, 2017, ARXIV170805038
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang LM, 2013, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2013.345
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Zaki HFM, 2017, PROC CVPR IEEE, P1619, DOI 10.1109/CVPR.2017.176
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
   Zhu J, 2013, IEEE I CONF COMP VIS, P3559, DOI 10.1109/ICCV.2013.442
NR 72
TC 8
Z9 8
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3122
EP 3135
DI 10.1109/TMM.2019.2919434
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200012
DA 2024-07-18
ER

PT J
AU Cao, C
   Lu, F
   Li, C
   Lin, S
   Shen, XK
AF Cao, Chong
   Lu, Feng
   Li, Chen
   Lin, Stephen
   Shen, Xukun
TI Makeup Removal via Bidirectional Tunable De-Makeup Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face; Task analysis; Training; Image synthesis; Facial features; Tuning;
   Gallium nitride; Makeup removal; generative adversarial learning;
   bidirectional network
ID STYLE TRANSFER; FACE; COLOR; SKIN
AB We present a deep learning-based method for removing makeup effects (de-makeup) in a face image. This problem poses a major challenge due to obscuring of the underlying facial features by cosmetics, which is very important in multimedia applications in the field of security, entertainment, and social networking. To address this task, we propose the bidirectional tunable de-makeup network (BTD-Net), which jointly learns the makeup process to aid in learning the de-makeup process. For tractable learning of the makeup process, which is a one-to-many mapping determined by the cosmetics that are applied, we introduce a latent variable that reflects the makeup style. This latent variable is extracted in the de-makeup process and used as a condition on the makeup process to constrain the one-to-many mapping to a specific solution. Through extensive experiments, our proposed BTD-Net is found to surpass the state-of-art techniques in estimating realistic non-makeup faces that correspond to the input makeup images. We additionally show that applications such as tuning the amount of makeup can be enhanced through the use of this method.
C1 [Cao, Chong; Shen, Xukun] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch New Media Art & Design, Beijing 100191, Peoples R China.
   [Lu, Feng] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Lu, Feng] Beihang Univ, Beijing Adv Innovat Ctr Big Data Based Precis Med, Beijing 100191, Peoples R China.
   [Li, Chen] Tencent Youtu Lab, Shenzhen 518000, Peoples R China.
   [Lin, Stephen] Microsoft Res, Visual Comp Grp, Beijing 100080, Peoples R China.
C3 Beihang University; Beihang University; Beihang University; Tencent;
   Microsoft
RP Lu, F (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
EM chong@buaa.edu.cn; lufeng@buaa.edu.cn; chaselli@tencent.com;
   stevelin@microsoft.com; xkshen@buaa.edu.cn
OI Lu, Feng/0000-0001-9064-7964
FU National Natural Science Foundation of China (NSFC) [61602020, 61732016]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grants 61602020 and 61732016. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. David Crandall.
CR Alashkar T, 2017, AAAI CONF ARTIF INTE, P941
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2016, P IEEE C COMPUTER VI
   [Anonymous], 2016, P NIPS WORKSH ADV TR
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.632
   [Anonymous], 2017, Advances in neural information processing systems
   [Anonymous], ACM T GRAPH
   [Anonymous], 2013, International Conference on Multimedia
   [Anonymous], 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7299058
   [Anonymous], 2017, P INT C LEARN REPR
   Antipov G, 2017, IEEE IMAGE PROC, P2089, DOI 10.1109/ICIP.2017.8296650
   Chen CJ, 2016, INFORM FUSION, V32, P80, DOI 10.1016/j.inffus.2015.09.005
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Chen YC, 2017, IEEE I CONF COMP VIS, P4511, DOI 10.1109/ICCV.2017.482
   Dantcheva A., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P391, DOI 10.1109/BTAS.2012.6374605
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Doi M, 2005, LECT NOTES COMPUT SC, V3540, P95
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo D, 2009, PROC CVPR IEEE, P73, DOI 10.1109/CVPRW.2009.5206833
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang DA, 2013, IEEE I CONF COMP VIS, P2496, DOI 10.1109/ICCV.2013.310
   Jampour M, 2017, COMPUT VIS IMAGE UND, V161, P29, DOI 10.1016/j.cviu.2017.05.008
   Jin ZP, 2016, IEEE IJCNN, P2568, DOI 10.1109/IJCNN.2016.7727520
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kemelmacher-Shlizerman I, 2014, PROC CVPR IEEE, P3334, DOI 10.1109/CVPR.2014.426
   Lample Guillaume, 2017, P ANN C NEUR INF PRO, P5967
   Li C, 2015, PROC CVPR IEEE, P4621, DOI 10.1109/CVPR.2015.7299093
   Li HL, 2011, IEEE T MULTIMEDIA, V13, P1230, DOI 10.1109/TMM.2011.2168814
   Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618
   Li Y, 2018, AAAI CONF ARTIF INTE, P7057
   Liao J., 2017, ACM Trans. Graph.
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Moriuchi Y, 2009, LECT NOTES COMPUT SC, V5575, P138, DOI 10.1007/978-3-642-02230-2_15
   Scherbaum K, 2011, COMPUT GRAPH FORUM, V30, P485, DOI 10.1111/j.1467-8659.2011.01874.x
   Shiri F., 2017, International Conference on Digital Image Computing: Techniques and Applications (DICTA), P1
   Song ZC, 2017, IEEE T MULTIMEDIA, V19, P702, DOI 10.1109/TMM.2016.2631123
   Tong WS, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P211, DOI 10.1109/PG.2007.31
   Tsumura N, 2003, ACM T GRAPHIC, V22, P770, DOI 10.1145/882262.882344
   Wang SY, 2016, AAAI CONF ARTIF INTE, P58
   Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yin X, 2017, IEEE I CONF COMP VIS, P4010, DOI 10.1109/ICCV.2017.430
   Zhang S, 2013, STRUCTURES AND ARCHITECTURE: CONCEPTS: APPLICATIONS AND CHALLENGES, P190
   Zhang W, 2013, IEEE T MULTIMEDIA, V15, P1594, DOI 10.1109/TMM.2013.2265675
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
   Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679
NR 47
TC 9
Z9 11
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2750
EP 2761
DI 10.1109/TMM.2019.2911457
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000005
DA 2024-07-18
ER

PT J
AU Lyu, F
   Wu, Q
   Hu, FY
   Wu, QY
   Tan, MK
AF Lyu, Fan
   Wu, Qi
   Hu, Fuyuan
   Wu, Qingyao
   Tan, Mingkui
TI Attend and Imagine: Multi-Label Image Classification With Visual
   Attention and Recurrent Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-label classification; visual attention; deep learning;
   Convolutional Neural Network (CNN); Recurrent Neural Network (RNN)
ID ANNOTATION
AB Real images often have multiple labels, i.e., each image is associated with multiple objects or attributes. Compared to single-label image classification, the multilabel classification problem is much more challenging due to several issues. At first, multiple objects can be anywhere in the image. Second, the importance of different regions in an image is different, and the regions of interest in a multilabel image can be very different from another one. Finally, multiple labels of an image can have label dependencies due to complex image structures. To address these challenges, in this paper, we propose to predict the labels sequentially by applying the recurrent neural networks (RNNs), which are used to encode the label dependencies. When predicting a specific label, we introduce a dynamic attention mechanism to enable the model to focus on only regions of interest in the image. Two benchmark datasets (i.e., Pascal VOC and MS-COCO) are adopted to demonstrate the effectiveness of ourwork. Moreover, we construct a new dataset, which includes many semantic dependent labels in each image, to verify the effectiveness of our model. Experimental results show that our method outperforms several state-of-the-arts, especially when predicting some semantic relative labels.
C1 [Lyu, Fan] Suzhou Univ Sci & Technol, Suzhou 215009, Peoples R China.
   [Lyu, Fan] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300000, Peoples R China.
   [Wu, Qi] Univ Adelaide, Sch Comp Sci, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia.
   [Hu, Fuyuan] Suzhou Univ Sci & Technol, Sch Elect & Informat Engn, Suzhou 215010, Peoples R China.
   [Wu, Qingyao; Tan, Mingkui] South China Univ Technol, Sch Software Engn, Guangzhou 510640, Guangdong, Peoples R China.
C3 Suzhou University of Science & Technology; Tianjin University;
   University of Adelaide; Suzhou University of Science & Technology; South
   China University of Technology
RP Hu, FY (corresponding author), Suzhou Univ Sci & Technol, Sch Elect & Informat Engn, Suzhou 215010, Peoples R China.; Tan, MK (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510640, Guangdong, Peoples R China.
EM fanlyu@tju.edu.cn; qi.wu01@adelaide.edu.au; fuyuanhu@mail.usts.edu.cn;
   qyw@scut.edu.cn; mingkuitan@scut.edu.cn
RI Lyu, Fan/AAC-3865-2020; Wu, Qi/ABD-6304-2021
OI Lyu, Fan/0000-0002-0878-5485; Wu, Qi/0000-0003-3631-256X; Hu,
   Fuyuan/0000-0002-6818-2221
FU Natural Science Foundation of China [61876121, 61472267, 61877038,
   61602185]; Primary Research AMP; Development Plan of Jiangsu Province
   [BE2017663]; Fundamental Research Funds for the Central Universities
   [D2172480]; Program for Guangdong Introducing Innovative and
   Entrepreneurial Teams [2017ZT07X183]
FX This work was supported in part by the Natural Science Foundation of
   China (61876121, 61472267, 61877038, 61602185), in part by the Primary
   Research & Development Plan of Jiangsu Province (BE2017663), in part by
   the Fundamental Research Funds for the Central Universities D2172480,
   and in part by the Program for Guangdong Introducing Innovative and
   Entrepreneurial Teams 2017ZT07X183. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Elisa Ricci. (Fan Lyu and Qi Wu contributed equally to this work.)
   (Corresponding authors: Fuyuan Hu and Mingkui Tan.)
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], PROC CVPR IEEE
   [Anonymous], IEEE T NEURAL NETW L
   [Anonymous], 2014, Learning to execute
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2018, IEEE T NEUR NET LEAR, DOI DOI 10.1109/TNNLS.2018.2827036
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2017, P 31 AAAI C ART INT
   [Anonymous], 2017, CVPR
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], J MACH LEARN RES
   [Anonymous], 2005, Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM'05, DOI [DOI 10.1145/1099554.1099591, 10.1145/1099554.1099591]
   [Anonymous], 2013, ARXIV13124894
   [Anonymous], 2016, ARXIV161101773
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Bao BK, 2012, IEEE T MULTIMEDIA, V14, P199, DOI 10.1109/TMM.2011.2170557
   Bengio Y., 2014, TECHNICAL REPORT
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Clare A, 2001, LNCS LNAI, P42, DOI [DOI 10.1007/3-540-44794-6_4, 10.1007/3-540-44794-6_4, DOI 10.1007/3-540-44794-6]
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding XM, 2016, IEEE T MULTIMEDIA, V18, P1616, DOI 10.1109/TMM.2016.2572000
   Elisseeff A, 2002, ADV NEUR IN, V14, P681
   Fürnkranz J, 2008, MACH LEARN, V73, P133, DOI 10.1007/s10994-008-5064-8
   Gan C, 2017, IEEE I CONF COMP VIS, P1829, DOI 10.1109/ICCV.2017.201
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Guo Y, 2018, AAAI CONF ARTIF INTE, P3134
   Harzallah H, 2009, IEEE I CONF COMP VIS, P237, DOI 10.1109/ICCV.2009.5459257
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Jin JR, 2016, INT C PATT RECOG, P2452, DOI 10.1109/ICPR.2016.7900004
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu F, 2017, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2017.443
   Lo HY, 2011, IEEE T MULTIMEDIA, V13, P518, DOI 10.1109/TMM.2011.2129498
   Mao Junhua., 2014, Explain images with multimodal recurrent neural networks
   Mavrovouniotis M, 2017, 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), P1577
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Read J, 2009, LECT NOTES ARTIF INT, V5782, P254, DOI 10.1007/978-3-642-04174-7_17
   Sutskever I, 2014, ADV NEUR IN, V27
   Tan MK, 2015, PROC CVPR IEEE, P4100, DOI 10.1109/CVPR.2015.7299037
   Tsoumakas G, 2007, LECT NOTES ARTIF INT, V4701, P406
   Tsoumakas G, 2011, IEEE T KNOWL DATA EN, V23, P1079, DOI 10.1109/TKDE.2010.164
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wei Yunchao., 2014, CoRR
   Wu J, 2017, IEEE T MULTIMEDIA, V19, P1156, DOI 10.1109/TMM.2017.2652065
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Xu JJ, 2014, IEEE T MULTIMEDIA, V16, P403, DOI 10.1109/TMM.2013.2291218
   Xu X, 2015, IEEE ICC, P2048, DOI 10.1109/ICC.2015.7248627
   Yang Z, 2016, INT CONF ACOUST SPEE, P3236, DOI 10.1109/ICASSP.2016.7472275
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhou JT, 2019, MACH LEARN, V108, P809, DOI 10.1007/s10994-019-05786-2
   Zhuang ZW, 2018, ADV NEUR IN, V31
NR 60
TC 48
Z9 50
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 1971
EP 1981
DI 10.1109/TMM.2019.2894964
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700007
DA 2024-07-18
ER

PT J
AU Lin, X
   Wang, ZJ
   Ma, LZ
   Wu, XB
AF Lin, Xiao
   Wang, Zhi-Jie
   Ma, Lizhuang
   Wu, Xiabao
TI Saliency Detection via Multi-Scale Global Cues
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency region; global prior; image smoothing and segmentation
ID REGION DETECTION; MODEL; ROBUST
AB The saliency detection technologies are very useful to analyze and extract important information from given multimedia data, and have already been extensively used in many multimedia applications. Past studies have revealed that utilizing the global cues is effective in saliency detection. Nevertheless, most of prior works mainly considered the single-scale segmentation when the global cues are employed. In this paper, we attempt to incorporate the multi-scale global cues for saliency detection problem. Achieving this proposal is interesting and also challenging (e.g., How to obtain appropriate foreground and background seeds effectively? How to merge rough saliency results into the final saliency map efficiently?). To alleviate the challenges, we present a three-phase solution that integrates several targeted strategies, first, a self-adaptive strategy for obtaining appropriate filter parameters; second, a cross-validation scheme for selecting appropriate background and foreground seeds; and third, a weight-based approach for merging the rough saliency maps. Our solution is easy to understand and implement, but without loss of effectiveness. Extensive experimental results based on benchmark datasets demonstrate the feasibility and competitiveness of our proposed solution.
C1 [Lin, Xiao] Shanghai Normal Univ, Dept Comp Sci, Shanghai 200234, Peoples R China.
   [Wang, Zhi-Jie] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangdong Key Lab Big Data Anal & Proc, Guangzhou 510275, Guangdong, Peoples R China.
   [Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200000, Peoples R China.
   [Ma, Lizhuang] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200241, Peoples R China.
   [Wu, Xiabao] Shanghai Zhihuan Software Technol Co Ltd, Shanghai 200062, Peoples R China.
C3 Shanghai Normal University; Sun Yat Sen University; Shanghai Jiao Tong
   University; East China Normal University
RP Wang, ZJ (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangdong Key Lab Big Data Anal & Proc, Guangzhou 510275, Guangdong, Peoples R China.
EM Lin6008@126.com; wangzhij5@mail.sysu.edu.cn; ma-lz@cs.sjtu.edu.cn;
   wuxiaobao@hotmail.com
RI Sun, Peng/KDO-4243-2024; Li, Chun/KBC-9591-2024; Yu,
   Chongxiu/KDM-7354-2024
OI Yu, Chongxiu/0000-0002-8221-6221; Wang, Zhi-Jie/0000-0002-6865-7899
FU National Key RAMP;D Program of China [2018YFC0810204, 2018YFB1004400];
   National Natural Science Foundation of China [61472245, 61472453,
   61502220, 61572326, 61775139, 61872242, U1401256, U1501252, U1611264,
   U1711261, U1711262, U1811264]; Shanghai Science and Technology
   Innovation Action Plan Project [16111107502, 17511107203]; program for
   tackling key problems in Henan Science and Technology [172102310636]
FX This work supported in part by the National Key R&D Program of China
   under Grants 2018YFC0810204 and 2018YFB1004400, in part by the National
   Natural Science Foundation of China under Grants 61472245, 61472453,
   61502220, 61572326, 61775139, 61872242, U1401256, U1501252, U1611264,
   U1711261, U1711262, and U1811264, in part by the Shanghai Science and
   Technology Innovation Action Plan Project under Grants 16111107502 and
   17511107203, and in part by the program for tackling key problems in
   Henan Science and Technology (172102310636).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
   Dou H, 2017, IEEE T MULTIMEDIA, V19, P1718, DOI 10.1109/TMM.2017.2689327
   Doulamis AD, 2000, SIGNAL PROCESS, V80, P1049, DOI 10.1016/S0165-1684(00)00019-0
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu KR, 2017, IEEE T MULTIMEDIA, V19, P1531, DOI 10.1109/TMM.2017.2679898
   Gao DS, 2009, IEEE T PATTERN ANAL, V31, P989, DOI 10.1109/TPAMI.2009.27
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   He SF, 2016, PROC CVPR IEEE, P5723, DOI 10.1109/CVPR.2016.617
   Hu XL, 2016, INT CONF ACOUST SPEE, P1946, DOI 10.1109/ICASSP.2016.7472016
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jeong SW, 2017, IEEE T MULTIMEDIA, V19, P2692, DOI 10.1109/TMM.2017.2710802
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li X, 2017, PROCEEDINGS OF 2017 IEEE 2ND INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC), P443, DOI 10.1109/ITNEC.2017.8284771
   Lin TH, 2016, COMPUT GRAPH FORUM, V35, P57, DOI 10.1111/cgf.13003
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Lu SJ, 2012, LECT NOTES COMPUT SC, V7578, P321, DOI 10.1007/978-3-642-33786-4_24
   Lu SJ, 2014, IEEE T PATTERN ANAL, V36, P195, DOI 10.1109/TPAMI.2013.158
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Privitera C., 1998, M9856 UCBERL COLL EN
   Qi JQ, 2017, NEUROCOMPUTING, V222, P81, DOI 10.1016/j.neucom.2016.10.007
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Siagian C, 2007, IEEE T PATTERN ANAL, V29, P300, DOI 10.1109/TPAMI.2007.40
   Sun JG, 2015, IEEE T IMAGE PROCESS, V24, P1639, DOI 10.1109/TIP.2015.2403241
   Tang YB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P618, DOI 10.1145/3123266.3123318
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Tong N, 2014, IEEE SIGNAL PROC LET, V21, P1035, DOI 10.1109/LSP.2014.2323407
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Wang ZL, 2017, IEEE T MULTIMEDIA, V19, P750, DOI 10.1109/TMM.2016.2636739
   WANG ZT, 2018, P COMBUST INST, V152, P1
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wong WCK, 2005, IEEE T IMAGE PROCESS, V14, P1512, DOI 10.1109/TIP.2005.852199
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang JM, 2012, PROC CVPR IEEE, P2296, DOI 10.1109/CVPR.2012.6247940
   Yi Xinyang, 2016, Advances in Neural Information Processing Systems, P4152
   Zhang LH, 2018, IEEE T IMAGE PROCESS, V27, P987, DOI 10.1109/TIP.2017.2766787
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang W, 2010, IEEE T MULTIMEDIA, V12, P300, DOI 10.1109/TMM.2010.2047607
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 70
TC 24
Z9 24
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1646
EP 1659
DI 10.1109/TMM.2018.2884474
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700003
DA 2024-07-18
ER

PT J
AU Yan, YC
   Ni, BB
   Zhang, WD
   Xu, JW
   Yang, XK
AF Yan, Yichao
   Ni, Bingbing
   Zhang, Wendong
   Xu, Jingwei
   Yang, Xiaokang
TI Structure-Constrained Motion Sequence Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Motion generation; structure condition; video analysis
ID MODEL; RECOGNITION; NETWORKS; TRACKING
AB Video generation is a challenging task due to the extremely high-dimensional distribution of the solution space. Good constraints in the solution domain would thus reduce the difficulty of approximating optimal solutions. In this paper, instead of directly generating high-dimensional video data, we propose using object landmarks as explicit structure constraints to address this issue. Specifically, we propose a two-stage framework for an action-conditioned video generation task. In our framework, the first stage aims to generate landmark sequences according to predefined motion types, and a recurrent model (RNN/LSTM) is adopted for this purpose. The landmark sequence can be regarded as a low-dimensional structure embedding of high-dimensional video data, and generating landmark sequences is much easier than generating videos. The second stage is inspired by a conditional generative adversarial network (CGAN), and we take the generated landmark sequence as a structure condition to learn a landmark-to-image translation network. Such a one-to-one translation framework avoids the difficulty of generating videos and instead transfers the video generation task to image generation, which is resolvable due to the maturity of current GAN-based models. The experimental results demonstrate that our model not only achieves promising results on rigid/nonrigid motion generation tasks but also can be extended to multiobject motion situations.
C1 [Yan, Yichao; Ni, Bingbing; Zhang, Wendong; Xu, Jingwei; Yang, Xiaokang] Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, AI Inst, Shanghai 200240, Peoples R China.
   [Yan, Yichao; Ni, Bingbing; Zhang, Wendong; Xu, Jingwei; Yang, Xiaokang] Shanghai Jiao Tong Univ, SJTU UCLA Joint Ctr Machine Percept & Inference, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Ni, BB (corresponding author), Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, AI Inst, Shanghai 200240, Peoples R China.; Ni, BB (corresponding author), Shanghai Jiao Tong Univ, SJTU UCLA Joint Ctr Machine Percept & Inference, Shanghai 200240, Peoples R China.
EM yanyichao@sjtu.edu.cn; nibingbing@sjtu.edu.cn; diergent@sjtu.edu.cn;
   xjwxjw@sjtu.edu.cn; xkyang@sjtu.edu.cn
RI Yan, Yichao/ADT-5511-2022; Yang, Xiaokang/C-6137-2009
OI Yan, Yichao/0000-0003-3209-8965; Yang, Xiaokang/0000-0003-4029-3322;
   Zhang, Wendong/0000-0001-9837-8930
FU State Key Research and Development Program [2016YFB1001003]; National
   Science Foundation of China [U1611461, 61502301, 61527804, 61521062];
   China's Thousand Youth Talents Plan; STCSM [18DZ1112300, 17511105401,
   18DZ2270700]
FX This work was supported in part by the State Key Research and
   Development Program (2016YFB1001003), in part by the National Science
   Foundation of China (U1611461, 61502301, 61527804, and 61521062), and in
   part by China's Thousand Youth Talents Plan, STCSM (18DZ1112300,
   17511105401, and 18DZ2270700).
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Agarwala A, 2005, ACM T GRAPHIC, V24, P821, DOI 10.1145/1073204.1073268
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], 2013, ACM T GRAPHICS TOG
   [Anonymous], 2014, VERY DEEP CONVOLUTIO
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.632
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2014, INT C LEARNING REPRE
   [Anonymous], 2014, VIDEO LANGUAGE MODEL
   [Anonymous], TRANSFORMATION BASED
   [Anonymous], 2016, TECH REP
   [Anonymous], 2016, ICLR WORKSHOP
   Arjovsky M., 2017, INT C MACH LEARN, P214
   Baltrusaitis T, 2016, IEEE WINT CONF APPL
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Cai HY, 2018, LECT NOTES COMPUT SC, V11206, P374, DOI [10.1007/978-3-030-01216-8_, 10.1007/978-3-030-01216-8_23]
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388
   Denton Emily, 2017, P ADV NEUR INF PROC, P4417
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Gong D, 2014, IEEE T PATTERN ANAL, V36, P1414, DOI 10.1109/TPAMI.2013.244
   Goodfellow I., 2016, ADV NEURAL INFORM PR, P64
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Howe NR, 2000, ADV NEUR IN, V12, P820
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Huang Y, 2002, INT C PATT RECOG, P552, DOI 10.1109/ICPR.2002.1044791
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jauvin M, 2017, IEEE INT CONF COMP, P199, DOI 10.1109/CIVEMSA.2017.7995326
   Ji XF, 2010, IEEE T SYST MAN CY C, V40, P13, DOI 10.1109/TSMCC.2009.2027608
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Ju SX, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P38, DOI 10.1109/AFGR.1996.557241
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kehl R, 2006, COMPUT VIS IMAGE UND, V104, P190, DOI 10.1016/j.cviu.2006.07.010
   Kingma Diederik P, 2014, INT C LEARNING REPRE
   Lotter W., 2016, arXiv
   Mathieu M., 2015, ARXIV
   Mikic I, 2003, INT J COMPUT VISION, V53, P199, DOI 10.1023/A:1023012723347
   Mirza Mehdi, 2014, COMPUTER SCI
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Oh J., 2015, P NEURIPS, P2863
   Ong EJ, 2006, COMPUT VIS IMAGE UND, V104, P178, DOI 10.1016/j.cviu.2006.08.004
   Owens A, 2016, PROC CVPR IEEE, P2405, DOI 10.1109/CVPR.2016.264
   Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424
   Poppe R, 2007, COMPUT VIS IMAGE UND, V108, P4, DOI 10.1016/j.cviu.2006.10.016
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Qi G.-J., 2017, Loss-sensitive generative adversarial networks on lipschitz densities
   Radford A., 2015, COMPUTER SCI
   RASHID RF, 1980, IEEE T PATTERN ANAL, V2, P574, DOI 10.1109/TPAMI.1980.6447705
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rupprecht T, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1772, DOI 10.1145/2976749.2989041
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Villegas R., 2017, Decomposing motion and content for natural video sequence prediction
   Villegas R., 2017, International Conference on Machine Learning, P3560
   Vondrick C, 2016, 30 C NEURAL INFORM P, V29
   Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51
   Wang L, 2017, IEEE T MULTIMEDIA, V19, P646, DOI 10.1109/TMM.2016.2617079
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Xu WR, 2017, IEEE T MULTIMEDIA, V19, P1494, DOI 10.1109/TMM.2017.2674622
   Xue Tianfan, 2016, Advances in Neural Information Processing Systems, P91
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhang SW, 2018, IEEE T MULTIMEDIA, V20, P769, DOI 10.1109/TMM.2017.2758524
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
NR 72
TC 8
Z9 9
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1799
EP 1812
DI 10.1109/TMM.2018.2885235
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700015
DA 2024-07-18
ER

PT J
AU He, L
   Jiang, DM
   Sahli, H
AF He, Lang
   Jiang, Dongmei
   Sahli, Hichem
TI Automatic Depression Analysis Using Dynamic Facial Appearance Descriptor
   and Dirichlet Process Fisher Encoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depression; nonverbal behaviors; dynamic feature descriptor; median
   robust local binary patterns from three orthogonal planes (MRLBP-TOP);
   Dirichlet process Fisher vector (DPFV)
ID RECOGNITION; DIAGNOSIS; CLASSIFICATION; INVENTORY; SEVERITY; MODELS
AB Depression causes mood disorders with noticeable problems in day-to-day activities. Current methods of assessing depression depend almost entirely on clinical interviews or questionnaires. They lack systematic and efficient ways of incorporating behavioral observations that are strong indicators of a psychological disorder. To help clinicians effectively and efficiently diagnose depression severity, automated systems, using objective and quantifiable data for depression assessment, are being developed. This paper presents a framework toward estimating a clinical depression-specific score, namely the Beck Depression Inventory-II (BDI-II) score, based on the analysis of facial expressions features. To extract facial dynamic features, we propose a novel dynamic feature descriptor denoted as median robust local binary patterns from three orthogonal planes (MRLBP-TOP), which can capture both the microstructure and macrostructure of facial appearance and dynamics. To aggregate the MRLBP-TOP over an image sequence, we propose a variant to the Fisher vector (FV) encoding scheme, denoted as the Dirichlet process FV (DPFV). DPFV adopts Dirichlet process Gaussian mixture models (DPGMM) to automatically learn the number of GMM mixtures and model parameters. Experimental results on the AVEC2013 and AVEC2014 depression databases have demonstrated the effectiveness of the proposed method.
C1 [He, Lang; Jiang, Dongmei] Northwestern Polytech Univ, Shaanxi Prov Key Lab Speech & Image Informat Proc, NPU VUB Joint AVSP Res Lab, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.
   [He, Lang] Vrije Univ Brussels, Dept Elect & Informat, B-1050 Brussels, Belgium.
   [Sahli, Hichem] Vrije Univ Brussel, Dept Elect & Informat, VUB NPU Joint AVSP Res Lab, B-1050 Brussels, Belgium.
   [Sahli, Hichem] Interuniv Microelect Ctr, B-3001 Heverlee, Belgium.
C3 Northwestern Polytechnical University; Vrije Universiteit Brussel; Vrije
   Universiteit Brussel; Interuniversity Microelectronics Centre
RP Jiang, DM (corresponding author), Northwestern Polytech Univ, Shaanxi Prov Key Lab Speech & Image Informat Proc, NPU VUB Joint AVSP Res Lab, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.
EM langhe@mail.nwpu.edu.cn; jiangdm@nwpu.edu.cn; hsahli@etrovub.be
OI HE, LANG/0000-0003-2515-8579; Sahli, Hichem/0000-0002-1774-2970
FU Shaanxi Provincial Key RD Program [2017KW-ZD-14]; National Natural
   Science Foundation of China [61273265]; China Scholarship Council
   [201606290171]; VUB Interdisciplinary Research Program through the
   EMO-App project
FX This work was supported in part by the Shaanxi Provincial Key R&D
   Program (2017KW-ZD-14), in part by the National Natural Science
   Foundation of China under Grant 61273265, in part by the VUB
   Interdisciplinary Research Program through the EMO-App project, and in
   part by the program of the China Scholarship Council (201606290171). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Abdulmotaleb El Saddik.
CR Afshar S., 2016, P IEEE C COMP VIS PA, P66
   Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469
   Alghowinem S, 2013, IEEE IMAGE PROC, P4220, DOI 10.1109/ICIP.2013.6738869
   Alghowinem S, 2013, INT CONF AFFECT, P283, DOI 10.1109/ACII.2013.53
   [Anonymous], 2013, ICCV WORKSH 2013
   [Anonymous], DIAGNOSTIC STAT MANU
   [Anonymous], 2016, ARXIV160800182
   [Anonymous], 2007, ADV NEURAL INFORM PR
   Baltrusaitis T, 2016, IEEE WINT CONF APPL
   BASSILI JN, 1979, J PERS SOC PSYCHOL, V37, P2049, DOI 10.1037/0022-3514.37.11.2049
   Beck AT, 1996, J PERS ASSESS, V67, P588, DOI 10.1207/s15327752jpa6703_13
   Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104
   Chang J., 2013, Advances in Neural Information Processing Systems, P620
   Chen HJ, 2016, INTERSPEECH, P923, DOI 10.21437/Interspeech.2016-313
   Cox M., 1990, J ROY SOC MED, V83
   Cummins N., 2013, P 3 ACM INT WORKSH A, P11
   De La Torre F, 2009, P 2009 3 INT C AFF C, P1, DOI 10.1109/ACII.2009.5349358
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Dhall A, 2015, INT CONF AFFECT, P255, DOI 10.1109/ACII.2015.7344580
   Dong L, 2017, DESTECH TRANS ENG, P195
   Ellgring H., 2007, NONVERBAL COMMUNICAT
   Girard JM, 2014, IMAGE VISION COMPUT, V32, P641, DOI 10.1016/j.imavis.2013.12.007
   Girdhar R, 2017, PROC CVPR IEEE, P3165, DOI 10.1109/CVPR.2017.337
   Görür D, 2010, J COMPUT SCI TECH-CH, V25, P653, DOI [10.1007/s11390-010-1051-1, 10.1007/s11390-010-9355-8]
   Gupta R, 2014, NAT PROD J, V4, P33, DOI 10.2174/2210315504666140515004826
   He L, 2015, INT CONF AFFECT, P260, DOI 10.1109/ACII.2015.7344581
   Herrick C., 2004, 100 QUESTIONS ANSWER
   Huang D, 2011, IEEE T SYST MAN CY C, V41, P765, DOI 10.1109/TSMCC.2011.2118750
   Jaakkola TS, 1999, ADV NEUR IN, V11, P487
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Jain S, 2004, J COMPUT GRAPH STAT, V13, P158, DOI 10.1198/1061860043001
   Jan Asim., 2014, Proceedings of the ACM 4th International Workshop on Audio/Visual Emotion Challenge, P73, DOI DOI 10.1145/2661806.2661812
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jiang BH, 2014, IEEE T CYBERNETICS, V44, P161, DOI 10.1109/TCYB.2013.2249063
   JONES IH, 1979, J NERV MENT DIS, V167, P402, DOI 10.1097/00005053-197907000-00002
   Joshi J, 2013, J MULTIMODAL USER IN, V7, P217, DOI 10.1007/s12193-013-0123-2
   Joshi J, 2012, INT C PATT RECOG, P2634
   Joshi K, 2013, INT CONF CONTEMP, P1, DOI 10.1109/IC3.2013.6612160
   Kachele M., 2014, P 4 INT WORKSHOP AUD, P41
   Karakashian Shant., 2013, Proceedings of the 27th Conference on Artificial Intelligence (AAAI 2013), P1
   Kaya Heysem., 2014, Proceedings of the ACM 4th International Workshop on Audio/Visual Emotion Challenge, P19, DOI [DOI 10.1145/2661806.2661814, 10.1145/2661806.2661814]
   Kroenke K, 2002, PSYCHIAT ANN, V32, P509, DOI 10.3928/0048-5713-20020901-06
   Kroenke K, 2009, J AFFECT DISORDERS, V114, P163, DOI 10.1016/j.jad.2008.06.026
   Kurihara K, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2796
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li ZK, 2011, 4TH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER THEORY AND ENGINEERING ( ICACTE 2011), P317
   Liu L, 2016, IEEE T IMAGE PROCESS, V25, P1368, DOI 10.1109/TIP.2016.2522378
   Maji S., 2008, CVPR
   Mathers C, 2008, GLOBAL BURDEN OF DISEASE: 2004 UPDATE, P1
   Meena A, 2014, J ORAL REHABIL, V41, P87, DOI 10.1111/joor.12122
   Meng H., 2013, P 3 ACM INT WORKSH A, P21, DOI [10.1145/2512530.2512532., DOI 10.1145/2512530.2512532]
   Mitra Vikramjit., 2014, PROC 4 INT WORKSHOP, P93
   MONTGOMERY SA, 1979, BRIT J PSYCHIAT, V134, P382, DOI 10.1192/bjp.134.4.382
   Mundt JC, 2007, J NEUROLINGUIST, V20, P50, DOI 10.1016/j.jneuroling.2006.04.001
   Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106
   Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653
   Pampouchidou A, 2016, IEEE ENG MED BIO, P3835, DOI 10.1109/EMBC.2016.7591564
   Perez Y, 2014, DEEP METAZOAN PHYLOGENY: THE BACKBONE OF THE TREE OF LIFE, P49
   Philippot P., 2003, Nonverbal behavior in clinical settings: Series in affective science
   Ringeval Fabien, 2017, P 7 ANN WORKSHOP AUD, P3, DOI DOI 10.1145/3133944.3133953
   Rush AJ, 2003, BIOL PSYCHIAT, V54, P573, DOI 10.1016/S0006-3223(02)01866-8
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Santos JF, 2014, 2014 14TH INTERNATIONAL WORKSHOP ON ACOUSTIC SIGNAL ENHANCEMENT (IWAENC), P55, DOI 10.1109/IWAENC.2014.6953337
   Sidorov Maxim., 2014, P 4 INT WORKSH AUD V, P81, DOI DOI 10.1145/2661806.2661816
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Stratou G, 2013, INT CONF AFFECT, P147, DOI 10.1109/ACII.2013.31
   Valstar M., 2014, P 4 INT WORKSH AUD V, P3, DOI 10.1109/FG.2015.7284874
   Valstar M., 2013, P 3 ACM INT WORKSHOP, DOI [10.1145/2512530.2512533, DOI 10.1145/2512530.2512533]
   Valstar M, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P3, DOI 10.1145/2988257.2988258
   Wen LY, 2015, IEEE T INF FOREN SEC, V10, P1432, DOI 10.1109/TIFS.2015.2414392
   Williamson J.R, 2014, 2014 ACM INT WORKSH, P65
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhu Y, 2018, IEEE T AFFECT COMPUT, V9, P578, DOI 10.1109/TAFFC.2017.2650899
NR 73
TC 44
Z9 46
U1 2
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1476
EP 1486
DI 10.1109/TMM.2018.2877129
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400011
DA 2024-07-18
ER

PT J
AU Peng, YX
   Qi, JW
AF Peng, Yuxin
   Qi, Jinwei
TI Show and Tell in the Loop: Cross-Modal Circular Correlation Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Circular correlation learning; cross-modal retrieval; image-to-text
   caption; text-to-image synthesis
ID REPRESENTATION
AB Multimedia data with various modalities, such as image and text, are huge in quantity but have inconsistent distribution and representation. Many works have been done to break the boundary between image and text to measure their correlation. However, they focus on either the transformation to common subspace or the unidirectional generation from one to another individually, which cannot fully explore their interactions. It is noted that the bidirectional generation between image and text not only can provide complementary hints and mutually boost to learn cross-modal correlation but also cross-modal correlation learning can feed back to give comprehensive clues for promoting the cross-modal generation process. Therefore, we have the motivation that information transmission between image and text should be treated as a circular process, which aims to fully understand their latent correlation, and further realize cross-modal generation to produce both realistic images and text descriptions in a unified framework. In this paper, we propose the cross-modal circular correlation learning approach to perform both cross-modal correlation learning and generation simultaneously through an efficient circular learning training procedure. First, we propose the cross-modal circular learning model to perform an image-to-text caption and text-to-image synthesis circularly and learn common representation as a round-trip bridge, which can realize efficient interactions to fully exploit latent cross-modal correlations. Second, a unified bidirectional framework is proposed to conduct cross-modal mutual generation and is trained in an efficient circular process to enhance the generative ability of common representation, which can feed back circularly to further promote cross-modal correlation learning. In summary, we simultaneously perform cross-modal retrieval, image-to-text caption, and text-to-image synthesis in a unified framework with the circular learning process, which has high scalability and generality to realize universal cognition on the cross-modal data. We conduct extensive experiments to not only evaluate the correlation performance by cross-modal retrieval but also to show the generation effectiveness of both image caption and synthesis on the MS-COCO dataset.
C1 [Peng, Yuxin; Qi, Jinwei] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Peng, YX (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
EM pengyuxin@pku.edu.cn
RI Peng, Yuxin/U-7376-2019; peng, yu/GXW-2071-2022
FU National Natural Science Foundation of China [61771025, 61532005]
FX This work was supported by National Natural Science Foundation of China
   under Grant 61771025 and Grant 61532005. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Guillaume Gravier.
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], 2003, P ACM INT C MULT ACM
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.345
   [Anonymous], 2011, P ICML
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2015, ARXIV150504467
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2016, ARXIV161009585
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graves A., 2013, Generating sequences with recurrent neural networks
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu XF, 2014, IEEE IJCNN, P7, DOI 10.1109/IJCNN.2014.6889605
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim Y, 2014, IEEE ASME INT C ADV, P1747, DOI 10.1109/AIM.2014.6878336
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mao Junhua, 2015, P INT C LEARN REPR
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2017, FRONT INFORM TECH EL, V18, P44, DOI 10.1631/FITEE.1601787
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Radford A, 2016, 4 INT C LEARNING REP
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Srivastava N., 2012, P INT C MACH LEARN W
   Tran TQN, 2016, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2016.225
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang C, 2017, P INT COMP SOFTW APP, P154, DOI 10.1109/COMPSAC.2017.66
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhai X., 2013, PROC 27 AAAI C ARTIF, P1198
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
NR 49
TC 5
Z9 9
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1538
EP 1550
DI 10.1109/TMM.2018.2877885
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400016
DA 2024-07-18
ER

PT J
AU Jia, LH
   Tsui, CY
   Au, OC
   Jia, KB
AF Jia, Luheng
   Tsui, Chi-Ying
   Au, Oscar C.
   Jia, Kebin
TI A New Rate-Complexity-Distortion Model for Fast Motion Estimation
   Algorithm in HEVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Rate distortion theory; joint rate-complexity-distortion optimization;
   fast motion estimation; HEVC
ID 3-STEP SEARCH ALGORITHM; SUCCESSIVE ELIMINATION; EFFICIENCY;
   OPTIMIZATION; PATTERN
AB In the high efficiency video coding (HEVC) standard, motion estimation (ME) adopts a quadtree coding structure and a larger search range to improve the coding performance. These advanced coding tools, however, dramatically increase the computational complexity. To accelerate ME, fast methods have been proposed that reduce ME complexity at the expense of rate-distortion (R-D) performance. However, none of these methods can claim their tradeoff to be optimal. In this paper, we propose an optimal fast motion estimation (FME) algorithm based on an analytical model of rate-complexity-distortion (R-C-D). We extend the traditional R-D model by introducing the ME complexity into it, which enables us to explicitly express the R-D performance under different complexity budgets. Based on the R-C-D model, the proposed FME finds the R-C-D optimized search ranges for some representative prediction units (PUs), which are then extended or refined dynamically according to motion characteristics for neighboring PUs. The proposed FME enables an R-D performance close to that of full search. When compared with the default FME method in the reference software of HEVC, the proposed fast algorithm can reduce the complexity by over 80% while improving the R-D performance. Furthermore, our proposed FME algorithm is hardware friendly, as regular data flow enables high data reuse efficiency.
C1 [Jia, Luheng; Jia, Kebin] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Tsui, Chi-Ying] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China.
   [Au, Oscar C.] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Au, Oscar C.] Origin Wireless Inc, Greenbelt, MD 20770 USA.
C3 Beijing University of Technology; Hong Kong University of Science &
   Technology; Hong Kong University of Science & Technology
RP Jia, LH (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
EM luhengjia@bjut.edu.cn; eetsui@ust.hk; eeau90@gmail.com;
   kebinj@bjut.edu.cn
CR Ahn TG, 2004, IEEE T CIRC SYST VID, V14, P1265, DOI 10.1109/TCSVT.2004.835146
   [Anonymous], 2013, IEEE POW ENG SOC GEN
   [Anonymous], 2008, ADV DIGITAL SIGNAL P
   [Anonymous], 2013, JCTVCO1002
   [Anonymous], JCTVCD600 ITUTISOIEC
   [Anonymous], 2001, VCEGM33
   Ardestani Majid R., 2010, 2010 17th International Conference on Telecommunications (ICT 2010), P923, DOI 10.1109/ICTEL.2010.5478827
   Bao XN, 2012, IEEE T MULTIMEDIA, V14, P237, DOI 10.1109/TMM.2011.2171677
   Chan YL, 2001, IEEE T IMAGE PROCESS, V10, P1223, DOI 10.1109/83.935038
   Cheung CH, 2005, IEEE T MULTIMEDIA, V7, P16, DOI 10.1109/TMM.2004.840609
   Cheung CH, 2002, IEEE T CIRC SYST VID, V12, P1168, DOI 10.1109/TCSVT.2002.806815
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Deng CW, 2010, IEEE IMAGE PROC, P2037, DOI 10.1109/ICIP.2010.5651661
   Fan R, 2017, IEEE T MULTIMEDIA, V19, P893, DOI 10.1109/TMM.2016.2642786
   Gibson J, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19110604
   Han JN, 2012, IEEE T IMAGE PROCESS, V21, P1874, DOI 10.1109/TIP.2011.2169976
   Hu N, 2014, IEEE T CIRC SYST VID, V24, P1310, DOI 10.1109/TCSVT.2014.2306035
   Hu Y, 2006, 2006 IEEE International Conference on Multimedia and Expo - ICME 2006, Vols 1-5, Proceedings, P1949, DOI 10.1109/ICME.2006.262939
   Jing X, 2004, IEEE T MULTIMEDIA, V6, P435, DOI 10.1109/TMM.2004.827517
   Jou SY, 2015, IEEE T CIRC SYST VID, V25, P1533, DOI 10.1109/TCSVT.2015.2389472
   Kamisli F, 2013, IEEE T IMAGE PROCESS, V22, P3916, DOI 10.1109/TIP.2013.2264679
   Ko YH, 2011, IEEE T CONSUM ELECTR, V57, P726, DOI 10.1109/TCE.2011.5955214
   Kotz Samuel., 2012, The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance
   Kuo TY, 2006, IEEE T CIRC SYST VID, V16, P1185, DOI 10.1109/TCSVT.2006.883512
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   LI RX, 1994, IEEE T CIRC SYST VID, V4, P438, DOI 10.1109/76.313138
   Li X, 2011, IEEE T CIRC SYST VID, V21, P957, DOI 10.1109/TCSVT.2011.2133750
   Liu  Y., 2004, P IEEE INT C AC SPEE, V3
   Lou CC, 2010, IEEE T CIRC SYST VID, V20, P1903, DOI 10.1109/TCSVT.2010.2087551
   Mallat S, 1998, IEEE T SIGNAL PROCES, V46, P1027, DOI 10.1109/78.668554
   Moon YH, 2013, IEEE T MULTIMEDIA, V15, P477, DOI 10.1109/TMM.2012.2232648
   Ng KH, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/385631
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   ONEAL JB, 1977, IEEE T INFORM THEORY, V23, P697, DOI 10.1109/TIT.1977.1055796
   PEACOCK JA, 1983, MON NOT R ASTRON SOC, V202, P615, DOI 10.1093/mnras/202.3.615
   Po LM, 1996, IEEE T CIRC SYST VID, V6, P313, DOI 10.1109/76.499840
   Rabbani H, 2008, SIGNAL PROCESS, V88, P158, DOI 10.1016/j.sigpro.2007.07.016
   Shimizu A, 2016, P PICT COD S DEC, P1
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   SUN H, 1989, 1989 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-3, P1516, DOI 10.1109/ISCAS.1989.100646
   Tourapis A. M., 2001, ISCAS 2001. The 2001 IEEE International Symposium on Circuits and Systems (Cat. No.01CH37196), P183, DOI 10.1109/ISCAS.2001.922015
   Tsai JJ, 2009, IEEE T CIRC SYST VID, V19, P108, DOI 10.1109/TCSVT.2008.2009248
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yeo CH, 2012, IEEE T CIRC SYST VID, V22, P545, DOI 10.1109/TCSVT.2011.2168291
   Yu CS, 2006, IEEE T MULTIMEDIA, V8, P1109, DOI 10.1109/TMM.2006.884635
   Zhu C, 2005, IEEE T IMAGE PROCESS, V14, P213, DOI 10.1109/TIP.2004.840702
   Zhu S, 1997, ICICS - PROCEEDINGS OF 1997 INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS AND SIGNAL PROCESSING, VOLS 1-3, P292, DOI 10.1109/ICICS.1997.647106
NR 48
TC 9
Z9 9
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 835
EP 850
DI 10.1109/TMM.2018.2866762
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700003
DA 2024-07-18
ER

PT J
AU Yang, Y
   Li, B
   Li, P
   Liu, Q
AF Yang, You
   Li, Bei
   Li, Pian
   Liu, Qiong
TI A Two-Stage Clustering Based 3D Visual Saliency Model for Dynamic
   Scenarios
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency model; 3D visual saliency; two-stage clustering
ID COMPUTATIONAL MODEL; OBJECT DETECTION; ATTENTION; DRIVEN; ALLOCATION;
   FRAMEWORK; COLOR; VIDEO
AB Three-dimensional (3D) visual saliency is fundamental for vision-guided applications such as human-computer interaction in virtual reality, image quality assessment, object tracking, and event retrieval. Classical models for 3D visual saliency can draw an appropriate saliency map when the quality of the required depth maps or auxiliary cues is high enough. However, the depth map is usually impaired with artifacts (such as holes or noise) from faults in stereo matching or multipaths in range sensors. In these cases, challenges arise in those 3D visual saliency models because the core preliminary processes, such as the detection of low-level visual features, may fail. To solve this problem, we proposed a two-stage clustering-based 3D visual saliency model for human visual fixation prediction in dynamic scenarios. In this model, a two-stage clustering scheme is designed to handle the negative influence of impaired depth videos. With the help of this scheme, representative cues are selected for saliency modeling. After that, multimodal saliency maps are obtained from depth, color, and 3D motion cues. Finally, a cross-Bayesian model is designed for the pooling of multimodal saliency maps. The experimental results demonstrate that the proposed 3D saliency model based on two-stage clustering outperforms other state-of-the-art models on a variety of metrics. Furthermore, the consistency and robustness of our model are also verified.
C1 [Yang, You; Li, Bei; Li, Pian; Liu, Qiong] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
   [Yang, You; Li, Bei; Li, Pian; Liu, Qiong] Wuhan Natl Lab Optoelect, Wuhan 430074, Hubei, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology
RP Liu, Q (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
RI Yang, You/O-5723-2019; Yang, You/A-9497-2019
FU National Key Research and Development Program of China [2017YFC0806202];
   National Natural Science Foundation of China [61571204, 61471178]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFC0806202 and in part by
   the National Natural Science Foundation of China under Grant 61571204
   and Grant 61471178. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Raouf Hamzaoui.
   (Corresponding author: Qiong Liu.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2012, P IET C IM PROC IPR
   [Anonymous], 2017, ARXIV170101081
   Banitalebi-Dehkordi A, 2017, MULTIMED TOOLS APPL, V76, P23859, DOI 10.1007/s11042-016-4155-y
   Banitalebi-Dehkordi A, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.1.013008
   Bonneh Y, 1999, VISION RES, V39, P271, DOI 10.1016/S0042-6989(98)00112-6
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Coria Lino, 2012, P 3DTV C TRUE VIS CA, P1
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Fang YM, 2017, IEEE T IMAGE PROCESS, V26, P4684, DOI 10.1109/TIP.2017.2721112
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P2625, DOI 10.1109/TIP.2014.2305100
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Feng D, 2016, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR.2016.257
   Ferraris Luca, 2015, 2015 17th European Conference on Power Electronics and Applications (EPE'15 ECCE-Europe), P1, DOI 10.1109/EPE.2015.7311697
   Ferreira L, 2015, SIGNAL PROCESS-IMAGE, V39, P98, DOI 10.1016/j.image.2015.09.005
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hobson JA, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01133
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Jiang QP, 2018, IEEE T CYBERNETICS, V48, P1276, DOI 10.1109/TCYB.2017.2690452
   Jiang QP, 2018, PATTERN RECOGN, V76, P242, DOI 10.1016/j.patcog.2017.11.001
   Jiang QP, 2015, SIGNAL PROCESS-IMAGE, V38, P57, DOI 10.1016/j.image.2015.04.007
   Jost T, 2005, COMPUT VIS IMAGE UND, V100, P107, DOI 10.1016/j.cviu.2004.10.009
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Judd T., 2012, MIT CSAIL TR
   Kim H, 2014, IEEE T IMAGE PROCESS, V23, P1476, DOI 10.1109/TIP.2014.2303640
   Kümmerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513
   Le Meur O, 2007, VISION RES, V47, P2483, DOI 10.1016/j.visres.2007.06.015
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P1937, DOI 10.1109/TIP.2014.2307434
   Lu HC, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2524198
   Ma CY, 2015, J VISION, V15, DOI 10.1167/15.6.19
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Maki A., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P734, DOI 10.1109/ICPR.1996.547661
   Marques O, 2007, VISAPP 2007: PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOLUME IU/MTSV, P294
   Ni BB, 2014, IEEE T MULTIMEDIA, V16, P1779, DOI 10.1109/TMM.2014.2329275
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Ouerhani N, 2000, INT C PATT RECOG, P375, DOI 10.1109/ICPR.2000.905356
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Qiuping Jiang, 2014, Journal of Software, V9, P1841, DOI 10.4304/jsw.9.7.1841-1847
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Rajashekar U, 2006, J VISION, V6, P379, DOI 10.1167/6.4.7
   Ren JK, 2015, EUROMICRO, P25, DOI 10.1109/ECRTS.2015.10
   Ren JR, 2018, J VIS COMMUN IMAGE R, V50, P227, DOI 10.1016/j.jvcir.2017.12.002
   Rosani A, 2015, IEEE T MULTIMEDIA, V17, P1359, DOI 10.1109/TMM.2015.2441003
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Song HK, 2016, INT CONF ACOUST SPEE, P1626, DOI 10.1109/ICASSP.2016.7471952
   Stuit SM, 2010, VISION RES, V50, P1913, DOI 10.1016/j.visres.2010.06.014
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Wang JL, 2013, IEEE T IMAGE PROCESS, V22, P2151, DOI 10.1109/TIP.2013.2246176
   Wang ZL, 2017, IEEE T MULTIMEDIA, V19, P750, DOI 10.1109/TMM.2016.2636739
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Yuming Fang, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P51, DOI 10.1109/QoMEX.2014.6982288
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang Y, 2010, LECT NOTES COMPUT SC, V5916, P314, DOI 10.1007/978-3-642-11301-7_33
   Zhang YT, 2016, IEEE T MULTIMEDIA, V18, P1604, DOI 10.1109/TMM.2016.2568138
NR 61
TC 129
Z9 152
U1 1
U2 61
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 809
EP 820
DI 10.1109/TMM.2018.2867742
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700001
DA 2024-07-18
ER

PT J
AU Xie, SY
   Hu, HF
AF Xie, Siyue
   Hu, Haifeng
TI Facial Expression Recognition Using Hierarchical Features With Deep
   Comprehensive Multipatches Aggregation Convolutional Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network; expressional transformation-invariant;
   facial expression recognition; feature aggregation
ID ACTION UNITS; PATCHES
AB Facial expression recognition (FER) has long been a challenging task in computer vision. In this paper, we propose a novel method, named deep comprehensive multipatches aggregation convolutional neural networks (CNNs), to solve the FER problem. The proposed method is a deep-based framework, which mainly consists of two branches of the CNN. One branch extracts local features from image patches while the other extracts holistic features from the whole expressional image. In the model, local features depict expressional details and holistic features characterize the high-level semantic information of an expression. We aggregate both local and holistic features before making classification. These two types of hierarchical features represent expressions in different scales. Compared with most current methods with single type of feature, the model can represent expressions more comprehensively. Additionally, in the training stage, a novel pooling strategy named expressional transformation-invariant pooling is proposed for handling nuisance variations, such as rotations, noises, etc. Extensive experiments are conducted on the famous the Extended Cohn-Kanade (CK+) dataset and the Japanese Female Facial Expression (JAFFE) database expression datasets, where the recognition results obtained.
C1 [Xie, Siyue; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Engn, Guangzhou 510275, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Engn, Guangzhou 510275, Guangdong, Peoples R China.
EM xiesy8@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn
RI Xie, Siyue/IAQ-8574-2023
OI Hu, Haifeng/0000-0002-4884-323X; Xie, Siyue/0000-0003-4362-6743
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; Natural Science Foundation of Guangdong [2017A030311029,
   2016B010109002, 2015B090912001, 2016B010123005, 2017B090909005]; Science
   and Technology Program of Guangzhou [201704020180, 201604020024];
   Fundamental Research Funds for the Central Universities of China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61673402, 61273270, and 60802069; in
   part by the Natural Science Foundation of Guangdong under Grants
   2017A030311029, 2016B010109002, 2015B090912001, 2016B010123005, and
   2017B090909005; in part by the Science and Technology Program of
   Guangzhou under Grants 201704020180 and 201604020024; and in part by the
   Fundamental Research Funds for the Central Universities of China.
CR [Anonymous], 2015, 2015 International Joint Conference on Neural Networks (IJCNN)
   [Anonymous], 2014, PROCEDIA IEEE COMPUT, DOI DOI 10.1109/CVPR.2014.233
   [Anonymous], 2016, IEEE T CYBERN
   [Anonymous], 2012, 2012 INT JOINT C NEU
   Bartlett MS, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P223, DOI 10.1109/fgr.2006.55
   Bartlett MS, 2005, PROC CVPR IEEE, P568
   Castillo J. A. R., 2012, P 19 IEEE INT C IM P
   Castillo JAR, 2012, IEEE IMAGE PROC, P2613, DOI 10.1109/ICIP.2012.6467434
   Cohn JF, 2004, IEEE SYS MAN CYBERN, P610
   Dhall Abhinav, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P878, DOI 10.1109/FG.2011.5771366
   Ekman P., 2002, Facs manual
   Ekman P., 1978, Facial action coding system
   Gao GD, 2015, 2015 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING (IIH-MSP), P302, DOI 10.1109/IIH-MSP.2015.33
   Happy SL, 2015, 2015 EIGHTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION (ICAPR), P67
   Happy SL, 2015, IEEE T AFFECT COMPUT, V6, P1, DOI 10.1109/TAFFC.2014.2386334
   IZARD CE, 1971, FACE EMOTION, V1
   Jabid T, 2010, ETRI J, V32, P784, DOI 10.4218/etrij.10.1510.0132
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Khorrami P, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P19, DOI 10.1109/ICCVW.2015.12
   Kobayashi H, 1997, IEEE SYS MAN CYBERN, P3732, DOI 10.1109/ICSMC.1997.633250
   Laptev D, 2016, PROC CVPR IEEE, P289, DOI 10.1109/CVPR.2016.38
   Lee SH, 2014, IEEE T AFFECT COMPUT, V5, P340, DOI 10.1109/TAFFC.2014.2346515
   Li Jiang, 2015, 2015 IEEE International Test Conference (ITC), P1, DOI 10.1109/TEST.2015.7342423
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lucey P., 2010, ieee computer society conference on computer vision and pattern recognition-workshops, P94
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Majumder A, 2014, PATTERN RECOGN, V47, P1282, DOI 10.1016/j.patcog.2013.10.010
   Mao QR, 2017, IEEE T MULTIMEDIA, V19, P861, DOI 10.1109/TMM.2016.2629282
   Mehrabian G., 2007, NONVERBAL COMMUNICAT
   Ming-Wei Huang, 2010, 2010 Proceedings of 3rd International Congress on Image and Signal Processing (CISP 2010), P1750, DOI 10.1109/CISP.2010.5647898
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Narwaria M, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.1.010501
   Ryan Andrew, 2009, 2009 IEEE 43rd International Carnahan Conference on Security Technology. ICCST 2009, P172, DOI 10.1109/CCST.2009.5335546
   Sandbach G, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.119
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P762, DOI 10.1016/j.imavis.2012.01.006
   Taheri S, 2014, IEEE T IMAGE PROCESS, V23, P3590, DOI 10.1109/TIP.2014.2331141
   Tawari A, 2013, IEEE T MULTIMEDIA, V15, P1543, DOI 10.1109/TMM.2013.2266635
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Tian YL, 2000, PROC CVPR IEEE, P294, DOI 10.1109/CVPR.2000.855832
   Tong Y, 2007, IEEE T PATTERN ANAL, V29, P1683, DOI 10.1109/TPAMI.2007.1094
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Vural E., 2008, International Conference on Automotive Technologies (ICAT), P1
   Wang QT, 2015, 2015 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING (IIH-MSP), P314, DOI 10.1109/IIH-MSP.2015.54
   Xu M, 2015, 2015 11TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC), P702, DOI 10.1109/ICNC.2015.7378076
   Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369
   Zhong L, 2015, IEEE T CYBERNETICS, V45, P1499, DOI 10.1109/TCYB.2014.2354351
   Zhong L, 2012, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR.2012.6247974
NR 51
TC 118
Z9 129
U1 1
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 211
EP 220
DI 10.1109/TMM.2018.2844085
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700018
DA 2024-07-18
ER

PT J
AU Chen, CLZ
   Li, S
   Qin, H
   Pan, ZK
   Yang, GW
AF Chen, Chenglizhao
   Li, Shuai
   Qin, Hong
   Pan, Zhenkuan
   Yang, Guowei
TI Bilevel Feature Learning for Video Saliency Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Spatial-temporal Saliency Consistency; Bi-level Morkov Random Field;
   Localized Feature Transformation; Video Saliency Detection
ID OBJECT DETECTION; SEGMENTATION; TRACKING; OPTIMIZATION; FUSION
AB This paper advocates a novel learning solution to the modeling of long-term spatial-temporal saliency consistency in order to boost the accuracy for video saliency detection. Conventional methods typically utilize the "slack" spatial-temporal model to locally ensure the smoothness of the computed video saliency, yet they could easily encounter the performance tradeoff dilemma (i.e., detection' accuracy and integrity). In contrast, our novel approach proposes the bilevel learning strategy to globally exploit the saliency consistency while overcoming the aforementioned difficulty. Our method first starts with the contrast computation of low-level saliency clues in a frame-wise manner. Then, based on such obtained saliency clues, we devise a novel bilevel Markov Random Field (bMRF) solution to conduct semantic labelling, which can explicitly indicates both the salient salient foregrounds and nonsalient nearby surroundings with high confidence while shrinking the low confidence remains. In such a way, the spatial-temporal consistency constraint is embedded intrinsically into the above explicit semantic labels, and we prevent the performance tradeoff problem from occurring. Next, based on those semantic labels made by our bMRF method, we further propose learnng multiple nonlinear feature transformations to enlarge the feature margin between the salient foregrounds and the non-salient nearby surroundings, whose key rationale is to resort to long-term common consistencies to enforce the spatial-temporal smoothness. Thus, we can utilize these learned non-linear feature transformations to simultaneously suppress those short-term false-alarms and correct those hollow effects. To validate our new approach, we conduct extensive experiments on five publicly available benchmarks, and make comprehensive, quantitative evaluations between our method and 17 state-of-the-art techniques. All of the results demonstrate our method's advantages in terms of accuracy, reliability, robustness, and versatility.
C1 [Chen, Chenglizhao; Pan, Zhenkuan] Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Peoples R China.
   [Li, Shuai] Beihang Univ, State Key Lab Virtual Reality Technol & Syst, Beijing 100083, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Stony Brook, NY 11794 USA.
   [Yang, Guowei] Qingdao Univ, Coll Elect Informat, Qingdao 266071, Peoples R China.
C3 Qingdao University; Beihang University; State University of New York
   (SUNY) System; State University of New York (SUNY) Stony Brook; Qingdao
   University
RP Pan, ZK (corresponding author), Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Peoples R China.
EM cclz123@163.com; lishuai@buaa.edu.cn; qin@cs.stonybrook.edu;
   zkpan@qdu.edu.cn; ygw_ustb@163.com
RI Liu, Liu/JXM-8208-2024
FU National Natural Science Foundation of China [61190120, 61190124,
   61190125, 61300067, 61672077, 6167214, 61602341, 61532002 61772277,
   61772294]; Applied Basic Research Program of Qingdao [161013xx];
   National Science Foundation of USA [IIS-1715985, IIS-0949467,
   IIS-1047715, IIS-1049448]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants No. 61190120, 61190124, 61190125, 61300067,
   61672077, 6167214, 61602341, 61532002 61772277 and 61772294, in part by
   Applied Basic Research Program of Qingdao (No. 161013xx) and in part by
   National Science Foundation of USA (No. IIS-1715985, IIS-0949467,
   IIS-1047715, and IIS-1049448). The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Elisa
   Ricci. (Corresponding author: Zhenkuan Pan.)
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], ARXIV160408010
   [Anonymous], TECHNICAL REPORT
   Aytekin C, 2018, IEEE T MULTIMEDIA, V20, P82, DOI 10.1109/TMM.2017.2713982
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Chen CLZ, 2018, IEEE SIGNAL PROC LET, V25, P154, DOI 10.1109/LSP.2017.2775212
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen CLZ, 2016, PATTERN RECOGN, V52, P410, DOI 10.1016/j.patcog.2015.09.033
   Chen CLZ, 2015, PATTERN RECOGN, V48, P2885, DOI 10.1016/j.patcog.2015.01.025
   Chen CLZ, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2403232
   Chen DY, 2013, IEEE T MULTIMEDIA, V15, P1616, DOI 10.1109/TMM.2013.2267725
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P3910, DOI 10.1109/TIP.2014.2336549
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Fukuchi K, 2009, IEEE INT CON MULTI, P638, DOI 10.1109/ICME.2009.5202577
   Gao Z, 2014, IEEE T PATTERN ANAL, V36, P1975, DOI 10.1109/TPAMI.2014.2314663
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Kim H, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425544
   Kulis B., 2006, P 23 INT C MACHINE L, P505
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li HX, 2011, PROC CVPR IEEE, P1305, DOI 10.1109/CVPR.2011.5995483
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Li YS, 2015, COMPUT VIS IMAGE UND, V135, P126, DOI 10.1016/j.cviu.2015.01.011
   Liang HR, 2016, IEEE T MULTIMEDIA, V18, P2271, DOI 10.1109/TMM.2016.2613681
   Liu Ce, 2009, THESIS
   Mahadevan V, 2010, IEEE T PATTERN ANAL, V32, P171, DOI 10.1109/TPAMI.2009.112
   Mathe S, 2012, LECT NOTES COMPUT SC, V7573, P842, DOI 10.1007/978-3-642-33709-3_60
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Russell AF, 2014, VISION RES, V94, P1, DOI 10.1016/j.visres.2013.10.005
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   Sun D., 2008, P EUR C COMPUT VIS
   Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x
   Tsai D, 2012, INT J COMPUT VISION, V100, P190, DOI 10.1007/s11263-011-0512-5
   Varadarajan S, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P63, DOI 10.1109/AVSS.2013.6636617
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yu JG, 2016, IEEE T MULTIMEDIA, V18, P273, DOI 10.1109/TMM.2015.2505908
   Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87
   Zhong S.-h., 2013, AAAI Conference on Artificial Intelligence, P1063
   Zhou F, 2014, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2014.429
   Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132
NR 50
TC 45
Z9 46
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3324
EP 3336
DI 10.1109/TMM.2018.2839523
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600012
DA 2024-07-18
ER

PT J
AU Chen, BL
   Jung, C
   Zhang, ZD
AF Chen, Baoliang
   Jung, Cheolkon
   Zhang, Zhendong
TI Variational Fusion of Time-of-Flight and Stereo Data for Depth
   Estimation Using Edge-Selective Joint Filtering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth upsampling; data fusion; edge selective; stereo vision;
   time-of-flight; total variation
AB In this paper, we propose variational fusion of time-of-flight (TOF) and stereo data for depth estimation using edge-selective joint filtering (ESJF). ESJF is able to adaptively select edges for depth upsampling from the TOF depth map, stereo matching-based disparity map, and stereo images. We adopt ESJF to produce high-resolution (HR) depth maps with accurate edge information from low-resolution ones captured by the TOF camera. First, we measure confidences of TOF and stereo data based on a Gaussian function to be used as fusion weights. Then, we upsample the TOF depth map using ESJF and extract vertical and horizontal discontinuity maps from it. Finally, we perform variational fusion of TOF and stereo depth data guided by the discontinuity maps. Experimental results show that the proposed method successfully produces HR depth maps and outperforms the state of the art in preserving edges and removing noise.
C1 [Chen, Baoliang; Jung, Cheolkon; Zhang, Zhendong] Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China.
C3 Xidian University
RP Jung, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China.
EM chenbaoliang@stu.xidian.edu.cn; zhengzk@xidian.edu.cn;
   zzd775089697@163.com
RI Chen, Baoliang/GPX-4976-2022
OI Chen, Baoliang/0000-0003-4884-6956; Zhang, Zhendong/0000-0001-5872-5176
FU National Natural Science Foundation of China [61271298]; International
   S&T Cooperation Program of China [2014DFG12780]
FX This work was supported by the National Natural Science Foundation of
   China (No. 61271298) and the International S&T Cooperation Program of
   China (No. 2014DFG12780).
CR [Anonymous], P IS T SPIE ELECT IM
   [Anonymous], Time-of-Flight and Structured Light Depth Cameras; Technology and Applications
   [Anonymous], P 3DPVT PAR FRANC
   [Anonymous], P SPIE OPTICAL ENG A
   Buttgen Bernhard., 2005, Citeseer, P21
   Chen BL, 2017, IEEE IMAGE PROC, P1437, DOI 10.1109/ICIP.2017.8296519
   Dal Mutto C, 2015, IEEE T PATTERN ANAL, V37, P2260, DOI 10.1109/TPAMI.2015.2408361
   Dal Mutto C, 2012, LECT NOTES COMPUT SC, V7583, P598, DOI 10.1007/978-3-642-33863-2_62
   Egnal G, 2004, IMAGE VISION COMPUT, V22, P943, DOI 10.1016/j.imavis.2004.03.018
   Gallup David, 2007, CVPR
   Guomundsson Sigurjon Arni, 2008, International Journal of Intelligent Systems Technologies and Applications, V5, P425, DOI 10.1504/IJISTA.2008.021305
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46
   Hung KW, 2010, IEEE IMAGE PROC, P3297, DOI 10.1109/ICIP.2010.5652082
   Kahlmann T, 2008, J APPL GEOD, V2, P1, DOI 10.1515/JAG.2008.001
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239453
   Marin G, 2016, LECT NOTES COMPUT SC, V9911, P386, DOI 10.1007/978-3-319-46478-7_24
   Mutto C.D., 2012, Time-of-Flight Cameras and Microsoft Kinect(TM)
   Nair Rahul, 2013, Time-Of-Flight and Depth Imaging. Sensors, Algorithms and Applications. Dagstuhl 2012 Seminar on Time-of-Flight Imaging and GCPR 2013 Workshop on Imaging New Modalities: LNCS 8200, P105, DOI 10.1007/978-3-642-44964-2_6
   Nair R, 2012, LECT NOTES COMPUT SC, V7584, P1, DOI 10.1007/978-3-642-33868-7_1
   Phan R, 2014, IEEE T MULTIMEDIA, V16, P122, DOI 10.1109/TMM.2013.2283451
   PoLin Lai, 2010, 2010 28th Picture Coding Symposium (PCS 2010), P9, DOI 10.1109/PCS.2010.5702589
   Qingxiong Yang, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P69, DOI 10.1109/MMSP.2010.5661996
   Reynolds M, 2011, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2011.5995550
   Sabov Alexander, 2008, Proceedings of the 24th Spring Conference on Computer Graphics, P135
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Scharstein D., 2010, MIDDLEBURY STEREO VI
   Tippetts B, 2016, J REAL-TIME IMAGE PR, V11, P5, DOI 10.1007/s11554-012-0313-2
   Wang L, 2014, IEEE T MULTIMEDIA, V16, P1905, DOI 10.1109/TMM.2014.2341599
   Yoon KJ, 2005, PROC CVPR IEEE, P924
   Zhu JJ, 2010, IEEE T PATTERN ANAL, V32, P899, DOI 10.1109/TPAMI.2009.68
   Zhu Jiejie., 2008, IEEE C COMPUTER VISI, P1, DOI [DOI 10.1109/CVPR.2008.4587761, DOI 10.1145/1477862.1477875]
NR 32
TC 24
Z9 24
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 2882
EP 2890
DI 10.1109/TMM.2018.2825883
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800002
DA 2024-07-18
ER

PT J
AU Wang, G
   Li, B
   Zhang, YF
   Yang, JH
AF Wang, Gang
   Li, Bo
   Zhang, Yongfei
   Yang, Jinhui
TI Background Modeling and Referencing for Moving Cameras-Captured
   Surveillance Video Coding in HEVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Surveillance video coding; HEVC; rate-distortion optimization; global
   motion estimation; motion background modeling; background reference
ID TERM REFERENCE FRAME; SUBTRACTION; PREDICTION; SELECTION
AB Surveillance video coding is crucial for improving compression efficiency in intelligent video surveillance systems and applications. Plenty of work has been done, which can be roughly divided into two categories: the former mainly focuses on low-complexity background modeling to obtain the clear background, while the latter focuses on an appropriate coding strategy to generate the high-quality background reference picture for effective background prediction. However, almost all existing works focus only on stationary camera scenes, while moving cameras-captured surveillance video coding is left untouched and is still an open problem. In this paper, a background modeling and referencing scheme for moving cameras-captured surveillance video coding in high-efficiency video coding (HEVC) is proposed. First, this paper proposes a low-complexity motion background modeling algorithm for surveillance video coding using the running average based on a global-motion-compensation method. To obtain the global motion vector, we propose a global motion detection method based on character blocks by establishing a low-rank singular value decomposition model for clustering and estimating motion vectors of background character blocks in the cameras movement circumstance. Second, we propose a background referencing coding strategy, in which the motion background coding tree units (MBCTUs) would be selected by anchoring the input video frame on the modeling background frame and coded with the optimized quantization parameter. Then, the reconstructed MBCTU will be used to update the previous coding tree unit in the global compensation location of the background reference picture. Extensive experimental results show that the proposed scheme can achieve significant bit savings of up to 26.6% and, on average, 6.7% with similar subjective quality and negligible encoding complexity, compared to HM12.0. Besides, the proposed scheme consistently outperforms two state-of-the-art surveillance video coding schemes with remarkable bitrate savings.
C1 [Wang, Gang; Li, Bo; Zhang, Yongfei; Yang, Jinhui] Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
   [Li, Bo; Yang, Jinhui] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University; Beihang University
RP Zhang, YF (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
EM smile588@sina.com; boli@buaa.edu.cn; yfzhang@buaa.edu.cn;
   jinhuiy@foxmail.com
RI Li, bo/IWL-9318-2023; Li, Bo/AAA-8968-2020; Zhang, Yongfei/A-1505-2010
OI Li, Bo/0000-0002-7294-6888; Zhang, Yongfei/0000-0002-5080-1733; wang,
   gang/0000-0001-5283-7126
FU National Key R&D Program of China [2016YFC0801001]; National Natural
   Science Foundation of China Key Project [61632001]; National Natural
   Science Foundation of China [61772054]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2016YFC0801001, in part by the National Natural Science
   Foundation of China Key Project under Grant 61632001, and in part by the
   National Natural Science Foundation of China under Grant 61772054.
CR Barnich O., 2011, IEEE T IMAGE PROCESS, V21, P2582
   Bhaskar H, 2015, SIGNAL PROCESS, V117, P343, DOI 10.1016/j.sigpro.2015.06.003
   Bouwmans T, 2017, COMPUT SCI REV, V23, P1, DOI 10.1016/j.cosrev.2016.11.001
   Bouwmans T, 2014, COMPUT SCI REV, V11-12, P31, DOI 10.1016/j.cosrev.2014.04.001
   Carman R., 2012, EDITORS GUIDE ADOBE
   Chakraverty S, 2014, ADV COMPU INTELL ROB, P1, DOI 10.4018/978-1-4666-4991-0
   Chau G, 2017, IEEE INT CONF COMP V, P1844, DOI 10.1109/ICCVW.2017.218
   Chen C., 2012, P 20 ACM INT C MULT, P713, DOI DOI 10.1145/2393347.2396294
   Chen CY, 2015, J VIS COMMUN IMAGE R, V26, P338, DOI 10.1016/j.jvcir.2014.12.001
   Chen FD, 2017, IEEE T CIRC SYST VID, V27, P2639, DOI 10.1109/TCSVT.2016.2593599
   Chen YM, 2011, IEEE T CIRC SYST VID, V21, P1316, DOI 10.1109/TCSVT.2011.2148490
   Dey B, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2445631
   Fan R, 2017, IEEE T MULTIMEDIA, V19, P893, DOI 10.1109/TMM.2016.2642786
   Ferone A, 2014, IEEE T SYST MAN CY-S, V44, P571, DOI 10.1109/TSMC.2013.2280121
   Gao W, 2014, IEEE INTELL SYST, V29, P30, DOI 10.1109/MIS.2013.101
   Guangfan Zhang, 2007, 2007 IEEE Aerospace Conference, P1, DOI 10.1109/ICEMI.2007.4350559
   Huang ZK, 2013, IEEE SIGNAL PROC LET, V20, P1058, DOI 10.1109/LSP.2013.2280138
   Javed S, 2018, IEEE T CIRC SYST VID, V28, P1315, DOI 10.1109/TCSVT.2016.2632302
   Kang Xue, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2949, DOI 10.1109/ICIP.2011.6116280
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Liu D, 2010, IEEE T CIRC SYST VID, V20, P325, DOI 10.1109/TCSVT.2009.2031442
   Liu Y, 2016, IEEE T MULTIMEDIA, V18, P351, DOI 10.1109/TMM.2016.2514848
   Mingchao Geng, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P61, DOI 10.1109/ICME.2012.65
   Paul M, 2014, IEEE T CIRC SYST VID, V24, P1729, DOI 10.1109/TCSVT.2014.2302555
   Shao ZF, 2018, IEEE T BIG DATA, V4, P105, DOI 10.1109/TBDATA.2017.2715815
   Sheikh Y, 2005, IEEE T PATTERN ANAL, V27, P1778, DOI 10.1109/TPAMI.2005.213
   Sheikh Y, 2009, IEEE I CONF COMP VIS, P1219, DOI 10.1109/ICCV.2009.5459334
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Song L, 2013, INT WORK QUAL MULTIM, P34, DOI 10.1109/QoMEX.2013.6603201
   St-Charles PL, 2016, IEEE T IMAGE PROCESS, V25, P4768, DOI 10.1109/TIP.2016.2598691
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tiwari M, 2008, IEEE SIGNAL PROC LET, V15, P249, DOI 10.1109/LSP.2007.914928
   Wang K, 2003, PATTERN RECOGN, V36, P2287, DOI 10.1016/S0031-3203(03)00082-7
   Wang MH, 2016, IEEE T IMAGE PROCESS, V25, P2943, DOI 10.1109/TIP.2016.2552646
   Wiegand T, 1999, IEEE T CIRC SYST VID, V9, P70, DOI 10.1109/76.744276
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Wu YY, 2017, IEEE T CIRC SYST VID, V27, P236, DOI 10.1109/TCSVT.2015.2493499
   Xiao J, 2016, IEEE T MULTIMEDIA, V18, P1691, DOI 10.1109/TMM.2016.2581590
   Yuan C., 2011, IEEE T PATTERN ANAL, V20, P1709
   Zamalieva D, 2014, COMPUT VIS IMAGE UND, V127, P73, DOI 10.1016/j.cviu.2014.06.007
   Zhang XG, 2014, IEEE T IMAGE PROCESS, V23, P4511, DOI 10.1109/TIP.2014.2352036
   Zhang XG, 2014, IEEE T IMAGE PROCESS, V23, P769, DOI 10.1109/TIP.2013.2294549
   Zhao L., 2013, P IEEE VEH TECHN C V, P1
   Zhao Liang., 2014, 2014 12th IEEE International Conference on Solid-State and Integrated Circuit Technology (ICSICT), P1
   Zhou ML, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107616
NR 48
TC 27
Z9 28
U1 1
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 2921
EP 2934
DI 10.1109/TMM.2018.2829163
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800005
DA 2024-07-18
ER

PT J
AU Kim, H
   Mei, T
   Byun, H
   Yao, T
AF Kim, Hoseong
   Mei, Tao
   Byun, Hyeran
   Yao, Ting
TI Exploiting Web Images for Video Highlight Detection With Triplet Deep
   Ranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video highlight detection; triplet deep ranking; weakly supervised Web
   images; category-independent learning; deep learning
ID MODEL
AB Highlight detection from videos has been widely studied due to the fast growth of video contents. However, most existing approaches to highlight detection, either handcraft feature based or deep learning based, heavily rely on human-curated training data, which is very expensive to obtain and, thus, hinders the scalability to large datasets and unlabeled video categories. We observe that the largely available Web images can he applied as a weak supervision for highlight detection. For example, the top-ranked images in reference to the query "skiing" returned by a search engine may contain considerable positive samples of "skiing" highlights. Motivated by this observation, we propose a novel triplet deep ranking approach to video highlight detection using Web images as a weak supervision. The approach handles the relative preference of highlight scores between highlighting frames, nonhighlighting frames, and Web images by the triplet ranking constraints. Our approach can iteratively train two interdependent deep models (i.e., a triplet highlight model and a pairwise noise model) to deal with the noisy Web images in a single framework. We train the two models with relative preferences to generalize the capability regardless of the categories of training data. Therefore, our approach is fully category independent and exploits weakly supervised Web images. We evaluate our approach on two challenging datasets and achieve impressive results compared with the state-of-the-art pairwise ranking support vector machines, a robust recurrent autoencoder, and spatial deep convolution neural networks. We also empirically verify through cross-dataset evaluation that our category-independent model is fairly generalizable even if two different datasets do not share exactly the same categories.
C1 [Kim, Hoseong; Byun, Hyeran] Yonsei Univ, Dept Comp Sci, Seoul 03722, South Korea.
   [Mei, Tao] JD Com, Artificial Intelligence Platform & Res, Beijing 100105, Peoples R China.
   [Yao, Ting] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Yonsei University; Microsoft Research Asia; Microsoft
RP Mei, T (corresponding author), JD Com, Artificial Intelligence Platform & Res, Beijing 100105, Peoples R China.
EM hskim.cvpr@yonsei.ac.kr; tmei@live.com; hrbyun@yonsei.ac.kr;
   tiyao@microsoft.com
RI 황선희, (일반대학원 컴퓨터과학과)/KRQ-3765-2024
FU National Research Foundation of Korea (NRF) - Korea government (MSIT)
   [NRF-2016R1A2B4009246]; MSIP (The Ministry of Science, ICT and Future
   Planning), Korea; Microsoft Research, under ICT/SW Creative research
   program [IITP-2015-R2212-15-0015]
FX This work was supported in part by the National Research Foundation of
   Korea (NRF) grant funded by the Korea government (MSIT)
   (NRF-2016R1A2B4009246) and in part by the MSIP (The Ministry of Science,
   ICT and Future Planning), Korea and Microsoft Research, under ICT/SW
   Creative research program supervised by the IITP (Institute for
   Information & Communications Technology Promotion)
   (IITP-2015-R2212-15-0015). Part of this work was performed when Hoseong
   Kim was a research intern at Microsoft Research Asia.
CR [Anonymous], P INT WORKSH MULT IN
   Bettadapura Vinay., 2016, P ACM INT C MULT, P908
   Chen F, 2014, IEEE T MULTIMEDIA, V16, P455, DOI 10.1109/TMM.2013.2291967
   Chen SZ, 2016, IEEE T IMAGE PROCESS, V25, P2353, DOI 10.1109/TIP.2016.2545929
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doman K, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P949, DOI 10.1145/2647868.2654973
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fu JL, 2015, IEEE I CONF COMP VIS, P1985, DOI 10.1109/ICCV.2015.230
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Gygli M, 2016, PROC CVPR IEEE, P1001, DOI 10.1109/CVPR.2016.114
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P1114, DOI 10.1109/TMM.2005.858397
   He K., 2015, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2015.7299173, DOI 10.1109/CVPR.2015.7299173]
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Khosla A, 2013, PROC CVPR IEEE, P2698, DOI 10.1109/CVPR.2013.348
   Kim G, 2014, PROC CVPR IEEE, P4225, DOI 10.1109/CVPR.2014.538
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuanar SK, 2015, IEEE T MULTIMEDIA, V17, P1166, DOI 10.1109/TMM.2015.2443558
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Li HJ, 2014, MACH VISION APPL, V25, P1661, DOI 10.1007/s00138-013-0537-6
   Lin YL, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P443, DOI 10.1109/ICCVW.2015.65
   Liu F, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P2058
   Liu YJ, 2016, IEEE T MULTIMEDIA, V18, P1269, DOI 10.1109/TMM.2016.2557061
   Lu SY, 2014, IEEE T MULTIMEDIA, V16, P1497, DOI 10.1109/TMM.2014.2319778
   Meng JJ, 2016, PROC CVPR IEEE, P1039, DOI 10.1109/CVPR.2016.118
   Mnih V., 2012, P 29 INT C MACH LEAR, P567
   Natarajan N., 2013, ADV NEURAL INFORM PR, P1196, DOI DOI 10.5555/2999611.2999745
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Rubinstein RY, 1997, EUR J OPER RES, V99, P89, DOI 10.1016/S0377-2217(96)00385-2
   Simo-Serra E, 2016, PROC CVPR IEEE, P298, DOI 10.1109/CVPR.2016.39
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Sukhbaatar S., 2015, P INT C LEARN REPR W, P1
   Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51
   Tang H., 2011, 2011 IEEE International Conference on Multimedia and Expo ICME, P1
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Yang H, 2015, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2015.526
   Yao T., 2015, COMPUT VIS IMAGE UND, V140, P58
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Zhang K, 2016, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2016.120
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhang R, 2017, MULTIMEDIA SYST, V23, P713, DOI 10.1007/s00530-016-0506-9
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
NR 51
TC 18
Z9 20
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2415
EP 2426
DI 10.1109/TMM.2018.2806224
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200014
DA 2024-07-18
ER

PT J
AU Chen, ZG
   Li, LX
   Peng, HP
   Liu, YH
   Yang, YX
AF Chen, Zigang
   Li, Lixiang
   Peng, Haipeng
   Liu, Yuhong
   Yang, Yixian
TI A Novel Digital Watermarking Based on General Non-Negative Matrix
   Factorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Digital watermarking; general non-negative matrix factorization
   (general-NMF); semi-tensor product (STP)
ID IMAGE AUTHENTICATION; NETWORKS; TRANSFORM; DYNAMICS; SCHEME
AB In this paper, we propose a novel general non-negative matrix factorization (general-NMF)-based digital watermarking scheme for copyright protection and integrity authentication of the image content. Specifically, the proposed general-NMF algorithm is able to factorize a matrix C is an element of R-+(s x t) a into a basis matrix A is an element of R-+( m x n) and a coefficient matrix B is an element of R-+(p x q) by removing the dimension-matching constraints required by the conventional NMF, where s = m, n = p, and t = q. In particular, s = m . l/n, t = l/p . q, and the variable l is the least common multiple of n and p. Furthermore, the generator factor of the random matrix and n are used as the keys of the proposed digital watermarking scheme. Experimental results show that the proposed digital watermarking scheme can effectively resist various attacks and tampering.
C1 [Chen, Zigang; Li, Lixiang; Peng, Haipeng; Yang, Yixian] Beijing Univ Posts & Telecommun, Informat Secur Ctr, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Chen, Zigang] Nanyang Inst Technol, Coll Comp & Informat Engn, Nanyang 473004, Henan, Peoples R China.
   [Liu, Yuhong] Santa Clara Univ, Dept Comp Engn, Santa Clara, CA 95053 USA.
C3 Beijing University of Posts & Telecommunications; Nanyang Institute of
   Technology; Santa Clara University
RP Li, LX (corresponding author), Beijing Univ Posts & Telecommun, Informat Secur Ctr, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
EM nyczg@163.com; li_lixiang2006@163.com; penghaipeng@bupt.edu.cn;
   yhliu@scu.edu; yangyixian@bupt.edu.cn
RI Chen, Zigang/GYD-9546-2022; Li, lixiang/G-6222-2011
OI Chen, Zigang/0000-0003-4754-7049; Lixiang, Li/0000-0001-9949-8731
FU National Key Research and Development Program [2016YFB0800602]; National
   Natural Science Foundation of China [61472045, 61573067, 61771071]
FX This work was supported in part by the National Key Research and
   Development Program under Grant 2016YFB0800602 and in part by the
   National Natural Science Foundation of China under Grants 61472045,
   61573067, and 61771071. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Honggang
   Wang.
CR [Anonymous], 2001, Sci. China, Ser. F: Info. Sci.
   Bellman R., 1970, Introduction To Matrix Analysis, V960
   Cheng D., 2012, INTRO SEMITENSOR PRO
   Cheng DZ, 2011, IEEE T AUTOMAT CONTR, V56, P2, DOI 10.1109/TAC.2010.2050161
   Cheng DZ, 2010, IEEE T AUTOMAT CONTR, V55, P2251, DOI 10.1109/TAC.2010.2043294
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Dhar PK, 2014, 2014 9TH INTERNATIONAL FORUM ON STRATEGIC TECHNOLOGY (IFOST), P144, DOI 10.1109/IFOST.2014.6991091
   Essid S, 2013, IEEE T MULTIMEDIA, V15, P415, DOI 10.1109/TMM.2012.2228474
   Gao ZP, 2016, IEEE T MULTIMEDIA, V18, P614, DOI 10.1109/TMM.2016.2523425
   Ghaderpanah M, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS, VOLS 1-7, P464, DOI 10.1109/ISIE.2006.295639
   Huang HC, 2010, TELECOMMUN SYST, V44, P241, DOI 10.1007/s11235-009-9262-x
   Karsh R. K., 2015, P TENCON 2015 IEEE R, P1
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   LI X, 2015, IEEE T IMAGE PROCESS, V24
   Li ZQ, 2010, INT J BIFURCAT CHAOS, V20, P561, DOI 10.1142/S0218127410025892
   Lu W, 2009, COMPUT ELECTR ENG, V35, P183, DOI 10.1016/j.compeleceng.2008.09.004
   Mane GV, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND COMPUTING RESEARCH (ICCIC), P55
   Ming T, 2008, 2008 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY, VOLS 1 AND 2, PROCEEDINGS, P385, DOI 10.1109/CIS.2008.56
   Murata H., 2016, P IEEE INT S BROADB, P1
   Ouhsain M, 2009, EXPERT SYST APPL, V36, P2123, DOI 10.1016/j.eswa.2007.12.046
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   Schneider M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P227, DOI 10.1109/ICIP.1996.560425
   Silja MS, 2009, 2009 INTERNATIONAL CONFERENCE ON ADVANCES IN RECENT TECHNOLOGIES IN COMMUNICATION AND COMPUTING (ARTCOM 2009), P279, DOI 10.1109/ARTCom.2009.198
   Singh D, 2017, MULTIMED TOOLS APPL, V76, P953, DOI 10.1007/s11042-015-3010-x
   Tang S, 2012, IEEE T MULTIMEDIA, V14, P43, DOI 10.1109/TMM.2011.2168198
   Tang ZJ, 2014, IEEE T KNOWL DATA EN, V26, P711, DOI 10.1109/TKDE.2013.45
   Tang ZJ, 2011, MULTIMED TOOLS APPL, V52, P325, DOI 10.1007/s11042-009-0437-y
   Tiwari A, 2017, AEU-INT J ELECTRON C, V78, P114, DOI 10.1016/j.aeue.2017.05.027
   Tong M, 2016, MULTIMED TOOLS APPL, V75, P8045, DOI 10.1007/s11042-015-2722-2
   WALTON S, 1995, DR DOBBS J, V20, P18
   Wang C, 2005, INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES 2005, VOLS 1 AND 2, PROCEEDINGS, P869
   Wang HY, 2014, APPL MECH MATER, V577, P754, DOI 10.4028/www.scientific.net/AMM.577.754
   Xiang SJ, 2012, EURASIP J ADV SIG PR, P1, DOI 10.1186/1687-6180-2012-77
   Zuo JL, 2010, ADV MATER RES-SWITZ, V121-122, P254, DOI 10.4028/www.scientific.net/AMR.121-122.254
NR 34
TC 34
Z9 35
U1 5
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 1973
EP 1986
DI 10.1109/TMM.2018.2794985
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600005
DA 2024-07-18
ER

PT J
AU Cho, SI
   Kang, SJ
AF Cho, Sung In
   Kang, Suk-Ju
TI Geodesic Path-Based Diffusion Acceleration for Image Denoising
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image denoising; anisotropic diffusion; diffusion acceleration;
   structure tensor; geodesic path
ID ANISOTROPIC DIFFUSION; EDGE-DETECTION; DECOMPOSITION; ALGORITHM
AB We propose an advanced anisotropic-diffusion (AD)based approach for an image denoising method, which utilizes a geodesic path to produce single-pass adaptive smoothing by analyzing the diffusion continuity. The proposed method consists of the following four procedures: element-weight determination, geodesic path-based kernel (GPK) generation, single-pass smoothing using the GPK, and post-processing of the GPK smoothing. In the first procedure, weights for neighboring pixels are calculated by diffusivity analysis. In the second procedure, a geodesic path is selected using a geodesic distance that is calculated by a diffusion continuity analysis. In the third procedure, GPK-based smoothing is applied to a given noisy image to extract the noise-free pixel value. Finally, a distant AD that uses the double diffusion length is applied to the resultant image by the GPK filtering to enhance the quality of noise suppression in smooth regions. In addition to the main procedures, schemes for the robust outlier reduction and complexity reduction are introduced. The simulation results showed that the proposed method improved the denoising quality by increasing the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) by up to 4.094 dB and 0.057, respectively, compared to the AD-based benchmark methods. Compared to block-matching and 3D filtering, the proposed method showed comparable quality of noise reduction with similar PSNR and SSIM values, which it accomplished with much less computation time.
C1 [Cho, Sung In] Daegu Univ, Dept Elect & Elect Engn, Gyongsan 38453, South Korea.
   [Kang, Suk-Ju] Sogang Univ, Dept Elect Engn, Seoul 121742, South Korea.
C3 Daegu University; Sogang University
RP Kang, SJ (corresponding author), Sogang Univ, Dept Elect Engn, Seoul 121742, South Korea.
EM csi2267@daegu.ac.kr; sjkang@sogang.ac.kr
CR [Anonymous], 1998, Anisotropic diffusion in image processing
   [Anonymous], 2002, DERFS COLLECTION FOO
   Benierbah S, 2006, IEE P-VIS IMAGE SIGN, V153, P237, DOI 10.1049/ip-vis:20050129
   Black MJ, 1998, IEEE T IMAGE PROCESS, V7, P421, DOI 10.1109/83.661192
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   CATTE F, 1992, SIAM J NUMER ANAL, V29, P182, DOI 10.1137/0729012
   Chao SM, 2010, PATTERN RECOGN LETT, V31, P2012, DOI 10.1016/j.patrec.2010.06.004
   Chen XG, 2015, IEEE T CIRC SYST VID, V25, P897, DOI 10.1109/TCSVT.2014.2365654
   Cho SI, 2016, DIGIT SIGNAL PROCESS, V48, P27, DOI 10.1016/j.dsp.2015.08.019
   Cho SI, 2014, PATTERN RECOGN LETT, V46, P36, DOI 10.1016/j.patrec.2014.05.003
   Criminisi A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857910
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai JJ, 2013, IEEE T CIRC SYST VID, V23, P128, DOI 10.1109/TCSVT.2012.2203203
   Fowlkes CC, 2007, J VISION, V7, DOI 10.1167/7.8.2
   Goffman-Vinopal L, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P353
   Grazzini J, 2009, PATTERN RECOGN, V42, P2306, DOI 10.1016/j.patcog.2008.11.004
   Ham B, 2013, IEEE T IMAGE PROCESS, V22, P1096, DOI 10.1109/TIP.2012.2226904
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   Jakhetiya V, 2017, IEEE T MULTIMEDIA, V19, P93, DOI 10.1109/TMM.2016.2609419
   Jones K., 2008, ENERGY STAR PROGRAM
   Kervrann C, 2006, IEEE T IMAGE PROCESS, V15, P2866, DOI 10.1109/TIP.2006.877529
   Kim S, 2016, J VIS COMMUN IMAGE R, V40, P384, DOI 10.1016/j.jvcir.2016.07.005
   Kim YH, 2009, IEEE T CIRC SYST VID, V19, P1051, DOI 10.1109/TCSVT.2009.2020251
   Li HC, 2012, ELECTRON LETT, V48, P827, DOI 10.1049/el.2011.3994
   Ma R, 2014, IEEE T SIGNAL PROCES, V62, P1671, DOI 10.1109/TSP.2014.2301139
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang HY, 2017, IEEE T MULTIMEDIA, V19, P969, DOI 10.1109/TMM.2016.2638624
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weickert J, 1999, INT J COMPUT VISION, V31, P111, DOI 10.1023/A:1008009714131
   Xiong RQ, 2016, IEEE T IMAGE PROCESS, V25, P5793, DOI 10.1109/TIP.2016.2614160
   Yu JH, 2008, PATTERN RECOGN LETT, V29, P1496, DOI 10.1016/j.patrec.2008.03.002
   Zhang HC, 2013, IEEE T CYBERNETICS, V43, P1035, DOI 10.1109/TSMCB.2012.2222375
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
NR 37
TC 18
Z9 18
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1738
EP 1750
DI 10.1109/TMM.2017.2781371
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100012
DA 2024-07-18
ER

PT J
AU Hu, H
   Li, YL
   Wen, YG
AF Hu, Han
   Li, Yuanlong
   Wen, Yonggang
TI Toward Rendering-Latency Reduction for Composable Web Services via
   Priority-Based Object Caching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Web page caching; submodular function maximization
AB Web services serve as the cornerstone of the Internet for rendering webpages. The initial rendering latency of webpages, which depends on a subset of critical objects required by the webpage, is a key metric for web services. In this work, we propose to identify this set of critical objects systematically with the goal of caching them at a higher priority to reduce the initial rendering time. We first conduct a measurement study on a mainstream content delivery network provider, the results of which suggest that not all currently cached objects are critical and that only a small portion of the critical objects are cached. Thus, we model the critical-object aware caching scheme as a constrained optimization problem. Using the stochastic optimization framework, we decompose the problem into a set of one-shot optimization problems, which are proved to be NP-hard. We then develop two greedy algorithms with different computational complexity but the same performance bound. Finally, we integrate the resulting approximation algorithms into an online algorithm. Through trace-based simulations, we verify that our proposed algorithm can reduce service latency and network traffic by ensuring a higher cache hit ratio.
C1 [Hu, Han; Li, Yuanlong; Wen, Yonggang] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Li, YL (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM hhu@ntu.edu.sg; liyuanl@ntu.edu.sg; ygwen@ntu.edu.sg
RI Wen, Yonggang/B-8848-2011; Wen, Yonggang/P-9406-2017
OI Wen, Yonggang/0000-0002-2751-5114; Hu, Han/0000-0001-7532-0496
FU AcRF Tier 1 [RG17/14, RG26/16]
FX This work was supported in part by the Grant of AcRF Tier 1 RG17/14 and
   AcRF Tier 1 RG26/16. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Christian
   Timmerer.
CR Agababov Victor, 2015, NSDI'15
   [Anonymous], 2015, 12th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 15)
   [Anonymous], 2015, USING AKAMAI PRAGMA
   [Anonymous], 2007, HAR 1 2 SPEC
   [Anonymous], 2015, AKAMAI CUSTOMER LIST
   Bolot JC, 1996, COMPUT NETWORKS ISDN, V28, P1397, DOI 10.1016/0169-7552(96)00073-6
   Brewington BE, 2000, COMPUT NETW, V33, P257, DOI 10.1016/S1389-1286(00)00045-1
   Cao P, 1997, PROCEEDINGS OF THE USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P193
   Chankhunthod A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P153
   Che H, 2002, IEEE J SEL AREA COMM, V20, P1305, DOI 10.1109/JSAC.2002.801752
   Ortiz JPG, 2008, IEEE T MULTIMEDIA, V10, P629, DOI 10.1109/TMM.2008.921738
   Gu XH, 2006, IEEE T MULTIMEDIA, V8, P141, DOI 10.1109/TMM.2005.861284
   Hamilton J., 2018, THE COST OF LATENCY
   Hu H, 2017, ACM T INTEL SYST TEC, V8, DOI 10.1145/3014431
   Hu H, 2017, IEEE J SEL AREA COMM, V35, P545, DOI 10.1109/JSAC.2017.2659478
   Hu H, 2016, IEEE T CIRC SYST VID, V26, P1320, DOI 10.1109/TCSVT.2015.2455712
   Jin YC, 2014, IEEE T MULTIMEDIA, V16, P1739, DOI 10.1109/TMM.2014.2329370
   Kong W., IEEE T CIRCUITS SYST
   Korupolu MR, 2002, IEEE T KNOWL DATA EN, V14, P1317, DOI 10.1109/TKDE.2002.1047770
   Li Zhichun, 2010, NSDI, V10, P143
   Mai H., 2012, P 4 USENIX WORKSH HO
   Meyerovich L.A., 2010, WWW 10, P711, DOI DOI 10.1145/1772690.1772763
   Narayanan SP, 2016, SIGMETRICS/PERFORMANCE 2016: PROCEEDINGS OF THE SIGMETRICS/PERFORMANCE JOINT INTERNATIONAL CONFERENCE ON MEASUREMENT AND MODELING OF COMPUTER SCIENCE, P89, DOI 10.1145/2896377.2901472
   Netravali R, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P123
   Padychova Zuzana, 2018, PAGE LOAD TIME AFFEC
   SCHRIJVER A., 2003, COMBINATORIAL OPTIMI, V24, P39
   Shafiq M. Z., 2016, P INF, P193
   Shudong Jin, 2000, Proceedings 20th IEEE International Conference on Distributed Computing Systems, P254, DOI 10.1109/ICDCS.2000.840936
   Souders S., Onload event and post-onload requests
   Wang J., 1999, ACM SIGCOMM COMP COM, V29, P36
   Wang Xiao Sophia, 2013, 10 USENIX S NETWORKE
   Wang Z, 2015, IEEE T MULTIMEDIA, V17, P92, DOI 10.1109/TMM.2014.2365364
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Zhou L, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2886776
   Zhou L, 2017, IEEE T CIRC SYST VID, V27, P84, DOI 10.1109/TCSVT.2016.2539698
   Zhou L, 2016, IEEE T MULTIMEDIA, V18, P905, DOI 10.1109/TMM.2016.2537782
   Zhou L, 2014, IEEE J SEL AREA COMM, V32, P760, DOI 10.1109/JSAC.2014.140408
NR 38
TC 7
Z9 7
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1864
EP 1875
DI 10.1109/TMM.2017.2779041
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100021
DA 2024-07-18
ER

PT J
AU Ke, Q
   Bennamoun, M
   An, SJ
   Sohel, F
   Boussaid, F
AF Ke, Qiuhong
   Bennamoun, Mohammed
   An, Senjian
   Sohel, Ferdous
   Boussaid, Farid
TI Leveraging Structural Context Models and Ranking Score Fusion for Human
   Interaction Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interaction prediction; interaction structure; LSTM; ranking score
   fusion
ID ACTION RECOGNITION
AB Predicting an interaction before it is fully executed is very important in applications, such as human-robot interaction and video surveillance. In a two-human interaction scenario, there are often contextual dependency structures between the global interaction context of the two humans and the local context of the different body parts of each human. In this paper, we propose to learn the structure of the interaction contexts and combine it with the spatial and temporal information of a video sequence to better predict the interaction class. The structural models, including the spatial and the temporal models, are learned with long short term memory (LSTM) networks to capture the dependency of the global and local contexts of each RGB frame and each optical flow image, respectively. LSTM networks are also capable of detecting the key information from global and local interaction contexts. Moreover, to effectively combine the structural models with the spatial and temporal models for interaction prediction, a ranking score fusion method is introduced to automatically compute the optimal weight of each model for score fusion. Experimental results on the BIT-Interaction Dataset and the UT-Interaction Dataset clearly demonstrate the benefits of the proposed method.
C1 [Ke, Qiuhong; Bennamoun, Mohammed; An, Senjian] Univ Western Australia, Sch Comp Sci & Software Engn, Crawley, WA 6009, Australia.
   [Sohel, Ferdous] Murdoch Univ, Sch Engn & Informat Technol, Murdoch, WA 6150, Australia.
   [Boussaid, Farid] Univ Western Australia, Sch Elect Elect & Comp Engn, Crawley, WA 6009, Australia.
C3 University of Western Australia; Murdoch University; University of
   Western Australia
RP Ke, Q (corresponding author), Univ Western Australia, Sch Comp Sci & Software Engn, Crawley, WA 6009, Australia.
EM qiuhong.ke@research.uwa.edu.au; mohammed.bennamoun@uwa.edu.au;
   senjian.an@uwa.edu.au; f.sohel@murdoch.edu.au; farid.boussaid@uwa.edu.au
RI Ke, Qiuhong/AAA-6166-2022; Bennamoun, Mohammed/C-2789-2013; An,
   Senjian/H-8746-2014; Sohel, Ferdous/C-2428-2013
OI Bennamoun, Mohammed/0000-0002-6603-3257; Sohel,
   Ferdous/0000-0003-1557-4907; An, Senjian/0000-0002-1758-6824; Ke,
   Qiuhong/0000-0001-9998-3614; BOUSSAID, FARID/0000-0001-7250-7407
FU Australian Research Council [DP150100294, DP150104251, DE120102960]
FX This work was supported by Australian Research Council Grants
   DP150100294, DP150104251, and DE120102960.
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.214
   Cao Y, 2013, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2013.343
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chung PC, 2008, PATTERN RECOGN, V41, P1572, DOI 10.1016/j.patcog.2007.10.022
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   De Cesarei A, 2011, PSYCHON B REV, V18, P840, DOI 10.3758/s13423-011-0133-6
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Eddy SR, 1996, CURR OPIN STRUC BIOL, V6, P361, DOI 10.1016/S0959-440X(96)80056-X
   Gan C, 2016, AAAI CONF ARTIF INTE, P3487
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hasan M, 2015, IEEE T MULTIMEDIA, V17, P1909, DOI 10.1109/TMM.2015.2477242
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jaeger H., 2002, Technical report GMD-Forschungszentrum Informationstechnik, V5
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Ke QH, 2017, IEEE SIGNAL PROC LET, V24, P731, DOI 10.1109/LSP.2017.2690339
   Ke QH, 2016, LECT NOTES COMPUT SC, V9914, P403, DOI 10.1007/978-3-319-48881-3_28
   Ke QH, 2014, PROC CVPR IEEE, P4146, DOI 10.1109/CVPR.2014.528
   Kolen JF, 2009, A Field Guide to Dynamical Recurrent Networks, DOI [DOI 10.1109/9780470544037.CH14, 10.1109/9780470544037]
   Kong Y, 2016, IEEE T PATTERN ANAL, V38, P1844, DOI 10.1109/TPAMI.2015.2491928
   Kong Y, 2014, LECT NOTES COMPUT SC, V8693, P596, DOI 10.1007/978-3-319-10602-1_39
   Kong Y, 2012, LECT NOTES COMPUT SC, V7572, P300, DOI 10.1007/978-3-642-33718-5_22
   Lafferty John, 2001, INT C MACH LEARN ICM
   Lan T, 2014, LECT NOTES COMPUT SC, V8691, P689, DOI 10.1007/978-3-319-10578-9_45
   Li YL, 2015, PATTERN RECOGN, V48, P3542, DOI 10.1016/j.patcog.2015.04.018
   Liu FY, 2015, PATTERN RECOGN, V48, P2983, DOI 10.1016/j.patcog.2015.04.019
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu Juncheng., 2017, P IEEE C COMPUTER VI, P792, DOI DOI 10.1109/CVPR.2017.391
   Mahasseni B, 2016, PROC CVPR IEEE, P3054, DOI 10.1109/CVPR.2016.333
   Hoai M, 2014, INT J COMPUT VISION, V107, P191, DOI 10.1007/s11263-013-0683-3
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Palangi H, 2016, IEEE-ACM T AUDIO SPE, V24, P694, DOI 10.1109/TASLP.2016.2520371
   Pascanu R., 2013, INT C MACH LEARN, P1310
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ramanathan V, 2015, IEEE I CONF COMP VIS, P4471, DOI 10.1109/ICCV.2015.508
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349
   Sak H, 2014, INTERSPEECH, P338
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shuai B, 2016, PROC CVPR IEEE, P3620, DOI 10.1109/CVPR.2016.394
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Song Y, 2013, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR.2013.457
   Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Zhang JG, 2010, PATTERN RECOGN, V43, P197, DOI 10.1016/j.patcog.2009.05.015
   Zuo Z, 2015, 2015 IEEE C COMP VIS, P18, DOI [10.1109/CVPRW.2015.7301268, DOI 10.1109/CVPRW.2015.7301268]
NR 54
TC 28
Z9 28
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1712
EP 1723
DI 10.1109/TMM.2017.2778559
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zou, WJ
   Yang, FZ
   Song, JR
   Wan, S
   Zhang, W
   Wu, HR
AF Zou, Wenjie
   Yang, Fuzheng
   Song, Jiarun
   Wan, Shuai
   Zhang, Wei
   Wu, Hong Ren
TI Event-Based Perceptual Quality Assessment for HTTP-Based Video Streaming
   With Playback Interruption
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of Experience; perceptual quality; HTTP-based video streaming;
   rebuffering; network video service
ID MODEL; EXPERIENCE
AB The popularity of HTTP-based video streaming services has been increasing in recent years. The quality of HTTP-based video streaming services is measured by the user's quality of experience (QoE). Perceptual quality (PQ), a strong indicator of the QoE, has been widely studied in the literature. Specifically, previous studies primarily focused on modeling the overall perceptual quality at the end of the viewing process. The specific influence of each event during the viewing process was neglected. Since the PQ is a function of time, which models the human perception of audiovisual services, this contribution focuses on investigating the time-varying feature of human perception. In this paper, the viewing timeline of subjects is divided into rebuffering and playback. An event-based perceptual quality assessment (EPQ) framework is introduced, which can assess the PQ of interrupted HTTP-based video streaming at any point in time. To understand the human perception of video streams with playback interruptions, a subjective quality assessment experiment was designed to obtain the PQ at a series of points in time. A between-subjects design was adopted in which each subject can view video content without repetition. Based on the experimental results, a PQ assessment model was proposed to predict the change in PQ (i.e. ,Delta PQ) after each rebuffering and playback. The overall PQ at any point in time is calculated as the summation of the Delta PQs of all the previous events. The experimental results demonstrated that the EPQ-based model outperforms the rebuffering components contained in the ITU-T Rec. P.1201.1, P.1201 Amd. 2 and P.1203.3 in our database.
C1 [Zou, Wenjie; Yang, Fuzheng; Song, Jiarun; Zhang, Wei] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Yang, Fuzheng] RMIT Univ, Sch Elect & Comp Engn, Melbourne, Vic 3001, Australia.
   [Wan, Shuai] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Shaanxi, Peoples R China.
   [Wu, Hong Ren] RMIT Univ, Sch Engn, Melbourne, Vic 3001, Australia.
C3 Xidian University; Royal Melbourne Institute of Technology (RMIT);
   Northwestern Polytechnical University; Royal Melbourne Institute of
   Technology (RMIT)
RP Zou, WJ (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM wjzou@xidian.edu.cn; fzhyang@mail.xidian.edu.cn; jrsong@xidian.edu.cn;
   swan@nwpu.edu.cn; wzhang@xidian.edu.cn; henry.wu@rmit.edu.au
RI 王, 宇佳/AGU-6378-2022; Wu, Hong Ren/C-6068-2014
OI 王, 宇佳/0000-0001-8434-4801; Wu, Hong Ren/0000-0002-7086-1629; Zou,
   Wenjie/0000-0002-6602-8001; Song, Jiarun/0000-0001-6718-4201
FU National Science Foundation of China [61371089, 61601349, 61571337];
   China Postdoctoral Science Foundation [2016M592757]; 111 Project
   [B08038]
FX This work was supported in part by the National Science Foundation of
   China under Grants 61371089, 61601349, and 61571337; in part by China
   Postdoctoral Science Foundation funded Project 2016M592757; and in part
   by the 111 Project B08038.
CR Alberti C, 2013, INT WORK QUAL MULTIM, P58, DOI 10.1109/QoMEX.2013.6603211
   Ameigeiras P, 2012, IEEE COMMUN LETT, V16, P278, DOI 10.1109/LCOMM.2011.121311.111682
   [Anonymous], 2012, PARAMETRIC NONINTRUS
   [Anonymous], 1999, document P.910, DOI 11.1002/1000/4751
   [Anonymous], USE P 1201 NONADAPTI
   [Anonymous], DEL PROGR DOWNL
   [Anonymous], 2014, METHODS SUBJECTIVE A
   [Anonymous], 2017, 2017 9 INT C QUAL MU
   [Anonymous], REC TD 109REV2 PLEN
   [Anonymous], QUALINET
   [Anonymous], 2011, HYBR PERC BITSTR GRO
   [Anonymous], P 11 INT C INT US IN
   [Anonymous], 21 ITC SPEC SEM MULT
   [Anonymous], 2016, Recommendation ITU-T P.1203
   [Anonymous], BT50013 INT TEL UN
   Bordens K., 2011, Research design and methods: A process approach, V8th
   Chen C, 2014, IEEE T IMAGE PROCESS, V23, P2206, DOI 10.1109/TIP.2014.2312613
   Cronbach LJ, 1951, PSYCHOMETRIKA, V16, P297
   De Pessemier T, 2013, IEEE T BROADCAST, V59, P47, DOI 10.1109/TBC.2012.2220231
   Ghadiyaram D, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P989, DOI 10.1109/GlobalSIP.2014.7032269
   GREENWALD AG, 1976, PSYCHOL BULL, V83, P314, DOI 10.1037/0033-2909.83.2.314
   Gustafsson J, 2008, IEEE IMAGE PROC, P405, DOI 10.1109/ICIP.2008.4711777
   Hossfeld T, 2012, INT WORK QUAL MULTIM, P1, DOI 10.1109/QoMEX.2012.6263849
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Kendall G. Maurice, 1979, ADV THEORY STAT
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Laghari KUR, 2012, IEEE COMMUN MAG, V50, P58, DOI 10.1109/MCOM.2012.6178834
   Liu H, 2013, 2013 15TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT), P413, DOI 10.1109/ICCT.2013.6820411
   Liu Y, 2015, IEEE T BROADCAST, V61, P651, DOI 10.1109/TBC.2015.2460611
   Mok R. K. P., 2011, 2011 IFIP/IEEE International Symposium on Integrated Network Management (IM 2011), P485, DOI 10.1109/INM.2011.5990550
   Montgomery D.C., 2010, Applied Statistics and Probability for Engineers (Edicao: 6)
   Moorthy AK, 2010, IEEE T CIRC SYST VID, V20, P587, DOI 10.1109/TCSVT.2010.2041829
   Oyman O, 2012, IEEE COMMUN MAG, V50, P20, DOI 10.1109/MCOM.2012.6178830
   Raake A., 2017, 2017 9 INT C QUAL MU, P1
   Reichl P, 2010, IEEE ICC
   Robitza Werner., 2017, PROC 9 INT C QUAL MU, P1
   Seshadrinathan K, 2011, INT CONF ACOUST SPEE, P1153
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Seyedebrahimi M, 2013, IEEE T CIRC SYST VID, V23, P2034, DOI 10.1109/TCSVT.2013.2270365
   Song JR, 2016, IEEE T MULTIMEDIA, V18, P444, DOI 10.1109/TMM.2016.2520090
   Wang KY, 2014, IEEE I C NETW INFRAS, P56, DOI 10.1109/ICNIDC.2014.7000265
   Wang Ruikang K., 2014, 2014 Conference on Lasers and Electro-Optics (CLEO) - Laser Science to Photonic Applications
   Yamagishi K, 2017, IEEE T MULTIMEDIA, V19, P1545, DOI 10.1109/TMM.2017.2669859
   Yao J, 2011, LECT NOTES COMPUT SC, V6640, P92, DOI 10.1007/978-3-642-20757-0_8
   Yeganeh H, 2014, IEEE IMAGE PROC, P2007, DOI 10.1109/ICIP.2014.7025402
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P2424, DOI 10.1109/TIP.2017.2681424
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P1275, DOI 10.1109/TIP.2017.2651410
   Zhang W, 2016, IEEE T NEUR NET LEAR, V27, P1266, DOI 10.1109/TNNLS.2015.2461603
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhao Y, 2011, IEEE T CIRC SYST VID, V21, P1890, DOI 10.1109/TCSVT.2011.2157189
   Zinner T., 2010, NEW DIMENSIONS ASSES
NR 51
TC 6
Z9 7
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1475
EP 1488
DI 10.1109/TMM.2017.2769449
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400015
DA 2024-07-18
ER

PT J
AU Song, H
   Wu, XX
   Yu, WN
   Jia, YD
AF Song, Hao
   Wu, Xinxiao
   Yu, Wennan
   Jia, Yunde
TI Extracting Key Segments of Videos for Event Detection by Learning From
   Web Sources
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Event detection; key segments; transfer learning; automatic concept
   discovery
ID DOMAIN ADAPTATION; RECOGNITION; REGULARIZATION; ANNOTATION; IMAGES
AB In this paper, we present a novel approach of extracting the key segments for event detection in unconstrained videos. The key segments are automatically extracted by transferring the knowledge learned from Web images and Web videos to consumer videos. We propose an adaptive latent structural support vector machine model, where the locations of key segments in videos are regarded as latent variables due to the unavailability of the ground truth of key-segment locations in training data. In order to alleviate the time-consuming and labor-expensive manual annotation of huge amounts of training videos, a large number of loosely labeledWeb images as well as videos are collected from the Web sources. Additionally, a limited number of labeled consumer videos are utilized to guarantee the precision of the model. Considering the semantic diversity of key segments, we learn a set of concepts as the semantic description of key segments and explore the temporal information of concepts to capture the sequential relations between the segments. The concepts are automatically discovered by using Web images and videos with their associated tags and description sentences. Comprehensive experiments on the Columbia's consumer video and the TRECVID 2014 Multimedia Event Detection datasets demonstrate that our method outperforms the state-of-the-art methods.
C1 [Song, Hao; Wu, Xinxiao; Yu, Wennan; Jia, Yunde] Beijing Inst Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Song, Hao; Wu, Xinxiao; Yu, Wennan; Jia, Yunde] Beijing Inst Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology; Beijing Institute of Technology
RP Wu, XX; Jia, YD (corresponding author), Beijing Inst Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.; Wu, XX; Jia, YD (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
EM songhao@bit.edu.cn; wuxinxiao@bit.edu.cn; yuwennan@bit.edu.cn;
   jiayunde@bit.edu.cn
OI Wu, Xinxiao/0000-0002-2056-6947
FU Natural Science Foundation of China [61673062, 61472038]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61673062 and 61472038. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sen-Ching Samson Cheung.
CR [Anonymous], INT C MULT RETR ICMR
   [Anonymous], 2014, TRECVID MULTIMEDIA E
   [Anonymous], 2013, P 3 ACM C INT C MULT
   [Anonymous], TRECVID 2009 PAPERS
   Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang X, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P581, DOI 10.1145/2733373.2806218
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Do Trinh-Minh-Tri., 2009, ICML '09 Proceedings of the 26th Annual International Conference on Machine Learning, P265
   Duan LX, 2012, IEEE T NEUR NET LEAR, V23, P504, DOI 10.1109/TNNLS.2011.2178556
   Duan LX, 2012, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2012.6247819
   Habibian A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P17, DOI 10.1145/2647868.2654913
   Habibian A, 2014, COMPUT VIS IMAGE UND, V124, P110, DOI 10.1016/j.cviu.2014.02.003
   Ho Chung Wu, 2008, ACM Transactions on Information Systems, V26
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jia Yangqing, 2014, ARXIV14085093, DOI [10.1145/2647868.2654889, DOI 10.1145/2647868.2654889]
   Jiang Y., 2011, P ACM INT C MULT RET
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588
   Lavee G, 2009, IEEE T SYST MAN CY C, V39, P489, DOI 10.1109/TSMCC.2009.2023380
   Li C., 2012, Cikm, P155, DOI DOI 10.1145/2396761.2396785
   Li WX, 2013, IEEE I CONF COMP VIS, P2728, DOI 10.1109/ICCV.2013.339
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Ma AJ, 2014, INT J COMPUT VISION, V109, P233, DOI 10.1007/s11263-014-0723-7
   Mazloom M., 2015, ARXIV151002899
   Mazloom M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P123, DOI 10.1145/2671188.2749402
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Myatt D. R., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P458
   Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Phan S, 2014, J SIGNAL PROCESS SYS, V74, P19, DOI 10.1007/s11265-013-0825-4
   Sefidgar YS, 2015, COMPUT VIS IMAGE UND, V135, P16, DOI 10.1016/j.cviu.2015.02.012
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Song H, 2017, MULTIMED TOOLS APPL, V76, P6111, DOI 10.1007/s11042-016-3253-1
   Sun C, 2014, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2014.329
   Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808
   WANG H, 2015, PROC AS CONF COMP, V25, P493
   Wang H, 2014, IEEE T MULTIMEDIA, V16, P1282, DOI 10.1109/TMM.2014.2312251
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yan Y, 2015, IEEE T IMAGE PROCESS, V24, P1867, DOI 10.1109/TIP.2015.2413294
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Zhang XS, 2015, IEEE T MULTIMEDIA, V17, P1562, DOI 10.1109/TMM.2015.2449660
NR 45
TC 14
Z9 14
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1088
EP 1100
DI 10.1109/TMM.2017.2763322
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400006
DA 2024-07-18
ER

PT J
AU Xu, K
   Jiang, XH
   Sun, TF
AF Xu, Ke
   Jiang, Xinghao
   Sun, Tanfeng
TI Anomaly Detection Based on Stacked Sparse Coding With Intraframe
   Classification Strategy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anomaly detection; foreground interest points; stacked sparse coding;
   intraframe classification
ID ABNORMAL EVENT DETECTION; LOCALIZATION; REPRESENTATIONS
AB Anomaly detection in videos is still a challenging task among the computer vision community. In this paper, an efficient anomaly detection method based on stacked sparse coding (SSC) with intraframe classification strategy is proposed. Each video is divided into blocks and the Foreground Interest Point (FIP) descriptor is proposed to describe the appearance and motion features for each block. The spatial-temporal features are then encoded with SSC. Specifically, the first stage of SSC encodes the spatial connections among blocks and the second stage of SSC encodes the temporal connections of all frame patches in each block. Finally, an intraframe classification strategy which uses the probabilistic outputs of SVM is proposed to evaluate the abnormality of each block. Contributions of this paper are listed as follows: 1) The FIP descriptor is proposed to describe the features of blocks, which reserves more spatial-temporal information. 2) The SSC encoding method encodes both the spatial and temporal connections of blocks, which makes the features more representative. 3) The intraframe classification strategy keeps the evaluation consistency among blocks and it helps to improve detection performance. The proposed method is examined on four public datasets with different background complexities and resolutions: UCSD Ped1 dataset, UCSD Ped2 dataset, Avenue dataset, and Subway dataset. The results are further compared with previous approaches to confirm the effectiveness and advantages of this method.
C1 [Xu, Ke; Jiang, Xinghao; Sun, Tanfeng] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Xu, K (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
EM l13025816@sjtu.edu.cn; xhjiang@sjtu.edu.cn; tfsun@sjtu.edu.cn
RI Tanfeng, Sun/J-7469-2015
FU Nature Science Foundation of China [61572321, 61572320]
FX This work was supported in part by the Nature Science Foundation of
   China under Grant 61572321 and Grant 61572320.
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   [Anonymous], 2011, Proceedings of the 2011 joint ACM workshop on Modeling and representing events, J-MRE'11, (New York, NY, USA)
   [Anonymous], 2015, PROC CVPR IEEE
   Bao T., 2016, MULTIMED TOOLS APPL, V75, P14617
   Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524
   Brun L, 2014, IEEE T CIRC SYST VID, V24, P1669, DOI 10.1109/TCSVT.2014.2302521
   Chen CZ, 2012, INT J PHOTOENERGY, V2012, DOI 10.1155/2012/768605
   Cong Y, 2013, IEEE T INF FOREN SEC, V8, P1590, DOI 10.1109/TIFS.2013.2272243
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Cosar S, 2017, IEEE T CIRC SYST VID, V27, P683, DOI 10.1109/TCSVT.2016.2589859
   Cui X., 2014, P INT C GRAPH IM PRO
   Tran D, 2014, IEEE T PATTERN ANAL, V36, P404, DOI 10.1109/TPAMI.2013.137
   Feng YC, 2017, NEUROCOMPUTING, V219, P548, DOI 10.1016/j.neucom.2016.09.063
   Han S, 2013, IEEE IMAGE PROC, P151, DOI 10.1109/ICIP.2013.6738032
   Jaechul Kim, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2921, DOI 10.1109/CVPRW.2009.5206569
   Krausz B, 2012, COMPUT VIS IMAGE UND, V116, P307, DOI 10.1016/j.cviu.2011.08.006
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lee DG, 2015, IEEE T CIRC SYST VID, V25, P1612, DOI 10.1109/TCSVT.2015.2395752
   Lee YJ, 2013, IEEE T KNOWL DATA EN, V25, P1460, DOI 10.1109/TKDE.2012.99
   Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Lin HT, 2007, MACH LEARN, V68, P267, DOI 10.1007/s10994-007-5018-6
   Lin Z.C., 2010, 100920105055 ARXIV, V1009, P5055, DOI [DOI 10.1016/J.JSB.2012.10.010, 10.1016/j.jsb.2012.10.010]
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Lu CW, 2014, IEEE T IMAGE PROCESS, V23, P837, DOI 10.1109/TIP.2013.2287602
   MAHADEVAN V, 2010, PROC CVPR IEEE, P1975, DOI DOI 10.1109/CVPR.2010.5539872
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Mo X, 2014, IEEE T CIRC SYST VID, V24, P631, DOI 10.1109/TCSVT.2013.2280061
   Morris BT, 2008, IEEE T CIRC SYST VID, V18, P1114, DOI 10.1109/TCSVT.2008.927109
   Piciarelli C, 2008, IEEE T CIRC SYST VID, V18, P1544, DOI 10.1109/TCSVT.2008.2005599
   Popoola OP, 2012, IEEE T SYST MAN CY C, V42, P865, DOI 10.1109/TSMCC.2011.2178594
   Reddy V., 2011, Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on, P55
   Revathi AR, 2017, SIGNAL IMAGE VIDEO P, V11, P291, DOI 10.1007/s11760-016-0935-0
   Sabokrou Mohammad, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P56, DOI 10.1109/CVPRW.2015.7301284
   Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Sodemann AA, 2012, IEEE T SYST MAN CY C, V42, P1257, DOI 10.1109/TSMCC.2012.2215319
   Song X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2438653.2438670
   Thida M, 2013, IEEE T CYBERNETICS, V43, P2147, DOI 10.1109/TCYB.2013.2242059
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wu JX, 2014, PROC CVPR IEEE, P2577, DOI 10.1109/CVPR.2014.330
   Xiao T, 2015, IEEE SIGNAL PROC LET, V22, P1477, DOI 10.1109/LSP.2015.2410031
   Xiong GG, 2012, NEUROCOMPUTING, V83, P121, DOI 10.1016/j.neucom.2011.12.007
   Xu D, 2014, NEUROCOMPUTING, V143, P144, DOI 10.1016/j.neucom.2014.06.011
   Yi S, 2014, PROC CVPR IEEE, P2219, DOI 10.1109/CVPR.2014.284
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Yuan Y, 2017, IEEE T CYBERNETICS, V47, P3597, DOI 10.1109/TCYB.2016.2572609
   Zhang YH, 2015, IEEE T CIRC SYST VID, V25, P1231, DOI 10.1109/TCSVT.2014.2355711
   Zhou SF, 2016, SIGNAL PROCESS-IMAGE, V47, P358, DOI 10.1016/j.image.2016.06.007
NR 50
TC 37
Z9 39
U1 1
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1062
EP 1074
DI 10.1109/TMM.2018.2818942
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400004
DA 2024-07-18
ER

PT J
AU Kazemi, M
   Iqbal, R
   Shirmohammadi, S
AF Kazemi, Mohammad
   Iqbal, Razib
   Shirmohammadi, Shervin
TI Joint Intra and Multiple Description Coding for Packet Loss Resilient
   Video Transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple description coding; intra coding; error resilient coding; mode
   decision
ID RATE-DISTORTION OPTIMIZATION; OPTIMAL-MODE SELECTION; REDUNDANCY
   ALLOCATION; COMMUNICATION; H.264/AVC
AB Multiple description coding (MDC) is a technique for video transmission over error prone networks where the descriptions are routed over multiple paths. Intra coding such as MDC provides error resiliency but coding in this mode must he decided with care since it degrades the compression ratio. In this paper, we present our investigation results for a new intra coding approach in MDC. We have found that, in MDC streams, the best policy is to encode selective frames as I-frame instead of coding some macroblocks of frames in intra mode. In order to find the most suitable I-frame positions within a given video stream, we developed a cost function based on which intra/inter frame type is decided. The MDC scheme with the proposed intra coding criterion, with and without redundancy optimization, is implemented in the H.264/AVC reference software, JM 16.0. Based on the experimental performance evaluation, we show that our method achieves higher average PSNR compared to the other optimized MDCs found in the literature.
C1 [Kazemi, Mohammad] Univ Isfahan, Dept Elect Engn, Esfahan, Iran.
   [Iqbal, Razib] Mississippi State Univ, Dept Comp Sci, Springfield, MO 65897 USA.
   [Shirmohammadi, Shervin] Univ Ottawa, Sch Elect Engn & Comp Sci, Ottawa, ON K1N 6N5, Canada.
C3 University of Isfahan; Mississippi State University; University of
   Ottawa
RP Kazemi, M (corresponding author), Univ Isfahan, Dept Elect Engn, Esfahan, Iran.
EM m.kazemi@eng.ui.ac.ir; riqbal@missouristate.edu; shervin@eecs.uottawa.ca
RI Shirmohammadi, Shervin/E-6945-2012; Kazemi, Mohammad/G-7733-2017
OI Shirmohammadi, Shervin/0000-0002-3973-4445; Iqbal,
   Razib/0000-0002-9293-2993
CR Alustiza I, 2015, IEEE T SIGNAL PROCES, V63, P3046, DOI 10.1109/TSP.2015.2416676
   [Anonymous], IEEE T CIRCUITS SYST
   Apostolopoulos JG, 2004, IEEE COMMUN MAG, V42, P80, DOI 10.1109/MCOM.2004.1321395
   Bai HH, 2014, IEEE T CIRC SYST VID, V24, P1390, DOI 10.1109/TCSVT.2014.2315770
   Chen J, 2016, MULTIMED TOOLS APPL, V75, P2801, DOI 10.1007/s11042-015-2546-0
   Correia P., 2015, EMERGING RES NETWORK
   Correia P, 2012, IEEE T MULTIMEDIA, V14, P923, DOI 10.1109/TMM.2011.2182184
   Côté G, 1999, SIGNAL PROCESS-IMAGE, V15, P25, DOI 10.1016/S0923-5965(99)00022-3
   Côté G, 2000, IEEE J SEL AREA COMM, V18, P952, DOI 10.1109/49.848249
   Dong M, 2016, J VIS COMMUN IMAGE R, V38, P378, DOI 10.1016/j.jvcir.2016.03.016
   Franchi N, 2005, IEEE T CIRC SYST VID, V15, P321, DOI 10.1109/TCSVT.2004.842606
   Ghahremani S, 2017, MULTIMED TOOLS APPL, V76, P9033, DOI 10.1007/s11042-016-3471-6
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Harmanci O, 2007, IEEE T IMAGE PROCESS, V16, P684, DOI 10.1109/TIP.2006.891047
   Kamnoonwatana N, 2012, IEEE T CIRC SYST VID, V22, P1, DOI 10.1109/TCSVT.2011.2129251
   Kazemi M, 2016, IEEE INT SYM MULTIM, P301, DOI [10.1109/ISM.2016.0066, 10.1109/ISM.2016.39]
   Kazemi M, 2017, IEEE T MULTIMEDIA, V19, P54, DOI 10.1109/TMM.2016.2607342
   Kazemi M, 2015, SIGNAL PROCESS-IMAGE, V36, P95, DOI 10.1016/j.image.2015.06.006
   Kazemi M, 2014, MULTIMEDIA SYST, V20, P283, DOI 10.1007/s00530-013-0319-z
   Lam E., 1999, IEEE T CIRCUITS SYST, V9, P608
   Lee YC, 2004, IEEE T CIRC SYST VID, V14, P122, DOI 10.1109/TCSVT.2003.819182
   Liao YT, 2011, IEEE T MULTIMEDIA, V13, P132, DOI 10.1109/TMM.2010.2089504
   Lin CY, 2011, IEEE T CIRC SYST VID, V21, P589, DOI 10.1109/TCSVT.2011.2129270
   Moon KM, 2011, PROCEEDINGS OF 2011 INTERNATIONAL CONFERENCE ON PUBLIC ADMINISTRATION (7TH), VOL III, P19
   Reibman AR, 2002, IEEE T CIRC SYST VID, V12, P193, DOI 10.1109/76.993440
   Shu HY, 2008, IEEE T MULTIMEDIA, V10, P97, DOI 10.1109/TMM.2007.911300
   Stockhammer T., 2002, PACK VID WORKSH 2002
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tillo T, 2008, IEEE T CIRC SYST VID, V18, P59, DOI 10.1109/TCSVT.2007.913751
   Wan S, 2007, IEEE T IMAGE PROCESS, V16, P1327, DOI 10.1109/TIP.2007.894230
   Wang Y, 2005, P IEEE, V93, P57, DOI 10.1109/JPROC.2004.839618
   Wang Y, 2002, IEEE T CIRC SYST VID, V12, P438, DOI 10.1109/TCSVT.2002.800320
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   Wang Y, 2006, IEEE T CIRC SYST VID, V16, P716, DOI 10.1109/TCSVT.2006.875203
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wiegand T, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P542, DOI 10.1109/ICIP.2001.958171
   Wiegand T, 2000, IEEE J SEL AREA COMM, V18, P1050, DOI 10.1109/49.848255
   Wu DP, 2000, IEEE J SEL AREA COMM, V18, P977, DOI 10.1109/49.848251
   Xu W., 2014, MULTIMED TOOLS APPL, V75, P2347
   Xu YY, 2013, IEEE T CIRC SYST VID, V23, P1523, DOI 10.1109/TCSVT.2013.2249018
   Yen-Chi L., 2000, P 2003 INT C MULT EX, V583, P581
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   Zhang Y, 2007, IEEE T MULTIMEDIA, V9, P445, DOI 10.1109/TMM.2006.887989
NR 43
TC 9
Z9 9
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 781
EP 795
DI 10.1109/TMM.2017.2758578
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000002
DA 2024-07-18
ER

PT J
AU Hernández-Cabronero, M
   Marcellin, MW
   Blanes, I
   Serra-Sagristà, J
AF Hernandez-Cabronero, Miguel
   Marcellin, Michael W.
   Blanes, Ian
   Serra-Sagrista, Joan
TI Lossless Compression of Color Filter Array Mosaic Images With
   Visualization via JPEG 2000
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayer color filter array (CFA); color filter arrays; image compression;
   JPEG 2000
ID DEMOSAICKING; CAMERAS; SENSOR
AB Digital cameras have become ubiquitous for amateur and professional applications. The raw images captured by digital sensors typically take the form of color filter array (CFA) mosaic images, which must be "developed" (via digital signal processing) before they can be viewed. Photographers and scientists often repeat the "development process" using different parameters to obtain images suitable for different purposes. Since the development process is generally not invertible, it is commonly desirable to store the raw (or undeveloped) mosaic images indefinitely. Uncompressed mosaic image file sizes can be more than 30 times larger than those of developed images stored in JPEG format. Thus, data compression is of interest. Several compression methods for mosaic images have been proposed in the literature. However, they all require a custom decompressor followed by development-specific software to generate a displayable image. In this paper, a novel compression pipeline that removes these requirements is proposed. Specifically, mosaic images can be losslessly recovered from the resulting compressed files, and, more significantly, images can be directly viewed (decompressed and developed) using only a JPEG 2000 compliant image viewer. Experiments reveal that the proposed pipeline attains excellent visual quality, while providing compression performance competitive to that of state-of-the-art compression algorithms for mosaic images.
C1 [Hernandez-Cabronero, Miguel; Blanes, Ian; Serra-Sagrista, Joan] Univ Autonoma Barcelona, Bellaterra 08193, Spain.
   [Marcellin, Michael W.] Univ Arizona, Tucson, AZ 85721 USA.
C3 Autonomous University of Barcelona; University of Arizona
RP Hernández-Cabronero, M (corresponding author), Univ Autonoma Barcelona, Bellaterra 08193, Spain.
EM mhernandez@deic.uab.cat; marcellin@ece.arizona.edu; ian.blanes@uab.cat;
   joan.serra@uab.cat
RI Serra-Sagristà, Joan/M-3284-2019; Hernández-Cabronero,
   Miguel/ABH-3319-2020; Serra-Sagrista, Joan/B-2000-2009
OI Serra-Sagristà, Joan/0000-0003-4729-9292; Serra-Sagrista,
   Joan/0000-0003-4729-9292; Hernandez-Cabronero,
   Miguel/0000-0001-9301-4337
FU FEDER; Spanish Government (MINECO); Catalan Government [TIN2015-71126-R,
   TIN2012-38102-C03-03, FPU AP2010-0172, 2014SGR-691]
FX This work was supported in part by the FEDER, in part by the Spanish
   Government (MINECO), and in part by the Catalan Government under Project
   TIN2015-71126-R, Project TIN2012-38102-C03-03, Project FPU AP2010-0172,
   and Project 2014SGR-691. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Lingfen Sun.
   (Corresponding author: Miguel Hernandez-Cabronero.)
CR [Anonymous], 1996, STANDARD DEFAULT COL
   [Anonymous], 2017, KAKADU JPEG2000 SOFT
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   [Anonymous], 2005, 154449 ISOIEC
   [Anonymous], 2012, DIGITAL NEGATIVE DNG
   [Anonymous], 2004, 154442 ISOIEC
   Atsumi E, 2003, P SOC PHOTO-OPT INS, V5017, P254, DOI 10.1117/12.476745
   Battiato S, 2003, IEEE T CONSUM ELECTR, V49, P773, DOI 10.1109/TCE.2003.1261151
   Battiato S., 2003, P IEEE EURASIP WORKS, P1
   Bayer B., 1976, U. S. Patent, Patent No. [3 971 065, 971065]
   Bazhyna A., 2007, INT S SIGN CIRC SYST, V2, P1
   Bazhyna A, 2008, IEEE T CONSUM ELECTR, V54, P1492, DOI 10.1109/TCE.2008.4711192
   Bruna A, 2004, HELS UNIV TECHNOL S, V46, P101
   Chang J, 2013, IEEE T IMAGE PROCESS, V22, P1186, DOI 10.1109/TIP.2012.2228489
   Chen XD, 2015, IEEE T CIRC SYST VID, V25, P1271, DOI 10.1109/TCSVT.2014.2313896
   Chen X, 2014, IEEE T CIRC SYST VID, V24, P255, DOI 10.1109/TCSVT.2013.2255421
   Chung KH, 2008, IEEE T IMAGE PROCESS, V17, P134, DOI 10.1109/TIP.2007.914153
   Coffin D., 2017, DCRAW D COFFINS RAW
   Cuce HI, 2006, IEEE IMAGE PROC, P1141, DOI 10.1109/ICIP.2006.312758
   Duran J, 2014, IEEE T IMAGE PROCESS, V23, P4031, DOI 10.1109/TIP.2014.2341928
   Fan YC, 2013, IEEE SENS J, V13, P2586, DOI 10.1109/JSEN.2013.2256779
   Fang L, 2012, IEEE T MULTIMEDIA, V14, P1359, DOI 10.1109/TMM.2012.2191269
   Hirakawa K, 2005, IEEE T IMAGE PROCESS, V14, P360, DOI 10.1109/TIP.2004.838691
   Image Technology Colour Management-Architecture, 2010, ICC1201012
   ISO/IEC and ITU-T, 2017, HIGH EFF VID COD HEV
   JBIG2, 1993, 11544 ISOIEC
   Jeon G, 2013, IEEE T IMAGE PROCESS, V22, P146, DOI 10.1109/TIP.2012.2214041
   JPEG XT, 2014, 18477 ISOIEC
   JPEG-LS, 1998, 144951 ISOIEC
   Khashabi D, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2359774
   Kim S, 2014, IEEE T CIRC SYST VID, V24, P1040, DOI 10.1109/TCSVT.2014.2302546
   Koh CC, 2003, IEEE T CONSUM ELECTR, V49, P1448, DOI 10.1109/TCE.2003.1261253
   Korneliussen JT, 2014, IEEE T IMAGE PROCESS, V23, P4539, DOI 10.1109/TIP.2014.2350911
   Lee SY, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P482, DOI 10.1109/ICIP.2001.958156
   Li X, 2008, PROC SPIE, V6822, DOI 10.1117/12.766768
   Lian NX, 2006, IEEE T IMAGE PROCESS, V15, P3261, DOI 10.1109/TIP.2006.882024
   Lukac R, 2006, IEEE T CONSUM ELECTR, V52, P299, DOI 10.1109/TCE.2006.1649641
   Monno Y, 2015, IEEE IMAGE PROC, P3861, DOI 10.1109/ICIP.2015.7351528
   Parrein B, 2004, IEEE IMAGE PROC, P521
   Pennebaker W. B., 1993, JPEG: Still image data compression standard
   Siddiqui H, 2016, IEEE IMAGE PROC, P1794, DOI 10.1109/ICIP.2016.7532667
   Toi T, 1999, IEEE T CONSUM ELECTR, V45, P176, DOI 10.1109/30.754434
   TSAI YT, 1991, IEEE T ELECTRON DEV, V38, P1226, DOI 10.1109/16.78401
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie X., 2004, PHYS REV E, P1
   Zhang C, 2016, IEEE T IMAGE PROCESS, V25, P5173, DOI 10.1109/TIP.2016.2601266
   Zhang N, 2006, IEEE T IMAGE PROCESS, V15, P1379, DOI 10.1109/TIP.2005.871116
NR 47
TC 21
Z9 21
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 257
EP 270
DI 10.1109/TMM.2017.2741426
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200001
OA Green Accepted, Green Published
DA 2024-07-18
ER

PT J
AU Hong, HJ
   El-Ganainy, T
   Hsu, CH
   Harras, KA
   Hefeeda, M
AF Hong, Hua-Jun
   El-Ganainy, Tarek
   Hsu, Cheng-Hsin
   Harras, Khaled A.
   Hefeeda, Mohamed
TI Disseminating Multilayer Multimedia Content Over Challenged Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Challenged networks; content distribution; multimedia; mobile devices;
   offline access
ID VIDEO; ADAPTATION; ACCESS
AB Mobile devices are getting increasingly popular all over the world. Mobile users in developing countries, however, rarely have Internet access, which puts them at economic and social disadvantages compared to their counterparts in developed countries. We propose mBridge: A distributed system to disseminate multimedia content to mobile users with intermittent Internet access and opportunistic ad hoc connectivity. By disseminating various multimedia content, such as news reports, notification messages, targeted advertisements, movie trailers, and TV shows, mBridge aims to eliminate the digital divide. We formulate an optimization problem to compute personalized distribution plans for individual mobile users, to maximize the overall user experience under various resource constraints. Our formulation jointly considers the characteristics of multimedia content, mobile users, and intermittent networks. We present an efficient distribution planning algorithm to solve our problem, and we develop several online heuristics to adapt to the system and network dynamics. We implement a prototype system and demonstrate that our algorithm outperforms the existing algorithms by up to 206%, 472%, and 188% in terms of user experience, disk efficiency, and energy efficiency, respectively. In addition, we conduct trace-driven simulations to rigorously evaluate the proposed system in different environments and for large-scale deployments. Our simulation results demonstrate that the proposed algorithm substantially outperforms the closest ones in the literature in all performance measures. We believe that mBridge can allow multimedia content providers to reach out to more mobile users, and mobile users to access multimedia content without always-on Internet access.
C1 [Hong, Hua-Jun; Hsu, Cheng-Hsin] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
   [El-Ganainy, Tarek; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
   [Harras, Khaled A.] Carnegie Mellon Univ, Comp Sci Dept, Doha 24866, Qatar.
C3 National Tsing Hua University; Simon Fraser University; Qatar Foundation
   (QF); Carnegie Mellon University in Qatar
RP Hsu, CH (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
EM hua.j.hong@gmail.com; tarek_elganainy@sfu.ca; cha16@alumni.sfu.ca;
   kharras@qatar.cmu.edu; mhefeeda@sfu.ca
OI Hsu, Cheng-Hsin/0000-0002-8116-2591
FU Natural Sciences and Engineering Research Council of Canada; Qatar
   National Research Fund [NPRP8-519-1-108]; Ministry of Science and
   Technology of Taiwan [105-2221-E-007-088, 106-2221-E-007-101]
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council of Canada, in part by the Qatar National Research Fund
   under Grant [NPRP8-519-1-108], and in part by the Ministry of Science
   and Technology of Taiwan under Grant 105-2221-E-007-088 and Grant
   106-2221-E-007-101. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Liang Zhou.
   (Corresponding author: Cheng-Hsin Hsu.)
CR Abdelkader T, 2016, IEEE NETWORK, V30, P46, DOI 10.1109/MNET.2016.7437024
   Aljarhi A, 2011, PROCEEDINGS OF THE 6TH ACM WORKSHOP ON CHALLENGED NETWORKS (CHANTS '11), P21
   [Anonymous], DTN2
   [Anonymous], P IEEE ANN C PERV CO
   [Anonymous], P IEEE INT C COMP CO
   [Anonymous], P ACM MOBIWAC
   [Anonymous], LETOR4 0 DATASET
   [Anonymous], BOUNTRY WORKERS CHIN
   [Anonymous], IBR DTN
   [Anonymous], WIRELESS COMMUN MOBI
   [Anonymous], CISCO VISUAL NETWORK
   [Anonymous], ITS TIME TAKE CLOSER
   [Anonymous], TOPIAS TERM EXTRACTO
   [Anonymous], SOCIAL DIGITAL MOBIL
   [Anonymous], KDD09 15 ACM SIGKDD
   [Anonymous], 2012, P ACM INT C MULT MM
   [Anonymous], IEEE T MOBILE COMPUT
   [Anonymous], ITU ICT FACTS FIGURE
   [Anonymous], 2000, TECH REP
   [Anonymous], WE ACCELERATE INTERN
   [Anonymous], IUI 2010
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Burges Christopher J. C., 2010, Learning, V11, P81
   Burgess J, 2006, IEEE INFOCOM SER, P1688, DOI 10.1109/infocom.2006.228
   Burns B, 2005, IEEE INFOCOM SER, P398
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P485, DOI 10.1109/TMM.2015.2405343
   Cheng PC, 2010, MOBILE NETW APPL, V15, P61, DOI 10.1007/s11036-009-0181-6
   Cho E., 2011, P 17 ACM SIGKDD INT, P1082, DOI DOI 10.1145/2020408.2020579
   Fall K, 2003, ACM SIGCOMM COMP COM, V33, P27
   Friedman R, 2013, IEEE T MOBILE COMPUT, V12, P1363, DOI 10.1109/TMC.2012.117
   Gao W, 2014, IEEE T MOBILE COMPUT, V13, P611, DOI 10.1109/TMC.2013.33
   Go Y, 2015, IEEE T MULTIMEDIA, V17, P1646, DOI 10.1109/TMM.2015.2451951
   González MC, 2008, NATURE, V453, P779, DOI 10.1038/nature06958
   Greene D., 2006, P 23 INT C MACH LEAR, V148, P377
   Guruprasad R, 2015, IEEE T MULTIMEDIA, V17, P1630, DOI 10.1109/TMM.2015.2436821
   Harras KA, 2009, WIREL COMMUN MOB COM, V9, P21, DOI 10.1002/wcm.653
   Hong HJ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P799, DOI 10.1145/2733373.2807970
   Hsu WJ, 2012, AD HOC NETW, V10, P1586, DOI 10.1016/j.adhoc.2011.06.004
   Hu YC, 2016, IEEE T MULTIMEDIA, V18, P840, DOI 10.1109/TMM.2016.2538721
   Isaacman S., 2011, Proceedings of the 20th international conference companion on World wide web, P473
   Jain S, 2013, ACM SIGCOMM COMP COM, V43, P3, DOI 10.1145/2534169.2486019
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Kang SS, 2004, SECOND IEEE ANNUAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS, PROCEEDINGS, P197
   Keller Lorenzo., 2012, ACM MOBISYS, P57
   Li Zhenhui, 2010, P KDD10 P 16 ACM SIG, P1099, DOI DOI 10.1145/1835804.1835942
   Lymberopoulos D, 2012, ASPLOS XVII: SEVENTEENTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P1
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   Mokhtarian K, 2013, IEEE T MULTIMEDIA, V15, P181, DOI 10.1109/TMM.2012.2225042
   Mota VFS, 2014, COMPUT COMMUN, V48, P5, DOI 10.1016/j.comcom.2014.03.019
   Mtibaa A, 2013, IEEE INT CONF MOB, P533, DOI 10.1109/MASS.2013.20
   Mtibaa A, 2013, COMPUT COMMUN, V36, P180, DOI 10.1016/j.comcom.2012.08.019
   Do NM, 2014, IEEE T MOBILE COMPUT, V13, P274, DOI 10.1109/TMC.2012.246
   Do NM, 2012, SYM REL DIST SYST, P352, DOI 10.1109/SRDS.2012.36
   Nimmagadda Y, 2010, IEEE T MULTIMEDIA, V12, P650, DOI 10.1109/TMM.2010.2052024
   Ntareme H., 2011, Proceedings of the 3rd Extreme Conference on Communication: The Amazon Expedition pp, P14
   Pagani E, 2015, AD HOC NETW, V25, P314, DOI 10.1016/j.adhoc.2014.07.005
   Pietiläinen AK, 2009, 2ND ACM SIGCOMM WORKSHOP ON ONLINE SOCIAL NETWORKS (WOSN 09), P49
   Piorkowski M., 2009, PROC IEEE INT COMMUN, P1
   Qian Feng., 2012, P 10 INT C MOBILE SY, P127, DOI DOI 10.1145/2307636.2307649
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shen YY, 2016, IEEE WIREL COMMUN, V23, P46, DOI 10.1109/MWC.2016.7462484
   Shen YY, 2016, IEEE T WIREL COMMUN, V15, P1575, DOI 10.1109/TWC.2015.2492967
   Wang JJ, 2016, IEEE WIREL COMMUN LE, V5, P300, DOI 10.1109/LWC.2016.2547395
   Wang ST, 2015, IEEE CONF COMPUT, P624, DOI 10.1109/INFCOMW.2015.7179455
   Wang Tiankai, 2012, Perspect Health Inf Manag, V9, P1
   Wang Y, 2007, IEEE T MULTIMEDIA, V9, P213, DOI 10.1109/TMM.2006.886253
   Wu XM, 2015, IEEE T MULTIMEDIA, V17, P2345, DOI 10.1109/TMM.2015.2476662
   Xiang Z, 2004, IEEE T MULTIMEDIA, V6, P343, DOI 10.1109/TMM.2003.822819
   Zhang AQ, 2016, IEEE T EMERG TOP COM, V4, P556, DOI 10.1109/TETC.2015.2430816
   Zhang G, 2015, IEEE T MULTIMEDIA, V17, P229, DOI 10.1109/TMM.2014.2383617
   Zhang YF, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P265, DOI 10.1145/2493432.2493484
   Zhao Wenrui., 2004, MOBIHOC 04, P187, DOI DOI 10.1145/989459.989483
   Zheng Y, 2008, PROCEEDINGS OF THE 10TH INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING (UBICOMP 2008), P312, DOI 10.1145/1409635.1409677
   Zhou L, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2886776
NR 74
TC 7
Z9 7
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 345
EP 360
DI 10.1109/TMM.2017.2744183
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200008
DA 2024-07-18
ER

PT J
AU Hu, HM
   Wu, JW
   Li, B
   Guo, Q
   Zheng, J
AF Hu, Hai-Miao
   Wu, Jiawei
   Li, Bo
   Guo, Qiang
   Zheng, Jin
TI An Adaptive Fusion Algorithm for Visible and Infrared Videos Based on
   Entropy and the Cumulative Distribution of Gray Levels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive video fusion; cumulative distribution function; entropy;
   infrared video; visible video
ID MEDICAL IMAGE FUSION; PERFORMANCE; TRANSFORM
AB Visible videos captured under different weather conditions may exhibit different characteristics, and thermal infrared videos are easily affected by ambient temperature variations; this sensitivity to environmental conditions makes the fusion of visible and thermal infrared videos a challenge. This paper proposes an adaptive fusion algorithm for visible and infrared videos, and uses cumulative distribution of gray levels and the entropy to adaptively retain infrared-hot targets and visible textures. The original visible and infrared frames are decomposed into two layers, namely, the base layer and the detail layer. The guided filter is employed to decompose frames due to its high efficiency. Two weight maps, one for the infrared base layer and one for the visible base layer, are adaptively generated based on the cumulative distribution of gray levels and the entropy, respectively. The visible base layer and the infrared base layer are fused based on their weight maps. The final fusion result is obtained by combining the fused base layer with the visible detail layer. Experimental results demonstrate that the proposed algorithm can achieve better fusion results compared with state-of-the-art methods.
C1 [Hu, Hai-Miao; Wu, Jiawei; Li, Bo; Guo, Qiang; Zheng, Jin] Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
   [Hu, Hai-Miao; Li, Bo; Zheng, Jin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University; Beihang University
RP Li, B (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.; Li, B (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM frank0139@163.com; wujiawei5837464@sina.com; boli@buaa.edu.cn;
   1263836618@qq.com; zhengjin@cse.buaa.edu.cn
RI Li, bo/IWL-9318-2023; Li, Bo/AAA-8968-2020
OI Li, Bo/0000-0002-7294-6888
FU National Key Research and Development Plan Grant [2016YFC0801003];
   National Natural Science Foundation of China [61370121, 61421003]
FX This work was supported in part by the National Key Research and
   Development Plan under Grant 2016YFC0801003 and in part by the National
   Natural Science Foundation of China under Grant 61370121 and Grant
   61421003. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Abdulmotaleb El
   Saddik. (Hai-Miao Hu and Jiawei Wu contributed equally to this work.)
   (Corresponding author: Bo Li.)
CR [Anonymous], P DIG PHOT
   Bavirisetti DP, 2016, IEEE SENS J, V16, P203, DOI 10.1109/JSEN.2015.2478655
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Boccignone G, 2001, IEEE T PATTERN ANAL, V23, P207, DOI 10.1109/34.908970
   Brink AD, 1996, PATTERN RECOGN LETT, V17, P29, DOI 10.1016/0167-8655(95)00096-8
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Davis JW, 2007, COMPUT VIS IMAGE UND, V106, P162, DOI 10.1016/j.cviu.2006.06.010
   Desale RP, 2013, INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, IMAGE PROCESSING AND PATTERN RECOGNITION (ICSIPR 2013), P66, DOI 10.1109/ICSIPR.2013.6497960
   Ding YY, 2011, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2011.5995445
   Du J, 2016, NEUROCOMPUTING, V194, P326, DOI 10.1016/j.neucom.2016.02.047
   Gade R, 2014, MACH VISION APPL, V25, P245, DOI 10.1007/s00138-013-0570-5
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu HM, 2017, NEUROCOMPUTING, V266, P361, DOI 10.1016/j.neucom.2017.05.052
   Ji Wang, 2016, 2016 Annual Conference on Information Science and Systems (CISS), P401, DOI 10.1109/CISS.2016.7460536
   Kordelas GA, 2016, IEEE T MULTIMEDIA, V18, P155, DOI 10.1109/TMM.2015.2505905
   Lewis J. J., 2006, ARCHITECTURE URBANIS
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Ma Y, 2016, NEUROCOMPUTING, V202, P12, DOI 10.1016/j.neucom.2016.03.009
   Mahyari AG, 2009, ICDIP 2009: INTERNATIONAL CONFERENCE ON DIGITAL IMAGE PROCESSING, PROCEEDINGS, P351, DOI 10.1109/ICDIP.2009.67
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Shen R, 2011, IEEE T IMAGE PROCESS, V20, P3634, DOI 10.1109/TIP.2011.2150235
   Shibata T, 2015, IEEE IMAGE PROC, P1, DOI 10.1109/ICIP.2015.7350747
   Toet A., 2007, ADA493104 TNO DEF SE, V104
   Wan T, 2009, IEEE T MULTIMEDIA, V11, P624, DOI 10.1109/TMM.2009.2017640
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu L, 2015, IET IMAGE PROCESS, V9, P318, DOI 10.1049/iet-ipr.2014.0245
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yang L, 2008, NEUROCOMPUTING, V72, P203, DOI 10.1016/j.neucom.2008.02.025
   Zhang Q, 2013, SIGNAL PROCESS, V93, P2485, DOI 10.1016/j.sigpro.2013.03.018
   Zhang Q, 2012, OPT COMMUN, V285, P3032, DOI 10.1016/j.optcom.2012.02.064
NR 30
TC 69
Z9 72
U1 5
U2 53
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2706
EP 2719
DI 10.1109/TMM.2017.2711422
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200006
DA 2024-07-18
ER

PT J
AU Bentaleb, A
   Begen, AC
   Zimmermann, R
   Harous, S
AF Bentaleb, Abdelhak
   Begen, Ali C.
   Zimmermann, Roger
   Harous, Saad
TI SDNHAS: An SDN-Enabled Architecture to Optimize QoE in HTTP Adaptive
   Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bitrate adaptation logic; convex optimization; DASH; fastMPC; HTTP
   adaptive streaming (HAS); instability; OpenFlow; quality of experience
   (QoE); reinforcement learning; scalability; software defined networking
   (SDN); streaming architecture; unfairness; underutilization
ID RATE ADAPTATION; QUALITY; FAIRNESS
AB HTTP adaptive streaming (HAS) is receiving much attention from both industry and academia as it has become the de facto approach to stream media content over the Internet. Recently, we proposed a streaming architecture called SDNDASH [1] to address HAS scalability issues including video instability, quality of experience (QoE) unfairness, and network resource underutilization, while maximizing per player QoE. While SDNDASH was a significant step forward, there were three unresolved limitations: 1) it did not scale well when the number of HAS players increased; 2) it generated communication overhead; and 3) it did not address client heterogeneity. These limitations could result in suboptimal decisions that led to viewer dissatisfaction. To that effect, we propose an enhanced intelligent streaming architecture, called SDNHAS, which leverages software defined networking (SDN) capabilities of assisting HAS players in making better adaptation decisions. This architecture accommodates large-scale deployments through a cluster-based mechanism, reduces communication overhead between the HAS players and SDN core, and allocates the network resources effectively in the presence of short-and long-term changes in the network.
C1 [Bentaleb, Abdelhak; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Begen, Ali C.] Ozyegin Univ, TR-34794 Istanbul, Turkey.
   [Begen, Ali C.] Networked Media, TR-34794 Istanbul, Turkey.
   [Harous, Saad] United Arab Emirates Univ, Coll Informat Technol, Al Ain 15551, U Arab Emirates.
C3 National University of Singapore; Ozyegin University; United Arab
   Emirates University
RP Bentaleb, A (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
EM bentaleb@comp.nus.edu.sg; rogerz@comp.nus.edu.sg
RI Begen, Ali C./R-5897-2016; Bentaleb, Abdelhak/ABI-3704-2020; Zimmermann,
   Roger/D-7944-2015; Harous, Saad/AAU-6859-2020
OI Begen, Ali C./0000-0002-0835-3017; Zimmermann,
   Roger/0000-0002-7410-2590; Harous, Saad/0000-0001-6524-7352
FU National Natural Science Foundation of China [61472266]; National
   University of Singapore (Suzhou) Research Institute; Turk
   Telekomunikasyon A.S,
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61472266, in part by the National
   University of Singapore (Suzhou) Research Institute, and in part by Turk
   Telekomunikasyon A.S,. The guest editor coordinating the review of this
   manuscript and approving it for publication was Dr. Mahbub Hassan.
   (Corresponding author: Abdelhak Bentaleb.)
CR [Anonymous], P 7 INT C MULT SYST
   [Anonymous], 1984, TR301
   [Anonymous], IPERF TCP UDP BANDWI
   [Anonymous], DEEP PACKET INSPECTI
   [Anonymous], 2015, 12 USENIX S NETW SYS
   [Anonymous], LIBCURL THE MULTIPRO
   [Anonymous], CISC VIS NETW IND FO
   [Anonymous], PEG DASH VS COMM PLA
   [Anonymous], ZETTABYTE ERA TRENDS
   [Anonymous], 1998, REINFORCEMENT LEARNI
   [Anonymous], CISC VIS NETW IND GL
   [Anonymous], MININET
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Carbone M, 2010, ACM SIGCOMM COMP COM, V40, P13, DOI 10.1145/1764873.1764876
   Claeys M, 2014, CONNECT SCI, V26, DOI 10.1080/09540091.2014.885273
   Dimopoulos G., 2016, P 2016 INT MEAS C SA, P513, DOI DOI 10.1145/2987443.2987459
   Duanmu ZF, 2017, IEEE J-STSP, V11, P154, DOI 10.1109/JSTSP.2016.2608329
   Eckert Marcus, 2013, Advances in Communication Networking. 19th EUNICE/IFIP WG 6.6 International Workshop. Proceedings: LNCS 8115, P112, DOI 10.1007/978-3-642-40552-5_11
   Feng Z., 2004, AAAI-04 Workshop on Learning and Planning in Markov Processes-Advances and Challenges, P7
   Fiedler M, 2010, IEEE NETWORK, V24, P36, DOI 10.1109/MNET.2010.5430142
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   Heorhiadi V, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P223
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Jiang JC, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P137
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Juluri P, 2015, IEEE INT CONF COMM, P1765, DOI 10.1109/ICCW.2015.7247436
   Kreutz D, 2015, P IEEE, V103, P14, DOI 10.1109/JPROC.2014.2371999
   Lederer S., 2012, P 3 MULT SYST C, P89
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liu X, 2012, ACM SIGCOMM COMP COM, V42, P359, DOI 10.1145/2377677.2377752
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   Mu M, 2016, IEEE J SEL AREA COMM, V34, P2168, DOI 10.1109/JSAC.2016.2577318
   Petrangeli S, 2016, INT J NETW MANAG, V26, P8, DOI 10.1002/nem.1931
   Powell WB, 2009, NAV RES LOG, V56, P239, DOI 10.1002/nav.20347
   Rainer B, 2017, IEEE T MULTIMEDIA, V19, P849, DOI 10.1109/TMM.2016.2629761
   Rehman A, 2015, PROC SPIE, V9394, DOI 10.1117/12.2077917
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Sherry J, 2015, ACM SIGCOMM COMP COM, V45, P213, DOI 10.1145/2829988.2787502
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Tavakoli S, 2016, IEEE J SEL AREA COMM, V34, P2141, DOI 10.1109/JSAC.2016.2577361
   Thomas E., 2015, ENHANCING MPEG DASH
   Tokic M, 2011, LECT NOTES ARTIF INT, V7006, P335, DOI 10.1007/978-3-642-24455-1_33
   Wang Y, 2010, IEEE T CONTR SYST T, V18, P267, DOI 10.1109/TCST.2009.2017934
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zhou W, 2014, 2014 28TH INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION NETWORKING AND APPLICATIONS WORKSHOPS (WAINA), P358, DOI 10.1109/WAINA.2014.153
NR 49
TC 83
Z9 85
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2136
EP 2151
DI 10.1109/TMM.2017.2733344
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600002
DA 2024-07-18
ER

PT J
AU Dou, H
   Ming, DL
   Yang, Z
   Pan, ZH
   Li, YS
   Tian, JW
AF Dou, Hao
   Ming, Delie
   Yang, Zhi
   Pan, Zhihong
   Li, Yansheng
   Tian, Jinwen
TI Object-Based Visual Saliency via Laplacian Regularized Kernel Regression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel regression; Laplacian regularized kernel regression (LKR);
   salient object detection; visual saliency
ID ATTENTION; MODEL
AB Saliency object detection has been a very active research topic recently, due to its extensive applications in image compression, scene understanding, image retrieval, and so forth. The overwhelming majority of existing computational models are designed based on computer vision techniques by using a lot of image cues and priors. In fact, salient object detection is derived from the biological perceptual mechanism, and biological evidence shows that the object-based saliency stems from the spread of the spatial attention. Inspired by this, we attempt to utilize the emerging spread mechanism of object attention to construct a new computational model. A novel Laplacian regularized kernel regression diffusion model is proposed to fulfill the spread process. The proposed diffusion model, which is able to fully capture both global and local structures of the image, thereby allows for effective propagation of spatial attention with visual grouping cues, yielding a well-structured object-based saliency map. Experimental results demonstrate that our method can achieve encouraging performance in comparison with the state-of-the-art methods.
C1 [Dou, Hao; Ming, Delie; Pan, Zhihong; Tian, Jinwen] Huazhong Univ Sci & Technol, Sch Automat, Wuhan 430074, Hubei, Peoples R China.
   [Yang, Zhi] Hubei Univ Technol, Sch Comp Sci, Wuhan 430068, Hubei, Peoples R China.
   [Li, Yansheng] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Hubei, Peoples R China.
C3 Huazhong University of Science & Technology; Hubei University of
   Technology; Wuhan University
RP Ming, DL (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Wuhan 430074, Hubei, Peoples R China.
EM douhao@hust.edu.cn; mingdelie@hust.edu.cn; zyang631@163.com;
   zpan@hust.edu.cn; yansheng.li@whu.edu.cn; jwtian@hust.edu.cn
RI Li, Yansheng/AAU-6392-2021
OI Li, Yansheng/0000-0001-8203-1246
FU National Natural Science Foundation of China [41601352, 61371339]; China
   Postdoctoral Science Foundation [2016M590716]; Fundamental Research
   Funds for the Central Universities [2042016KF0054]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 41601352 and Grant 61371339, in part by
   the China Postdoctoral Science Foundation under Grant 2016M590716, and
   in part by the Fundamental Research Funds for the Central Universities
   under Grant 2042016KF0054. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Ivan Bajic.
   (Corresponding author: Delie Ming.)
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Ando R. K., 2007, Advances in Neural Information Processing Systems, P25
   [Anonymous], 2005, Advances in neural information processing systems, DOI DOI 10.5555/2976248.2976268
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Borji A., 2014, CoRR, Vabs/1411.5878
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Bose NK, 2006, IEEE T IMAGE PROCESS, V15, P2239, DOI 10.1109/TIP.2006.877406
   Chen Z, 2012, ATTEN PERCEPT PSYCHO, V74, P784, DOI 10.3758/s13414-012-0322-z
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   DUNCAN J, 1984, J EXP PSYCHOL GEN, V113, P501, DOI 10.1037/0096-3445.113.4.501
   Elad M, 2002, IEEE T IMAGE PROCESS, V11, P1141, DOI 10.1109/TIP.2002.801126
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   Hardle, 1990, APPL NONPARAMETRIC R
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He XF, 2011, IEEE T KNOWL DATA EN, V23, P1406, DOI 10.1109/TKDE.2010.259
   Hou Q., 2016, CORR
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P, 2015, IEEE I CONF COMP VIS, P217, DOI 10.1109/ICCV.2015.33
   Judd T., 2012, A benchmark of computational models of saliency to predict human fixations
   Knutsson H., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P515, DOI 10.1109/CVPR.1993.341081
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li YS, 2016, INFORM SCIENCES, V369, P548, DOI 10.1016/j.ins.2016.07.042
   Li YS, 2016, IEEE GEOSCI REMOTE S, V13, P157, DOI 10.1109/LGRS.2015.2503142
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu RS, 2014, PROC CVPR IEEE, P3866, DOI 10.1109/CVPR.2014.494
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z, 2012, IEEE T MULTIMEDIA, V14, P1275, DOI 10.1109/TMM.2012.2190385
   Lu HC, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2524198
   Lu S, 2014, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2014.357
   Ma JY, 2015, IEEE T GEOSCI REMOTE, V53, P6469, DOI 10.1109/TGRS.2015.2441954
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Pham TQ, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/83268
   Poort J, 2012, NEURON, V75, P143, DOI 10.1016/j.neuron.2012.04.032
   Roelfsema PR, 1998, NATURE, V395, P376, DOI 10.1038/26475
   Scholl BJ, 2001, COGNITION, V80, P1, DOI 10.1016/S0010-0277(00)00152-9
   Seo HJ, 2010, IEEE T PATTERN ANAL, V32, P1688, DOI 10.1109/TPAMI.2009.153
   Shen CY, 2015, IEEE T MULTIMEDIA, V17, P2084, DOI 10.1109/TMM.2015.2483370
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Sun YR, 2003, ARTIF INTELL, V146, P77, DOI 10.1016/S0004-3702(02)00399-5
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Tan YH, 2016, J OPT SOC AM A, V33, P887, DOI 10.1364/JOSAA.33.000887
   Tang JH, 2009, SIGNAL PROCESS, V89, P2313, DOI 10.1016/j.sigpro.2009.01.020
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wand M.P., 1994, KERNEL SMOOTHING
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yanulevskaya V, 2013, J VISION, V13, DOI 10.1167/13.13.27
   Yee P., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P597, DOI 10.1109/ICASSP.1993.319189
   Yu JG, 2016, IEEE T MULTIMEDIA, V18, P273, DOI 10.1109/TMM.2015.2505908
   Yu JG, 2014, IEEE T CYBERNETICS, V44, P1661, DOI 10.1109/TCYB.2013.2292054
   Zhang HC, 2013, IEEE T CYBERNETICS, V43, P1035, DOI 10.1109/TSMCB.2012.2222375
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhou D., 2004, P ICML WORKSH STAT R, P132
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 75
TC 14
Z9 14
U1 6
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1718
EP 1729
DI 10.1109/TMM.2017.2689327
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400003
DA 2024-07-18
ER

PT J
AU Yao, R
   Xia, SX
   Zhang, Z
   Zhang, YN
AF Yao, Rui
   Xia, Shixiong
   Zhang, Zhen
   Zhang, Yanning
TI Real-Time Correlation Filter Tracking by Efficient Dense Belief
   Propagation With Structure Preserving
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE correlation filter; efficient inference; patch based tracking; Visual
   object tracking
ID VISUAL TRACKING; OBJECT TRACKING; MODEL
AB Patch-based models that combine local image features or regions into loose geometric assemblies are a powerful paradigm for visual object tracking, and they present favorable properties such as robustness to partial occlusion, deformation, and the ability to address viewpoint changes. However, effectively exploiting the spatial-temporal confidence scores of each patch to construct a robust tracker while ensuring a low computational cost with dense discrete search remains a challenging problem. In this paper, we propose a unified Markov random field (MRF) model that can effectively capture spatio-temporal intrapatch relations and occlusion priors to enhance the tracking performance, and we derive a highly efficient dense belief propagation for inference of the proposed MRF model. We propose a tracker that models the tracking object with a constellation topology (i.e., a global object and several local patches), where the graph structure model describes the pairwise spatial structure and the image observation model corresponding to the correlation filter with occlusion handlingmeasuring the appearance similarity. Furthermore, these two models are updated online by exploiting the flexibility of local patches. Extensive experimental results on single-and multipleobject tracking show that the proposed algorithm performs favorably against state-of-the-art methods and runs in real time.
C1 [Yao, Rui; Xia, Shixiong] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
   [Zhang, Zhen; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
C3 China University of Mining & Technology; Northwestern Polytechnical
   University
RP Yao, R (corresponding author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
EM ruiyao@cumt.edu.cn; xiasx@cumt.edu.cn; zhenzhang@mail.nwpu.edu.cn;
   ynzhang@nwpu.edu.cn
RI Zhang, Zhen/AAV-3609-2020
OI Zhang, Zhen/0000-0003-2805-4396; Yao, Rui/0000-0003-2734-915X; Zhang,
   Zhen/0000-0003-3409-6787
FU National Natural Science Foundation of China [61402483]; China
   Postdoctoral Science Foundation [2016T90524]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61402483, and in part by the China
   Postdoctoral Science Foundation under Grant 2016T90524. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Shu-Ching Chen. (Corresponding author: Rui Yao.)
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], P EUR C COMPUT VIS
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cai ZW, 2014, IEEE T IMAGE PROCESS, V23, P5497, DOI 10.1109/TIP.2014.2364919
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Grabner H., 2006, BMVC, P47
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong S., 2013, CORR
   Hua G, 2004, PROC CVPR IEEE, P826
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Zdenek, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2756, DOI 10.1109/ICPR.2010.675
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kim DW, 2012, PROCEEDINGS OF THE ASME/STLE INTERNATIONAL JOINT TRIBOLOGY CONFERENCE - 2011, P203
   Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Kwon J, 2009, PROC CVPR IEEE, P1208, DOI 10.1109/CVPRW.2009.5206502
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Lin WC, 2007, IEEE T PATTERN ANAL, V29, P777, DOI 10.1109/TPAMI.2007.1053
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Nebehay G, 2015, PROC CVPR IEEE, P2784, DOI 10.1109/CVPR.2015.7298895
   Nejhum S.M. Shahed., 2008, Proceedings IEEE Conference on Computer Vision and Pattern Recognition, P1
   Park M., 2008, IEEE C COMPUTER VISI, P1
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Shu G, 2012, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2012.6247879
   Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang N., 2013, Advances in Neural Information Processing Systems, P809
   Wang NY, 2015, IEEE I CONF COMP VIS, P3101, DOI 10.1109/ICCV.2015.355
   Wang XY, 2010, LECT NOTES COMPUT SC, V6313, P200
   Wu MJ, 2010, COMPUT ELECTR ENG, V36, P927, DOI 10.1016/j.compeleceng.2009.12.013
   Wu MJ, 2010, SIGNAL PROCESS, V90, P1518, DOI 10.1016/j.sigpro.2009.10.023
   Wu Y, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1094, DOI 10.1109/ICCV.2003.1238471
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yang M, 2009, IEEE T PATTERN ANAL, V31, P1195, DOI 10.1109/TPAMI.2008.146
   Yao R, 2012, LECT NOTES COMPUT SC, V7574, P158, DOI 10.1007/978-3-642-33712-3_12
   Yao R, 2013, PROC CVPR IEEE, P2363, DOI 10.1109/CVPR.2013.306
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yuan Y, 2015, IEEE T MULTIMEDIA, V17, P1125, DOI 10.1109/TMM.2015.2440996
   Zhang K., 2012, P 12 EUR C COMP VIS, P236
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang L, 2014, IEEE T PATTERN ANAL, V36, P756, DOI 10.1109/TPAMI.2013.221
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhang TZ, 2014, PROC CVPR IEEE, P1258, DOI 10.1109/CVPR.2014.164
   Zhu GB, 2015, IEEE T IMAGE PROCESS, V24, P5140, DOI 10.1109/TIP.2015.2479460
NR 59
TC 35
Z9 39
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 772
EP 784
DI 10.1109/TMM.2016.2631727
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500009
DA 2024-07-18
ER

PT J
AU Huang, C
   He, ZH
   Cao, GT
   Cao, WM
AF Huang, Chen
   He, Zhihai
   Cao, Guitao
   Cao, Wenming
TI Task-Driven Progressive Part Localization for Fine-Grained Object
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; deformable part-based model; fine-grained recognition;
   regional convolutional neural network; spatial pyramid pooling
ID CATEGORIZATION; NETWORKS
AB The problem of fine-grained object recognition is very challenging due to the subtle visual differences between different object categories. In this paper, we propose a task-driven progressive part localization (TPPL) approach for fine-grained object recognition. Most existing methods follow a two-step approach that first detects salient object parts to suppress the interference from background scenes and then classifies objects based on features extracted from these regions. The part detector and object classifier are often independently designed and trained. In this paper, our major finding is that the part detector should be jointly designed and progressively refined with the object classifier so that the detected regions can provide the most distinctive features for final object recognition. Specifically, we develop a part-based SPP-net (Part-SPP) as our baseline part detector. We then establish a TPPL framework, which takes the predicted boxes of Part-SPP as an initial guess, and then examines new regions in the neighborhood using a particle swarm optimization approach, searching for more discriminative image regions to maximize the objective function and the recognition performance. This procedure is performed in an iterative manner to progressively improve the joint part detection and object classification performance. Experimental results on the Caltech-UCSD-200-2011 dataset demonstrate that our method outperforms state-of-the-art fine-grained categorization methods both in part localization and classification, even without requiring a bounding box during testing.
C1 [Huang, Chen; He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Cao, Guitao] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
   [Cao, Wenming] Shenzhen Univ, Coll Informat Engn, Shenzhen, Peoples R China.
C3 University of Missouri System; University of Missouri Columbia; East
   China Normal University; Shenzhen University
RP Cao, GT (corresponding author), East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.; Cao, WM (corresponding author), Shenzhen Univ, Coll Informat Engn, Shenzhen, Peoples R China.
EM chn69@mail.missouri.edu; hezhi@missouri.edu; gtcao@sei.ecnu.edu.cn;
   caom@shenzhenu.edu.cn
RI Huang, Chen/AAY-4934-2020; He, Zhihai/A-5885-2019
OI Huang, Chen/0000-0002-9912-9298
FU National Science Foundation [CyberSEES-1539389]
FX This work was supported in part by the National Science Foundation under
   Grant CyberSEES-1539389. (Corresponding authors: Guitao Cao and Wenming
   Cao.)
CR Angelova A, 2014, IEEE WINT CONF APPL, P532, DOI 10.1109/WACV.2014.6836056
   [Anonymous], 2014, P BRIT MACH VIS C, DOI 10.5244/C.28.87
   [Anonymous], 2013, Tech. rep.
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2011, TECH REP
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P INT C MANCH LEARN
   Azizpour H, 2012, LECT NOTES COMPUT SC, V7572, P836, DOI 10.1007/978-3-642-33718-5_60
   Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413
   Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47
   Chen G, 2014, PROC CVPR IEEE, P3598, DOI 10.1109/CVPR.2014.460
   Chen G, 2015, IEEE WINT CONF APPL, P860, DOI 10.1109/WACV.2015.119
   Farrell R, 2011, IEEE I CONF COMP VIS, P161, DOI 10.1109/ICCV.2011.6126238
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215
   Göring C, 2014, PROC CVPR IEEE, P2489, DOI 10.1109/CVPR.2014.319
   Guo ZY, 2013, IEEE T MULTIMEDIA, V15, P621, DOI 10.1109/TMM.2012.2234729
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Huang C. W., 2016, 2016 International Conference of Asian Union of Magnetics Societies (ICAUMS), DOI 10.1109/ICAUMS.2016.8479951
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3_36
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu JX, 2012, LECT NOTES COMPUT SC, V7572, P172, DOI 10.1007/978-3-642-33718-5_13
   Martínez-Muñoz G, 2009, PROC CVPR IEEE, P549, DOI 10.1109/CVPRW.2009.5206574
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Parkhi OM, 2011, IEEE I CONF COMP VIS, P1427, DOI 10.1109/ICCV.2011.6126398
   Russakovsky O, 2013, IEEE I CONF COMP VIS, P2064, DOI 10.1109/ICCV.2013.258
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vedaldi A, 2014, PROC CVPR IEEE, P3622, DOI 10.1109/CVPR.2014.463
   Wah C, 2011, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2011.6126539
   Wu ZF, 2015, IEEE T MULTIMEDIA, V17, P1960, DOI 10.1109/TMM.2015.2477681
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Xie LX, 2013, IEEE I CONF COMP VIS, P1641, DOI 10.1109/ICCV.2013.206
   Yao BP, 2012, PROC CVPR IEEE, P3466, DOI 10.1109/CVPR.2012.6248088
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang N, 2013, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2013.96
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang N, 2012, PROC CVPR IEEE, P3665, DOI 10.1109/CVPR.2012.6248364
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
NR 51
TC 28
Z9 28
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2372
EP 2383
DI 10.1109/TMM.2016.2602060
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200005
OA hybrid
DA 2024-07-18
ER

PT J
AU Jiang, YG
   Wang, JJ
   Wang, Q
   Liu, W
   Ngo, CW
AF Jiang, Yu-Gang
   Wang, Jiajun
   Wang, Qiang
   Liu, Wei
   Ngo, Chong-Wah
TI Hierarchical Visualization of Video Search Results for Topic-Based
   Browsing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Search result visualization; video search; hierarchical structure;
   visual analysis
AB Existing video search engines return a ranked list of videos for each user query, which is not convenient for browsing the results of query topics that have multiple facets, such as the "early life," "personal life," and "presidency" of a query "Barack Obama." Organizing video search results into semantically structured hierarchies with nodes covering different topic facets can significantly improve the browsing efficiency for such queries. In this paper, we introduce a hierarchical visualization approach for video search result browsing, which can help users quickly understand the multiple facets of a query topic in a very well-organized manner. Given a query, our approach starts from the hierarchy of its textual descriptions normally available onWikipedia and then adjusts the hierarchical structure by analyzing the video information to reflect the topic structure of the search result. After that, a simple optimization problem is formulated to perform the video-to-node association considering three important criteria. Furthermore, additional topic facets are mined to complement the contents of the existing semantic hierarchies. A large YouTube video dataset is constructed to evaluate our approach both quantitatively and qualitatively. A demo system is also developed for users to interact with the proposed browsing approach.
C1 [Jiang, Yu-Gang; Wang, Jiajun; Wang, Qiang] Fudan Univ, Sch Comp Sci, Shanghai 201203, Peoples R China.
   [Liu, Wei] Tencent Inc, Shenzhen 518057, Peoples R China.
   [Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 Fudan University; Tencent; City University of Hong Kong
RP Jiang, YG (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 201203, Peoples R China.
EM ygj@fudan.edu.cn; jiajunwang13@fudan.edu.cn; qiangwang14@fudan.edu.cn;
   wliu@ee.columbia.edu; cscwngo@cityu.edu.hk
RI Liu, Wei/L-1951-2019
OI Liu, Wei/0000-0002-3865-8145
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 11210514]; NSF China [61572134, U1509206]; STCSM, Shanghai,
   China [16QA1400500]
FX This work was supported in part by the Research Grants Council of the
   Hong Kong Special Administrative Region, China, under Grant CityU
   11210514, in part by the NSF China under Grant 61572134 and Grant
   U1509206, and in part by the STCSM, Shanghai, China, under Grant
   16QA1400500. The guest editor coordinating the review of this manuscript
   and approving it for publication was Prof. Yingcai Wu.
CR [Anonymous], 2008, P 31 ANN INT ACM SIG
   [Anonymous], 2004, Proceedings of the 12th ACM International Conference on Multimedia, DOI DOI 10.1145/1027527.1027747
   [Anonymous], 2008, ACM MULTIMEDIA
   [Anonymous], 2009, P ACM INT C MULT, DOI DOI 10.1145/1631272.1631295
   Bin Gao, 2005, 13th Annual ACM International Conference on Multimedia, P112
   Calado P., 2003, CIKM, P394
   Carmel D, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P139, DOI 10.1145/1571941.1571967
   Dumais S., 2000, SIGIR Forum, V34, P256
   Goldberger J, 2006, IEEE T IMAGE PROCESS, V15, P449, DOI 10.1109/TIP.2005.860593
   Hindle A, 2011, WORLD WIDE WEB, V14, P53, DOI 10.1007/s11280-010-0097-x
   Hsu WH, 2006, IEEE IMAGE PROC, P141, DOI 10.1109/ICIP.2006.312379
   Hu XH, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P389
   Hua-Jun Zeng, 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P210
   Huang C.-C., 2004, Proceedings of the 13th International Conference on World Wide Web, P184
   Ide I., 2010, P APSIPA ANN SUMM C, P996
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jing F., 2006, PROC MM 06, P377, DOI DOI 10.1145/1180639.1180720
   Krishnamachari S, 1999, IEEE SYMP COMP COMMU, P301, DOI 10.1109/ISCC.1999.780837
   Kuo Y.-H., 2000, P PAC RIM INT C ART
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ming Z.-Y., 2010, P PROT HIER BAS CLUS, P845
   Noll MG, 2008, APPLIED COMPUTING 2008, VOLS 1-3, P2315
   Recupero DR, 2007, INFORM RETRIEVAL, V10, P563, DOI 10.1007/s10791-007-9035-7
   Sedding J., 2004, P 3 WORKSHOP ROBUST, P104, DOI [10.3115/1220355.1220356, DOI 10.3115/1621445.1621458]
   Tan S, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2578394
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   Wang JJ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P845, DOI 10.1145/2647868.2655012
   Wu A. G., 2007, P ACM MM, P218
   Wu X, 2006, IEEE SIGNAL PROC MAG, V23, P59
   Wu XA, 2011, IEEE MULTIMEDIA, V18, P38, DOI 10.1109/MMUL.2011.12
   Xie L, 2011, P 19 ACM INT C MULT, P53, DOI DOI 10.1145/2072298.2072307
   Yu-Gang Jiang, 2016, IEEE Transactions on Big Data, V2, P32, DOI 10.1109/TBDATA.2016.2530714
   Zhu XW, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P233
NR 33
TC 10
Z9 12
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2161
EP 2170
DI 10.1109/TMM.2016.2614233
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900004
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Lee, H
   Kim, H
   Kim, JI
AF Lee, Hasup
   Kim, HyungSeok
   Kim, Jee-In
TI Background Subtraction Using Background Sets With Image- and Color-Space
   Reduction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Background subtraction; change detection; foreground segmentation; video
   signal processing
ID MOTION DETECTION; NEURAL-NETWORK; VIDEO; MODEL; SEGMENTATION; TRACKING
AB Background subtraction is a basic step for a variety of multimedia applications such as live video, traffic monitoring, communication system, interactive learning space, etc. Many approaches have been proposed for this problem, but the need for lower cost approaches still exists. In this paper, a relatively low-cost background-subtraction method is proposed, using background sets with image- and color-space reduction. The background sets are used to detect objects from dynamic backgrounds, which contain waves, trees, and fountains. The image space is reduced to deal with jittered and unsteady frames, e.g., the input from handheld mobile devices. The color space is reduced to compensate for color noise, e.g., the scattered RGB values of a digital camera. To reduce the cost, a combination of color-space reduction and hash-table look-up operations are used. The results compared with other methods show the feasibility of our method; moreover, it can be useful in mobile or embedded environments.
C1 [Lee, Hasup; Kim, HyungSeok; Kim, Jee-In] Konkuk Univ, Dept Internet & Multimedia Engn, Seoul 143701, South Korea.
C3 Konkuk University
RP Kim, H (corresponding author), Konkuk Univ, Dept Internet & Multimedia Engn, Seoul 143701, South Korea.
EM hasups@gmail.com; hyuskim@konkuk.ac.kr; jnkm@konkuk.ac.kr
FU Next-Generation Information Computing Development Program through the
   National Research Foundation of Korea - Ministry of Science, ICT, and
   Future Planning [2012M3C4A7032185]; Bio-Synergy Research Project of the
   Ministry of Science, ICT, and Future Planning through the National
   Research Foundation [2013M3A9C4078140]
FX This work was supported in part by the Next-Generation Information
   Computing Development Program through the National Research Foundation
   of Korea funded by the Ministry of Science, ICT, and Future Planning
   under Grant 2012M3C4A7032185, and in part by the Bio-Synergy Research
   Project under Grant 2013M3A9C4078140 of the Ministry of Science, ICT,
   and Future Planning through the National Research Foundation. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Yonghong Tian. (Corresponding
   author: HyungSeok Kim.)
CR Al-Qizwini M., 2015, P 49 ANN C INF SCI S, P1
   [Anonymous], CORR
   [Anonymous], 2000, P EUR C COMP VIS, DOI DOI 10.1007/3-540-45053-X_48
   Azmat S, 2014, IEEE SW SYMP IMAG, P81, DOI 10.1109/SSIAI.2014.6806034
   Benezeth Y, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3456695
   Bouwmans Thierry, 2011, Recent Patents on Computer Science, V4, P147, DOI 10.2174/1874479611104030147
   Bouwmans T., 2008, Recent Patents Comput. Sci., V1, P219
   Brutzer S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1937, DOI 10.1109/CVPR.2011.5995508
   Butler D, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P349
   Butler D. E., 2005, EURASIP J ADV SIG PR, V2005, P1
   Cambridge in Colour, 2016, DIG CAM IM NOIS 2
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Casares M, 2011, IEEE T CIRC SYST VID, V21, P1438, DOI 10.1109/TCSVT.2011.2162762
   Casares M, 2010, COMPUT VIS IMAGE UND, V114, P1223, DOI 10.1016/j.cviu.2010.03.023
   Chen BH, 2014, IEEE T MULTIMEDIA, V16, P837, DOI 10.1109/TMM.2014.2298377
   Cheng FC, 2015, ACM T INTEL SYST TEC, V7, DOI 10.1145/2746409
   Cheng FC, 2015, ENG APPL ARTIF INTEL, V38, P138, DOI 10.1016/j.engappai.2014.10.023
   Chien SY, 2002, IEEE T CIRC SYST VID, V12, P577, DOI 10.1109/TCSVT.2002.800516
   Cormen T.H., 2009, INTRO ALGORITHMS, V3rd, P253
   Cuevas C, 2013, IEEE T CIRC SYST VID, V23, P1, DOI 10.1109/TCSVT.2012.2202191
   Culibrk D, 2007, IEEE T NEURAL NETWOR, V18, P1614, DOI 10.1109/TNN.2007.896861
   El Baf F., 2007, 2007 14th International Workshop in Systems, Signals and Image Processing and 6th EURASIP Conference focused on Speech and Image Processing, Multimedia Communications and Services - EC-SIPMCS 2007, P385, DOI 10.1109/IWSSIP.2007.4381122
   El Baf F, 2008, LECT NOTES COMPUT SC, V5358, P772, DOI 10.1007/978-3-540-89639-5_74
   Elqursh A, 2012, LECT NOTES COMPUT SC, V7577, P228, DOI 10.1007/978-3-642-33783-3_17
   Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919
   Guo JM, 2013, IEEE T CIRC SYST VID, V23, P1809, DOI 10.1109/TCSVT.2013.2269011
   Guo JM, 2011, IEEE T CIRC SYST VID, V21, P804, DOI 10.1109/TCSVT.2011.2133270
   Hamid R., 2014, CORR
   Hirose M, 1999, IEEE MULTIMEDIA, V6, P14, DOI 10.1109/93.790608
   Huang SC, 2014, IEEE T CYBERNETICS, V44, P114, DOI 10.1109/TCYB.2013.2248057
   Huang SC, 2013, IEEE T NEUR NET LEAR, V24, P1920, DOI 10.1109/TNNLS.2013.2270314
   Huang SC, 2014, IEEE T IND ELECTRON, V61, P2099, DOI 10.1109/TIE.2013.2262764
   Huang SC, 2011, IEEE T CIRC SYST VID, V21, P1, DOI 10.1109/TCSVT.2010.2087812
   Javed S., 2015, P 2015 21 KOR JAP JO, P1
   Jung CR, 2009, IEEE T MULTIMEDIA, V11, P571, DOI 10.1109/TMM.2009.2012924
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Lee B., 2002, IMAGE VISION COMPUT, P315
   Lin HH, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P893, DOI 10.1109/ICIP.2002.1039116
   Maddalena L, 2008, IEEE T IMAGE PROCESS, V17, P1168, DOI 10.1109/TIP.2008.924285
   MCFARLANE NJB, 1995, MACH VISION APPL, V8, P187, DOI 10.1007/BF01215814
   Messelodi S, 2005, LECT NOTES COMPUT SC, V3617, P163, DOI 10.1007/11553595_20
   Navasca C., 2015, P IEEE INT C COMP VI, P99
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Oreifej O, 2013, IEEE T PATTERN ANAL, V35, P450, DOI 10.1109/TPAMI.2012.97
   Piccardi M, 2004, IEEE SYS MAN CYBERN, P3099, DOI 10.1109/ICSMC.2004.1400815
   Sarisaray-Boluk P, 2014, INT WIREL COMMUN, P779, DOI 10.1109/IWCMC.2014.6906455
   Seo JW, 2014, IEEE T MULTIMEDIA, V16, P2333, DOI 10.1109/TMM.2014.2353772
   Sheikh Y, 2009, IEEE I CONF COMP VIS, P1219, DOI 10.1109/ICCV.2009.5459334
   Shi JR, 2016, ALGORITHMS, V9, DOI 10.3390/a9020028
   Sigari MH, 2008, INT J COMPUT SCI NET, V8, P138
   Sobral A, 2014, LECT NOTES COMPUT SC, V8814, P94, DOI 10.1007/978-3-319-11758-4_11
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Valentine B, 2010, COMPUT VIS IMAGE UND, V114, P1152, DOI 10.1016/j.cviu.2010.03.014
   Wang Y, 2014, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2014.126
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Xiang SM, 2011, IEEE T MULTIMEDIA, V13, P342, DOI 10.1109/TMM.2010.2103930
   Xiao M, 2008, 2008 ISECS INTERNATIONAL COLLOQUIUM ON COMPUTING, COMMUNICATION, CONTROL, AND MANAGEMENT, VOL 1, PROCEEDINGS, P47, DOI 10.1109/CCCM.2008.294
   Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156
   Yamazaki M, 2006, LECT NOTES COMPUT SC, V3852, P467
   Zheng JY, 2006, TRANSPORT RES REC, P82
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 62
TC 27
Z9 29
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 2093
EP 2103
DI 10.1109/TMM.2016.2595262
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800015
DA 2024-07-18
ER

PT J
AU Siegert, Y
   Jiang, XY
   Krieg, V
   Bartholomäus, S
AF Siegert, Yannik
   Jiang, Xiaoyi
   Krieg, Volker
   Bartholomaeus, Sebastian
TI Classification-Based Record Linkage With Pseudonymized Data for
   Epidemiological Cancer Registries
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cancer registries; healthcare; machine learning; registry integration;
   record linkage
AB Cancer is one of the widest spread diseases in human society. Therefore, the need has grown to monitor, evaluate, and predict its development. Cancer registries address this problem by collecting data on cancer cases, striving for high quality, accuracy, and completeness. One of the basic challenges in this context is the linkage of data from multiple sources. In order to link new cancer records with existing ones, the cancer registries typically use an algorithm referred to as record linkage. Although the algorithm has automated a significant amount of the linking process, there still is a certain percentage of records that cannot be linked automatically. This study addresses the problem of reducing the need of manually matching records with machine learning methods. The particular challenge is caused by pseudonymization of the data. The main contribution is thus finding ways to encode the-pseudonymized-data, i.e., feature extraction so that it can be interpreted by a classifier. Three classifiers (neural network, support vector machines, decision tree) manage to achieve at least 93% classification rate on a dataset of 73 000 cancer records extracted from the inventory of a cancer registry. In addition, ensemble techniques boost the performance further to over 95%. We present an in-depth discussion of the experimental results from a perspective of applying the classification-based record linkage in real practice. Two scenarios of translating to practice will be outlined with a potential of reducing the human workload by an order of magnitude of hundreds of hours.
C1 [Siegert, Yannik; Krieg, Volker; Bartholomaeus, Sebastian] Epidemiol Canc Registry North Rhine Westphalia, D-48149 Munster, Germany.
   [Jiang, Xiaoyi] Univ Muunster, Dept Math & Comp Sci, D-48149 Munster, Germany.
RP Jiang, XY (corresponding author), Univ Muunster, Dept Math & Comp Sci, D-48149 Munster, Germany.
EM Yannik.Siegert@krebsregister.nrw.de; xjiang@uni-muenster.de;
   volker.krieg@krebsregister.nrw.de;
   sebastian.Bartholomaus@krebsregister.nrw.de
RI Jiang, Xiaoyi/AAA-3532-2022
OI Jiang, Xiaoyi/0000-0001-7678-9528
CR [Anonymous], 2011, ICD 10: International statistical classification of diseases and related health problems, V10th
   [Anonymous], 2015, CANCER
   Bartholomäus S, 2015, STUD HEALTH TECHNOL, V210, P424, DOI 10.3233/978-1-61499-512-8-424
   Christen P., 2008, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P151, DOI DOI 10.1145/1401890.1401913
   Christen P., 2012, PAC AS C KNOWL DISC
   DUNN HL, 1946, AM J PUBLIC HEALTH, V36, P1412
   Elfeky MG, 2002, PROC INT CONF DATA, P17, DOI 10.1109/ICDE.2002.994694
   FELLEGI IP, 1969, J AM STAT ASSOC, V64, P1183, DOI 10.2307/2286061
   Fuhs A, 2014, BUNDESGESUNDHEITSBLA, V57, P60, DOI 10.1007/s00103-013-1870-7
   Giraud-Carrier C, 2015, KNOWL INF SYST, V45, P389, DOI 10.1007/s10115-014-0812-5
   Hall R, 2010, LECT NOTES COMPUT SC, V6344, P269, DOI 10.1007/978-3-642-15838-4_24
   Jamil HM, 2015, IEEE ACM T COMPUT BI, V12, P473, DOI 10.1109/TCBB.2014.2355213
   Krieg V, 2001, GESUNDHEITSWESEN, V63, P376, DOI 10.1055/s-2001-15686
   Kuncheva L. I., 2004, COMBINING PATTERN CL, V390, P413
   Mojon-Azzi SM, 2003, NATURE, V421, P209, DOI 10.1038/421209a
   NEWCOMBE HB, 1959, SCIENCE, V130, P954, DOI 10.1126/science.130.3381.954
   Pixton B., 2006, 6 ANN WORKSH TECHN F
   Platt JC, 2000, ADV NEUR IN, P61
   Siegert Y., 2015, THESIS
   Subitha S., 2014, INT J COMPUT APPL, V104, P17
   Vatsalan D, 2013, INFORM SYST, V38, P946, DOI 10.1016/j.is.2012.11.005
   WAGNER RA, 1974, J ACM, V21, P168, DOI 10.1145/321796.321811
   Wilson D. R., 2011, P ROOTSTECH, P331
   Wilson DR, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P9, DOI 10.1109/IJCNN.2011.6033192
   Xiao J, 2012, EXPERT SYST APPL, V39, P3668, DOI 10.1016/j.eswa.2011.09.059
   Yu ZW, 2016, IEEE T CYBERNETICS, V46, P1263, DOI 10.1109/TCYB.2015.2443857
   Yu ZW, 2015, IEEE T CYBERNETICS, V45, P177, DOI 10.1109/TCYB.2014.2322195
NR 27
TC 8
Z9 10
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 1929
EP 1941
DI 10.1109/TMM.2016.2598482
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800002
DA 2024-07-18
ER

PT J
AU Ding, XM
   Li, B
   Xiong, WH
   Guo, W
   Hu, WM
   Wang, B
AF Ding, Xinmiao
   Li, Bing
   Xiong, Weihua
   Guo, Wen
   Hu, Weiming
   Wang, Bo
TI Multi-Instance Multi-Label Learning Combining Hierarchical Context and
   its Application to Image Annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image annotation; instance context; label context; multi-instance;
   multi-label
ID CLASSIFICATION; CATEGORIZATION
AB In image annotation, one image is often modeled as a bag of regions ("instances") associated with multiple labels, which is a typical application of multi-instance multi-label learning (MIML). Although lots of research has shown that the interplay embedded among instances and labels can largely boost the image annotation accuracy, most existing MIML methods consider none or partial context cues. In this paper, we propose a novel context-aware MIML model to integrate the instance context and label context into a general framework. Specially, the instance context is constructed with multiple graphs, while the label context is built up through a linear combination of several common latent conceptions that link low level features and high level semantic labels. Comparison with other leading methods on several benchmark datasets in terms of image annotation shows that our proposed method can get better performance than the state-of-the-art approaches.
C1 [Ding, Xinmiao; Guo, Wen] Shandong Technol & Business Univ, Yantai 264005, Peoples R China.
   [Li, Bing; Xiong, Weihua; Wang, Bo] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Hu, Weiming] Chinese Acad Sci, Inst Automat, CAS Ctr Excellence Brain Sci & Intelligence Techn, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 Shandong Technology & Business University; Chinese Academy of Sciences;
   Institute of Automation, CAS; Chinese Academy of Sciences; Institute of
   Automation, CAS
RP Li, B (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM dingxinmiao@126.com; bli@nlpr.ia.ac.cn; wallace.xiong@gmail.com;
   wguo@nlpr.ia.ac.cn; wmhu@nlpr.ia.ac.cn; bo.wang2014@nlpr.ia.ac.cn
RI Li, Bing/AAX-5919-2021
FU 973 basic research program of China [2014CB349303]; Natural Science
   Foundation of China [61472421, 61370038, 61303086, 61572296, 61503219,
   61472227]; Strategic Priority Research Program of the CAS [XDB02070003];
   Natural Science Foundation of Shandong Province [ZR2015FL020]
FX This work was supported in part by the 973 basic research program of
   China under Grant 2014CB349303, in part by the Natural Science
   Foundation of China under Grant 61472421, Grant 61370038, Grant
   61303086, Grant 61572296, Grant 61503219, and Grant 61472227, in part by
   the Strategic Priority Research Program of the CAS under Grant
   XDB02070003, and in part by the Natural Science Foundation of Shandong
   Province under Grant ZR2015FL020. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Jing-Ming Guo. (Corresponding author: Bing Li.)
CR Altintakan UL, 2015, IEEE T MULTIMEDIA, V17, P323, DOI 10.1109/TMM.2014.2388312
   [Anonymous], 2001, Encyclopedia of Mathematics
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Berg C., 1984, Harmonic analysis on semigroups: theory of positive definite and related functions, DOI [DOI 10.1007/978-1-4612-1128-0, 10.1007/978-1-4612-1128-0]
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Briggs F, 2012, P 18 ACM SIGKDD INT, P534, DOI [DOI 10.1145/2339530.2339616, 10.1145/2339530.2339616]
   Chen YX, 2004, J MACH LEARN RES, V5, P913
   Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPR.2009.5206800, 10.1109/CVPRW.2009.5206800]
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Ding X., 2012, P AS C COMP VIS, P599
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   GAFNI EM, 1984, SIAM J CONTROL OPTIM, V22, P936, DOI 10.1137/0322061
   Gartner T., 2002, P 19 INT C MACH LEAR, V2, P179
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Gu ZW, 2008, IEEE T MULTIMEDIA, V10, P1605, DOI 10.1109/TMM.2008.2007290
   Hu WM, 2016, IEEE T MULTIMEDIA, V18, P76, DOI 10.1109/TMM.2015.2496372
   Huang SJ, 2014, AAAI CONF ARTIF INTE, P1868
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Ji SW, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754431
   Jin R, 2009, PROC CVPR IEEE, P896, DOI 10.1109/CVPRW.2009.5206684
   Kumar A., 2012, P 29 INT C MACH LEAR, P1383
   Lang P. J., 1999, TECH REP
   Li B, 2016, INT J COMPUT VISION, V117, P21, DOI 10.1007/s11263-015-0844-7
   Li B, 2015, IEEE T IMAGE PROCESS, V24, P5193, DOI 10.1109/TIP.2015.2479400
   Li H, 2009, INT CONF DAT MIN WOR, P164, DOI 10.1109/ICDMW.2009.46
   Li Y.-F., 2012, P 26 AAAI C ART INT, P1012
   Li YX, 2012, IEEE ACM T COMPUT BI, V9, P98, DOI 10.1109/TCBB.2011.73
   Liu WF, 2016, NEUROCOMPUTING, V172, P3, DOI 10.1016/j.neucom.2014.06.096
   Liu WF, 2014, COMPUT VIS IMAGE UND, V118, P50, DOI 10.1016/j.cviu.2013.03.007
   Liu WF, 2013, IEEE T IMAGE PROCESS, V22, P2676, DOI 10.1109/TIP.2013.2255302
   Luo J., 2010, Advances in neural information processing systems, V23
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Maron O., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P341
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Nam Nguyen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P384, DOI 10.1109/ICDM.2010.109
   Nguyen C, 2013, P 23 INT JOINT C ART, P1558
   Nguyen C.-T., 2010, P 19 ACM INT C INF K, P1481
   Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923
   Schmidt M, 2007, LECT NOTES ARTIF INT, V4701, P286
   Schölkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Surdeanu M., 2012, P 2012 JOINT C EMP M, P455
   Tao DP, 2013, IEEE T MULTIMEDIA, V15, P833, DOI 10.1109/TMM.2013.2238909
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   TURING AM, 1948, Q J MECH APPL MATH, V1, P287, DOI 10.1093/qjmam/1.1.287
   Wahba G, 1999, ADVANCES IN KERNEL METHODS, P69
   Wang H., 2009, IEEE 12 C COMP VIS K
   Wang H, 2009, J OPTOELECTRON BIOME, V1, P1
   Wang H, 2011, PROC CVPR IEEE, P793, DOI 10.1109/CVPR.2011.5995379
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Wolfe P., 1967, Nonlinear Programming, P97
   Xu JJ, 2014, IEEE T MULTIMEDIA, V16, P403, DOI 10.1109/TMM.2013.2291218
   Xu X, 2004, LECT NOTES ARTIF INT, V3056, P272
   Xu X.-S., 2011, P 19 ACM INT C MULT, P1153, DOI [10.1145/2072298.2071962, DOI 10.1145/2072298.2071962]
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Yang S.-H., 2009, P ADV NEURAL INFORM, P2143
   Zha ZJ, 2008, PROC CVPR IEEE, P333
   Zhang ML, 2010, PROC INT C TOOLS ART, P207, DOI 10.1109/ICTAI.2010.102
   Zhou Z.-H., 2007, P ADV NEUR INF PROC, P1609, DOI DOI 10.1016/J.PATCOG.2006.12.019
   Zhou ZH, 2012, ARTIF INTELL, V176, P2291, DOI 10.1016/j.artint.2011.10.002
   Zhou Zhi-Hua, 2009, PROC ANN INT C MACH, V382, P1249
NR 62
TC 29
Z9 32
U1 1
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1616
EP 1627
DI 10.1109/TMM.2016.2572000
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000014
DA 2024-07-18
ER

PT J
AU Mao, Y
   Cheung, GN
   Ji, YS
AF Mao, Yu
   Cheung, Gene
   Ji, Yusheng
TI On Constructing <i>z</i>-Dimensional DIBR-Synthesized Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color-plus-depth representation; depth-image-based rendering (DIBR);
   graph signal processing (GSP)
ID MULTIVIEW VIDEO; DEPTH; INTERPOLATION
AB The "color-plus-depth" format represents a 3D scene using multiple color and depth images captured by an array of closely spaced cameras. Using this format, a novel image as observed from a horizontally shifted virtual viewpoint can be synthesized via depth-image-based rendering (DIBR), using neighboring camera-captured viewpoint images as reference. In this paper, using the same popularized color-plus-depth representation, we propose to construct, in addition, novel images as observed from virtual viewpoints closer to the 3D scene, enabling a new dimension of view navigation. To construct this new image type, we first perform a new DIBR pixel-mapping for z-dimensional camera movement. We then identify expansion holes-a new kind of missing pixels unique in z-dimensional DIBR-mapped images-using a depth layering procedure. To fill expansion holes we formulate a patch-based maximum a posteriori problem, where the patches are appropriately spaced using diamond tiling. Leveraging on recent advances in graph signal processing, we define a graph-signal smoothness prior to regularize the inverse problem. Finally, we design a fast iterative reweighted least square algorithm to solve the posed problem efficiently. Experimental results show that our z-dimensional synthesized images outperform images rendered by a naive modification of VSRS 3.5 by up to 4.01 dB.
C1 [Mao, Yu; Cheung, Gene; Ji, Yusheng] Natl Inst Informat, Tokyo 1018430, Japan.
   [Mao, Yu; Cheung, Gene; Ji, Yusheng] Grad Univ Adv Studies, Tokyo 1018430, Japan.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; Graduate University for Advanced
   Studies - Japan
RP Mao, Y (corresponding author), Natl Inst Informat, Tokyo 1018430, Japan.; Mao, Y (corresponding author), Grad Univ Adv Studies, Tokyo 1018430, Japan.
EM mao@nii.ac.jp; cheung@nii.ac.jp; kei@nii.ac.jp
RI Cheung, Gene/AAB-9284-2020; Ji, Yusheng/AAF-1537-2020
OI Ji, Yusheng/0000-0003-4364-8491; Cheung, Gene/0000-0002-5571-4137
FU Graduate University for Advanced Studies (SOKENDAI), Hayama, Japan
FX This work was supported by the Graduate University for Advanced Studies
   (SOKENDAI), Hayama, Japan. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Maria
   Martini.
CR [Anonymous], P 3DTV C TRUE VIS CA
   [Anonymous], SIGN INF PROC ASS AN
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], IEEE INT C IM PROC O
   [Anonymous], FP7ICT247996 EUR COM
   [Anonymous], FP7ICT247010 HOL
   [Anonymous], P 3DTV C TRUE VIS CA
   [Anonymous], P SOC OPT ENG 3 DIM
   [Anonymous], IEEE INT WORKSH MULT
   Battisti F, 2015, SIGNAL PROCESS-IMAGE, V30, P78, DOI 10.1016/j.image.2014.10.005
   Bigun J., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P433
   Bredies K, 2015, SIAM J IMAGING SCI, V8, P2814, DOI 10.1137/15M1023865
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   Chen SH, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P872, DOI 10.1109/GlobalSIP.2014.7032244
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P744, DOI 10.1109/TIP.2010.2070074
   Chung F. R. K., 1997, Spectral graph theory
   Daribo Ismael, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P167, DOI 10.1109/MMSP.2010.5662013
   Daubechies I, 2010, COMMUN PUR APPL MATH, V63, P1, DOI 10.1002/cpa.20303
   de With, 2007, 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D Video, 2007, P1
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Hartley RI, 1999, INT J COMPUT VISION, V35, P115, DOI 10.1023/A:1008115206617
   HOU HS, 1978, IEEE T ACOUST SPEECH, V26, P508
   Hu W, 2015, IEEE T IMAGE PROCESS, V24, P419, DOI 10.1109/TIP.2014.2378055
   Hu W, 2013, IEEE INT WORKSH MULT, P1, DOI 10.1109/MMSP.2013.6659254
   Ilkoo Ahn, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P109, DOI 10.1109/ICME.2012.95
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Liu XM, 2011, IEEE T IMAGE PROCESS, V20, P3455, DOI 10.1109/TIP.2011.2150234
   Liu Z, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2330332
   Macchiavello B, 2014, IEEE T MULTIMEDIA, V16, P711, DOI 10.1109/TMM.2014.2299768
   Mao Y, 2013, INT CONF ACOUST SPEE, P1859, DOI 10.1109/ICASSP.2013.6637975
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Narang SK, 2013, INT CONF ACOUST SPEE, P5445, DOI 10.1109/ICASSP.2013.6638704
   Pang JH, 2015, INT CONF ACOUST SPEE, P2294, DOI 10.1109/ICASSP.2015.7178380
   Ren DN, 2015, IEEE T MULTIMEDIA, V17, P307, DOI 10.1109/TMM.2015.2389714
   Ren DN, 2014, IEEE T MULTIMEDIA, V16, P1874, DOI 10.1109/TMM.2014.2332139
   Romano Y, 2014, IEEE T IMAGE PROCESS, V23, P3085, DOI 10.1109/TIP.2014.2325774
   Scharstein D., 2007, PROC IEEE C COMPUT V, P1
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Tian D, 2009, PROC SPIE, V7443, DOI 10.1117/12.829372
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Toni L, 2015, IEEE T MULTIMEDIA, V17, P1604, DOI 10.1109/TMM.2015.2450020
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wan PF, 2014, IEEE IMAGE PROC, P4052, DOI 10.1109/ICIP.2014.7025823
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiu XY, 2012, IEEE T MULTIMEDIA, V14, P1109, DOI 10.1109/TMM.2012.2191267
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yin Z, 2014, 3DTV C TRUE VIS CAPT, P1
   Yu GS, 2012, IEEE T IMAGE PROCESS, V21, P2481, DOI 10.1109/TIP.2011.2176743
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
NR 56
TC 15
Z9 17
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1453
EP 1468
DI 10.1109/TMM.2016.2573142
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000001
DA 2024-07-18
ER

PT J
AU Wang, HB
   Feng, L
   Zhang, J
   Liu, Y
AF Wang, Huibing
   Feng, Lin
   Zhang, Jing
   Liu, Yang
TI Semantic Discriminative Metric Learning for Image Similarity Measurement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Divergence balance; geometric mean; image similarity measurement; metric
   learning; semantic discriminative metric learning (SDML); semantic
   information
ID DISTANCE; SCALE
AB Along with the arrival of multimedia time, multimedia data has replaced textual data to transfer information in various fields. As an important form of multimedia data, images have been widely utilized by many applications, such as face recognition and image classification. Therefore, how to accurately annotate each image from a large set of images is of vital importance but challenging. To perform these tasks well, it is crucial to extract suitable features to character the visual contents of images and learn an appropriate distance metric to measure similarities between all images. Unfortunately, existing feature operators, such as histogram of gradient, local binary pattern, and color histogram, care more about the visual character of images and lack the ability to distinguish semantic information. Similarities between those features cannot reflect the real category correlations due to the well-known semantic gap. In order to solve this problem, this paper proposes a regularized distance metric framework called semantic discriminative metric learning (SDML). SDML combines geometric mean with normalized divergences and separates images from different classes simultaneously. The learned distance metric can treat all images from different classes equally. And distinctions between similar classes with entirely different semantic contents are emphasized by SDML. This procedure ensures the consistency between dissimilarities and semantic distinctions and avoids inaccuracy similarities incurred by unbalanced locations of samples. Various experiments on benchmark image datasets show the excellent performance of the novel method.
C1 [Wang, Huibing; Zhang, Jing; Liu, Yang] Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116024, Peoples R China.
   [Feng, Lin] Dalian Univ Technol, Sch Innovat & Entrepreneurship, Dalian 116024, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Wang, HB (corresponding author), Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116024, Peoples R China.
EM whb08421005@mail.dlut.edu.cn; fenglin@dlut.edu.cn;
   zhangjing_0412@163.com; dlut_liuyang@mail.dlut.edu.cn
FU National Natural Science Foundation of China [61173163, 61370200]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61173163 and Grant 61370200. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Winston Hsu.
CR [Anonymous], 2009, P ART INT STAT CLEAR
   [Anonymous], P ADV NEURAL INFORM
   [Anonymous], 2010, Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, July 25-28, 2010, DOI DOI 10.1145/1835804.183594
   Beyer K, 1999, LECT NOTES COMPUT SC, V1540, P217
   Cao Q, 2013, IEEE I CONF COMP VIS, P2408, DOI 10.1109/ICCV.2013.299
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Cinbis RG, 2011, IEEE I CONF COMP VIS, P1559, DOI 10.1109/ICCV.2011.6126415
   Cover T. M., 2012, ELEMENTS INFORM THEO
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Davis J. V., 2007, ICML, P209
   Davis J. V., 2008, P 14 ACM SIGKDD INT, P195, DOI DOI 10.1145/1401890.1401918
   Duda R. O., 2012, PATTERN CLASSIFICATI, DOI DOI 10.1007/978-3-319-57027-3_4
   Goldberger J., 2004, Advances in Neural Information Processing Systems
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   He XF, 2004, ADV NEUR IN, V16, P153
   Huang KZ, 2009, IEEE DATA MINING, P189, DOI 10.1109/ICDM.2009.22
   Jiang N, 2011, IEEE T IMAGE PROCESS, V20, P2288, DOI 10.1109/TIP.2011.2114895
   Jing YS, 2013, IEEE T MULTIMEDIA, V15, P2022, DOI 10.1109/TMM.2013.2279663
   Liu W., 2010, 24 AAAI C ART INT AT
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Ng A. Y., 2002, Advances in Neural Information Processing Systems, P1473
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Qi Guo-Jun., 2009, ICML, P106
   Qiao LS, 2010, PATTERN RECOGN, V43, P331, DOI 10.1016/j.patcog.2009.05.005
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang F, 2011, IEEE T SYST MAN CY B, V41, P931, DOI 10.1109/TSMCB.2010.2101593
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang XY, 2010, LECT NOTES COMPUT SC, V6313, P200
   Wang ZX, 2010, LECT NOTES COMPUT SC, V6311, P706, DOI 10.1007/978-3-642-15549-9_51
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Ying Yiming., 2009, ADV NEURAL INFORM PR, P2214
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P4636, DOI 10.1109/TIP.2012.2207395
NR 38
TC 41
Z9 42
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1579
EP 1589
DI 10.1109/TMM.2016.2569412
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000011
DA 2024-07-18
ER

PT J
AU He, YH
   Xiang, SM
   Kang, CC
   Wang, J
   Pan, CH
AF He, Yonghao
   Xiang, Shiming
   Kang, Cuicui
   Wang, Jian
   Pan, Chunhong
TI Cross-Modal Retrieval via Deep and Bidirectional Representation Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bidirectional modeling; convolutional neural network; cross-modal
   retrieval; representation learning; word embedding
AB Cross-modal retrieval emphasizes understanding inter-modality semantic correlations, which is often achieved by designing a similarity function. Generally, one of the most important things considered by the similarity function is how to make the cross-modal similarity computable. In this paper, a deep and bidirectional representation learning model is proposed to address the issue of image-text cross-modal retrieval. Owing to the solid progress of deep learning in computer vision and natural language processing, it is reliable to extract semantic representations from both raw image and text data by using deep neural networks. Therefore, in the proposed model, two convolution-based networks are adopted to accomplish representation learning for images and texts. By passing the networks, images and texts are mapped to a common space, in which the cross-modal similarity is measured by cosine distance. Subsequently, a bidirectional network architecture is designed to capture the property of the cross-modal retrieval-the bidirectional search. Such architecture is characterized by simultaneously involving the matched and unmatched image-text pairs for training. Accordingly, a learning framework with maximum likelihood criterion is finally developed. The network parameters are optimized via backpropagation and stochastic gradient descent. A great deal of experiments are conducted to sufficiently evaluate the proposed method on three publicly released datasets: IAPRTC-12, Flickr30k, and Flickr8k. The overall results definitely show that the proposed architecture is effective and the learned representations have good semantics to achieve superior cross-modal retrieval performance.
C1 [He, Yonghao; Xiang, Shiming; Wang, Jian; Pan, Chunhong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Kang, Cuicui] Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Institute of Information Engineering, CAS
RP Xiang, SM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM yhhe@nlpr.ia.ac.cn; smxiang@nlpr.ia.ac.cn; kangcuicui@iie.ac.cn;
   jian.wang@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn
FU National Basic Research Program of China [2012CB316304]; Strategic
   Priority Research Program of the CAS [XDB02060009]; National Natural
   Science Foundation of China [61272331, 91338202]; Beijing Natural
   Science Foundation [4162064]
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2012CB316304, in part by the Strategic Priority
   Research Program of the CAS under Grant XDB02060009, in part by the
   National Natural Science Foundation of China under Grant 61272331 and
   Grant 91338202, and in part by the Beijing Natural Science Foundation
   under Grant 4162064. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Enrico Magli.
   (Corresponding author: Shiming Xiang.)
CR [Anonymous], 2014, Advances in Neural Information Processing Systems
   [Anonymous], 2013, P 26 INT C NEUR INF
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   [Anonymous], 2011, P ICML
   [Anonymous], 2013, CORR
   [Anonymous], 2014, CORR
   [Anonymous], 2014, Eprint Arxiv
   [Anonymous], 2014, CORR
   [Anonymous], 2014, PROC IEEE C COMPUTER, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]
   [Anonymous], 2014, PROC VLDB ENDOW
   [Anonymous], 2014, CORR
   [Anonymous], 2014, P ADV NEUR INF PROC
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue Jeff., 2014, CoRR
   Dos Santos C., 2014, Coling, P69
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Grubinger M., 2006, Language Resources and Evaluation, P13
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2009, INT C NEURAL INF PRO, P1607
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Huang PS, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2333
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Karpathy Andrej, 2014, Advances in neural information processing systems, P1889
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liang X, 2013, P 3 ACM INT C MULT R, P175
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu XY, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P433
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Mukundan Ramakrishnan., 1998, MOMENT FUNCTIONS IMA, V100
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salakhutdinov R., 2009, AISTATS
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Socher R., 2011, PROC INT C MACH LEAR, P129
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Sun Yi, 2014, NEURIPS, DOI DOI 10.1007/978-3-030-01252-6_48
   Sutskever Ilya, 2011, P 28 INT C MACH LEAR, P1017
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Vinyals O., 2014, CORR
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang W, 2016, VLDB J, V25, P79, DOI 10.1007/s00778-015-0391-4
   Wu F., 2013, P ACM INT C MULT, P877
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Yu Z, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P395, DOI 10.1145/2600428.2609563
   Zhai XH, 2012, LECT NOTES COMPUT SC, V7131, P312
   Zhai XH, 2012, INT CONF ACOUST SPEE, P2337, DOI 10.1109/ICASSP.2012.6288383
   Zhang Xiang., 2015, CoRR
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 61
TC 92
Z9 101
U1 3
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1363
EP 1377
DI 10.1109/TMM.2016.2558463
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600012
DA 2024-07-18
ER

PT J
AU Jin, J
   Wang, AH
   Zhao, Y
   Lin, CY
   Zeng, B
AF Jin, Jian
   Wang, Anhong
   Zhao, Yao
   Lin, Chunyu
   Zeng, Bing
TI Region-Aware 3-D Warping for DIBR
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D video (3DV); depth-image-based rendering (DIBR); multi-view; region
   partitioning; region-aware 3-D warping
ID VIEW SYNTHESIS; 3D VIDEO
AB In 3-D video (3DV) applications, depth-image-based rendering (DIBR) has been widely employed to synthesize virtual views. However, this approach is performed in a frame-based way, meaning each whole frame is dealt with and the characteristics of different regions in the frame are ignored. As a result, redundant pixels in some regions are abused during the subsequent warping and blending stage. This paper proposes a region-aware 3-D warping approach for DIBR in which warped frames are reasonably divided beforehand so that only the indispensable regions are used. With the proposed scheme, it is possible to avoid noneffective and repeated pixels during the warping stage. In addition, the blending process is also saved. The experimental results show that compared to the state-of-the-art VSRS3.5 and VSRS-1D-fast algorithms, our approach can achieve significant computation savings without sacrificing synthesis quality.
C1 [Jin, Jian; Zhao, Yao; Lin, Chunyu] Beijing Jiao Tong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Jin, Jian; Zhao, Yao; Lin, Chunyu] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Wang, Anhong] Taiyuan Univ Sci & Technol, Sch Elect Informat Engn, Taiyuan 030024, Peoples R China.
   [Zeng, Bing] Univ Elect Sci & Technol China, Inst Image Proc, Chengdu 611731, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Taiyuan
   University of Science & Technology; University of Electronic Science &
   Technology of China
RP Wang, AH (corresponding author), Taiyuan Univ Sci & Technol, Sch Elect Informat Engn, Taiyuan 030024, Peoples R China.
EM jianjin@bjtu.edu.cn; wah_ty@163.com; yzhao@bjtu.edu.cn;
   cylin@bjtu.edu.cn; eezeng@uestc.edu.cn
RI Lin, Chunyu/AAI-5185-2021
OI Lin, Chunyu/0000-0003-2847-0349
FU National Natural Science Foundation of China [61210006, 61402034,
   61272262]; Beijing Natural Science Foundation [4154082]; Fundamental
   Research Funds for the Central Universities [2015JBM032]; International
   Cooperative Program of Shanxi Province [2015081015]; Scientific and
   Technological project of Shanxi Province [2015031003-2]; Shanxi
   Scholarship Council of China [2014-056]; Program for New Century
   Excellent Talent in Universities [NCET-12-1037]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61210006, Grant 61402034, and Grant 61272262, by the
   Beijing Natural Science Foundation under Grant 4154082, by the
   Fundamental Research Funds for the Central Universities under Grant
   2015JBM032, by the International Cooperative Program of Shanxi Province
   under Grant 2015081015, by the Scientific and Technological project of
   Shanxi Province under Grant 2015031003-2, by the Shanxi Scholarship
   Council of China under Grant 2014-056, and by the Program for New
   Century Excellent Talent in Universities under Grant NCET-12-1037. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Cha Zhang. (Corresponding author:
   Anhong Wang.)
CR [Anonymous], 2012, JCT3VA1005
   [Anonymous], 2010, 2010 INT S SIGN SYST, DOI DOI 10.1103/PHYSREVC.81.044612
   [Anonymous], 2012, JCT3VB0124
   [Anonymous], 2003, P IIASTED VIIP
   [Anonymous], 2008, 3DV SEQ ETRI GIST
   [Anonymous], 2014, JCT3VH1003
   [Anonymous], 2008, 3DV SEQ NAG U
   Bruls F., 2009, ISOIECJTC1SC29WG11
   Chen K., 2013, IEEE MTT S INT MICR, P1, DOI DOI 10.1109/MWSYM.2013.6697759
   Dongbo Min, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P249
   Fraunhofer Heinrich Hertz Institute, 2013, 3DV Sequences of HHI
   Greene N., 1993, Computer Graphics Proceedings, P231, DOI 10.1145/166117.166147
   Horng YR, 2011, IEEE T CIRC SYST VID, V21, P1329, DOI 10.1109/TCSVT.2011.2148410
   Lee PJ, 2011, IEEE T MULTIMEDIA, V13, P246, DOI 10.1109/TMM.2010.2100372
   Lei JJ, 2015, IEEE T MULTIMEDIA, V17, P457, DOI 10.1109/TMM.2015.2400823
   Ma SW, 2014, IEEE T MULTIMEDIA, V16, P266, DOI 10.1109/TMM.2013.2284751
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Oliveira MM, 2000, COMP GRAPH, P359, DOI 10.1145/344779.344947
   Tanimoto M, 2009, ISOIECJTC1SC29WG11MP
   Tech G., 2012, 1 M STOCKH SWED JUL
   Tian D, 2009, PROC SPIE, V7443, DOI 10.1117/12.829372
   Tsung P.-K., 2009, 2009 3DTV Conference: The True Vision-Capture, Transmission and Display of 3D Video, P1
   Vijayanagar KR, 2013, IEEE IMAGE PROC, P2197, DOI 10.1109/ICIP.2013.6738453
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao-han Lu, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P339, DOI 10.1109/ICME.2012.116
   Zhao Y, 2011, IEEE T BROADCAST, V57, P510, DOI 10.1109/TBC.2011.2120730
   Zhao Y, 2011, IEEE T IMAGE PROCESS, V20, P2221, DOI 10.1109/TIP.2011.2118218
NR 29
TC 22
Z9 24
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 953
EP 966
DI 10.1109/TMM.2016.2539825
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100001
DA 2024-07-18
ER

PT J
AU Zhang, C
   Kang, K
   Li, HS
   Wang, XG
   Xie, R
   Yang, XK
AF Zhang, Cong
   Kang, Kai
   Li, Hongsheng
   Wang, Xiaogang
   Xie, Rong
   Yang, Xiaokang
TI Data-Driven Crowd Understanding: A Baseline for a Large-Scale Crowd
   Dataset
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowd features; crowd scene understanding; data-driven methods;
   large-scale benchmark
ID TRACKING; PATTERN; MOTION; VIDEO; CLASSIFICATION; MODEL; SCENE
AB Crowd understanding has drawn increasing attention from the computer vision community, and its progress is driven by the availability of public crowd datasets. In this paper, we contribute a large-scale benchmark dataset collected from the Shanghai 2010 World Expo. It includes 2630 annotated video sequences captured by 245 surveillance cameras, far larger than any public dataset. It covers a large number of different scenes and is suitable for evaluating the performance of crowd segmentation and estimation of crowd density, collectiveness, and cohesiveness, all of which are universal properties of crowd systems. In total, 53 637 crowd segments are manually annotated with the three crowd properties. This dataset is released to the public to advance research on crowd understanding. The large-scale annotated dataset enables using data-driven approaches for crowd understanding. In this paper, a data-driven approach is proposed as a baseline of crowd segmentation and estimation of crowd properties for the proposed dataset. Novel global and local crowd features are designed to retrieve similar training scenes and to match spatio-temporal crowd patches so that the labels of the training scenes can be accurately transferred to the query image. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art approaches for crowd understanding.
C1 [Zhang, Cong; Xie, Rong; Yang, Xiaokang] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
   [Zhang, Cong; Kang, Kai; Li, Hongsheng; Wang, Xiaogang] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
C3 Shanghai Jiao Tong University; Chinese University of Hong Kong
RP Zhang, C; Xie, R; Yang, XK (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.; Kang, K; Li, HS; Wang, XG (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
EM zhangcong0929@gmail.com; kkang@ee.cuhk.edu.hk; hsli@ee.cuhk.edu.hk;
   xgwang@ee.cuhk.edu.hk; xierong@sjtu.edu.cn; xkyang@sjtu.edu.cn
RI Yang, Xiaokang/C-6137-2009; Wang, Xiaogang/L-4369-2014; Li,
   Hongsheng/AES-5328-2022; Li, Haisheng/ABG-8847-2020
OI Yang, Xiaokang/0000-0003-4029-3322; Li, Hongsheng/0000-0002-2664-7975; 
FU NSFC [61527804, 61221001, 61301269, 61371192]; STCSM [14XD1402100]; 111
   Program [B07022]; General Research Fund - Research Grants Council of
   Hong Kong [CUHK 419412, CUHK 147011]; Hong Kong Innovation and
   Technology Support Programme Project [ITS/221/13FP]; Shenzhen Basic
   Research Program [JCYJ20130402113127496]; Ph.D. Programs Foundation of
   China [20130185120039]; Sichuan Hi-tech RD Program [2014GZX0009]; China
   Postdoctoral Foundation [2014M552339]
FX This work was supported in part by the NSFC under Grant 61527804, Grant
   61221001, Grant 61301269, Grant 61371192, and Grant 61301269, in part by
   the STCSM under Grant 14XD1402100, 111 Program (B07022), in part by the
   General Research Fund sponsored by the Research Grants Council of Hong
   Kong Project CUHK 419412 and CUHK 147011, in part by the Hong Kong
   Innovation and Technology Support Programme Project ITS/221/13FP, in
   part by the Shenzhen Basic Research Program under Grant
   JCYJ20130402113127496, in part by the Ph.D. Programs Foundation of China
   under Grant 20130185120039, in part by the Sichuan Hi-tech R&D Program
   under Grant 2014GZX0009, and in part by the China Postdoctoral
   Foundation under Grant 2014M552339. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Martha Larson.
CR Ali I, 2012, IMAGE VISION COMPUT, V30, P966, DOI 10.1016/j.imavis.2012.08.013
   Ali S, 2008, LECT NOTES COMPUT SC, V5303, P1, DOI 10.1007/978-3-540-88688-4_1
   Ali S, 2007, PROC CVPR IEEE, P65
   [Anonymous], P BRIT MACH VIS C
   Arandjelovic O., 2008, P BRIT MACH VIS ASS, P1
   Arteta C, 2014, LECT NOTES COMPUT SC, V8691, P504, DOI 10.1007/978-3-319-10578-9_33
   AVENI AF, 1977, SOCIOMETRY, V40, P96, DOI 10.2307/3033551
   Ballan L, 2012, IEEE T MULTIMEDIA, V14, P1234, DOI 10.1109/TMM.2012.2191268
   Blumer H., 1951, COLLECTIVE BEHAV
   Buhl J, 2006, SCIENCE, V312, P1402, DOI 10.1126/science.1125142
   Castellano C, 2009, REV MOD PHYS, V81, P591, DOI 10.1103/RevModPhys.81.591
   Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569
   Chan AB, 2008, IEEE T PATTERN ANAL, V30, P909, DOI 10.1109/TPAMI.2007.70738
   Chaté H, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.046113
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Chen K, 2013, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2013.319
   Courty N, 2014, PATTERN RECOGN LETT, V44, P161, DOI 10.1016/j.patrec.2014.01.004
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dion K.L., 2000, GROUP DYN-THEOR RES, V4, P7, DOI [DOI 10.1037/1089-2699.4.1.7, 10.1037/1089-2699.4.1.7]
   Dong L, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1011
   Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4
   Ferris J, 2009, PALG STUD THEAT PERF, P1
   Fiaschi L, 2012, INT C PATT RECOG, P2685
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176
   HENDERSON LF, 1971, NATURE, V229, P381, DOI 10.1038/229381a0
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Idrees H, 2014, IMAGE VISION COMPUT, V32, P14, DOI 10.1016/j.imavis.2013.10.006
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Ikizler-Cinbis N, 2012, IEEE T MULTIMEDIA, V14, P1031, DOI 10.1109/TMM.2012.2187180
   Jacobs H., 1967, COLUMBIA JOURNAL REV, V5, P37
   Jain A, 2013, PROC CVPR IEEE, P2571, DOI 10.1109/CVPR.2013.332
   Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771
   Kuettel D, 2010, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2010.5539869
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Le Bon Gustave, 1897, The Crowd: A Study of the Popular Mind
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li J., 2008, ECCV, P383
   Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Loy CC, 2009, PROC CVPR IEEE, P1988, DOI 10.1109/CVPRW.2009.5206827
   Ma Z, 2013, PROC CVPR IEEE, P2539, DOI 10.1109/CVPR.2013.328
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Marana AN, 1998, SIBGRAPI '98 - INTERNATIONAL SYMPOSIUM ON COMPUTER GRAPHICS, IMAGE PROCESSING, AND VISION, PROCEEDINGS, P354, DOI 10.1109/SIBGRA.1998.722773
   McPhail Clark., 1991, The Myth of the Madding Crowd
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Moussaïd M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010047
   Moussaid M, 2009, TOP COGN SCI, V1, P469, DOI 10.1111/j.1756-8765.2009.01028.x
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Parrish JK, 1999, SCIENCE, V284, P99, DOI 10.1126/science.284.5411.99
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Rodriguez M, 2011, IEEE I CONF COMP VIS, P2423, DOI 10.1109/ICCV.2011.6126526
   Rodriguez M, 2011, IEEE I CONF COMP VIS, P1235, DOI 10.1109/ICCV.2011.6126374
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Shao J, 2014, PROC CVPR IEEE, P2227, DOI 10.1109/CVPR.2014.285
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   VICSEK T, 1995, PHYS REV LETT, V75, P1226, DOI 10.1103/PhysRevLett.75.1226
   Wang F, 2012, IEEE T MULTIMEDIA, V14, P76, DOI 10.1109/TMM.2011.2165531
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Yi S, 2014, PROC CVPR IEEE, P2219, DOI 10.1109/CVPR.2014.284
   Zhang HP, 2010, P NATL ACAD SCI USA, V107, P13626, DOI 10.1073/pnas.1001651107
   Zhao T, 2008, IEEE T PATTERN ANAL, V30, P1198, DOI 10.1109/TPAMI.2007.70770
   Zhou BL, 2012, LECT NOTES COMPUT SC, V7573, P857, DOI 10.1007/978-3-642-33709-3_61
   Zhou BL, 2014, IEEE T PATTERN ANAL, V36, P1586, DOI 10.1109/TPAMI.2014.2300484
   Zhou BL, 2013, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2013.392
   Zhou BL, 2012, PROC CVPR IEEE, P2871, DOI 10.1109/CVPR.2012.6248013
   Zhu F, 2014, LECT NOTES COMPUT SC, V8694, P139, DOI 10.1007/978-3-319-10599-4_10
NR 71
TC 74
Z9 83
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1048
EP 1061
DI 10.1109/TMM.2016.2542585
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100009
DA 2024-07-18
ER

PT J
AU Yang, K
   Liu, Z
   Jia, XH
   Shen, XS
AF Yang, Kan
   Liu, Zhen
   Jia, Xiaohua
   Shen, Xuemin Sherman
TI Time-Domain Attribute-Based Access Control for Cloud-Based Video Content
   Sharing: A Cryptographic Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE ABAC; cloud computing; MA-CP-ABE; multimedia; time-domain; time-domain
   attribute-based access control (TAAC); video content sharing
ID SELECTIVE ENCRYPTION; EFFICIENT; CHALLENGES; PROTECTION; NETWORKS;
   SECURITY; SERVICES; PRIVACY; MEDIA
AB With the ever-increasing demands on multimedia applications, cloud computing, due to its economical but powerful resources, is becoming a natural platform to process, store, and share multimedia contents. However, the employment of cloud computing also brings new security and privacy issues as few public cloud servers can be fully trusted by users. In this paper, we focus on how to securely share video contents to a certain group of people during a particular time period in cloud-based multimedia systems, and propose a cryptographic approach, a provably secure time-domain attribute-based access control (TAAC) scheme, to secure the cloud-based video content sharing. Specifically, we first propose a provably secure time-domain attribute-based encryption scheme by embedding the time into both the ciphertexts and the keys, such that only users who hold sufficient attributes in a specific time slot can decrypt the video contents. We also propose an efficient attribute updating method to achieve the dynamic change of users' attributes, including granting new attributes, revoking previous attributes, and regranting previously revoked attributes. We further discuss on how to control those video contents that can be commonly accessed in multiple time slots and how to make special queries on video contents generated in previous time slots. The security analysis and performance evaluation show that TAAC is provably secure in generic group model and efficient in practice.
C1 [Yang, Kan; Shen, Xuemin Sherman] Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
   [Liu, Zhen] Hong Kong Appl Sci & Technol Res Inst Co Ltd, Secur & Data Sci, Hong Kong, Hong Kong, Peoples R China.
   [Jia, Xiaohua] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 University of Waterloo; Hong Kong Applied Science & Technology Research
   Institute Company Limited (ASTRI); City University of Hong Kong
RP Yang, K; Shen, XS (corresponding author), Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.; Liu, Z (corresponding author), Hong Kong Appl Sci & Technol Res Inst Co Ltd, Secur & Data Sci, Hong Kong, Hong Kong, Peoples R China.; Jia, XH (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM kan.yang@uwaterloo.ca; liu.zhen@my.cityu.edu.hk; csjia@cityu.edu.hk;
   sshen@uwaterloo.ca
RI Yang, Kan/T-2616-2019; Shen, Xuemin/AAH-2564-2020
OI Yang, Kan/0000-0003-4234-9596; Shen, Xuemin/0000-0002-4140-287X; JIA,
   Xiaohua/0000-0001-8702-8302
CR Algin GB, 2011, J VIS COMMUN IMAGE R, V22, P353, DOI 10.1016/j.jvcir.2011.02.005
   [Anonymous], 2012, Public Key Cryptographic Algorithm SM2 Based on Elliptic Curves
   [Anonymous], 2011, P 6 ACM S INF COMP C, DOI DOI 10.1007/978-3-642-19379-8_19
   Bethencourt J, 2007, P IEEE S SECUR PRIV, P321, DOI 10.1109/sp.2007.11
   Boldyreva A, 2008, CCS'08: PROCEEDINGS OF THE 15TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P417
   Boneh D, 2005, LECT NOTES COMPUT SC, V3494, P440, DOI 10.1007/11426639_26
   Chase M, 2007, LECT NOTES COMPUT SC, V4392, P515
   Chase M, 2009, CCS'09: PROCEEDINGS OF THE 16TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P121
   Díaz-Sánchez D, 2011, IEEE T CONSUM ELECTR, V57, P970, DOI 10.1109/TCE.2011.5955247
   ELGAMAL T, 1985, IEEE T INFORM THEORY, V31, P469, DOI 10.1109/TIT.1985.1057074
   Goyal V., 2006, P 2006 INT C PRIVACY, P1
   Hefeeda M, 2015, IEEE T MULTIMEDIA, V17, P420, DOI 10.1109/TMM.2015.2389628
   Lewko A, 2011, LECT NOTES COMPUT SC, V6632, P568, DOI 10.1007/978-3-642-20465-4_31
   Mell P., 2009, 800145 NIST SP
   Naor D., 2001, Advances in Cryptology - CRTPTO 2001. 21st Annual International Cryptology Conference, Proceedings (Lecture Notes in Computer Science Vol.2139), P41
   Qiao L., 1997, Las Vegas : Proceedings of the 1s International Conference on Imaging Science, Systems and Technology, P21
   Ren J, 2015, COMPUT COMMUN, V65, P55, DOI 10.1016/j.comcom.2015.01.022
   Ren J, 2015, IEEE COMMUN MAG, V53, P98, DOI 10.1109/MCOM.2015.7060488
   Sahai A, 2012, LECT NOTES COMPUT SC, V7417, P199
   Shahid Z, 2014, IEEE T MULTIMEDIA, V16, P24, DOI 10.1109/TMM.2013.2281029
   Shen HJ, 2014, IET INFORM SECUR, V8, P199, DOI 10.1049/iet-ifs.2012.0349
   Shoup V., 1997, Advances in Cryptology - EUROCRYPT '97. International Conference on the Theory and Application of Cryptographic Techniques Proceedings, P256
   Stütz T, 2012, IEEE T CIRC SYST VID, V22, P325, DOI 10.1109/TCSVT.2011.2162290
   Wang XF, 2013, IEEE T MULTIMEDIA, V15, P811, DOI 10.1109/TMM.2013.2239630
   Waters B, 2011, LECT NOTES COMPUT SC, V6571, P53, DOI 10.1007/978-3-642-19379-8_4
   Wu YD, 2013, IEEE T MULTIMEDIA, V15, P778, DOI 10.1109/TMM.2013.2238910
   Yang K., Proceedings of the 8th ACM SIGSAC symposium on Information, computer and communications security, 2013, P523
   Yang K, 2015, IEEE T PARALL DISTR, V26, P3461, DOI 10.1109/TPDS.2014.2380373
   Yang K, 2015, IEEE COMMUN MAG, V53, P75, DOI 10.1109/MCOM.2015.7180511
   Yang K, 2014, IEEE T PARALL DISTR, V25, P1735, DOI 10.1109/TPDS.2013.253
   Yang K, 2013, IEEE T INF FOREN SEC, V8, P1790, DOI 10.1109/TIFS.2013.2279531
   Yang K, 2012, INT CON DISTR COMP S, P536, DOI 10.1109/ICDCS.2012.42
   Zeng B, 2014, IEEE T INF FOREN SEC, V9, P309, DOI 10.1109/TIFS.2013.2293955
   Zhang H, 2016, IEEE T CONTR SYST T, V24, P843, DOI 10.1109/TCST.2015.2462741
   Zhang K, 2014, IEEE COMMUN MAG, V52, P58, DOI 10.1109/MCOM.2014.6766086
   Zhang YM, 2016, IEEE ACM T NETWORK, V24, P1632, DOI 10.1109/TNET.2015.2425146
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 37
TC 60
Z9 62
U1 2
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2016
VL 18
IS 5
SI SI
BP 940
EP 950
DI 10.1109/TMM.2016.2535728
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DK5YD
UT WOS:000374996200013
DA 2024-07-18
ER

PT J
AU Wu, PP
   Liu, H
   Li, XF
   Fan, T
   Zhang, XW
AF Wu, Pingping
   Liu, Hong
   Li, Xiaofei
   Fan, Ting
   Zhang, Xuewu
TI A Novel Lip Descriptor for Audio-Visual Keyword Spotting Based on
   Adaptive Decision Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual fusion; keyword spotting; noisy conditions; visual speech
   recognition
ID SPEECH RECOGNITION; ALIGNMENT
AB Keyword spotting remainsa challenge when applied to real-world environments with dramatically changing noise. In recent studies, audio-visual integration methods have demonstrated superiorities since visual speech is not influenced by acoustic noise. However, for visual speech recognition, individual utterance mannerisms can lead to confusion and false recognition. To solve this problem, a novel lip descriptor is presented involving both geometry-based and appearance-based features in this paper. Specifically, a set of geometry-based features is proposed based on an advanced facial landmark localization method. In order to obtain robust and discriminative representation, a spatiotemporal lip feature is put forward concerning similarities among textons and mapping the feature to intra-class subspace. Moreover, a parallel two-step keyword spotting strategy based on decision fusion is proposed in order to make the best use of audio-visual speech and adapt to diverse noise conditions. Weights generated using a neural network combine acoustic and visual contributions. Experimental results on the OuluVS dataset and PKU-AV dataset demonstrate that the proposed lip descriptor shows competitive performance compared to the state of the art. Additionally, the proposed audio-visual keyword spotting (AV-KWS) method based on decision-level fusion significantly improves the noise robustness and attains better performance than feature-level fusion, which is also capable of adapting to various noisy conditions.
C1 [Wu, Pingping; Liu, Hong] Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing 100871, Peoples R China.
   [Wu, Pingping; Liu, Hong; Fan, Ting; Zhang, Xuewu] Peking Univ, Shenzhen Grad Sch, Engn Lab Intelligent Percept Internet Things ELIP, Beijing 100871, Peoples R China.
   [Li, Xiaofei] INRIA Grenoble Rhone Alpes, PERCEPTION Team, F-28330 Grenoble, France.
C3 Peking University; Peking University
RP Liu, H (corresponding author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing 100871, Peoples R China.
EM wupingping@pku.edu.cn; hongliu@pku.edu.cn; xiaofei.li@inria.fr;
   fanting19900126@126.com; zhangxuewu@sz.pku.edu.cn
RI Li, xiaofei/GXF-7187-2022
FU National Natural Science Foundation of China (NSFC) [61340046, 60875050,
   60675025]; National High Technology Research and Development Programme
   of China (863 Programme) [2006AA04Z247]; Guangdong Natural Science
   Foundation of China [2015A030311034]; Specialized Research Fund for the
   Doctoral Programme of Higher Education [20130001110011]
FX This work is supported by the National Natural Science Foundation of
   China (NSFC) under Grant 61340046, Grant 60875050, and Grant 60675025,
   by the National High Technology Research and Development Programme of
   China (863 Programme) under Grant 2006AA04Z247, by the Guangdong Natural
   Science Foundation of China under Grant 2015A030311034, and by the
   Specialized Research Fund for the Doctoral Programme of Higher Education
   under Grant 20130001110011. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Martha
   Larson.
CR Abdelaziz AH, 2015, IEEE-ACM T AUDIO SPE, V23, P863, DOI 10.1109/TASLP.2015.2409785
   Abdelhamid A.A., 2012, Proceedings of the International Conference on Imaging and Signal Processing for Healthcare and Technology (ISPHT'12), P150
   Adjoudani A., 1996, Speechreading by Humans and Machines, P461
   Akbacak M, 2013, INT CONF ACOUST SPEE, P8267, DOI 10.1109/ICASSP.2013.6639277
   [Anonymous], P ICSLP2000
   [Anonymous], 2011, PROC CVPR IEEE
   Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Duffy N, 2002, MACH LEARN, V47, P153, DOI 10.1023/A:1013685603443
   Estellers V, 2012, IEEE T AUDIO SPEECH, V20, P1145, DOI 10.1109/TASL.2011.2172427
   Foote JT, 1997, COMPUT SPEECH LANG, V11, P207, DOI 10.1006/csla.1997.0027
   Gurban M, 2009, IEEE T SIGNAL PROCES, V57, P4765, DOI 10.1109/TSP.2009.2026513
   Harte N, 2015, IEEE T MULTIMEDIA, V17, P603, DOI 10.1109/TMM.2015.2407694
   Ishi CT, 2008, IEEE T ROBOT, V24, P759, DOI 10.1109/TRO.2008.919305
   Jesorsky O, 2001, LECT NOTES COMPUT SC, V2091, P90
   Jiang H, 2010, COMPUT SPEECH LANG, V24, P589, DOI 10.1016/j.csl.2009.08.002
   Jong-Seok Lee, 2008, Speech Recognition - Technologies and Applications, P275
   Keshet J, 2009, SPEECH COMMUN, V51, P317, DOI 10.1016/j.specom.2008.10.002
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Lee JS, 2008, IEEE T MULTIMEDIA, V10, P767, DOI 10.1109/TMM.2008.922789
   Lewis T.W., 2004, P 27 AUSTRALASIAN C, V26, P305
   Li XF, 2013, IEEE T CYBERNETICS, V43, P1199, DOI 10.1109/TSMCB.2012.2226443
   Liang L, 2008, LECT NOTES COMPUT SC, V5303, P72, DOI 10.1007/978-3-540-88688-4_6
   Liu H, 2014, IEEE INT CONF ROBOT, P6644, DOI 10.1109/ICRA.2014.6907840
   Liu M, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P785
   Matthews I, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P38, DOI 10.1109/ICSLP.1996.607019
   Mitra Vikramjit, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P7143, DOI 10.1109/ICASSP.2014.6854986
   Morgan N, 2012, IEEE T AUDIO SPEECH, V20, P7, DOI 10.1109/TASL.2011.2116010
   Motlicek P, 2012, INT CONF ACOUST SPEE, P4413, DOI 10.1109/ICASSP.2012.6288898
   Pei YR, 2013, IEEE I CONF COMP VIS, P129, DOI 10.1109/ICCV.2013.23
   Potamianos G., 2004, ISSUES VISUAL AUDIO
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rogozan A, 1998, SPEECH COMMUN, V26, P149, DOI 10.1016/S0167-6393(98)00056-9
   ROSE RC, 1990, INT CONF ACOUST SPEE, P129, DOI 10.1109/ICASSP.1990.115555
   Shivappa Shankar T., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P107, DOI 10.1109/CVPR.2009.5204224
   Shivappa ST, 2010, IEEE J-STSP, V4, P882, DOI 10.1109/JSTSP.2010.2057890
   Soltau H., 2014, NATURAL LANGUAGE PRO, P409
   Szoke I., 2005, P JOINT WORKSH MULT
   Tamura S, 2005, INT CONF ACOUST SPEE, P469
   Tariquzzaman M., 2011, Proceedings of the 2011 International Conference on Internet Computing and Information Services (ICICIS 2011), P203, DOI 10.1109/ICICIS.2011.58
   Wan SH, 2014, PATTERN RECOGN, V47, P1859, DOI 10.1016/j.patcog.2013.11.025
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Weng C, 2015, IEEE-ACM T AUDIO SPE, V23, P300, DOI 10.1109/TASLP.2014.2381931
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Young S., 2009, HTK HIDDEN MARKOV MO
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
   Zhou ZH, 2014, IMAGE VISION COMPUT, V32, P590, DOI 10.1016/j.imavis.2014.06.004
   Zhou ZH, 2014, IEEE T PATTERN ANAL, V36, P181, DOI 10.1109/TPAMI.2013.173
   Zhu SC, 2005, INT J COMPUT VISION, V62, P121, DOI 10.1007/s11263-005-4638-1
NR 52
TC 37
Z9 39
U1 0
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 326
EP 338
DI 10.1109/TMM.2016.2520091
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Qian, SS
   Zhang, TZ
   Xu, CS
   Shao, J
AF Qian, Shengsheng
   Zhang, Tianzhu
   Xu, Changsheng
   Shao, Jie
TI Multi-Modal Event Topic Model for Social Event Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Event evolution; multi-modality; social event tracking; social media;
   topic model
AB With the massive growth of social events in Internet, it has become more and more difficult to exactly find and organize the interesting events from massive social media data, which is useful to browse, search, and monitor social events by users or governments. To deal with this problem, we propose a novel multi-modal social event tracking and evolution framework to not only effectively capture multi-modal topics of social events, but also obtain the evolutionary trends of social events and generate effective event summary details over time. To achieve this goal, we propose a novel multi-modal event topic model (mmETM), which can effectively model social media documents, including long text with related images, and learn the correlations between textual and visual modalities to separate the visual-representative topics and non-visual-representative topics. To apply the mmETM model to social event tracking, we adopt an incremental learning strategy denoted as incremental mmETM, which can obtain informative textual and visual topics of social events over time to help understand these events and their evolutionary trends. To evaluate the effectiveness of our proposed algorithm, we collect a real-world dataset to conduct various experiments. Both qualitative and quantitative evaluations demonstrate that the proposed mmETM algorithm performs favorably against several state-of-the-art methods.
C1 [Qian, Shengsheng; Zhang, Tianzhu] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Shao, Jie] Univ Elect Sci & Technol, Sch Comp Sci & Engn, Big Media Comp Ctr, Chengdu 610051, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; University of
   Electronic Science & Technology of China
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM shengsheng.qian@nlpr.ia.ac.cn; tzzhang@nlpr.ia.ac.cn;
   shaojie@uestc.edu.cn
RI Zhang, Tianzhu/AGY-9389-2022; xu, cj/HJZ-3488-2023
OI Zhang, Tianzhu/0000-0003-0764-6106; 
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009, 61432019, 61572498,
   61303173, 61532009, 61472115, 61472379, U1435211, 61572296]; Beijing
   Natural Science Foundation [4131004]
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2012CB316304, in part by the National Natural
   Science Foundation of China under Grant 61225009, Grant 61432019, Grant
   61572498, Grant 61303173, Grant 61532009, Grant 61472115, Grant
   61472379, Grant U1435211, and Grant 61572296, and in part by the Beijing
   Natural Science Foundation under Grant 4131004. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. K. Selcuk Candan. (Corresponding author:
   Changsheng Xu.)
CR Allan J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P37, DOI 10.1145/290941.290954
   AlSumait L, 2008, IEEE DATA MINING, P3, DOI 10.1109/ICDM.2008.140
   [Anonymous], 2008, P 31 ANN INT ACM SIG, DOI DOI 10.1145/1390334.1390387
   [Anonymous], 2013, PROC THE 21 ACM INT
   [Anonymous], 2008, Advances in Neural Information Processing Systems
   [Anonymous], 2006, Adv. Neural Inf. Process. Syst
   [Anonymous], 2008, P ADV NEURAL INFORM
   [Anonymous], 2012, P 20 ACM INT C MULTI
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Blei D.M., 2006, INT C MACHINE LEARNI, DOI [DOI 10.1145/1143844.1143859, 10.1145/1143844.1143859]
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Chen X, 2012, IEEE T MULTIMEDIA, V14, P3, DOI 10.1109/TMM.2011.2167223
   Diakopoulos N., 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P115, DOI 10.1109/VAST.2010.5652922
   Dodge Jesse., 2012, HLT-NAACL, P762
   Firan C.S., 2010, Proceedings of the 19th ACM international conference on Information and knowledge management, P189
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Haghighi Aria, 2009, P HUMAN LANGUAGE TEC, P362, DOI DOI 10.3115/1620754.1620807
   Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289
   Hong L, 2011, Proceedings of the 17th ACM SIGKDD Interna- tional Conference on Knowledge Discovery and Data Mining, P832
   Hu X., 2013, WSDM
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Kalamaras I, 2014, IEEE T MULTIMEDIA, V16, P1460, DOI 10.1109/TMM.2014.2316473
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Lin YR, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071400
   Liu W, 2014, IEEE T MULTIMEDIA, V16, P2242, DOI 10.1109/TMM.2014.2359332
   Liu XC, 2013, PROCEEDINGS OF THE 1ST INTERNATIONAL JOINT SYMPOSIUM ON JOINING AND WELDING, P151
   Makkonen J, 2004, INFORM RETRIEVAL, V7, P347, DOI 10.1023/B:INRT.0000011210.12953.86
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Patel D., 2008, P 2008 ACM SIGMOD IN, P393, DOI DOI 10.1145/1376616.1376658
   Putthividhya D, 2010, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2010.5540000
   Qian S., 2014, TOMCCAP, V11
   Qian SS, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P99, DOI 10.1145/2733373.2806234
   Reuter T., 2012, PROC ANN ACM INT C M, P22
   Reuter Timo., 2013, P MEDIAEVAL 2013 MUL, P1
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Sun AX, 2011, SOCIAL MEDIA MODELING AND COMPUTING, P3, DOI 10.1007/978-0-85729-436-4_1
   Virtanen S., 2012, Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence, UAI '12, P843
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xie L, 2004, IEEE IMAGE PROC, P2383
   Yang X., 2015, TOMCCAP, V11
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang Y., 2002, P 8 ACM SIGKDD INT C, P688, DOI DOI 10.1145/775047.775150
   Yang YM, 1999, IEEE INTELL SYST APP, V14, P32, DOI 10.1109/5254.784083
   Yihong Gong, 2001, SIGIR Forum, P19
   Zhang TZ, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2602633
   Zhang TZ, 2012, IEEE T MULTIMEDIA, V14, P1206, DOI 10.1109/TMM.2012.2191944
   Zhao Y, 2004, MACH LEARN, V55, P311, DOI 10.1023/B:MACH.0000027785.44527.d6
   Zhou WB, 2014, 2014 IEEE 15TH INTERNATIONAL CONFERENCE ON INFORMATION REUSE AND INTEGRATION (IRI), P585, DOI 10.1109/IRI.2014.7051942
NR 52
TC 118
Z9 122
U1 3
U2 53
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2016
VL 18
IS 2
BP 233
EP 246
DI 10.1109/TMM.2015.2510329
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DB3HX
UT WOS:000368402400008
DA 2024-07-18
ER

PT J
AU Sunderrajan, S
   Manjunath, BS
AF Sunderrajan, Santhoshkumar
   Manjunath, B. S.
TI Context-Aware Hypergraph Modeling for Re-identification and
   Summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Camera network; person re-identification; search; summarization
ID PERSON REIDENTIFICATION; TRACKING
AB Tracking and re-identification in wide-area camera networks is a challenging problem due to non-overlapping visual fields, varying imaging conditions, and appearance changes. We consider the problem of person re-identification and tracking, and propose a novel clothing context-aware color extraction method that is robust to such changes. Annotated samples are used to learn color drift patterns in a non-parametric manner using the random forest distance (RFD) function. The color drift patterns are automatically transferred to associate objects across different views using a unified graph matching framework. A hypergraph representation is used to link related objects for search and re-identification. A diverse hypergraph ranking technique is proposed for person-focused network summarization. The proposed algorithm is validated on a wide-area camera network consisting of ten cameras on bike paths. Also, the proposed algorithm is compared with the state of the art person re-identification algorithms on the VIPeR dataset [1].
C1 [Sunderrajan, Santhoshkumar; Manjunath, B. S.] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
C3 University of California System; University of California Santa Barbara
RP Sunderrajan, S (corresponding author), Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
EM santhosh@ece.ucsb.edu; manj@ece.ucsb.edu
RI Manjunath, B S/AAM-8190-2020
OI Manjunath, B S/0000-0003-2804-3611
FU ONR [N00014-12-1-0503]; Army Research Laboratory [W911NF-09-2-0053]
FX This work was supported by the ONR under Grant N00014-12-1-0503 and by
   the Army Research Laboratory under Cooperative Agreement Number
   W911NF-09-2-0053 (the ARL Network Science CTA).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2009, IEEE I CONF COMP VIS, DOI 10.1109/ICCV.2009.5459175
   [Anonymous], SIDLWP19990120 STAND
   [Anonymous], 2012, ACM SIGKDD
   [Anonymous], P ADV NEURAL INFORM
   [Anonymous], CORR
   [Anonymous], 2010, P BRIT MACH VIS C
   [Anonymous], P IEEE C MULT EXP JU
   Arth Clemens, 2007, 2007 First ACM/IEEE International Conference on Distributed Smart Cameras, P156, DOI 10.1109/ICDSC.2007.4357519
   Cour T., 2007, Advances in Neural Information Processing Systems, V19, P313
   Davis J. V., 2007, ICML, P209
   Du Pan., 2010, PROC ACM C INFORM KN, P1757
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Frome A., 2007, NIPS, V19, P417
   Gallagher AC, 2008, PROC CVPR IEEE, P1073
   Gheissari N., 2006, 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06), V2, P1528
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197
   He J., P 12 ANN ACM INT C M, P9, DOI 10.1145/1027527.1027531
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Huang YC, 2010, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2010.5540012
   Javed O, 2005, PROC CVPR IEEE, P26
   Javed O, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P952
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Kuo CH, 2013, IEEE WORK APP COMP, P281, DOI 10.1109/WACV.2013.6475030
   Li L, 2003, ICCAD-2003: IEEE/ACM DIGEST OF TECHNICAL PAPERS, P2
   Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62
   Mazzeo PL, 2009, LECT NOTES COMPUT SC, V5807, P516
   Mei Q., 2010, KDD, P1009
   Ming Yang, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2937, DOI 10.1109/ICIP.2011.6116276
   Ni Zefeng., 2012, Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Confer- ence on, P7
   Park U, 2006, INT C PATT RECOG, P1204
   Prosser B., 2008, P BMVC, V8, P164
   Sankaranarayanan AC, 2008, P IEEE, V96, P1606, DOI 10.1109/JPROC.2008.928758
   Sugar CA, 2003, J AM STAT ASSOC, V98, P750, DOI 10.1198/016214503000000666
   Sunderrajan S., 2013, ICDSC, P1
   Tao H, 2007, 10 INT WORKSH PERF E
   Thakoor NS, 2015, COMPUTER, V48, P78, DOI 10.1109/MC.2015.83
   Wang XQ, 2007, INT J THERM SCI, V46, P1, DOI 10.1016/j.ijthermalsci.2006.06.010
   Wu G., 2006, IEEE C COMPUTER VISI, P561
   Wu L., 2009, Advances in Neural Information Processing Systems, V22, P2089
   Xu JJ, 2013, IEEE T MULTIMEDIA, V15, P2046, DOI 10.1109/TMM.2013.2281019
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zhu X., 2007, P HUM LANG TECHN C N, P97
NR 47
TC 26
Z9 28
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2016
VL 18
IS 1
BP 51
EP 63
DI 10.1109/TMM.2015.2496139
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CZ5JW
UT WOS:000367139700006
OA Green Published
DA 2024-07-18
ER

PT J
AU Abdulnabi, AH
   Wang, G
   Lu, JW
   Jia, K
AF Abdulnabi, Abrar H.
   Wang, Gang
   Lu, Jiwen
   Jia, Kui
TI Multi-Task CNN Model for Attribute Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep CNN; latent tasks matrix; multi-task learning; semantic attributes
ID OBJECT CLASSES; CLASSIFICATION
AB This paper proposes a joint multi-task learning algorithm to better predict attributes in images using deep convolutional neural networks (CNN). We consider learning binary semantic attributes through a multi-task CNN model, where each CNN will predict one binary attribute. The multi-task learning allows CNN models to simultaneously share visual knowledge among different attribute categories. Each CNN will generate attribute-specific feature representations, and then we apply multi-task learning on the features to predict their attributes. In our multi-task framework, we propose a method to decompose the overall model's parameters into a latent task matrix and combination matrix. Furthermore, under-sampled classifiers can leverage shared statistics from other classifiers to improve their performance. Natural grouping of attributes is applied such that attributes in the same group are encouraged to share more knowledge. Meanwhile, attributes in different groups will generally compete with each other, and consequently share less knowledge. We show the effectiveness of our method on two popular attribute datasets.
C1 [Abdulnabi, Abrar H.] Nanyang Technol Univ, Rapid Rich Object Search Lab, Singapore 637553, Singapore.
   [Abdulnabi, Abrar H.] Illinois Singapore Pte Ltd, Adv Digital Sci Ctr, Singapore 138632, Singapore.
   [Wang, Gang] Nanyang Technol Univ, Sch Elect & Elect Engn, Dept Elect & Elect Engn, Singapore 637553, Singapore.
   [Lu, Jiwen] Adv Digital Sci Ctr, Dept Automat, Singapore 138632, Singapore.
   [Lu, Jiwen] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Jia, Kui] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
C3 Nanyang Technological University; Nanyang Technological University;
   Tsinghua University; University of Macau
RP Abdulnabi, AH (corresponding author), Nanyang Technol Univ, Rapid Rich Object Search Lab, Singapore 637553, Singapore.
EM abrarham001@ntu.edu.sg; wanggang@ntu.edu.sg; jiwen.lu@adsc.com.sg;
   kuijia@umac.mo
RI Wang, Gang/B-7027-2013; Lu, Jiwen/C-5291-2009
OI Lu, Jiwen/0000-0002-6121-5529
FU National Research Foundation, Singapore, under its Interactive & Digital
   Media (IDM) Strategic Research Programme
FX The authors would like to thank NVIDIA Corporation for their donation of
   Tesla K40 GPUs used in this research at the Rapid-Rich Object Search
   Lab. This research was carried out at both the Advanced Digital Sciences
   Center (ADSC), Illinois at Singapore Pte Ltd., Singapore, and at the
   Rapid-Rich Object Search (ROSE) Laboratory, Nanyang Technological
   University, Singapore. The ROSE Lab is supported by the National
   Research Foundation, Singapore, under its Interactive & Digital Media
   (IDM) Strategic Research Programme.
CR [Anonymous], 2014, P BMVC
   [Anonymous], 2010, P ICML
   [Anonymous], 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587383
   [Anonymous], CORR
   [Anonymous], LECT NOTES COMPUT SC
   [Anonymous], SIAM J OPTIMIZ UNPUB
   [Anonymous], 2014, ARXIV14065726
   [Anonymous], CORR
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2013, CORR
   Argyriou A., 2007, NIPS, P41
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chang SY, 2013, IEEE DATA MINING, P979, DOI 10.1109/ICDM.2013.49
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Chen Y., 2014, Convolutional neural network for sentence classification (Master's thesis)
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4
   Donahue J., 2013, Decaf: A deep convolutional activation feature for generic visual recognition
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Ferrari V., 2007, P 20 INT C NEUR INF, P433
   Gallagher AC, 2008, PROC CVPR IEEE, P1073
   Gong Pinghua, 2012, KDD, V2012, P895
   Gong Yunchao., 2013, Deep convolutional ranking for multilabel image annotation
   Hwang SJ, 2011, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2011.5995543
   Jayaraman D., 2014, CORR
   Jayaraman D, 2014, PROC CVPR IEEE, P1629, DOI 10.1109/CVPR.2014.211
   Kim S, 2012, ANN APPL STAT, V6, P1095, DOI 10.1214/12-AOAS549
   Kim Seyoung, 2010, ICML, P543
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar A., 2012, P 29 INT C MACH LEAR, P1383
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li CT, 2014, PR MACH LEARN RES, V32, P415
   Ma SG, 2012, LECT NOTES COMPUT SC, V7585, P61, DOI 10.1007/978-3-642-33885-4_7
   Ozeki M., 2014, Asian conference on computer vision, P362
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Rai P, 2010, P 13 INT C ARTIFICIA, P613
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Read J, 2009, LECT NOTES ARTIF INT, V5782, P254, DOI 10.1007/978-3-642-04174-7_17
   Salakhutdinov R, 2011, PROC CVPR IEEE, P1481, DOI 10.1109/CVPR.2011.5995720
   Sandeep RN, 2014, PROC CVPR IEEE, P3614, DOI 10.1109/CVPR.2014.462
   Scheirer WJ, 2012, PROC CVPR IEEE, P2933, DOI 10.1109/CVPR.2012.6248021
   Siddiquie B, 2011, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2011.5995329
   Simonyan K., 2014, CORR
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1, DOI DOI 10.4018/JDWM.2007070101
   Vedaldi A, 2014, PROC CVPR IEEE, P3622, DOI 10.1109/CVPR.2014.463
   Wang G, 2009, IEEE I CONF COMP VIS, P537, DOI 10.1109/ICCV.2009.5459194
   Xi C, 2012, IEEE IJCNN, P1
   Yang M., 2013, P 30 INT C MACH LEAR, P423
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang Y., 2010, Advances in Neural Information Processing Systems, P2550
   Zhou Q, 2013, IEEE I CONF COMP VIS, P2264, DOI 10.1109/ICCV.2013.281
NR 57
TC 179
Z9 209
U1 5
U2 67
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 1949
EP 1959
DI 10.1109/TMM.2015.2477680
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400008
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Zhang, XS
   Yang, Y
   Zhang, YD
   Luan, HB
   Li, JT
   Zhang, HW
   Chua, TS
AF Zhang, Xishan
   Yang, Yang
   Zhang, Yongdong
   Luan, Huanbo
   Li, Jintao
   Zhang, Hanwang
   Chua, Tat-Seng
TI Enhancing Video Event Recognition Using Automatically Constructed
   Semantic-Visual Knowledge Base
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Concept detection; event recognition; knowledge base
AB The task of recognizing events from video has attracted a lot of attention in recent years. However, due to the complex nature of user-defined events, the use of purely audio-visual content analysis without domain knowledge has been found to be grossly inadequate. In this paper, we propose to construct a semantic-visual knowledge base to encode the rich event-centric concepts and their relationships from the well-established lexical databases, including FrameNet, as well as the concept-specific visual knowledge from ImageNet. Based on this semantic-visual knowledge bases, we design an effective system for video event recognition. Specifically, in order to narrow the semantic gap between the high-level complex events and low-level visual representations, we utilize the event-centric semantic concepts encoded in the knowledge base as the intermediate-level event representation, which offers both human-perceivable and machine-interpretable semantic clues for event recognition. In addition, in order to leverage the abundant ImageNet images, we propose a robust transfer learning model to learn the noise-resistant concept classifiers for videos. Extensive experiments on various real-world video datasets demonstrate the superiority of our proposed system as compared to the state-of-the-art approaches.
C1 [Zhang, Xishan; Zhang, Yongdong; Li, Jintao] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Zhang, Xishan] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Yang, Yang] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Zhang, Hanwang; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Luan, Huanbo] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; University of Electronic Science & Technology of China; National
   University of Singapore; Tsinghua University
RP Zhang, XS (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM zhangxishan@ict.ac.cn; dlyyang@gmail.com; zhyd@ict.ac.cn;
   luanhuanbo@gmail.com; jtli@ict.ac.cn; hanwang@comp.nus.edu.sg;
   chuats@comp.nus.edu.sg
RI yang, yang/HGT-7999-2022; li, xiaomin/KCX-9845-2024; Lang,
   Ming/HIK-0758-2022; yang, yang/GVT-5210-2022
OI Zhang, Hanwang/0000-0001-7374-8739
FU National High Technology and Research Development Program of China under
   the 863 Program [2014AA015202]; National Natural Science Foundation of
   China [61303075]
FX This work was supported by the National High Technology and Research
   Development Program of China under the 863 Program, Grant 2014AA015202,
   and by the National Natural Science Foundation of China under Grant
   61303075. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Enrico Magli.
CR [Anonymous], 2013, P 31 INT C MACHINE L
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], ACM INT C MULT RETR
   [Anonymous], P 20 ACM INT C MULT
   [Anonymous], 2010, P NIPS
   [Anonymous], 2013, P 3 ACM C INT C MULT
   [Anonymous], PROC CVPR IEEE
   [Anonymous], ACM INT C MULT
   [Anonymous], 2012, Long Papers
   [Anonymous], P ACM MM
   Baker C.F., 1998, P 36 ANN M ASS COMP, P86, DOI [DOI 10.3115/980845.980860, DOI 10.3115/980451.980860]
   Ballan L, 2010, MULTIMED TOOLS APPL, V48, P313, DOI 10.1007/s11042-009-0342-4
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Buitelaar P., 2006, LREC, P2321
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong J, 2013, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.2013.112
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Duan LX, 2009, PROC CVPR IEEE, P1375, DOI [10.1109/CVPRW.2009.5206747, 10.1109/CVPR.2009.5206747]
   Habibian A, 2013, P 3 ACM C INT C MULT, P89, DOI DOI 10.1145/2461466.2461482
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Izadinia H, 2012, LECT NOTES COMPUT SC, V7575, P430, DOI 10.1007/978-3-642-33765-9_31
   Liu JE, 2013, IEEE WORK APP COMP, P339, DOI 10.1109/WACV.2013.6475038
   Ma Z., 2012, MM 2012 - Proceedings of the 20th ACM International Conference on Multimedia, P469, DOI DOI 10.1145/2393347.2393414
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Natarajan P, 2012, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2012.6247814
   Natsev A., 2005, 13th Annual ACM International Conference on Multimedia, P598, DOI 10.1145/1101149.1101288
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Over P., 2011, P TRECVID 2010, P10
   Raina R., 2007, P 24 INT C MACH LEAR, P759
   Ramanathan V, 2013, IEEE I CONF COMP VIS, P905, DOI 10.1109/ICCV.2013.117
   Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318
   Sun C, 2013, IEEE I CONF COMP VIS, P913, DOI 10.1109/ICCV.2013.453
   Sun C, 2013, IEEE WORK APP COMP, P15, DOI 10.1109/WACV.2013.6474994
   Tamrakar A, 2012, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2012.6248114
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Yang T., 2012, CORR, Vabs/1206. 4629
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Yang Y, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457456
NR 42
TC 29
Z9 29
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1562
EP 1575
DI 10.1109/TMM.2015.2449660
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000016
DA 2024-07-18
ER

PT J
AU Miller, K
   Bethanabhotla, D
   Caire, G
   Wolisz, A
AF Miller, Konstantin
   Bethanabhotla, Dilip
   Caire, Giuseppe
   Wolisz, Adam
TI A Control-Theoretic Approach to Adaptive Video Streaming in Dense
   Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HTTP-based adaptive streaming; MPEG-DASH; small-cell wireless networks;
   streaming media
ID LAYER RESOURCE-ALLOCATION; QUALITY; ADAPTATION; STABILITY; EFFICIENCY
AB Recently, the way people consume video content has been undergoing a dramatic change. Plain TV sets, that have been the center of home entertainment for a long time, are losing ground to hybrid TVs, PCs, game consoles, and, more recently, mobile devices such as tablets and smartphones. The new predominant paradigm is: watch what I want, when I want, and where I want. The challenges of this shift are manifold. On the one hand, broadcast technologies such as DVB-T/C/S need to be extended or replaced by mechanisms supporting asynchronous viewing, such as IPTV and video streaming over best-effort networks, while remaining scalable to millions of users. On the other hand, the dramatic increase of wireless data traffic begins to stretch the capabilities of the existing wireless infrastructure to its limits. Finally, there is a challenge to video streaming technologies to cope with a high heterogeneity of end-user devices and dynamically changing network conditions, in particular in wireless and mobile networks. In the present work, our goal is to design an efficient system that supports a high number of unicast streaming sessions in a dense wireless access network. We address this goal by jointly considering the two problems of wireless transmission scheduling and video quality adaptation, using techniques inspired by the robustness and simplicity of proportional-integral-derivative (PID) controllers. We show that the control-theoretic approach allows to efficiently utilize available wireless resources, providing high quality of experience (QoE) to a large number of users.
C1 [Miller, Konstantin; Caire, Giuseppe; Wolisz, Adam] Tech Univ Berlin, D-10623 Berlin, Germany.
   [Bethanabhotla, Dilip] Univ So Calif, Los Angeles, CA 90089 USA.
C3 Technical University of Berlin; University of Southern California
RP Miller, K (corresponding author), Tech Univ Berlin, D-10623 Berlin, Germany.
EM konstantin.miller@tu-berlin.de; bethanab@usc.edu; caire@tu-berlin.de;
   adam.wolisz@tu-berlin.de
RI Caire, Giuseppe/Q-7275-2018
OI Caire, Giuseppe/0000-0002-7749-1333; Wolisz, Adam/0000-0002-2969-1234
FU FITweltweit Program, German Academic Exchange Service (DAAD)
FX The work of K. Miller was supported in part by the FITweltweit Program,
   German Academic Exchange Service (DAAD). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Wenwu Zhu.
CR Alvarez-Ramirez J, 2003, AUTOMATICA, V39, P989, DOI 10.1016/S0005-1098(03)00035-9
   [Anonymous], 2014, CONVIVA 2014 VIEW EX
   [Anonymous], 2012, 230091 ISOIEC
   [Anonymous], 2014, Cisco visual networking index: Forecast and methodology, 2013 - 2018
   [Anonymous], 2014, P 13 ACM WORKSH HOT
   [Anonymous], 2007, Tech. Rep. D1.1.2
   [Anonymous], 2010, Wireless Communications
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Bethanabhotla D, 2015, IEEE T COMMUN, V63, P268, DOI 10.1109/TCOMM.2014.2378774
   Carlucci G., 2015, Proceedings of the 30th ACM/SIGAPP Symposium on Applied Computing, P1
   Chandrasekhar V, 2008, IEEE COMMUN MAG, V46, P59, DOI 10.1109/MCOM.2008.4623708
   Chen C, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2337277
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   Cicalò S, 2014, IEEE T MULTIMEDIA, V16, P848, DOI 10.1109/TMM.2014.2300442
   Djama I, 2008, IEEE T MULTIMEDIA, V10, P105, DOI 10.1109/TMM.2007.911243
   Evensen K, 2012, SIGNAL PROCESS-IMAGE, V27, P312, DOI 10.1016/j.image.2011.10.007
   Finamore A., 2011, ACM IMC, P345, DOI DOI 10.1145/2068816.2068849
   Golrezaei N, 2013, IEEE COMMUN MAG, V51, P142, DOI 10.1109/MCOM.2013.6495773
   Guo Y, 2004, IEEE T MULTIMEDIA, V6, P387, DOI 10.1109/TMM.2003.822786
   He J, 2014, IEEE T MULTIMEDIA, V16, P242, DOI 10.1109/TMM.2013.2284894
   HOPPENSTEADT F, 1974, J DIFFER EQUATIONS, V15, P510, DOI 10.1016/0022-0396(74)90070-9
   Hu H, 2013, IEEE T MULTIMEDIA, V15, P1638, DOI 10.1109/TMM.2013.2266092
   Huang YS, 2009, IEEE T MULTIMEDIA, V11, P1072, DOI 10.1109/TMM.2009.2026085
   Huh H, 2012, IEEE T WIREL COMMUN, V11, P3226, DOI 10.1109/TWC.2012.070912.111383
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Joseph V, 2014, IEEE INFOCOM SER, P82, DOI 10.1109/INFOCOM.2014.6847927
   Lederer S., 2012, P 3 MULT SYST C, P89
   Leighton T, 2009, COMMUN ACM, V52, P44, DOI 10.1145/1461928.1461944
   Li MD, 2013, IEEE T MULTIMEDIA, V15, P1519, DOI 10.1109/TMM.2013.2267207
   Li Y, 2006, IEEE T MULTIMEDIA, V8, P830, DOI 10.1109/TMM.2006.876236
   Liu CH, 2012, SIGNAL PROCESS-IMAGE, V27, P288, DOI 10.1016/j.image.2011.10.001
   Liu X, 2012, ACM SIGCOMM COMP COM, V42, P359, DOI 10.1145/2377677.2377752
   Miller K., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P173, DOI 10.1109/PV.2012.6229732
   Miller K., 2013, PACK VID WORKSH PV 2, P1
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   Narwaria M, 2012, IEEE T MULTIMEDIA, V14, P525, DOI 10.1109/TMM.2012.2190589
   Ong EH, 2011, 2011 IEEE 22ND INTERNATIONAL SYMPOSIUM ON PERSONAL INDOOR AND MOBILE RADIO COMMUNICATIONS (PIMRC), P849, DOI 10.1109/PIMRC.2011.6140087
   Reiter U, 2014, T-LAB SER TELECOMMUN, P55, DOI 10.1007/978-3-319-02681-7_4
   Rusek F, 2013, IEEE SIGNAL PROC MAG, V30, P40, DOI 10.1109/MSP.2011.2178495
   Saki H, 2015, IEEE T MULTIMEDIA, V17, P333, DOI 10.1109/TMM.2015.2389032
   Sesia S., 2009, LTE: the Long Term Evolution-From theory to practice
   Seufert M, 2013, INT WORK QUAL MULTIM, P52, DOI 10.1109/QoMEX.2013.6603210
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Tarbouriech S, 2009, IET CONTROL THEORY A, V3, P1, DOI 10.1049/iet-cta:20070435
   Trammell B, 2014, IEEE INTERNET COMPUT, V18, P60, DOI 10.1109/MIC.2014.91
   Tse D., 2005, Fundementals of Wireless Communications
   Vetro A, 2005, IEEE T MULTIMEDIA, V7, P418, DOI 10.1109/TMM.2005.846795
   Yan JY, 2012, IEEE T MULTIMEDIA, V14, P1579, DOI 10.1109/TMM.2012.2187182
   Yim C, 2011, SIGNAL PROCESS-IMAGE, V26, P24, DOI 10.1016/j.image.2010.11.002
   Zhu XQ, 2013, IEEE INT WORKSH MULT, P230, DOI 10.1109/MMSP.2013.6659293
NR 51
TC 43
Z9 49
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1309
EP 1322
DI 10.1109/TMM.2015.2441002
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kang, LW
   Hsu, CC
   Zhuang, BQ
   Lin, CW
   Yeh, CH
AF Kang, Li-Wei
   Hsu, Chih-Chung
   Zhuang, Boqi
   Lin, Chia-Wen
   Yeh, Chia-Hung
TI Learning-Based Joint Super-Resolution and Deblocking for a Highly
   Compressed Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dictionary learning; image decomposition; image super-resolution;
   morphological component analysis (MCA); self-learning; sparse
   representation
ID SPARSE; VIDEO; REPRESENTATIONS; DECOMPOSITION; REDUCTION
AB A highly compressed image is usually not only of low resolution, but also suffers from compression artifacts (blocking artifact is treated as an example in this paper). Directly performing image super-resolution (SR) to a highly compressed image would also simultaneously magnify the blocking artifacts, resulting in an unpleasing visual experience. In this paper, we propose a novel learning-based framework to achieve joint single-image SR and deblocking for a highly-compressed image. We argue that individually performing deblocking and SR (i.e., deblocking followed by SR, or SR followed by deblocking) on a highly compressed image usually cannot achieve a satisfactory visual quality. In our method, we propose to learn image sparse representations for modeling the relationship between low-and high-resolution image patches in terms of the learned dictionaries for image patches with and without blocking artifacts, respectively. As a result, image SR and deblocking can be simultaneously achieved via sparse representation and morphological component analysis (MCA)-based image decomposition. Experimental results demonstrate the efficacy of the proposed algorithm.
C1 [Kang, Li-Wei] Natl Yunlin Univ Sci & Technol, Doctoral Program, Grad Sch Engn Sci & Technol, Yunlin 64002, Taiwan.
   [Kang, Li-Wei] Natl Yunlin Univ Sci & Technol, Dept Comp Sci & Informat Engn, Yunlin 64002, Taiwan.
   [Hsu, Chih-Chung; Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
   [Zhuang, Boqi] Altek Corp, Hsinchu 30078, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 30013, Taiwan.
   [Lin, Chia-Wen] Asia Univ, Dept Comp Sci & Informat Engn, Taichung 41354, Taiwan.
   [Yeh, Chia-Hung] Natl Sun Yat Sen Univ, Dept Elect Engn, Kaohsiung 80424, Taiwan.
C3 National Yunlin University Science & Technology; National Yunlin
   University Science & Technology; National Tsing Hua University; National
   Tsing Hua University; Asia University Taiwan; National Sun Yat Sen
   University
RP Kang, LW (corresponding author), Natl Yunlin Univ Sci & Technol, Doctoral Program, Grad Sch Engn Sci & Technol, Yunlin 64002, Taiwan.
EM cwlin@ee.nthu.edu.tw
RI Hsu, Chih-Chung/Y-4835-2019; Lin, Chia-Wen/ABH-6075-2020; Lin,
   Chia-Wen/M-4571-2013
OI Hsu, Chih-Chung/0000-0002-2083-4438; Lin, Chia-Wen/0000-0002-9097-2318
FU Taiwan Ministry of Science and Technology [MOST 101-2221-E-007-121-MY3,
   103-2221-E-007-046-MY3, MOST 103-2221-E-224-034-MY2]
FX This work was supported in part by the Taiwan Ministry of Science and
   Technology under Grant MOST 101-2221-E-007-121-MY3, Grant
   103-2221-E-007-046-MY3, and Grant MOST 103-2221-E-224-034-MY2. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Shahram Shirani.
CR [Anonymous], P 16 ACM INT C MULT
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dong Weisheng, 2011, IEEE Trans Image Process, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Fadili MJ, 2010, P IEEE, V98, P983, DOI 10.1109/JPROC.2009.2024776
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   HOU HS, 1978, IEEE T ACOUST SPEECH, V26, P508
   Hsu CC, 2015, IEEE T IMAGE PROCESS, V24, P919, DOI 10.1109/TIP.2014.2387416
   Hsu CC, 2014, IEEE J-STSP, V8, P377, DOI 10.1109/JSTSP.2014.2311884
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   Kang LW, 2013, IEEE INT WORKSH MULT, P224, DOI 10.1109/MMSP.2013.6659292
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Lim H, 2011, IEEE T CIRC SYST VID, V21, P879, DOI 10.1109/TCSVT.2011.2133250
   List P, 2003, IEEE T CIRC SYST VID, V13, P614, DOI 10.1109/TCSVT.2003.815175
   Liu C, 2011, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2011.5995614
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Pham T. Q., 2006, P SOC PHOTO-OPT INS, V6077
   Ren J, 2013, IEEE T IMAGE PROCESS, V22, P1454, DOI 10.1109/TIP.2012.2231690
   Shen MY, 1998, J VIS COMMUN IMAGE R, V9, P2, DOI 10.1006/jvci.1997.0378
   Starck JL, 2005, IEEE T IMAGE PROCESS, V14, P1570, DOI 10.1109/TIP.2005.852206
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Triantafyllidis GA, 2002, IEEE T CIRC SYST VID, V12, P877, DOI 10.1109/TCSVT.2002.804880
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiong ZW, 2010, IEEE T IMAGE PROCESS, V19, P2017, DOI 10.1109/TIP.2010.2045707
   Yamaguchi T, 2011, LECT NOTES COMPUT SC, V6495, P127, DOI 10.1007/978-3-642-19282-1_11
   Yang CY, 2011, LECT NOTES COMPUT SC, V6494, P497, DOI 10.1007/978-3-642-19318-7_39
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Yeh CH, 2014, J VIS COMMUN IMAGE R, V25, P891, DOI 10.1016/j.jvcir.2014.02.012
NR 37
TC 68
Z9 70
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 921
EP 934
DI 10.1109/TMM.2015.2434216
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300001
DA 2024-07-18
ER

PT J
AU Duan, LY
   Lin, J
   Wang, Z
   Huang, TJ
   Gao, W
AF Duan, Ling-Yu
   Lin, Jie
   Wang, Zhe
   Huang, Tiejun
   Gao, Wen
TI Weighted Component Hashing of Binary Aggregated Descriptors for Fast
   Visual Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aggregating local features; hamming space; hashing; visual search
ID PRODUCT QUANTIZATION; CODES
AB Towards low bit rate mobile visual search, recent works have proposed to aggregate the local features and compress the aggregated descriptor (such as Fisher vector, the vector of locally aggregated descriptors) for low latency query delivery as well as moderate search complexity. Even though Hamming distance can be computed very fast, the computational cost of exhaustive linear search over the binary descriptors grows linearly with either the length of a binary descriptor or the number of database images. In this paper, we propose a novel weighted component hashing (WeCoHash) algorithm for long binary aggregated descriptors to significantly improve search efficiency over a large scale image database. Accordingly, the proposed WeCoHash has attempted to address two essential issues in Hashing algorithms: "what to hash" and "how to search." "What to hash" is tackled by a hybrid approach, which utilizes both image-specific component (i.e., visual word) redundancy and bit dependency within each component of a binary aggregated descriptor to produce discriminative hash values for bucketing. "How to search" is tackled by an adaptive relevance weighting based on the statistics of hash values. Extensive comparison results have shown that WeCoHash is at least 20 times faster than linear search and 10 times faster than local sensitive hash (LSH) when maintaining comparable search accuracy. In particular, the WeCoHash solution has been adopted by the emerging MPEG compact descriptor for visual search (CDVS) standard to significantly speed up the exhaustive search of the binary aggregated descriptors.
C1 [Duan, Ling-Yu; Lin, Jie; Wang, Zhe; Huang, Tiejun; Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China.
C3 Peking University
RP Duan, LY (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China.
EM lingyu@pku.edu.cn; jielin@pku.edu.cn; zhew@pku.edu.cn;
   tjhuang@pku.edu.cn; wgao@pku.edu.cn
RI Huang, Tiejun/D-6161-2011
FU Chinese Natural Science Foundation [61271311, 61390515]; National
   Hightech R&D Program of China (863 Program) [2015AA016302]
FX This work was supported by the Chinese Natural Science Foundation under
   Contract 61271311 and Contract 61390515, and by the National Hightech
   R&D Program of China (863 Program) under Grant 2015AA016302. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Cees Snoek.
CR Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   [Anonymous], 2011, JTC1SC29WG11N12202 I
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI 10.1145/2502081.2502171
   [Anonymous], 2014, JTC1SC29WG11W14392 I
   [Anonymous], 2011, P 2 ANN ACM C MULTIM
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Charikar Moses S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Chen D, 2013, SIGNAL PROCESS, V93, P2316, DOI 10.1016/j.sigpro.2012.06.005
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Douze M, 2011, PROC CVPR IEEE, P745, DOI 10.1109/CVPR.2011.5995595
   Duan LY, 2014, IEEE MULTIMEDIA, V21, P30, DOI 10.1109/MMUL.2013.66
   Esmaeili MM, 2012, IEEE T PATTERN ANAL, V34, P2481, DOI 10.1109/TPAMI.2012.170
   Fleuret F, 2004, J MACH LEARN RES, V5, P1531
   Friedman J., ACM T MATH SOFTW, V3
   Ge TZ, 2013, PROC CVPR IEEE, P2946, DOI 10.1109/CVPR.2013.379
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Heo JP, 2014, PROC CVPR IEEE, P2139, DOI 10.1109/CVPR.2014.274
   Jaakkola TS, 1999, ADV NEUR IN, V11, P487
   Jain M., 2011, Proceedings of the 19th ACM international conference on Multimedia, P1441
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Ji RR, 2012, INT J COMPUT VISION, V96, P290, DOI 10.1007/s11263-011-0472-9
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Kalantidis Y, 2014, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR.2014.298
   Lin J, 2014, IEEE SIGNAL PROC LET, V21, P195, DOI 10.1109/LSP.2013.2296532
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikulík A, 2010, LECT NOTES COMPUT SC, V6313, P1
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   Norouzi M.E., 2011, ICML
   Norouzi M, 2012, PROC CVPR IEEE, P3108, DOI 10.1109/CVPR.2012.6248043
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Perronnin F., 2012, IEEE C COMP VIS PATT
   PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009
   Perronnin F, 2007, PROC CVPR IEEE, P2272
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Silpa-Anan C, 2008, PROC CVPR IEEE, P2308
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Thomas T. M.CoverandJ. A., 1991, ELEMENTS INFORM THEO, DOI 10.1002/0471200611
   Tolias G, 2014, LECT NOTES COMPUT SC, V8694, P382, DOI 10.1007/978-3-319-10599-4_25
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   Torralba A., 2008, PROC CVPR 08, P1
   Trzcinski T, 2012, PATTERN RECOGN LETT, V33, P2173, DOI 10.1016/j.patrec.2012.08.006
   Weiss Y., 2008, NIPS, V21, P1753
   Zhang SL, 2014, IEEE T IMAGE PROCESS, V23, P3671, DOI 10.1109/TIP.2014.2330794
   Zhou W., 2014, IEEE T MULTIMEDIA, V28, P61
NR 55
TC 34
Z9 34
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2015
VL 17
IS 6
BP 828
EP 842
DI 10.1109/TMM.2015.2419973
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CI1TM
UT WOS:000354527500006
DA 2024-07-18
ER

PT J
AU Zhang, SL
   Yu, X
   Sui, Y
   Zhao, SC
   Zhang, L
AF Zhang, Shunli
   Yu, Xin
   Sui, Yao
   Zhao, Sicong
   Zhang, Li
TI Object Tracking With Multi-View Support Vector Machines
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Entropy criterion; multi-view learning; object tracking; subspace
   evolution; support vector machines (SVM)
ID VISUAL TRACKING; ENSEMBLE
AB How to build an accurate and reliable appearance model to improve the performance is a crucial problem in object tracking. Since the multi-view learning can lead to more accurate and robust representation of the object, in this paper, we propose a novel tracking method via multi-view learning framework by using multiple support vector machines (SVM). The multi-view SVMs tracking method is constructed based on multiple views of features and a novel combination strategy. To realize a comprehensive representation, we select three different types of features, i.e., gray scale value, histogram of oriented gradients (HOG), and local binary pattern (LBP), to train the corresponding SVMs. These features represent the object from the perspectives of description, detection, and recognition, respectively. In order to realize the combination of the SVMs under the multi-view learning framework, we present a novel collaborative strategy with entropy criterion, which is acquired by the confidence distribution of the candidate samples. In addition, to learn the changes of the object and the scenario, we propose a novel update scheme based on subspace evolution strategy. The new scheme can control the model update adaptively and help to address the occlusion problems. We conduct our approach on several public video sequences and the experimental results demonstrate that our method is robust and accurate, and can achieve the state-of-the-art tracking performance.
C1 [Zhang, Shunli; Yu, Xin; Sui, Yao; Zhao, Sicong; Zhang, Li] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Zhang, SL (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
EM sl-zhang11@mail.tsinghua.edu.cn; xin-yu09@mail.tsinghua.edu.cn;
   suiy10@mail.tsinghua.edu.cn; zhaosc10@mails.tsinghua.edu.cn;
   chinazhangli@mail.tsinghua.edu.cn
RI jing, wang/KCZ-2144-2024
OI Yu, Xin/0000-0002-0269-5649
FU National Natural Science Foundation of China [61172125, 61132007]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61172125 and Grant 61132007. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Klara Nahrstedt.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bai YC, 2012, PROC CVPR IEEE, P1854, DOI 10.1109/CVPR.2012.6247884
   Cehovin L, 2014, IEEE WINT CONF APPL, P540, DOI 10.1109/WACV.2014.6836055
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollar P., Piotr's Image and Video Matlab Toolbox (PMT)
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Grabner H., 2006, BMVC, P47
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Huchuan Lu, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P539, DOI 10.1109/FG.2011.5771455
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kwon J, 2011, IEEE I CONF COMP VIS, P1195, DOI 10.1109/ICCV.2011.6126369
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Leistner C, 2010, LECT NOTES COMPUT SC, V6376, P493
   Li HX, 2011, PROC CVPR IEEE, P1305, DOI 10.1109/CVPR.2011.5995483
   Li X, 2013, IEEE T PATTERN ANAL, V35, P863, DOI 10.1109/TPAMI.2012.166
   Liu R, 2009, IEEE I CONF COMP VIS, P1459, DOI 10.1109/ICCV.2009.5459285
   Lu C, 2013, IEEE T MULTIMEDIA, V15, P70, DOI 10.1109/TMM.2012.2225036
   Lu HC, 2011, LECT NOTES COMPUT SC, V6494, P511, DOI 10.1007/978-3-642-19318-7_40
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sabirin H, 2012, IEEE T MULTIMEDIA, V14, P657, DOI 10.1109/TMM.2012.2187777
   Santner J, 2010, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2010.5540145
   Tang F, 2007, IEEE I CONF COMP VIS, P992
   Vedaldi A., 2009, Advances in Neural Information Processing Systems, V22, P1928
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xin Sun, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3393, DOI 10.1109/CVPR.2011.5995656
   Xu C., 2013, ARXIV13045634, P1
   Yang F, 2011, LECT NOTES COMPUT SC, V6495, P39, DOI 10.1007/978-3-642-19282-1_4
   Yang HX, 2011, NEUROCOMPUTING, V74, P3823, DOI 10.1016/j.neucom.2011.07.024
   Yao R, 2013, PROC CVPR IEEE, P2363, DOI 10.1109/CVPR.2013.306
   Yeasin M, 2004, IEEE T MULTIMEDIA, V6, P398, DOI 10.1109/TMM.2004.827514
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yu Q, 2008, LECT NOTES COMPUT SC, V5303, P678
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zhu X, 2009, Synthesis Lectures on Artificial Intelligence and Machine Learning, V3, P1, DOI 10.1007/978-3-031-01548-9
NR 49
TC 90
Z9 95
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 265
EP 278
DI 10.1109/TMM.2015.2390044
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700001
DA 2024-07-18
ER

PT J
AU Duan, YZ
   Sun, J
   Yan, LJ
   Chen, KJ
   Guo, ZM
AF Duan, Yizhou
   Sun, Jun
   Yan, Leju
   Chen, Keji
   Guo, Zongming
TI Novel Efficient HEVC Decoding Solution on General-Purpose Processors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Decoding; high efficiency video coding (HEVC); SIMD; video codecs
ID COMPLEXITY; IMPLEMENTATION
AB Although the emerging video coding standard High Efficiency Video Coding (HEVC) successfully doubles the compression efficiency of H. 264/AVC, its growing computational complexity makes real-time decoding of high-definition HEVC videos a very challenging issue for the existing personal computers and mobile devices. In this paper, a systematical, efficient HEVC decoding solution on general processors is provided, consisting of structure-level, data-level, and task-level approaches. First, a redesigned overall structure of a HEVC decoder with data redundancy reduction mechanism is introduced, which cuts down basic data operation cost and achieves an average decoding speedup of compared to the HM 10.0 decoder. On this basis, novel single-instruction multiple-data (SIMD) algorithms such as low-complexity motion compensation, transpose-free transform, symmetric deblocking filter, and parallel-index sample adaptive offset are developed, which further parallelize the data operations of each decoding task and bring another decoding speedup. Finally, a frame-based task-level parallel framework is employed with a flexible entry scheme to efficiently support the simultaneous processing of multiple decoding tasks for different HEVC parallel strategies. The overall solution achieves decoding fps of 40-75 for 4k HEVC videos on the Intel i7-2600 3.4 GHz quad-core processor (4-thread decoding) and 35-55 for 720p videos on the ARM Cortex-A9 1.2 GHz duo-core processor (2-thread decoding). This proposal is the recommended cross-platform HEVC decoding solution of Intel, AMD, and Cisco, and has provided HEVC service to over 1500 million people in China via the Xunlei Kankan video client.
C1 [Duan, Yizhou; Sun, Jun; Yan, Leju; Chen, Keji; Guo, Zongming] Peking Univ, Inst Comp Sci & Technol, Beijing 100080, Peoples R China.
C3 Peking University
RP Duan, YZ (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100080, Peoples R China.
EM duanyizhou@pku.edu.cn; jsun@pku.edu.cn; yanleju@pku.edu.cn;
   chenkeji@pku.edu.cn; guozongming@pku.edu.cn
FU National Natural Science Foundation of China [61271020]; National Key
   Technology R&D Program of China [2012BAH18B03]; Beijing Natural Science
   Foundation [4142021]
FX This work was supported by the National Natural Science Foundation of
   China under Contract 61271020, the National Key Technology R&D Program
   of China under Grant 2012BAH18B03, and the Beijing Natural Science
   Foundation under Contract 4142021. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Yiannis Andreopoulos.
CR [Anonymous], ARM ARCH
   [Anonymous], 2013, Technical Report JCTVC-L1100
   Borkar S, 2011, COMMUN ACM, V54, P67, DOI 10.1145/1941487.1941507
   Bossen F., 2011, JCTVCF537
   Bossen F, 2012, IEEE T CIRC SYST VID, V22, P1685, DOI 10.1109/TCSVT.2012.2221255
   Buckley P., 2010, EETIMES         1211
   Casalino F, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P363, DOI 10.1109/MMCS.1999.779231
   Chavarrias M, 2013, IEEE T CONSUM ELECTR, V59, P839, DOI 10.1109/TCE.2013.6689697
   Chen KH., 2012, SAE Int., V1, P1, DOI DOI 10.4271/2012-01-0645
   Chen YK, 2006, J VIS COMMUN IMAGE R, V17, P509, DOI 10.1016/j.jvcir.2005.05.004
   Chi CC, 2012, IEEE T CIRC SYST VID, V22, P1827, DOI 10.1109/TCSVT.2012.2223056
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Fuldseth A., 2011, JCTVCE408
   Henry F., 2011, JCTVCE196
   Intel Corporation, Intel 64 and IA-32 Architectures Software Developer's Manuals
   Lai P., 2011, JCTVCF303
   Lappalainen V, 2003, IEEE T CIRC SYST VID, V13, P717, DOI 10.1109/TCSVT.2003.814968
   Leju Yan, 2012, 2012 Visual Communications and Image Processing (VCIP), DOI 10.1109/VCIP.2012.6410857
   Lu Y., 2014, REAL TIME END TO END
   Malvar HS, 2003, IEEE T CIRC SYST VID, V13, P598, DOI 10.1109/TCSVT.2003.814964
   Marpe D, 2010, IEEE T CIRC SYST VID, V20, P1676, DOI 10.1109/TCSVT.2010.2092615
   McCann K., 2011, JCTVCG988
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Pescador F, 2009, IEEE T CONSUM ELECTR, V55, P205, DOI 10.1109/TCE.2009.4814436
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Vanne J, 2012, IEEE T CIRC SYST VID, V22, P1885, DOI 10.1109/TCSVT.2012.2223013
   Veera K., 2012, JCTVCH0693
   Wang S., 2013, OPTIMIZING H 265 HEV
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yang ZG, 2006, IEEE T CONSUM ELECTR, V52, P1267, DOI 10.1109/TCE.2006.273144
NR 30
TC 20
Z9 23
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1915
EP 1928
DI 10.1109/TMM.2014.2337834
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300010
DA 2024-07-18
ER

PT J
AU Schoeffmann, K
   Ahlström, D
   Hudelist, MA
AF Schoeffmann, Klaus
   Ahlstroem, David
   Hudelist, Marco A.
TI 3-D Interfaces to Improve the Performance of Visual Known-Item Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image and video browsing; image search and retrieval; known item search;
   video search and retrieval
AB Most interfaces in the field of image and video search use a two-dimensional grid interface, which presents image thumbnails in a left-to-right arrangement that can be browsed from top to bottom. This grid interface, however, has several drawbacks that become particularly apparent when performing interactive search tasks for target items in large collections of images or videos. Therefore, we propose to use 3-D interfaces as an alternative to the grid interface for interactive known-item search in visual data as they can partially overcome these drawbacks. In this paper, we first summarize our ideas and discuss design aspects of a 3-D ring and a 3-D globe interface. Next, we present results from four different user studies, where we evaluated the performance of these interfaces for known-item search tasks in image collections. Our results from these studies show that the proposed 3-D interfaces allow for significantly faster visual target search on desktop computers with mouse interaction as well as on tablet devices. The interfaces also achieve better subjective ratings. However, our evaluation also shows that on smartphones with 3.5-in screens an improvement over the grid interface in terms of visual search time is only possible in collections with more than 200 images.
C1 [Schoeffmann, Klaus; Ahlstroem, David; Hudelist, Marco A.] Alpen Adria Univ Klagenfurt, A-9020 Klagenfurt, Austria.
C3 University of Klagenfurt
RP Schoeffmann, K (corresponding author), Alpen Adria Univ Klagenfurt, A-9020 Klagenfurt, Austria.
EM ks@itec.aau.at; david@isys.uni-klu.ac.at; MarcoAn-drea.Hudelist@aau.at
RI Ahlstrom, David/H-3967-2019
OI Ahlstrom, David/0000-0001-9752-1248
FU Austrian Federal Ministry for Transport, Innovation and Technology
   (bmvit); Austrian Science Fund (FWF) [TRP 273-N15]; European Regional
   Development Fund; Carinthian Econ. Prom. Fund (KWF); Lakeside Labs,
   Klagenfurt, Austria
FX This work was supported in part by the Austrian Federal Ministry for
   Transport, Innovation and Technology (bmvit), the Austrian Science Fund
   (FWF) under Grant TRP 273-N15, the European Regional Development Fund,
   and the Carinthian Econ. Prom. Fund (KWF), and supported by Lakeside
   Labs, Klagenfurt, Austria. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Tao Mei.
CR Ahlstrom D., 2012, P 20 ACM INT C MULT, P925
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2007, CIVR '07
   Bruneau P, 2010, PATTERN RECOGN, V43, P485, DOI 10.1016/j.patcog.2009.03.024
   Chiu P., 2005, 13th Annual ACM International Conference on Multimedia, P213, DOI 10.1145/1101149.1101182
   Chiu P., 2008, Proceedings of Multimedia'08, P1107
   Christmann O, 2010, INTERACT COMPUT, V22, P399, DOI 10.1016/j.intcom.2010.02.005
   Chun J, 2011, INT J IND ERGONOM, V41, P280, DOI 10.1016/j.ergon.2011.02.009
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   De Rooij O., 2008, P ACM INT C IM VID R, P485
   de Rooij O., 2007, P 6 ACM INT C IM VID, P649, DOI DOI 10.1145/1282280.1282376
   Divakaran A, 2005, IEEE ICCE, P43, DOI 10.1109/ICCE.2005.1429708
   Geetha P., 2008, Journal of Computer Sciences, V4, P474, DOI 10.3844/jcssp.2008.474.486
   Gomi A., 2012, P 27 ANN ACM S APPL, P989
   HART S G, 1988, P139
   Hudelist M. A., 2013, P 3 ACM C INT C MULT, P299
   Hudelist MA, 2013, IEEE INT SYM MULTIM, P1, DOI 10.1109/ISM.2013.11
   Müller C, 2012, IEEE INT CONF MULTI, P665, DOI 10.1109/ICMEW.2012.121
   Nakazato M., 2001, P IEEE INT C MULT EX, P44
   Qing Xu, 2010, 2010 IEEE International Conference on Systems, Man and Cybernetics (SMC 2010), P52, DOI 10.1109/ICSMC.2010.5642204
   Rodden K., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P190, DOI 10.1145/365024.365097
   Schaefer G, 2010, MULTIMED TOOLS APPL, V47, P105, DOI 10.1007/s11042-009-0409-2
   Schoeffmann K., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P848, DOI 10.1109/ICME.2012.62
   Schoeffmann K., 2012, 13 INT WORKSH IM AN, P1
   Schoeffmann K, 2012, INT J MULTIMED DATA, V3, P49, DOI 10.4018/jmdem.2012010104
   Schonert-Reichl KA., 2012, SCH MENT HEALTH, V4, P1
   Slimi J., 2013, OAIR, P213
   Smeaton A., 2012, TRECVID 2012 KNOWN I
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Wittenburg K., 2003, P 16 ANN ACM S USER, P115, DOI [10.1145/964696.964709, DOI 10.1145/964696.964709]
   Zheng Y., 2009, P ACM INT C IM VID R
NR 33
TC 14
Z9 15
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1942
EP 1951
DI 10.1109/TMM.2014.2333666
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300012
DA 2024-07-18
ER

PT J
AU Yuan, ZQ
   Sang, JT
   Xu, CS
   Liu, Y
AF Yuan, Zhaoquan
   Sang, Jitao
   Xu, Changsheng
   Liu, Yan
TI A Unified Framework of Latent Feature Learning in Social Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; feature learning; india buffet process; social media
ID LINK-PREDICTION; IMAGE RETRIEVAL; ALGORITHM; RELEVANCE; MODEL
AB The current trend in social media analysis and application is to use the pre-defined features and devoted to the later model development modules to meet the end tasks. Representation learning has been a fundamental problem in machine learning, and widely recognized as critical to the performance of end tasks. In this paper, we provide evidence that specially learned features will addresses the diverse, heterogeneous, and collective characteristics of social media data. Therefore, we propose to transfer the focus from the model development to latent feature learning, and present a unified framework of latent feature learning on social media. To address the noisy, diverse, heterogeneous, and interconnected characteristics of social media data, the popular deep learning is employed due to its excellent abstract abilities. In particular, we instantiate the proposed framework by (1) designing a novel relational generative deep learning model to solve the social media link analysis task, and (2) developing a multimodal deep learning to lambda rank model towards the social image retrieval task. We show that the derived latent features lead to improvement in both of the social media tasks.
C1 [Yuan, Zhaoquan; Sang, Jitao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Liu, Yan] Hong Kong Polytech Univ, Dept Comp, Kowloon 999077, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Hong Kong
   Polytechnic University
RP Yuan, ZQ (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM zqyuan@nlpr.ia.ac.cn; jtsang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn;
   csyliu@comp.polyu.edu.hk
RI xu, cj/HJZ-3488-2023; liu, yan/HGV-1365-2022
OI LIU, Yan/0000-0003-4242-4840
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009, 61373122, 61303176];
   Beijing Natural Science Foundation [4131004]; Singapore National
   Research Foundation under International Research Centre at the Singapore
   Funding Initiative
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2012CB316304, the National Natural Science
   Foundation of China under Grants 61225009, 61373122, and 61303176, the
   Beijing Natural Science Foundation under Grant 4131004, and the
   Singapore National Research Foundation under its International Research
   Centre at the Singapore Funding Initiative and administered by the IDM
   Programme Office. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Cees Snoek.
CR Ahlqvist Toni., 2008, VTT Tiedotteita - ValtionTeknillinenTutkimuskeskusPublicshEdita Prima Oy, P13
   Airoldi EM, 2008, J MACH LEARN RES, V9, P1981
   [Anonymous], P ASONAM
   [Anonymous], 2008, PROC INT C MACHINE L
   [Anonymous], 2012, P IEEE, DOI DOI 10.1109/JPROC.2012.2191529
   [Anonymous], 2013, Proceedings of the 21st ACM international conference on Multimedia
   [Anonymous], 2006, P SDM 06 WORKSH LINK
   [Anonymous], P UNS TRANSF LEARN W
   [Anonymous], 2010, P ACM MULTIMEDIA
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], P 21 NAT C AI
   [Anonymous], ADV NEURAL INF PROCE
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Biel JI, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037690
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Burges C. J. C., 2006, P ADV NEUR INF PROC, V19, P193
   Caicedo JuanC., 2012, Proceedings of the 2nd ACM International Conference on Multimedia Retrieval, P56
   Cao L., 2009, P 17 ACM INT C MULTI, P125
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Dong YX, 2012, IEEE DATA MINING, P181, DOI 10.1109/ICDM.2012.140
   Friedland Gerald., 2010, P 18 ACM INT C MULTI, P1245, DOI DOI 10.1145/1873951.1874197
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Griffiths TL, 2005, TECHNICAL REPORT
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Kang Y, 2011, LECT NOTES ARTIF INT, V6912, P130, DOI 10.1007/978-3-642-23783-6_9
   Kashima H, 2006, IEEE DATA MINING, P340
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591
   Lin YR, 2011, ACM T KNOWL DISCOV D, V5, DOI 10.1145/1993077.1993081
   Liu D., 2012, ACM MM'12', P659
   Liu L., 2008, P 17 INT C WORLD WID, P1009
   Lucchi A, 2012, LECT NOTES COMPUT SC, V7572, P130, DOI 10.1007/978-3-642-33718-5_10
   Menon AK, 2011, LECT NOTES ARTIF INT, V6912, P437, DOI 10.1007/978-3-642-23783-6_28
   Naphade MR, 2004, J VIS COMMUN IMAGE R, V15, P348, DOI 10.1016/j.jvcir.2004.04.010
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rendle S, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168771
   Robert Christian P., 2004, Springer Texts in Statistics, Vsecond
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Smolensky P., 1986, Information processing in dynamical systems: Foundations of harmony theory
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vogel J, 2004, LECT NOTES COMPUT SC, V3115, P207
   Wu A. G., 2007, P ACM MM, P218
NR 55
TC 21
Z9 25
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1624
EP 1635
DI 10.1109/TMM.2014.2322338
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200011
DA 2024-07-18
ER

PT J
AU Liang, H
   Yuan, JS
   Thalmann, D
AF Liang, Hui
   Yuan, Junsong
   Thalmann, Daniel
TI Parsing the Hand in Depth Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth-context feature; hand parsing; Markov random field
ID POSE ESTIMATION
AB Hand pose tracking and gesture recognition are useful for human-computer interaction, while a major problem is the lack of discriminative features for compact hand representation. We present a robust hand parsing scheme to extract a high-level description of the hand from the depth image. A novel distance-adaptive selection method is proposed to get more discriminative depth-context features. Besides, we propose a Superpixel-Markov Random Field (SMRF) parsing scheme to enforce the spatial smoothness and the label co-occurrence prior to remove the misclassified regions. Compared to pixel-level filtering, the SMRF scheme is more suitable to model the misclassified regions. By fusing the temporal constraints, its performance can be further improved. Overall, the proposed hand parsing scheme is accurate and efficient. The tests on synthesized dataset show it gives much higher accuracy for single-frame parsing and enhanced robustness for continuous sequence parsing compared to benchmarks. The tests on real-world depth images of the hand and human body show the robustness to complex hand configurations of our method and its generalization power to different kinds of articulated objects.
C1 [Liang, Hui; Thalmann, Daniel] Nanyang Technol Univ, Inst Media Innovat, Being There Ctr, Singapore 637553, Singapore.
   [Yuan, Junsong] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Liang, H (corresponding author), Nanyang Technol Univ, Inst Media Innovat, Being There Ctr, Singapore 637553, Singapore.
EM hliang1@e.ntu.edu.sg; jsyuan@ntu.edu.sg; danielthalmann@ntu.edu.sg
RI Thalmann, Daniel/A-4347-2008; Yuan, Junsong/A-5171-2011; Thalmann,
   Daniel/AAL-1097-2020
OI Thalmann, Daniel/0000-0002-0451-7491; Yuan, Junsong/0000-0002-7901-8793
FU Singapore National Research Foundation under its International Research
   Centre @ Singapore Funding Initiative
FX This work, which was carried out at BeingThere Centre, was supported by
   the Singapore National Research Foundation under its International
   Research Centre @ Singapore Funding Initiative and administered by the
   IDM Programme Office. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Chong-Wah Ngo.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], P ECCV
   Aristidou A., 2010, P ISCCSP
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Baak A, 2011, IEEE I CONF COMP VIS, P1092, DOI 10.1109/ICCV.2011.6126356
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BESAG J, 1986, J R STAT SOC B, V48, P259
   Bhuyan M. K., 2013, INT J MACH LEARN CYB
   Bo L, 2011, P IROS
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Choi J, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.2.027206
   Dorner B., 1994, THESIS SIMON FRASER
   Eichner M, 2012, INT J COMPUT VISION, V99, P190, DOI 10.1007/s11263-012-0524-9
   Erol A., 2005, P CVPR
   Heikkila J., 1997, P CVPR
   Hernandez-Vela A., 2012, P CVPR
   Kerdvibulvech C., P 2009 EURASIP
   Keskin C., 2011, P ICCV
   Kindermann R., 1980, Markov random fields and their applications, V547, DOI DOI 10.1090/CONM/001
   Lewis J., 2000, P SIGGRAPH
   Li Z., 2009, P ACRA
   Liang H, 2013, VISUAL COMPUT, V29, P837, DOI 10.1007/s00371-013-0822-4
   Lin J., 2000, P HUMO
   Lu S., P 2003 CVPR
   Ramanan D., 2007, P NIPS
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Ren Z, 2011, IEEE I CONF COMP VIS, P303, DOI 10.1109/ICCV.2011.6126256
   Rusu R. B., 2009, P ICRA KOB JAP
   Shen XH, 2012, IMAGE VISION COMPUT, V30, P227, DOI 10.1016/j.imavis.2011.11.003
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Tarabalka Y, 2010, IEEE GEOSCI REMOTE S, V7, P736, DOI 10.1109/LGRS.2010.2047711
   Tighe J., 2010, Proc. ECCV
   Van den Bergh M., 2009, P WACV
   Wachs JP, 2011, COMMUN ACM, V54, P60, DOI 10.1145/1897816.1897838
   Wang H., 2006, J INF SCI ENG
   Wang R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531397
   Wang X., 2009, P ICME
   Yao Y., 2012, P ICME
   Zhu L., 2008, P CVPR
NR 39
TC 64
Z9 74
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1241
EP 1253
DI 10.1109/TMM.2014.2306177
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600007
DA 2024-07-18
ER

PT J
AU Qian, ZX
   Zhang, XP
   Wang, SZ
AF Qian, Zhenxing
   Zhang, Xinpeng
   Wang, Shuozhong
TI Reversible Data Hiding in Encrypted JPEG Bitstream
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encrypted image; image recovery; information hiding; JPEG; reversible
   data hiding
ID WATERMARKING
AB This correspondence proposes a framework of reversible data hiding (RDH) in an encrypted JPEG bitstream. Unlike existing RDH methods for encrypted spatial-domain images, the proposed method aims at encrypting a JPEG bitstream into a properly organized structure, and embedding a secret message into the encrypted bitstream by slightly modifying the JPEG stream. We identify usable bits suitable for data hiding so that the encrypted bitstream carrying secret data can be correctly decoded. The secret message bits are encoded with error correction codes to achieve a perfect data extraction and image recovery. The encryption and embedding are controlled by encryption and embedding keys respectively. If a receiver has both keys, the secret bits can be extracted by analyzing the blocking artifacts of the neighboring blocks, and the original bitstream perfectly recovered. In case the receiver only has the encryption key, he/she can still decode the bitstream to obtain the image with good quality without extracting the hidden data.
C1 [Qian, Zhenxing; Zhang, Xinpeng; Wang, Shuozhong] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 Shanghai University
RP Qian, ZX (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM zxqian@shu.edu.cn
RI Qian, Zhenxing/AHC-9176-2022
FU Natural Science Foundation of China [61103181]; Shanghai Rising-Star
   Program [14QA1401900]; Research Fund for the Doctoral Program of Higher
   Education of China [20113108110010]; Program for Professor of Special
   Appointment (Eastern Scholar) at Shanghai Institutions of Higher
   Learning; Shanghai Pujiang Program [13PJ1403200]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61103181, Shanghai Rising-Star Program under Grant
   14QA1401900, the Research Fund for the Doctoral Program of Higher
   Education of China under Grant 20113108110010, the Program for Professor
   of Special Appointment (Eastern Scholar) at Shanghai Institutions of
   Higher Learning, and Shanghai Pujiang Program under Grant 13PJ1403200.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Shahram Shirani.
CR Deng MN, 2009, MM&SEC'09: PROCEEDINGS OF THE 2009 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P9
   Fridrich J, 2002, P SOC PHOTO-OPT INS, V4675, P572, DOI 10.1117/12.465317
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Kalker T, 2002, DSP 2002: 14TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING PROCEEDINGS, VOLS 1 AND 2, P71, DOI 10.1109/ICDSP.2002.1027818
   Li XL, 2011, IEEE T IMAGE PROCESS, V20, P3524, DOI 10.1109/TIP.2011.2150233
   Lian SG, 2007, IEEE T CIRC SYST VID, V17, P774, DOI 10.1109/TCSVT.2007.896635
   Luo LX, 2010, IEEE T INF FOREN SEC, V5, P187, DOI 10.1109/TIFS.2009.2035975
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Memon N, 2001, IEEE T IMAGE PROCESS, V10, P643, DOI 10.1109/83.913598
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   RYAN W., 2004, CRC HDB CODING SIGNA
   Sachnev V, 2009, IEEE T CIRC SYST VID, V19, P989, DOI 10.1109/TCSVT.2009.2020257
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Varodayan D, 2006, SIGNAL PROCESS, V86, P3123, DOI 10.1016/j.sigpro.2006.03.012
   Willems F. M., 2004, DIMACS SERIES DISCRE, V66, P61
   Xuan GR, 2007, LECT NOTES COMPUT SC, V4633, P715
   Zhang WM, 2012, IEEE T IMAGE PROCESS, V21, P2991, DOI 10.1109/TIP.2012.2187667
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
NR 21
TC 188
Z9 208
U1 1
U2 63
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1486
EP 1491
DI 10.1109/TMM.2014.2316154
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600027
DA 2024-07-18
ER

PT J
AU Su, L
   Yeh, CCM
   Liu, JY
   Wang, JC
   Yang, YH
AF Su, Li
   Yeh, Chin-Chia Michael
   Liu, Jen-Yu
   Wang, Ju-Chiang
   Yang, Yi-Hsuan
TI A Systematic Evaluation of the Bag-of-Frames Representation for Music
   Information Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bag-of-frames model; music information retrieval; sparse coding;
   unsupervised feature learning
ID SPARSE; CLASSIFICATION
AB There has been an increasing attention on learning feature representations from the complex, high-dimensional audio data applied in various music information retrieval (MIR) problems. Unsupervised feature learning techniques, such as sparse coding and deep belief networks have been utilized to represent music information as a term-document structure comprising of elementary audio codewords. Despite the widespread use of such bag-of-frames (BoF) model, few attempts have been made to systematically compare different component settings. Moreover, whether techniques developed in the text retrieval community are applicable to audio codewords is poorly understood. To further our understanding of the BoF model, we present in this paper a comprehensive evaluation that compares a large number of BoF variants on three different MIR tasks, by considering different ways of low-level feature representation, codebook construction, codeword assignment, segment-level and song-level feature pooling, tf-idf term weighting, power normalization, and dimension reduction. Our evaluations lead to the following findings: 1) modeling music information by two levels of abstraction improves the result for difficult tasks such as predominant instrument recognition, 2) tf-idf weighting and power normalization improve system performance in general, 3) topic modeling methods such as latent Dirichlet allocation does not work for audio codewords.
C1 [Su, Li; Yeh, Chin-Chia Michael; Liu, Jen-Yu; Yang, Yi-Hsuan] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 11564, Taiwan.
   [Wang, Ju-Chiang] Acad Sinica, Inst Informat Sci, Taipei 11564, Taiwan.
C3 Academia Sinica - Taiwan; Academia Sinica - Taiwan
RP Su, L (corresponding author), Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 11564, Taiwan.
EM lisu@citi.sinica.edu.tw; mcyeh@citi.sinica.edu.tw;
   ciaua@citi.sinica.edu.tw; asriver@iis.sinica.edu.tw;
   yang@citi.sinica.edu.tw
RI Yeh, Michael/J-1738-2019; Su, Li/ABA-7758-2020
OI Yeh, Michael/0000-0002-9807-2963; Su, Li/0000-0003-4275-8832; Liu,
   Jen-Yu/0000-0003-1299-6688
FU National Science Council of Taiwan [NSC 101-2221-E-001-017, NSC
   102-2221-E-001-004-MY3]; Academia Sinica Career Development Award
FX This work was supported in part by the National Science Council of
   Taiwan under Grants NSC 101-2221-E-001-017, NSC 102-2221-E-001-004-MY3
   and in part by the Academia Sinica Career Development Award. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Tao Li.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P IEEE ICASSP
   [Anonymous], ACM T INTELL SYST TE
   [Anonymous], 2011, P AISTATS
   [Anonymous], 2011, P 12 ISMIR C MIAM FL
   [Anonymous], 2010, P ICML
   [Anonymous], P INT C DIG AUD EFF
   [Anonymous], 2012, Proc. International Society for Music Information Retrieval Conference (ISMIR)
   [Anonymous], 2012, P 13 INT SOC MUS INF
   [Anonymous], P ISMIR
   [Anonymous], 2012, VECTOR QUANTIZATION
   [Anonymous], 2008, P INT C DIG AUD EFF
   [Anonymous], IEEE T MULT IN PRESS
   [Anonymous], P ACM ICMR
   [Anonymous], P ISMIR
   Aryafar Kamelia., 2011, Proceedings of the 1st international ACM workshop on Music information retrieval with user-centered and multimodal strategies, P33
   Aucouturier JJ, 2007, J ACOUST SOC AM, V122, P881, DOI 10.1121/1.2750160
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Battenberg E., 2012, Proceedings of the 13th International Society for Music Information Retrieval Conference, P37
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Berenzweig A, 2004, COMPUT MUSIC J, V28, P63, DOI 10.1162/014892604323112257
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   Coates A., 2011, P 28 INT C MACH LEAR, V28, P921
   Coviello Emanuele., 2012, ISMIR, P547
   Da Vitoria Mosteiro S.Bento, 2012, PROC ISMIR, P325
   Dieleman S., 2011, 12th International Society for Music Information Retrieval Conference (ISMIR-2011), P669
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Fu ZY, 2011, PATTERN RECOGN LETT, V32, P1768, DOI 10.1016/j.patrec.2011.06.026
   Fuhrmann F, 2012, Ph.D. dissertation
   Gemmeke JF, 2011, IEEE T AUDIO SPEECH, V19, P2067, DOI 10.1109/TASL.2011.2112350
   Hamel P., 2010, ISMIR, P339
   Hamel P., 2011, Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR), P729
   Haro M, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0033993
   Humphrey EJ, 2012, INT CONF ACOUST SPEE, P453, DOI 10.1109/ICASSP.2012.6287914
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Lacoste A, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/43745
   Lan M, 2009, IEEE T PATTERN ANAL, V31, P721, DOI 10.1109/TPAMI.2008.110
   Lyon RF, 2010, NEURAL COMPUT, V22, P2390, DOI 10.1162/NECO_a_00011
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Maji S., 2008, CVPR
   Manzagol P.-A., 2008, 9th International Conference on Music Information Retrieval (ISMIR'08), P14
   McFee B, 2012, IEEE T AUDIO SPEECH, V20, P2207, DOI 10.1109/TASL.2012.2199109
   Müller M, 2011, IEEE J-STSP, V5, P1088, DOI 10.1109/JSTSP.2011.2112333
   Nam J., 2011, P 12 INT SOC MUS INF, P175
   Pampalk E., 2002, Proceedings of the tenth ACM international conference on Multimedia, P570, DOI DOI 10.1145/641007.641121
   Plumbley MD, 2010, P IEEE, V98, P995, DOI 10.1109/JPROC.2009.2030345
   Riley M., 2008, INT S MUSIC INFORM R, P295
   Robertson S, 2004, J DOC, V60, P503, DOI 10.1108/00220410410560582
   Robertson S., 1995, 4 TEXT RETRIEVAL C T, P73
   Schedl M, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1993036.1993038
   Schluter J., 2011, Proceedings of the 2011 Tenth International Conference on Machine Learning and Applications (ICMLA 2011), P118, DOI 10.1109/ICMLA.2011.102
   Scholler S, 2011, IEEE J-STSP, V5, P933, DOI 10.1109/JSTSP.2011.2161264
   Sculley D., 2010, P INT C WORLD WID WE, V19, P1177, DOI [DOI 10.1145/1772690.1772862, 10.1145/1772690.1772862]
   Smith EC, 2006, NATURE, V439, P978, DOI 10.1038/nature04485
   Sturm B.L., 2012, P 2 INT ACM WORKSHOP, V2012, P7, DOI 10.1145/2390848.2390851
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Turnbull Douglas, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P439, DOI 10.1145/1277741.1277817
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Wulfing J., 2012, ISMIR, P139
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
NR 63
TC 26
Z9 33
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1188
EP 1200
DI 10.1109/TMM.2014.2311016
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600002
DA 2024-07-18
ER

PT J
AU Fu, HZ
   Cao, XC
   Tang, D
   Han, YH
   Xu, D
AF Fu, Huazhu
   Cao, Xiaochun
   Tang, Dai
   Han, Yahong
   Xu, Dong
TI Regularity Preserved Superpixels and Supervoxels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Over-segmentation; spatial structure; superpixels; supervoxels
ID SEGMENTATION
AB Most existing superpixel algorithms ignore the spatial structure and regularity properties, which result in undesirable sizes and location relationships for the subsequent processing. In this paper, we introduce a new method to generate the regularity preserved superpixels. Starting from the lattice seeds, our method relocates them to the pixel with locally maximal edge magnitudes and treats them as the superpixel junctions. Then, the shortest path algorithm is employed to find the local optimal boundary connecting each adjacent junction pair. Thanks to the local constraints, our method obtains homogeneous superpixels with adjacency in lowly textured and uniform regions and simultaneously preserves the boundary adherence in the high contrast contents. Our method preserves the regularity property without significantly sacrificing the segmentation accuracy. Moreover, we extend this regular constraint for generating the supervoxels. Our method obtains the regular supervoxels, which preserves the structural relation on both spatial and temporal spaces of the video. Quantitative and qualitative experimental results on benchmark datasets demonstrate that our simple but effective method outperforms the existing regular superpixel methods.
C1 [Fu, Huazhu; Han, Yahong] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
   [Fu, Huazhu; Xu, Dong] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Security, Beijing 100093, Peoples R China.
   [Tang, Dai] Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
C3 Tianjin University; Nanyang Technological University; Chinese Academy of
   Sciences; Institute of Information Engineering, CAS; Tianjin University
RP Fu, HZ (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
EM hzfu@ntu.edu.sg; caoxiaochun@iie.ac.cn; debelletang@gmail.com;
   yahong@tju.edu.cn; dongxu@ntu.edu.sg
RI Xu, Dong/A-3694-2011; Wang, Meng/ITR-8699-2023; Fu, Huazhu/A-1411-2014
OI Fu, Huazhu/0000-0002-9702-5524
FU NSFC [61332012, 61003200, 61202166]; National Basic Research Program of
   China [2013CB329305]; 100 Talents Programme of The Chinese Academy of
   Sciences, and Strategic Priority Research Program of the Chinese Academy
   of Sciences [XDA06030601]
FX This work was supported by NSFC (No. 61332012; 61003200), National Basic
   Research Program of China (2013CB329305), 100 Talents Programme of The
   Chinese Academy of Sciences, and Strategic Priority Research Program of
   the Chinese Academy of Sciences (XDA06030601). The work of Y. Han was
   supported in part by the NSFC (under Grant 61202166). This paper is an
   extended version of the original paper which appeared in the Proceedings
   of IEEE ICME 2012 Conference and was among the top-rated 4% of ICME'12
   submissions. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Sen-Ching Cheung.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2009, IEEE I CONF COMP VIS, DOI 10.1109/ICCV.2009.5459175
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Carr P, 2009, IEEE I CONF COMP VIS, P2042, DOI 10.1109/ICCV.2009.5459450
   Chen CY, 2012, PROC CVPR IEEE, P1274, DOI 10.1109/CVPR.2012.6247811
   Cheng J, 2013, IEEE T MED IMAGING, V32, P1019, DOI 10.1109/TMI.2013.2247770
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Choi J, 2013, COMPUT VIS IMAGE UND, V117, P660, DOI 10.1016/j.cviu.2013.02.003
   Choi MJ, 2012, IEEE T PATTERN ANAL, V34, P240, DOI 10.1109/TPAMI.2011.119
   Dai Tang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P765, DOI 10.1109/ICME.2012.184
   Dijkstra E. W., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   DOLLAR P, 2006, P IEEE C COMP VIS PA, P1964, DOI DOI 10.1109/CVPR.2006.298
   Felzenszwalb PF, 2011, IEEE T PATTERN ANAL, V33, P721, DOI 10.1109/TPAMI.2010.135
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   HOU X, 2013, PROC CVPR
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Kohli Pushmeet., 2008, CVPR, DOI DOI 10.1109/CVPR.2008.4587417
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lee YJ, 2012, IEEE T PATTERN ANAL, V34, P346, DOI 10.1109/TPAMI.2011.122
   Levinshtein A, 2011, LECT NOTES COMPUT SC, V6492, P369, DOI 10.1007/978-3-642-19315-6_29
   Levinshtein A, 2010, LECT NOTES COMPUT SC, V6312, P480, DOI 10.1007/978-3-642-15552-9_35
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Li ZG, 2012, PROC CVPR IEEE, P789, DOI [10.1109/CVPR.2012.6247750, 10.1109/ISRA.2012.6219309]
   LIANG L, 2013, PROC CVPR
   Liu S, 2012, IEEE T MULTIMEDIA, V14, P361, DOI 10.1109/TMM.2011.2174780
   Lucchi A, 2012, IEEE T MED IMAGING, V31, P474, DOI 10.1109/TMI.2011.2171705
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323
   MONTERO R, 2009, P INT MATH FORUM, V4, P1305
   Moore AP, 2010, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2010.5539890
   MOORE AP, 2008, P IEEE C COMP VIS PA, P1
   Mori G, 2004, PROC CVPR IEEE, P326
   Paris S, 2008, LECT NOTES COMPUT SC, V5303, P460, DOI 10.1007/978-3-540-88688-4_34
   PAYNE LE, 1967, SIAM REV, V9, P453, DOI 10.1137/1009070
   Perronnin F, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P329
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   SARGIN M, 2009, PROC ICCV, P560
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tsai D., 2012, INT J COMPUT VISION, P1
   Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x
   Veksler O, 2010, LECT NOTES COMPUT SC, V6315, P211, DOI 10.1007/978-3-642-15555-0_16
   Wang P, 2013, INT J COMPUT VISION, V103, P1, DOI 10.1007/s11263-012-0588-6
   Wang XF, 2009, IEEE INT CON MULTI, P642, DOI 10.1109/ICME.2009.5202578
   Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45
   Xu CL, 2012, PROC CVPR IEEE, P1202, DOI 10.1109/CVPR.2012.6247802
   Zeng K, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640445
NR 53
TC 38
Z9 40
U1 2
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1165
EP 1175
DI 10.1109/TMM.2014.2305571
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800023
DA 2024-07-18
ER

PT J
AU Bin Amin, T
   Marziliano, P
   German, JS
AF Bin Amin, Talal
   Marziliano, Pina
   German, James Sneed
TI Glottal and Vocal Tract Characteristics of Voice Impersonators
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Acoustic; disguise; formant; glottal; open quotient; speech rate; vocal
   tract; voice identity; voice impersonator
ID SPEAKER RECOGNITION; DIALECT; SPEECH; VOWEL; SEX
AB Voice impersonators possess a flexible voice which allows them to imitate and create different voice identities. These impersonations present a challenge for forensic analysis and speaker identification systems. To better understand the phenomena underlying successful voice impersonation, we collected a database of synchronous speech and ElectroGlottoGraphic (EGG) signals from three voice impersonators each producing nine distinct voice identities. We analyzed glottal and vocal tract measures including F0, speech rate, vowel formant frequencies, and timing characteristics of the vocal folds. Our analysis confirmed that the impersonators modulated all four parameters in producing the voices, and provides a lower bound on the scale of variability that is available to impersonators. Importantly, vowel formant differences across voices were highly dependent on vowel category, showing that such effects cannot be captured by global transformations that ignore the linguistic parse. We address this issue through the development of a no-reference objective metric based on the vowel-dependent variance of the formants associated with each voice. This metric both ranks the impersonators natural voices highly, and correlates strongly with the results of a subjective listening test. Together, these results demonstrate the utility of voice variability data for the development of voice disguise detection and speaker identification applications.
C1 [Bin Amin, Talal] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Marziliano, Pina] Nanyang Technol Univ, Div Informat Engn, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [German, James Sneed] Nanyang Technol Univ, Sch Humanities & Social Sci, Div Linguist & Multilingual Studies, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University;
   Nanyang Technological University
RP Bin Amin, T (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM talal1@e.ntu.edu.sg
RI Amin, Talal/AAT-7490-2020
OI German, James Sneed/0000-0002-4972-2421
CR Amin T. B., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P450, DOI 10.1109/ICME.2012.142
   Amin T. B., 2013, P IEEE INT C INF COM
   [Anonymous], 1971, Acoustic theory of speech production: with calculations based on X-ray studies of Russian articulations Internet
   [Anonymous], 2002, THESIS U CALIFORNIA
   [Anonymous], 2000, ACOUSTIC PHONETICS
   Boersma P., 2009, Praat: Doing Phonetics by Computer
   Bradlow AR, 1996, SPEECH COMMUN, V20, P255, DOI 10.1016/S0167-6393(96)00063-5
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Brend R., 1975, LANG SEX DIFF DOMIN, V86
   BYRD D, 1994, SPEECH COMMUN, V15, P39, DOI 10.1016/0167-6393(94)90039-6
   Campbell JP, 2009, IEEE SIGNAL PROC MAG, V26, P95, DOI 10.1109/MSP.2008.931100
   CAMPBELL WallaceH., 2003, Introduction to geomagnetic fields, P215
   CHILDERS DG, 1985, CRIT REV BIOMED ENG, V12, P131
   COLEMAN RO, 1976, J SPEECH HEAR RES, V19, P168, DOI 10.1044/jshr.1901.168
   Edwards J, 2004, J SPEECH LANG HEAR R, V47, P421, DOI 10.1044/1092-4388(2004/034)
   Eriksson A., 1997, Proceedings of the European Conference on Speech Technology, Rhodes, P1043
   Fant Gunnar., 1966, Speech Transmission Laboratory Quarterly Progress and Status Report, V1, P22
   Farrús M, 2010, INT J SPEECH LANG LA, V17, P119, DOI 10.1558/ijsll.v17i1.119
   Fourcin A, 2000, VOICE QUALITY MEASUR
   German JS, 2013, J PHONETICS, V41, P228, DOI 10.1016/j.wocn.2013.03.001
   Henrich N, 2004, J ACOUST SOC AM, V115, P1321, DOI 10.1121/1.1646401
   HIGGINS MB, 1991, J SPEECH HEAR RES, V34, P1000, DOI 10.1044/jshr.3405.1000
   Hillenbrand JM, 2009, ATTEN PERCEPT PSYCHO, V71, P1150, DOI 10.3758/APP.71.5.1150
   Hollien H., 2002, FORENSIC VOICE IDENT
   Honorof DN, 2010, J ACOUST SOC AM, V128, P3095, DOI 10.1121/1.3488347
   Jacewicz E, 2009, LANG VAR CHANGE, V21, P233, DOI 10.1017/S0954394509990093
   Kitamura T, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P813
   Latinus M, 2011, CURR BIOL, V21, pR143, DOI 10.1016/j.cub.2010.12.033
   Lee E., 2006, P 42 ANN M CHIC LING
   Ma EPM, 2010, J VOICE, V24, P146, DOI 10.1016/j.jvoice.2008.08.004
   Machado A. F., 2010, P SOUND MUS COMP SMC
   Magrin-Chagnolleau, 2003, P EUR ISCA GEN SWITZ, P33
   Mary L, 2012, INT J SPEECH TECHNOL, V15, P407, DOI 10.1007/s10772-012-9163-3
   MOON SJ, 1994, J ACOUST SOC AM, V96, P40, DOI 10.1121/1.410492
   Munson B, 2001, J SPEECH LANG HEAR R, V44, P778, DOI 10.1044/1092-4388(2001/061)
   Neocleous A., 1998, PROC EUR SIGNAL PROC, P697
   Perrot P, 2007, LECT NOTES COMPUT SC, V4391, P101
   PETERSON GE, 1952, J ACOUST SOC AM, V24, P175, DOI 10.1121/1.1906875
   Pierrehumbert JB, 2004, J ACOUST SOC AM, V116, P1905, DOI 10.1121/1.1788729
   ROTHENBERG M, 1992, J VOICE, V6, P36, DOI 10.1016/S0892-1997(05)80007-4
   SAMBUR MR, 1975, IEEE T ACOUST SPEECH, VAS23, P176, DOI 10.1109/TASSP.1975.1162664
   SMITH BL, 1987, J SPEECH HEAR RES, V30, P522, DOI 10.1044/jshr.3004.522
   Spearman C, 1904, AM J PSYCHOL, V15, P72, DOI 10.2307/1412159
   Stylianou Y, 2009, INT CONF ACOUST SPEE, P3585, DOI 10.1109/ICASSP.2009.4960401
   van Rie J., 1995, P 13 INT C PHON SCI, P290
   Winkler R, 2005, ZAS PAPERS LINGUISTI, V40, P213
   Wolk L, 2012, J VOICE, V26, pE111, DOI 10.1016/j.jvoice.2011.04.007
   Yuan J, 2008, J ACOUST SOC AM, V124, P2078, DOI 10.1121/1.2968700
   Yuan JH, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P541
   Zetterholm E., 2006, Proc. Int. Conf. Speech Sci. Tech, P70
   Zetterholm E., 2009, LUND WORKING PAPERS, V46, P269
NR 51
TC 11
Z9 14
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 668
EP 678
DI 10.1109/TMM.2014.2300071
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500008
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Kim, T
   Kang, J
   Lee, S
   Bovik, AC
AF Kim, Taewan
   Kang, Jiwoo
   Lee, Sanghoon
   Bovik, Alan C.
TI Multimodal Interactive Continuous Scoring of Subjective 3D Video Quality
   of Experience
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal interactive continuous scoring of quality (MICSQ); 3D quality
   of experience (QoE); subjective assessment; visual comfort evaluation;
   interactive continuous subjective quality assessment; empirical 3D
   distortion
ID VISUAL FATIGUE; ISSUES; DEPTH
AB People experience a variety of 3D visual programs, such as 3D cinema, 3D TV and 3D games, making it necessary to deploy reliable methodologies for predicting each viewer's subjective experience. We propose a new methodology that we call multi-modal interactive continuous scoring of quality (MICSQ). MICSQ is composed of a device interaction process between the 3D display and a separate device (PC, tablet, etc.) used as an assessment tool, and a human interaction process between the subject(s) and the separate device. The scoring process is multimodal, using aural and tactile cues to help engage and focus the subject(s) on their tasks by enhancing neuroplasticity. Recorded human responses to 3D visualizations obtained via MICSQ correlate highly with measurements of spatial and temporal activity in the 3D video content. We have also found that 3D quality of experience (QoE) assessment results obtained using MICSQ are more reliable over a wide dynamic range of content than obtained by the conventional single stimulus continuous quality evaluation (SSCQE) protocol. Moreover, the wireless device interaction process makes it possible for multiple subjects to assess 3D QoE simultaneously in a large space such as a movie theater, at different viewing angles and distances. We conducted a series of interesting 3D experiments showing the accuracy and versatility of the new system, while yielding new findings on visual comfort in terms of disparity, motion and an interesting relation between the naturalness and depth of field (DOF) of a stereo camera.
C1 [Kim, Taewan; Kang, Jiwoo; Lee, Sanghoon] Yonsei Univ, Dept Elect & Elect Engn, Seoul 120749, South Korea.
   [Bovik, Alan C.] Univ Texas Austin, Dept Elect & Comp Engn, LIVE, Austin, TX 78712 USA.
C3 Yonsei University; University of Texas System; University of Texas
   Austin
RP Lee, S (corresponding author), Yonsei Univ, Dept Elect & Elect Engn, Seoul 120749, South Korea.
EM enoughrice21@yonsei.ac.kr; jwkang@yonsei.ac.kr; slee@yonsei.ac.kr
RI Lee, Sanghoon/A-3430-2019; kim, taewan/AAM-6887-2020; Kang,
   Jiwoo/AAS-9155-2021; Bovik, Alan/B-6717-2012
OI Lee, Sanghoon/0000-0001-9895-5347; Kang, Jiwoo/0000-0001-7622-0817; kim,
   taewan/0000-0003-3319-7797; Bovik, Alan/0000-0001-6067-710X
FU Technology Innovation Program [10042402]; Ministry of Knowledge Economy
   (MKE) of Korea; MSIP (Ministry of Science, ICT&Future Planning), Korea,
   under the C-ITRC (Convergence Information Technology Research Center)
   support program [NIPA-2013-H0401-13-1003]
FX This work was supported by the Technology Innovation Program (10042402,
   Standardization of 3D human factors and 3D medical application services)
   funded by the Ministry of Knowledge Economy (MKE) of Korea and the MSIP
   (Ministry of Science, ICT&Future Planning), Korea, under the C-ITRC
   (Convergence Information Technology Research Center) support program
   (NIPA-2013-H0401-13-1003) supervised by the NIPA (National IT Industry
   Promotion Agency). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Yiannis
   Andreopoulos.
CR [Anonymous], 2012, METH SUBJ ASS QUAL T
   [Anonymous], 2012, P33331 IEEE
   [Anonymous], 2008, Subjective video quality assessment methods for multimedia applications. Recommendation P.910
   Barbieri T., 2006, SPECIALIZED TRAINING, P234
   Blohm W., 1997, Journal of the Society for Information Display, V5, P307, DOI 10.1889/1.1985167
   Cao X, 2011, IEEE MULTIMEDIA, V18, P12, DOI 10.1109/MMUL.2011.65
   Daly SJ, 2011, IEEE T BROADCAST, V57, P347, DOI 10.1109/TBC.2011.2127630
   Goldmann L., 2010, ELECT IMAG 3D IMAGE
   Hale KS, 2009, ERGONOMICS, V52, P187, DOI 10.1080/00140130802376000
   Ijsselsteijn W, 1998, DISPLAYS, V18, P207, DOI 10.1016/S0141-9382(98)00022-5
   Image Safety, 2005, IWA32005 INT STAND O
   Jia DW, 2013, IEEE T HAPTICS, V6, P46, DOI [10.1109/TOH.2012.41, 10.1109/ToH.2012.41]
   Kim L., 2008, P 18 INT C ART REAL
   Lambooij M, 2011, DISPLAYS, V32, P209, DOI 10.1016/j.displa.2011.05.012
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lee K., IEEE T IMAG IN PRESS
   Liou HL, 1997, J OPT SOC AM A, V14, P1684, DOI 10.1364/JOSAA.14.001684
   Massimino M. J., 1992, Sensory substitution for force feedback in space teleoperation
   Münch TA, 2009, NAT NEUROSCI, V12, P1308, DOI 10.1038/nn.2389
   Naganuma T, 2005, NEUROSCI RES, V51, P147, DOI 10.1016/j.neures.2004.10.009
   OHZAWA I, 1990, SCIENCE, V249, P1037, DOI 10.1126/science.2396096
   Pastoor S., 1992, P IEE C STER TEL
   Ricciardi S, 2010, J VISUAL LANG COMPUT, V21, P33, DOI 10.1016/j.jvlc.2009.07.001
   Saygili G, 2011, IEEE T BROADCAST, V57, P593, DOI 10.1109/TBC.2011.2131450
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Speranza F., 2006, P STEREOSCOPIC DISPL, V6055
   Steffin M., VISUAL HAPTIC INTERF
   Stone RJ, 2002, HUM FAC ER, P827
   Tam WJ, 2011, IEEE T BROADCAST, V57, P335, DOI 10.1109/TBC.2011.2125070
   Tanimoto M., 2008, ISOIIEC JTCLISC29 WG
   WALTHER D, 2004, P 8 EUR C COMP VIS
   Wickens C.D., 1992, ENG PSYCHOL HUMAN PE
   WOO GCS, 1974, VISION RES, V14, P473, DOI 10.1016/0042-6989(74)90035-2
   Yang SN, 2012, OPTOMETRY VISION SCI, V89, P1068, DOI 10.1097/OPX.0b013e31825da430
   Yano S, 2004, DISPLAYS, V25, P141, DOI 10.1016/j.displa.2004.09.002
   Yano S, 2002, DISPLAYS, V23, P191, DOI 10.1016/S0141-9382(02)00038-0
   Zhang L, 2011, IEEE T BROADCAST, V57, P572, DOI 10.1109/TBC.2011.2131491
   Zilly F, 2011, P IEEE, V99, P590, DOI 10.1109/JPROC.2010.2095810
NR 39
TC 37
Z9 40
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 387
EP 402
DI 10.1109/TMM.2013.2292592
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Toni, L
   Maugey, T
   Frossard, P
AF Toni, Laura
   Maugey, Thomas
   Frossard, Pascal
TI Correlation-Aware Packet Scheduling in Multi-Camera Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Foresighted packet scheduling; source correlation analysis; multiview
   streaming; interview correlation; rate-distortion optimization;
   multimedia communication
ID DISTRIBUTED SOURCE; VIDEO; INFORMATION
AB In multiview applications, multiple cameras acquire the same scene from different viewpoints and generally produce correlated video streams. This results in large amounts of highly redundant data. In order to save resources, it is critical to handle properly this correlation during encoding and transmission of the multiview data. In this work, we propose a correlation-aware packet scheduling algorithm for multi-camera networks, where information from all cameras are transmitted over a bottleneck channel to clients that reconstruct the multiview images. The scheduling algorithm relies on a new rate-distortion model that captures the importance of each view in the scene reconstruction. We propose a problem formulation for the optimization of the packet scheduling policies, which adapt to variations in the scene content. Then, we design a low complexity scheduling algorithm based on a trellis search that selects the subset of candidate packets to be transmitted towards effective multiview reconstruction at clients. Extensive simulation results confirm the gain of our scheduling algorithm when inter-source correlation information is used in the scheduler, compared to scheduling policies with no information about the correlation or non-adaptive scheduling policies. We finally show that increasing the optimization horizon in the packet scheduling algorithm improves the transmission performance, especially in scenarios where the level of correlation rapidly varies with time.
C1 [Toni, Laura; Maugey, Thomas; Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Toni, L (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
EM laura.toni@epfl.ch; thomas.maugey@epfl.ch; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019
OI Toni, Laura/0000-0002-8441-8791
CR Anantrasirichai Nantheera, 2008, 5th International Conference on Visual Information Engineering, VIE 2008, P67, DOI 10.1049/cp:20080285
   [Anonymous], 2012, 802112012 IEEE
   Bandari D, 2012, IEEE T WIREL COMMUN, V11, P4438, DOI 10.1109/TWC.2012.101912.112297
   Chen ZB, 2009, IEEE INT SYMP CIRC S, P1795, DOI 10.1109/ISCAS.2009.5118125
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P3179, DOI 10.1109/TIP.2011.2158230
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P744, DOI 10.1109/TIP.2010.2070074
   Cheung NM, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P269
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Cisco Systems Inc, 2009, TRANSP DIV PERF ROUT
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Dai R, 2009, IEEE T MULTIMEDIA, V11, P1148, DOI 10.1109/TMM.2009.2026100
   Jia J. T. Q., 2004, U.S. Patent, Patent No. [WO2004059913 A3, 2004059913]
   Kubota A, 2007, IEEE SIGNAL PROC MAG, V24, P10, DOI 10.1109/MSP.2007.905873
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Kurutepe E, 2008, IEEE IMAGE PROC, P3088, DOI 10.1109/ICIP.2008.4712448
   Li SZ, 2011, THEORETICAL ASPECTS OF DISTRIBUTED COMPUTING IN SENSOR NETWORKS, P191, DOI 10.1007/978-3-642-14849-1_7
   Liu YW, 2010, J VIS COMMUN IMAGE R, V21, P523, DOI 10.1016/j.jvcir.2010.02.004
   Lou JG, 2007, ADV MULTIMED, V2007, DOI 10.1155/2007/97535
   Maugey T, 2013, IEEE T MULTIMEDIA, V15, P1070, DOI 10.1109/TMM.2013.2246147
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Pan  Z., 2011, P IEEE ICC, P1
   Shiang HP, 2010, IEEE T CIRC SYST VID, V20, P505, DOI 10.1109/TCSVT.2009.2035837
   SLEPIAN D, 1973, IEEE T INFORM THEORY, V19, P471, DOI 10.1109/TIT.1973.1055037
   Toni L., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P77, DOI 10.1109/PV.2012.6229746
   Toni L., 2012, ARXIV12124455
   Vuran MC, 2006, IEEE ACM T NETWORK, V14, P316, DOI 10.1109/TNET.2006.872544
   Wang P., 2011, P IEEE INT C COMP CO, P746
   Wu YN, 2009, IEEE T INFORM THEORY, V55, P1709, DOI 10.1109/TIT.2009.2013016
   WYNER AD, 1976, IEEE T INFORM THEORY, V22, P1, DOI 10.1109/TIT.1976.1055508
   Xiong ZX, 2004, IEEE SIGNAL PROC MAG, V21, P80, DOI 10.1109/MSP.2004.1328091
   Yang Z., 2010, ACM T MULTIMEDIA COM, V6
   Yiu WPK, 2007, IEEE J SEL AREA COMM, V25, P1717, DOI 10.1109/JSAC.2007.071210
   Zhou L, 2011, IEEE T MULTIMEDIA, V13, P1040, DOI 10.1109/TMM.2011.2160716
NR 33
TC 11
Z9 13
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 496
EP 509
DI 10.1109/TMM.2013.2291531
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Guan, T
   He, YF
   Gao, J
   Yang, JZ
   Yu, JQ
AF Guan, Tao
   He, Yunfeng
   Gao, Juan
   Yang, Jianzhong
   Yu, Junqing
TI On-Device Mobile Visual Location Recognition by Integrating Vision and
   Inertial Sensors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile visual location recognition; on-device; vector quantization;
   vision and inertial sensors integration
AB This paper deals with the problem of city scale on-device mobile visual location recognition by fusing the inertial sensors and computer vision techniques. The main contributions are as follows: Firstly, we design an efficient vector quantization strategy by combining the Transform Coding (TC) and Residual Vector Quantization (RVQ). Our method can compress a visual descriptor into only several bytes while providing reasonable searching accuracy, which makes the managing of city scale image database directly on mobile devices come true. Secondly, we integrate the information from inertial sensors into the Vector of Locally Aggregated Descriptors (VLAD) generation and image similarity evaluation processes. Our method is not only fast enough for on-device implementation, but it also can improve the location recognition accuracy obviously. Thirdly, we also release a set of 1.295 million geo-tagged street view images with the information from inertial sensors, as well as a difficult set of query images. These resources can be used as a new benchmark to facilitate further research in the area. Experimental results prove the validity of the proposed methods for on-device mobile visual location recognition applications.
C1 [Guan, Tao; He, Yunfeng; Gao, Juan; Yang, Jianzhong; Yu, Junqing] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP He, YF (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
EM qd_gt@mail.hust.edu.cn; he_yunfeng@126.com
RI he, yun/JMB-6362-2023
FU National Natural Science Foundation of China (NSFC) [60903095, 61173114,
   61272202, 61202300, 61272236]
FX This work was supported by the National Natural Science Foundation of
   China (NSFC) under Grant No. 60903095, 61173114, 61272202, 61202300, and
   61272236. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Chia-Wen Lin.
CR [Anonymous], P 11 EUR C COMP VIS
   Baatz G., 2010, P 11 EUR C COMP VIS
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Brandt J, 2010, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2010.5539852
   Chandrasekhar V, 2012, INT J COMPUT VISION, V96, P384, DOI 10.1007/s11263-011-0453-z
   Chen D., 2011, P AS C SIGN SYST COM
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Chen DM, 2008, IEEE DATA COMPR CONF, P143, DOI 10.1109/DCC.2009.33
   Chen T, 2011, IEEE T CIRC SYST VID, V21, P1476, DOI 10.1109/TCSVT.2011.2161413
   Chen YJ, 2010, SENSORS-BASEL, V10, P11259, DOI 10.3390/s101211259
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Ji RR, 2012, INT J COMPUT VISION, V96, P290, DOI 10.1007/s11263-011-0472-9
   Ji RR, 2011, INT CONF ACOUST SPEE, P2400
   Kurz D., 2011, P IEEE C COMP VIS PA
   Li Z, 2012, IEEE SIGNAL PROC LET, V19, P459, DOI 10.1109/LSP.2012.2203120
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   Schindler G., 2007, P IEEE C COMP VIS PA, V2007, P1
   Schroth G, 2012, INT CONF ACOUST SPEE, P2357, DOI 10.1109/ICASSP.2012.6288388
   Schroth G, 2011, IEEE SIGNAL PROC MAG, V28, P77, DOI 10.1109/MSP.2011.940882
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Takacs G., 2008, MIR 08, P427, DOI DOI 10.1145/1460096.1460165
   Weiss Y., 2008, ADV NEURAL INF PROCE, V21, P1
NR 24
TC 67
Z9 70
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1688
EP 1699
DI 10.1109/TMM.2013.2265674
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800019
DA 2024-07-18
ER

PT J
AU Wu, Y
   Zhang, ZZ
   Wu, C
   Li, ZP
   Lau, FCM
AF Wu, Yu
   Zhang, Zhizhong
   Wu, Chuan
   Li, Zongpeng
   Lau, Francis C. M.
TI CloudMoV: Cloud-Based Mobile Social TV
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computers and information processing; Mobile computing; Communications
   technology; TV; Mobile TV
AB The rapidly increasing power of personal mobile devices (smartphones, tablets, etc.) is providing much richer contents and social interactions to users on the move. This trend however is throttled by the limited battery lifetime of mobile devices and unstable wireless connectivity, making the highest possible quality of service experienced by mobile users not feasible. The recent cloud computing technology, with its rich resources to compensate for the limitations of mobile devices and connections, can potentially provide an ideal platform to support the desired mobile services. Tough challenges arise on how to effectively exploit cloud resources to facilitate mobile services, especially those with stringent interaction delay requirements. In this paper, we propose the design of a Cloud-based, novel Mobile sOcial tV system (CloudMoV). The system effectively utilizes both PaaS (Platform-as-a-Service) and IaaS (Infrastructure-as-a-Service) cloud services to offer the living-room experience of video watching to a group of disparate mobile users who can interact socially while sharing the video. To guarantee good streaming quality as experienced by the mobile users with time-varying wireless connectivity, we employ a surrogate for each user in the IaaS cloud for video downloading and social exchanges on behalf of the user. The surrogate performs efficient stream transcoding that matches the current connectivity quality of the mobile user. Given the battery life as a key performance bottleneck, we advocate the use of burst transmission from the surrogates to the mobile users, and carefully decide the burst size which can lead to high energy efficiency and streaming quality. Social interactions among the users, in terms of spontaneous textual exchanges, are effectively achieved by efficient designs of data storage with BigTable and dynamic handling of large volumes of concurrent messages in a typical PaaS cloud. These various designs for flexible transcoding capabilities, battery efficiency of mobile devices and spontaneous social interactivity together provide an ideal platform for mobile social TV services. We have implemented CloudMoV on Amazon EC2 and Google App Engine and verified its superior performance based on real-world experiments.
C1 [Wu, Yu; Zhang, Zhizhong; Wu, Chuan; Lau, Francis C. M.] Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Li, Zongpeng] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Hong Kong; University of Calgary
RP Wu, Y (corresponding author), Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM ywu@cs.hku.hk; zzzhang@cs.hku.hk; cwu@cs.hku.hk; zongpeng@ucalgary.ca;
   fcmlau@cs.hku.hk
RI Lau, Francis/AAN-8816-2020; Wu, Chuan/E-9919-2010
OI Wu, Chuan/0000-0002-3144-4398
FU RGC [HKU 717812E]; Shenzhen Key Lab of Cloud Computing Tech. App.
   (SZCCTA)
FX This work was supported in part by a grant from RGC under the contract
   HKU 717812E, an open grant from Shenzhen Key Lab of Cloud Computing
   Tech. & App. (SZCCTA). The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Joel
   Rodrigues.
CR *3GPP, 25331 3GPP TS
   Anastasi G., 2004, P WORKSH LING THEOR, P24
   Carroll A., 2010, P USENIXATC
   Chang F., 2006, P OSDI
   Chorianopoulos K, 2008, INT J HUM-COMPUT INT, V24, P113, DOI 10.1080/10447310701821574
   COPPENS T, 2004, P EUROITV
   Ducheneaut N, 2008, INT J HUM-COMPUT INT, V24, P136, DOI 10.1080/10447310701821426
   Flinn J, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P48, DOI 10.1145/319344.319155
   Huang ZX, 2011, IEEE INFOCOM SER, P201, DOI 10.1109/INFCOM.2011.5935009
   Kosta S., 2012, P IEEE INFOCOM
   Liu Zimu, 2012, P IEEE INFOCOM
   Mei C., 2003, CHI EA 03, P926
   Pereira R, 2011, IEEE DATA COMPR CONF, P471, DOI 10.1109/DCC.2011.75
   Santos J, 2008, COMPUT NETW, V52, P228, DOI 10.1016/j.comnet.2007.09.002
   Satyanarayanan M, 2009, IEEE PERVAS COMPUT, V8, P14, DOI 10.1109/MPRV.2009.82
   Schatz R., 2007, P ITI
   Schatz R., 2008, P 2008 INT S BROADB
   Yuan W., 2003, Operating Systems Review, V37, P149, DOI 10.1145/1165389.945460
   Zhang W., 2012, P IEEE GLOBECOM
   Zhang X., 2011, MULTIMED TOOLS APPL, P1
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 21
TC 24
Z9 24
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 821
EP 832
DI 10.1109/TMM.2013.2240670
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500011
DA 2024-07-18
ER

PT J
AU Jung, IL
   Chung, TY
   Sim, JY
   Kim, CS
AF Jung, Il-Lyong
   Chung, Tae-Young
   Sim, Jae-Young
   Kim, Chang-Su
TI Consistent Stereo Matching Under Varying Radiometric Conditions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color similarity; color transform; consistency criterion; forward
   mapping; intermediate view synthesis; inverse mapping; stereo matching
AB A consistent stereo matching (CSM) algorithm under varying radiometric conditions, such as lighting and exposure variations, for intermediate view synthesis is proposed in this work. First, we transform the colors of stereo images adaptively so that they are similar at corresponding pixels. Since the correspondences are generally unknown before stereo matching, we estimate pseudo-disparity vectors by sorting pixels based on the cumulative color histograms and use those pseudo vectors in the color transform. Then, to improve the accuracy of stereo matching, we jointly estimate the disparity maps for virtual intermediate views as well as those for real views, based on the consistency criterion that an object point should have the same disparity through all the views. Specifically, we compute matching costs using the reliability term and aggregate the costs to obtain initial disparity maps. We then refine the initial disparity maps by minimizing an energy function, which includes the consistency term. Experimental results show that the proposed CSM algorithm significantly reduces the error rate of disparity estimation under different radiometric conditions and synthesizes high quality intermediate views.
C1 [Jung, Il-Lyong; Chung, Tae-Young; Kim, Chang-Su] Korea Univ, Sch Elect Engn, Seoul, South Korea.
   [Sim, Jae-Young] Ulsan Natl Inst Sci & Technol, Sch Elect & Comp Engn, Ulsan, South Korea.
C3 Korea University; Ulsan National Institute of Science & Technology
   (UNIST)
RP Jung, IL (corresponding author), Korea Univ, Sch Elect Engn, Seoul, South Korea.
EM illyong@korea.ac.kr; lovelool17@korea.ac.kr; jysim@unist.ac.kr;
   changsukim@korea.ac.kr
OI Kim, Chang-Su/0000-0002-4276-1831
FU National Research Foundation of Korea (NRF); Ministry of Education,
   Science and Technology (MEST) [2012-011031]; Basic Science Research
   Program through the NRF of Korea; MEST [2010-0006595]
FX This work was supported in part by the National Research Foundation of
   Korea (NRF) grant funded by the Ministry of Education, Science and
   Technology (MEST) (No. 2012-011031), and in part by the Basic Science
   Research Program through the NRF of Korea funded by the MEST
   (2010-0006595). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Pascal Frossard.
CR [Anonymous], P IEEE WORKSH MULT S
   [Anonymous], DIGIAL IMAGE PROCESS
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], AI MEMO
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Egnal G, 2004, IMAGE VISION COMPUT, V22, P943, DOI 10.1016/j.imavis.2004.03.018
   Finlayson G. D., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P475, DOI 10.1007/BFb0055685
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   Forsyth D. A., 2002, Computer vision: a modern approach, DOI DOI 10.5555/580035
   Fua P., 1991, P 12 INT JOINT C ART, P1292
   FUNT BV, 1995, IEEE T PATTERN ANAL, V17, P522, DOI 10.1109/34.391390
   Gijsenij A, 2011, IEEE T IMAGE PROCESS, V20, P2475, DOI 10.1109/TIP.2011.2118224
   Gong ML, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P610
   Heo YS, 2011, IEEE T PATTERN ANAL, V33, P807, DOI 10.1109/TPAMI.2010.136
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hirschmüller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221
   Ince S, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/803231
   Jung IL, 2007, IEEE IMAGE PROC, P545
   Juyang Weng, 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P64, DOI 10.1109/CCV.1988.589972
   Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394
   Klaus A, 2006, INT C PATT RECOG, P15
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lin MH, 2004, IEEE T PATTERN ANAL, V26, P1073, DOI 10.1109/TPAMI.2004.54
   Mancini A, 1998, P SOC PHOTO-OPT INS, V3295, P53, DOI 10.1117/12.307193
   MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482
   McVeigh JS, 1996, SIGNAL PROCESS-IMAGE, V9, P21, DOI 10.1016/S0923-5965(96)00005-7
   Pritchett P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P754, DOI 10.1109/ICCV.1998.710802
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Scharstein D., 2007, PROC IEEE C COMPUT V, P1
   Sun J, 2005, PROC CVPR IEEE, P399
   Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509
   Tao H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P532, DOI 10.1109/ICCV.2001.937562
   Tombari F., 2008, Computer Vision and Pattern Recognition, P1
   TRAHANIAS PE, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL III, P545, DOI 10.1109/ICPR.1992.202045
   Trucco E., 1998, INTRO TECHNIQUES 3D
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Yang RG, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P225, DOI 10.1109/PCCGA.2002.1167864
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Yu W, 2010, IEEE T CIRC SYST VID, V20, P1509, DOI 10.1109/TCSVT.2010.2077771
   Zhang GF, 2009, IEEE T PATTERN ANAL, V31, P974, DOI 10.1109/TPAMI.2009.52
   Zitnick CL, 2007, INT J COMPUT VISION, V75, P49, DOI 10.1007/s11263-006-0018-8
NR 44
TC 24
Z9 27
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 56
EP 69
DI 10.1109/TMM.2012.2225041
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600005
DA 2024-07-18
ER

PT J
AU Li, P
   Wang, M
   Cheng, J
   Xu, CS
   Lu, HQ
AF Li, Peng
   Wang, Meng
   Cheng, Jian
   Xu, Changsheng
   Lu, Hanqing
TI Spectral Hashing With Semantically Consistent Graph for Image Indexing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graph Laplacian; metric learning; similarity search; spectral hashing
ID APPROXIMATE NEAREST-NEIGHBOR; DIMENSIONALITY; ALGORITHM
AB The ability of fast similarity search in a large-scale dataset is of great importance to many multimedia applications. Semantic hashing is a promising way to accelerate similarity search, which designs compact binary codes for a large number of images so that semantically similar images are mapped to close codes. Retrieving similar neighbors is then simply accomplished by retrieving images that have codes within a small Hamming distance of the code of the query. Among various hashing approaches, spectral hashing (SH) has shown promising performance by learning the binary codes with a spectral graph partitioning method. However, the Euclidean distance is usually used to construct the graph Laplacian in SH, which may not reflect the inherent distribution of the data. Therefore, in this paper, we propose a method to directly optimize the graph Laplacian. The learned graph, which can better represent similarity between samples, is then applied to SH for effective binary code learning. Meanwhile, our approach, unlike metric learning, can automatically determine the scale factor during the optimization. Extensive experiments are conducted on publicly available datasets and the comparison results demonstrate the effectiveness of our approach.
C1 [Li, Peng; Cheng, Jian; Xu, Changsheng; Lu, Hanqing] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Wang, Meng] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Hefei
   University of Technology
RP Li, P (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
EM pli@nlpr.ia.ac.cn; eric.mengwang@gmail.com; jcheng@nlpr.ia.ac.cn;
   csxu@nlpr.ia.ac.cn; luhq@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; , chengjian/KGL-5551-2024; Wang,
   Meng/ITR-8699-2023; YAN, LING/JXY-6904-2024
OI , chengjian/0000-0003-1289-2758; 
FU 973 Program [2010CB327905]; National Natural Science Foundation of China
   [61170127, 60975010, 60833006, 61070104]
FX This work was supported in part by the 973 Program under Project
   2010CB327905, by the National Natural Science Foundation of China under
   Grant 61170127, 60975010, 60833006, and 61070104. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xian-Sheng Hua.
CR Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   [Anonymous], P 20 INT C MACH LEAR
   [Anonymous], P C ADV NEUR INF PRO
   [Anonymous], 2004, P 20 ACM S COMP
   [Anonymous], P 1 ACM INT C MULT R
   [Anonymous], P 28 INT C MACH LEAR
   [Anonymous], 2006, Book Annosearch: Image auto-annotation by search, DOI DOI 10.1109/CVPR.2006.58
   [Anonymous], P ACM INT C MULT
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2001, J OPER RES SOC
   [Anonymous], P 33 INT ACM SIGIR C
   [Anonymous], 2009, NUSWIDE: A real-world web image database from National University of Singapore, DOI DOI 10.1145/1646396.1646452
   [Anonymous], P 8 IEEE WORKSH APPL
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 1997, SPECTRAL GRAPH THEOR
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Frome A., 2007, PROC 11 IEEE INT C C, P1
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Goldman J.G., 2005, Parkinson's Disease, P1
   He JR, 2006, IEEE T IMAGE PROCESS, V15, P3170, DOI 10.1109/TIP.2006.877491
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jain P., 2008, PROC IEEE C COMPUTER
   Jiang Y.-G., 2011, P 1 ACM INT C MULT R, P1
   Jin R, 2009, PROC CVPR IEEE, P896, DOI 10.1109/CVPRW.2009.5206684
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Mu YD, 2010, PROC CVPR IEEE, P3344, DOI 10.1109/CVPR.2010.5540024
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Schapire R. E., 2002, P MSRI WORKSH NONL E
   Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750
   Silpa- Anan C., 2008, PROC IEEE C COMPUTER, P1
   Stein Benno, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P527, DOI 10.1145/1277741.1277832
   Torralba A.B., 2008, PROC IEEE C COMPUTER
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang F, 2008, IEEE T KNOWL DATA EN, V20, P55, DOI 10.1109/TKDE.2007.190672
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, COMPUT VIS IMAGE UND, V113, P384, DOI 10.1016/j.cviu.2008.08.003
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   Weinberger K., 2005, P NIPS, V18, P1
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008
   Zhang H., 2006, 2006 IEEE COMP SOC C, V2, P2126, DOI DOI 10.1109/CVPR.2006.301
NR 49
TC 105
Z9 111
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 141
EP 152
DI 10.1109/TMM.2012.2199970
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600012
DA 2024-07-18
ER

PT J
AU Liu, Z
   Shi, R
   Shen, LQ
   Xue, YZ
   Ngan, KN
   Zhang, ZY
AF Liu, Zhi
   Shi, Ran
   Shen, Liquan
   Xue, Yinzhu
   Ngan, King Ngi
   Zhang, Zhaoyang
TI Unsupervised Salient Object Segmentation Based on Kernel Density
   Estimation and Two-Phase Graph Cut
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color saliency; graph cut; kernel density estimation; saliency model;
   salient object segmentation; seed adjustment; spatial saliency
ID VISUAL-ATTENTION; REGION DETECTION; FEATURE MAPS; IMAGE; MODEL; VIDEO;
   EXTRACTION; COLOR
AB In this paper, we propose an unsupervised salient object segmentation approach based on kernel density estimation (KDE) and two-phase graph cut. A set of KDE models are first constructed based on the pre-segmentation result of the input image, and then for each pixel, a set of likelihoods to fit all KDE models are calculated accordingly. The color saliency and spatial saliency of each KDE model are then evaluated based on its color distinctiveness and spatial distribution, and the pixel-wise saliency map is generated by integrating likelihood measures of pixels and saliency measures of KDE models. In the first phase of salient object segmentation, the saliency map based graph cut is exploited to obtain an initial segmentation result. In the second phase, the segmentation is further refined based on an iterative seed adjustment method, which efficiently utilizes the information of minimum cut generated using the KDE model based graph cut, and exploits a balancing weight update scheme for convergence of segmentation refinement. Experimental results on a dataset containing 1000 test images with ground truths demonstrate the better segmentation performance of our approach.
C1 [Liu, Zhi; Shi, Ran; Shen, Liquan; Xue, Yinzhu; Zhang, Zhaoyang] Shanghai Univ, Sch Commun & Informat Engn, Shanghai, Peoples R China.
   [Liu, Zhi; Shen, Liquan; Zhang, Zhaoyang] Shanghai Univ, Minist Educ, Key Lab Adv Display & Syst Applicat, Shanghai, Peoples R China.
   [Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
C3 Shanghai University; Shanghai University; Chinese University of Hong
   Kong
RP Liu, Z (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai, Peoples R China.
EM liuzhisjtu@163.com; dnasr@sohu.com; jsslq@163.com; mhtymhty@163.com;
   knngan@ee.cuhk.edu.hk; zhyzhang@staff.shu.edu.cn
RI LIU, Zhi/D-4518-2012; Shen, Liquan/D-4832-2012; Zhang,
   Zhaoyang/AFQ-9161-2022; Ngan, N/E-8240-2014
OI LIU, Zhi/0000-0002-8428-1131; Ngan, N/0000-0003-1946-3235
FU National Natural Science Foundation of China [61171144, 60602012];
   Shanghai Natural Science Foundation [11ZR1413000]; Shanghai Municipal
   Education Commission [12ZZ086]; Chinese Ministry of Education [212053]
FX This work was supported by National Natural Science Foundation of China
   under Grant No. 61171144 and No. 60602012, Shanghai Natural Science
   Foundation (No. 11ZR1413000), Innovation Program of Shanghai Municipal
   Education Commission (No. 12ZZ086), and the Key (Key grant) Project of
   Chinese Ministry of Education (No. 212053). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Charles D. (Chuck) Creusere.
CR Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2008, 2008 19 INT C PATT R
   Aziz MZ, 2008, IEEE T IMAGE PROCESS, V17, P633, DOI 10.1109/TIP.2008.919365
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cheng WH, 2007, IEEE T CIRC SYST VID, V17, P43, DOI 10.1109/TCSVT.2006.885717
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Elgammal A, 2002, P IEEE, V90, P1151, DOI 10.1109/JPROC.2002.801448
   Fu H, 2006, PATTERN RECOGN, V39, P1604, DOI 10.1016/j.patcog.2005.12.015
   Gopalakrishnan V, 2010, IEEE T IMAGE PROCESS, V19, P3232, DOI 10.1109/TIP.2010.2053940
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Hae Jong Seo, 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P45, DOI 10.1109/CVPR.2009.5204207
   Hall P, 1996, J MULTIVARIATE ANAL, V56, P165, DOI 10.1006/jmva.1996.0009
   Han JW, 2006, IEEE T CIRC SYST VID, V16, P141, DOI 10.1109/TCSVT.2005.859028
   Hou X., 2007, P IEEE CVPR
   Hu YQ, 2004, LECT NOTES COMPUT SC, V3332, P993
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jung C, 2010, IEEE INT CON MULTI, P590, DOI 10.1109/ICME.2010.5582577
   Kim W, 2010, IEEE INT CON MULTI, P1260, DOI 10.1109/ICME.2010.5583287
   Ko BC, 2006, J OPT SOC AM A, V23, P2462, DOI 10.1364/JOSAA.23.002462
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Le Meur O, 2010, IEEE T IMAGE PROCESS, V19, P2801, DOI 10.1109/TIP.2010.2052262
   Liu F, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1477, DOI 10.1109/ICME.2006.262821
   Liu T., 2007, P IEEE CVPR JUN
   Liu Z, 2010, IEEE IMAGE PROC, P253, DOI 10.1109/ICIP.2010.5652613
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Ngan K. N., 2009, IEEE COMMUN SOC MU E, V4, P6
   Park KT, 2007, INT CONF ACOUST SPEE, P617
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Setlur V, 2007, IEEE COMPUT GRAPH, V27, P80, DOI 10.1109/MCG.2007.133
   Toet A, 2011, IEEE T PATTERN ANAL, V33, P2131, DOI 10.1109/TPAMI.2011.53
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wang W, 2010, PROC CVPR IEEE, P2368, DOI 10.1109/CVPR.2010.5539927
   Xiang SM, 2010, IEEE T IMAGE PROCESS, V19, P3024, DOI 10.1109/TIP.2010.2052268
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang W, 2010, IEEE T MULTIMEDIA, V12, P300, DOI 10.1109/TMM.2010.2047607
NR 40
TC 113
Z9 120
U1 2
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1275
EP 1289
DI 10.1109/TMM.2012.2190385
PN 2
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400013
DA 2024-07-18
ER

PT J
AU Sabirin, H
   Kim, M
AF Sabirin, Houari
   Kim, Munchurl
TI Moving Object Detection and Tracking Using a Spatio-Temporal Graph in
   H.264/AVC Bitstreams for Video Surveillance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graph-based method; H.264/AVC; object tracking; spatio-temporal graph;
   surveillance video
AB This paper presents a spatio-temporal graph-based method of detecting and tracking moving objects by treating the encoded blocks with non-zero motion vectors and/or non-zero residues as potential parts of objects in H. 264/AVC bitstreams. A spatio-temporal graph is constructed by first clustering the encoded blocks of potential object parts into block groups, each of which is defined as an attributed subgraphwhere the attributes of the vertices represent the positions, motion vectors and residues of the blocks. In order to remove false-positive blocks and to track the real objects, temporal connections between subgraphs in two consecutive frames are constructed and the similarities between subgraphs are computed, which constitutes a spatio-temporal graph. We show the experimental results that the proposed spatio-temporal graph-based representation of potential object blocks enables effective detection for the small-sized objects and the objects with small motion vectors and residues, and allows for reliable tracking of the detected objects even under occlusion. The identification of the detected moving objects is determined as rectangular regions of interest (ROIs) for which the ROI sizes and positions are adaptively adjusted to give the best approximation of the real shapes and positions of the objects.
C1 [Sabirin, Houari; Kim, Munchurl] Korea Adv Inst Sci & Technol, Dept Informat & Commun Engn, Taejon 305701, South Korea.
   [Kim, Munchurl] Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Korea Advanced
   Institute of Science & Technology (KAIST)
RP Sabirin, H (corresponding author), Korea Adv Inst Sci & Technol, Dept Informat & Commun Engn, Taejon 305701, South Korea.
EM houari@kaist.ac.kr; mkim@ee.kaist.ac.kr
RI Kim, Munchurl/AAQ-9591-2020; Kim, Munchurl/C-1759-2011
FU MKE/KEIT [10039199]
FX This work was supported by the IT R&D program of MKE/KEIT (10039199, A
   Study on Core Technologies of Perceptual Quality based Scalable 3D Video
   Codecs). The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Weisi Lin.
CR Andrews F. M., 2007, Proceedings of the 29th Bain Fallon Memorial Lectures: Behaviour, Lameness and Medicine, Fremantle, Western Australia, Australia, 1-7 July 2007, P1
   Arvanitidou M. G., 2011, P IEEE INT C MULT EX
   Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28
   Cesar RM, 2005, PATTERN RECOGN, V38, P2099, DOI 10.1016/j.patcog.2005.05.007
   Chen YM, 2011, IEEE T MULTIMEDIA, V13, P421, DOI 10.1109/TMM.2011.2127464
   De Bruyne S, 2009, IEEE INT CON MULTI, P330, DOI 10.1109/ICME.2009.5202501
   Dolby Laboratories Inc.  Fraunhofer-Institute HHI and Microsoft Corporation, H 264 14496 10 AVC R
   Gomila C, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P41
   Käs C, 2009, LECT NOTES COMPUT SC, V5414, P318, DOI 10.1007/978-3-540-92957-4_28
   Kapotas S. K., 2010, 2010 IEEE INT C IMAG, P325
   Kas C., 2009, P 3 ACM IEEE INT C D, P1
   Kim DH, 2010, PATTERN RECOGN, V43, P914, DOI 10.1016/j.patcog.2009.09.012
   Moura Ronaldo C., 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P427, DOI 10.1109/AVSS.2010.82
   Pallavi V, 2008, IEEE T MULTIMEDIA, V10, P794, DOI 10.1109/TMM.2008.922869
   Poppe C, 2009, J VIS COMMUN IMAGE R, V20, P428, DOI 10.1016/j.jvcir.2009.05.001
   Qu W, 2007, IEEE T IMAGE PROCESS, V16, P2129, DOI 10.1109/TIP.2007.899619
   Sabirin H., 2011, P IEEE INT IN PRESS
   Schneiderman R, 2010, IEEE SIGNAL PROC MAG, V27, P6, DOI 10.1109/MSP.2010.938113
   Szczerba K, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P478, DOI 10.1109/AVSS.2009.78
   You W., 2007, MULTIMEDIA CONTENT A, P483
   You W., 2009, REAL TIME IMAGE VIDE, V7244
   Zhou GL, 2009, 2009 ISECS INTERNATIONAL COLLOQUIUM ON COMPUTING, COMMUNICATION, CONTROL, AND MANAGEMENT, VOL I, P99, DOI 10.1109/CCCM.2009.5268142
NR 22
TC 40
Z9 42
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 657
EP 668
DI 10.1109/TMM.2012.2187777
PN 2
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700001
DA 2024-07-18
ER

PT J
AU Shao, F
   Jiang, GY
   Yu, M
   Chen, K
   Ho, YS
AF Shao, Feng
   Jiang, Gangyi
   Yu, Mei
   Chen, Ken
   Ho, Yo-Sung
TI Asymmetric Coding of Multi-View Video Plus Depth Based 3-D Video for
   View Rendering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D video; asymmetric coding; bit allocation; chrominance
   reconstruction; view rendering
AB The recent years have witnessed three-dimensional (3-D) video technology to become increasingly popular, as it can provide high-quality and immersive experience to end users, where view rendering with depth-image-based rendering (DIBR) technique is employed to generate the virtual views. Distortions in depth map may induce geometry changes in the virtual views, and distortions in texture video may be propagated to the virtual views. Thus, effective compression of both texture videos and depth maps is important for 3-D video system. From the perspective of bit allocation, asymmetric coding of the texture videos and depth maps is an effective way to get the optimal solution of 3-D video compression and view rendering problems. In this paper, a novel asymmetric coding method of multi-view video plus depth (MVD) based 3-D video is proposed on purpose of providing high-quality view rendering. In the proposed method, two models are proposed to characterize view rendering distortion and binocular suppression in 3-D video. Then, an asymmetric coding method of MVD-based 3-D video is proposed by combining two models in encoding framework. Finally, a chrominance reconstruction algorithm is presented to achieve accurate reconstruction. Experimental results show that compared with other methods, the proposed method can obtain higher performance of view rendering under the total bitrate constraint. Moreover, the perceptual visual quality of 3-D video is almost unaffected with the proposed method.
C1 [Shao, Feng; Jiang, Gangyi; Yu, Mei; Chen, Ken] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol GIST, Sch Informat & Commun, Kwangju 500712, South Korea.
C3 Ningbo University; Gwangju Institute of Science & Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
EM shaofeng@nbu.edu.cn; jianggangyi@nbu.edu.cn; yumei@nbu.edu.cn;
   chenken@nbu.edu.cn; hoyo@gist.ac.kr
RI jiang, gang/KII-8233-2024
FU Natural Science Foundation of China [60902096, 60832003, 60872094,
   61071120]; Specialized Research Fund for the Doctoral Program of Higher
   Education of China [20093305120002]; Ningbo University
FX This work was supported in part by the Natural Science Foundation of
   China (grants 60902096, 60832003, 60872094, and 61071120), the
   Specialized Research Fund for the Doctoral Program of Higher Education
   of China (20093305120002). It was also sponsored by K.C.Wong Magna Fund
   in Ningbo University. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Ming-Ting
   Sun.
CR Aflaki P, 2010, IEEE IMAGE PROC, P4021, DOI 10.1109/ICIP.2010.5650661
   [Anonymous], JVTAC207 ISOIEC JTC1
   [Anonymous], EURASIP SP IC
   [Anonymous], JVTAC207 ISOIEC MPEG
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P PICT COD S LISB PO
   [Anonymous], M16390 ISOIEC JTC1S
   [Anonymous], M171880 ISOIEC JTC1S
   [Anonymous], SG16Q6 ITUT
   [Anonymous], JVTZ207 ISOIEC JTC1S
   [Anonymous], P SPIE
   [Anonymous], JVTG012 ISOIEC JTC1S
   [Anonymous], M18356 ISOIEC JTC1SC
   [Anonymous], N11274 ISOIEC JTC1SC
   [Anonymous], P IEEE ICIP
   [Anonymous], P SPIE VISUAL COMMUN
   Chen Y, 2008, IEEE IMAGE PROC, P1944, DOI 10.1109/ICIP.2008.4712162
   Daribo I, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/258920
   Fehn C, 2006, P IEEE, V94, P524, DOI 10.1109/JPROC.2006.870688
   Fehn Christoph, 2007, 3DTV Conference, 2007, P1
   Kim WS, 2009, IEEE IMAGE PROC, P721, DOI 10.1109/ICIP.2009.5414304
   Lee HJ, 2000, IEEE T CIRC SYST VID, V10, P878, DOI 10.1109/76.867926
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Liu YW, 2009, SIGNAL PROCESS-IMAGE, V24, P666, DOI 10.1016/j.image.2009.06.002
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   Maitre M, 2010, J VIS COMMUN IMAGE R, V21, P513, DOI 10.1016/j.jvcir.2010.03.005
   Merkle P, 2009, SIGNAL PROCESS-IMAGE, V24, P73, DOI 10.1016/j.image.2008.10.010
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   Seuntiens P., 2006, ACM T APPL PERCEPT, V3, P95
   Shao F, 2010, IEEE T CONSUM ELECTR, V56, P2460, DOI 10.1109/TCE.2010.5681128
   Shao F, 2010, J VIS COMMUN IMAGE R, V21, P392, DOI 10.1016/j.jvcir.2010.03.001
   Smolic A, 2007, IEEE T CIRC SYST VID, V17, P1606, DOI 10.1109/TCSVT.2007.909972
   Smolic A, 2011, P IEEE, V99, P607, DOI 10.1109/JPROC.2010.2098350
   Tanimoto M, 2006, SIGNAL PROCESS-IMAGE, V21, P454, DOI 10.1016/j.image.2006.03.009
   Termin Y, 2007, OPT ENG, V46, DOI 10.1117/1.2772235
   Torralba A, 2003, PROC CVPR IEEE, P383
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 37
TC 73
Z9 91
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 157
EP 167
DI 10.1109/TMM.2011.2169045
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100015
DA 2024-07-18
ER

PT J
AU Chan, KHK
   Chan, SHG
   Begen, AC
AF Chan, K. -H. Kelvin
   Chan, S. -H. Gary
   Begen, Ali C.
TI SPANC: Optimizing Scheduling Delay for Peer-to-Peer Live Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Network coding; optimization; peer-to-peer (P2P) streaming; scheduling;
   substream pushing
ID BANDWIDTH
AB In peer-to-peer (P2P) live streaming using unstructured mesh, packet scheduling is an important factor in overall playback delay. In this paper, we propose a scheduling algorithm to minimize scheduling delay. To achieve low delay, our scheduling is predominantly push in nature, and the schedule needs to be changed only upon significant change in network states (due to, for examples, bandwidth change or parent churns). Our scheme, termed SPANC (Substream Pushing and Network Coding), pushes video packets in substreams and recovers packet loss using network coding. Given heterogeneous contents, delays, and bandwidths of parents of a peer, we formulate the substream assignment (SA) problem to assign substreams to parents with minimum delay. The SA problem can be optimally solved in polynomial time by transforming it to a max-weighted bipartite matching problem. We then formulate the fast recovery with network coding (FRNC) problem, which is to assign network coded packets to each parent to achieve minimum recovery delay. The FRNC problem can also be solved exactly in polynomial time with dynamic programming. Simulation results show that SPANC achieves substantially lower delay with little cost in bandwidth, as compared with recent approaches based on pull, network coding and hybrid pull-push.
C1 [Chan, K. -H. Kelvin; Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
   [Begen, Ali C.] Cisco Syst Inc, Video & Content Platforms, Res & Adv Dev, San Jose, CA 95134 USA.
C3 Hong Kong University of Science & Technology; Cisco Systems Inc
RP Chan, KHK (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
EM chankh@cse.ust.hk; gchan@cse.ust.hk; abegen@cisco.com
RI Begen, Ali C./R-5897-2016
OI Begen, Ali C./0000-0002-0835-3017; Chan, Gary Shueng
   Han/0000-0003-4207-764X
FU Research Grant Council of the Hong Kong Special Administrative Region,
   China [611209]; Cisco University; Silicon Valley Community Foundation
   [SVCF08/09.EG01, GMGS08/09.EG05]
FX Manuscript received August 28, 2009; revised December 24, 2009 and May
   16, 2010; accepted June 07, 2010. Date of publication June 21, 2010;
   date of current version October 15, 2010. This work was supported in
   part by the General Research Fund from the Research Grant Council of the
   Hong Kong Special Administrative Region, China (611209) and by the Cisco
   University Research Program Fund, a corporate advised fund of the
   Silicon Valley Community Foundation under Grant
   VCF08/09.EG01&GMGS08/09.EG05. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Thinh Nguyen.
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   [Anonymous], 1982, COMBINATORIAL OPTIMI
   [Anonymous], P ACM MULT VANC BC O
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   CHAN KHK, 2010, P IEEE CONS COMM NET
   CHOU YWP, 2003, P ALL C COMM CONTR C
   Chu YH, 2002, IEEE J SEL AREA COMM, V20, P1456, DOI 10.1109/JSAC.2002.803066
   Gkantsidis C, 2005, IEEE INFOCOM SER, P2235
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Jin X, 2007, IEEE T MULTIMEDIA, V9, P1580, DOI 10.1109/TMM.2007.907459
   Kwong KW, 2008, IEEE ACM T NETWORK, V16, P281, DOI 10.1109/TNET.2007.899026
   Li B, 2007, IEEE J SEL AREA COMM, V25, P1627, DOI 10.1109/JSAC.2007.071203
   LI Z, 2009, P NETWORKING, P728
   Liao XF, 2006, IEEE INFOCOM SER, P2411
   LIU Y, 2007, P ACM MULT, P127, DOI DOI 10.1145/1291233.1291259
   Liu ZY, 2008, I C NETWORK PROTOCOL, P94, DOI 10.1109/ICNP.2008.4697028
   Magharei N, 2007, IEEE INFOCOM SER, P1415
   Magharei N, 2007, IEEE INFOCOM SER, P1424
   MEDINA A, 2001, P 9 INT S MOD AN SIM, P346, DOI DOI 10.1109/MASC0T.2001.948886
   Meo M, 2005, IEEE INFOCOM SER, P2798
   Padmanabhan V.N., 2002, P 12 INT WORKSHOP NE, P177
   Padmanabhan VN, 2003, 11TH IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P16, DOI 10.1109/ICNP.2003.1249753
   Pai V, 2005, LECT NOTES COMPUT SC, V3640, P127
   Ren D, 2008, IEEE INFOCOM SER, P1732
   Small T, 2007, IEEE J SEL AREA COMM, V25, P35, DOI 10.1109/JSAC.2007.070105
   SOMMERS J, 2005, P ACM SIGCOMM, P157
   Veloso E, 2006, IEEE ACM T NETWORK, V14, P133, DOI 10.1109/TNET.2005.863709
   Wang M, 2007, IEEE J SEL AREA COMM, V25, P1655, DOI 10.1109/JSAC.2007.071205
   Wang M, 2007, IEEE T MULTIMEDIA, V9, P1554, DOI 10.1109/TMM.2007.907460
   Zhang M, 2007, IEEE J SEL AREA COMM, V25, P1678, DOI 10.1109/JSAC.2007.071207
   Zhang M, 2006, GLOB TELECOMM CONF
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
   Zheng GF, 2009, IEEE INT CON MULTI, P1158, DOI 10.1109/ICME.2009.5202705
NR 33
TC 27
Z9 29
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2010
VL 12
IS 7
BP 743
EP 753
DI 10.1109/TMM.2010.2053524
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 670SV
UT WOS:000283448500011
DA 2024-07-18
ER

PT J
AU Lee, J
   Marsella, SC
AF Lee, Jina
   Marsella, Stacy C.
TI Predicting Speaker Head Nods and the Effects of Affective Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Embodied conversational agents; emotion; head nods; machine learning;
   nonverbal behaviors; virtual agents
ID HIDDEN MARKOV-MODELS; MOVEMENT; CORPUS; MOTION
AB During face-to-face conversation, our body is continually in motion, displaying various head, gesture, and posture movements. Based on findings describing the communicative functions served by these nonverbal behaviors, many virtual agent systems have modeled them to make the virtual agent look more effective and believable. One channel of nonverbal behaviors that has received less attention is head movements, despite the important functions served by them. The goal for this work is to build a domain-independent model of speaker's head movements that could be used to generate head movements for virtual agents. In this paper, we present a machine learning approach for learning models of head movements by focusing on when speaker head nods should occur, and conduct evaluation studies that compare the nods generated by this work to our previous approach of using handcrafted rules [1]. To learn patterns of speaker head nods, we use a gesture corpus and rely on the linguistic and affective features of the utterance. We describe the feature selection process and training process for learning hidden Markov models and compare the results of the learned models under varying conditions. The results show that we can predict speaker head nods with high precision (.84) and recall (.89) rates, even without a deep representation of the surface text and that using affective information can help improve the prediction of the head nods (precision: .89, recall: .90). The evaluation study shows that the nods generated by the machine learning approach are perceived to be more natural in terms of nod timing than the nods generated by the rule-based approach.
C1 [Lee, Jina; Marsella, Stacy C.] Univ So Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Lee, J (corresponding author), Univ So Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
EM jlee@ict.usc.edu; marsella@ict.usc.edu
FU NSF EAPSI; Japan Society of for the Promotion of Science (JSPS); U.S.
   Army Research, Development, and Engineering Command (RDECOM)
FX Manuscript received December 01, 2009; revised March 26, 2010; accepted
   May 10, 2010. Date of current version September 15, 2010. This work was
   supported in part by the NSF EAPSI fellowship, in part by the Japan
   Society of for the Promotion of Science (JSPS) fellowship, and in part
   by the U.S. Army Research, Development, and Engineering Command
   (RDECOM). The content does not necessarily reflect the position or the
   policy of the Government, and no official endorsement should be
   inferred. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Nadia
   Magnenat-Thalmann.
CR [Anonymous], 1997, Nonverbal communication in human interaction
   [Anonymous], 1993, Statistical Language Learning
   BENTE G, EMOTION INT IN PRESS
   Breitfuss W, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P319
   Burgoon J.K., 1994, HDB INTERPERSONAL CO, V2nd, P229
   Busso C, 2007, IEEE T AUDIO SPEECH, V15, P1075, DOI 10.1109/TASL.2006.885910
   Carletta J, 2007, LANG RESOUR EVAL, V41, P181, DOI 10.1007/s10579-007-9040-x
   Cassell J, 2000, COMMUN ACM, V43, P70, DOI 10.1145/332051.332075
   Cassell J., 2001, Proceedings of SIGGRAPH 2001, P477
   Cathcart N, 2003, EACL 2003: 10TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P51
   Charniak E, 2000, 6TH APPLIED NATURAL LANGUAGE PROCESSING CONFERENCE/1ST MEETING OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE AND PROCEEDINGS OF THE ANLP-NAACL 2000 STUDENT RESEARCH WORKSHOP, pA132
   Foster ME, 2007, LANG RESOUR EVAL, V41, P305, DOI 10.1007/s10579-007-9055-3
   Graf HP, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P396, DOI 10.1109/AFGR.2002.1004186
   HEYLEN D, 2005, P SOC PRES CUES S AI
   Hill R. W., 2006, P 25 ARM SCI C ASC 2
   HWANG BH, 1991, TECHNOMETRICS, V33, P251
   Kendon A., 2002, Gesture, V2, P147, DOI 10.1075/gest.2.2.03ken
   Kenny P, 2007, LECT NOTES ARTIF INT, V4722, P197
   Kipp M, 2007, LECT NOTES ARTIF INT, V4722, P15
   KNOPPEL FLA, 2008, P 7 INT JOINT C AUT, P112
   Lance B, 2010, AUTON AGENT MULTI-AG, V20, P50, DOI 10.1007/s10458-009-9097-6
   LEE J, 2009, P 3 INT C AFF COMP I
   Lee J, 2006, LECT NOTES ARTIF INT, V4133, P243
   Lee JW, 2009, ASIAN ECON PAP, V8, P9, DOI 10.1162/asep.2009.8.1.9
   Maatman RM, 2005, LECT NOTES ARTIF INT, V3661, P25
   MATSUMOTO D, 2002, J CROSS CULTURAL PSY, V23, P72
   MAYNARD SK, 1987, J PRAGMATICS, V11, P589, DOI 10.1016/0378-2166(87)90181-0
   McClave EZ, 2000, J PRAGMATICS, V32, P855, DOI 10.1016/S0378-2166(99)00079-X
   Mignault A, 2003, J NONVERBAL BEHAV, V27, P111, DOI 10.1023/A:1023914509763
   Morency LP, 2008, LECT NOTES COMPUT SC, V5208, P176
   Munhall KG, 2004, PSYCHOL SCI, V15, P133, DOI 10.1111/j.0963-7214.2004.01502010.x
   Neviarouskaya A, 2007, LECT NOTES COMPUT SC, V4738, P218
   NISHIMURA R, TSD 2007, P599
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   STRAUSS M, 2008, P 7 INT JOINT C AUT, P97
   Swartout W, 2006, AI MAG, V27, P96
   Tickle-Degnen Linda, 1990, Psychological Inquiry, V1, P285, DOI DOI 10.1207/S15327965PLI01041
   TRAUM D, 2007, P 8 SIGDIAL WORKSH D, P71
   WARD T, 2004, PRAGMATICS, V23, P1177
   Wilson AD, 1999, IEEE T PATTERN ANAL, V21, P884, DOI 10.1109/34.790429
NR 40
TC 22
Z9 25
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2010
VL 12
IS 6
BP 552
EP 562
DI 10.1109/TMM.2010.2051874
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 668TB
UT WOS:000283291900009
DA 2024-07-18
ER

PT J
AU Qi, GJ
   Hua, XS
   Rui, Y
   Tang, JH
   Zhang, HJ
AF Qi, Guo-Jun
   Hua, Xian-Sheng
   Rui, Yong
   Tang, Jinhui
   Zhang, Hong-Jiang
TI Image Classification With Kernelized Spatial-Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 2-D hidden Markov model; image classification; kernel method; spatial
   context
ID SCALE
AB The goal of image classification is to classify a collection of unlabeled images into a set of semantic classes. Many methods have been proposed to approach this goal by leveraging visual appearances of local patches in images. However, the spatial context between these local patches also provides significant information to improve the classification accuracy. Traditional spatial contextual models, such as two-dimensional hidden Markov model, attempt to construct one common model for each image category to depict the spatial structures of the images in this class. However due to large intra-class variances in an image category, one single model has difficulties in representing various spatial contexts in different images. In contrast, we propose to construct a prototype set of spatial contextual models by leveraging the kernel methods rather than only one model. Such an algorithm combines the advantages of rich representation ability of spatial contextual models as well as the powerful classification ability of kernel method. In particular, we propose a new distance measure between different spatial contextual models by integrating joint appearance-spatial image features. Such a distance measure can be efficiently computed in a recursive formulation that scales well to image size. Extensive experiments demonstrate that the proposed approach significantly outperforms the state-of-the-art approaches.
C1 [Qi, Guo-Jun] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
   [Hua, Xian-Sheng] Microsoft Res Asia, Internet Media Grp, Beijing 100190, Peoples R China.
   [Rui, Yong; Zhang, Hong-Jiang] Microsoft Adv Technol Ctr, Beijing 100190, Peoples R China.
   [Tang, Jinhui] Natl Univ Singapore, Sch Comp, Singapore 119076, Singapore.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   Microsoft; Microsoft Research Asia; Microsoft; National University of
   Singapore
RP Qi, GJ (corresponding author), Univ Illinois, Dept Elect & Comp Engn, 1406 W Green St, Urbana, IL 61801 USA.
EM qi4@illinois.edu; xshua@microsoft.com; yongrui@microsoft.com;
   tangjh@comp.nus.edu.sg; hjzhang@microsoft.com
RI Qi, Guo-Jun/AAH-8294-2019; Tang, Jinhui/KBR-0891-2024
OI Qi, Guo-Jun/0000-0003-3508-1851
CR [Anonymous], 2007, P IEEE CVPR
   [Anonymous], P IEEE CVPR
   [Anonymous], 2008, P IEEE C COMP VIS PA
   Chang C.C., LIBSVM: a library for support vector machines
   Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248
   Cover T. M, 2006, Elements of Information Theory, V2nd
   DESANDE KV, 2008, P IEEE CVPR
   Do MN, 2002, IEEE T MULTIMEDIA, V4, P517, DOI 10.1109/TMM.2002.802019
   FEIFEI L, 2005, P IEEE CVPR
   Fergus R, 2003, PROC CVPR IEEE, P264
   Gauvain JL, 1994, IEEE T SPEECH AUDI P, V2, P291, DOI 10.1109/89.279278
   He J., 2008, P IEEE CVPR, P1
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Li J, 2000, IEEE T SIGNAL PROCES, V48, P517, DOI 10.1109/78.823977
   LIN YY, 2007, P IEEE CVPR
   LIU P, 2007, P IEEE ICASSP
   MERIALDO B, 2006, P 4 INT WORKSH AD MU
   Quattoni A., 2004, P ADV NEUR INF PROC
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE
   SINGER Y, 1998, P NIPS
   Tang JY, 2007, IEEE T CIRC SYST VID, V17, P384, DOI 10.1109/TCSVT.2006.888941
   YU F, 2006, P IEEE ICME
   Yu J, 2008, IEEE T PATTERN ANAL, V30, P451, DOI 10.1109/TPAMI.2007.70714
   Zhang H., 2006, P IEEE CVPR
NR 25
TC 24
Z9 26
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2010
VL 12
IS 4
BP 278
EP 287
DI 10.1109/TMM.2010.2046270
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 596ER
UT WOS:000277668100005
DA 2024-07-18
ER

PT J
AU Wang, ZS
   Li, BX
AF Wang, Zheshen
   Li, Baoxin
TI A Bayesian Approach to Automated Creation of Tactile Facial Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image matching; image shape analysis; pattern recognition; tactile
   graphics
AB Portrait photos (facial images) play important social and emotional roles in our life. This type of visual media is unfortunately inaccessible by users with visual impairment. This paper proposes a systematic approach for automatically converting human facial images into a tactile form that can be printed on a tactile printer and explored by a user who is blind. We propose a deformable Bayesian Active Shape Model (BASM), which integrates anthropometric priors with shape and appearance information learnt from a face dataset. We design an inference algorithm under this model for processing new face images to create an input-adaptive face sketch. Further, the model is enhanced by input-specific details through semantic-aware processing. We report experiments on evaluating the accuracy of face alignment using the proposed method, with comparison with other state-of-the-art results. Furthermore, subjective evaluations of the produced tactile face images were performed by 17 persons including six visually-impaired users, confirming the effectiveness of the proposed approach in conveying via haptics vital visual information in a face image.
C1 [Wang, Zheshen; Li, Baoxin] Arizona State Univ, Dept Comp Sci & Engn, Tempe, AZ 85287 USA.
C3 Arizona State University; Arizona State University-Tempe
RP Wang, ZS (corresponding author), Arizona State Univ, Dept Comp Sci & Engn, Tempe, AZ 85287 USA.
EM zheshen.wang@asu.edu; baoxin.li@asu.edu
RI Zhang, Can/JUU-9511-2023
FU NSF [0845469]; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [0845469] Funding Source: National
   Science Foundation
FX This work was supported in part by an NSF grant (Award # 0845469). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Nicu Sebe.
CR [Anonymous], IEEE COMP SOC C COMP
   Benavente R, 1998, 24 COMP VIS CTR
   Bhattacharyya Anil, 1943, B CALCUTTA MATH SOC, V35, P99
   Blanz Volker, 1999, ACM SIGGRAPH, V2, P4
   COLBRY D, 2007, IEEE COMP SOC C COMP
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   COUGHLAN J, 2002, EUR C COMP VIS ECCV
   Edman Polly K., 1992, Tactile graphics
   Farkas LG, 1994, Anthropometry of Head and Face, Vsecond
   GU L, 2008, EUR C COMP VIS ECCV
   HUANG Y, 2007, IEEE 11 INT C COMP V
   INA S, 1996, SIGCAPH COMP PHYS HA
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   LADNER RE, 2005, ACM SIGACCESS C ASS
   LIANG L, 2006, IEEE COMP SOC C COMP
   Liang L., 2008, EUR C COMP VIS ECCV
   Liu X., 2007, IEEE C COMP VIS PATT
   Milborrow S., 2008, EUR C COMP VIS ECCV
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   ROMDHANI S, 2007, IEEE COMP SOC C COMP
   Stegmann MB, 2003, IEEE T MED IMAGING, V22, P1319, DOI 10.1109/TMI.2003.817780
   TORRE FDL, 2008, IEEE COMP SOC C COMP
   TU J, 2004, IEEE COMP SOC C COMP
   TWINING C, 2001, BRIT MACH VIS C BMVC
   Wang ZS, 2007, EURASIP J IMAGE VIDE, DOI 10.1155/2007/18019
   Way T P, 1997, IEEE Trans Rehabil Eng, V5, P95, DOI 10.1109/86.559354
   Way T P, 1997, IEEE Trans Rehabil Eng, V5, P81, DOI 10.1109/86.559353
   WU H, 2008, IEEE COMP SOC C COMP
   ZHOU Y, 2003, IEEE COMP SOC C COMP
NR 30
TC 3
Z9 4
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2010
VL 12
IS 4
BP 233
EP 246
DI 10.1109/TMM.2010.2046267
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 596ER
UT WOS:000277668100001
DA 2024-07-18
ER

PT J
AU Zhao, GY
   Barnard, M
   Pietikäinen, M
AF Zhao, Guoying
   Barnard, Mark
   Pietikainen, Matti
TI Lipreading With Local Spatiotemporal Descriptors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lipreading; local binary patterns; spatiotemporal descriptors; visual
   speech recognition
ID BINARY PATTERNS; RECOGNITION
AB Visual speech information plays an important role in lipreading under noisy conditions or for listeners with a hearing impairment. In this paper, we present local spatiotemporal descriptors to represent and recognize spoken isolated phrases based solely on visual input. Spatiotemporal local binary patterns extracted from mouth regions are used for describing isolated phrase sequences. In our experiments with 817 sequences from ten phrases and 20 speakers, promising accuracies of 62% and 70% were obtained in speaker-independent and speaker-dependent recognition, respectively. In comparison with other methods on AVLetters database, the accuracy, 62.8%, of our method clearly outperforms the others. Analysis of the confusion matrix for 26 English letters shows the good clustering characteristics of visemes for the proposed descriptors. The advantages of our approach include local processing and robustness to monotonic gray-scale changes. Moreover, no error prone segmentation of moving lips is needed.
C1 [Zhao, Guoying; Pietikainen, Matti] Univ Oulu, Machine Vis Grp, Infotech Oulu & Dept Elect & Informat Engn, FI-90014 Oulu, Finland.
   [Barnard, Mark] Univ Surrey, Fac Engn & Phys Sci, Guildford GU2 7XH, Surrey, England.
C3 University of Oulu; University of Surrey
RP Zhao, GY (corresponding author), Univ Oulu, Machine Vis Grp, Infotech Oulu & Dept Elect & Informat Engn, FI-90014 Oulu, Finland.
EM gyzhao@ee.oulu.fi; Mark.Barnard@surrey.ac.uk; mkp@ee.oulu.fi
RI Zhao, Guoying/G-5383-2012; Zhao, Guoying/ABE-7716-2020
OI Zhao, Guoying/0000-0003-3694-206X
FU Academy of Finland
FX Manuscript received February 03, 2009; revised April 21, 2009. First
   published August 18, 2009; current version published October 16, 2009.
   This work was supported by the Academy of Finland. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shrikanth Narayanan.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Aleksic PS, 2006, P IEEE, V94, P2025, DOI 10.1109/JPROC.2006.886017
   Aleksic PS, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P481
   Aleksic PS, 2002, EURASIP J APPL SIG P, V2002, P1213, DOI 10.1155/S1110865702206162
   ARSIC I, 2006, P 14 EUR SIGN PROC C
   BASU S, 1999, P IEEE 3 WORKSH MULT, P475
   Brooke NM, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P1656, DOI 10.1109/ICSLP.1996.607943
   Chen T, 1998, P IEEE, V86, P837, DOI 10.1109/5.664274
   Chiou GI, 1997, IEEE T IMAGE PROCESS, V6, P1192, DOI 10.1109/83.605417
   Cohen M.M., 1993, MODELS TECHNIQUES CO
   DUCHNOWSKI P, 1995, INT CONF ACOUST SPEE, P109, DOI 10.1109/ICASSP.1995.479285
   FOX NA, 2003, P ACM SIGMM 2003 MUL, P25
   Frischholz RW, 2000, COMPUTER, V33, P64, DOI 10.1109/2.820041
   Gowdy JN, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P993
   Gurban M., 2005, P 13 EUR SIGN PROC C
   Hadid A, 2007, LECT NOTES COMPUT SC, V4778, P1
   HAZEN T, 2005, P ICMI
   Lee B., 2004, AVICAR AUDIO VISUAL, P2489, DOI DOI 10.21437/INTERSPEECH.2004-424
   Lee S., 2002, PRICAI 02, P563
   Lucey P, 2004, P 10 AUSTR INT C SPE
   Luettin J, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P62, DOI 10.1109/ICSLP.1996.607030
   Mase K., 1991, Systems and Computers in Japan, V22, P67
   Matthews I, 2002, IEEE T PATTERN ANAL, V24, P198, DOI 10.1109/34.982900
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Messer K., 1999, P 2 INT C AUD VID BA
   Nefian AV, 2002, EURASIP J APPL SIG P, V2002, P1274, DOI 10.1155/S1110865702206083
   NETI C, 2000, CTR LANG SPEECH PROC
   Niu ZH, 2006, INT C PATT RECOG, P1216
   NIYOGI P, 1999, P AUD VIS SPEECH C
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   Potamianos G, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P173, DOI 10.1109/ICIP.1998.999008
   Potamianos G., 2004, ISSUES AUDIO VISUAL
   Saenko K, 2005, IEEE I CONF COMP VIS, P1424, DOI 10.1109/ICCV.2005.251
   SAENKO K, 2005, P ICASSP, P473
   SANDERSON C, 2002, IDIAP COMMUNICATION
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Zhang GC, 2004, LECT NOTES COMPUT SC, V3338, P179
   ZHAO G, 2009, PATTERN REC IN PRESS
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao Guoying., 2007, Proceedings of the International Workshop on Human-centered Multimedia, HCM '07, P57
NR 41
TC 194
Z9 210
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1254
EP 1265
DI 10.1109/TMM.2009.2030637
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300004
DA 2024-07-18
ER

PT J
AU Li, Y
   Li, Z
   Chiang, M
   Calderbank, AR
AF Li, Ying
   Li, Zhu
   Chiang, Mung
   Calderbank, A. Robert
TI Content-Aware Distortion-Fair Video Streaming in Congested Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE Global Telecommunication Conference (GLOBECOM 08)
CT IEEE Global Telecommunications Conference (GLOBECOM 08)
CY NOV, 2008
CY NOV 30-DEC 04, 2008
CL New Orleans, LA
CL New Orleans, LA
SP IEEE
DE Fairness; Internet; multimedia communication; optimization; rate
   distortion; resource allocation; video
ID FLOW-CONTROL; OPTIMIZATION
AB Internet is experiencing a substantial growth of video traffic. Given the limited network bandwidth resources, how to provide Internet users with good video playback quality-of-service (QoS) is a key problem. For video clips competing bandwidth, we propose an approach of Content-Aware distortion-Fair (CAF) video delivery scheme, which is aware of the characteristics of video frames and ensures max-min distortion-fair sharing among video flows. CAF leverages content-awareness to prioritize packet dropping during congestion. Different from bandwidth fair sharing, CAF targets end-to-end video playback quality fairness among users. The proposed CAF approach does not require rate-distortion modeling of the source, which is difficult to estimate. Instead, it exploits the temporal prediction structure of the video sequences along with a frame drop distortion metric to guide resource allocations and coordinations. Experimental results show that the proposed approach operates with limited overhead in computation and communication, and yields better QoS, especially when the network is congested.
C1 [Li, Ying; Chiang, Mung; Calderbank, A. Robert] Princeton Univ, Dept Elect Engn, Princeton, NJ 08540 USA.
   [Li, Zhu] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
C3 Princeton University; Hong Kong Polytechnic University
RP Li, Y (corresponding author), Princeton Univ, Dept Elect Engn, Princeton, NJ 08540 USA.
EM yingli@princeton.edu; zhu.li@ieee.org; chiangm@princeton.edu;
   calderbk@princeton.edu
RI Li, Zhu/E-8092-2010
CR [Anonymous], 2006, Elements of Information Theory
   [Anonymous], 144962 ISOIEC
   [Anonymous], 1998, J. Oper. Res. Soc.
   APOSTOLOPOULOS J, 2002, P IEEE INFOCOM JUN
   BACCICHET P, 2007, P IEEE ICME JUL, P2
   BAJAJ S, 1998, P ACM SIGCOMM OCT
   CAO Z, 1999, P IEEE INFOCOM MAR
   CHAKARESKI J, 2002, P WORKSH MULT SIGN P
   Chakareski J, 2008, IEEE T MULTIMEDIA, V10, P858, DOI 10.1109/TMM.2008.921846
   Chen JC, 2004, IEEE J SEL AREA COMM, V22, P1920, DOI 10.1109/JSAC.2004.836000
   CHIANG M, 2008, P C INF SCI SYST PRI
   Chiang M, 2007, P IEEE, V95, P255, DOI 10.1109/JPROC.2006.887322
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   GIROD B, 2002, IEEE WIREL COMMUN, V2, P549
   HASKELL BG, 1997, DIGITAL VIDEO OVERVI
   HORMIS R, 2007, P IEEE GLOB NOV
   Huang JW, 2008, IEEE T CIRC SYST VID, V18, P582, DOI 10.1109/TCSVT.2008.919109
   KALMAN M, 2003, P IEEE ICIP SEP
   Kanakia H, 1995, IEEE ACM T NETWORK, V3, P671, DOI 10.1109/90.477713
   LI Y, 2008, P IEEE GLOB NOV
   Li Y, 2008, IEEE T MULTIMEDIA, V10, P885, DOI 10.1109/TMM.2008.922860
   Li Z, 2005, IEEE T CIRC SYST VID, V15, P1245, DOI 10.1109/TCSVT.2005.854230
   Li Z, 2005, IEEE T IMAGE PROCESS, V14, P1550, DOI 10.1109/TIP.2005.854477
   Low SH, 2003, IEEE ACM T NETWORK, V11, P525, DOI 10.1109/TNET.2003.815297
   Low SH, 1999, IEEE ACM T NETWORK, V7, P861, DOI 10.1109/90.811451
   Mastronarde NH, 2007, IEEE T MULTIMEDIA, V9, P1493, DOI 10.1109/TMM.2007.906568
   Ohm JR, 2005, P IEEE, V93, P42, DOI 10.1109/JPROC.2004.839611
   QUAGLIA D, 2002, P IEEE ICME AUG
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   SEFEROGLU H, 2007, P PACK VID
   Shiang HP, 2007, IEEE J SEL AREA COMM, V25, P770, DOI 10.1109/JSAC.2007.070513
   Sullivan GJ, 2005, P IEEE, V93, P18, DOI 10.1109/JPROC.2004.839617
   Wang WH, 2006, IEEE ACM T NETWORK, V14, P1282, DOI 10.1109/TNET.2006.886318
   Wu DL, 2007, IEEE J SEL AREA COMM, V25, P841, DOI 10.1109/JSAC.2007.070519
   Wu DP, 2000, P IEEE, V88, P1855, DOI 10.1109/5.899055
   Wu F, 2001, IEEE T CIRC SYST VID, V11, P332, DOI 10.1109/76.911159
   Xin J, 2005, P IEEE, V93, P84, DOI 10.1109/JPROC.2004.839620
   ZHAI F, 2003, P IEEE ICME JUL
   Zhang Q, 2008, P IEEE, V96, P64, DOI 10.1109/JPROC.2007.909930
   Zhang Y, 2007, ACS CHEM BIOL, V2, P320, DOI 10.1021/cb7000044
NR 40
TC 47
Z9 51
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1182
EP 1193
DI 10.1109/TMM.2009.2026102
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, CM
   Lin, CW
   Chuang, CY
AF Huang, Chung-Ming
   Lin, Chung-Wei
   Chuang, Cheng-Yen
TI A Multilayered Audiovisual Streaming System Using the Network Bandwidth
   Adaptation and the Two-Phase Synchronization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit-sliced arithmetic coding (BSAC); conditional retransmission;
   de-jitter; fine-granular scalability (FGS); playout synchronization;
   universal audiovisual streaming service
ID VIDEO; PLAYOUT; DELAY
AB Synchronous audiovisual streaming and playout are two of the major issues in the multimedia communication network. However, the past corresponding researches of media synchronization mainly focused on the mono-quality and single-layer (nonscalable) audiovisual data. To overcome challenges of ubiquitous multimedia streaming, a scalable audiovisual coder that can provide flexible scalabilities and adaptive streaming control to adapt to complicated network situations are both required. This paper proposes a multilayered audiovisual streaming scheme to deliver layered audiovisual data synchronously, which is called ML-AVSS. Fine-granular scalability (FGS) and bit-sliced arithmetic coding (BSAC) techniques are used to segment video and audio data into one base-layer and multiple enhancement-layer bitstreams. With advantages of audiovisual layer coding, a de-jitter procedure, a conditional retransmission mechanism and a playout synchronization mechanism are designed to transmit hybrid multilayered audiovisual bitstreams in consideration of the result of a network bandwidth adaptation and the distinct decoding time-complexity. Experimental results show that the proposed ML-AVSS is a feasible streaming scheme to overcome challenges of ubiquitous multimedia streaming, e. g., constrained channel bandwidth, quality degradation, unsmooth playout, etc.
C1 [Huang, Chung-Ming; Lin, Chung-Wei; Chuang, Cheng-Yen] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Lab Multimedia Mobile Network, Tainan 70101, Taiwan.
C3 National Cheng Kung University
RP Huang, CM (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Lab Multimedia Mobile Network, Tainan 70101, Taiwan.
EM huangcm@locust.csie.ncku.edu.tw; cwlin@locust.csie.ncku.edu.tw;
   chuangcy@locust.csie.ncku.edu.tw
FU National Science Council of the Republic of China, Taiwan [NSC
   96-2219-E-006-008]; Information Communication Laboratory (ICL),
   Industrial Technology Research Institute (ITRI), Taiwan, Republic of
   China; Intel Microelectronics Asia Ltd., Taiwan Branch
FX This work was supported in part by the National Science Council of the
   Republic of China, Taiwan, under Contract NSC 96-2219-E-006-008,
   Information Communication Laboratory (ICL), Industrial Technology
   Research Institute (ITRI), Taiwan, Republic of China, and Intel
   Microelectronics Asia Ltd., Taiwan Branch.
CR Aramvith S, 2002, IEEE T CIRC SYST VID, V12, P558, DOI 10.1109/TCSVT.2002.800326
   Feiten B, 2005, IEEE T MULTIMEDIA, V7, P446, DOI 10.1109/TMM.2005.846793
   Huang CM, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P923
   Huang CM, 2009, COMPUT J, V52, P171, DOI 10.1093/comjnl/bxn011
   Ishibashi Y, 2000, C LOCAL COMPUT NETW, P337, DOI 10.1109/LCN.2000.891066
   *ISO IEC, 1999, 144962JTC1SC29WG11N2
   *ISO IEC, 1999, 144963JTC1SC29WG11N2
   JO J, 2003, P IEEE INT C COMM MA, V1, P542
   Kim HS, 2004, CONTROL ENG PRACT, V12, P527, DOI 10.1016/S0967-0661(03)00132-1
   Laoutaris N, 2002, IEEE NETWORK, V16, P30, DOI 10.1109/MNET.2002.1002997
   LEE S, 2006, P IEEE INT C CONS EL, P257
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   LIU H, 2005, P IEEE INT C MULT EX
   Liu HH, 2006, IEEE INT SYMP CIRC S, P5027
   Liu HN, 2006, WIREL NETW, V12, P511, DOI 10.1007/s11276-006-6549-7
   Park S, 2008, J VIS COMMUN IMAGE R, V19, P106, DOI 10.1016/j.jvcir.2007.09.002
   Perkins C, 1998, IEEE NETWORK, V12, P40, DOI 10.1109/65.730750
   Rejaie R, 2000, IEEE J SEL AREA COMM, V18, P2530, DOI 10.1109/49.898735
   Ries M, 2005, 2ND INTERNATIONAL SYMPOSIUM ON WIRELESS COMMUNICATIONS SYSTEMS 2005 (ISWCS 2005), P173, DOI 10.1109/ISWCS.2005.1547680
   Sandler M, 2006, IEE P-VIS IMAGE SIGN, V153, P331, DOI 10.1049/ip-vis:20050054
   Seo KD, 2007, LECT NOTES COMPUT SC, V4490, P621
   Tasaka S, 2000, IEEE ICC, P1535, DOI 10.1109/ICC.2000.853753
   Tung YS, 2002, IEEE T CIRC SYST VID, V12, P730, DOI 10.1109/TCSVT.2002.800855
   Vetro A, 2004, IEEE MULTIMEDIA, V11, P84, DOI 10.1109/MMUL.2004.1261111
   WEI J, 2005, P IEEE C FUZZ INF PR, P700
   Yu R, 2006, IEEE T AUDIO SPEECH, V14, P1352, DOI 10.1109/TSA.2005.860841
   Zhang Q, 2004, IEEE T MULTIMEDIA, V6, P897, DOI 10.1109/TMM.2004.837249
   ZHU H, 2003, P IEEE INT C VEH TEC, V5, P3390
NR 28
TC 6
Z9 6
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 797
EP 809
DI 10.1109/TMM.2009.2021719
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300001
DA 2024-07-18
ER

PT J
AU Cao, LL
   Luo, JB
   Kautz, H
   Huang, TS
AF Cao, Liangliang
   Luo, Jiebo
   Kautz, Henry
   Huang, Thomas S.
TI Image Annotation Within the Context of Personal Photo Collections Using
   Hierarchical Event and Scene Models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Consumer photo collections; CRF; GPS; scene and event annotation
ID PICTURES
AB Most image annotation systems consider a single photo at a time and label photos individually. In this work, we focus on collections of personal photos and exploit the contextual information naturally implied by the associated GPS and time metadata. First, we employ a constrained clustering method to partition a photo collection into event-based subcollections, considering that the GPS records may be partly missing (a practical issue). We then use conditional random field (CRF) models to exploit the correlation between photos based on 1) time-location constraints and 2) the relationship between collection-level annotation (i.e., events) and image-level annotation (i.e., scenes). With the introduction of such a multilevel annotation hierarchy, our system addresses the problem of annotating consumer photo collections that requires a more hierarchical description of the customers' activities than do the simpler image annotation tasks. The efficacy of the proposed system is validated by extensive evaluation using a sizable geotagged personal photo collection database, which consists of over 100 photo collections and is manually labeled for 12 events and 12 scenes to create ground truth.
C1 [Cao, Liangliang; Huang, Thomas S.] Univ Illinois, Beckman Inst, Urbana, IL 61801 USA.
   [Cao, Liangliang; Huang, Thomas S.] Univ Illinois, Dept Elect & Comp Engn, Coordinated Sci Lab, Urbana, IL 61801 USA.
   [Luo, Jiebo; Kautz, Henry] Eastman Kodak Co, Kodak Res Labs Eastman, Rochester, NY 14615 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   University of Illinois System; University of Illinois Urbana-Champaign;
   Eastman Kodak
RP Cao, LL (corresponding author), Univ Illinois, Beckman Inst, Urbana, IL 61801 USA.
EM cao4@ifp.uiuc.edu; jiebo.luo@kodak.com; henry.kautz@kodak.com;
   huang@ifp.uiuc.edu
RI yan, shuicheng/HCH-9860-2022; Kautz, Henry/AAF-5190-2020; yan,
   shuicheng/A-8531-2014; Luo, Jiebo/AAI-7549-2020
OI yan, shuicheng/0000-0003-4527-1018; yan, shuicheng/0000-0001-8906-3777;
   Kautz, Henry/0000-0001-5219-2970; Luo, Jiebo/0000-0002-4516-9729
CR AGARWAL M, 2006, INT C PATT REC
   [Anonymous], ACM MULT WORKSH MULT
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2003, NIST TRECVID 2003
   ASSFALG J, 2002, IEEE T MULTIMEDIA
   Aytar Y., 2007, P ICME
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cooper M, 2007, ICSC 2007: INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING, PROCEEDINGS, P337, DOI 10.1109/ICSC.2007.57
   Cooper M, 2005, ACM T MULTIM COMPUT, V1
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   JIANG W, 2007, P CVPR WORKSH SEM LE
   JOHNSON M, 2005, BRIT MACH VIS C
   JOSHI D, 2008, ACM C IM VID RETR
   Kennedy Lyndon., 2007, P 15 INT C MULTIMEDI, P631
   Kumar S, 2006, INT J COMPUT VISION, V68, P179, DOI 10.1007/s11263-006-7007-9
   Lafferty John, 2001, INT C MACH LEARN ICM
   LAZEBNIK S, 2005, IEEE C COMP VIS PATT
   Li LJ, 2007, IEEE I CONF COMP VIS, P345
   LIAO L, 2005, NEURAL INFORM PROCES
   Lim JH, 2003, IEEE MULTIMEDIA, V10, P28, DOI 10.1109/MMUL.2003.1237548
   Loui AC, 2003, IEEE T MULTIMEDIA, V5, P390, DOI 10.1109/TMM.2003.814723
   Luo JB, 2006, IEEE SIGNAL PROC MAG, V23, P101
   MA W, 1998, P SIGN SYST COMP
   Naaman M, 2004, ACM-IEEE J CONF DIG, P53, DOI 10.1145/996350.996366
   NAAMAN M, 2005, THESIS STANFORD U ST
   Pantofaru C., 2005, A comparison of image segmentation algorithms, P383
   QUELHAS P, 2005, IEEE INT C COMP VIS, V1
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   RUSSELL B, 2007, INT J COMPUT VIS
   Scholkopf B., 1999, Making large scale svm learning practical, P41
   Sha F, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P213
   Simon I, 2007, IEEE I CONF COMP VIS, P274
   SIVIC J, 2005, IEEE INT C COMP VIS, V1
   SUTTON CA, 2004, INT C MACH LEARN
   TAL D, 2001, IEEE P INT C COMP VI
   TIEU K, 2004, INT J COMPUT VIS
   WANG J, 1999, IEEE T PATTERN ANAL, V23, P947
   Weiss Y, 2001, NEU INF PRO, P229
   Wu Y, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1986
   Wu Y, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P529, DOI 10.1109/ICME.2006.262442
   YAN R, 2006, IEEE INT C MULT EXP
   YAVLINSKI A, 2007, ACM P MULT
   Yuan JS, 2008, PROC CVPR IEEE, P47
NR 45
TC 30
Z9 34
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 208
EP 219
DI 10.1109/TMM.2008.2009693
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800003
DA 2024-07-18
ER

PT J
AU Shen, JL
   Tao, DC
   Li, XL
AF Shen, Jialie
   Tao, Dacheng
   Li, Xuelong
TI QUC-Tree: Integrating Query Context Information for Efficient Music
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Indexing structure; KNN; music; QUC-tree; similarity query
AB In this paper, we introduce a novel indexing scheme-QUery Context tree (QUC-tree) to facilitate efficient query sensitive music search under different query contexts. Distinguished from the previous approaches, QUC-tree is a balanced multiway tree structure, where each level represents the data space at different dimensionality. Before the tree structure construction, Principle Component Analysis (PCA) is applied for data analysis and transforming the raw composite features into a new feature space sorted by the importance of acoustic features. The PCA transformed data and reduced dimensions in the upper levels can alleviate suffering from dimensionality curse. To accurately mimic human perception, an extension called QUC+ -tree is proposed, which further applies multivariate regression and EM based algorithm to estimate the weight of each individual feature. The comprehensive extensive experiments to evaluate the proposed structures against state-of-art techniques based on different datasets. The experimental results demonstrate the superiority of our technique.
C1 [Shen, Jialie] Singapore Management Univ, Sch Informat Syst, Singapore, Singapore.
   [Tao, Dacheng] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
   [Li, Xuelong] Univ London, Sch Comp Sci & Informat Syst, London WC1E 7HK, England.
C3 Singapore Management University; Nanyang Technological University;
   University of London
RP Shen, JL (corresponding author), Singapore Management Univ, Sch Informat Syst, Singapore, Singapore.
EM jlshen@smu.edu.sg; dctao@ntu.edu.sg; xuelong@dcs.bbk.ac.uk
RI Shen, Jialie/AAX-6851-2020; Tao, Dacheng/A-5449-2012; Li,
   Xuelong/ABF-3381-2020; SHEN, Jialie/E-8573-2012; li,
   xiang/GWM-6319-2022; Li, Xuelong/Z-3785-2019
OI Tao, Dacheng/0000-0001-7225-5449; Li, Xuelong/0000-0002-0019-4197
CR AGGARWAL AHC, 2001, ICDT 01, P420
   [Anonymous], 1994, The VLDB Journal, DOI DOI 10.1007/BF01231606
   Bayer R., 1972, Acta Informatica, V1, P173, DOI 10.1007/BF00288683
   BENTLEY J, 1997, IEEE T SOFTWARE ENG, V5, P333
   Berchtold S, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P28
   Berchtold S., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), P577, DOI 10.1109/ICDE.2000.839456
   BERCHTOLD S, 1998, ACM SIGMOD, P142
   Böhm C, 2001, ACM COMPUT SURV, V33, P322, DOI 10.1145/502807.502809
   Byrd D, 2002, INFORM PROCESS MANAG, V38, P249, DOI 10.1016/S0306-4573(01)00033-4
   CAI R, 2007, ACM MM 07, P553
   Casella G., 2001, Statistical Inference, V2nd
   Ciaccia P, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P426
   CIACCIA P, 1997, VLDB 97, P194
   Cui B., 2003, PROC ACM SIGMOD INT, P479
   CUI B, 2006, ACM MM 06, P47
   DATAR M, 2004, ACM SCG 04
   DOWNIE S, 2000, ACM SIGIR 00, P73
   Golub G.H., 2013, Matrix Computations, DOI DOI 10.56021/9781421407944
   Jagadish HV, 2005, ACM T DATABASE SYST, V30, P364, DOI 10.1145/1071610.1071612
   Law MHC, 2006, IEEE T PATTERN ANAL, V28, P377, DOI 10.1109/TPAMI.2006.56
   LI T, 2003, ACM SIGIR C RES DEV, P282
   NIEVERGELT J, 1984, ACM T DATABASE SYST, V9, P38, DOI 10.1145/348.318586
   Pierce JohnR., 1992, SCI MUSICAL SOUND
   Samet H, 2006, FDN MULTIDIMENSIONAL
   Samet H., 1990, The Design and Analysis of Spatial Data Structures
   SHEN J, 2004, ACM CIKM 04, P154
   Shen JL, 2006, IEEE T MULTIMEDIA, V8, P1179, DOI 10.1109/TMM.2006.884618
   Shen X., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P43, DOI 10.1145/1076034.1076045
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   YU C, 2001, VLDB, P421
NR 31
TC 13
Z9 13
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 313
EP 323
DI 10.1109/TMM.2008.2009719
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800011
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Ji, XY
   Zhao, DB
   Gao, W
AF Ji, Xiangyang
   Zhao, Debin
   Gao, Wen
TI Concealment of Whole-Picture Loss in Hierarchical B-Picture Scalable
   Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Error concealment; H.264/AVC; hierarchical B-picture; motion
   compensation; scalable video coding
ID ERROR CONCEALMENT; MODE SELECTION; H.264/AVC
AB H.264/AVC scalable video coding (H.264/AVC SVC), as the scalable extension of H.264/AVC, offers the flexible adaptivity in terms of spatial, temporal and SNR scalabilities for the generated bitstream. However, such compressed video still suffers from the bad playback quality when packet loss occurs over unreliable networks. In this paper, we present an error concealment algorithm to tackle the whole-picture loss problem in H.264/AVC SVC when hierarchical B-picture coding is used to support temporal scalability. In the proposed algorithm, by taking advantage of the temporal relationship among the adjacent video pictures, the motion information of the lost picture is derived simply and efficiently based on the principle of temporal direct mode. Utilizing the derived motion information, the lost picture is concealed by performing motion compensation on the correctly received temporally previous and future video pictures. The experimental results, demonstrate that as a post-processing tool, the proposed error concealment algorithm is able to significantly improve both the objective and subjective qualities of the decoded video pictures in the presence of packet losses when compared to the error concealment algorithm used in H.264/AVC SVC reference software. The proposed method can also be applied to H.264/AVC with hierarchical R-picture coding for error concealment.
C1 [Ji, Xiangyang] Chinese Acad Sci, Grad Sch, Beijing 100080, Peoples R China.
   [Ji, Xiangyang] Chinese Acad Sci, Inst Comp Technol, Beijing 100080, Peoples R China.
   [Zhao, Debin] Harbin Inst Technol, Dept Comp Sci, Harbin 150001, Peoples R China.
   [Gao, Wen] Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Computing Technology,
   CAS; Harbin Institute of Technology; Peking University
RP Ji, XY (corresponding author), Tsinghua Univ, Automat Dept, Broadband Networks & Digital Media Lab, Beijing 100084, Peoples R China.
EM xyji@mail.tsinghua.edu; dbzhao@jdl.ac.cn; wgao@pku.edu.cn
RI Zhao, Debin/JEP-0204-2023
FU National Science Foundation of China [60736043]
FX Manuscript received September 05, 2007; revised August 06, 2008. Current
   version published January 08, 2009. This work was supported by National
   Science Foundation of China (60736043). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Deepa Kundur.
CR Alkachouh Z, 2000, IEEE T IMAGE PROCESS, V9, P729, DOI 10.1109/83.841948
   [Anonymous], JVTQ046
   [Anonymous], P IEEE INT C MULT EX
   Atzori L, 2001, IEEE T MULTIMEDIA, V3, P326, DOI 10.1109/6046.944476
   BAJIC IV, 2007, IEEE T CIRCUITS  APR, P508
   BELFIORE M, 2003, IEEE T MULTIMEDIA, V7, P316
   Belfiore S, 2003, SIGNAL PROCESS-IMAGE, V18, P907, DOI 10.1016/j.image.2003.08.008
   Chen Ying, 2006, Journal of Zhejiang University (Science), V7, P677, DOI 10.1631/jzus.2006.A0677
   Feng J, 1997, IEEE T CONSUM ELECTR, V43, P183, DOI 10.1109/30.585539
   Flierl M, 2003, IEEE T CIRC SYST VID, V13, P587, DOI 10.1109/TCSVT.2003.814963
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   *JVT, 2006, JVTT202 ISOIEC MPEG
   LAM WM, 1993, P ICASSP, V5, P417
   Meisinger K, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P209
   PARK CS, 1994, P IEEE INT S CIRC SY, V3, P229
   Peng Q, 2002, 2002 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, CIRCUITS AND SYSTEMS AND WEST SINO EXPOSITION PROCEEDINGS, VOLS 1-4, P10, DOI 10.1109/ICCCAS.2002.1180560
   Schwarz H., 2005, JVTP014
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Suh JW, 1997, IEEE T CONSUM ELECTR, V43, P295, DOI 10.1109/30.628616
   Tourapis AM, 2005, IEEE T CIRC SYST VID, V15, P119, DOI 10.1109/TCSVT.2004.837021
   WANG Y, 1993, IEEE T COMMUN, V41, P1544, DOI 10.1109/26.237889
   Wenger S, 2003, IEEE T CIRC SYST VID, V13, P645, DOI 10.1109/TCSVT.2003.814966
   WENGER S, 1999, Q15116RL ITU TEL STA
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zheng J., 2005, P IEEE INT C IM PROC, V2, P265
NR 25
TC 18
Z9 22
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 11
EP 22
DI 10.1109/TMM.2008.2008874
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700002
DA 2024-07-18
ER

PT J
AU Shen, LQ
   Liu, Z
   Zhang, ZY
   Shi, XL
AF Shen, Liquan
   Liu, Zhi
   Zhang, Zhaoyang
   Shi, Xuli
TI Fast Inter Mode Decision Using Spatial Property of Motion Field
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264; image coding; image motion analysis; mode decision
AB Variable size motion estimation with multiple reference frames has been adopted by the new video coding standard H.264. It can achieve significant coding efficiency compared to coding a macroblock (NIB) in regular size with single reference frame. On the other hand, it causes high computational complexity of motion estimation at the encoder. Rate distortion optimized (RDO) decision is one powerful method to choose the best coding mode among all combinations of block sizes and reference frames, but it requires extremely high computation. In this paper, a fast inter mode decision is proposed to decide best prediction mode utilizing the spatial continuity of motion field, which is generated by motion vectors from 4 X 4 motion estimation. Motion continuity of each MB is decided based on the motion edge map detected by the Sobel operator. Based on the motion continuity of a MB, only a small number of block sizes are selected in motion estimation and RDO computation process. Simulation results show that our algorithm can save more than 50% computational complexity, with negligible loss of coding efficiency.
C1 [Shen, Liquan; Liu, Zhi] Shanghai Univ, Sch Commun & Informat Engn, Shanghai, Peoples R China.
   [Zhang, Zhaoyang; Shi, Xuli] Shanghai Univ, Minist Educ, Key Lab Adv Display & Syst Applicat, Shanghai, Peoples R China.
C3 Shanghai University; Shanghai University
RP Shen, LQ (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai, Peoples R China.
EM jsslq@163.com
RI Zhang, Zhaoyang/AFQ-9161-2022; Shen, Liquan/D-4832-2012; LIU,
   Zhi/D-4518-2012
OI LIU, Zhi/0000-0002-8428-1131
CR Choi I, 2006, IEEE T CIRC SYST VID, V16, P1557, DOI 10.1109/TCSVT.2006.883506
   *ITU T, 2005, H264 ITUT
   Jing X, 2004, ELECTRON LETT, V40, P1050, DOI 10.1049/el:20045243
   Kim YH, 2004, ELECTRON LETT, V40, P1172, DOI 10.1049/el:20046155
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Wang HL, 2007, IEEE T MULTIMEDIA, V9, P882, DOI 10.1109/TMM.2007.893345
   Wu D, 2005, IEEE T CIRC SYST VID, V15, P953, DOI 10.1109/TCSVT.2005.848304
   YANG G, 2002, CHINA SAFETY SCI J, V12, P10
   Yin P, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P853
   You J, 2006, IEEE T CONSUM ELECTR, V52, P1377, DOI 10.1109/TCE.2006.273159
   YU AC, 2004, EFFICIENT BLOCK SIZE, V1, P169
NR 11
TC 48
Z9 57
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1208
EP 1214
DI 10.1109/TMM.2008.2001358
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600022
DA 2024-07-18
ER

PT J
AU Chen, M
   Su, GM
   Wu, M
AF Chen, Meng
   Su, Guan-Ming
   Wu, Min
TI Dynamic resource allocation for robust distributed multi-point video
   conferencing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME 2006)
CY JUL 09-12, 2006
CL Toronto, CANADA
SP IEEE, IEEE Circuits & Syst Soc, IEEE Commun Soc, IEEE Comp Soc, IEEE Signal Proc Soc
DE multi-point video conferencing; packet-division multiple access (PDMA);
   unequal error protection
ID QUALITY; MULTIMEDIA; CODES
AB This paper proposes a distributed multi-point video conferencing system over packet erasure channels, where the aggregation of multiple video streams and resource allocation are performed in a distributed manner. Video stream combiners, which are located in different geographical areas and serve as portals for conferees, aggregate incoming streams supplied by local users with other streams aggregated from nearby video stream combiners. A packet-division multiple-access (PDMA)-based error protection scheme is proposed to be performed at each video stream combiner to minimize the maximal expected video distortion among aggregated streams. The proposed error protection scheme for multi-stream aggregation also supports user preference. In order to deliver video streams to end users with different preferred quality, a consensus algorithm is proposed to adaptively perform resource allocation based on user preference. Simulation results show that the proposed multi-stream aggregation and error protection scheme has significant gains over traditional multi-stream error protection schemes for a multi-point video conferencing system.
C1 [Wu, Min] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park
RP Chen, M (corresponding author), PCTEL Inc, Germantown, MD 20876 USA.
EM margaret_ch@yahoo.com; guanmingsu@yahoo.com; minwu@eng.umd.edu
RI Wu, Min/B-7501-2009; Wu, Min/P-2009-2019
OI Wu, Min/0000-0001-7672-9357
CR [Anonymous], P INT C DAT ENG
   CHANG R, 2004, P IEEE INT C MULT EX, V3, P2111
   Chen M, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1149, DOI 10.1109/ICME.2006.262739
   CHEN TC, 1994, IEEE T CIRCUITS SYST, V4, P425
   Fung KT, 2004, IEEE T MULTIMEDIA, V6, P31, DOI 10.1109/TMM.2003.819761
   Gringeri S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P113, DOI 10.1145/319463.319478
   HUO L, 2004, P PICT COD S DEC
   Huo LS, 2005, REAL-TIME IMAGING, V11, P300, DOI 10.1016/j.rti.2005.05.001
   Ji Z, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P844
   Lin CW, 2003, IEEE T CIRC SYST VID, V13, P982, DOI 10.1109/TCSVT.2003.816505
   Lin S., 1983, Error Control Coding: Fundamentals and Applications
   Majumdar A, 2002, IEEE T CIRC SYST VID, V12, P524, DOI 10.1109/TCSVT.2002.800315
   Markopoulou AP, 2003, IEEE ACM T NETWORK, V11, P747, DOI 10.1109/TNET.2003.818179
   McCanne S, 1997, IEEE J SEL AREA COMM, V15, P983, DOI 10.1109/49.611154
   MEHRA P, 2003, P PACK VID WORKSH AP
   Puri R, 2001, SIGNAL PROCESS-IMAGE, V16, P745, DOI 10.1016/S0923-5965(01)00005-4
   Radha H, 2004, IEEE IMAGE PROC, P1747
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   Ringenburg MF, 2004, IEEE DATA COMPR CONF, P222
   Shan YF, 2004, IEEE IMAGE PROC, P3133
   Stankovic VM, 2004, IEEE T CIRC SYST VID, V14, P1064, DOI 10.1109/TCSVT.2004.831964
   Su GM, 2005, IEEE T CIRC SYST VID, V15, P1124, DOI 10.1109/TCSVT.2005.852626
   Su GM, 2006, IEEE IMAGE PROC, P21, DOI 10.1109/ICIP.2006.313151
   Sun MT, 1997, IEEE T CIRC SYST VID, V7, P855, DOI 10.1109/76.644065
   TAN WT, 2001, IEEE T CIRCUITS SYST, V11, P524
   Xu XF, 2004, IEEE IMAGE PROC, P1759
   YU B, 2003, ACM MULTIMEDIA   NOV, P646
   Zhang XM, 2003, IEEE T CIRC SYST VID, V13, P121, DOI 10.1109/TCSVT.2002.808437
NR 28
TC 3
Z9 4
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 910
EP 925
DI 10.1109/TMM.2008.922846
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800021
DA 2024-07-18
ER

PT J
AU Erol, B
   Berkner, K
   Joshi, S
AF Erol, Berna
   Berkner, Kathrin
   Joshi, Siddharth
TI Multimedia clip generation from documents for browsing on mobile devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive content delivery; content navigation; document conversion;
   document repurposing; mobile document; multimedia generation; Multimedia
   Thumbnail
AB Small displays on mobile handheld devices, such as personal digital assistants (PDAs) and cellular phones, are the bottlenecks for usability of most content browsing applications. Generally, conventional content such as documents and web pages need to be modified for effective presentation on mobile devices. This paper proposes a novel visualization for documents, called Multimedia Thumbnails, which consists of text and image content converted into playable multimedia clips. A Multimedia Thumbnail utilizes visual and audio channels of small portable devices as well as both spatial and time dimensions to communicate text and image information of a single document. The proposed algorithm for generating Multimedia Thumbnails includes 1) a semantic document analysis step, where salient content from a source document is extracted; 2) an optimization step, where a subset of this extracted content is selected based on time, display, and application constraints; and 3) a composition step, where the selected visual and audible document content is combined into a Multimedia Thumbnail. Scalability of MMNails that allows generation of multimedia clips of various lengths is also described. A user study is presented that evaluates the effectiveness of the proposed Multimedia Thumbnail visualization.
C1 [Erol, Berna; Berkner, Kathrin] RICOH Innovat, Calif Res Ctr, Menlo Pk, CA 94025 USA.
   [Joshi, Siddharth] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
C3 Stanford University
RP Erol, B (corresponding author), RICOH Innovat, Calif Res Ctr, Menlo Pk, CA 94025 USA.
EM berna_erol@rii.ricoh.com; berkner@rii.ricoh.com; sidj@stanford.edu
CR *AD, PDF ACC VIS IMP
   [Anonymous], 1544412000 ISOIEC
   [Anonymous], ACM MULTIMEDIA
   [Anonymous], CHI 99
   BERKNER K, 2004, P SOC PHOTO-OPT INS, V5296, P53
   Breuel TM, 2002, INT C PATT RECOG, P476, DOI 10.1109/ICPR.2002.1044766
   Chen Y, 2005, IEEE INTERNET COMPUT, V9, P50, DOI 10.1109/MIC.2005.5
   EROL B, 2006, P ICME 2006, P2133
   EROL B, 2004, P IEEE INT C PATT RE
   EROL B., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, P231
   Fan X, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P53
   *ISO IEC, 1449610 ISOIEC 10
   Lam Heidi., 2005, P SIGCHI C HUMAN FAC, P681
   Liu Feng., 2006, ACM MULTIMEDIA 2006, P241, DOI DOI 10.1145/1180639.1180702
   MADERLECHNER G, 1999, P DLIA, P216
   MARSHALL CC, 2002, P 2 ACM IEEE CS JOIN, P56
   Neelamani R, 2002, IEEE IMAGE PROC, P381
   Otterbacher J., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P589, DOI 10.1145/1148170.1148271
   Parente P., 2004, ASSETS 2004. The Sixth International ACM SIGACCESS Conference on Computers and Accessibility, P2
   RIVEST RL, 1997, INTRO ALGORITHMS
   Salton G., 1989, Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer
   Wang JJ, 2006, NSREC: 2006 IEEE RADIATION EFFECTS DATA WORKSHOP, WORKSHOP RECORD, P101
   WOODRUFF A, 2001, P SIGCHI C HUM FACT, P198
   *WORLD WID WEB CON, DOC OBJ MOD LEV 1 SP
   Xie X, 2006, IEEE T MULTIMEDIA, V8, P707, DOI 10.1109/TMM.2006.876294
NR 25
TC 2
Z9 2
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 711
EP 723
DI 10.1109/TMM.2008.922784
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800005
DA 2024-07-18
ER

PT J
AU Lee, JS
   Park, CH
AF Lee, Jong-Seok
   Park, Cheol Hoon
TI Robust audio-visual speech recognition based on late integration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio-visual speech recognition; late integration; robustness hidden
   Markov model; interframe correlation; neural network; stochastic
   optimization
ID FUSION
AB Audio-visual speech recognition (AVSR) using acoustic and visual signals of speech has received attention because of its robustness in noisy environments. In this paper, we present a late integration scheme-based AVSR system whose robustness under various noise conditions is improved by enhancing the performance of the three parts composing the system. First, we improve the performance of the visual subsystem by using the stochastic optimization method for the hidden Markov models as the speech recognizer. Second, we propose a new method of considering dynamic characteristics of speech for improved robustness of the acoustic subsystem. Third, the acoustic and the visual subsystems are effectively integrated to produce final robust recognition results by using neural networks. We demonstrate the performance of the proposed methods via speaker-independent isolated word recognition experiments. The results show that the proposed system improves robustness over the conventional system under various noise conditions without a priori knowledge about the noise contained in the speech.
C1 [Lee, Jong-Seok; Park, Cheol Hoon] Korea Adv Inst Sci & Technol, Sch Elect Engn & Comp Sci, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Lee, JS (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn & Comp Sci, Taejon 305701, South Korea.
EM jslee@nnmi.kaist.ac.kr; chpark@kaist.ac.kr
RI Lee, Jong-Seok/AAF-5197-2020
OI Lee, Jong-Seok/0000-0001-5255-4425
CR Adjoudani A., 1996, Speechreading by Humans and Machines, P461
   APPLEBAUM TH, 1991, INT CONF ACOUST SPEE, P985, DOI 10.1109/ICASSP.1991.150506
   Arai T, 1998, INT CONF ACOUST SPEE, P933, DOI 10.1109/ICASSP.1998.675419
   Arnold P, 2001, BRIT J PSYCHOL, V92, P339, DOI 10.1348/000712601162220
   Bartels C, 2005, 2005 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), P191
   Belegundu A., 1999, Optimization Concepts and Applications in Engineering
   BENOIT C, 2000, STRUCTURE MULTIMODAL, V2, P485
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Chibelushi CC, 2002, IEEE T MULTIMEDIA, V4, P23, DOI 10.1109/6046.985551
   Coen MichaelH., 2001, Proceedings of the 17th International Joint Conference on Artificial Intelligence - Volume 2. IJCAI'01, P1417
   Conrey B, 2006, J ACOUST SOC AM, V119, P4065, DOI 10.1121/1.2195091
   Dasarathy BV, 1997, P IEEE, V85, P24, DOI 10.1109/5.554206
   DRULLMAN R, 1994, J ACOUST SOC AM, V95, P1053, DOI 10.1121/1.408467
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   EDDY SR, 1995, P 3 INT C INT SYST M, P114
   FURUI S, 1986, IEEE T ACOUST SPEECH, V34, P52, DOI 10.1109/TASSP.1986.1164788
   Haykin S., 1999, NEURAL NETWORKS COMP, DOI [10.1017/S0269888998214044, DOI 10.1017/S0269888998214044]
   Hazen TJ, 2006, IEEE T AUDIO SPEECH, V14, P1082, DOI 10.1109/TSA.2005.857572
   Heckmann M, 2002, EURASIP J APPL SIG P, V2002, P1260, DOI 10.1155/S1110865702206150
   Hermansky H, 1994, IEEE T SPEECH AUDI P, V2, P578, DOI 10.1109/89.326616
   Lee JS, 2006, IEEE SYS MAN CYBERN, P198, DOI 10.1109/ICSMC.2006.384382
   LEE JS, 2006, THESIS KAIST DAEJEON
   LEWIS TW, 2004, P 27 C AUSTR COMP SC, P305
   Li D., 2003, P 11 ACM INT C MULTI, P604
   MARCHERET E, 2007, P ICASSP HON HI APR, V4, P945
   Massaro D.W., 1987, Speech perception by ear and eye: A paradigm for psychological inquiry
   *MATHWORKS INC, 2005, OPT TOOLB US GUID
   METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114
   Meyer G. F., 2004, Information Fusion, V5, P91, DOI 10.1016/j.inffus.2003.07.001
   Nam D, 2004, IEICE T INF SYST, VE87D, P2499
   OUELLETTE DV, 1981, LINEAR ALGEBRA APPL, V36, P187, DOI 10.1016/0024-3795(81)90232-9
   Papoulis A., 1991, Probability, Random Variables and Stochastic Processes
   PAUL D, 1985, P ICASSP TAMP FL MAR, P13
   PETAJAN ED, 1984, P IEEE GLOB TEL C AT, P265
   Rogozan A, 1998, SPEECH COMMUN, V26, P149, DOI 10.1016/S0167-6393(98)00056-9
   Ross LA, 2007, CEREB CORTEX, V17, P1147, DOI 10.1093/cercor/bhl024
   SZU H, 1987, PHYS LETT A, V122, P157, DOI 10.1016/0375-9601(87)90796-1
   TAMURA S, 2005, P IEEE INT C AC SPEE, V1, P469
   VARGA A, 1993, SPEECH COMMUN, V12, P247, DOI 10.1016/0167-6393(93)90095-3
   VERMA A, 1999, P WORKSH AUT SPEECH, P71
   Xuan Huang, 2021, 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), P867, DOI 10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00144
NR 41
TC 42
Z9 45
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 767
EP 779
DI 10.1109/TMM.2008.922789
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800010
DA 2024-07-18
ER

PT J
AU Pikrakis, A
   Giannakopoulos, T
   Theodoridis, S
AF Pikrakis, Aggelos
   Giannakopoulos, Theodoros
   Theodoridis, Sergios
TI Speech/music discriminator of radio recordings based on dynamic
   programming and Bayesian networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayesian networks; dynamic programming; speech-music discrimination
ID SEGMENTATION
AB This paper presents a multistage system for speech/music discrimination which is based on a three-step procedure. The first step is a computationally efficient scheme consisting of a region growing technique and operates on a 1-D feature sequence, which is extracted from the raw audio stream, This scheme is used as a preprocessing stage and yields segments with high music and speech precision at the expense of leaving certain parts of the audio recording unclassified. The unclassified parts of the audio stream are then fed as input to a more computationally demanding scheme. The latter treats speech/music discrimination of radio recordings as a probabilistic segmentation task, where the solution is obtained by means of dynamic programming. The proposed scheme seeks the sequence of segments and respective class labels (i.e., speech/music) that maximize the product of posterior class probabilities, given the data that form the segments. To this end, a Bayesian Network combiner is embedded as a posterior probability estimator. At a final stage, an algorithm that performs boundary correction is applied to remove possible errors at the boundaries of the segments (speech or music) that have been previously generated. The proposed system has been tested on radio recordings from various sources. The overall system accuracy is approximately 96%. Performance results are also reported on a musical genre basis and a comparison with existing methods is given.
C1 [Pikrakis, Aggelos; Giannakopoulos, Theodoros; Theodoridis, Sergios] Univ Athens, Dept Informat & Telecommun, Athens 15784, Greece.
   [Pikrakis, Aggelos] Univ Piraeus, Piraeus, Greece.
C3 National & Kapodistrian University of Athens; University of Piraeus
RP Pikrakis, A (corresponding author), Univ Athens, Dept Informat & Telecommun, Athens 15784, Greece.
EM pikrakis@di.uoa.gr; tyiannak@di.uoa.gr; stheodor@di.uoa.gr
RI Giannakopoulos, Theodoros/I-4678-2012; Pikrakis, Aggelos/AAR-1334-2021;
   Theodoridis, Sergios/C-3142-2016
OI Pikrakis, Aggelos/0000-0001-7355-327X; 
CR Ajmera J, 2003, SPEECH COMMUN, V40, P351, DOI 10.1016/S0167-6393(02)00087-0
   [Anonymous], 2001, Probability, Random Variables and Stochastic Processes
   Bartsch MA, 2005, IEEE T MULTIMEDIA, V7, P96, DOI 10.1109/TMM.2004.840597
   Carey MJ, 1999, INT CONF ACOUST SPEE, P149, DOI 10.1109/ICASSP.1999.758084
   CASAGRANDE N, 2005, P ISMIR 2005 LOND UK
   El-Maleh K, 2000, INT CONF ACOUST SPEE, P2445, DOI 10.1109/ICASSP.2000.859336
   Garg A, 2002, INT C PATT RECOG, P779, DOI 10.1109/ICPR.2002.1048418
   Heckerman D., 1995, MSRTR9506
   Misra H., 2004, P IEEE INT C AC SPEE
   Moreno PJ, 2000, INT CONF ACOUST SPEE, P2417, DOI 10.1109/ICASSP.2000.859329
   Panagiotakis C, 2005, IEEE T MULTIMEDIA, V7, P155, DOI 10.1109/TMM.2004.840604
   Saunders J, 1996, INT CONF ACOUST SPEE, P993, DOI 10.1109/ICASSP.1996.543290
   Scheirer E, 1997, INT CONF ACOUST SPEE, P1331, DOI 10.1109/ICASSP.1997.596192
   Theodoridis S, 2006, PATTERN RECOGNITION, 3RD EDITION, P1
   WILLIAMS G, 1999, P EUR 99 BUD HUNG SE, V2, P687
   Zhang T, 2001, IEEE T SPEECH AUDI P, V9, P441, DOI 10.1109/89.917689
NR 16
TC 36
Z9 39
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 846
EP 857
DI 10.1109/TMM.2008.922870
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800016
DA 2024-07-18
ER

PT J
AU Ribeiro, MX
   Traina, AJM
   Traina, C
   Azevedo-Marques, PM
AF Ribeiro, Marcela X.
   Traina, Agma J. M.
   Traina, Caetano, Jr.
   Azevedo-Marques, Paulo M.
TI An association rule-based method to support medical image diagnosis with
   efficiency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE association rules; data pre-processing; image mining; support of medical
   diagnoses
AB In this paper, we propose a method based on association rule-mining to enhance the diagnosis of medical images (mammograms). It combines low-level features automatically extracted from images and high-level knowledge from specialists to search for patterns. Our method analyzes medical images and automatically generates suggestions of diagnoses employing mining of association rules. The suggestions of diagnosis are used to accelerate the image analysis performed by specialists as well as to provide them an alternative to work on. The proposed method uses two new algorithms, PreSAGe and HiCARe. The PreSAGe algorithm combines, in a single step, feature selection and discretization, and reduces the mining complexity. Experiments performed on PreSAGe show that this algorithm is highly suitable to perform feature selection and discretization in medical images. HiCARe is a new associative classifier. The HiCARe algorithm has an important property that makes it unique: it assigns multiple keywords per image to suggest a diagnosis with high values of accuracy. Our method was applied to real datasets, and the results show high sensitivity (up to 95%) and accuracy (up to 92%), allowing us to claim that the use of association rules is a powerful means to assist in the diagnosing task.
C1 [Ribeiro, Marcela X.; Traina, Agma J. M.; Traina, Caetano, Jr.; Azevedo-Marques, Paulo M.] Univ Sao Paulo, Dept Comp Sci, BR-13560 Sao Carlos, SP, Brazil.
C3 Universidade de Sao Paulo
RP Ribeiro, MX (corresponding author), Univ Sao Paulo, Dept Comp Sci, BR-13560 Sao Carlos, SP, Brazil.
EM mxavier@icnic.usp.br; agma@icmc.usp.br; caetano@icinc.usp.br;
   pmarques@fmrp.usp.br
RI Ribeiro, Marcela/C-9156-2016; Incod, Inct/J-8375-2013; Traina, Caetano
   Jr/E-9814-2011; Traina, Agma J. M./F-1299-2011; Azevedo-Marques,
   Paulo/A-1305-2008
OI Ribeiro, Marcela/0000-0002-2323-5326; Azevedo-Marques,
   Paulo/0000-0002-7271-2774
CR Agrawal R., 1993, SIGMOD Record, V22, P207, DOI 10.1145/170036.170072
   Agrawal R., 1994, P INT VLDB C VLDB 94, P487, DOI DOI 10.5555/645920.672836
   [Anonymous], 11 C UNC ART INT
   ANTIONIE ML, 2003, LECT NOTES ARTIF INT, V2797, P68
   Beyer K., 1999, Proceedings of the 7th International Conference on Database Theory, ICDT'99, DOI [10.1007/3-540-49257-7_15, DOI 10.1007/3-540-49257-7_15]
   Felipe JC, 2003, COMP MED SY, P175
   FOSCHI PG, 2002, P 8 INT WORKSH MULT, P103
   Haiwei P, 2005, P ANN INT IEEE EMBS, P3308
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   HSU W, 2000, P ACM SIGMOD, P593
   Kazemzadeh RS, 2006, COMP MED SY, P321, DOI 10.1109/CBMS.2006.99
   Kira K, 1992, P 9 INT WORKSH MACH, P249
   Klemettinen M., 1994, CIKM 94. Proceedings of the Third International Conference on Information and Knowledge Management, P401, DOI 10.1145/191246.191314
   LIU Y, 2001, VISIM WORKSH INF RET, P4
   Mudigonda NR, 2001, IEEE T MED IMAGING, V20, P1215, DOI 10.1109/42.974917
   Olukunle A, 2002, IEEE CCEC 2002: CANADIAN CONFERENCE ON ELECTRCIAL AND COMPUTER ENGINEERING, VOLS 1-3, CONFERENCE PROCEEDINGS, P1181, DOI 10.1109/CCECE.2002.1013116
   Ordonez C, 2006, KNOWL INF SYST, V9, P259, DOI 10.1007/s10115-005-0226-5
   Ordonez C, 1999, P IEEE INT FORUM RES, P38, DOI 10.1109/ADL.1999.777689
   Pan HW, 2005, LECT NOTES ARTIF INT, V3584, P598
   Quinlan J.R., 1993, C4.5 : programs for machine learning
   WANG X, 2004, P IEEE CCGEI 2004, P1495
   Zaiane O.R., 2002, MDMKDD, P62
NR 22
TC 42
Z9 44
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2008
VL 10
IS 2
BP 277
EP 285
DI 10.1109/TMM.2007.911837
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 254HC
UT WOS:000252576700012
DA 2024-07-18
ER

PT J
AU Tian, DH
   AlRegib, G
AF Tian, Dihong
   AlRegib, Ghassan
TI Multistreaming of 3-D scenes with optimized transmission and rendering
   scalability
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multiresolution; multistreaming; partial reliability; rate-distortion
   optimization; 3-D scene
AB Three-dimensional (3-D) graphic scenes require considerable network bandwidth to be transmitted and computing power to be rendered on a user's terminal. Toward high-quality display in real time, we propose a sender-driven mechanism for streaming 3-D scenes in a resource-constrained environment. In doing so, objects are encoded into multiresolutions to provide transmission and rendering scalability, and a weighted distortion metric is developed to measure the quality of a scene rendered with multiresolution objects, modeling objects' unequal importance regarding display. To preserve the manipulation independency of multiple objects in data delivery while provide preferential treatment for different objects as well as different layers of each object, transmission of the objects is performed over multiple streams in a partially sequenced and partially reliable fashion. A rate-distortion optimization framework is developed, which determines an optimal level of reliability for every chunk of data in each stream, taking into account the rendering importance of the object, the distortion-rate performance of the data chunks, and the statistics of the network link. Compared with heuristical methods, simulation results show that the proposed framework maximizes the display quality of the scene while minimizing the amount of data that needs to be processed by the client's rendering engine.
C1 Georgia Inst Technol, Sch Elect & Comp Engn, Savannah, GA 31407 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Tian, DH (corresponding author), Cisco Syst Inc, San Jose, CA 95134 USA.
EM dtian@cisco.com; gregib@ece.gatech.edu
RI Tian, Dihong/F-4973-2019
OI AlRegib, Ghassan/0000-0001-6818-8001
CR Alliez P, 2001, COMP GRAPH, P195, DOI 10.1145/383259.383281
   Alregib G, 2005, IEEE T MULTIMEDIA, V7, P1149, DOI 10.1109/TMM.2005.858404
   Chen Z., 2003, Proc. Web3D, P161
   CIGNONI P, 1998, P EUROGRAPHICS, V17, P167
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Harris A F., 2002, Proc. NOSSDAV, P43
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   KHODAKOVSKY A, 2000, P ACM SIGGRAPH, P270
   Li JK, 1998, P IEEE, V86, P1052, DOI 10.1109/5.687829
   Pajarola R, 2000, IEEE T VIS COMPUT GR, V6, P79, DOI 10.1109/2945.841122
   STEWART R, 2004, 3758 RFC INT SOC
   STEWART R, 2000, 2960 RFC INT SOC
   TAUBIN G, 1999, P EUR MIL IT
   TAUBIN G, 1998, P SIGGRAPH 98, P123
   Yan ZD, 2005, IEEE T CIRC SYST VID, V15, P138, DOI 10.1109/TCSVT.2004.837023
   ZHANG H, 1998, THESIS DEP COMPUT SC
NR 16
TC 8
Z9 8
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 736
EP 745
DI 10.1109/TMM.2007.893341
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200006
DA 2024-07-18
ER

PT J
AU Ammicht, E
   Fosler-Lussier, E
   Potarnianos, A
AF Ammicht, Egbert
   Fosler-Lussier, Eric
   Potamianos, Alexandros
TI Information seeking spoken dialogue systems - Part I: Semantics and
   pragmatics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia communication; natural language interfaces; speech
   communication
AB In this paper, the semantic and pragmatic modules of a spoken dialogue system development platform are presented and evaluated. The main goal of this research is to create spoken dialogue system modules that are portable across applications domains and interaction modalities. We propose a hierarchical semantic representation that encodes all information supplied by the user over multiple dialogue turns and can efficiently represent and be used to argue with ambiguous or conflicting information. Implicit in this semantic representation is a pragmatic module, consisting of context tracking, pragmatic analysis and pragmatic scoring submodules, which computes pragmatic confidence scores for all system beliefs. These pragmatic scores are obtained by combining semantic and pragmatic evidence from the various submodules (taking into account the modality of input) and are used to rank-order attribute-value pairs in the semantic representation, as well as identifying and resolving ambiguities. These modules were implemented and evaluated within a travel reservation dialogue system under the auspices of the DARPA Communicator project, as well as for a movie information application. Formal evaluation of the semantic and pragmatic modules has shown that by incorporating pragmatic analysis and scoring, the quality of the system improves for over 20% of the dialogue fragments examined.
C1 Bell Labs, Lucent Technol, Whippany, NJ 07981 USA.
   Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
   Tech Univ Crete, Dept Elect & Comp Engn, Khania 73100, Greece.
C3 Alcatel-Lucent; Lucent Technologies; AT&T; University System of Ohio;
   Ohio State University; Technical University of Crete
RP Ammicht, E (corresponding author), Bell Labs, Lucent Technol, Whippany, NJ 07981 USA.
EM eammicht@lucent.com; fosler@cse.ohio-state.edu; potam@telecom.tuc.gr
OI Fosler-Lussier, Eric/0000-0001-8004-5169
CR ABRAHAMS MK, 1996, NEOCLASSIC TUTORIAL
   AMMICHT E, 2001, P EUR C SPEECH COMM
   BOHUS D, 2005, P SIGDIAL WORKSH DIS
   BOHUS D, 2005, P WORKSH AUT SPEECH
   Carpenter Bob., 1998, Type-Logical Semantics
   CHUCARROLL J, 2000, P 6 ACL C APPL NAT L
   CHUCARROLL J, 1999, P EUR C SPEECH COMM
   DENECKE M, 1999, P EUR C SPEECH COMM
   DENECKE M, 2002, P INT C COMP LING TA
   GODDEAU D, 1996, P INT C SPEECH LANG
   Hayes P.J., 1985, Formal Theories of the Commonsense World
   HECKERMAN D, 1986, UNCERTAINTY ARTIFICI, P11
   HIGASHINAKA R, 2003, P ANN M ASS COMP LIN
   Johnston M., 2005, Natural Language Engineering, V11, P159, DOI 10.1017/S1351324904003572
   KOMATANI K, 2000, P INT C SPEECH LANG
   LARSEN LB, 1999, P ESCA WORKSH INT DI
   Larsson S., 2000, Natural Language Engineering, V6, P323, DOI 10.1017/S1351324900002539
   LEMON O, 2002, P 3 SIGDIAL WORKSH D
   LEVIN E, 1999, P WORKSH AUT SPEECH
   POTAMIANOS A, 2007, IN PRESS IEEE T MULT, V9
   POTAMIANOS A, 1999, P ESCA WORKSH INT DI
   POTAMIANOS A, 2000, P INT C SPEECH LANG
   RUDNICKY A, 1999, P EUR C SPEECH COMM
   RUDNICKY A, 1999, P WORKSH AUT SPEECH
   Russell S., 1995, Prentice Hall series in artificial intelligence, V25, P27
   SANSEGUNDO R, 2001, P INT C SPEECH LANG
   SENEFF S, 1998, P INT C SPEECH LANG
   Shortliffe E, 2012, COMPUTER BASED MED C, V2
   WALKER M, 2000, P INT C LANG RES EV
   Walker M., 2001, P ANN M ASS COMP LIN
   WALKER M, 2001, P HUM LANG TECHN C S
   WALKER M, 2000, NAT LANG ENG SPECIAL
   WALKER MA, 2002, P INT C SPEECH LANG
   WARD W, 1999, P WORKSH AUT SPEECH
   XU W, 2002, P 3 SIGDIAL WORKSH D
   ZHOU Q, 2000, P INT C SPEECH LANG
NR 36
TC 5
Z9 7
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 532
EP 549
DI 10.1109/TMM.2006.888011
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100009
DA 2024-07-18
ER

PT J
AU Liu, N
   Amin, P
   Subbalakshmi, KP
AF Liu, Ning
   Amin, Palak
   Subbalakshmi, K. P.
TI Security and robustness enhancement for image data hiding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE blind data hiding; distortion compensation; JPEG compression attack;
   secure image data-hiding; steganography
ID QUANTIZATION INDEX MODULATION; DIGITAL WATERMARKING; SPREAD-SPECTRUM;
   INFORMATION; VIDEO
AB In many applications, data hiding can be viewed as a tradeoff between capacity, robustness (against attacks), and embedding induced distortion. In this paper, we consider a fourth parameter: the security of the hidden information. Specifically, we propose a hash-based randomized embedding algorithm (HRE) that increases the security of the hidden data. We then optimize this algorithm against JPEG attacks. We derive a mathematical expression for the security of our algorithm, using which we show that the security of our algorithm can be increased independent of capacity, robustness, and embedding induced distortion. The maximum security depends only on the length of the key sequence, which is limited only by the size of the host image. Using a joint security and capacity measure, we show that the proposed scheme performs better than current secure quantization based data hiding schemes. We also derive the optimal value of distortion compensation factor of the HRE algorithm against JPEG compression attack. Experimental results show that the operating points achieved by the proposed scheme are 7 dB better than current blind data hiding schemes against the JPEG attack.
C1 Stevens Inst Technol, Dept Elect & Comp Engn, Hoboken, NJ 07030 USA.
C3 Stevens Institute of Technology
RP Liu, N (corresponding author), Stevens Inst Technol, Dept Elect & Comp Engn, Hoboken, NJ 07030 USA.
EM nliu@stevens.edu; pamin@stevens.edu; ksubbala@stevens.edu
RI Subbalakshmi, Koduvayur/JYO-3634-2024
OI Subbalakshmi, Koduvayur/0000-0002-1670-9378
CR Celik MU, 2002, IEEE T IMAGE PROCESS, V11, P585, DOI 10.1109/TIP.2002.1014990
   CHAE J, 1998, P IEEE INT C IM PROC
   Chae JJ, 1998, P IEEE INT FORUM RES, P319, DOI 10.1109/ADL.1998.670432
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Chen B, 2001, J VLSI SIG PROCESS S, V27, P7, DOI 10.1023/A:1008107127819
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   COSTA MHM, 1983, IEEE T INFORM THEORY, V29, P439, DOI 10.1109/TIT.1983.1056659
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Cox IJ, 2002, EURASIP J APPL SIG P, V2002, P126, DOI 10.1155/S1110865702000525
   Eggers JJ, 2003, IEEE T SIGNAL PROCES, V51, P1003, DOI 10.1109/TSP.2003.809366
   FRIDRICH J, 1999, P 5 INT S SIGN PROC
   GOTETI AK, 2004, P IEEE INT C IM PROC
   Kang XG, 2003, IEEE T CIRC SYST VID, V13, P776, DOI 10.1109/TCSVT.2003.815957
   KUNDUR D, 1999, THESIS U TORONTO TOR
   LIN CY, 2001, P IEEE INT C INFORM
   LIU N, 2004, P IEEE INT C IM PROC
   LIU N, 2004, P SPIE
   LU ACK, 2005, INT J IMAGE GRAPH, V5, P1
   Malvar HS, 2003, IEEE T SIGNAL PROCES, V51, P898, DOI 10.1109/TSP.2003.809385
   Moulin P, 2004, IEEE IMAGE PROC, P1173
   Pennebaker W. B., 1992, JPEG STILL IMAGE DAT
   Pérez-González F, 2003, IEEE T SIGNAL PROCES, V51, P960, DOI 10.1109/TSP.2003.809368
   Ramkumar M., 2000, DATA HIDING MULTIMED
   RIVEST RL, 1992, INTERNET ACTIVITIES, V3
   Sayood K, 2017, Introduction to data compression
   Solanki K, 2004, IEEE T IMAGE PROCESS, V13, P1627, DOI 10.1109/TIP.2004.837557
   Swanson MD, 1998, IEEE J SEL AREA COMM, V16, P540, DOI 10.1109/49.668976
   TZSCHOPPE R, 2005, P SPIE, V5681
   Venkatesan R, 2000, IEEE IMAGE PROC, P403, DOI 10.1109/ICIP.2000.900980
   VILAFORCEN J, 2005, P SPIE, V5681
   Wong PHW, 2003, IEEE T CIRC SYST VID, V13, P746, DOI 10.1109/TCSVT.2003.815949
   Wong PW, 2001, IEEE T IMAGE PROCESS, V10, P1593, DOI 10.1109/83.951543
   Wu M, 2003, IEEE T CIRC SYST VID, V13, P831, DOI 10.1109/TCSVT.2003.815951
   Wu M, 2003, IEEE T IMAGE PROCESS, V12, P685, DOI 10.1109/TIP.2003.810588
   WU M, 1998, P IEEE INT C IM PROC
   Yeung MM, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P680, DOI 10.1109/ICIP.1997.638587
NR 36
TC 6
Z9 9
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 466
EP 474
DI 10.1109/TMM.2006.888005
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100003
DA 2024-07-18
ER

PT J
AU Wong, TT
   Leung, CS
   Heng, PA
   Wang, JQ
AF Wong, Tien-Tsin
   Leung, Chi-Sing
   Heng, Pheng-Ann
   Wang, Jianqing
TI Discrete wavelet transform on consumer-level graphics hardware
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE discrete wavelet transform; graphics processing unit; JPEG2000; shader
ID COMPRESSION
AB Discrete wavelet transform (DWT) has been heavily studied and developed in various scientific and engineering fields. Its multiresolution and locality nature facilitates applications requiring progressiveness and capturing high-frequency details. However, when dealing with enormous data volume, its performance may drastically reduce. On the other hand, with the recent advances in consumer-level graphics hardware, personal computers nowadays usually equip with a graphics processing unit (GPU) based graphics accelerator which offers SIMD-based parallel processing power. This paper presents a SIMD algorithm that performs the convolution-based DWT completely on a GPU, which brings us significant performance gain on a normal PC without extra cost. Although the for ward and inverse wavelet transforms are mathematically different, the proposed algorithm unifies them to an almost identical process that can be efficiently implemented on GPU. Different wavelet kernels and boundary extension schemes can be easily incorporated by simply modifying input parameters. To demonstrate its applicability and performance, we apply it to wavelet-based geometric design, stylized image processing, texture-illuminance decoupling, and JPEG2000 image encoding.
C1 Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong; City University of Hong Kong
RP Wong, TT (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM ttwong@acm.org; eeleungc@cityu.edu.hk; pheng@cse.cuhk.edu.hk
OI LEUNG, Chi Sing Andrew/0000-0003-0962-6723; Heng, Pheng
   Ann/0000-0003-3055-5034
CR ADAMS MD, 2000, P IEEE ICIP
   Andra K, 2002, IEEE T SIGNAL PROCES, V50, P966, DOI 10.1109/78.992147
   [Anonymous], ACM T GRAPHICS
   Chui C. K., 1992, An Introduction to Wavelets, DOI 10.2307/2153134
   FINKELSTEIN A, 1996, P 23 ANN C COMP GRAP
   Garcia A, 2005, VISUAL COMPUT, V21, P755, DOI 10.1007/s00371-005-0332-0
   HOPF M, 2000, P EG IEEE TCVG S VIS
   HUANG CT, 2003, P ICIP 2003
   KRUGER J, 2003, ACM T GRAPHICS
   Lam PM, 2004, SIGNAL PROCESS-IMAGE, V19, P741, DOI 10.1016/j.image.2004.04.007
   MARK W, 2003, ACM T GRAPHICS
   MORELAND K, 2003, P HWWS
   Oh BM, 2001, COMP GRAPH, P433
   PIEGL L, 1991, IEEE COMPUT GRAPH, V11, P55, DOI 10.1109/38.67702
   STOLLNITZ E.J., 1996, WAVELETS COMPUTER GR
   Strang G., 1996, Wavelets and Filter Banks
   Sweldens W, 1998, SIAM J MATH ANAL, V29, P511, DOI 10.1137/S0036141095289051
   Ten Daubechies I., 1992, lecture on wavelets
   Wang Z, 2004, COMPUT VIS IMAGE UND, V96, P327, DOI 10.1016/j.cviu.2004.03.017
   Wong TT, 2003, IEEE T CIRC SYST VID, V13, P1107, DOI 10.1109/TCSVT.2003.817628
   WONG TT, 2003, GRAPHICS PROGRAMMING, P375
   WONG TT, 2004, SHADER 103 ADV RENDE
NR 22
TC 63
Z9 72
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 668
EP 673
DI 10.1109/TMM.2006.887994
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100020
DA 2024-07-18
ER

PT J
AU Atzori, L
   Krunz, M
   Hassan, M
AF Atzori, Luigi
   Krunz, Marwan
   Hassan, Mohamed
TI Cycle-based rate control for one-way and interactive video
   communications over wireless channels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 12th ACM Multimedia Conference
CY OCT 10-16, 2004
CL New York, NY
SP ACM
DE adaptive forward error correction (FEC); channel-code optimization;
   playback buffer control; source rate control; wireless channels
ID BIT-RATE VIDEO; STATISTICS; SELECTION; INTERNET
AB We propose a joint source-rate/channel-code control scheme for streaming video over a wireless channel. The scheme is designed to maximize the achievable source rate while guaranteeing an upper bound on the probability of starvation at the playback buffer. It can be applied to both one-way and interactive video communications. Rate control is performed adaptively on a per-cycle basis, where a cycle consists of a "good" channel period and the ensuing "bad" period. This cycle-based approach has two advantages. First, it reduces the fluctuations in the source bit rate, ensuring smooth variations in video quality. Second, it makes it possible to derive simple expressions for the starvation probability at the playback buffer, which we use to determine the optimal source rate and channel code for the good and bad periods of the subsequent cycle.
C1 Univ Cagliari, Dept Elect & Elect Engn, Cagliari, Italy.
   Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA.
   Amer Univ Sharjah, Dept Elect Engn, Sharjah, U Arab Emirates.
C3 University of Cagliari; University of Arizona; American University of
   Sharjah
RP Atzori, L (corresponding author), Univ Cagliari, Dept Elect & Elect Engn, Cagliari, Italy.
EM krunz@ece.arizona.edu
RI Hassan, Mohamed/KJL-9008-2024
OI Atzori, Luigi/0000-0003-1350-3574; Hassan, Mohamed/0000-0001-6318-0748
CR Amaya C., 2003, P COST272 EUR UN FOR
   Aramvith S, 2002, IEEE T CIRC SYST VID, V12, P558, DOI 10.1109/TCSVT.2002.800326
   ATZORI L, 2006, CYCLE BASED RATE CON
   Cabrera J, 2002, IEEE T CIRC SYST VID, V12, P496, DOI 10.1109/TCSVT.2002.800306
   CABRERA J, 1999, IEEE T CIRCUITS SYST, V9, P172
   Chen GA, 2001, IEEE J SEL AREA COMM, V19, P132, DOI 10.1109/49.909615
   DENG RH, 1994, IEEE T COMMUN, V42, P2239, DOI 10.1109/26.293675
   HASE Y, 1991, IEEE T COMMUN, V39, P664, DOI 10.1109/26.87157
   Hassan M, 2004, IEEE T WIREL COMMUN, V3, P821, DOI 10.1109/TWC.2004.827729
   Hsu CY, 1997, IEEE J SEL AREA COMM, V15, P1016, DOI 10.1109/49.611156
   HU PC, 2000, P IEEE INT C IM PROC, V2, P124
   Krauss TP, 2000, IEEE T SIGNAL PROCES, V48, P2473, DOI 10.1109/78.863050
   Lee HJ, 2000, IEEE T CIRC SYST VID, V10, P878, DOI 10.1109/76.867926
   LEE WCY, 1974, IEEE T COMMUN, VCO22, P869, DOI 10.1109/TCOM.1974.1092290
   Ribas-Corbera J, 1999, IEEE T CIRC SYST VID, V9, P172, DOI 10.1109/76.744284
   Song HJ, 2001, IEEE T CIRC SYST VID, V11, P512, DOI 10.1109/76.915357
   Wiegand T, 1996, IEEE T CIRC SYST VID, V6, P182, DOI 10.1109/76.488825
   Wu DP, 2001, IEEE T CIRC SYST VID, V11, P282, DOI 10.1109/76.911156
   Zhang Q, 2005, P IEEE, V93, P123, DOI 10.1109/JPROC.2004.839603
NR 19
TC 5
Z9 6
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 176
EP 184
DI 10.1109/TMM.2006.886382
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU He, LW
   Zhang, ZY
AF He, Li-Wei
   Zhang, Zhengyou
TI Real-time whiteboard capture and processing using a video camera for
   remote collaboration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE image analysis; image classification; meetings; teleconferencing; video
   cameras; video recording; video signal processing; whiteboard capture
AB This paper describes our recently developed system which captures pen strokes on physical whiteboards in real time using an off-the-shelf video camera. Unlike many existing tools, our system does not instrument the pens or the whiteboard. It analyzes the sequence of captured video images in real time, classifies the pixels into whiteboard background, pen strokes and foreground objects (e.g., people in front of the whiteboard), extracts newly written pen strokes, and corrects the color to make the whiteboard completely white. This allows us to transmit whiteboard contents using very low bandwidth to remote meeting participants. Combined with other teleconferencing tools such as voice conference and application sharing, our system becomes a powerful tool to share ideas during online meetings.
C1 Microsoft Res, Redmond, WA 98052 USA.
C3 Microsoft
RP He, LW (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
RI Zhang, Zhang/JAX-2097-2023; zhang, zheng/HCH-9684-2022
CR ABOWD GD, 1998, P CHI 98, P440
   CHIU P, P ACM MULT 99 NEW YO, P149
   Duda R. O., 2000, PATTERN CLASSIFICATI
   HE L, P ICASSP 2003, V5, P776
   HE L, P ICASSP 2005, V2, P1113
   Ju SX, 1998, IEEE T CIRC SYST VID, V8, P686, DOI 10.1109/76.718513
   MALVAR HS, P ICASSP 2004, V3, P485
   MORAN TP, 1997, P CHI 97, P202, DOI DOI 10.1145/258549.258704
   Rousseeuw P.J., 1987, ROBUST REGRESSION OU
   Saund E., 1999, IMAGE MOSAICING DIAG
   Stifelman L.J., 1993, P INTERCHI AMST, P179
   WEBER K, P CHI 94, P58
   WHITTAKER S, P CHI 94, P271
   WILCOX LD, P CHI 97, P186
   WOLF C, P CSCW 92, P322
   [No title captured]
NR 16
TC 19
Z9 26
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 198
EP 206
DI 10.1109/TMM.2006.886385
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500019
DA 2024-07-18
ER

PT J
AU Kankanhalli, MS
   Wang, J
   Jain, R
AF Kankanhalli, Mohan S.
   Wang, Jun
   Jain, Ramesh
TI Experiential sampling in multimedia systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE dynamical systems; experiential computing; experiential sampling;
   sampling; visual attention
ID ATTENTION
AB Multimedia systems must deal with multiple data streams. Each data stream usually contains significant volume of redundant noisy data. In many real-time applications, it is essential to focus the computing resources on a relevant subset of data streams at any given time instant and use it to build the model of the environment. We formulate this problem as an experiential sampling problem and propose an approach to utilize computing resources efficiently on the most informative subset of data streams. First, in this paper, we focus on theoretical background and develop a theoretical framework for a single data stream. We generalize the notion of static visual attention in a dynamical systems setting and propose a dynamical attention-orientated analysis method. This is achieved by a sampling representation that utilizes the current context and past experience for attention evolution. Hence, the multimedia analysis task at hand can select its data of interest while immediately discarding the irrelevant data to achieve efficiency and adaptability.
C1 Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
   Delft Univ Technol, Fac Elect Engn Math & Comp Sci, NL-2628 CD Delft, Netherlands.
   Univ Calif Irvine, Donald Bren Sch Informat & Comp Sci, Irvine, CA 92697 USA.
C3 National University of Singapore; Delft University of Technology;
   University of California System; University of California Irvine
RP Kankanhalli, MS (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
EM mohan@comp.nus.edu.sg; jun.wang@tudelft.nl; jain@ics.uci.edu
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
CR [Anonymous], 200101 CRL
   Carpenter J., 1999, BUILDING ROBUST SIMU
   CHUNG D, 2002, P 2 WORKSH BIOL MOT
   Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038
   Ghahramani Z., 1996, PARAMETER ESTIMATION
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Itti L, 2001, J ELECTRON IMAGING, V10, P161, DOI 10.1117/1.1333677
   IYENGAR G, 2003, P ICASSP
   Jain R, 2003, COMMUN ACM, V46, P48, DOI 10.1145/792704.792729
   JAIN R, 2003, INT C MULT MED MOD T
   Kankanhalli MS, 2006, IEEE T MULTIMEDIA, V8, P947, DOI 10.1109/TMM.2006.879875
   Li D., 2003, P ACM INT C MULT BER
   LI SZ, 2002, P 7 EUR C COMP VIS C
   Lieberman H, 2000, IBM SYST J, V39, P617, DOI 10.1147/sj.393.0617
   Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847
   MacKay D., 2003, INFORM THEORY INFERE
   Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227
   NAVALPAKKAM V, 2002, P 2 WORKSH BIOL MOT, P453
   Neisser U., 1976, Cognition and reality
   NETI C, 2002, P RIAO COMP ASS INF
   Richards W., 1996, Perception as Bayesian Inference, P63
   Scholl BJ, 2001, COGNITION, V80, P1, DOI 10.1016/S0010-0277(00)00152-9
   Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951
   Walther D, 2002, LECT NOTES COMPUT SC, V2525, P472
   WANG J, 2002, THESIS NATL U SINGAP
   WANG J, 2003, P 28 INT C AC SPEECH
NR 26
TC 20
Z9 22
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 937
EP 946
DI 10.1109/TMM.2006.879876
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400006
OA Green Published
DA 2024-07-18
ER

PT J
AU Fei, ZM
   Yang, MK
AF Fei, Zongming
   Yang, Mengkun
TI A segmentation-based fine-grained peer sharing technique for delivering
   large media files in content distribution networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE nultimedia streaming; peer-to-peer; video segmentation
AB Delivering large media files over the Internet is a challenging task because it has some unique features that are different from delivering conventional web documents. In this paper, we propose a fine-grained peer sharing technique for dealing with the problem in the context of content distribution networks. The key difference of the technique from conventional peer-to-peer systems is that the unit of peer sharing is not a complete media file, but at a finer granularity. By doing so, we improve the flexibility of replica servers for handling client requests. We analyze the storage requirement. at replica servers and design a scheduling algorithm to coordinate the delivery process from multiple replica servers to a client. Our simulations show that the fine-grained peer sharing approach can reduce the initial latency of clients and the rejection rate of the system significantly over a simple peer sharing method.
C1 Univ Kentucky, Dept Comp Sci, Lexington, KY 40506 USA.
C3 University of Kentucky
RP Fei, ZM (corresponding author), Univ Kentucky, Dept Comp Sci, Lexington, KY 40506 USA.
EM fei@cs.uky.edu; myang0@cs.uky.edu
CR DRAPEAU AL, 1994, P 21 INT S COMP ARCH
   GAO L, 1998, P NOSSDAV 98
   KANGASHARJU J, 2001, P 6 INT WORKSH WEB C, P39
   KIM T, 2001, P NOSSDAV 01
   Kleinrock L., 1975, Queueing Systems-Volume 1: Theory, V1
   LUPARELLO D, 2001, P 6 INT WORKSH WEB C
   MILLS DL, 1969, RFC 1305
   RAMESH S, 2001, P IEEE INFOCOM 01
   SANTOS JR, 2000, P ACM SIGM SANT CLAR
   YANG M, 2003, P 8 INT WORKSH WEB C
   YANG M, 2004, P IEEE INT C COMM IC
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
NR 12
TC 3
Z9 5
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 821
EP 829
DI 10.1109/TMM.2006.876278
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300015
DA 2024-07-18
ER

PT J
AU Chou, PA
   Miao, ZR
AF Chou, PA
   Miao, ZR
TI Rate-distortion optimized streaming of packetized media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Review
DE audio coding; channel coding; error correction; Internet; Markov
   processes; multimedia communication; optimal control; protocols; video
   coding
ID OPTIMAL BIT ALLOCATION; GRACEFUL DEGRADATION; VIDEO TRANSMISSION;
   ERASURE CHANNELS; INTERNET; MULTICAST
AB This paper addresses the problem of streaming packetized media over a lossy packet network in a rate-distortion optimized way. We show that although the data units in a media presentation generally depend on each other according to a directed acyclic graph, the problem of rate-distortion optimized streaming of an entire presentation can be reduced to the problem of error-cost optimized transmission of an isolated data unit. We show how to solve the latter problem in a variety of scenarios, including the important common scenario of sender-driven streaming with feedback over a best-effort network, which we couch in the framework of Markov decision processes. We derive a fast practical algorithm for nearly optimal streaming in this scenario, and we derive a general purpose iterative descent algorithm for locally optimal streaming in arbitrary scenarios. Experimental results show that systems based on our algorithms have steady-state gains of 2-6 dB or more over systems that are not rate-distortion optimized. Furthermore, our systems essentially achieve the best possible performance: the operational distortion-rate function of the source at the capacity of the packet erasure channel.
C1 Microsoft Corp, Redmond, WA 98052 USA.
   Sony Corp, Santa Clara, CA 95054 USA.
C3 Microsoft; Sony Corporation
RP Microsoft Corp, Redmond, WA 98052 USA.
EM pachou@microsoft.com; zmiao@ieee.org
CR Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   BEGEN AC, 2003, P ICIP BARC CAT SPAI
   BOLOT J, 1999, P IEEE INF NEW YORK
   BOLOT JC, CASE FEC BASED ERROR
   Boyce JM, 1999, SIGNAL PROCESS-IMAGE, V15, P7, DOI 10.1016/S0923-5965(99)00021-1
   BUSSE I, 1995, 1 INT WORKSH HIGH SP
   Carle G, 1997, IEEE NETWORK, V11, P24, DOI 10.1109/65.642357
   Chakareski J, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P49
   Chakareski J, 2002, CONF REC ASILOMAR C, P1310
   Chakareski J, 2003, IEEE DATA COMPR CONF, P203
   Chakareski J, 2002, INT CONF ACOUST SPEE, P2513
   Chakareski J, 2002, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.2002.999943
   CHAKARESKI J, 2003, P ICIP BARC CAT SPAI
   CHAKARESKI J, 2003, P 11 ACM INT C MULT, P422
   CHAKARESKI J, 2002, MSRTR200281 MICR RES
   CHAKARESKI J, 2004, P SPIE VIS COMM IM P
   CHAKARESKI J, UNPUB IEEE ACM T NET
   CHAKARESKI J, 2002, IEEE T COMMUN, V8, P1675
   Chande V, 2000, IEEE J SEL AREA COMM, V18, P850, DOI 10.1109/49.848239
   CHANDE V, 1998, P IEEE INF THEOR WOR
   CHANG CL, 2004, P SPIE VIS COMM IM P
   CHAWLA K, 2000, P IEEE ICME NEW YORK
   Chen JJ, 1997, IEEE J SEL AREA COMM, V15, P1002, DOI 10.1109/49.611155
   CHEUNG G, 2002, P IEEE ICME LAUS SWI, V2, P81
   CHOI JH, 1994, IEEE T IMAGE PROCESS, V3, P546, DOI 10.1109/83.334986
   Chou P. A., 2000, Proceedings DCC 2000. Data Compression Conference, P440, DOI 10.1109/DCC.2000.838184
   CHOU PA, 1989, IEEE T INFORM THEORY, V35, P299, DOI 10.1109/18.32124
   Chou PA, 2001, IEEE T MULTIMEDIA, V3, P108, DOI 10.1109/6046.909598
   Chou PA, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1221, DOI 10.1109/ICME.2000.870987
   CHOU PA, 2001, MSRTR200135 MICR RES
   Davis G., 1996, P SPIE C WAV APPL DI
   De Vito F, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P141, DOI 10.1109/ICME.2002.1035738
   DEMARTIN JC, 2001, P ICME TOK JAP AUG
   DEMARTIN JC, 2001, P ICASSP SALT LAK CI
   Dumitrescu S, 2004, IEEE T MULTIMEDIA, V6, P230, DOI 10.1109/TMM.2003.822793
   FLETCHER R., 1987, PRACTICAL METHODS OP
   Girod B, 2002, WIREL COMMUN MOB COM, V2, P573, DOI 10.1002/wcm.87
   GUNARWARDENA D, 2003, P EEE IFIP ACM INT W
   Horn U, 1999, SIGNAL PROCESS-IMAGE, V15, P77, DOI 10.1016/S0923-5965(99)00025-9
   Hsu CY, 1997, IEEE J SEL AREA COMM, V15, P1016, DOI 10.1109/49.611156
   HU PC, 2000, P IEEE INT C IM PROC, V2, P124
   *ITU T, 1998, SG16Q15 ITUT
   Jiang W., 1999, P SPIE VIS COMM IM P
   Kalman M, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P189, DOI 10.1109/ICIP.2002.1038937
   Kalman M, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P869, DOI 10.1109/ICME.2002.1035920
   KALMAN M, 2003, P ICIP BARC CAT SPAI
   LI S, 2000, M5742
   LI W, 1999, JTC1SC29WG11 ISOIEC
   LI X, 1997, P INT WORKSH NETW OP
   LU J, 1998, P ICIP CHIC IL OCT
   MEHROTRA S, 2000, THESIS STANFORD U ST
   Miao Z., 2002, P INT PACK VID WORKS
   MIAO Z, 2002, THESIS U SO CALIFORN
   Miao ZR, 2000, CONF REC ASILOMAR C, P1357, DOI 10.1109/ACSSC.2000.911213
   MIGUEL AC, 1999, P ICIP KOB JAP OCT
   Mohr AE, 2000, IEEE J SEL AREA COMM, V18, P819, DOI 10.1109/49.848236
   Mohr AE, 1999, IEEE DATA COMPR CONF, P92, DOI 10.1109/DCC.1999.755658
   MOHR AE, 2000, P ICIP VANC BC CAN S
   Mood A.M., 1974, Introduction to the theory of statistics
   MUKHERJEE A, 1994, INTERNETWORKING, V5, P163
   ORTEGA A, 1994, IEEE T IMAGE PROCESS, V3, P26, DOI 10.1109/83.265978
   PADMANABHAN V, 1999, SPIE RTAS WORKSH QOS
   PAPADOPOULOS C, 1996, P INT WORKSH NETW OP
   PODOLSKY M, 1998, UCBCSD981024
   PODOLSKY M, 1998, P IEEE INF SAN FRANC
   Podolsky MG, 2001, J VLSI SIG PROC SYST, V27, P81, DOI 10.1023/A:1008123631453
   Puri R., 1999, Proceedings of the 33rd Asilomar Confe. Signals, Systems, V1, P342
   QUAGLIA D, 2002, P IEEE INT C MULT EX, V2, P85
   Radha H, 1999, SIGNAL PROCESS-IMAGE, V15, P95, DOI 10.1016/S0923-5965(99)00026-0
   RAMANATHAN P, 2003, P ICIP BARC CAT SPAI
   REIBMAN AR, 2000, P IEEE INT C IM PROC, V2, P136
   RISKIN EA, 1991, IEEE T INFORM THEORY, V37, P400, DOI 10.1109/18.75264
   RODER M, 2004, P DAT COMPR C SNOWB
   ROSS SM, 1974, STOCHASTIC PROCESSES
   Ruf MJ, 1999, IEEE T IMAGE PROCESS, V8, P305, DOI 10.1109/83.748887
   Sachs DG, 2000, PROC SPIE, V3974, P300, DOI 10.1117/12.382963
   Sehgal A, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P857, DOI 10.1109/ICME.2002.1035917
   Sehgal A, 2002, INT CONF ACOUST SPEE, P1973
   SERVETTO SD, 1999, THESIS U ILLINOIS UR
   SERVETTO SD, 1997, P ICIP SANT BARB CA
   SHIN J, 2000, P IEEE INT C IM PROC, V3, P536
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Song H, 1999, SIGNAL PROCESS-IMAGE, V15, P127, DOI 10.1016/S0923-5965(99)00027-2
   STANKOVIC V, 2002, P ICIP ROCH NY SEP
   Stockhammer T, 2004, IEEE T MULTIMEDIA, V6, P268, DOI 10.1109/TMM.2003.822795
   STOCKHAMMER T, 2001, P EURASIP IEEE INT P
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   TIAN T, 2000, P IEEE INT C IM PROC, V3, P400
   TURNER DA, 2000, P IEEE ICME NEW YORK
   WANG H, 2003, P SPIE S EL IM SAN J
   Wu DP, 2000, IEEE T CIRC SYST VID, V10, P923, DOI 10.1109/76.867930
   Wu DP, 2000, P IEEE, V88, P1855, DOI 10.1109/5.899055
   YANG F, 2001, P INT C 3 GEN WIR SA
   Zhai F, 2005, IEEE T MULTIMEDIA, V7, P716, DOI 10.1109/TMM.2005.850989
   ZHAI F, 2003, P ICME BALT MD JUL
   ZHAI F, 2003, P ICIP BARC CAT SPAI
   ZHANG Q, 2001, P EURASIP IEEE INT P
   Zhang R, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P861, DOI 10.1109/ICME.2002.1035918
   Zhang R, 2001, CONF REC ASILOMAR C, P210, DOI 10.1109/ACSSC.2001.986907
   Zhou J., 2001, P IEEE INT C MULT EX
   ZHU W, 2001, P IEEE INT S CIRC SY
NR 101
TC 266
Z9 310
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 390
EP 404
DI 10.1109/TMM.2005.864313
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300018
DA 2024-07-18
ER

PT J
AU Duan, LY
   Jin, JS
   Tian, Q
   Xu, CS
AF Duan, LY
   Jin, JS
   Tian, Q
   Xu, CS
TI Nonparametric motion characterization for robust classification of
   camera motion patterns
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE camera motion; nonparametric motion analysis; video databases; video
   indexing
ID MEAN SHIFT; VIDEO; ANNOTATION; RETRIEVAL
AB Motion characterization plays a critical role in video indexing. An effective way of characterizing camera motion facilitates the video representation, indexing and retrieval tasks. This paper describes a novel nonparametric motion representation to achieve an effective and robust recognition of parts of the video in which camera is static, or panning, or tilting, or zooming, etc. This representation employs the mean shift filtering and the vector histograms to produce a compact description of a motion field. The basic idea is to perform spatio-temporal mode-seeking in the motion feature space and use the histograms-based spatial distributions of dominant motion modes to represent a motion field. Unlike most existing approaches, which focus on the estimation of a parametric motion model from a dense optical flow field (OFF) or a block matching-based motion vector field (MVF), the proposed method combines the motion representation and machine learning techniques (e.g., support vector machines) to perform camera motion analysis from the classification point of view. The main motivation lies in the impossibility of uniformly securing a proper parametric assumption in a wide range of video scenarios. The diverse camera shot sizes and frequent occurrences of bad OFF[MVF necessitates a learning mechanism, which can not only capture the domain-independent parametric constraints, but also acquire the domain-dependent knowledge to tolerate the influence of bad OFF/MVF. In order to improve performance, we can use this learning-based method to train enhanced classifiers aiming at a certain context (i.e., shot size, neighbor OFF/MVFs, and video genre). Other visual cues (e.g., dominant color) can also be incorporated for further motion analysis. Our main aim is to use a generic feature space analysis method to explore a flexible OFF/MVF representation in a nonparametric technique, which could be fed into a learning framework to robustly capture the global motion by incorporating the context information. Results on videos with various types of content (23 191 MVFs culled from MPEG-7 dataset, and 20 000 MVFs culled from broadcast tennis, soccer, and basketball videos) are reported to validate the proposed approach.
C1 Inst Infocomm Res, Singapore 119613, Singapore.
   Univ Newcastle, Sch Design Commun & Informat Technol, Callaghan, NSW 2308, Australia.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); University of Newcastle
RP Inst Infocomm Res, Singapore 119613, Singapore.
EM lingyu@i2r.a-star.edu.sg; Jesse.Jin@newcastle.edu.au;
   tian@i2r.a-star.edu.sg; xucs@i2r.a-star.edu.sg
RI xu, cj/HJZ-3488-2023; chen, yue/JXW-9556-2024
CR [Anonymous], IEEE COMPUT
   [Anonymous], 1995, PROC ICJAI, DOI DOI 10.1145/217279.215068
   [Anonymous], 2002, P 10 ACM INT C MULT
   [Anonymous], P ACM C MULT
   Bouthemy P, 1999, IEEE T CIRC SYST VID, V9, P1030, DOI 10.1109/76.795057
   CAELLI T, 1997, MACHINE LEARNING IMA, P189
   Chang SF, 1998, IEEE T CIRC SYST VID, V8, P602, DOI 10.1109/76.718507
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   DeMenthon D., 2003, P 11 ACM INT C MULTI, P508
   DIVARANRAN A, 2000, MPEG99M5030 ISOICE
   Duan L.Y., 2003, Proc. ACM Int. Conf. Multimedia, P33
   Duan LY, 2004, IEEE IMAGE PROC, P1597
   Duan LY, 2005, IEEE T MULTIMEDIA, V7, P1066, DOI 10.1109/TMM.2005.858395
   DUAN LY, 2004, P ACM MULT NEW YORK, P754
   DUAN LY, 2004, P ACM MULTIMEDIA NEW, P328
   Duric Z., 1995, CARTR778 U MAR
   Fablet R, 2002, IEEE T IMAGE PROCESS, V11, P393, DOI 10.1109/TIP.2002.999674
   HAMRAPUR A, 1997, P SOC PHOTO-OPT INS, P188
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427
   *I INF RES, 12R I INF RES
   Irani M, 1998, P IEEE, V86, P905, DOI 10.1109/5.664279
   Jain AK, 1999, MULTIMEDIA SYST, V7, P369, DOI 10.1007/s005300050139
   Jeannin S, 2001, IEEE T CIRC SYST VID, V11, P720, DOI 10.1109/76.927428
   Jin J. S., 2000, IEEE Transactions on Intelligent Transportation Systems, V1, P32, DOI 10.1109/6979.869019
   KEE KW, 1999, MPEG99M5400 ISOICE
   Kim JG, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1171, DOI 10.1109/ICME.2000.871569
   Kobla V, 2000, PROC SPIE, V3972, P332
   Lee S, 2002, INT CONF ACOUST SPEE, P3664
   Ma YF, 2003, EURASIP J APPL SIG P, V2003, P199, DOI 10.1155/S1110865703211021
   MAPHADE M, IBM TRECVID CONCEPT
   Mojsilovic A, 2000, IEEE T IMAGE PROCESS, V9, P38, DOI 10.1109/83.817597
   Mojsilovic A, 2002, IEEE T IMAGE PROCESS, V11, P1238, DOI 10.1109/TIP.2002.804260
   Ngo C.W., 2001, Proc. ACM Multimedia, P51, DOI DOI 10.1145/500141.500151
   Ngo CW, 2002, INT J COMPUT VISION, V50, P127, DOI 10.1023/A:1020341931699
   Patel NV, 1997, PATTERN RECOGN, V30, P583, DOI 10.1016/S0031-3203(96)00114-8
   PEKER KA, J VIS COMMUN IMAGE R, V14, P2003
   PENTLAND A, 1994, P SOC PHOTO-OPT INS, V2185, P34, DOI 10.1117/12.171786
   ROWE LA, ACM SIGMM RETREAT RE
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   Silverman, 2018, DENSITY ESTIMATION S, DOI 10.1201/9781315140919
   Sudhir G, 1996, J VIS COMMUN IMAGE R, V7, P354, DOI 10.1006/jvci.1996.0031
   Szummer M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P823, DOI 10.1109/ICIP.1996.560871
   Tan YP, 2000, IEEE T CIRC SYST VID, V10, P133, DOI 10.1109/76.825867
   Wactlar HD, 1996, COMPUTER, V29, P46, DOI 10.1109/2.493456
   Wang R, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL V, P21, DOI 10.1109/ISCAS.2000.857353
   Wolf W, 1996, INT CONF ACOUST SPEE, P1228, DOI 10.1109/ICASSP.1996.543588
   Xie LX, 2004, PATTERN RECOGN LETT, V25, P767, DOI 10.1016/j.patrec.2004.01.005
   Xiong W, 1998, COMPUT VIS IMAGE UND, V71, P166, DOI 10.1006/cviu.1998.0711
   YAO YS, 1995, CARTR790 U MAR
   Yu X., 2003, PROC 11 ACM INT C MU, P11
NR 53
TC 23
Z9 23
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 323
EP 340
DI 10.1109/TMM.2005.864344
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300013
DA 2024-07-18
ER

PT J
AU Briassouli, A
   Tsakalides, P
   Stouraitis, A
AF Briassouli, A
   Tsakalides, P
   Stouraitis, A
TI Hidden messages in heavy-tails: DCT-domain watermark detection using
   alpha-stable models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE alpha-stable distributions; discrete cosine transform; image
   watermarking; Neyman-Pearson detector; statistical modeling
ID PROTECTION; IMAGES; PERFORMANCE; NOISE
AB This paper addresses issues that arise in copyright protection systems of digital images, which employ blind watermark verification structures in the discrete cosine transform (DCT) domain. First, we observe that statistical distributions with heavy algebraic tails, such as the alpha-stable family, are in many cases more accurate modeling tools for the DCT coefficients of JPEG-analyzed images than families with exponential tails such as the generalized Gaussian. Motivated by our modeling results, we then design a new processor for blind watermark detection using the Cauchy member of the alpha-stable family. The Cauchy distribution is chosen because it is the only non-Gaussian symmetric alphastable distribution that exists in closed form and also because it leads to the design of a nearly optimum detector with robust detection performance. We analyze the performance of the new detector in terms of the associated probabilities of detection and false alarm and we compare it to the performance of the generalized Gaussian detector by performing experiments with various test images.
C1 Univ Illinois, Beckman Inst, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
   Univ Crete, Dept Comp Sci, GR-71110 Iraklion, Greece.
   FORTH, ICS, GR-71110 Iraklion, Greece.
   Univ Patras, Dept Elect & Comp Engn, GR-26110 Patras, Greece.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   University of Crete; Foundation for Research & Technology - Hellas
   (FORTH); University of Patras
RP Univ Illinois, Beckman Inst, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
EM briassou@vision.ai.uiuc.edu; tsakalid@ics.forth.gr; thanos@ee.upatras.gr
RI Tsakalides, Panagiotis/O-2063-2015
OI Tsakalides, Panagiotis/0000-0003-4918-603X; Briassouli,
   Alexia/0000-0002-0545-3215; Stouraitis, Athanasios/0000-0002-3696-4958
CR Achim A, 2001, IEEE T MED IMAGING, V20, P772, DOI 10.1109/42.938245
   ADLER R, 1998, GUIDE HEAVY TAILS ST
   AHUMADA AJ, 1992, P SPIE HUMAN VISION, V1666, P3665
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], 1997, Statistical analysis of extreme values: with applications to insurance, finance, hydrology and other fields [Book]
   Barni M, 1998, SIGNAL PROCESS, V66, P357, DOI 10.1016/S0165-1684(98)00015-2
   BIRNEY KA, 1995, IEEE T IMAGE PROCESS, V4, P186, DOI 10.1109/83.342184
   Bloom JA, 1999, P IEEE, V87, P1267, DOI 10.1109/5.771077
   Clarke R.J., 1985, TRANSFORM CODING IMA
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Cox IJ, 1999, P IEEE, V87, P1127, DOI 10.1109/5.771068
   EGGERS JJ, 2001, SIGNAL PROCESS, V81
   Georgiou PG, 1999, IEEE T MULTIMEDIA, V1, P291, DOI 10.1109/6046.784467
   Hartung F, 1999, P IEEE, V87, P1079, DOI 10.1109/5.771066
   HARTUNG F, 1999, P SPIE SEC WAT MULT
   Hernández JR, 2000, IEEE T IMAGE PROCESS, V9, P55, DOI 10.1109/83.817598
   Hernández JR, 1999, P IEEE, V87, P1142, DOI 10.1109/5.771069
   Moulin P, 2000, INT CONF ACOUST SPEE, P3630, DOI 10.1109/ICASSP.2000.860188
   Nelson M.A., 1992, The Data Compression Book
   Nolan JP, 1999, MAXIMUM LIKELIHOOD E
   Poor H. Vincent, 1994, An introduction to signal detection and estimation
   Proakis J. G., 1995, DIGITAL COMMUNICATIO
   REININGER RC, 1983, IEEE T COMMUN, V31, P835, DOI 10.1109/TCOM.1983.1095893
   SHAO M, 1993, USCSIPI231
   Solomon J. A., 1994, Proceedings DCC '94. Data Compression Conference (Cat. No.94TH0626-2), P361, DOI 10.1109/DCC.1994.305944
   Swanson MD, 1996, 1996 IEEE DIGITAL SIGNAL PROCESSING WORKSHOP, PROCEEDINGS, P37, DOI 10.1109/DSPWS.1996.555454
   Tsakalides P., 1995, THESIS U SO CALIFORN
   TSIHRINTZIS GA, 1995, IEEE T COMMUN, V43, P904, DOI 10.1109/26.380123
   TSIHRINTZIS GA, 1994, ADV STAT SIGNAL PROC
   Voyatzis G, 1999, P IEEE, V87, P1197, DOI 10.1109/5.771072
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   WILLIAMS JW, 1995, J GEN INTERN MED, V10, P7, DOI 10.1007/BF02599568
   Wolfgang RB, 1999, P IEEE, V87, P1108, DOI 10.1109/5.771067
   Zeng WJ, 1999, IEEE T IMAGE PROCESS, V8, P1534, DOI 10.1109/83.799882
NR 34
TC 96
Z9 104
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 700
EP 715
DI 10.1109/TMM.2005.850970
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, L
   Aghvami, AH
   Chambers, WG
AF Wang, L
   Aghvami, AH
   Chambers, WG
TI Design issues of uplink media access control (MAC) protocols for
   multimedia traffic over DS-CDMA systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE CDMA; media access control (MAC); multimedia
ID PACKET RADIO NETWORKS; MULTIPLE-ACCESS; INTEGRATED VOICE; POWER-CONTROL;
   TRANSMISSION; PERFORMANCE; SERVICES; MODELS; VIDEO
AB We propose an efficient uplink media access control (MAC) protocol for a variable spreading gain interference-limited wideband CDMA system. It can, with high spectral efficiency, support both real-time traffic like speech and video and also nonreal-time data traffic based on packet transmission. The schemes for power allocation, joint scheduling, and transmission rate adaptation for nonreal-time data traffic are designed as integrated parts of the MAC, working together to improve the system performance in terms of capacity and delay. With these associated resource management mechanisms, the performances of the MAC protocol with two different channel-allocation methods for real-time traffic are numerically compared. One is demanding channel allocation, and the other is reserve channel allocation, in which a certain bandwidth is reserved for concurrent real-time traffic.
C1 Kings Coll London, Ctr Telecommun Res, London WC2R 2LS, England.
C3 University of London; King's College London
RP Kings Coll London, Ctr Telecommun Res, London WC2R 2LS, England.
EM lin.wang@kcl.ac.uk; hamid.aghvami@kcl.ac.uk; bill.chambers@kcl.ac.uk
CR *3GPP, TSG RAN UE RAD TRANS
   *3GPP, TGS RAN SPREAD MOD F
   *3GPP TSG RAN, PHYS LAY PROC FDD TS
   *3GPP TSG RAN, QOS CONC ARCH TS 23
   Adachi F, 1998, IEEE COMMUN MAG, V36, P56, DOI 10.1109/35.714618
   Berggren F, 2001, IEEE J SEL AREA COMM, V19, P1860, DOI 10.1109/49.957302
   BRADY PT, 1969, AT&T TECH J, V48, P2445, DOI 10.1002/j.1538-7305.1969.tb01181.x
   Brand AE, 1996, IEEE J SEL AREA COMM, V14, P1698, DOI 10.1109/49.545692
   CHIHLIN I, 1995, P IEEE VEH TECHN C, P907
   Choi S, 1999, IEEE ACM T NETWORK, V7, P616, DOI 10.1109/90.803378
   Comaniciu C, 2000, IEEE J SEL AREA COMM, V18, P112, DOI 10.1109/49.821725
   CONAN J, 1984, IEEE T COMMUN, V32, P1050, DOI 10.1109/TCOM.1984.1096180
   *ETSI, 1997, SMG2 ETSI
   Fantacci R, 2000, IEEE J SEL AREA COMM, V18, P1441, DOI 10.1109/49.864009
   FITZEK FHP, 2000, MPEG 4 H 263 VIDEO T
   GARRETT MW, ACM SIGCOMM 1994, P269
   GERANIOTIS E, 1995, IEEE T COMMUN, V43, P1756, DOI 10.1109/26.380226
   GOLAUP A, 2002, P IEEE VTC FALL 2002
   GOODMAN DJ, 1989, IEEE T COMMUN, V37, P885, DOI 10.1109/26.31190
   HILL JR, 1995, IEEE T AUTOMAT CONTR, V40, P1305
   Izquierdo MR, 1999, MULTIMEDIA SYST, V7, P199, DOI 10.1007/s005300050122
   JANSEN MG, 1995, IEEE T VEH TECHNOL, V44, P67, DOI 10.1109/25.350271
   Jäntti R, 2001, IEEE COMMUN LETT, V5, P200, DOI 10.1109/4234.922759
   Lee SJ, 1999, IEEE T VEH TECHNOL, V48, P376, DOI 10.1109/25.752561
   Liu TK, 1998, IEEE J SEL AREA COMM, V16, P845, DOI 10.1109/49.709448
   MAGLARIS B, 1988, IEEE T COMMUN, V36, P834, DOI 10.1109/26.2812
   Manji S, 2000, IEEE T VEH TECHNOL, V49, P911, DOI 10.1109/25.845109
   Oh SJ, 1999, IEEE J SEL AREA COMM, V17, P918, DOI 10.1109/49.768205
   OHTA N, 1994, PACKET VIDEO MODELIN
   SABNANI KK, 1995, P IEEE ICC 95, P725
   Sampath A, 1997, IEEE J SEL AREA COMM, V15, P1511, DOI 10.1109/49.634790
   Tan LJ, 1996, IEEE J SEL AREA COMM, V14, P1717, DOI 10.1109/49.545694
   Viterbi A. J., 1995, CDMA PRINCIPLES SPRE
   WANG L, P IEEE GLOB 1999, P2778
   WILSON ND, 1993, IEEE J SEL AREA COMM, V11, P870, DOI 10.1109/49.232296
   YANG WB, 1994, IEEE J SEL AREA COMM, V12, P654, DOI 10.1109/49.286672
NR 36
TC 2
Z9 2
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 551
EP 562
DI 10.1109/TMM.2005.846786
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200017
DA 2024-07-18
ER

PT J
AU Zheng, JH
   Chau, LP
AF Zheng, JH
   Chau, LP
TI Efficient motion vector recovery algorithm for H.264 based on a
   polynomial model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error concealment; H.264
ID ERROR-CONCEALMENT TECHNIQUES; VIDEO TRANSMISSION
AB In this paper, we propose an efficient motion vector recovery algorithm for the new coding standard H.264, which is based on a polynomial model. To achieve better coding efficiency, the motion estimation scheme used in H.264 is different from previous coding standards. In H.264, a 16 x 16 macroblock can be divided into different block shapes for motion estimation. Each macroblock contains more motion vectors than previous coding standards. For nature video, the blocks within a small area likely belong to the same object, hence the motion vectors of neighboring blocks are highly correlated. Based on the correlation of neighboring motion vectors, we can use the motion vectors that are adjacent to the lost motion vectors to constitute a polynomial model, which can describe the change tendency of motion vectors within a small area. Through this model, the lost motion vectors can be predicted and the lost macroblocks can be reconstructed. Different video sequences are used to test the performance of proposed method. The simulation results show that the quality of corrupted video can be obviously improved by proposed algorithm.
C1 Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM lpchau@ieee.org
RI Chau, Lap-Pui/A-5149-2011; 金华, 郑/GWM-7529-2022
OI Chau, Lap-Pui/0000-0003-4932-0593; 
CR Bystrom M, 1999, IEEE T CIRC SYST VID, V9, P868, DOI 10.1109/76.785725
   Cen S, 2003, IEEE T MULTIMEDIA, V5, P1, DOI 10.1109/TMM.2003.808825
   Chen MJ, 1997, IEEE T CIRC SYST VID, V7, P560, DOI 10.1109/76.585936
   Han YH, 1998, IEEE T CIRC SYST VID, V8, P221, DOI 10.1109/76.664106
   Lee YS, 2003, IEEE T CIRC SYST VID, V13, P176, DOI 10.1109/TCSVT.2002.808430
   Li X, 2002, IEEE T CIRC SYST VID, V12, P857, DOI 10.1109/TCSVT.2002.804882
   Park JW, 1997, IEEE T CIRC SYST VID, V7, P845, DOI 10.1109/76.644064
   Park JW, 1999, IEEE T CIRC SYST VID, V9, P1003, DOI 10.1109/76.795052
   Shao YF, 2001, PROCEEDINGS OF 2001 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P535, DOI 10.1109/ISIMP.2001.925451
   Shirani S, 2000, IEEE J SEL AREA COMM, V18, P1122, DOI 10.1109/49.848261
   Shirer DL, 2000, COMPUT SCI ENG, V2, P3
   Suh JW, 1997, INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, 1997 DIGEST OF TECHNICAL PAPERS, P26
   Suh JW, 2002, IEEE T BROADCAST, V48, P299, DOI 10.1109/TBC.2002.806797
   Tsekeridou S, 2000, IEEE T CIRC SYST VID, V10, P646, DOI 10.1109/76.845010
   Turaga DS, 2002, IEEE T CIRC SYST VID, V12, P483, DOI 10.1109/TCSVT.2002.800318
   WANG Y, 1993, IEEE T COMMUN, V41, P1544, DOI 10.1109/26.237889
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wang YK, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P729
   Wang Z, 1998, IEEE T IMAGE PROCESS, V7, P1056, DOI 10.1109/83.701166
   WIEGAND T, 2002, 1449610 ISOIEC AVC
   Zhang Y, 2003, IEEE T IMAGE PROCESS, V12, P236, DOI 10.1109/TIP.2003.809003
   Zhu QF, 1993, IEEE T CIRC SYST VID, V3, P248, DOI 10.1109/76.224235
NR 22
TC 30
Z9 39
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 507
EP 513
DI 10.1109/TMM.2005.843343
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200013
DA 2024-07-18
ER

PT J
AU Pan, YX
   Cheng, I
   Basu, A
AF Pan, YX
   Cheng, I
   Basu, A
TI Quality metric for approximating subjective evaluation of 3-D objects
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D graphics; image quality; perceptual metric; subjective evaluation
AB Many factors, such as the number of vertices and the resolution of texture, can affect the display quality of three-dimensional (3-D) objects. When the resources of a graphics system are not sufficient to render the ideal image, degradation is inevitable. It is, therefore, important to study how individual factors will affect the overall quality, and how the degradation can be controlled given limited resources. In this paper, the essential factors determining the display quality are reviewed. We then integrate two important ones, resolution of texture and resolution of wireframe, and use them in our model as a perceptual metric. We assess this metric using statistical data collected from a 3-D quality evaluation experiment. The statistical model and the methodology to assess the display quality metric are discussed. A preliminary study of the reliability of the estimates is also described. The contribution of this paper lies in: 1) determining the relative importance of wireframe versus texture resolution in perceptual quality evaluation and 2) proposing an experimental strategy for verifying and fitting a quantitative model that estimates 3-D perceptual quality. The proposed quantitative method is found to fit closely to subjective ratings by human observers based on preliminary experimental results.
C1 Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
C3 University of Alberta
RP Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
EM irene@zoomage.com
CR [Anonymous], 1936, Psychometric methods
   Basu A, 1998, IEEE T SYST MAN CY A, V28, P137, DOI 10.1109/3468.661143
   BOLIN MR, 1998, P 25 ANN C COMP GRAP, P299
   CIGNONI P, 1997, COMPUTERS GRAPHICS, V22
   DEERING M, 1995, P 22 ANN C COMP GRAP, P13
   Ferwerda J. A., 1997, Proc. ACM SIGGRAPH, P143
   Haeberli Paul., 1993, Texture Mapping as a Fundamental Drawing Primitive
   HECKBERT PS, 1997, SURVEY POLYGONAL SUF
   LIMB JO, 1979, IEEE T SYST MAN CYB, V9, P778, DOI 10.1109/TSMC.1979.4310129
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   NAGATA S, 1984, P SID, V25, P239
   Nagata S., 1991, PICTORIAL COMMUNICAT, P527
   Rogowitz BE, 2001, P SOC PHOTO-OPT INS, V4299, P340, DOI 10.1117/12.429504
   Rushmeier H, 2000, P SOC PHOTO-OPT INS, V3959, P372, DOI 10.1117/12.387174
   Siegel M, 2000, IEEE T CIRC SYST VID, V10, P387, DOI 10.1109/76.836283
   STEIN CS, 1989, P SOC PHOTO-OPT INS, V1977, P198
   TEO PC, 1994, P SOC PHOTO-OPT INS, V2179, P127, DOI 10.1117/12.172664
   TORBORG J, 1996, P SIGGRAPH, P353
   VANDEN CJ, 1996, P SOC PHOTO-OPT INS, P450
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
   Watson B, 2001, COMP GRAPH, P213, DOI 10.1145/383259.383283
   WEBSTER AA, 1993, P SOC PHOTO-OPT INS, V1913, P15, DOI 10.1117/12.152700
   WNKLER S, 1999, P SPIE HUMAN VISION, P23
   Yu YZ, 2003, IEEE T MULTIMEDIA, V5, P466, DOI 10.1109/TMM.2003.814725
NR 24
TC 59
Z9 68
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 269
EP 279
DI 10.1109/TMM.2005.843364
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400009
DA 2024-07-18
ER

PT J
AU Ma, HD
   Shin, KG
AF Ma, HD
   Shin, KG
TI Checking consistency in multimedia synchronization constraints
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE consistency checking; graph; multimedia authoring; synchronization
   constraint
ID SPECIFICATION; OBJECTS; VERIFICATION; MODELS
AB Constraint-based design is often used to correctly author a multimedia scenario due to its flexibility and efficiency. However, such a system must provide a mechanism with which users can easily manipulate the underlying structures to meet the application requirements. This paper proposes a novel method for analyzing multimedia synchronization constraints based on the constraint graph and classification, which is essential in developing efficient system support tools for constraint-based authoring systems. We specify temporal and spatial relations between multimedia objects, and use a directed graph to represent the constraints among the objects in a multimedia scenario. Moreover, we develop a method for analyzing temporal and spatial synchronization constraints based on graph theory, solving the problems of completeness checking, consistency checking, constraints relaxation and automatic spatio-temporal layout generation in a unified theoretical framework. We also discuss the effects of user interactive authoring. Compared with other methods, the proposed approach is simpler, more efficient, and easier to implement.
C1 Beijing Univ Posts & Telecommun, Sch Comp Sci & Technol, Beijing 100876, Peoples R China.
   Univ Michigan, Dept Elect Engn & Comp Sci, Real Time Comp Lab, Ann Arbor, MI 48109 USA.
C3 Beijing University of Posts & Telecommunications; University of Michigan
   System; University of Michigan
RP Beijing Univ Posts & Telecommun, Sch Comp Sci & Technol, Beijing 100876, Peoples R China.
EM mhd@bupt.edu.cn; kgshin@eecs.umich.edu
CR ALLEN JF, 1983, COMMUN ACM, V26, P832, DOI 10.1145/182.358434
   Ates AF, 1996, IEEE J SEL AREA COMM, V14, P126, DOI 10.1109/49.481699
   Bertino E, 1998, IEEE T KNOWL DATA EN, V10, P612, DOI 10.1109/69.706060
   Blakowski G, 1996, IEEE J SEL AREA COMM, V14, P5, DOI 10.1109/49.481691
   Buchanan M. C., 1992, Proceeding of the ACM Conference on Hypertext, P262, DOI 10.1145/168466.171513
   Cormen T.H., 1990, Introduction to Algorithms
   Courtiat J.-P., 1996, Proceedings ACM Multimedia 96, P141, DOI 10.1145/244130.244178
   DAY YF, 1995, PROC INT CONF DATA, P401, DOI 10.1109/ICDE.1995.380357
   HOEPNER P, 1992, COMPUT COMMUN, V15, P557, DOI 10.1016/0140-3664(92)90053-H
   Kwon YM, 1999, DATA KNOWL ENG, V30, P217, DOI 10.1016/S0169-023X(99)00012-9
   LITTLE TDC, 1990, IEEE J SEL AREA COMM, V8, P413, DOI 10.1109/49.53017
   Ma HD, 2003, J COMPUT SCI TECH-CH, V18, P172, DOI 10.1007/BF02948882
   Ma Hua-Dong, 1998, Journal of Software, V9, P889
   Ma Huadong, 1999, Journal of Computer Science and Technology (English Language Edition), V14, P188, DOI 10.1007/BF02946527
   Qazi N. U., 1993, Proceedings ACM Multimedia 93, P147, DOI 10.1145/166266.166283
   Song JW, 1999, MULTIMEDIA SYST, V7, P424, DOI 10.1007/s005300050143
   [No title captured]
   [No title captured]
   [No title captured]
NR 19
TC 6
Z9 6
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2004
VL 6
IS 4
BP 565
EP 574
DI 10.1109/tmm.2004.830807
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 838RC
UT WOS:000222729800005
DA 2024-07-18
ER

PT J
AU Zotkin, DN
   Duraiswami, R
   Davis, LS
AF Zotkin, DN
   Duraiswami, R
   Davis, LS
TI Rendering localized spatial audio in a virtual auditory space
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio user interfaces; head-related transfer function; spatial audio;
   3-D audio processing; user interfaces; virtual auditory spaces; virtual
   environments; virtual reality
ID POLE-ZERO APPROXIMATIONS; EAR TRANSFER-FUNCTIONS; SOUND; HEAD; MODEL;
   CUES; PINNA; REFLECTIONS; PLANE
AB High-quality virtual audio scene rendering is required for emerging virtual and augmented reality applications, perceptual user interfaces, and sonification of data. We describe algorithms for creation of virtual auditory spaces by rendering cues that arise from anatomical scattering, environmental scattering, and dynamical effects. We use a novel way of personalizing the head related transfer functions (HRTFs) from a database, based on anatomical measurements. Details of algorithms for HRTF interpolation, room impulse response creation, HRTF selection from a database, and audio scene presentation are presented. Our system runs in real time on an office PC without specialized DSP hardware.
C1 Univ Maryland, Inst Adv Comp Studies, Perceptual Interfaces & Reality Lab, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park
RP Univ Maryland, Inst Adv Comp Studies, Perceptual Interfaces & Reality Lab, College Pk, MD 20742 USA.
EM dz@umiacs.umd.edu; ramani@umiacs.umd.edu; lsd@umiacs.umd.edu
RI Duraiswami, Ramani/J-6070-2012
OI Duraiswami, Ramani/0000-0002-5596-8460
CR Algazi VR, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P99, DOI 10.1109/ASPAA.2001.969552
   Algazi VR, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P103, DOI 10.1109/ASPAA.2001.969553
   ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599
   [Anonymous], 1962, J. Audio Eng. Soc.
   BATTEAU DW, 1967, PROC R SOC SER B-BIO, V168, P158, DOI 10.1098/rspb.1967.0058
   Begault DR, 2001, J AUDIO ENG SOC, V49, P904
   Bellik Y, 1997, 1997 IEEE FIRST WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P31, DOI 10.1109/MMSP.1997.602609
   Blauert J., 1997, SPATIAL HEARING, DOI [10.7551/mitpress/6391.001.0001, DOI 10.7551/MITPRESS/6391.001.0001]
   Blommer MA, 1997, IEEE T SPEECH AUDI P, V5, P278, DOI 10.1109/89.568734
   BLY S, 1994, SFI S SCI C, V18, P405
   BORISH J, 1984, J ACOUST SOC AM, V75, P1827, DOI 10.1121/1.390983
   Bregman AS., 1994, AUDITORY SCENE ANAL
   Brewster S. A., 1998, ACM Transactions on Computer-Human Interaction, V5, P224, DOI 10.1145/292834.292839
   Brown CP, 1998, IEEE T SPEECH AUDI P, V6, P476, DOI 10.1109/89.709673
   BRUNGART DS, P ICAD 1996
   Carlile S, 1998, P ANN INT IEEE EMBS, V20, P1090, DOI 10.1109/IEMBS.1998.747061
   Carlile S., 2000, P 1 IEEE PACIFIC RIM, P220
   Carlile S., 1996, Virtual auditory space: Generation and applications
   CASEY M, 1995, P 99 AES CONV NEW YO, P1
   CHEN JS, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P188, DOI 10.1109/VRAIS.1993.380779
   DIXON NF, 1980, PERCEPTION, V9, P719, DOI 10.1068/p090719
   Duda RO, 1998, J ACOUST SOC AM, V104, P3048, DOI 10.1121/1.423886
   DUDA RO, 1993, P 27 AS C SIGN SYST, P457
   Duraiswami R, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P203, DOI 10.1109/ASPAA.2001.969578
   DURAISWAMI R, P 140 M ASA
   Durant EA, 2002, IEEE T SPEECH AUDI P, V10, P18, DOI 10.1109/89.979382
   FREY AR, 1982, FUNDAMENTALS ACOUSTI, P313
   Funkhouser T, 1999, COMP GRAPH, P365, DOI 10.1145/311535.311590
   GARDNER MB, 1973, J ACOUST SOC AM, V53, P400, DOI 10.1121/1.1913336
   GARDNER WG, 1995, J AUDIO ENG SOC, V43, P127
   HAN HL, 1994, J AUDIO ENG SOC, V42, P15
   Hartmann WH, 1999, PHYS TODAY, V52, P24, DOI 10.1063/1.882727
   Jin C., 2000, P IEEE 2000 INT S MU, P235
   Jin CT, 2000, ADV NEUR IN, V12, P768
   Jot J.-M., 1997, P ICMC, P236
   Jot JM, 1999, MULTIMEDIA SYST, V7, P55, DOI 10.1007/s005300050111
   KEIDEL WD, 1975, HDB SENSORY PHYSL, P247
   Kramer Gregory, 2010, Sonification report: Status of the field and research agenda
   Kulkarni A, 1999, J ACOUST SOC AM, V105, P2821, DOI 10.1121/1.426898
   Kulkarni A., 1995, J ACOUST SOC AM, V97, P3278, DOI [10.1121/1.411579, DOI 10.1121/1.411579]
   Kyriakakis C, 1998, P IEEE, V86, P941, DOI 10.1109/5.664281
   Loomis JM, 1998, PRESENCE-TELEOP VIRT, V7, P193, DOI 10.1162/105474698565677
   LopezPoveda EA, 1996, J ACOUST SOC AM, V100, P3248, DOI 10.1121/1.417208
   Mcanally KI, 2002, J AUDIO ENG SOC, V50, P263
   McKinley R.L., 1997, binaural and spatial hearing in real and virtual environments, P683
   Middlebrooks JC, 2000, J ACOUST SOC AM, V108, P3088, DOI 10.1121/1.1322026
   Middlebrooks JC, 1999, J ACOUST SOC AM, V106, P1480, DOI 10.1121/1.427176
   Middlebrooks JC, 1999, J ACOUST SOC AM, V106, P1493, DOI 10.1121/1.427147
   MOLLER H, 1992, APPL ACOUST, V36, P171, DOI 10.1016/0003-682X(92)90046-U
   Morimoto M., 1980, Journal of the Acoustical Society of Japan (E), V1, P167, DOI 10.1250/ast.1.167
   Perrett S, 1997, J ACOUST SOC AM, V102, P2325, DOI 10.1121/1.419642
   RAKERD B, 1985, J ACOUST SOC AM, V78, P524, DOI 10.1121/1.392474
   RUNKLE P, 2000, P ICAD 2000 ATL GA
   SCHOUKENS J, 1990, IEEE T INSTRUM MEAS, V39, P905, DOI 10.1109/19.65795
   Shaw E., 1997, BINAURAL SPATIAL HEA, P25
   SHIMADA S, 1994, J AUDIO ENG SOC, V42, P577
   Shinn-Cunningham B. G., 2001, P ACM SIGGRAPH EUR C
   Shinn-Cunningham B.G., 2000, IEEE Pacific-Rim Conference on Multimedia, P227, DOI 10.1.1.73.2560
   Shinn-Cunningham BG, 2000, J ACOUST SOC AM, V107, P1627, DOI 10.1121/1.428447
   Slaney M., 1998, Computational auditory scene analysis, P27
   Strutt J.W., 1907, Philosophical Magazine, V13, P214, DOI DOI 10.1080/14786440709463595
   TSINGOS N, 2001, P ICAD 2001 ESP FINL, P38
   Wallach H, 1940, J EXP PSYCHOL, V27, P339, DOI 10.1037/h0054629
   Wenzel E., 1999, P AES 16 INT C SPATI, P42
   WENZEL EM, 1993, J ACOUST SOC AM, V94, P111, DOI 10.1121/1.407089
   WENZEL EM, 2000, P ICAD 2000 ATL GA, P151
   WENZEL EM, 1998, P 16 INT C AC ICA SE, P2405
   Wightman FL, 1999, J ACOUST SOC AM, V105, P2841, DOI 10.1121/1.426899
   Woodworth R.S., 1962, EXP PSYCHOL, P349
   WRIGHT D, 1974, J ACOUST SOC AM, V56, P957, DOI 10.1121/1.1903355
NR 70
TC 102
Z9 137
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2004
VL 6
IS 4
BP 553
EP 564
DI 10.1109/tmm.2004.827516
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 838RC
UT WOS:000222729800004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ramkumar, M
   Akansu, AN
AF Ramkumar, M
   Akansu, AN
TI A robust protocol for proving ownership of multimedia content
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE counterfeit attacks; data hiding; watermarking
ID IMAGE WATERMARKING
AB We explore the problem of proving ownership or origin of multimedia content like image/video or audio signals through watermarking. The need for watermarking arises out of the insufficiency of present copyright laws for claiming ownership of digital content. Watermarking schemes however, are threatened by counterfeit attacks, which primarily use the freedom available in choice of signature or choice of the watermarking method. A restrictive protocol for watermarking could go a long way in rendering counterfeit attacks extremely difficult. We suggest a comprehensive protocol as an extension of the one suggested by Craver et al. [1], that makes it possible for the true owner to claim ownership unambiguously, while making it extremely difficult for a pirate to do so.
C1 Mississippi State Univ, Dept Comp Sci & Engn, Mississippi State, MS 39762 USA.
   New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
C3 Mississippi State University; New Jersey Institute of Technology
RP Mississippi State Univ, Dept Comp Sci & Engn, Mississippi State, MS 39762 USA.
EM ramkumar@cse.msstate.edu; ali@njit.edu
CR [Anonymous], Probability, Random Variables and Stochastic Processes
   Chen B, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P273, DOI 10.1109/MMSP.1998.738946
   Coltuc D, 1999, P SOC PHOTO-OPT INS, V3657, P252, DOI 10.1117/12.344674
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Craver S, 1997, P SOC PHOTO-OPT INS, V3022, P310, DOI 10.1117/12.263419
   INOUE H, 1998, P IEEE ICIP, V3, P391
   Kutter M, 1999, PROC SPIE, V3657, P226, DOI 10.1117/12.344672
   MEMON N, 1998, IEEE 2 WORKSH MULT S, P273
   Ozer IB, 2000, INT CONF ACOUST SPEE, P1963, DOI 10.1109/ICASSP.2000.859215
   Qiao LT, 1998, J VIS COMMUN IMAGE R, V9, P194, DOI 10.1006/jvci.1998.0391
   RAMKUMAR M, 2000, IEEE INT C INF TECHN, P22
   RAMKUMAR M, 1999, P CONT SEC DAT HID D
   RAMKUMAR M, 1998, P SPIE MULT SYST APP, V3528, P474
   RAMKUMAR M, 1999, IEEE INT C CIRC SYST, V3, P520
   RAMKUMAR M, 2000, IEEE INT C COMM NEW
   RAMKUMAR M, 1999, 33 AS C SIGN SYST CO, V2, P1528
   RAMKUMAR M, 2000, P IEEE ICIP, V4, P1231
   RAMKUMAR M, 1999, SPIE INT WORKSH VOIC, V3845
   Rongen PMJ, 1999, PROC SPIE, V3657, P273, DOI 10.1117/12.344676
   Ruanaidh JJKO, 1998, SIGNAL PROCESS, V66, P303, DOI 10.1016/S0165-1684(98)00012-7
   Schneier B., 2015, APPL CRYPTOGRAPHY, VSecond
   Wolfgang RB, 1998, P SOC PHOTO-OPT INS, V3228, P297, DOI 10.1117/12.300900
   Wu M, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P437, DOI 10.1109/ICIP.1998.723413
   YU HH, 1999, SPIE INT S VOIC VID, V3845
   ZENG W, 1997, P IEEE INT C IM PROC, V1, P552
NR 25
TC 15
Z9 16
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 469
EP 478
DI 10.1109/TMM.2004.827494
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200010
DA 2024-07-18
ER

PT J
AU Izquierdo, E
AF Izquierdo, E
TI Efficient and accurate image based camera registration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE camera calibration; disparity estimation; epipolar geometry
ID AUGMENTED REALITY; ALGORITHM; OBJECTS; SCENE
AB A technique for efficient and accurate camera registration based on stereo image analysis is presented. Initially, few correspondences are estimated with high accuracy using a probabilistic relaxation technique. Accuracy is achieved by considering the continuous approximations of selected image areas using second order polynomials and a relaxation rule defined according to the likelihood that estimates obey stereoscopic constraints. The extrinsic camera parameters are then obtained using a novel efficient and robust approach derived from the classic eight point algorithm. Efficiency is achieved by solving a parametric linear optimization problem rather than a nonlinear one as more conventional methods attempt to do. Robustness is obtained by applying two novel strategies: normalization of the initial data via a simple but efficient diagonal scaling approach, and regularization of the underlying linear parametric optimization problem using meaningful constraints. The performance of the presented methods is assessed in several computer experiments using natural video data.
C1 Queen Mary Univ London, Dept Elect Engn, London E1 4NS, England.
C3 University of London; Queen Mary University London
RP Queen Mary Univ London, Dept Elect Engn, London E1 4NS, England.
EM ebroul.izquierdo@elec.qmul.ac.uk
CR BAJURA M, 1995, IEEE COMPUT GRAPH, V15, P52, DOI 10.1109/38.403828
   Faugeras O. D., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P15
   Faugeras O. D., 1989, Proceedings. Workshop on Visual Motion (IEEE Cat. No.89CH2716-9), P248, DOI 10.1109/WVM.1989.47116
   Golub G.H., 1997, MATRIX COMPUTATION, V3rd
   GRIMSON WEL, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P430, DOI 10.1109/CVPR.1994.323862
   Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Izquierdo E, 1998, COMPUT VIS IMAGE UND, V71, P231, DOI 10.1006/cviu.1998.0706
   Izquierdo E, 1997, IEEE T CIRC SYST VID, V7, P629, DOI 10.1109/76.611174
   Izquierdo E, 1999, IEEE T CIRC SYST VID, V9, P336, DOI 10.1109/76.752100
   KRUPPA E, SITZ BER AKAD WISS W, V122, P1939
   Kutulakos KN, 1996, P IEEE VIRT REAL ANN, P25, DOI 10.1109/VRAIS.1996.490507
   LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0
   Mellor J.P., 1995, P COMP VIS VIRT REAL, P471
   NEUMANN U, 1996, P ACM VIRT REAL SOFT, P109
   Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215
   SHARMA R, 1994, P SPIE TEL TEL TECHN, V2351, P220
   STURM R, MATH ANN, V1, P533
   Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471
   TUCERYAN M, 1995, IEEE T VIS COMPUT GR, V1, P255, DOI 10.1109/2945.466720
   UENOHARA M, 1995, P COMP VIS VIRT REAL, P13
   Willson Reg., 1995, TSAI CAMERA CALIBRAT
   Xu G., 1996, EPIPOLAR GEOMETRY ST, DOI DOI 10.1007/978
   ZHANG Z, 1994, 2273 INRIA
NR 26
TC 6
Z9 6
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 293
EP 302
DI 10.1109/TMM.2003.814910
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500002
DA 2024-07-18
ER

PT J
AU Chen, JX
   Fan, JY
   Ye, HC
   Li, J
   Liao, YB
   Chen, T
AF Chen, Jiaxiang
   Fan, Jiayuan
   Ye, Hancheng
   Li, Jie
   Liao, Yongbing
   Chen, Tao
TI Exploring Kernel-Based Texture Transfer for Pose-Guided Person Image
   Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Image synthesis; Clothing; Uncertainty; Shape; Optical flow;
   Task analysis; Human pose transfer; person image synthesis; texture
   distribution; texture-fusion
AB Pose-guided person image generation that aims to transfer the pose of a given person to a target pose has recently received lots of research attention. Due to the spatial misalignment and occlusions of different local body parts by pose variations, this task is still challenging especially in maintaining high-fidelity textures and body structures in generated images. Besides, most works also suffer from the limited number of texture styles in the given person datasets, restricting the diversity of generated persons' appearances. To solve these problems, we design a Kernel-based Texture-Fusion Joint Refinement Network (TFJR-Net) to jointly refine the structure and texture information of generated images. First, we leverage a bone-map representation to guide the generation of human parsing maps, which has more structure priors and richer context information than traditional key-point maps, thus reduce the uncertainty of generated body structures. Next, a Texture-Kernel Injection Normalization module (TKIN) is proposed to inject the per-region texture-kernel into the corresponding semantic region from the human parsing map, which decouples the texture and shape information, and also preserves fine-grained features for complex textures. Furthermore, we are the first to introduce external texture patterns outside of the dataset in human semantic regions such as the upper clothes. We fuse the two texture domains in a shared texture space through our designed texture-fusion TKIN modules. Extensive experiments are conducted on the Deepfashion dataset, with the DTD dataset as an external texture source. The experimental results demonstrate the superiority of our proposed method in generating persons of better textures and structures than state-of-the-art works, and also show the generalization ability of our proposed method to absorb diversified external textures for generating person images. The source codes are available at https://github.com/pilgrim00/TKIN.
C1 [Chen, Jiaxiang; Ye, Hancheng; Liao, Yongbing; Chen, Tao] Fudan Univ, Sch Informat Sci & Technol, Shanghai 200438, Peoples R China.
   [Fan, Jiayuan] Fudan Univ, Acad Engn & Technol, Shanghai 200433, Peoples R China.
   [Li, Jie] Tencent GY Lab, Shanghai 200000, Peoples R China.
C3 Fudan University; Fudan University
RP Fan, JY (corresponding author), Fudan Univ, Acad Engn & Technol, Shanghai 200433, Peoples R China.
EM chenjx20@fudan.edu.cn; jyfan@fudan.edu.cn; yehc20@fudan.edu.cn;
   jieli_cn@163.com; ybliao19@fudan.edu.cn; eetchen@fudan.edu.cn
RI Chen, Tao/IQV-1588-2023
OI Chen, Tao/0000-0003-4565-5548; Li, Jie/0000-0002-8064-947X; Ye,
   Hancheng/0000-0002-6272-2792; Chen, Tao/0000-0002-0779-9818; Chen,
   Jiaxiang/0000-0002-9937-3960
FU National Natural Science Foundation of China [62101137, 62071127];
   Zhejiang Lab Project [2021KH0AB05]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62101137 and 62071127 and in part by
   Zhejiang Lab Project under Grant 2021KH0AB05.
CR Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen BY, 2022, IEEE T CIRC SYST VID, V32, P302, DOI 10.1109/TCSVT.2021.3059706
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
   Dong H., 2018, NeurIPS, P474
   Dong X, 2022, PROC CVPR IEEE, P3470, DOI 10.1109/CVPR52688.2022.00347
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Gatys L., 2015, NIPS
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grigorev A, 2019, PROC CVPR IEEE, P12127, DOI 10.1109/CVPR.2019.01241
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   Hensel M, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kingma D. P., 2014, arXiv
   Lewis K, 2021, Arxiv, DOI arXiv:2101.02285
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381
   Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600
   Liu YB, 2020, PROC CVPR IEEE, P4462, DOI 10.1109/CVPR42600.2020.00452
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lv ZY, 2021, PROC CVPR IEEE, P10801, DOI 10.1109/CVPR46437.2021.01066
   Ma LQ, 2017, ADV NEUR IN, V30
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Parmar R., 2022, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P11410
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Ren YR, 2020, IEEE T IMAGE PROCESS, V29, P8622, DOI 10.1109/TIP.2020.3018224
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Zhan FN, 2021, PROC CVPR IEEE, P15023, DOI 10.1109/CVPR46437.2021.01478
   Zhang JS, 2021, PROC CVPR IEEE, P7978, DOI 10.1109/CVPR46437.2021.00789
   Zhang P., 2022, P IEEECVF C COMPUTER, P7713
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou XR, 2021, PROC CVPR IEEE, P11460, DOI 10.1109/CVPR46437.2021.01130
   Zhou Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201285
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 52
TC 0
Z9 0
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7337
EP 7349
DI 10.1109/TMM.2022.3221351
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000046
DA 2024-07-18
ER

PT J
AU Chen, Z
   Luo, YD
   Wang, S
   Li, JJ
   Huang, Z
AF Chen, Zhi
   Luo, Yadan
   Wang, Sen
   Li, Jingjing
   Huang, Zi
TI GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; generative flow; zero-shot learning
AB Generalized Zero-Shot Learning (GZSL) aims to recognize images not only for seen classes but also for unseen ones by transferring semantic-visual relationships from the seen to the unseen classes. It is an intuitive solution to take the advantage of generative models to hallucinate realistic unseen samples based on the knowledge learned from the seen classes. However, due to the generation shifts, the synthesized samples by most existing methods may drift from the real distribution of the unseen data. To address this issue, we propose a novel flow-based generative framework that consists of multiple conditional affine coupling layers for learning unseen data generation. Specifically, we investigate and address three essential problems that trigger the generation shifts, i.e., semantic inconsistency, variance collapse, and structure disorder. First, to improve the reflection of the semantic information in the generated samples, we proactively embed the semantic information into the transformation in each conditional affine coupling layer. Second, to promote the intrinsic feature variance of the unseen classes, we introduce a boundary sample mining strategy with entropy maximization to discover ambiguous visual variants of semantic prototypes and hereby calibrate the decision boundary of the classifiers. Third, a relative positioning strategy is proposed to revise the attribute embeddings, guiding which to fully preserve the inter-class geometric structure and further avoid structure disorder in the semantic space. Extensive experimental results on four GZSL benchmark datasets demonstrate that GSMFlow achieves the state-of-the-art performance on GZSL.
C1 [Chen, Zhi; Luo, Yadan; Wang, Sen; Huang, Zi] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
   [Li, Jingjing] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610056, Sichuan, Peoples R China.
C3 University of Queensland; University of Electronic Science & Technology
   of China
RP Huang, Z (corresponding author), Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
EM zhi.chen@uq.net.au; lyadanluol@gmail.com; sen.wang@uq.edu.au;
   jjl@uestc.edu.cn; huang@itee.uq.edu.au
RI Luo, Yadan/AAY-5893-2021; Li, Jingjing/T-6522-2019
OI Luo, Yadan/0000-0001-6272-2971; Chen, Zhi/0000-0002-9385-144X; Wang,
   Sen/0000-0002-5414-8276; HUANG, ZI/0000-0002-9738-4949
FU Australian Research Council [DP190101985, CE200100025, DE200101610];
   National Natural Science Foundation of China [62176042]; Australian
   Research Council [DE200101610] Funding Source: Australian Research
   Council
FX This work was supported in part by Australian Research Council
   DP190101985, CE200100025, DE200101610, and in part by the National
   Natural Science Foundation of China under Grant 62176042.
CR Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Alemi A. A., 2018, P C UNC ART INT UDL
   Alemi A. A., 2017, ICLR, DOI DOI 10.48550/ARXIV.1612.00410
   Ardizzone L., 2018, ARXIV180804730, P1
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Chen SM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P122, DOI 10.1109/ICCV48922.2021.00019
   Chen Z., 2021, ICCV, P8712
   Chen Z, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P844, DOI 10.1145/3474085.3475258
   Chen Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3413, DOI 10.1145/3394171.3413813
   Chen Z, 2020, IEEE WINT CONF APPL, P863, DOI [10.1109/wacv45572.2020.9093610, 10.1109/WACV45572.2020.9093610]
   Dinh Laurent, 2017, 5 INT C LEARN REPR I
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   Han T, 2017, AAAI CONF ARTIF INTE, P1976
   Han ZY, 2021, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR46437.2021.00240
   Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089
   Jiang HJ, 2019, IEEE I CONF COMP VIS, P9764, DOI 10.1109/ICCV.2019.00986
   Kingma D. P., 2014, arXiv
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Li JJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1348, DOI 10.1145/3394171.3413503
   Li JJ, 2022, IEEE T CYBERNETICS, V52, P8167, DOI 10.1109/TCYB.2021.3050803
   Li JJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1587, DOI 10.1145/3343031.3350901
   Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758
   Li JJ, 2019, AAAI CONF ARTIF INTE, P4189
   Li Y, 2023, IEEE T MULTIMEDIA, V25, P1600, DOI 10.1109/TMM.2021.3139211
   Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653
   Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Romera-Paredes Bernardino, 2015, ICML
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Shaobo Min, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12661, DOI 10.1109/CVPR42600.2020.01268
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang Z., 2021, P IEEE CVF INT C COM, P834
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xie GS, 2022, IEEE T NEUR NET LEAR, V33, P2903, DOI 10.1109/TNNLS.2020.3046924
   Xingyu Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P572, DOI 10.1007/978-3-030-58586-0_34
   YANG Y, 2016, P 24 ACM INT C MULT, P1286, DOI DOI 10.1145/2964284.2964319
   Ye YL, 2023, IEEE T MULTIMEDIA, V25, P2252, DOI 10.1109/TMM.2022.3145237
   Yuming Shen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P614, DOI 10.1007/978-3-030-58517-4_36
   Yunlong Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14032, DOI 10.1109/CVPR42600.2020.01405
   Zhao L., 2020, Advances in Neural Information Processing Systems
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
   Zhu YZ, 2019, IEEE I CONF COMP VIS, P9843, DOI 10.1109/ICCV.2019.00994
NR 54
TC 12
Z9 13
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5374
EP 5385
DI 10.1109/TMM.2022.3190678
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300054
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, Z
   Yin, F
   Yang, Q
   Liu, CL
AF Chen, Zhuo
   Yin, Fei
   Yang, Qing
   Liu, Cheng-Lin
TI Cross-Lingual Text Image Recognition via Multi-Hierarchy Cross-Modal
   Mimic
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-lingual text image recognition; cross-modal mimic; multihierarchy
   mimic
ID SCENE TEXT
AB Optical character recognition and machine translation are usually studied and applied separately. In this paper, we consider a new problem named cross-lingual text image recognition (CLTIR) that integrates these two tasks together. The core of this problem is to recognize source language texts shown in images and transcribe them to the target language in an end-to-end manner. Traditional cascaded systems perform text image recognition and text translation sequentially. This can lead to error accumulation and parameter redundancy problems. To overcome these problems, we propose a multihierarchy cross-modal mimic (MHCMM) framework for end-to-end CLTIR, which can be trained with a massive bilingual text corpus and a small number of bilingual annotated text images. In this framework, a plug-in machine translation model is used as a teacher to guide the CLTIR model for learning representations compatible with image and text modes. Via adversarial learning and attention mechanisms, the proposed mimic method can integrate both global and local information in the semantic space. Experiments on a newly collected dataset demonstrate the superiority of the proposed framework. Our method outperforms other pipelines while containing fewer parameters. Additionally, the MHCMM framework can utilize a large-scale bilingual corpus to further improve the performance efficiently. The visualization of attention scores indicates that the proposed model can read text images in a fashion similar to the machine translation model reading text tokens.
C1 [Chen, Zhuo; Yin, Fei; Yang, Qing; Liu, Cheng-Lin] Chinese Acad Sci, Natl Lab Pattern Recognit NLPR, Inst Automat, Beijing 100190, Peoples R China.
   [Chen, Zhuo; Yin, Fei; Yang, Qing; Liu, Cheng-Lin] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Liu, CL (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit NLPR, Inst Automat, Beijing 100190, Peoples R China.
EM zhuo.chen@nlpr.ia.ac.cn; fyin@nlpr.ia.ac.cn; qyang@nlpr.ia.ac.cn;
   liucl@nlpr.ia.ac.cn
OI yan, fei/0000-0002-6412-9140; Liu, Cheng-Lin/0000-0002-6743-4175
FU National Key Research and Development Program [2020AAA0108003]; National
   Natural Science Foundation of China [61733007, 61721004]
FX This work was supported in part by the National Key Research and
   Development Program under Grant 2020AAA0108003 and in part by the
   National Natural Science Foundation of China under Grants 61733007 and
   61721004.
CR AIChallenger, 2018, Ai challenger
   Albanie S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P292, DOI 10.1145/3240508.3240578
   [Anonymous], 2013, EMNLP
   Ba LJ, 2014, ADV NEUR IN, V27
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bhunia AK, 2019, PROC CVPR IEEE, P4762, DOI 10.1109/CVPR.2019.00490
   Chen X, 2021, INT J COMPUT VISION, V129, P638, DOI 10.1007/s11263-020-01396-x
   Chen YC, 2019, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2019.00189
   Chen Z, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107555
   Chen Z, 2017, PROC INT CONF DOC, P525, DOI 10.1109/ICDAR.2017.92
   Cho K., 2014, ARXIV14061078
   Dabre R, 2020, Arxiv, DOI arXiv:2001.01115
   Dabre R, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3406095
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Gehring J, 2017, PR MACH LEARN RES, V70
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang FR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1874, DOI 10.1145/3240508.3240614
   Jaderberg M, 2014, Arxiv, DOI arXiv:1406.2227
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   Kingma D. P., 2014, arXiv
   Kundu JN, 2019, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2019.00152
   Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173
   Lin QX, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107692
   Lin SH, 2019, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR.2019.00290
   Lin ZH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2649
   Long SB, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01369-0
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Mehta S., 2021, P INT C LEARN REPR, P1243
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Paszke A, 2019, ADV NEUR IN, V32
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stahlberg F, 2020, J ARTIF INTELL RES, V69, P343, DOI 10.1613/jair.1.12007
   Thoker FM, 2019, IEEE IMAGE PROC, P6, DOI [10.1109/icip.2019.8802909, 10.1109/ICIP.2019.8802909]
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan Z., 2020, P IEEE CVF C COMP VI, P11425
   Wang DL, 2019, PR MACH LEARN RES, V97
   Wang SF, 2021, PATTERN RECOGN, V114, DOI 10.1016/j.patcog.2021.107837
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Wu YC, 2017, PROC INT CONF DOC, P79, DOI 10.1109/ICDAR.2017.22
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yang SH, 2020, Arxiv, DOI arXiv:2002.07526
   Yin F, 2017, Arxiv, DOI arXiv:1709.01727
   Zhang YP, 2018, PATTERN RECOGN LETT, V106, P20, DOI 10.1016/j.patrec.2018.02.006
   Zhao L, 2020, PROC CVPR IEEE, P6527, DOI 10.1109/CVPR42600.2020.00656
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
   Zhuo Chen, 2020, 2020 25th International Conference on Pattern Recognition (ICPR), P3122, DOI 10.1109/ICPR48806.2021.9412281
NR 55
TC 1
Z9 1
U1 4
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4830
EP 4841
DI 10.1109/TMM.2022.3183386
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300016
DA 2024-07-18
ER

PT J
AU Cheng, J
   Wu, FX
   Liu, L
   Zhang, QS
   Rutkowski, L
   Tao, DC
AF Cheng, Jun
   Wu, Fuxiang
   Liu, Liu
   Zhang, Qieshi
   Rutkowski, Leszek
   Tao, Dacheng
TI InDecGAN: Learning to Generate Complex Images From Captions via
   Independent Object-Level Decomposition and Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Layout; Task analysis; Generators; Shape; Semantics; Generative
   adversarial networks; Image synthesis; Complex scene; independent
   object-level pathway; size information; text-to-image synthesis
AB Text-to-image synthesis is a challenging problem, in which a complex scene contains diverse objects of various sizes and sub-images of objects belonging to the same class have diverse forms from different perspectives. Thus, synthesis models have difficulty in capturing varied objects in the complex scene. To alleviate these problems, we devise an independent object-level decomposing and enhancing generative adversarial networks, denoted as InDecGAN, to synthesize complex images and capture varied objects in a complex scene. Specifically, InDecGAN fully utilizes the independent object-level information, bounding boxes and high-resolution images of objects in training, by employing independent object-level pathways to synthesize varied objects. The independent object-level pathway integrates an independent object-level adversarial loss and the bounding box information to learn the visual features of objects independently, then, the main pathway exploits the features provided by the object-level pathway to compose the full scene and synthesize images. In addition, we analyze the generalization properties of the proposed InDecGAN and demonstrate the improvement from the perspective of the model architecture. Moreover, extensive experiments conducted on a widely used dataset are presented to demonstrate that the proposed model with an independent object-level pathway produces synthesized images of significantly improved quality.
C1 [Cheng, Jun; Wu, Fuxiang; Zhang, Qieshi] Synergy Syst Shenzhen Inst Adv Technol, Chinese Acad Sci, CAS Key Lab Human Machine Intelligence, Beijing 100045, Peoples R China.
   [Cheng, Jun; Wu, Fuxiang; Zhang, Qieshi] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [Liu, Liu; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia.
   [Rutkowski, Leszek] Polish Acad Sci, Syst Res Inst, PL-01447 Warsaw, Poland.
   [Rutkowski, Leszek] Univ Social Sci, Informat Technol Inst, PL-90113 Lodz, Poland.
C3 Chinese Academy of Sciences; Chinese University of Hong Kong; University
   of Sydney; Polish Academy of Sciences; Systems Research Institute of the
   Polish Academy of Sciences; University of Social Sciences
RP Wu, FX (corresponding author), Synergy Syst Shenzhen Inst Adv Technol, Chinese Acad Sci, CAS Key Lab Human Machine Intelligence, Beijing 100045, Peoples R China.; Wu, FX (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.
EM jun.cheng@siat.ac.cn; fx.wu1@siat.ac.cn; liu.liu1@sydney.edu.au;
   qs.zhang@siat.ac.cn; lrutkowski@san.edu.pl; dacheng.tao@sydney.edu.au
RI Tao, Dacheng/A-5449-2012
OI Tao, Dacheng/0000-0001-7225-5449; Zhang, Qieshi/0000-0001-6358-1840;
   Rutkowski, Leszek/0000-0001-6960-9525; liu, liu/0000-0002-8128-2788
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdulnabi AH, 2018, IEEE T MULTIMEDIA, V20, P1656, DOI 10.1109/TMM.2017.2774007
   [Anonymous], P IEEE CVF C COMP VI, P8110
   Arora S, 2017, PR MACH LEARN RES, V70
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Barratt S, 2018, Arxiv, DOI [arXiv:1801.01973, DOI 10.48550/ARXIV.1801.01973]
   Brock A, 2019, Arxiv, DOI [arXiv:1809.11096, DOI 10.48550/ARXIV.1809.11096]
   Ding M, 2022, Arxiv, DOI [arXiv:2204.14217, 10.48550/arXiv.2204.14217]
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hinz T, 2019, Arxiv, DOI arXiv:1901.00686
   Hinz T, 2022, IEEE T PATTERN ANAL, V44, P1552, DOI 10.1109/TPAMI.2020.3021209
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiadong Liang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P491, DOI 10.1007/978-3-030-58548-8_29
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Joseph KJ, 2019, IEEE WINT CONF APPL, P358, DOI 10.1109/WACV.2019.00044
   Jun Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10908, DOI 10.1109/CVPR42600.2020.01092
   Kiros R., P ADV NEUR INF PROC, P3294
   Li B., 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P7880
   Li K, 2019, IEEE I CONF COMP VIS, P4219, DOI 10.1109/ICCV.2019.00432
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Li Y., 2020, P IEEECVF C COMPUTER, P8365
   Li YT, 2019, PROC CVPR IEEE, P6322, DOI 10.1109/CVPR.2019.00649
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Miyato T, 2018, Arxiv, DOI arXiv:1802.05957
   Park CC, 2019, IEEE T PATTERN ANAL, V41, P999, DOI 10.1109/TPAMI.2018.2824816
   Pavllo D., 2020, P EUR C COMP VIS, P482
   Qiao T., 2019, ADV NEURAL INFORM PR, V32, P885
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Qiao YY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2085, DOI 10.1145/3474085.3475363
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Reed S, 2016, PR MACH LEARN RES, V48
   Salimans T, 2016, ADV NEUR IN, V29
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sharma S, 2018, Arxiv, DOI [arXiv:1802.08216, 10.48550/ARXIV.1802.08216]
   Song XN, 2021, IEEE T NEUR NET LEAR, V32, P2458, DOI 10.1109/TNNLS.2020.3005574
   Sun W, 2019, IEEE I CONF COMP VIS, P10530, DOI 10.1109/ICCV.2019.01063
   Sylvain T, 2021, AAAI CONF ARTIF INTE, V35, P2647
   Tan FW, 2019, PROC CVPR IEEE, P6703, DOI 10.1109/CVPR.2019.00687
   Tang H, 2023, IEEE T NEUR NET LEAR, V34, P1972, DOI 10.1109/TNNLS.2021.3105725
   Tian XY, 2022, IEEE T CIRC SYST VID, V32, P4804, DOI 10.1109/TCSVT.2021.3121987
   Wang M., 2020, P IEEE INT C MULT EX, P1
   Wu FX, 2022, IEEE T CYBERNETICS, V52, P568, DOI 10.1109/TCYB.2020.2979258
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243
   Yoshida Y, 2017, Arxiv, DOI arXiv:1705.10941
   Yuan MK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1407, DOI 10.1145/3240508.3240559
   Zhang H, 2021, PROC CVPR IEEE, P833, DOI 10.1109/CVPR46437.2021.00089
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang P., 2018, arXiv
   Zhao B, 2020, INT J COMPUT VISION, V128, P2418, DOI 10.1007/s11263-020-01300-7
   Zheng ZQ, 2023, IEEE T MULTIMEDIA, V25, P2474, DOI 10.1109/TMM.2022.3147425
   Zhou YF, 2022, PROC CVPR IEEE, P17886, DOI 10.1109/CVPR52688.2022.01738
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
   Zhu XJ, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P632
NR 57
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8279
EP 8293
DI 10.1109/TMM.2023.3256798
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000040
DA 2024-07-18
ER

PT J
AU Cheng, JD
   Yang, X
   Pu, YC
   Guo, P
AF Cheng, Junda
   Yang, Xin
   Pu, Yuechuan
   Guo, Peng
TI Region Separable Stereo Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; feature extraction; image matching; stereo image
   processing; supervised learning
ID COST AGGREGATION; DEPTH PREDICTION; FUSION; ACCURATE; NETWORK; NET
AB Convolutional neural networks (CNNs) have shown attractive performance for stereo matching. However, spatially shared convolution weights of CNN-based methods usually face a dilemma that the convolution weights suitable for aggregating contextual information in smooth regions often blur local matching details of textured regions and vice versa. This paper tries to find a way out of the dilemma via a novel region separable stereo matching (RSSM) method, which is universally applicable to CNN stereo models based on 4D cost volumes and can greatly improve the accuracy and efficiency of existing models. The key idea of our method is to automatically group image pixels into regions according to the gradients, and then construct and process the respective cost volume of each region separately. To perform cost aggregation, we propose a two-stage network consisted of regional grouping aggregation (RGA) and regional fusion aggregation (RFA). In RGA, convolutions are grouped in channel-wise, and each group of convolutions learn dedicated weights for the corresponding region via regional supervision. Through RGA, each group of convolutions can extract the most representative features from the corresponding region. In RFA, we combine matching clues of all convolution groups from RGA to output the final prediction map. We further extend the idea of regional grouping to feature extraction and modify the skip connection in aggregation networks to better adapt our method to stereo matching models. Experimental results on five public datasets show that our method can significantly improve several state-of-the-art 3D CNN based stereo models.
C1 [Cheng, Junda; Yang, Xin; Pu, Yuechuan; Guo, Peng] Huazhong Univ Sci & Technol, Elect Informat & Commun, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Guo, P (corresponding author), Huazhong Univ Sci & Technol, Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM cjd@hust.edu.cn; xinyang2014@hust.edu.cn; yuechuanpu@hust.edu.cn;
   guopeng@hust.edu.cn
FU National Natural Science Foundation of China [61872417, 62061160490,
   62122029, U20B2064]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61872417, 62061160490, 62122029, and U20B2064
CR Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen BL, 2018, IEEE T MULTIMEDIA, V20, P2882, DOI 10.1109/TMM.2018.2825883
   Chen JY, 2021, IEEE T PATTERN ANAL, V43, P2598, DOI 10.1109/TPAMI.2020.2977021
   Deng Y, 2022, IEEE T MULTIMEDIA, V24, P2739, DOI 10.1109/TMM.2021.3087017
   Duggal S, 2019, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2019.00448
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Hisham MB, 2015, IEEE ST CONF RES DEV, P100, DOI 10.1109/SCORED.2015.7449303
   Ilg E, 2018, LECT NOTES COMPUT SC, V11216, P626, DOI 10.1007/978-3-030-01258-8_38
   Jie ZQ, 2018, PROC CVPR IEEE, P3838, DOI 10.1109/CVPR.2018.00404
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Khamis S, 2018, LECT NOTES COMPUT SC, V11219, P596, DOI 10.1007/978-3-030-01267-0_35
   Kingma D. P., 2014, arXiv
   Klaus A, 2006, INT C PATT RECOG, P15
   Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee Z, 2015, IEEE T MULTIMEDIA, V17, P792, DOI 10.1109/TMM.2015.2425141
   Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Mei X, 2013, PROC CVPR IEEE, P313, DOI 10.1109/CVPR.2013.47
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Paszke A, 2019, ADV NEUR IN, V32
   Rao ZB, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.16
   REDDI SS, 1984, IEEE T SYST MAN CYB, V14, P661, DOI 10.1109/TSMC.1984.6313341
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Al-amri SS, 2010, Arxiv, DOI [arXiv:1005.4020, DOI 10.48550/ARXIV.1005.4020]
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Schöps T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272
   Shen ZL, 2021, PROC CVPR IEEE, P13901, DOI 10.1109/CVPR46437.2021.01369
   Song X, 2019, LECT NOTES COMPUT SC, V11365, P20, DOI 10.1007/978-3-030-20873-8_2
   Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509
   Watman C, 2004, IEEE INT CONF ROBOT, P4827, DOI 10.1109/ROBOT.2004.1302482
   Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203
   Yang GR, 2018, LECT NOTES COMPUT SC, V11211, P660, DOI 10.1007/978-3-030-01234-2_39
   Yang QX, 2012, PROC CVPR IEEE, P1402, DOI 10.1109/CVPR.2012.6247827
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2701, DOI 10.1109/TMM.2019.2912121
   Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620
   Yoo JC, 2009, CIRC SYST SIGNAL PR, V28, P819, DOI [10.1007/s00034-009-9130-7, 10.1007/S00034-009-9130-7]
   Yuan ZK, 2022, IEEE T MULTIMEDIA, V24, P4092, DOI 10.1109/TMM.2021.3114546
   Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767
   Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027
   Zhang HY, 2022, IEEE T MULTIMEDIA, V24, P3835, DOI 10.1109/TMM.2021.3108900
   Zhang K, 2009, IEEE T CIRC SYST VID, V19, P1073, DOI 10.1109/TCSVT.2009.2020478
   Zhang Y., 2018, EUROPEAN C COMPUTER, P784
NR 48
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4880
EP 4893
DI 10.1109/TMM.2022.3183392
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300020
DA 2024-07-18
ER

PT J
AU Fang, H
   Jia, ZY
   Zhou, H
   Ma, ZH
   Zhang, WM
AF Fang, Han
   Jia, Zhaoyang
   Zhou, Hang
   Ma, Zehua
   Zhang, Weiming
TI Encoded Feature Enhancement in Watermarking Network for Distortion in
   Real Scenes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep-learning Watermarking; practical distortions triple-phase;
   mask-guided frequency enhancement
ID IMAGE WATERMARKING
AB Deep-learning based watermarking framework has been extensively studied recently. The main structure of such framework is an encoder, a noise layer and a decoder. By training with different distortion sets in the noise layer, the whole network can realize different robustness. However, such framework has a huge drawback that the noise layer must be differentiable, otherwise it cannot be trained end-to-end. But for practical use, much distortions are non-differentiable, so such framework cannot be applied. To address such limitations, this paper propose a triple-phase watermarking framework for practical distortions. The proposed framework consists of three phases including a noise-free initial phase, a mask-guided frequency enhancement phase and an adversarial-training phase. Phase 1 aims to initialize an encoder to embed watermark with high visual quality and a decoder to extract the watermark. In order to generate high quality watermarked image, we design the just noticeable difference (JND)-mask image loss in phase 1 to guide the encoder. At phase 2, based on the investigation of the encoded features and distortions, we propose a mask-guided frequency enhancement algorithm to enhance the encoded feature which ensures the survival of such features after distortion, so that there will be enough features to be learned in phase 3. And phase 3 aims to train a stronger decoder to extract the watermark from the image after practical distortions. The combination of these 3 phases can well handle the non-differentiable problems and make the whole network trainable. Various experiments indicate the superior performance of the proposed scheme in the view of traditional differentiable image processing distortion robustness and practical non-differentiable distortion robustness.
C1 [Fang, Han; Jia, Zhaoyang; Ma, Zehua; Zhang, Weiming] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230027, Peoples R China.
   [Fang, Han] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Zhou, Hang] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC, Canada.
   [Jia, Zhaoyang; Ma, Zehua; Zhang, Weiming] Chinese Acad Sci, Key Lab Electromagnet Space Informat, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; National University of Singapore; Simon Fraser University;
   Chinese Academy of Sciences
RP Zhang, WM (corresponding author), Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230027, Peoples R China.
EM fanghan@mail.ustc.edu.cn; jzy_ustc@mail.ustc.edu.cn;
   zhouhang2991@gmail.com; mzh045@mail.ustc.edu.cn; zhangwm@ustc.edu.cn
RI Zhou, Hang/AAI-5565-2021
OI Zhou, Hang/0000-0001-7860-8452; Zhang, Weiming/0000-0001-5576-6108; Ma,
   Zehua/0000-0002-8153-341X
FU Natural Science Foundation of China [62072421, 62121002, U20B2047];
   Anhui Science Foundation of China [2008085QF296]; Exploration Fund
   Project of University of Science and Technology of China [YD3480002001]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 62072421, 62002334, 62121002 and U20B2047, in part by
   the Anhui Science Foundation of China under Grant 2008085QF296, and in
   part by the Exploration Fund Project of University of Science and
   Technology of China under Grant YD3480002001.& nbsp;
CR Ahmadi M, 2020, EXPERT SYST APPL, V146, DOI 10.1016/j.eswa.2019.113157
   Andalibi M, 2015, IEEE T IMAGE PROCESS, V24, P5060, DOI 10.1109/TIP.2015.2476961
   [Anonymous], 2011, Torch7: A matlab-like environment for machine learning
   Chang CS, 2017, IEEE T IMAGE PROCESS, V26, P3921, DOI 10.1109/TIP.2017.2706502
   Chen BJ, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103015
   Chen ZG, 2018, IEEE T MULTIMEDIA, V20, P1973, DOI 10.1109/TMM.2018.2794985
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Ding WP, 2022, IEEE TETCI, V6, P613, DOI 10.1109/TETCI.2021.3055520
   Fang H, 2019, IEEE T INF FOREN SEC, V14, P1403, DOI 10.1109/TIFS.2018.2878541
   Gao YC, 2019, J REAL-TIME IMAGE PR, V16, P565, DOI 10.1007/s11554-018-0812-x
   Gugelmann D, 2018, INT CONF CYBER CONFL, P391, DOI 10.23919/CYCON.2018.8405027
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu HT, 2019, DIGIT SIGNAL PROCESS, V87, P75, DOI 10.1016/j.dsp.2019.01.006
   Hua G, 2020, IEEE SIGNAL PROC LET, V27, P236, DOI 10.1109/LSP.2020.2965331
   Huang Y, 2019, IEEE T MULTIMEDIA, V21, P2447, DOI 10.1109/TMM.2019.2907475
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kang XG, 2011, IEEE T MULTIMEDIA, V13, P181, DOI 10.1109/TMM.2010.2098850
   Kang XG, 2010, IEEE T INF FOREN SEC, V5, P1, DOI 10.1109/TIFS.2009.2039604
   Kingma D. P., 2014, arXiv
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1509, DOI 10.1145/3343031.3351025
   Ma ZH, 2021, IEEE T CIRC SYST VID, V31, P4826, DOI 10.1109/TCSVT.2021.3055255
   Mathon B, 2014, IEEE T IMAGE PROCESS, V23, P1694, DOI 10.1109/TIP.2014.2305873
   Mellimi S, 2021, PATTERN RECOGN LETT, V151, P222, DOI 10.1016/j.patrec.2021.08.015
   Nakamura T., 2004, P 3 INT C MOB UB MUL, P101
   Pramila A, 2017, MULTIMED TOOLS APPL, V76, P16063, DOI 10.1007/s11042-016-3895-z
   Pramila A, 2012, SIGNAL IMAGE VIDEO P, V6, P211, DOI 10.1007/s11760-011-0211-2
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   sipi.usc, The USC-SIPI image database
   Tancik M, 2020, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR42600.2020.00219
   VANSCHYNDELL RG, 1994, IEEE IMAGE PROC, P86, DOI 10.1109/ICIP.1994.413536
   Ware C., Information Visualization, VFourth, P95, DOI [10.1016/B978-0-12-812875-6.00004-9, DOI 10.1016/B978-0-12-812875-6.00004-9]
   Wengrowski E, 2019, PROC CVPR IEEE, P1515, DOI 10.1109/CVPR.2019.00161
   Zhang X, 2018, IEEE T MULTIMEDIA, V20, P3223, DOI 10.1109/TMM.2018.2838334
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
   Zong TR, 2015, IEEE T CIRC SYST VID, V25, P717, DOI 10.1109/TCSVT.2014.2363743
NR 36
TC 5
Z9 5
U1 3
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2648
EP 2660
DI 10.1109/TMM.2022.3149641
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600015
DA 2024-07-18
ER

PT J
AU Feng, YJ
   Yu, J
   Chen, F
   Ji, YM
   Wu, F
   Liu, SD
   Jing, XY
AF Feng, Yujian
   Yu, Jian
   Chen, Feng
   Ji, Yimu
   Wu, Fei
   Liu, Shangdon
   Jing, Xiao-Yuan
TI Visible-Infrared Person Re-Identification via Cross-Modality Interaction
   Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modality attention mechanism; image representations;
   visible-infrared person re-identification; visual transformer.
ID FAKE NEWS DETECTION; PLUS
AB Visible-infrared person re-identification (VI Re-ID) is designed to match person images of the same identity from visible and infrared cameras. Transformer structures have been successfully applied in the field of VI Re-ID. However, previous Transformer-based methods were mainly designed to capture global content information in a single modality, and could not simultaneously perceive semantic information between two modalities from a global perspective. To solve this problem, we propose a novel framework named the cross-modality interaction Transformer (CMIT). It has strong abilities in modeling spatial and sequential features that can capture dependencies between long-range features, and explicitly improves the discriminativeness of features by exchanging information across modalities, thus contributing to obtaining modality-invariant representations. Specifically, CMIT utilizes a cross-modality attention mechanism to enrich the feature representations of each patch token by interacting with the patch tokens of the other modality, and aggregates local features of the CNN structure and global information of the Transformer structure to mine feature saliency representation. Furthermore, the modality-discriminative (MD) loss function is proposed to learn potential consistency between modalities to encourage intra-modality compactness within class and inter-modality separation between classes. Extensive experiments on two benchmarks demonstrate that our approach outperforms state-of-the-art methods.
C1 [Feng, Yujian] Nanjing Univ Posts & Telecommun, Sch Internet Things, Nanjing 210049, Peoples R China.
   [Yu, Jian] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Coll Artificial Intelligence, Nanjing 210016, Peoples R China.
   [Chen, Feng] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
   [Ji, Yimu; Liu, Shangdon] Nanjing Univ Posts & Telecommun, Sch Comp Sci & Technol, Nanjing 210049, Peoples R China.
   [Wu, Fei] Nanjing Univ Posts & Telecommun, Coll Automat, Nanjing 210049, Peoples R China.
   [Jing, Xiao-Yuan] Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
   [Jing, Xiao-Yuan] Guangdong Univ Petrochem Technol, Guangdong Prov Key Labo Petrochem Equipment Fault, Maoming 525000, Guangdong, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Nanjing University of
   Aeronautics & Astronautics; University of Adelaide; Nanjing University
   of Posts & Telecommunications; Nanjing University of Posts &
   Telecommunications; Wuhan University; Guangdong University of
   Petrochemical Technology
RP Ji, YM (corresponding author), Nanjing Univ Posts & Telecommun, Sch Comp Sci & Technol, Nanjing 210049, Peoples R China.; Wu, F (corresponding author), Nanjing Univ Posts & Telecommun, Coll Automat, Nanjing 210049, Peoples R China.
EM fengyujian_904@163.com; yujian_928@163.com; chenfeng1271@gmail.com;
   jiym@njupt.edu.cn; wufei_8888@126.com; lsd@njupt.edu.cn;
   jingxy_2000@126.com
OI Chen, Feng/0000-0003-1800-8441; Liu, Shangdong/0000-0002-8511-7544; ji,
   yimu/0000-0001-7019-3942; Yu, Jian/0000-0002-0832-7775; Wu,
   Fei/0000-0001-5498-4947; Feng, Yujian/0000-0003-1051-7217
FU National Natural Science Foundation of China [61902194, 62076139,
   62176069, 61933013]; National Key Research and Development Program of
   China [2018AAA0103302, 2018AAA0103300]; Natural Science Foundation of
   Jiangsu Province (Higher Education Institutions) [BK20170900,
   19KJB520046, 20KJA520001]; 14th Five-Year Plan Project of Equipment
   Development Department [315107402]; Innovative and Entrepreneurial
   Talents Projects of Jiangsu Province; Jiangsu Planned Projects for
   Postdoctoral Research Funds [2019K024]; Six Talent Peak Projects in
   Jiangsu Province [JY02]; Postgraduate Research & Practice Innovation
   Program of Jiangsu Province [KYCX19_0921, KYCX19_0906]; Open Research
   Project of Zhejiang Laboratory [2021KF0AB05]; Nanjing University of
   Posts and Telecommunications (NUPT) DingShan Scholar Project [NY219132]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61902194, 62076139, 62176069, and
   61933013, in part by the National Key Research and Development Program
   of China under Grants 2018AAA0103302 and 2018AAA0103300, in part by the
   Natural Science Foundation of Jiangsu Province (Higher Education
   Institutions) under Grants BK20170900, 19KJB520046, and 20KJA520001, in
   part by the 14th Five-Year Plan Project of Equipment Development
   Department under Grant 315107402, in part by the Innovative and
   Entrepreneurial Talents Projects of Jiangsu Province, in part by the
   Jiangsu Planned Projects for Postdoctoral Research Funds under Grant
   2019K024, in part by Six Talent Peak Projects in Jiangsu Province under
   Grant JY02, in part by the Postgraduate Research & Practice Innovation
   Program of Jiangsu Province under Grants KYCX19_0921 and KYCX19_0906, in
   part by the Open Research Project of Zhejiang Laboratory under Grant
   2021KF0AB05, in part by the Nanjing University of Posts and
   Telecommunications (NUPT) DingShan Scholar Project and NUPT SF under
   Grant NY219132, and in part by the 1311 Talent Program of Nanjing
   University of Posts and Telecommunications.
CR Alemi A. A., 2017, ICLR, DOI DOI 10.48550/ARXIV.1612.00410
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bian T, 2020, AAAI CONF ARTIF INTE, V34, P549
   Boididou C., 2016, P MULT RETR ACC EXPL, P1
   Cao J., 2020, Disinformation, Misinformation, and Fake News in Social Media (Lecture Notes in Social Networks), P141, DOI 10.1007/978-3-030-42699-6_8
   Cao M, 2021, IEEE T MULTIMEDIA, V23, P1239, DOI 10.1109/TMM.2020.2994524
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Castillo C., 2011, P 20 INT C WORLD WID, P675, DOI DOI 10.1145/1963405.1963500
   Chang J, 2020, PROC CVPR IEEE, P5709, DOI 10.1109/CVPR42600.2020.00575
   Chen CQ, 2022, IEEE T IMAGE PROCESS, V31, P2352, DOI 10.1109/TIP.2022.3141868
   Chen HS, 2021, PROC CVPR IEEE, P10374, DOI 10.1109/CVPR46437.2021.01024
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen X., 2021, arXiv
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen XS, 2020, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR42600.2020.00336
   Chen YHS, 2021, PROC CVPR IEEE, P587, DOI 10.1109/CVPR46437.2021.00065
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   French Robert M., 1993, P 6 INT C NEURAL INF, V6, P1176
   Fung YR, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P1683
   Gao J, 2020, NEURAL COMPUT, V32, P829, DOI 10.1162/neco_a_01273
   Garrido-Merchán EC, 2020, LECT NOTES ARTIF INT, V12344, P13, DOI 10.1007/978-3-030-61705-9_2
   Gildenblat J., 2021, PyTorch Library for CAM Methods
   Ha T., 2021, PROC PACIFIC RIM INT, V13032, P100
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu D., 2022, P 16 INT WORKSH SEM, P335
   Huetal D., 2021, J.Comput.Res.Develop., V58
   Jin ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P795, DOI 10.1145/3123266.3123454
   Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078
   Jin ZW, 2014, IEEE DATA MINING, P230, DOI 10.1109/ICDM.2014.91
   Kai Shu, 2017, ACM SIGKDD Explorations Newsletter, V19, P22, DOI 10.1145/3137597.3137600
   Kendall Alex, 2017, ADV NEURAL INFORM PR, V30, DOI DOI 10.5555/3295222.3295309
   Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552
   Kim Y., 2014, P 2014 C EMP METH NA, P1746, DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon S, 2013, IEEE DATA MINING, P1103, DOI 10.1109/ICDM.2013.61
   LeCun Y, 1999, LECT NOTES COMPUT SC, V1681, P319, DOI 10.1007/3-540-46805-6_19
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li MH, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2021, P274
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liang TF, 2021, Arxiv, DOI arXiv:2110.08994
   Liu HJ, 2021, IEEE T MULTIMEDIA, V23, P4414, DOI 10.1109/TMM.2020.3042080
   Liu Y, 2018, AAAI CONF ARTIF INTE, P354
   Luo H, 2019, PATTERN RECOGN, V94, P53, DOI 10.1016/j.patcog.2019.05.028
   Ma J., 2015, P 24 ACM INT C INF K, P1751, DOI DOI 10.1145/2806416.2806607
   Ma J., 2016, P 25 INT JOINT C ART, P3818
   Ma J, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P708, DOI 10.18653/v1/P17-1066
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Miao Z, 2021, PROC JCAI, P916
   Morris Meredith Ringel, 2012, P ACM 2012 C COMP SU, P441, DOI DOI 10.1145/2145204.2145274
   Nan Q, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P3343, DOI 10.1145/3459637.3482139
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Qazvinian V., 2011, RUMOR HAS IT IDENTIF
   Qian SS, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P153, DOI 10.1145/3404835.3462871
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Silva A, 2021, AAAI CONF ARTIF INTE, V35, P557
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singhal S, 2020, AAAI CONF ARTIF INTE, V34, P13915
   Singhal S, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P39, DOI [10.1109/BigMM.2019.00-44, 10.1109/BigMM.2019.00018]
   Song CG, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2020.102437
   Sun C, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P380
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Teja SP, 2021, PROC CVPR IEEE, P9608, DOI 10.1109/CVPR46437.2021.00949
   Tian D.P., 2013, International Journal of Multimedia and Ubiquitous Engineering, V8, P385
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tredici M. D., 2020, P 28 INT C COMPUTATI, P5467
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang PY, 2021, IEEE T MULTIMEDIA, V23, P1474, DOI 10.1109/TMM.2020.2999180
   Wang YQ, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3708, DOI 10.1145/3447548.3467153
   Wang YQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P849, DOI 10.1145/3219819.3219903
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wangetal Y., 2021, P INT AAAI C WEB SOC, P776, DOI 10.1609/icwsm.v15i1.18102
   Wei LW, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P3845
   Wei LW, 2021, LECT NOTES ARTIF INT, V12459, P633, DOI 10.1007/978-3-030-67664-3_38
   Wu AC, 2020, INT J COMPUT VISION, V128, P1765, DOI 10.1007/s11263-019-01290-1
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu L, 2019, IEEE T NEUR NET LEAR, V30, P3347, DOI 10.1109/TNNLS.2019.2891244
   Wu MH, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P7291
   Wu Y, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P2560
   Xue JX, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2021.102610
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yang JC, 2020, AAAI CONF ARTIF INTE, V34, P9378
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Yu F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3901
   Yu ZX, 2022, IEEE T MULTIMEDIA, V24, P4482, DOI 10.1109/TMM.2021.3119133
   Zhang DY, 2018, IEEE INT CONF BIG DA, P891, DOI 10.1109/BigData.2018.8622344
   Zhang DM, 2022, IEEE T CIRC SYST VID, V32, P5361, DOI 10.1109/TCSVT.2022.3144775
   Zhang GW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P516, DOI 10.1145/3474085.3475202
   Zhang HW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1942, DOI 10.1145/3343031.3350850
   Zhang T, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206973
   Zhao JQ, 2023, IEEE T MULTIMEDIA, V25, P3668, DOI 10.1109/TMM.2022.3163847
   Zhao ZW, 2021, AAAI CONF ARTIF INTE, V35, P3520
   Zhou LY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2964, DOI 10.1145/3394171.3413515
   Zhou XY, 2020, LECT NOTES ARTIF INT, V12085, P354, DOI 10.1007/978-3-030-47436-2_27
NR 114
TC 8
Z9 8
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7647
EP 7659
DI 10.1109/TMM.2022.3224663
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400003
DA 2024-07-18
ER

PT J
AU Gama, PHT
   Oliveira, H
   Marcato, J Jr
   dos Santos, JA
AF Gama, Pedro H. T.
   Oliveira, Hugo
   Marcato Jr, Jose
   dos Santos, Jefersson A.
TI Weakly Supervised Few-Shot Segmentation via Meta-Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Task analysis; Semantics; Annotations; Prototypes;
   Biomedical imaging; Training; Agriculture; few-shot; medical imaging
   analysis; meta learning; remote sensing; semantic segmentation; weakly
   supervised
ID IMAGES
AB Semantic segmentation is a classic computer vision task with multiple applications, which includes medical and remote sensing image analysis. Despite recent advances with deep-based approaches, labeling samples (pixels) for training models is laborious and, in some cases, unfeasible. In this paper, we present two novel meta-learning methods, named WeaSeL and ProtoSeg, for the few-shot semantic segmentation task with sparse annotations. We conducted an extensive evaluation of the proposed methods in different applications (12 datasets) in medical imaging and agricultural remote sensing, which are very distinct fields of knowledge and usually subject to data scarcity. The results demonstrated the potential of our method, achieving suitable results for segmenting both coffee/orange crops and anatomical parts of the human body in comparison with full dense annotation.
C1 [Gama, Pedro H. T.; dos Santos, Jefersson A.] Univ Fed Minas Gerais, Dept Comp Sci, BR-31270901 Belo Horizonte, Brazil.
   [Oliveira, Hugo] Univ Sao, Inst Math & Stat IME, BR-05508060 Sao Paulo, Brazil.
   [Marcato Jr, Jose] Univ Fed Mato Grosso do Sul, Fac Engn Architecture & Urbanism & Geog, BR-79070900 Campo Grande, MS, Brazil.
C3 Universidade Federal de Minas Gerais; Universidade Federal de Mato
   Grosso do Sul
RP Gama, PHT (corresponding author), Univ Fed Minas Gerais, Dept Comp Sci, BR-31270901 Belo Horizonte, Brazil.
EM pehtg13@gmail.com; hugo.neves2010@gmail.com; jose.marcato@ufms.br;
   jefersson@dcc.ufmg.br
RI Gama, Pedro Henrique Targino/ADU-4263-2022; Oliveira, Hugo
   Neves/HLW-3162-2023; dos Santos, Jefersson/G-6498-2012
OI Oliveira, Hugo Neves/0000-0001-8760-9801; dos Santos,
   Jefersson/0000-0002-8889-1586; Targino Gama, Pedro
   Henrique/0000-0002-9802-593X
FU Serrapilheira Institute [SerraR-2011-37776]; Minas Gerais Research
   Funding Foundation (FAPEMIG) [APQ-00449-17]; National Council for
   Scientific and Technological Development (CNPq) [311395/2018-0,
   424700/2018-2]; Coordenacao de Aperfeicoamento de Pessoal de Nivel
   Superior- Brasil (CAPES) [001]; Fundacao de Amparo a Pesquisa do Estado
   de Sao Paulo (FAPESP) [2020/06744-5, 2021/13424-0]
FX This work was supported in part by Serrapilheira Institute under Grant
   Serra-R-2011-37776, in part by Minas Gerais Research Funding Foundation
   (FAPEMIG) under Grant APQ-00449-17, in part by the National Council for
   Scientific and Technological Development (CNPq) under Grants
   311395/2018-0 and 424700/2018-2, in part by Coordenacao de
   Aperfeicoamento de Pessoal de Nivel Superior- Brasil (CAPES) - Finance
   Code 001, and in part by Fundacao de Amparo a Pesquisa do Estado de Sao
   Paulo (FAPESP under Grants 2020/06744-5 and 2021/13424-0. The Guest
   Editor coordinating the review of this manuscript and approving it for
   publication was Prof. Junwei Han.
CR Abdi AH, 2015, J MED IMAGING, V2, DOI 10.1117/1.JMI.2.4.044003
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Bai WJ, 2018, LECT NOTES COMPUT SC, V11073, P586, DOI 10.1007/978-3-030-00937-3_67
   Bokhorst J.-M., 2018, INT C MED IM DEEP LE, P84
   Cai JZ, 2018, LECT NOTES COMPUT SC, V11073, P396, DOI 10.1007/978-3-030-00937-3_46
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Dong N., 2018, BMVC, V4, P4
   Ferreira E., 2018, P C IB C PATT REC, P72
   Finn C, 2017, PR MACH LEARN RES, V70
   Gama PHT, 2021, SIBGRAPI, P89, DOI 10.1109/SIBGRAPI54419.2021.00021
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hospedales T, 2020, Arxiv, DOI arXiv:2004.05439
   Hu T, 2019, AAAI CONF ARTIF INTE, P8441
   Jaeger S, 2014, QUANT IMAG MED SURG, V4, P475, DOI 10.3978/j.issn.2223-4292.2014.11.20
   Kaiser P, 2017, IEEE T GEOSCI REMOTE, V55, P6054, DOI 10.1109/TGRS.2017.2719738
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Moreira IC, 2012, ACAD RADIOL, V19, P236, DOI 10.1016/j.acra.2011.09.014
   Nogueira K, 2015, LECT NOTES COMPUT SC, V9423, P67, DOI 10.1007/978-3-319-25751-8_9
   Oliveira H, 2020, PATTERN RECOGN LETT, V140, P10, DOI 10.1016/j.patrec.2020.09.021
   Penatti Otavio A. B., 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P44, DOI 10.1109/CVPRW.2015.7301382
   Raghu A., 2020, PROC INT C LEARN REP
   Rakelly K., 2018, P ICLR WORKSH
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shiraishi J, 2000, AM J ROENTGENOL, V174, P71, DOI 10.2214/ajr.174.1.1740071
   Silva G, 2018, EXPERT SYST APPL, V107, P15, DOI 10.1016/j.eswa.2018.04.001
   Silvestri G., 2018, P C MED IM DEEP LEAR, P1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snell J, 2017, ADV NEUR IN, V30
   Suckling J., MAMMOGRAPHIC IMAGE A
   Tajbakhsh N, 2020, MED IMAGE ANAL, V63, DOI 10.1016/j.media.2020.101693
   Tang YB, 2019, PR MACH LEARN RES, V102, P457
   Tuggener L., 2021, IMAGENET REPRESENTAT
   Vernaza P, 2017, PROC CVPR IEEE, P2953, DOI 10.1109/CVPR.2017.315
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang GT, 2018, IEEE T MED IMAGING, V37, P1562, DOI 10.1109/TMI.2018.2791721
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang XS, 2017, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2017.369
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhang ZX, 2019, Arxiv, DOI [arXiv:1906.07367, 10.48550/arXiv.1906.07367, DOI 10.48550/ARXIV.1906.07367]
   Zhu HD, 2019, LECT NOTES COMPUT SC, V11769, P576, DOI 10.1007/978-3-030-32226-7_64
NR 42
TC 5
Z9 5
U1 13
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1784
EP 1797
DI 10.1109/TMM.2022.3162951
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, FX
   Zhang, L
   Zhou, YH
   Gao, XB
AF Huang, Fuxiang
   Zhang, Lei
   Zhou, Yuhang
   Gao, Xinbo
TI Adversarial and Isotropic Gradient Augmentation for Image Retrieval With
   Text Feedback
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gradient augmentation; image retrieval with text feedback (IRTF); model
   generalisation
AB Image Retrieval with Text Feedback (IRTF) is an emerging research topic where the query consists of an image and a text expressing a requested attribute modification. The goal is to retrieve the target images similar to the query text modified query image. The existing methods usually adopt feature fusion of the query image and text to match the target image. However, they ignore two crucial issues: overfitting and low diversity of training data, which make the feature fusion based IRTF task not generalizable. Conventional generation based data augmentation is an effective way to alleviate overfitting and improve diversity, but increases the volume of training data and generation model parameters, which is bound to bring huge computation costs. By rethinking the conventional data augmentation mechanism, we propose a plug-and-play Gradient Augmentation (GA) based regularization approach. Specifically, GA contains two items: 1) To alleviate model overfitting on the training set, we deduce an explicit adversarial gradient augmentation from the perspective of adversarial training, which challenges the "no free lunch" philosophy. 2) To improve the diversity of training set, we propose an implicit isotropic gradient augmentation from the perspective of gradient descent-based optimization, which achieves the goal of big gain but no pain. Besides, we introduce deep metric learning to train the model and provide theoretical insights of GA on generalisation. Finally, we propose a new evaluation protocol called Weighted Harmonic Mean (WHM) to assess the model generalisation. Experiments show that our GA outperforms the state-of-the-art methods by 6.2 and 4.7% on CSS and Fashion200 k datasets, respectively, without bells and whistles.
C1 [Huang, Fuxiang; Zhang, Lei; Zhou, Yuhang] Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400044, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Chongqing University; Chongqing University of Posts & Telecommunications
RP Zhang, L (corresponding author), Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400044, Peoples R China.
EM huangfuxiang@cqu.edu.cn; leizhang@cqu.edu.cn; yuhangzhou@cqu.edu.cn;
   gaoxb@cqupt.edu.cn
OI Zhang, Lei/0000-0002-5305-8543
FU National Key R&D Program of China [2021YFB3100800]; National Natural
   Science Fund of China [62271090]; Chongqing Natural Science Fund
   [cstc2021jcyj-jqX0023]; CCF Hikvision Open Fund [20210002]; CAAI-Huawei
   MindSpore Open Fund; Beijing Academy of Artificial Intelligence (BAAI)
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021YFB3100800, in part by the National Natural Science Fund
   of China under Grant 62271090, in part by Chongqing Natural Science Fund
   under Grant cstc2021jcyj-jqX0023, in part by CCF Hikvision Open Fund
   under Grant CCF-HIKVISION OF 20210002, in part by CAAI-Huawei MindSpore
   Open Fund, and in part by Beijing Academy of Artificial Intelligence
   (BAAI).
CR Ak KE, 2018, PROC CVPR IEEE, P7708, DOI 10.1109/CVPR.2018.00804
   Albuquerque I, 2021, Arxiv, DOI arXiv:1911.00804
   Anwaar MU, 2021, IEEE WINT CONF APPL, P1139, DOI 10.1109/WACV48630.2021.00118
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   BISHOP CM, 1995, NEURAL COMPUT, V7, P108, DOI 10.1162/neco.1995.7.1.108
   Chaturvedi A, 2021, IEEE T NEUR NET LEAR, V32, P1801, DOI 10.1109/TNNLS.2020.2984972
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen YB, 2020, PROC CVPR IEEE, P2998, DOI 10.1109/CVPR42600.2020.00307
   Chun S, 2021, PROC CVPR IEEE, P8411, DOI 10.1109/CVPR46437.2021.00831
   Couairon G., 2022, P IEEE CVF C COMP VI, P18270
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fengchun Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12553, DOI 10.1109/CVPR42600.2020.01257
   Fuxiang Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9579, DOI 10.1109/CVPR42600.2020.00960
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   Gu CB, 2022, NEUROCOMPUTING, V496, P166, DOI 10.1016/j.neucom.2022.01.078
   Guo X., 2018, P NEUR INF PROC SYST
   Guo YY, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3295822
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   Hosseinzadeh M, 2020, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR42600.2020.00365
   Hou YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12127, DOI 10.1109/ICCV48922.2021.01193
   Hu P, 2021, PROC CVPR IEEE, P5399, DOI 10.1109/CVPR46437.2021.00536
   Huang FX, 2022, IEEE T NEUR NET LEAR, V33, P5641, DOI 10.1109/TNNLS.2021.3071127
   Inoue H., 2018, P INT C LEARN REPR
   Isola P, 2015, PROC CVPR IEEE, P1383, DOI 10.1109/CVPR.2015.7298744
   Jiang WT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2095, DOI 10.1109/ICCV48922.2021.00212
   Lashgari E, 2020, J NEUROSCI METH, V346, DOI 10.1016/j.jneumeth.2020.108885
   Lee S, 2021, PROC CVPR IEEE, P802, DOI 10.1109/CVPR46437.2021.00086
   Liu ZP, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4289, DOI 10.1145/3394171.3413689
   Miyato T., 2017, ICLR
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Nagarajan T, 2018, LECT NOTES COMPUT SC, V11205, P172, DOI 10.1007/978-3-030-01246-5_11
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Noh H, 2016, PROC CVPR IEEE, P30, DOI 10.1109/CVPR.2016.11
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Rauber Jonas, 2020, Journal of Open Source Software, V5, P2607, DOI DOI 10.21105/JOSS.02607
   Rouhani BD, 2019, IEEE SECUR PRIV, V17, P31, DOI 10.1109/MSEC.2018.2888779
   Santoro A, 2017, ADV NEUR IN, V30
   Sharma R., 2019, arXiv
   Shen YM, 2020, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR42600.2020.00289
   Shin M, 2021, Arxiv, DOI arXiv:2104.03015
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Simon-Gabriel C., 2019, ICML, P5809
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Song JK, 2020, INT J COMPUT VISION, V128, P2243, DOI 10.1007/s11263-020-01305-2
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Takahashi R, 2020, IEEE T CIRC SYST VID, V30, P2917, DOI 10.1109/TCSVT.2019.2935128
   Tautkute I., 2021, arXiv
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Vo NN, 2016, LECT NOTES COMPUT SC, V9905, P494, DOI 10.1007/978-3-319-46448-0_30
   Volpi R, 2018, ADV NEUR IN, V31
   Wang H, 2022, IEEE T PATTERN ANAL, V44, P9181, DOI 10.1109/TPAMI.2021.3123315
   Wang YL, 2019, ADV NEUR IN, V32
   Wei K, 2022, IEEE T CYBERNETICS, V52, P13788, DOI 10.1109/TCYB.2021.3110369
   Wu H, 2021, PROC CVPR IEEE, P11302, DOI 10.1109/CVPR46437.2021.01115
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Xie C, 2020, PROC CVPR IEEE, P816, DOI 10.1109/CVPR42600.2020.00090
   Yanbei Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P136, DOI 10.1007/978-3-030-58542-6_9
   Yang X, 2022, IEEE T PATTERN ANAL, V44, P1992, DOI 10.1109/TPAMI.2020.3026079
   Yang YH, 2021, IEEE T IMAGE PROCESS, V30, P2798, DOI 10.1109/TIP.2021.3055062
   Yu A, 2019, PROC CVPR IEEE, P708, DOI 10.1109/CVPR.2019.00080
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhou KY, 2020, AAAI CONF ARTIF INTE, V34, P13025
NR 66
TC 2
Z9 2
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7415
EP 7427
DI 10.1109/TMM.2022.3222624
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000050
DA 2024-07-18
ER

PT J
AU Li, FY
   Pei, ZJ
   Zhang, XP
   Qin, C
AF Li, Fengyong
   Pei, Zhenjia
   Zhang, Xinpeng
   Qin, Chuan
TI Image Manipulation Localization Using Multi-Scale Feature Fusion and
   Adaptive Edge Supervision
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image manipulation detection; image forgery; convolutional neural
   network (CNN); multi-scale feature fusion; tamper localization
ID FORGERY
AB Image manipulation localization is a technique that can efficiently segment the tampered regions from a suspicious image. Existing work usually trains a detection model by fusing the features from diverse data streams, e.g., noise inconsistency, recompression inconsistency, and local inconsistency. They, however, ignore a fact that not all tampered images contain these data streams. As a result, high feature redundancy may cause a large number of false detection for tampered region. To address this problem, this paper designs an end-to-end high-confidence localization network architecture. First, deep convolutional neural networks are utilized to extract multi-scale feature sets from the RGB streams. We then design a semantic refined bi-directional feature integration module to fully fuse multi-scale adjacent features and significantly enhance feature representation. Subsequently, morphological operations are introduced to extract multi-scale edge information, which can efficiently reduce feature redundancy by generating wider high-resolution edges during image reconstructing. Finally, a deep semantic residual decoder is sequentially re-constructed by spreading deep semantic information into each decoding stage. The proposed method can not only improve the manipulation localization accuracy, but also guarantee the model robustness. Extensive experiments demonstrate that our method can obtain an effective performance in locating forged regions over different large-scale image sets, and outperforms most of state-of-the-art methods with higher localization accuracy and stronger robustness.
C1 [Li, Fengyong; Pei, Zhenjia] Shanghai Univ Elect Power, Coll Comp Sci & Technol, Shanghai 201306, Peoples R China.
   [Pei, Zhenjia] Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
   [Zhang, Xinpeng] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Qin, Chuan] Univ Shanghai Sci & Technol, Sch Opt & Comp Engn, Shanghai 200093, Peoples R China.
C3 Shanghai University of Electric Power; Guangxi Normal University; Fudan
   University; University of Shanghai for Science & Technology
RP Qin, C (corresponding author), Univ Shanghai Sci & Technol, Sch Opt & Comp Engn, Shanghai 200093, Peoples R China.
EM fyli@shiep.edu.cn; 1140974496@qq.com; zhangxinpeng@fudan.edu.cn;
   qin@usst.edu.cn
RI Qin, Chuan/C-1106-2017
OI Qin, Chuan/0000-0002-0370-4623; Li, Fengyong/0000-0002-3385-8164
FU National Natural Science Foundation of China [U1936213, U20B2051,
   62172280]; Natural Science Foundation of Shanghai [20ZR1421600,
   21ZR1444600]; Research Fund of Guangxi Key Lab of Multi-source
   Information Mining and Security [MIMS21-M-02]; STCSM Capability
   Construction Project for Shanghai Municipal Universities [20060502300]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1936213, U20B2051, and 62172280, in
   part by the Natural Science Foundation of Shanghai under Grants
   20ZR1421600 and 21ZR1444600, in part by Research Fund of Guangxi Key Lab
   of Multi-source Information Mining and Security under Grant MIMS21-M-02,
   and in part by the STCSM Capability Construction Project for Shanghai
   Municipal Universities under Grant 20060502300.
CR Alahmadi AA, 2013, IEEE GLOB CONF SIG, P253, DOI 10.1109/GlobalSIP.2013.6736863
   [Anonymous], 2015, Int. Res. J. Eng. Technol.
   Bappy JH, 2019, IEEE T IMAGE PROCESS, V28, P3286, DOI 10.1109/TIP.2019.2895466
   Chen BJ, 2021, IEEE T MULTIMEDIA, V23, P3506, DOI 10.1109/TMM.2020.3026868
   Chen HP, 2022, MULTIMEDIA SYST, V28, P363, DOI 10.1007/s00530-021-00801-w
   Chen XR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14165, DOI 10.1109/ICCV48922.2021.01392
   Cozzolino D, 2015, IEEE INT WORKS INFOR
   Dong J., 2010, PROC IEEE CHINA SUM, P422
   Dumoulin V, 2018, Arxiv, DOI [arXiv:1603.07285, DOI 10.48550/ARXIV.1603.07285]
   Ferrara P, 2012, IEEE T INF FOREN SEC, V7, P1566, DOI 10.1109/TIFS.2012.2202227
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Gloe T., 2010, ACM S APPL COMP, P1584, DOI DOI 10.1080/15567281.2010.531500
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Guan HY, 2019, IEEE WINT CONF APPL, P63, DOI 10.1109/WACVW.2019.00018
   Hao J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15035, DOI 10.1109/ICCV48922.2021.01478
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hsu T.-T. Ng. J., 2009, Columbia Image Splicing Detection Evaluation Dataset
   Huang YP, 2011, FORENSIC SCI INT, V206, P178, DOI 10.1016/j.forsciint.2010.08.001
   Huh M, 2018, LECT NOTES COMPUT SC, V11215, P106, DOI 10.1007/978-3-030-01252-6_7
   Iakovidou C, 2018, J VIS COMMUN IMAGE R, V54, P155, DOI 10.1016/j.jvcir.2018.05.011
   Islam A, 2020, PROC CVPR IEEE, P4675, DOI 10.1109/CVPR42600.2020.00473
   Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374
   Krawetz Neal, 2007, Hacker Factor Solut., V6, P2
   Li SB, 2023, IEEE T NEUR NET LEAR, V34, P5614, DOI 10.1109/TNNLS.2021.3130168
   Liang J., 2021, PROV IEEE HOT C MUL, P1
   Liang XP, 2023, IEEE T MULTIMEDIA, V25, P1085, DOI 10.1109/TMM.2021.3139217
   Liu B, 2014, SCI WORLD J, DOI 10.1155/2014/230425
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Lyu SW, 2014, INT J COMPUT VISION, V110, P202, DOI 10.1007/s11263-013-0688-y
   Ma MC, 2021, AAAI CONF ARTIF INTE, V35, P2311
   Mahdian B, 2009, IMAGE VISION COMPUT, V27, P1497, DOI 10.1016/j.imavis.2009.02.001
   Niyishaka P, 2021, MULTIMED TOOLS APPL, V80, P2161, DOI 10.1007/s11042-020-09707-7
   Pomari T, 2018, IEEE IMAGE PROC, P3788, DOI 10.1109/ICIP.2018.8451227
   Popescu AC, 2005, IEEE T SIGNAL PROCES, V53, P3948, DOI 10.1109/TSP.2005.855406
   Qiao T, 2019, IEEE T MULTIMEDIA, V21, P1077, DOI 10.1109/TMM.2018.2872863
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qiu Y., 2022, IEEE TROMS MULTIMED, DOI [10.1109/TMML2022.3141933, DOI 10.1109/TMML2022.3141933]
   Rukundo O, 2019, Arxiv, DOI arXiv:1211.1768
   Salloum R, 2018, J VIS COMMUN IMAGE R, V51, P201, DOI 10.1016/j.jvcir.2018.01.010
   Su LC, 2018, IEEE T MULTIMEDIA, V20, P825, DOI 10.1109/TMM.2017.2760098
   Wang JK, 2022, PROC CVPR IEEE, P2354, DOI 10.1109/CVPR52688.2022.00240
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wenyan Cong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8391, DOI 10.1109/CVPR42600.2020.00842
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HW, 2022, PROC CVPR IEEE, P13430, DOI 10.1109/CVPR52688.2022.01308
   Wu Y, 2018, LECT NOTES COMPUT SC, V11210, P170, DOI 10.1007/978-3-030-01231-1_11
   Wu Y, 2019, PROC CVPR IEEE, P9535, DOI 10.1109/CVPR.2019.00977
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Xuefeng Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P312, DOI 10.1007/978-3-030-58589-1_19
   Yang C, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102825
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou P, 2020, AAAI CONF ARTIF INTE, V34, P13058
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
   Zhu XS, 2018, SIGNAL PROCESS-IMAGE, V67, P90, DOI 10.1016/j.image.2018.05.015
   Zhuo L, 2022, IEEE T INF FOREN SEC, V17, P819, DOI 10.1109/TIFS.2022.3152362
NR 60
TC 4
Z9 4
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7851
EP 7866
DI 10.1109/TMM.2022.3231110
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400019
DA 2024-07-18
ER

PT J
AU Li, R
   Xue, DN
   Zhu, Y
   Wu, H
   Sun, JQ
   Zhang, YN
AF Li, Rui
   Xue, Danna
   Zhu, Yu
   Wu, Hao
   Sun, Jinqiu
   Zhang, Yanning
TI Self-Supervised Monocular Depth Estimation With Frequency-Based
   Recurrent Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Self-supervised depth estimation; wavelet; recurrent depth coefficient
   refinement; image-based depth enhancement
ID PREDICTION; FUSION
AB Self-supervised monocular depth estimation has succeeded in learning scene geometry from only image pairs or sequences. However, it is still highly ill-posed for self-supervised depth estimation to generate high-quality depth maps with both global high accuracy and local fine details. To address this issue, we propose a novel frequency-based recurrent refinement scheme to improve the self-supervised depth estimation. Since the global and local depth representation can be correlated to high/low frequency coefficients in the frequency domain, we propose a frequency-based recurrent depth coefficient refinement (RDCR) scheme, which progressively refines both low frequency and high frequency depth coefficients with an RNN-based architecture in a multi-level manner. During the recurrent process, the depth coefficients generated from the previous time step are used as the input to generate the current depth coefficients, yielding progressively optimized depth estimations. Meanwhile, considering that the depth details often appear in areas with high image frequency, we further improve depth details during the RDCR process by leveraging the image-based high frequency components. Specifically, in each RDCR module, we enhance the high frequency depth representations by selecting and feeding the informative image-based high frequency features with a learned feature weighting mask. Extensive experiments show that the proposed method achieves globally accurate estimation with fine local details, outperforming other self-supervised methods in both quantitative and qualitative comparisons.
C1 [Li, Rui; Xue, Danna; Zhu, Yu; Wu, Hao; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China.
   [Sun, Jinqiu] Northwestern Polytech Univ, Sch Astronaut, Xian 710129, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University
RP Sun, JQ (corresponding author), Northwestern Polytech Univ, Sch Astronaut, Xian 710129, Peoples R China.
EM lirui.david@gmail.com; danna_xue@mail.nwpu.edu.cn; yuzhu@nwpu.edu.cn;
   kyon.h.wu@gmail.com; sunjinqiu@nwpu.edu.cn; ynzhang@nwpu.edu.cn
OI Li, Rui/0000-0002-6366-546X
FU National Science Foundation of China [U19B2037]; National Science
   Foundation (NSF) of China [61901384]; Natural Science Basic Research
   Program of Shaanxi Program [2021JCW-03]; National Engineering Laboratory
   for Integrated Aero-Space-Ground-Ocean Big Data Application Technology
FX This work was supported in part by the National Science Foundation of
   China under Grant U19B2037, in part by the National Science Foundation
   (NSF) of China under Grant 61901384, in part by the Natural Science
   Basic Research Program of Shaanxi Program under Grant 2021JCW-03, and in
   part by the National Engineering Laboratory for Integrated
   Aero-Space-Ground-Ocean Big Data Application Technology.
CR Bian J., 2019, ADV NEURAL INFORM PR, P35
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Chen BL, 2018, IEEE T MULTIMEDIA, V20, P2882, DOI 10.1109/TMM.2018.2825883
   Cho K., 2014, ARXIV14061078
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dong WS, 2017, IEEE T MULTIMEDIA, V19, P293, DOI 10.1109/TMM.2016.2613824
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Düzçeker A, 2021, PROC CVPR IEEE, P15319, DOI 10.1109/CVPR46437.2021.01507
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gu Xiaodong, 2021, arXiv
   Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256
   Guo TT, 2017, IEEE COMPUT SOC CONF, P1100, DOI 10.1109/CVPRW.2017.148
   Haar A, 1910, MATH ANN, V69, P331, DOI 10.1007/BF01456326
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Kingma D. P., 2014, arXiv
   Kline J, 2020, PROCEEDINGS OF THE 2020 32ND INTERNATIONAL TELETRAFFIC CONGRESS (ITC 32), P1, DOI [10.1109/ITC3249928.2020.00009, 10.1007/978-3-030-58565-5_35]
   Kumar ACS, 2018, IEEE COMPUT SOC CONF, P396, DOI 10.1109/CVPRW.2018.00066
   Li QF, 2020, PROC CVPR IEEE, P7243, DOI 10.1109/CVPR42600.2020.00727
   Li R., 2021, arXiv
   Li R, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3108, DOI 10.1145/3394171.3413706
   Li R, 2019, NEUROCOMPUTING, V328, P88, DOI 10.1016/j.neucom.2018.02.103
   Lin Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P86, DOI 10.1007/978-3-030-58601-0_6
   Liu XM, 2019, IEEE T IMAGE PROCESS, V28, P1636, DOI 10.1109/TIP.2018.2875506
   Luo CC, 2020, PROC CVPR IEEE, P2404, DOI 10.1109/CVPR42600.2020.00248
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Luo Xiaotong, 2020, PROC IEEE C COMPUT V
   Lyu XY, 2021, AAAI CONF ARTIF INTE, V35, P2294
   Rahaman N, 2019, PR MACH LEARN RES, V97
   Ramamonjisoa M, 2021, PROC CVPR IEEE, P11084, DOI 10.1109/CVPR46437.2021.01094
   Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shi XJ, 2015, ADV NEUR IN, V28
   Song XB, 2022, IEEE T MULTIMEDIA, V24, P4113, DOI 10.1109/TMM.2021.3118282
   Tosi F, 2019, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR.2019.01003
   Unser M, 2003, IEEE T IMAGE PROCESS, V12, P1080, DOI 10.1109/TIP.2003.812329
   Wang R, 2019, PROC CVPR IEEE, P5647, DOI 10.1109/CVPR.2019.00570
   Watson J, 2021, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR46437.2021.00122
   Watson J, 2019, IEEE I CONF COMP VIS, P2162, DOI 10.1109/ICCV.2019.00225
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P775, DOI 10.1007/978-3-030-58452-8_45
   Yang M., 2020, P IEEECVF C COMPUTER, P12885, DOI DOI 10.1109/CVPR42600.2020.01290
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2701, DOI 10.1109/TMM.2019.2912121
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Yuan ZK, 2022, IEEE T MULTIMEDIA, V24, P4092, DOI 10.1109/TMM.2021.3114546
   Zhang HK, 2019, IEEE I CONF COMP VIS, P1725, DOI 10.1109/ICCV.2019.00181
   Zhang HY, 2022, IEEE T MULTIMEDIA, V24, P3835, DOI 10.1109/TMM.2021.3108900
   Zhao Wang, 2020, P IEEE CVF C COMP VI
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zuo YF, 2021, IEEE T MULTIMEDIA, V23, P772, DOI 10.1109/TMM.2020.2987706
NR 57
TC 2
Z9 2
U1 3
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5626
EP 5637
DI 10.1109/TMM.2022.3197367
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300072
DA 2024-07-18
ER

PT J
AU Li, YQ
   Du, J
   Zhang, JS
   Wu, CJ
AF Li, Yunqing
   Du, Jun
   Zhang, Jianshu
   Wu, Changjie
TI A Tree-Structure Analysis Network on Handwritten Chinese Character Error
   Correction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Handwriting recognition; Analytical models; Error analysis;
   Statistical analysis; Layout; Error correction; Handwritten Chinese
   character error correction; CNN; tree-structure analysis network;
   triplet loss; quantitative analysis
ID RECOGNITION
AB Existing researches on handwritten Chinese characters are mainly based on recognition network designed to solve the complex structure and numerous amount characteristics of Chinese characters. In this paper, we investigate Chinese characters from the perspective of error correction, which is to diagnose a handwritten character to be right or wrong and provide a feedback on error analysis. For this handwritten Chinese character error correction task, we define a benchmark by unifying both the evaluation metrics and data splits for the first time. Then we design a diagnosis system that includes decomposition, judgement and correction stages. Specifically, a novel tree-structure analysis network (TAN) is proposed to model a Chinese character as a tree layout, which mainly consists of a CNN-based encoder and a tree-structure based decoder. Using the predicted tree layout for judgement, correction operation is performed for the wrongly written characters to do error analysis. The correction stage is composed of three steps: fetch the ideal character, correct the errors and locate the errors. Additionally, we propose a novel bucketing mining strategy to apply triplet loss at radical level to alleviate feature dispersion. Experiments on handwritten character dataset demonstrate that our proposed TAN shows great superiority on all three metrics comparing with other state-of-the-art recognition models. Through quantitative analysis, TAN is proved to capture more accurate spatial position information than regular encoder-decoder models, showing better generalization ability.
C1 [Li, Yunqing; Du, Jun; Wu, Changjie] Univ Sci & Technol China, Natl Engn Res Ctr Speech & Language Informat Proc, Hefei 230026, Peoples R China.
   [Zhang, Jianshu] AI Res iFLYTEK, Hefei 230088, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Du, J (corresponding author), Univ Sci & Technol China, Natl Engn Res Ctr Speech & Language Informat Proc, Hefei 230026, Peoples R China.
EM lyq123@mail.ustc.edu.cn; jundu@ustc.edu.cn; jszhang6@iflytek.com;
   wucj@mail.ustc.edu.cn
CR Cao Z, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107488
   CHANG TH, 2014, WORKSH P 22 INT C CO, P48
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Ciresan D, 2015, IEEE IJCNN
   Dai Ruwei, 2007, Frontiers of Computer Science in China, V1, P126, DOI 10.1007/s11704-007-002-5
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   HERMANS A, 2017, IN DEFENSE OF THE TR
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   HUANG S, 2016, P 3RDWORKSHOP NATURA, P148
   Iijima T., 1974, Learning systems and intelligent robots, P437, DOI DOI 10.1007/978-1-4684-2106-4_22
   Kimura F., 1987, IEEE Transactions on Pattern Analysis and Machine Intelligence, VPAMI-9, P149, DOI 10.1109/TPAMI.1987.4767881
   Kukich K., 1992, Computing Surveys, V24, P377, DOI 10.1145/146370.146380
   LEE LH, 2015, P 2 WORKSH NAT LANG, P1, DOI DOI 10.18653/V1/W15-4401
   LEE LH, 2014, P COLING 14, P67
   LEVENSHT.VI, 1965, DOKL AKAD NAUK SSSR+, V163, P845
   Li C, 2018, NATURAL LANGUAGE PROCESSING TECHNIQUES FOR EDUCATIONAL APPLICATIONS, P60
   Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li SL, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P1, DOI 10.1109/MIPR.2018.00009
   Li YQ, 2021, INT C PATT RECOG, P4191, DOI 10.1109/ICPR48806.2021.9412918
   Li ZY, 2018, INT J DOC ANAL RECOG, V21, P233, DOI 10.1007/s10032-018-0311-4
   Liang Deng, 2020, P 6 WORKSHOP NATURAL, P57
   Liu CL, 2005, PATTERN RECOGN, V38, P2242, DOI 10.1016/j.patcog.2005.04.019
   Liu CL, 2004, IEEE T NEURAL NETWOR, V15, P430, DOI 10.1109/TNN.2004.824263
   Liu CL, 2003, PATTERN RECOGN, V36, P2271, DOI 10.1016/S0031-3203(03)00085-2
   Mikolov T, 2013, P WORKSHOP ICLR 2013
   Ren HK, 2018, LECT NOTES ARTIF INT, V11109, P401, DOI 10.1007/978-3-319-99501-4_36
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tang J, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P287, DOI 10.1145/2872427.2883041
   TSUKUMO J, 1988, P 9 INT C PATT REC, P168
   Wang AB, 2001, PATTERN RECOGN, V34, P15, DOI 10.1016/S0031-3203(99)00207-1
   Wang TW, 2019, PATTERN RECOGN LETT, V125, P821, DOI 10.1016/j.patrec.2019.08.005
   Wang TQ, 2017, PROC INT CONF DOC, P579, DOI 10.1109/ICDAR.2017.100
   Wang WC, 2018, INT CONF FRONT HAND, P104, DOI 10.1109/ICFHR-2018.2018.00027
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328
   Xiao XF, 2017, PATTERN RECOGN, V72, P72, DOI 10.1016/j.patcog.2017.06.032
   YAMADA H, 1990, PATTERN RECOGN, V23, P1023, DOI 10.1016/0031-3203(90)90110-7
   Yue X., 2020, ECCV, VVolume 12364, P135, DOI [DOI 10.1007/978-3-030-58529-7_9, 10.1007/978-3-030-58529-7_9]
   Zhang F., 2019, INT C MACHINE LEARNI, P7434
   Zhang HW, 2016, PROC CVPR IEEE, P2809, DOI 10.1109/CVPR.2016.307
   ZHANG J, 2020, PATTERN RECOGNIT, V103
   Zhang JS, 2021, IEEE T MULTIMEDIA, V23, P2471, DOI 10.1109/TMM.2020.3011316
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang XY, 2017, PATTERN RECOGN, V61, P348, DOI 10.1016/j.patcog.2016.08.005
   Zhong ZY, 2015, PROC INT CONF DOC, P846, DOI 10.1109/ICDAR.2015.7333881
NR 51
TC 3
Z9 3
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3615
EP 3627
DI 10.1109/TMM.2022.3163517
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500009
DA 2024-07-18
ER

PT J
AU Liang, Q
   Li, Q
   Nie, WZ
   Liu, AA
AF Liang, Qi
   Li, Qiang
   Nie, Weizhi
   Liu, An-An
TI Unsupervised Cross-Media Graph Convolutional Network for 2D Image-Based
   3D Model Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D model; graph embedding; image based; unsupervised
AB With the rapid development of 3D construction technology, 3D models have been implemented in many applications. In particular, the fields of virtual and augmented reality have created a considerable demand for rapid access to large sets of 3D models in recent years. An effective method for addressing the demand is to search 3D models based on 2D images because 2D images can be easily captured by smartphones or other lightweight vision sensors. In this paper, we propose a novel unsupervised cross-media graph convolutional network (UCM-GCN) for 3D model retrieval based on 2D images. Here, we render views from 3D models to construct a graph model based on 3D model structural information. Then, we utilize the 2D image's visual information to bridge the gap between cross-modality data. Then, the proposed UCM-GCN is utilized to update the feature vector of the 2D image and the 3D model. Here, we introduce correlation loss to mitigate the distribution discrepancy across different modalities, which can fully consider the structural and visual similarities between the 2D image and 3D model to embed the final different modalities into the same feature space. To demonstrate the performance of our approach, we conducted a series of experiments on the MI3DOR dataset, which is utilized in SHREC19. We also compared it with other similar methods on the 3D-FUTURE dataset. The experimental results demonstrate the superiority of our proposed method over state-of-the-art methods.
C1 [Liang, Qi; Li, Qiang] Tianjin Univ, Sch Microelect, Tianjin 300072, Peoples R China.
   [Liang, Qi; Liu, An-An] Hefei Comprehens Natl Sci Ctr, Inst Artifificial Intelligence, Hefei 230088, Peoples R China.
   [Nie, Weizhi; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University; Tianjin University
RP Nie, WZ (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM tjuliangqi@tju.edu.cn; liqiang@tju.edu.cn; weizhinie@tju.edu.cn;
   anan0422@gmail.com
OI Qi, Liang/0000-0001-5598-6012; nie, weizhi/0000-0002-0578-8138
FU National Key Research and Development Program of China [2020YFB1711704];
   National Natural Science Foundation of China [61872267, 61471263];
   Tianjin New Generation Artificial Intelligence Major Program
   [16JCZDJC31100]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB1711704, in part by the
   National Natural Science Foundation of China under Grants 61872267 and
   61471263, and the in part by the Tianjin New Generation Artificial
   Intelligence Major Program under Grant 16JCZDJC31100.
CR Biasotti S., 2019, 12 EUR WORKSH 3D OBJ
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Dai GX, 2018, IEEE T IMAGE PROCESS, V27, P3374, DOI 10.1109/TIP.2018.2817042
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Grabner A, 2018, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2018.00319
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Li HH, 2017, IEEE INT CON MULTI, P1434, DOI 10.1109/ICME.2017.8019464
   Liu AA, 2019, IEEE ACCESS, V7, P153021, DOI 10.1109/ACCESS.2019.2947245
   Liu AA, 2018, IEEE T CYBERNETICS, V48, P916, DOI 10.1109/TCYB.2017.2664503
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi Charles R, POINTNET DEEP LEARNI
   Rauber P., 2016, P EUR IEEE VGTC C VI, P73
   Ren WQ, 2019, IEEE I CONF COMP VIS, P9387, DOI 10.1109/ICCV.2019.00948
   Ribeiro LFR, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P385, DOI 10.1145/3097983.3098061
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sfikas K, 2018, COMPUT GRAPH-UK, V71, P208, DOI 10.1016/j.cag.2017.12.001
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Song D, 2022, IEEE T CYBERNETICS, V52, P8114, DOI 10.1109/TCYB.2021.3051016
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su YT, 2021, IEEE T MULTIMEDIA, V23, P2127, DOI 10.1109/TMM.2020.3008056
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Wang NY, 2021, IEEE T PATTERN ANAL, V43, P3600, DOI 10.1109/TPAMI.2020.2984232
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xie J, 2017, IEEE T MULTIMEDIA, V19, P2463, DOI 10.1109/TMM.2017.2698200
   Xu L, 2019, PROC CVPR IEEE, P3328, DOI 10.1109/CVPR.2019.00345
   Yu S, 2022, IEEE T MOBILE COMPUT, V21, P421, DOI 10.1109/TMC.2020.3007654
   Zhang C, 2020, IEEE T IND INFORM, V16, P2335, DOI 10.1109/TII.2019.2943195
   Zhang H, 2016, PROC CVPR IEEE, P1105, DOI 10.1109/CVPR.2016.125
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhou H., 2020, P 28 ACM INT C MULT, P925
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
   Zhou HY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1667, DOI 10.1145/3343031.3351011
NR 46
TC 4
Z9 4
U1 6
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3443
EP 3455
DI 10.1109/TMM.2022.3160616
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200039
DA 2024-07-18
ER

PT J
AU Ma, BW
   Jia, T
   Su, M
   Jia, XD
   Chen, DY
   Zhang, YC
AF Ma, Bowen
   Jia, Tong
   Su, Min
   Jia, Xiaodong
   Chen, Dongyue
   Zhang, Yichun
TI Automated Segmentation of Prohibited Items in X-Ray Baggage Images Using
   Dense De-Overlap Attention Snake
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention deforming module; baggage screening; dense de-overlap module;
   image analysis; prohibited item segmentation; X-ray dataset
ID DATASETS
AB Prohibited item segmentation has a wide range of applications in the security check field, such as computer-aided screening, threat image projection and material discrimination. However, the severe object overlapping in X-ray baggage images restricts the performance of common CNN-based segmentation methods greatly. Worse, no public dataset can be used to promote research in this challenging and promising area. In this paper, to cope with these problems, we present the first Prohibited Item X-ray segmentation dataset named PIXray. PIXray comprises 5,046 X-ray images, in which 15 classes of 15,201 prohibited items are annotated as instance-level masks. Besides, we contribute a dense de-overlap attention snake (DDoAS) in the context of deep learning for automated and real-time prohibited item segmentation. DDoAS mainly includes a dense de-overlap module (DDoM) and an attention deforming module (ADM). Specifically, DDoM is designed to infer prohibited item information accurately from extreme background overlaps through dense reversed connections. ADM aims to improve the low learning efficiency introduced by large variations in shapes and sizes among different prohibited items. Comprehensive evaluation on the PIXray shows the effectiveness and superiority of DDoM and ADM. DDoM excels at recognizing prohibited items from complex backgrounds than other in-domain methods and achieves consistent performance gain over various network backbones, extending the idea of tackling overlapping images data. ADM can ease the model training and further refine the mask quality. Furthermore, out-of-domain experiments prove that DDoAS can also be applied to natural images and achieves comparable performance to the state-of-the-art methods, which implies its potential applications in other fields. The dataset and source code are available at https://github.com/Mbwslib/DDoAS.
C1 [Ma, Bowen; Jia, Tong; Su, Min; Jia, Xiaodong; Chen, Dongyue; Zhang, Yichun] Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
C3 Northeastern University - China
RP Jia, T (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
EM mabowen@stumail.neu.edu.cn; jiatong@ise.neu.edu.cn;
   sumin1@stumail.neu.edu.cn; jiaxiaodong@stumail.neu.edu.cn;
   chendongyue@ise.neu.edu.cn; zhangyichun@vip.sina.com
OI Ma, Bowen/0000-0003-2523-0497
FU National Natural Science Foundation of China [62173083]; National Key
   Research and Development Program of China [2018YFB14041]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62173083, and in part by the National
   Key Research and Development Program of China under Grant 2018YFB14041.
CR Abidi BR, 2006, IEEE T SYST MAN CY C, V36, P784, DOI 10.1109/TSMCC.2005.855523
   Acuna D, 2018, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2018.00096
   Akcay S, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108245
   Akcay S, 2018, IEEE T INF FOREN SEC, V13, P2203, DOI 10.1109/TIFS.2018.2812196
   An JY, 2019, LECT NOTES COMPUT SC, V11935, P495, DOI 10.1007/978-3-030-36189-1_41
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Bhowmik Neelanjan, 2019, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), P986, DOI 10.1109/ICMLA.2019.00168
   Castrejón L, 2017, PROC CVPR IEEE, P4485, DOI 10.1109/CVPR.2017.477
   Chen T, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102965
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai QQ, 2020, IEEE T MULTIMEDIA, V22, P2564, DOI 10.1109/TMM.2019.2958760
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Gaus Y. F. A., 2019, P INT JOINT C NEUR N, P1
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Griffin LD, 2019, IEEE T INF FOREN SEC, V14, P1539, DOI 10.1109/TIFS.2018.2881700
   Gu DH, 2020, IEEE T MULTIMEDIA, V22, P1720, DOI 10.1109/TMM.2020.2971170
   Hassan T, 2020, ACCV, P257
   Hassan T, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03383-7
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heitz G, 2010, PROC CVPR IEEE, P2093, DOI 10.1109/CVPR.2010.5539887
   Hou SH, 2019, AAAI CONF ARTIF INTE, P8425
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jaccard N., 2016, P 7 INT C IM CRIM DE, P1
   Jeon M, 2013, IEEE T MULTIMEDIA, V15, P975, DOI 10.1109/TMM.2013.2244203
   Li LN, 2018, NEUROCOMPUTING, V275, P1650, DOI 10.1016/j.neucom.2017.10.002
   Liang Justin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9128, DOI 10.1109/CVPR42600.2020.00915
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Ling H, 2019, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR.2019.00540
   Liu ZC, 2021, IEEE WINT CONF APPL, P345, DOI 10.1109/WACV48630.2021.00039
   Lu Q, 2006, IEEE T SYST MAN CY C, V36, P750, DOI 10.1109/TSMCC.2005.855532
   [马博文 Ma Bowen], 2021, [计算机学报, Chinese Journal of Computers], V44, P395
   Ma BW, 2020, IEEE ANN INT CONF CY, P130, DOI 10.1109/CYBER50695.2020.9279111
   Meng FM, 2018, IEEE T MULTIMEDIA, V20, P310, DOI 10.1109/TMM.2017.2739919
   Mery D, 2017, IEEE COMPUT SOC CONF, P251, DOI 10.1109/CVPRW.2017.37
   Mery D, 2017, IEEE T SYST MAN CY-S, V47, P682, DOI 10.1109/TSMC.2016.2628381
   Mery D, 2015, J NONDESTRUCT EVAL, V34, DOI 10.1007/s10921-015-0315-7
   Miao CJ, 2019, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR.2019.00222
   Morris T, 2018, 2018 17TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P285, DOI 10.1109/ICMLA.2018.00049
   Müller H, 2017, IEEE T MULTIMEDIA, V19, P2093, DOI 10.1109/TMM.2017.2729400
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren S., 2015, NEURAL INFORM PROCES, V28, P91, DOI DOI 10.1109/TPAMI.2016.2577031
   Riffo V, 2016, IEEE T SYST MAN CY-S, V46, P472, DOI 10.1109/TSMC.2015.2439233
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sida Peng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8530, DOI 10.1109/CVPR42600.2020.00856
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Do TT, 2018, IEEE T MULTIMEDIA, V20, P2849, DOI 10.1109/TMM.2018.2814346
   Torres C, 2018, IEEE T MULTIMEDIA, V20, P3057, DOI 10.1109/TMM.2018.2829162
   Wang T, 2016, IEEE T MULTIMEDIA, V18, P2358, DOI 10.1109/TMM.2016.2600441
   Wang Z, 2019, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2019.00768
   Wei YL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P138, DOI 10.1145/3394171.3413828
   Xu WQ, 2019, IEEE I CONF COMP VIS, P5167, DOI 10.1109/ICCV.2019.00527
   Yao YZ, 2020, IEEE T KNOWL DATA EN, V32, P1199, DOI 10.1109/TKDE.2019.2903036
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yao YZ, 2017, NEUROCOMPUTING, V236, P23, DOI 10.1016/j.neucom.2016.07.066
   Yao YZ, 2016, IEEE INT CON MULTI, DOI 10.1109/ICME.2016.7552988
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zhao F, 2021, IEEE T MULTIMEDIA, V23, P2745, DOI 10.1109/TMM.2020.3016123
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
NR 63
TC 7
Z9 7
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4374
EP 4386
DI 10.1109/TMM.2022.3174339
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200022
DA 2024-07-18
ER

PT J
AU Nie, DY
   Liu, JL
   Fei, H
   Sun, XY
AF Nie, Dongyan
   Liu, Jialin
   Fei, Hong
   Sun, Xiaoying
TI Neuromorphic Similarity Measurement of Tactile Stimuli in HumanMachine
   Interface
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Neurons; Computational modeling; Mathematical models; Biological system
   modeling; Adaptation models; Membrane potentials; Measurement;
   Neuromorphic; similarity measurement; multimedia; tactile; human-machine
   interface
ID VIBROTACTILE; INFORMATION; QUALITY; SIGNALS; MODEL
AB The interpretation of tactile stimuli empowers humans to identify substances, distinguish materials, and engage in tactile communication. For stimulus design in human-computer interaction, objective similarity measures improve efficiency and save costs. Inspired by the fact that biological systems are robust in recognizing multimedia stimuli, we propose a neuromorphic method for similarity measurement. The method is divided into two steps. First, tactile information is translated into biological representations by mimicking a low-threshold mechanoreceptor through a physiological neuronal model. Then, three measures are nominated to assess the similarity of neural spike trains from the following perspectives: interval spike counting, temporal matching, and vector space embedding. Regression analysis showed that the linearity of these measures was significant, indicating that the filtering ability of the physiological neuron model is robust. One of the measures is selected for comparison with the signal-to-noise ratio, structural similarity, and hybrid metric. The results suggest that the correlation between the predictions of our method and the subjective evaluation is stable, above 0.9 for each experimental stimulus. We achieve a mutual interpretation between quantitative measures of vibrotactile similarity and subjective cognitive outcomes. Furthermore, the feasibility of this method in material classification has been substantiated through an exploratory experiment.
C1 [Nie, Dongyan; Liu, Jialin; Sun, Xiaoying] Jilin Univ, Coll Commun Engn, Changchun 130022, Peoples R China.
   [Nie, Dongyan] Jilin Univ, Int Ctr Future Sci, Changchun 130012, Peoples R China.
   [Fei, Hong] Jilin Univ, Sch Artificial Intelligence, Changchun 132012, Peoples R China.
C3 Jilin University; Jilin University; Jilin University
RP Sun, XY (corresponding author), Jilin Univ, Coll Commun Engn, Changchun 130022, Peoples R China.
EM niedongyan@126.com; liujl20@mails.jlu.edu.cn;
   feihong20@mails.jlu.edu.cn; sunxy@jlu.edu.cn
FU National Natural Science Foundation of China [61631010]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61631010. The Associate Editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Metin Sezgin.
CR Abraira VE, 2013, NEURON, V79, P618, DOI 10.1016/j.neuron.2013.07.051
   [Anonymous], 2001, ISO 13091-1 Std
   [Anonymous], 2013, Principles of Neural Science
   Apostolopoulos JG, 2012, P IEEE, V100, P974, DOI 10.1109/JPROC.2011.2182069
   Bi T, 2021, IEEE T MULTIMEDIA, V23, P3494, DOI 10.1109/TMM.2020.3025669
   Chortos A, 2016, NAT MATER, V15, P937, DOI 10.1038/NMAT4671
   Covaci A, 2020, IEEE T MULTIMEDIA, V22, P1249, DOI 10.1109/TMM.2019.2941274
   Culbertson H, 2014, IEEE HAPTICS SYM, P319, DOI 10.1109/HAPTICS.2014.6775475
   El Saddik A, 2020, IEEE MULTIMEDIA, V27, P5, DOI 10.1109/MMUL.2020.2980098
   Hassen R, 2021, IEEE T MULTIMEDIA, V23, P4455, DOI 10.1109/TMM.2020.3042674
   Hassen R, 2020, IEEE T HAPTICS, V13, P25, DOI 10.1109/TOH.2019.2962446
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Johansson RS, 2009, NAT REV NEUROSCI, V10, P345, DOI 10.1038/nrn2621
   JOHANSSON RS, 1982, BRAIN RES, V244, P17, DOI 10.1016/0006-8993(82)90899-X
   Kessler R, 2017, IEEE T NEUR SYS REH, V25, P279, DOI 10.1109/TNSRE.2016.2569258
   Kreuz T, 2007, J NEUROSCI METH, V165, P151, DOI 10.1016/j.jneumeth.2007.05.031
   Liao ZY, 2020, IEEE ROBOT AUTOM LET, V5, P5732, DOI 10.1109/LRA.2020.3010447
   Liu X, 2020, IEEE T MULTIMEDIA, V22, P921, DOI 10.1109/TMM.2019.2936305
   Mirzaei M, 2020, IEEE T VIS COMPUT GR, V26, P2084, DOI 10.1109/TVCG.2020.2973441
   Muniak MA, 2007, J NEUROSCI, V27, P11687, DOI 10.1523/JNEUROSCI.1486-07.2007
   Muschter E, 2021, IEEE T HAPTICS, V14, P291, DOI 10.1109/TOH.2021.3077191
   Nie DY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3096858
   Noll A, 2022, IEEE HAPTICS SYM, DOI 10.1109/HAPTICS52432.2022.9765599
   Noll A, 2021, 2021 IEEE WORLD HAPTICS CONFERENCE (WHC), P427, DOI 10.1109/WHC49131.2021.9517217
   Noll A, 2020, IEEE HAPTICS SYM, P854, DOI 10.1109/HAPTICS45997.2020.ras.HAP20.6.422bbc6e
   Sinapov J, 2011, IEEE T ROBOT, V27, P488, DOI 10.1109/TRO.2011.2127130
   STEIN RB, 1967, BIOPHYS J, V7, P37, DOI 10.1016/S0006-3495(67)86574-3
   Steinbach E, 2019, P IEEE, V107, P447, DOI 10.1109/JPROC.2018.2867835
   Tan HZ, 2020, P IEEE, V108, P945, DOI 10.1109/JPROC.2020.2992561
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   VERRILLO RT, 1971, PERCEPT PSYCHOPHYS, V9, P329, DOI 10.3758/BF03208688
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310
   Vimal AK, 2020, IEEE T NEUR SYS REH, V28, P2890, DOI 10.1109/TNSRE.2020.3035833
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
   Zhang RS, 2020, IEEE T HAPTICS, V13, P102, DOI 10.1109/TOH.2020.2966483
   Zheng HT, 2016, IEEE T MULTIMEDIA, V18, P2407, DOI 10.1109/TMM.2016.2598140
NR 38
TC 2
Z9 2
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6436
EP 6445
DI 10.1109/TMM.2022.3208740
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500056
DA 2024-07-18
ER

PT J
AU Pang, C
   Lu, XQ
   Lyu, L
AF Pang, Chen
   Lu, Xuequan
   Lyu, Lei
TI Skeleton-Based Action Recognition Through Contrasting Two-Stream
   Spatial-Temporal Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrastive learning; graph convolutional network; skeleton-based action
   recognition; transformer
AB For pursuing accurate skeleton-based action recognition, most prior methods combine Graph Convolution Networks (GCNs) with attention-based methods in a serial way. However, they regard the human skeleton as a complete graph, resulting in less variations between different actions (e.g., the connection between the elbow and head in action "clapping hands"). For this, we propose a novel Contrastive GCN-Transformer Network (ConGT) which fuses the spatial and temporal modules in a parallel way. The ConGT involves two parallel streams: Spatial-Temporal Graph Convolution stream (STG) and Spatial-Temporal Transformer stream (STT). STG is designed to obtain action representations maintaining the natural topology structure of the human skeleton. STT is devised to acquire action representations containing the global relationships among joints. Since the action representations produced from these two streams contain different characteristics, and each of them knows little information of the other, we introduce the contrastive learning paradigm to guide their output representations of the same sample to be as close as possible in a self-supervised manner. Through the contrastive learning, they can learn information from each other to enrich the action features by maximizing the mutual information between the two types of action representations. To further improve action recognition accuracy, we introduce the Cyclical Focal Loss (CFL) which can focus on confident training samples in early training epochs, with an increasing focus on hard samples during the middle epochs. We conduct experiments on three benchmark datasets, which demonstrate that our model achieves state-of-the-art performance in action recognition.
C1 [Pang, Chen; Lyu, Lei] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
   [Lu, Xuequan] Deakin Univ, Sch Informat Technol, Geelong, Vic 3216, Australia.
C3 Shandong Normal University; Deakin University
RP Lyu, L (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
EM chenp0721@163.com; xuequan.lu@deakin.edu.au; lvlei@sdnu.edu.cn
RI Chen, Fang/JZE-4446-2024
OI Lu, Xuequan/0000-0003-0959-408X; pang, chen/0009-0002-2358-7038
FU National Natural Science Foundation of China
FX No Statement Available
CR Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Cho HY, 2021, Arxiv, DOI arXiv:2003.02692
   Chuang Gan, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P5589, DOI 10.1109/CVPR.2018.00586
   Defferrard M, 2016, ADV NEUR IN, V29
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Duan H., 2022, CVPR, P2969, DOI DOI 10.1109/CVPR52688.2022.00298
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Guo TY, 2022, AAAI CONF ARTIF INTE, P762
   Hassani K, 2020, PR MACH LEARN RES, V119
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li B, 2019, AAAI CONF ARTIF INTE, P8561
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li LG, 2021, PROC CVPR IEEE, P4739, DOI 10.1109/CVPR46437.2021.00471
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li YS, 2019, IEEE INT CON MULTI, P1066, DOI 10.1109/ICME.2019.00187
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Ming Y, 2021, NEUROCOMPUTING, V450, P362, DOI 10.1016/j.neucom.2021.03.120
   Moliner O, 2022, IEEE COMPUT SOC CONF, P4153, DOI 10.1109/CVPRW56347.2022.00460
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Smith LN, 2022, Arxiv, DOI arXiv:2202.08835
   Niepert M, 2016, PR MACH LEARN RES, V48
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Plizzari C, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103219
   Qiu JZ, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1150, DOI 10.1145/3394486.3403168
   Rao HC, 2021, INFORM SCIENCES, V569, P90, DOI 10.1016/j.ins.2021.04.023
   Rodríguez-Moreno I, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20082436
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L., 2020, P AS C COMP VIS, V5, P38
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Smith L, 2022, Arxiv, DOI arXiv:2202.08978
   Thoker FM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1655, DOI 10.1145/3474085.3475307
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang HY, 2021, PROC CVPR IEEE, P5459, DOI 10.1109/CVPR46437.2021.00542
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang P, 2022, IEEE T IMAGE PROCESS, V31, P6224, DOI 10.1109/TIP.2022.3207577
   Wang PC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P97, DOI 10.1145/2964284.2967191
   Wei C, 2019, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2019.00201
   Wu B, 2017, Arxiv, DOI arXiv:1712.04443
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xu SH, 2023, IEEE T MULTIMEDIA, V25, P624, DOI 10.1109/TMM.2021.3129616
   Xu ZM, 2020, ENG APPL ARTIF INTEL, V95, DOI 10.1016/j.engappai.2020.103859
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zaremba W, 2015, Arxiv, DOI arXiv:1409.2329
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Zhu JG, 2019, NEUROCOMPUTING, V370, P109, DOI 10.1016/j.neucom.2019.08.043
NR 59
TC 4
Z9 5
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8699
EP 8711
DI 10.1109/TMM.2023.3239751
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Prétet, L
   Richard, G
   Souchier, C
   Peeters, G
AF Pretet, Laure
   Richard, Gael
   Souchier, Clement
   Peeters, Geoffroy
TI Video-to-Music Recommendation Using Temporal Alignment of Segments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal recommendation; self-supervised learning; triplet loss
ID AUDIO
AB We study cross-modal recommendation of musictracks to be used as soundtracks for videos. This problem is known as the music supervision task. We build on a self-supervised system that learns a content association between music and video. In addition to the adequacy of content, adequacy of structure is crucial in music supervision to obtain relevant recommendations. We propose a novel approach to significantly improve the system's performance using structure-aware recommendation. The core idea is to consider not only the full audio-video clips, but rather shorter segments for training and inference. We find that using semantic segments and ranking the tracks according to sequence alignment costs significantly improves the results. We investigate the impact of different ranking metrics and segmentation methods.
C1 [Pretet, Laure; Richard, Gael; Peeters, Geoffroy] Telecom Paris, IDS, F-91123 Palaiseau, France.
   [Pretet, Laure; Souchier, Clement] Bridge Audio, R&D, F-75003 Paris, France.
C3 IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Telecom
   Paris
RP Prétet, L (corresponding author), Telecom Paris, IDS, F-91123 Palaiseau, France.
EM laure.pretet@bridge.audio; gael.richard@telecom-paristech.fr;
   clement.souchier@bridge.audio; geoffroy.peeters@telecom-paris.fr
OI RICHARD, Gael/0000-0002-4960-0010
FU company Bridge.audio
FX & nbsp;This work was supported by the company Bridge.audio.& nbsp;&
   nbsp;
CR Abu-El-Haija S., 2016, ARXIV
   Anger V., 2021, TRADUCTION TRANSMEDI, P49
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Aytar Y, 2016, ADV NEUR IN, V29
   Cheng ZY, 2016, ACM T INFORM SYST, V34, DOI 10.1145/2846092
   Foote J, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P452, DOI 10.1109/ICME.2000.869637
   Gabeur Valentin, 2020, P EUR C COMP VIS, V12349, P214, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Gillet O, 2007, IEEE T CIRC SYST VID, V17, P347, DOI 10.1109/TCSVT.2007.890831
   Goto M, 2006, IEEE T AUDIO SPEECH, V14, P1783, DOI 10.1109/TSA.2005.863204
   Hong S, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P353, DOI 10.1145/3206025.3206046
   Hsia C. -C., 2018, PROC LATE BREAKING R
   Hu D., 2020, Advances in Neural Information Processing Systems, P10077
   Hu D, 2020, Arxiv, DOI arXiv:2001.09414
   Hua X. -S., 2004, PROC 12 ANN ACM INT, P472
   Inskip C., 2008, P INT C MUS INF RETR, P477
   Kuo FF, 2013, IEEE INT CON MULTI
   Li B., 2019, T INT SOC MUSIC INF, V2
   Li Bochen, 2019, INT SOC MUSIC INFORM, P604
   Liao C, 2009, LECT NOTES COMPUT SC, V5371, P401, DOI 10.1007/978-3-540-92892-8_41
   Liem CCS, 2013, INT J MULTIMED INF R, V2, P15, DOI 10.1007/s13735-012-0031-3
   Lin JC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P519, DOI 10.1145/3123266.3123399
   Lin JC, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P899, DOI 10.1145/2733373.2806359
   McFee B. etal, 2015, P 14 PYTH SCI C, V8, P18
   McFee B, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6854594
   Minuk Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P156, DOI 10.1007/978-3-030-58604-1_10
   NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4
   Nieto O., 2016, P INT SOC MUS INF RE, P547
   Noulas A, 2012, IEEE T PATTERN ANAL, V34, P79, DOI 10.1109/TPAMI.2011.47
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Pretet L., 2021, PROC INT C MUSIC INF
   Prétet L, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533662
   Sasaki Shoto, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P299, DOI 10.1007/978-3-319-14442-9_33
   Schindler A., 2016, ACM T INTEL SYST TEC, V8, P1
   Schindler A, 2020, Arxiv, DOI arXiv:2002.00251
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Serra J., 2012, Proc. of the 26th AAAI Conference on Artificial Intelligence, P1613
   Shah RR, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P607, DOI 10.1145/2647868.2654919
   Shang L., 2021, SOCIAL NETW ANAL MIN, V11, P1
   Shin KH, 2017, INT CONF BIG DATA, P47, DOI 10.1109/BIGCOMP.2017.7881714
   SMITH TF, 1981, J MOL BIOL, V147, P195, DOI 10.1016/0022-2836(81)90087-5
   Souček T, 2019, Arxiv, DOI arXiv:1906.03363
   Su H., 2013, P MEDIAEVAL MULT BEN
   Surís D, 2019, LECT NOTES COMPUT SC, V11132, P711, DOI 10.1007/978-3-030-11018-5_62
   Wang J C, 2012, P 20 ACM INT C MULT, P1379, DOI [10.1145/2393347.2396494, DOI 10.1145/2393347.2396494]
   Wang JR, 2020, IEEE WINT CONF APPL, P3298, DOI 10.1109/WACV45572.2020.9093345
   Wang JJ, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1897, DOI 10.1109/ICME.2006.262926
   Wang JJ, 2007, IEEE T MULTIMEDIA, V9, P576, DOI 10.1109/TMM.2006.888013
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Yu Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3281746
   Zeng DH, 2018, IEEE INT SYM MULTIM, P143, DOI 10.1109/ISM.2018.00-21
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
NR 52
TC 5
Z9 5
U1 3
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2898
EP 2911
DI 10.1109/TMM.2022.3152598
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600035
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sun, MJ
   Xiao, JM
   Lim, EG
   Zhao, Y
AF Sun, Mingjie
   Xiao, Jimin
   Lim, Eng Gee
   Zhao, Yao
TI Starting Point Selection and Multiple-Standard Matching for Video Object
   Segmentation With Language Annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Annotations; Proposals; Visualization; Standards; Image segmentation;
   Object segmentation; Motion segmentation; Starting point; matching
   strategy; video object segmentation; language annotation
AB In this study, we investigate language-level video object segmentation, where first-frame language annotation is used to describe the target object. Because a language label is typically compatible with all frames in a video, the proposed method can choose the most suitable starting frame to mitigate initialization failure. Apart from extracting the visual feature from a static video frame, a motion-language score based on optical flow is also proposed to describe moving objects more accurately. Scores of multiple standards are then aggregated using an attention-based mechanism to predict the final result. The proposed method is evaluated on four widely-used video object segmentation datasets, including the DAVIS 2017, DAVIS 2016, SegTrack V2 and YouTubeObject datasets, and a novel accuracy measured as mean region similarity is obtained on both the DAVIS 2017 (67.2%) and DAVIS 2016 (83.5%) datasets. The code will be published.
C1 [Sun, Mingjie] Univ Liverpool, Liverpool L69 3BX, England.
   [Sun, Mingjie; Xiao, Jimin; Lim, Eng Gee] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou 215000, Peoples R China.
   [Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
C3 University of Liverpool; Xi'an Jiaotong-Liverpool University; Beijing
   Jiaotong University
RP Xiao, JM (corresponding author), Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou 215000, Peoples R China.
EM trap94@126.com; jimin.xiao@xjtlu.edu.cn; enggee.lim@xjtlu.edu.cn;
   yzhao@bjtu.edu.cn
RI lim, eng GEE/JMC-6208-2023
OI lim, eng GEE/0000-0003-0199-7386; Zhao, Yao/0000-0002-8581-9554; SUN,
   MINGJIE/0000-0002-3697-7927
FU National Key Research and Development of China [2018AAA0102100];
   National Natural Science Foundation of China [61972323, U1936212,
   62120106009]
FX This work was supported in part by the National Key Research and
   Development of China under Grant 2018AAA0102100, and in part by the
   National Natural Science Foundation of China under Grants 61972323,
   U1936212, and 62120106009. The Associate Editor coordinating the review
   of this manuscript and approving it for publication was Dr. Liangliang
   Cao.
CR Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chen K, 2018, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2018.00425
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Ge WB, 2021, PROC CVPR IEEE, P16831, DOI 10.1109/CVPR46437.2021.01656
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu WP, 2021, IEEE T MULTIMEDIA, V23, P145, DOI 10.1109/TMM.2020.2980201
   Khoreva A, 2019, LECT NOTES COMPUT SC, V11364, P123, DOI 10.1007/978-3-030-20870-7_8
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li Y., 2020, Adv. Neural Inf. Proces. Syst., V33, P1218
   Li ZY, 2017, PROC CVPR IEEE, P7350, DOI 10.1109/CVPR.2017.777
   Liang Y., 2020, Adv. Neural Inf. Proces. Syst., V33, P3430
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu DQ, 2019, IEEE I CONF COMP VIS, P4672, DOI 10.1109/ICCV.2019.00477
   Liu XJ, 2019, IEEE I CONF COMP VIS, P2611, DOI 10.1109/ICCV.2019.00270
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P2228, DOI 10.1109/TPAMI.2020.3040258
   Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Pont-Tuset J, 2018, Arxiv, DOI arXiv:1704.00675
   Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Seoung Wug Oh, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P9225, DOI 10.1109/ICCV.2019.00932
   Shi HC, 2021, IEEE T MULTIMEDIA, V23, P995, DOI 10.1109/TMM.2020.2991504
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun M., 2020, P IEEE CVF C COMP VI, P10791
   Sun MJ, 2021, PROC CVPR IEEE, P14055, DOI 10.1109/CVPR46437.2021.01384
   Sun MJ, 2021, IEEE T PATTERN ANAL, V43, P4189, DOI 10.1109/TPAMI.2021.3058684
   Sun MJ, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107465
   Tokmakov P, 2017, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2017.480
   Ventura C, 2019, PROC CVPR IEEE, P5272, DOI 10.1109/CVPR.2019.00542
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Voigtlaender P., 2017, 2017 DAVIS CHALLENGE, V5
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P985, DOI 10.1109/TPAMI.2018.2819173
   Xiankai Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P661, DOI 10.1007/978-3-030-58580-8_39
   Yang JC, 2020, IEEE T MULTIMEDIA, V22, P2635, DOI 10.1109/TMM.2019.2961209
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Yang ZY, 2019, IEEE I CONF COMP VIS, P4682, DOI 10.1109/ICCV.2019.00478
   Ye LW, 2020, IEEE T MULTIMEDIA, V22, P3224, DOI 10.1109/TMM.2020.2971171
   Yu Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P735, DOI 10.1007/978-3-030-58607-2_43
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Yu SY, 2021, AAAI CONF ARTIF INTE, V35, P3234
   Zhang L, 2019, IEEE I CONF COMP VIS, P5581, DOI 10.1109/ICCV.2019.00568
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 55
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3354
EP 3363
DI 10.1109/TMM.2022.3159403
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200031
DA 2024-07-18
ER

PT J
AU Szabó, P
   Simiscuka, AA
   Masneri, S
   Zorrilla, M
   Muntean, GM
AF Szabo, Peter
   Simiscuka, Anderson Augusto
   Masneri, Stefano
   Zorrilla, Mikel
   Muntean, Gabriel-Miro
TI A CNN-Based Framework for Enhancing 360° VR Experiences With
   Multisensorial Effects
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multisensory media; neural networks; image recognition; olfaction;
   haptics; machine learning
ID SENSE
AB Improving user experience during the delivery of immersive content is crucial for its success for both the content creators and audience. Creators can express themselves better with multisensory stimulation, while the audience can experience a higher level of involvement. The rapid development of mulsemedia devices provides better access for stimuli such as olfaction and haptics. Nevertheless, due to the required manual annotation process of adding mulsemedia effects, the amount of content available with sensorial effects is still limited. This work introduces an innovative mulsemedia-enhancement solution capable of automatically generating olfactory and haptic content based on 360 degrees video content, with the use of neural networks. Two parallel neural networks are responsible for automatically adding scents to 360 degrees videos: a scene detection network (responsible for static, global content) and an action detection network (responsible for dynamic, local content). A 360 degrees video dataset with scent labels is also created and used for evaluating the robustness of the proposed solution. The solution achieves a 69.19% olfactory accuracy and 72.26% haptics accuracy during evaluation using two different datasets.
C1 [Szabo, Peter; Masneri, Stefano; Zorrilla, Mikel] Vicomtech, Dept Digital Media, Donostia San Sebastian 20009, Spain.
   [Simiscuka, Anderson Augusto; Muntean, Gabriel-Miro] Dublin City Univ, Sch Elect Engn, Performance Engn Lab, Dublin, Ireland.
C3 Dublin City University
RP Simiscuka, AA (corresponding author), Dublin City Univ, Sch Elect Engn, Performance Engn Lab, Dublin, Ireland.
EM pszabo@vicomtech.org; anderson.simiscuka2@mail.dcu.ie;
   smasneri@vicomtech.org; mzorrilla@vicomtech.org; gabriel.muntean@dcu.ie
RI Simiscuka, Anderson Augusto/AAA-5484-2020
OI Simiscuka, Anderson Augusto/0000-0002-0851-2452
FU European Union [870610]; Science Foundation Ireland (SFI) Research
   Centres Program [SFI/12/RC/2289_P2, SFI/16/SP/3804]; European Regional
   Development Fund
FX This work was supported in part by the European Union's Horizon 2020
   Research and Innovation program under Grant 870610 for the TRAC-TION
   Project, in part by the Science Foundation Ireland (SFI) Research
   Centres Program under Grants SFI/12/RC/2289_P2 (Insight) and
   SFI/16/SP/3804 (EN-ABLE), and in part by European Regional Development
   Fund. The Associate Editor coordinating the review of this manuscript
   and approving it for publication was Dr. Zhi Wang.& nbsp;
CR Ahmed K, 2017, 2017 INTELLIGENT SYSTEMS AND COMPUTER VISION (ISCV)
   Asadi-Aghbolaghi M, 2017, IEEE INT CONF AUTOMA, P476, DOI 10.1109/FG.2017.150
   Baus O, 2017, VIRTUAL REAL-LONDON, V21, P59, DOI 10.1007/s10055-016-0299-3
   Bi T., 2020, P IEEE INT S BROADB, P1
   Bi T, 2018, PROCEEDINGS OF THE 10TH ACM WORKSHOP ON IMMERSIVE MIXED AND VIRTUAL ENVIRONMENT SYSTEMS (MMVE'18), P1, DOI 10.1145/3210438.3210443
   Bordegoni M, 2016, J COMPUT INF SCI ENG, V16, DOI 10.1115/1.4033229
   Calvi E, 2020, BRAIN BEHAV, V10, DOI 10.1002/brb3.1585
   Cha J, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1596990.1596993
   Cohen T.S., 2018, P 6 INT C LEARN REPR, P1
   Comsa IS, 2020, IEEE MULTIMEDIA, V27, P27, DOI 10.1109/MMUL.2019.2954405
   Craig AlanB., 2009, Developing Virtual Reality Applications: Foundations of Effective Design
   Danieau F, 2014, IEEE MULTIMEDIA, V21, P11, DOI 10.1109/MMUL.2013.64
   Danieau F, 2013, IEEE T HAPTICS, V6, P193, DOI [10.1109/TOH.2012.70, 10.1109/ToH.2012.70]
   De Barros P.G., 2013, Proceedings of the 1st symposium on Spatial user interaction, P41, DOI DOI 10.1145/2491367.2491371
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   Dosovitskiy Alexey, 2020, ABS201011929 CORR
   Egan D., 2017, P 2 INT WORKSH MULT, P15, DOI [https://doi.org/10.1145/3132361.3132363, DOI 10.1145/3132361.3132363]
   Engelbrecht H, 2019, FRONT ROBOT AI, V6, DOI 10.3389/frobt.2019.00101
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Guinness D, 2018, PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ISS'18), P203
   Heba S, 2020, NEURAL PLAST, V2020, DOI 10.1155/2020/9125913
   Israr A, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P183, DOI 10.1145/2993369.2993404
   Jalal L, 2018, IEEE T BROADCAST, V64, P552, DOI 10.1109/TBC.2018.2823914
   Kay W, 2017, COMPUT RES REPOSITOR, P1
   Maugey T, 2017, IEEE INT WORKSH MULT
   Mazzoni A, 2016, ENTERTAIN COMPUT, V17, P9, DOI 10.1016/j.entcom.2016.06.002
   Melo M, 2022, IEEE T VIS COMPUT GR, V28, P1428, DOI 10.1109/TVCG.2020.3010088
   Munyan BG, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0157568
   Murray N, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3108243
   Radford A, 2021, PR MACH LEARN RES, V139
   Radosavovic Ilija, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10425, DOI 10.1109/CVPR42600.2020.01044
   Ranasinghe N, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174151
   Saleme EB, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3319853
   Sevilla-Lara Laura, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P281, DOI 10.1007/978-3-030-12939-2_20
   Sexton JP, 2021, IEEE ACCESS, V9, P133156, DOI 10.1109/ACCESS.2021.3115701
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Stojnic R., Papers with code
   Tomono A, 2011, ELECTR COMMUN JPN, V94, P9, DOI 10.1002/ecj.10319
   Ur Réhman S, 2008, IEEE T MULTIMEDIA, V10, P1022, DOI 10.1109/TMM.2008.2001352
   Villamarín D, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21072325
   Wang KH, 2019, INT CONF ACOUST SPEE, P3642, DOI [10.1109/icassp.2019.8683093, 10.1109/ICASSP.2019.8683093]
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xu M, 2022, IEEE T PATTERN ANAL, V44, P2198, DOI 10.1109/TPAMI.2020.3028509
   Yan ZH, 2021, IEEE T INTELL VEHICL, V6, P622, DOI 10.1109/TIV.2020.3044180
   Yang WY, 2018, INT C PATT RECOG, P2190, DOI 10.1109/ICPR.2018.8546070
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 48
TC 4
Z9 4
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3245
EP 3258
DI 10.1109/TMM.2022.3157556
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200023
OA Green Accepted, hybrid
DA 2024-07-18
ER

PT J
AU Tan, WT
   Zhu, L
   Li, JJ
   Zhang, HX
   Han, JW
AF Tan, Wentao
   Zhu, Lei
   Li, Jingjing
   Zhang, Huaxiang
   Han, Junwei
TI Teacher-Student Learning: Efficient Hierarchical Message Aggregation
   Hashing for Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal; Message Aggregation; Supervised Hashing; Knowledge
   Distillation; Lightweight
AB Inspired by the powerful representation capability of deep neural networks, deep cross-modal hashing methods have recently drawn much attention and various deep cross-modal hashing methods have been developed. However, two key problems have not been solved well yet: 1) With advanced neural network models, how to seek the multi-modal alignment space which can effectively model the intrinsic multi-modal correlations and reduce the heterogeneous modality gaps. 2) How to effectively and efficiently preserve the modelled multi-modal semantic correlations into the binary hash codes under the deep learning paradigm. In this paper, we propose a Hierarchical Message Aggregation Hashing (HMAH) method within an efficient teacher-student learning framework. Specifically, on the teacher end, we develop hierarchical message aggregation networks to construct a multi-modal complementary space by aggregating the semantic messages hierarchically across different modalities, which can better align the heterogeneous modalities and model the fine-grained multi-modal correlations. On the student end, we train a couple of student modules that learn hash functions to support cross-modal retrieval. We design a cross-modal correlation knowledge distillation strategy which seamlessly transfers the modelled fine-grained multi-modal semantic correlations from the teacher to the lightweight student modules. With the fine-grained knowledge supervision from teacher module, the semantic representation capability of hash functions can be enhanced. In addition, the whole learning framework avoids the time-consuming finetuning on the pre-trained deep models as existing methods and it is computationally efficient. Experimental results demonstrate the significant performance improvement of the proposed method on both retrieval accuracy and efficiency, compared with the state-of-the-art deep cross-modal hashing methods.
C1 [Tan, Wentao; Zhu, Lei; Zhang, Huaxiang] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
   [Li, Jingjing] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Han, Junwei] Northwestern Polytech Univ, Sch Automat, Xian 710129, Peoples R China.
C3 Shandong Normal University; University of Electronic Science &
   Technology of China; Northwestern Polytechnical University
RP Zhu, L (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
EM tan.wt.lucky@gmail.com; leizhu0608@gmail.com; lijin117@yeah.net;
   huaxzhang@163.com; junweihan2010@gmail.com
RI Zhu, Lei/GQQ-1130-2022; Li, Jingjing/T-6522-2019
OI Zhu, Lei/0000-0002-5348-7532; Zhu, Lei/0000-0002-2993-7142; zhang, hua
   xiang/0000-0001-6259-7533
FU National Natural Science Foundation of China [62172263, U1836216];
   Natural Science Foundation of Shandong, China [ZR2020YQ47, ZR2019QF002];
   Major Fundamental Research Project of Shandong, China [ZR2019ZD03];
   Youth Innovation Project of Shandong Universities, China [2019KJN040];
   Taishan Scholar Project of Shandong, China [ts20190924]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62172263 and Grant U1836216, in part by
   the Natural Science Foundation of Shandong, China, under Grant
   ZR2020YQ47 and Grant ZR2019QF002, in part by the Major Fundamental
   Research Project of Shandong, China, under Grant ZR2019ZD03, in part by
   the Youth Innovation Project of Shandong Universities, China, under
   Grant 2019KJN040, and in part by the Taishan Scholar Project of
   Shandong, China, under Grant ts20190924.
CR [Anonymous], 2013, Book Linear cross-modal hashing for efficient multimedia search, DOI DOI 10.1145/2502081.2502107
   Cao Y, 2017, AAAI CONF ARTIF INTE, P3974
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Chatfield K, 2014, Arxiv, DOI arXiv:1405.3531
   Chen ZD, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1694, DOI 10.1145/3343031.3350862
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P29
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Hu HT, 2020, PROC CVPR IEEE, P3120, DOI 10.1109/CVPR42600.2020.00319
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang QY, 2019, IEEE T IMAGE PROCESS, V28, P3490, DOI 10.1109/TIP.2019.2897944
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kingma D. P., 2014, arXiv
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Li C, 2019, AAAI CONF ARTIF INTE, P176
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li CX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1, DOI 10.1145/3240508.3240547
   Li N, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2397
   Lin QB, 2021, IEEE T MULTIMEDIA, V23, P550, DOI 10.1109/TMM.2020.2984081
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1589, DOI 10.1145/3240508.3240684
   Liu JH, 2021, IEEE T CIRC SYST VID, V31, P3242, DOI 10.1109/TCSVT.2020.3037661
   Liu X, 2021, IEEE T PATTERN ANAL, V43, P964, DOI 10.1109/TPAMI.2019.2940446
   Liu XW, 2019, AAAI CONF ARTIF INTE, P4400
   Lu X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P715, DOI 10.1145/3331184.3331217
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Shen YM, 2020, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR42600.2020.00289
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Tang J, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2564638
   Tu RC, 2020, IEEE T KNOWL DATA EN
   Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang YX, 2021, IEEE T KNOWL DATA EN, V33, P3507, DOI 10.1109/TKDE.2020.2974825
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Xie L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3133
   Xie L, 2016, AAAI CONF ARTIF INTE, P294
   Xu RQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P982
   Yang EK, 2017, AAAI CONF ARTIF INTE, P1618
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang J, 2018, AAAI CONF ARTIF INTE, P539
   Zhang P.-F., 2021, IEEE Trans. Multimedia
   Zheng CQ, 2020, IEEE T KNOWL DATA EN, V32, P2171, DOI 10.1109/TKDE.2019.2913388
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
   Zhou X, 2020, IEEE T CYBERNETICS, V50, P1460, DOI 10.1109/TCYB.2018.2883970
   Zhu L, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3365841
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
   Zhu L, 2018, IEEE T NEUR NET LEAR, V29, P5264, DOI 10.1109/TNNLS.2018.2797248
NR 58
TC 5
Z9 5
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4520
EP 4532
DI 10.1109/TMM.2022.3177901
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200032
DA 2024-07-18
ER

PT J
AU Wang, CT
   Zhang, TJ
   Chen, H
   Huang, Q
   Ni, JQ
   Zhang, XP
AF Wang, Chuntao
   Zhang, Tianjian
   Chen, Hao
   Huang, Qiong
   Ni, Jiangqun
   Zhang, Xinpeng
TI A Novel Encryption-Then-Lossy-Compression Scheme of Color Images Using
   Customized Residual Dense Spatial Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cryptography; Image coding; Image reconstruction; Color; Feature
   extraction; Deep learning; Cloud computing; Encrypted image compression;
   super-resolution reconstruction; residual dense network; spatial
   attention mechanism
ID SUPERRESOLUTION
AB Nowadays it has still remained as a big challenge to efficiently compress color images in the encrypted domain. In this paper we present a novel deep-learning-based approach to encryption-then-lossy-compression (ETC) of color images by incorporating the domain knowledge of the encrypted image reconstruction process. In specific, a simple yet effective uniform down-sampling is utilized for lossy compression of images encrypted with a modulo-256 addition, and the task of image reconstruction from an encrypted down-sampled image is then formulated as a problem of constrained super-resolution (SR) reconstruction. A customized residual dense spatial network (RDSN) is proposed to solve the formulated constrained SR task by taking advantage of spatial attention mechanism (SAM), global skip connection (GSC), and uniform down-sampling constraint (UDC) that is specific to an ETC system. Extensive experimental results show that the proposed ETC scheme achieves significant performance improvement compared with other state-of-the-art ETC methods, indicating the feasibility and effectiveness of the proposed deep-learning based ETC scheme.
C1 [Wang, Chuntao; Zhang, Tianjian; Chen, Hao; Huang, Qiong] South China Agr Univ, Coll Math & Informat, Guangzhou 510642, Peoples R China.
   [Wang, Chuntao; Zhang, Tianjian; Huang, Qiong] Minist Agr & Rural Affairs, Key Lab Smart Agr Technol Trop South China, Guangzhou, Peoples R China.
   [Wang, Chuntao; Zhang, Tianjian; Huang, Qiong] Guangzhou Key Lab Intelligent Agr, Guangzhou 510642, Peoples R China.
   [Chen, Hao] Bank Guangzhou, Guangzhou 510623, Peoples R China.
   [Ni, Jiangqun] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Ni, Jiangqun] Peng Cheng Lab, Cyberspace Secur Res Ctr, Shenzhen, Peoples R China.
   [Zhang, Xinpeng] Fudan Univ, Sch Comp Sci, Shanghai 200438, Peoples R China.
C3 South China Agricultural University; Ministry of Agriculture & Rural
   Affairs; Sun Yat Sen University; Peng Cheng Laboratory; Fudan University
RP Wang, CT (corresponding author), South China Agr Univ, Coll Math & Informat, Guangzhou 510642, Peoples R China.
EM wangct@scau.edu.cn; 1029279754@qq.com; 707347865@qq.com;
   csqhuang-c@my.cityu.edu.hk; issjqni@mail.sysu.edu.cn;
   zhangxinpeng@fudan.edu.cn
OI Wang, Chuntao/0000-0002-5482-1766
FU National Natural Science Foundation of China [62172165, 61672242,
   61702199, 61872152, U1936212, U1736215, 61772573, U1936214]; Natural
   Science Foundation of Guangdong Province [2022A1515010325]; Guangdong
   Major Project of Basic and Applied Basic Research [2019B030302008];
   Science and Technology Program of Guangzhou [201902010081]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172165, 61672242, 61702199, 61872152,
   U1936212, U1736215, 61772573, and U1936214, in part by the Natural
   Science Foundation of Guangdong Province under Grant 2022A1515010325, in
   part by the Guangdong Major Project of Basic and Applied Basic Research
   under Grant 2019B030302008, and in part by the Science and Technology
   Program of Guangzhou under Grant 201902010081.
CR Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Gschwandtner M, 2006, LECT NOTES COMPUT SC, V4237, P141
   Han W, 2018, PROC CVPR IEEE, P1654, DOI 10.1109/CVPR.2018.00178
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hu YT, 2020, IEEE T CIRC SYST VID, V30, P3911, DOI 10.1109/TCSVT.2019.2915238
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Johnson M, 2004, IEEE T SIGNAL PROCES, V52, P2992, DOI 10.1109/TSP.2004.833860
   Kang XG, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-32
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   KUMAR AA, 2009, P TENCON IEEE REGION, P1
   Kumar AA, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P764
   Lai W-S, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lazzeretti R., 2008, P 16 EUR SIGN PROC C, P1
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu W, 2010, IEEE T IMAGE PROCESS, V19, P1097, DOI 10.1109/TIP.2009.2038773
   Liu YQ, 2022, IEEE T MULTIMEDIA, V24, P2259, DOI 10.1109/TMM.2021.3078615
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Qin C, 2019, IEEE T CIRC SYST VID, V29, P3341, DOI 10.1109/TCSVT.2018.2878026
   Ran Hu, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P7387, DOI 10.1109/ICASSP.2014.6855035
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schonberg D, 2008, IEEE T INF FOREN SEC, V3, P749, DOI 10.1109/TIFS.2008.2007244
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi Y, 2017, IEEE T MULTIMEDIA, V19, P2804, DOI 10.1109/TMM.2017.2711263
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Song C, 2014, IEEE GLOB COMM CONF, P64, DOI 10.1109/GLOCOM.2014.7036785
   SONG X, IEEE T MULTIMEDIA
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang CT, 2018, CMC-COMPUT MATER CON, V56, P107, DOI 10.3970/cmc.2018.02477
   Wang CT, 2018, J VIS COMMUN IMAGE R, V51, P122, DOI 10.1016/j.jvcir.2018.01.007
   Wang CT, 2018, IEEE T INF FOREN SEC, V13, P1271, DOI 10.1109/TIFS.2017.2784379
   Wang CT, 2015, 2015 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING (IIH-MSP), P409, DOI 10.1109/IIH-MSP.2015.22
   Wang CT, 2015, SIGNAL PROCESS-IMAGE, V39, P141, DOI 10.1016/j.image.2015.09.009
   Wang YF, 2018, IEEE COMPUT SOC CONF, P977, DOI 10.1109/CVPRW.2018.00131
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Xinpeng Zhang, 2011, 2011 Seventh International Conference on Intelligent Information Hiding and Multimedia Signal Processing, P222, DOI 10.1109/IIHMSP.2011.12
   Yan YT, 2022, IEEE T MULTIMEDIA, V24, P1473, DOI 10.1109/TMM.2021.3065731
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zhang B, 2018, 2018 IEEE 18TH INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT), P1203, DOI 10.1109/ICCT.2018.8599926
   Zhang DY, 2021, IEEE T MULTIMEDIA, V23, P2172, DOI 10.1109/TMM.2020.3008041
   Zhang XP, 2014, IEEE T MULTIMEDIA, V16, P1327, DOI 10.1109/TMM.2014.2315974
   Zhang XP, 2014, MULTIMED TOOLS APPL, V72, P489, DOI 10.1007/s11042-013-1392-1
   Zhang XP, 2012, IEEE T IMAGE PROCESS, V21, P3108, DOI 10.1109/TIP.2012.2187671
   Zhang XP, 2011, IEEE T INF FOREN SEC, V6, P53, DOI 10.1109/TIFS.2010.2099114
   Zhang YB, 2020, IEEE T IMAGE PROCESS, V29, P1101, DOI 10.1109/TIP.2019.2938347
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   ZHAO T, 2018, UNSUPERVISED DEGRADA
   Zhou JT, 2014, IEEE T INF FOREN SEC, V9, P1857, DOI 10.1109/TIFS.2014.2352455
   Zhou JT, 2014, IEEE T INF FOREN SEC, V9, P39, DOI 10.1109/TIFS.2013.2291625
NR 59
TC 5
Z9 5
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4026
EP 4040
DI 10.1109/TMM.2022.3171099
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500036
DA 2024-07-18
ER

PT J
AU Wang, M
   Zhou, WG
   Tian, Q
   Li, HQ
AF Wang, Min
   Zhou, Wengang
   Tian, Qi
   Li, Houqiang
TI Deep Graph Convolutional Quantization Networks for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep quantization; graph convolutional neural network; image retrieval
AB To achieve real-time online search, most image retrieval methods aim to learn compact feature representation while keeping their semantic information or intra-class relevance. In this paper, we propose a new compact feature learning method to embed the underlying manifold information from database. It integrates deep convolutional neural network (CNN) and graph convolutional neural networks (GCN) into a unified end-to-end learning framework. In the proposed method, the deep feature extracted by CNN is automatically embedded with the information from its neighbors by GCN, which possesses the ability of exploring the semantic relevance on the database manifold. Since constructing a graph over the whole database costs unaffordable memory, we build a landmark graph as database sketch. The landmark graph contains two kinds of nodes, including codewords and memory bank samples. Given an image, the deep architecture outputs the discriminative feature and its similarity with all the graph nodes. We directly use the indices of the most similar codeword nodes as the compact feature representation. To make the proposed method scalable to large datasets, a multi-graph strategy is adopted to generate compact features with adaptable code length. The experiments on two benchmark datasets demonstrate the effectiveness of the proposed method.
C1 [Wang, Min] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
   [Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
   [Tian, Qi] Huawei Technol Co Ltd, Shenzhen 518129, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Huawei Technologies
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
EM wangmin@iai.ustc.edu.cn; zhwg@ustc.edu.cn; tian.qi1@huawei.com;
   lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
OI Wang, Min/0000-0003-3048-6980
FU National Natural Science Foundation of China [62102128, 61822208]; Youth
   Innovation Promotion Association CAS [2018497]; GPU cluster built by MCC
   Laboratory of Information Science and Technology Institution, USTC
FX This work was supported in part by the National Natural Science
   Foundation of China under Contracts 62102128, 61822208, and 61822208, in
   part by the Youth Innovation Promotion Association CAS underGrant
   2018497, and in part by the GPU cluster built by MCC Laboratory of
   Information Science and Technology Institution, USTC.
CR Alqaisi T., 2012, 2012 19th IEEE International Conference on Image Processing (ICIP 2012), P2385, DOI 10.1109/ICIP.2012.6467377
   [Anonymous], 2014, PR MACH LEARN RES
   Cao Y, 2017, PROC CVPR IEEE, P916, DOI 10.1109/CVPR.2017.104
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Carbonera JL, 2019, INT J ADV COMPUT SC, V10, P1
   Carreira-Perpiñán MA, 2015, PROC CVPR IEEE, P557, DOI 10.1109/CVPR.2015.7298654
   Chen YD, 2019, IEEE I CONF COMP VIS, P9795, DOI 10.1109/ICCV.2019.00989
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Donoser M, 2013, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2013.174
   Gao LL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P723
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Iscen A, 2017, PROC CVPR IEEE, P926, DOI 10.1109/CVPR.2017.105
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jiang B, 2019, PROC CVPR IEEE, P10406, DOI 10.1109/CVPR.2019.01066
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Q, 2017, ADV NEUR IN, V30
   Li WJ, 2016, IJCAI, P1711
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Luo X, 2022, Arxiv, DOI arXiv:2003.03369
   Nister David, 2006, CVPR
   Norouzi M.E., 2011, ICML
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Olszewska J. I., 2012, Proceedings of the 2012 IEEE 16th International Conference on Intelligent Engineering Systems (INES), P91, DOI 10.1109/INES.2012.6249809
   Song JK, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P912
   Su S., 2018, P 32 INT C NEURAL IN, P798
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang M, 2020, IEEE T MULTIMEDIA, V22, P1507, DOI 10.1109/TMM.2019.2943778
   Wang M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1707, DOI 10.1145/3123266.3123415
   Wang XF, 2017, LECT NOTES COMPUT SC, V10111, P70, DOI 10.1007/978-3-319-54181-5_5
   Wang XJ, 2016, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2016.222
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xu RQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P982
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yu T, 2018, LECT NOTES COMPUT SC, V11205, P191, DOI 10.1007/978-3-030-01246-5_12
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
NR 46
TC 4
Z9 4
U1 7
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2164
EP 2175
DI 10.1109/TMM.2022.3143694
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100044
DA 2024-07-18
ER

PT J
AU Wang, R
   Hao, YX
   Hu, L
   Chen, JC
   Chen, M
   Wu, D
AF Wang, Rui
   Hao, Yixue
   Hu, Long
   Chen, Jincai
   Chen, Min
   Wu, Di
TI Self-Supervised Learning With Data-Efficient Supervised Fine-Tuning for
   Crowd Counting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data models; Head; Self-supervised learning; Annotations; Training; Task
   analysis; Computational modeling; Crowd counting; augmentation
   transformation; self-supervised learning; self-supervised loss
AB Due to the expensive and laborious annotations of labeled data required by fully-supervised learning in the crowd counting task, it is desirable to explore a method to reduce the labeling burden. There exists a large number of unlabeled images in the wild that can be easily obtained compared to labeled datasets. Based on the characteristics of consistent spatial transformation with the annotations of heads and image, this paper proposes a self-supervised learning framework with unlabeled and limited labeled data for pre-training and fine-tuning crowd counting model (SSL-FT). It includes an online network and a target network that receive the same images but are randomly processed by two defined augmentation transformations. We leverage unlabeled data to pre-train the online network based on a self-supervised loss and small-scale labeled data to transfer the model to a specific domain based on a fully-supervised loss. We demonstrate the effectiveness of the SSL-FT on four public datasets including ShanghaiTech PartA, PartB, UCF-QNRF and WorldExpo'10 utilizing a classical counting model. Experimental results show that our approach performs better than state-of-art semi-supervised methods.
C1 [Wang, Rui; Hao, Yixue; Hu, Long] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
   [Chen, Jincai] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
   [Chen, Min] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510640, Peoples R China.
   [Chen, Min] Pazhou Lab, Guangzhou 510640, Peoples R China.
   [Wu, Di] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
   [Wu, Di] Guangdong Key Lab Big Data Anal & Proc, Guangzhou 510006, Guangdong, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; South China University of Technology; Pazhou Lab;
   Sun Yat Sen University
RP Hu, L (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.; Chen, JC (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.; Chen, M (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510640, Peoples R China.
EM ruiwang2020@hust.edu.cn; yixuehao@hust.edu.cn; hulong@hust.edu.cn;
   jccchen@hust.edu.cn; minchen@ieee.org; wudi27@mail.sysu.edu.cn
RI Hao, Yixue/H-8549-2017; wu, di/IYS-9217-2023
OI Hao, Yixue/0000-0001-7296-2522; Chen, Jincai/0000-0001-7368-1677
FU National Natural Science Foundation of China (NSFC) [62176101, 62272178,
   U1911201, U2001209]; Hubei Natural Science Foundation [2021CFB050]
FX This work was supported by National Natural Science Foundation of China
   (NSFC) under Grants 62176101, 62272178, U1911201, and U2001209. The work
   of Yixue Hao was supported by the Hubei Natural Science Foundation under
   Grant 2021CFB050.
CR Chen Ting, 2019, 25 AMERICAS C INFORM
   Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625
   Duan H., 2021, arXiv
   Gao JQ, 2023, Arxiv, DOI [arXiv:2201.04819, DOI 10.48550/ARXIV.2201.04819]
   Grill Jean-Bastien, 2020, ADV NEURAL INFORM PR
   He KM, 2019, IEEE I CONF COMP VIS, P4917, DOI 10.1109/ICCV.2019.00502
   Hou SW, 2022, IEEE T CIRC SYST VID, V32, P2079, DOI 10.1109/TCSVT.2021.3082775
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Jiang XH, 2021, IEEE T MULTIMEDIA, V23, P443, DOI 10.1109/TMM.2020.2980945
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Li J., 2021, PROC INT C LEARN REP
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192
   Liu H., 2021, IJCAI, P860
   Liu XL, 2018, PROC CVPR IEEE, P7661, DOI 10.1109/CVPR.2018.00799
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Meng YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15529, DOI 10.1109/ICCV48922.2021.01526
   Reddy MKK, 2022, IEEE T MULTIMEDIA, V24, P1008, DOI 10.1109/TMM.2021.3062481
   Sam DB, 2022, LECT NOTES COMPUT SC, V13691, P186, DOI 10.1007/978-3-031-19821-2_11
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550
   Sindagi Vishwanath A., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P212, DOI 10.1007/978-3-030-58621-8_13
   Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Song QY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3345, DOI 10.1109/ICCV48922.2021.00335
   Tarvainen A, 2017, ADV NEUR IN, V30
   Verma V, 2022, NEURAL NETWORKS, V145, P90, DOI 10.1016/j.neunet.2021.10.008
   Wang MJ, 2023, IEEE T MULTIMEDIA, V25, P2074, DOI 10.1109/TMM.2022.3142398
   Wang Q, 2021, IEEE T PATTERN ANAL, V43, P2141, DOI 10.1109/TPAMI.2020.3013269
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Xie Q., 2020, Unsupervised data augmentation for consistency training, P6256
   Yan Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P242, DOI 10.1007/978-3-030-58555-6_15
   Yu LQ, 2019, LECT NOTES COMPUT SC, V11765, P605, DOI 10.1007/978-3-030-32245-8_67
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zhen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P565, DOI 10.1007/978-3-030-58565-5_34
   Zoph B., 2020, ARXIV200606882, V33, P3833
NR 41
TC 3
Z9 3
U1 2
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1538
EP 1546
DI 10.1109/TMM.2023.3251106
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000013
DA 2024-07-18
ER

PT J
AU Wang, S
   Wu, ZH
   Hu, XB
   Lin, YF
   Lv, K
AF Wang, Shuo
   Wu, Zhihao
   Hu, Xiaobo
   Lin, Youfang
   Lv, Kai
TI Skill-Based Hierarchical Reinforcement Learning for Target Visual
   Navigation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Navigation; Visualization; Task analysis; Feature extraction;
   Reinforcement learning; Semantics; Behavioral sciences; Target visual
   navigation; skills; hierarchical reinforcement learning; readability;
   abstract representation
AB Target visual navigation aims at controlling the agent to find a target object based on a monocular visual RGB image in each step. It is crucial for the agent to adapt to new environments. As target visual navigation is a complex task, understanding the behavior of the agent is beneficial for analyzing the reasons for failure. This work focuses on improving the readability and success rate of navigation policies. In this paper, we propose a framework named Skill-based Hierarchical Reinforcement Learning (SHRL) for target visual navigation. SHRL contains a high-level policy and three low-level skills. The high-level policy accomplishes the task by utilizing or stopping low-level skills at each step. Low-level skills are designed to separately solve three sub-tasks, i.e., Search, Adjustment, and Exploration. In addition, we propose an Abstract Representation and two penalty items to feed robust features to the high-level policy. Abstract Representation is designed to focus on selecting low-level skills rather than the details of navigation. Experimental results in the artificial environment AI2-Thor indicate that the proposed method outperforms state-of-the-art by a large margin in unseen indoor environments. Moreover, we also provide case studies to illustrate the advantages of SHRL.
C1 [Wang, Shuo; Wu, Zhihao; Hu, Xiaobo; Lin, Youfang; Lv, Kai] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Lv, K (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
EM shuo.wang@bjtu.edu.cn; zhwu@bjtu.edu.cn; xiaobohu@bjtu.edu.cn;
   yflin@bjtu.edu.cn; lvkai@bjtu.edu.cn
RI Hu, Xiaobo/A-1444-2011
OI Wang, Shuo/0000-0001-6599-3638; Hu, Xiaobo/0000-0001-6541-2784; Lv,
   Kai/0000-0001-6533-5176
FU National Natural Science Foundation of China
FX No Statement Available
CR An D, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5101, DOI 10.1145/3474085.3475282
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chaplot DS, 2020, ADV NEURAL INFORM PR, V33
   Das A., 2018, P C ROB LEARN
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dissanayake MWMG, 2001, IEEE T ROBOTIC AUTOM, V17, P229, DOI 10.1109/70.938381
   Du H., 2021, P INT C LEARN REPR, P1
   Duh PJ, 2021, IEEE T MULTIMEDIA, V23, P1567, DOI 10.1109/TMM.2020.3001500
   Eysenbach B., 2018, 7 INT C LEARN REPR N
   Harb J, 2018, AAAI CONF ARTIF INTE, P3165
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heming Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P19, DOI 10.1007/978-3-030-58571-6_2
   Jain U., 2020, EUR C COMP VIS
   Juncheng Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12120, DOI 10.1109/CVPR42600.2020.01214
   Kidono K, 2002, ROBOT AUTON SYST, V40, P121, DOI 10.1016/S0921-8890(02)00237-3
   Kolve E, 2022, Arxiv, DOI [arXiv:1712.05474, DOI 10.48550/ARXIV.1712.05474]
   Kulkarni TD, 2016, ADV NEUR IN, V29
   Li SY, 2019, ADV NEUR IN, V32
   Li WJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4343, DOI 10.1145/3474085.3475575
   Lv K, 2021, IEEE T MULTIMEDIA, V23, P4198, DOI 10.1109/TMM.2020.3038311
   Ma R, 2018, IEEE T MULTIMEDIA, V20, P1595, DOI 10.1109/TMM.2017.2779039
   Mayo B, 2021, PROC CVPR IEEE, P16893, DOI 10.1109/CVPR46437.2021.01662
   Mnih V, 2016, PR MACH LEARN RES, V48
   Moghaddam M. K., 2021, CORR
   Mousavian A, 2019, IEEE INT CONF ROBOT, P8846, DOI 10.1109/icra.2019.8793493
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1021, DOI 10.1109/TMM.2020.2991532
   ORIOLO G, 1995, IEEE INT CONF ROBOT, P2900, DOI 10.1109/ROBOT.1995.525695
   Schull J, 2015, ASSETS'15: PROCEEDINGS OF THE 17TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS & ACCESSIBILITY, P1, DOI 10.1145/2700648.2809870
   Sharma Archit, 2020, ICLR
   Sutton Richard S, 1998, ICML, V98, P556
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2018, INT C LEARN REPR
   Vezhnevets AS, 2017, PR MACH LEARN RES, V70
   Wortsman M, 2019, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR.2019.00691
   Yang W, 2019, P INT C LEARN REPR
   Ye X., 2021, P IEEE CVF C COMP VI, P14101
   Zhang SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15110, DOI 10.1109/ICCV48922.2021.01485
   Zhu YK, 2017, INT CONF ACOUST SPEE, P5335, DOI 10.1109/ICASSP.2017.7953175
   Zingg S, 2010, IEEE INT CONF ROBOT, P3361, DOI 10.1109/ROBOT.2010.5509777
NR 41
TC 5
Z9 5
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8920
EP 8932
DI 10.1109/TMM.2023.3243618
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000034
DA 2024-07-18
ER

PT J
AU Xie, J
   Pang, YW
   Nie, J
   Cao, J
   Han, JG
AF Xie, Jin
   Pang, Yanwei
   Nie, Jing
   Cao, Jiale
   Han, Jungong
TI Latent Feature Pyramid Network for Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Detectors; Object detection; Convolution; Neural
   networks; Proposals; Computational modeling; Feature pyramid network;
   latent space; long-range dependency; object detection
AB Object detection methods based on Convolution Neural Networks (CNN) usually utilize feature pyramid networks to detect objects with various scales. The state-of-the-art feature pyramid networks improve detection accuracy by enhancing multi-level feature representations. Fusing multi-level features is the most effective manner to enhance the feature representations. However, the existing feature pyramid networks usually fuse multi-level features by element-wise operations. It leads to the lack of long-range dependencies in the feature fusion. To address the problem, we propose a simple yet efficient feature pyramid network named latent feature pyramid network (LFPN). LFPN can enhance the feature representations by modeling inner-scale and cross-scale long-range dependencies through conducting inner-scale and cross-scale feature fusion in the latent space. Comprehensive experiments are performed on two challenge object detection datasets: MS COCO and Pascal VOC. The experimental results show consistent improvements on various feature pyramid networks, backbones, and object detectors, which demonstrates the effectiveness and generality of our LFPN.
C1 [Xie, Jin; Pang, Yanwei; Nie, Jing; Cao, Jiale] Tianjin Univ, Tianjin Key Lab Brain Inspired Intelligence Techno, Tianjin 300072, Peoples R China.
   [Xie, Jin; Pang, Yanwei; Nie, Jing; Cao, Jiale] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Xie, Jin] Chongqing Univ, Sch Big Data & Software Engn, Chongqing 401331, Peoples R China.
   [Han, Jungong] Aberystwyth Univ, Sch Comp Sci, Aberystwyth SY23 3FL, Wales.
C3 Tianjin University; Tianjin University; Chongqing University;
   Aberystwyth University
RP Pang, YW (corresponding author), Tianjin Univ, Tianjin Key Lab Brain Inspired Intelligence Techno, Tianjin 300072, Peoples R China.; Pang, YW (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM jinxie@tju.edu.cn; pyw@tju.edu.cn; jingnie@tju.edu.cn;
   connor@tju.edu.cn; jungonghan77@gmail.com
RI Xie, Jindou/ABB-6044-2020
OI Xie, Jin/0000-0001-6978-8834; Nie, Jing/0000-0001-9872-9286
FU National Key Research and Development Program of China [2018AAA0102800];
   Tianjin Science and Technology Program [19ZXZNGX00050]; China
   Postdoctoral Science Foundation [2021M700613]; CAAI-Huawei MindSpore
   Open Fund
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102800, in part by the
   Tianjin Science and Technology Program under Grant 19ZXZNGX00050, in
   part by the China Postdoctoral Science Foundation under Grant
   2021M700613, and in part by the CAAI-Huawei MindSpore Open Fund.
CR Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Cao Y, 2019, Arxiv, DOI arXiv:1904.11492
   Chaoxu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12592, DOI 10.1109/CVPR42600.2020.01261
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen L, 2021, IEEE T MULTIMEDIA, V23, P4297, DOI 10.1109/TMM.2020.3040539
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Chen YP, 2018, ADV NEUR IN, V31
   Dai JF, 2016, ADV NEUR IN, V29
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R., 2014, P 2014 IEEE C COMPUT, P580, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jiale Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11482, DOI 10.1109/CVPR42600.2020.01150
   Jiaqi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P403, DOI 10.1007/978-3-030-58548-8_24
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kim SW, 2018, LECT NOTES COMPUT SC, V11209, P239, DOI 10.1007/978-3-030-01228-1_15
   Kong T, 2020, Arxiv, DOI [arXiv:1904.03797, DOI 10.48550/ARXIV.1904.03797]
   Kong T, 2018, LECT NOTES COMPUT SC, V11209, P172, DOI 10.1007/978-3-030-01228-1_11
   Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li QM, 2018, Arxiv, DOI arXiv:1801.07606
   Li Y, 2018, ADV NEUR IN, V31
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu ST, 2019, Arxiv, DOI arXiv:1911.09516
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Paszke Adam, 2017, NIPS 2017 WORKSH AUT
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Ren S., 2015, ADV NEURAL INFORM PR, V28, P91
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu S, 2022, IEEE T MULTIMEDIA, V24, P2058, DOI 10.1109/TMM.2021.3075323
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xinjiang Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13356, DOI 10.1109/CVPR42600.2020.01337
   Xu H, 2019, IEEE I CONF COMP VIS, P6648, DOI 10.1109/ICCV.2019.00675
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Yuhang Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11580, DOI 10.1109/CVPR42600.2020.01160
   Zhang HK, 2020, Arxiv, DOI arXiv:2004.06002
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang SY, 2019, Arxiv, DOI arXiv:1905.11634
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
   Zhu Z, 2020, P 1 INT WORKSH HUM C, P73, DOI [10.1145/3422852.34234, DOI 10.1145/3422852.3423477]
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 63
TC 19
Z9 19
U1 36
U2 78
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2153
EP 2163
DI 10.1109/TMM.2022.3143707
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100043
DA 2024-07-18
ER

PT J
AU Yi, R
   Ye, ZP
   Sun, ZY
   Zhang, JY
   Zhang, GX
   Wan, PF
   Bao, HJ
   Liu, YJ
AF Yi, Ran
   Ye, Zipeng
   Sun, Zhiyao
   Zhang, Juyong
   Zhang, Guoxin
   Wan, Pengfei
   Bao, Hujun
   Liu, Yong-Jin
TI Predicting Personalized Head Movement From Short Video and Speech Signal
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generative models; head motion behavior pattern; speech-driven
   animation; talking face video synthesis
ID FACIAL EXPRESSION; GAN; MOTION
AB Audio-driven talking face video generation has attracted much attention recently. However, few existing works pay attention to machine learning of talking head movement, especially based on the phonetic study. Observing that real-world talking faces often accompany natural head movement, in this paper, we model the relation between speech signal and talking head movement, which is a typical one-to-many mapping problem. To solve this problem, we propose a novel two-step mapping strategy: (1) in the first step, we train an encoder that predicts a head motion behavior pattern (modeled as a feature vector) from the head motion sequence of a short video of 10-15 seconds, and (2) in the second step, we train a decoder that predict a unique head motion sequence from both the motion behavior pattern and the auditory features of an arbitrary speech signal. Based on the proposed mapping strategy, we build a deep neural network model that takes a speech signal of a source person and a short video of a target person as input, and outputs a synthesized high-fidelity talking face video with personalized head pose. Extensive experiments and a user study show that our method can generate high-quality personalized head movement in synthesized talking face videos, and meanwhile, has comparable facial animation quality (e.g., lip synchronization and expression) with the state-of-the-art methods.
C1 [Yi, Ran] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Ye, Zipeng; Sun, Zhiyao; Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, BNRist, Beijing 100190, Peoples R China.
   [Zhang, Juyong] Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Peoples R China.
   [Zhang, Guoxin; Wan, Pengfei] Kuaishou Technol, Beijing 100085, Peoples R China.
   [Bao, Hujun] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
C3 Shanghai Jiao Tong University; Tsinghua University; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS; Zhejiang
   University
RP Liu, YJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, BNRist, Beijing 100190, Peoples R China.
EM ranyi@sjtu.edu.cn; yezp17@mails.tsinghua.edu.cn;
   sunzy21@mails.tsinghua.edu.cn; juyong@ustc.edu.cn;
   zhangguoxin@kuaishou.com; wanpengfei@kuaishou.com; bao@cad.zju.edu.cn;
   liuyongjin@tsinghua.edu.cn
RI Yi, Ran/AAU-6636-2021
OI Yi, Ran/0000-0003-1858-3358; Bao, Hujun/0000-0002-2662-0334; Sun,
   Zhiyao/0000-0002-6377-7103
FU National Key Research and Development Program of China
FX No Statement Available
CR Afouras T, 2018, Arxiv, DOI arXiv:1809.00496
   Averbuch-Elor H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130818
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chung J. S., 2017, P BRIT MACH VIS C
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   el Kaliouby R, 2005, LECT NOTES COMPUT SC, V3784, P582
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fried O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323028
   Garrido P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2890493
   Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874
   Greenwood D, 2018, INTERSPEECH, P2484, DOI 10.21437/Interspeech.2018-2587
   Guo YD, 2019, IEEE T PATTERN ANAL, V41, P1294, DOI 10.1109/TPAMI.2018.2837742
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283
   Lele Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P35, DOI 10.1007/978-3-030-58545-7_3
   Lu YX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480484
   Munhall KG, 2004, PSYCHOL SCI, V15, P133, DOI 10.1111/j.0963-7214.2004.01502010.x
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Narvekar ND, 2009, INT WORK QUAL MULTIM, P87, DOI 10.1109/QOMEX.2009.5246972
   NAZZARO JR, 1970, J EXP PSYCHOL, V84, P477, DOI 10.1037/h0020861
   Oh TH, 2019, PROC CVPR IEEE, P7531, DOI 10.1109/CVPR.2019.00772
   Olszewski K, 2017, IEEE I CONF COMP VIS, P5439, DOI 10.1109/ICCV.2017.580
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175
   Siarohin A, 2019, ADV NEUR IN, V32
   Song Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P919
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Thies Justus, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P716, DOI 10.1007/978-3-030-58517-4_42
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Villani C., 2003, TOPICS OPTIMAL TRANS, V58
   Vougioukas S., 2018, BRIT MACH VIS C
   Wang Ting-Chun, 2019, Advances in neural information processing systems (NIPS), P5014
   Wei DX, 2021, IEEE T MULTIMEDIA, V23, P2457, DOI 10.1109/TMM.2020.3011290
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Yang T, 2021, PROC CVPR IEEE, P672, DOI 10.1109/CVPR46437.2021.00073
   Yehia HC, 2002, J PHONETICS, V30, P555, DOI 10.1006/jpho.2002.0165
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhang C., 2021, P IEEECVF INT C COMP, P3867
   Zhang CX, 2023, IEEE T VIS COMPUT GR, V29, P1438, DOI 10.1109/TVCG.2021.3117484
   Zhang JL, 2021, Arxiv, DOI arXiv:2101.02000
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhang ZM, 2021, PROC CVPR IEEE, P3660, DOI 10.1109/CVPR46437.2021.00366
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhou Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417774
   Zhu LW, 2020, IEEE T MULTIMEDIA, V22, P45, DOI 10.1109/TMM.2019.2924591
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P523, DOI 10.1111/cgf.13382
NR 57
TC 2
Z9 2
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6315
EP 6328
DI 10.1109/TMM.2022.3207606
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300005
DA 2024-07-18
ER

PT J
AU Zhao, HY
   Wang, Q
   Zhan, GW
   Min, WD
   Zou, Y
   Cui, SM
AF Zhao, Haoyu
   Wang, Qi
   Zhan, Guowei
   Min, Weidong
   Zou, Yi
   Cui, Shimiao
TI Need Only One More Point (NOOMP): Perspective Adaptation Crowd Counting
   in Complex Scenes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptation models; Training; Task analysis; Labeling; Computer science;
   Kernel; Feature extraction; Crowd counting; multi-head parallel network;
   need only one more point; perspective-adaptive
AB Recently, solving the crowd counting problem under occlusion and complex perspective is a hot but difficult topic. Existing methods mainly constructed counters in parallel perspective, but when facing complex perspective, such as the influences of height difference and heavy occlusions, they fail to get good accuracy. To alleviate these problems, this work proposes a novel and interesting framework NOOMP (Need Only One More Point) for perspective adaptation crowd counting task in complex nature scenes. Firstly, this work considers that the common scenes in our daily life usually have the height difference, which brings complex perspective to crowd counting. So, a new labeled method, Absolute-geometry Gaussian Generation is proposed, which only needs one more point for each person in image and gets better accuracy. Secondly, the NOOMP framework consists of meta-learning structure and uses the few-shot way to train the counting model, which can implement the perspective adaptation effective and solve the problem of high label cost. Thirdly, for fitting the characteristic of few-shot learning, this work proposes a new Multi-head Parallel Network (MPNet) for NOOMP. The feature of crowd is extracted by MPNet, which is a hybrid structure composed of shallow network and deep network. This network can save the features of shallow network and the deeper network effectively, which makes MPNet performs well in NOOMP. In addition, this work collects a new dataset, named Multiple Height Differences in Mall (MHDM) for NOOMP, which contains images of different views and height differences from shopping malls and supermarkets. Experiments based on MHDM and other benchmarks show that the NOOMP has good performances in model accuracy and works well for solving perspective change problem.
C1 [Zhao, Haoyu; Zhan, Guowei; Min, Weidong; Zou, Yi; Cui, Shimiao] Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.
   [Zhao, Haoyu] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.
   [Zhao, Haoyu] Shanghai Collaborat Innovat Ctr Intelligent Visual, Shanghai 200433, Peoples R China.
   [Wang, Qi] Nanchang Univ, Sch Software, Nanchang, Peoples R China.
   [Min, Weidong] Nanchang Univ, Inst Metaverse, Nanchang 330031, Peoples R China.
   [Min, Weidong] Jiangxi Key Lab Smart City, Nanchang 330031, Peoples R China.
C3 Nanchang University; Fudan University; Nanchang University; Nanchang
   University
RP Min, WD (corresponding author), Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.
EM zhaohaoyu@email.ncu.edu.cn; wangqi@ncu.edu.cn;
   zhanguowei@email.ncu.edu.cn; minweidong@ncu.edu.cn;
   6109117109@email.ncu.edu.cn; 406100210099@email.ncu.edu.cn
RI Min, Weidong/D-4585-2017
OI Min, Weidong/0000-0003-2526-2181; Zhao, Haoyu/0000-0003-3832-6439
FU National Natural Science Foundation of China [62076117, 62166026];
   Jiangxi Key Laboratory of Smart City [20192BCD40002]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62076117 and 62166026, and in part by
   Jiangxi Key Laboratory of Smart City under Grant 20192BCD40002.
CR Chai LY, 2022, IEEE T PATTERN ANAL, V44, P2856, DOI 10.1109/TPAMI.2020.3043372
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Ding GC, 2023, IEEE T MULTIMEDIA, V25, P4665, DOI 10.1109/TMM.2022.3180222
   Finn C, 2017, PR MACH LEARN RES, V70
   Go H, 2021, IEEE IMAGE PROC, P509, DOI 10.1109/ICIP42928.2021.9506384
   Hu D, 2020, Arxiv, DOI arXiv:2005.07097
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Kong XY, 2020, INT CONF ACOUST SPEE, P2722, DOI [10.1109/ICASSP40776.2020.9054258, 10.1109/icassp40776.2020.9054258]
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Liang DK, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3445-y
   Liu CC, 2019, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2019.00131
   Liu L, 2021, IEEE T MULTIMEDIA, V23, P1060, DOI 10.1109/TMM.2020.2992979
   Liu LB, 2021, PROC CVPR IEEE, P4821, DOI 10.1109/CVPR46437.2021.00479
   Liu WZ, 2022, IEEE T PATTERN ANAL, V44, P8151, DOI 10.1109/TPAMI.2021.3102690
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Liu XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3195, DOI 10.1109/ICCV48922.2021.00320
   Liu YT, 2019, PROC CVPR IEEE, P6462, DOI 10.1109/CVPR.2019.00663
   Ma YJ, 2022, IEEE T MULTIMEDIA, V24, P261, DOI 10.1109/TMM.2021.3050059
   Ma ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3185, DOI 10.1109/ICCV48922.2021.00319
   Meng YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15529, DOI 10.1109/ICCV48922.2021.01526
   Reddy MKK, 2022, IEEE T MULTIMEDIA, V24, P1008, DOI 10.1109/TMM.2021.3062481
   Reddy MKK, 2020, IEEE WINT CONF APPL, P2803, DOI [10.1109/WACV45572.2020.9093409, 10.1109/wacv45572.2020.9093409]
   Sam DB, 2021, IEEE T PATTERN ANAL, V43, P2739, DOI 10.1109/TPAMI.2020.2974830
   Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969
   Song QY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3345, DOI 10.1109/ICCV48922.2021.00335
   Tian Y, 2021, Arxiv, DOI [arXiv:2109.14483, DOI 10.48550/ARXIV.2109.14483]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan J, 2022, IEEE T PATTERN ANAL, V44, P1357, DOI 10.1109/TPAMI.2020.3022878
   Wan J, 2019, IEEE I CONF COMP VIS, P1130, DOI 10.1109/ICCV.2019.00122
   Wang MJ, 2023, IEEE T MULTIMEDIA, V25, P2074, DOI 10.1109/TMM.2022.3142398
   Wang Q, 2022, IEEE T MULTIMEDIA, V24, P1031, DOI 10.1109/TMM.2021.3104141
   Wei ZP, 2022, AAAI CONF ARTIF INTE, P2668
   Wu ZX, 2021, INT J COMPUT VISION, V129, P2965, DOI 10.1007/s11263-021-01508-1
   Wu ZX, 2022, IEEE T PATTERN ANAL, V44, P1699, DOI 10.1109/TPAMI.2020.3029425
   Xu CF, 2019, IEEE I CONF COMP VIS, P8381, DOI 10.1109/ICCV.2019.00847
   Yan ZY, 2023, IEEE T CIRC SYST VID, V33, P6544, DOI 10.1109/TCSVT.2021.3137593
   Yan ZY, 2022, IEEE T MULTIMEDIA, V24, P2633, DOI 10.1109/TMM.2021.3086709
   Yang H, 2021, IEEE T MULTIMEDIA, V23, P572, DOI 10.1109/TMM.2020.2985536
   Zhang AR, 2023, IEEE T MULTIMEDIA, V25, P1773, DOI 10.1109/TMM.2022.3162710
   Zhang AR, 2022, IEEE T CIRC SYST VID, V32, P6686, DOI 10.1109/TCSVT.2022.3179824
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang Q, 2021, PROC CVPR IEEE, P557, DOI 10.1109/CVPR46437.2021.00062
   Zhang Q, 2019, PROC CVPR IEEE, P8289, DOI 10.1109/CVPR.2019.00849
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhao HY, 2022, IEEE T EMERG TOP COM, V10, P1645, DOI 10.1109/TETC.2021.3115625
   Zhou OETY, 2022, IEEE T PATTERN ANAL, V44, P3602, DOI 10.1109/TPAMI.2021.3056518
   Zhu PF, 2021, IEEE T IMAGE PROCESS, V30, P5339, DOI 10.1109/TIP.2021.3082297
NR 47
TC 2
Z9 2
U1 2
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1414
EP 1426
DI 10.1109/TMM.2022.3230337
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000002
DA 2024-07-18
ER

EF